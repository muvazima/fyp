@&#MAIN-TITLE@&#Multi-scale prediction of water temperature using empirical mode decomposition with back-propagation neural networks

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           The novel model which combines EMD and BPNN algorithm is presented to predict water temperature in intensive aquaculture..


                        
                        
                           
                           Using EMD technology adaptively decomposed the original water temperature data into a finite set of IMFs and a residue.


                        
                        
                           
                           EMD-BPNN has higher prediction accuracy and better generalization performance than standard BPNN and standard SVR.


                        
                        
                           
                           EMD-BPNN can be used as a suitable and effective modeling tool for predicting water temperature in intensive aquaculture.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Empirical mode decomposition

Back-propagation neural network

Water temperature

Multi-scale prediction

@&#ABSTRACT@&#


               
               
                  In order to reduce aquaculture risks and optimize the operation of water quality management in prawn engineering culture ponds, this paper proposes a novel water temperature forecasting model based on empirical mode decomposition (EMD) and back-propagation neural network (BPNN). First, the original water temperature datasets are decomposed into a collection of intrinsic mode functions (IMFs) and a residue by EMD yields relatively stationary sub-series that can be readily modeled by BPNN. Second, both IMF components and residue is applied to establish the corresponding BPNN models. Then, each sub-series is predicted using the corresponding BPNN. Finally, the prediction values of the original water temperature datasets are calculated by the sum of the forecasting values of every sub-series. The proposed hybrid model was applied to predict water temperature in prawn culture ponds. Compared with traditional models, the simulation results of the hybrid EMD–BPNN model demonstrate that de-noising and capturing non-stationary characteristics of water temperature signals after EMD comprise a very powerful and reliable method for predicting water temperature in intensive aquaculture accurately and quickly.
               
            

@&#INTRODUCTION@&#

In the engineering breeding process of prawn, water temperature that is too high or too low will affect the growth and metabolism of shrimp [1,2], and can directly and indirectly influence other water quality indexes, trigger deterioration of water quality, and lead to disease outbreaks seriously affecting normal production and business operation [3]. Therefore, it is of utmost importance to use intelligent information processing technology to forecast water temperature and dissolved oxygen content [4]. More specifically, accurately predicting water quality not only provides a basis for water quality control and management decisions that can minimize aquaculture risks and optimize the treatment operation, but also ensures the healthy and sustainable development of the shrimp industry [5,6].

In recent years, many domestic and foreign experts have studied artificial neural networks (ANN) for water quality prediction [7–10]. For example, Ma et al. utilized backpropagation neural network as a modeling tool for the prediction of water quality in intensive Litopenaeus vannamei shrimp tanks [7]. Faruk proposed a hybrid ANN and ARIMA model to predict a time series of water quality data; their results indicate that the hybrid model produces better predictions than either the ARIMA model or the neural network [8]. Sarkar and Pandey presented a feed forward error back propagation neural network to estimate the DO concentrations at the downstream of Mathura city, India, located at the bank of river Yamuna in the state of Uttar Pradesh, India. The experimental results demonstrate its effectiveness [9]. DeWeber and Wagne developed an artificial neural network (ANN) ensemble model to predict mean daily water temperature in 197,402 individual stream reaches during the warm season throughout the native range of brook trout Salvelinus fontinalis in the eastern U.S. [10]. These studies indicate a growing interest in using ANN as a useful technique for predicting water quality due to the ability to capture subtle functional relationships within empirical data even when the underlying relationships are unknown or difficult to describe [11,12]. They do not require strong model assumptions and can map any nonlinear function without a priori assumption about the properties of the data [13,14] and as such have higher forecasting accuracy. These results encourage the adoption of back-propagation neural networks (BPNN) for studies of water quality forecasting.

When using BPNN for water quality prediction, the observed original values of prediction variables are usually directly used for building prediction models [14]. However, as water quality in prawn intensive aquacultures consistently suffers from various factors contributing to instability, including unpredictable weather conditions, biological factors, chemical in the water, dynamic hydrodynamics, human activity and more [5,12,13], the water quality series often shows a highly nonlinear and inherently non-stationary characteristic, which makes capturing its non-stationary properties and accurately describing water quality trends immensely difficult.

The empirical mode decomposition (EMD) approach can decompose complicated signal into a series of intrinsic mode functions (IMFs) through the sifting process [14,15]. Each IMF component represents only one mode of oscillation imbedded in the signal at a certain scale or frequency band that is quite distinct from others. It also can reveal hidden patterns and trends of time series, which can effectively assist in designing prediction models for various applications, such as in the prediction of non-stationary wind speed [16,17], automatic bearing fault diagnosis [18], kurtosis forecasting of bearing vibration signal [19], PM2.5 forecasting [20], and electricity demand forecasting [21], among others. However, existing literatures regarding water quality forecasting have not adopted EMD processes. This study attempts to fill this gap.

Motivated by previous studies [14,16], we present a novel prediction model based on EMD and BP neural networks models to predict water quality in intensive prawn aquaculture ponds. This paper is organized as follows: Section 2 describes EMD and BP neural networks methods; Section 3 describes the proposed model; Section 4 presents the experimental results; and Section 5 provides the conclusions of this study.

Empirical mode decomposition (EMD) is a nonlinear signal adaptive decomposition technique proposed by Huang et al. [22]. It is used to decompose a nonlinear and non-stationary time series into a sum of intrinsic mode function (IMF) components with individual intrinsic time scale properties. In contrast to other data analysis methods, the key innovation embodied in the EMD, which has allowed it to be applied broadly, is that the basic functions are derived directly from the signal itself [23].

According to Huang et al. [22], an IMF is a function that represents a hidden oscillation mode embedded in the data series, which must satisfy the following two conditions: (1) the number of extrema and the number of zero crossings are either equal or differ at most by one, which corresponds loosely to finding “narrow-band” signals or eliminating “riding-waves”; and (2) at any point, the mean of its upper and lower envelopes equals zero, which ensure that the instantaneous frequency will not have unwanted fluctuations arising from asymmetric wave forms. The detailed decomposition process of EMD is presented by Huang et al. [16]. Suppose that a data time series x(t) can be decomposed according to the following procedure [22,23]:
                           
                              (1)
                              Identify all the local maxima and minima of x(t).

Connect all local extrema by a cubic spline line to generate x(t) upper and lower envelopes xup
                                 (t) and xlow
                                 (t) of the x(t).

Adopt the upper envelope xu
                                    p
                                 (t) and the lower envelope xl
                                    ow
                                 (t) to compute the first mean time series m
                                 1(t), i.e., m1
                                 (t) = [xup
                                 (t) + xlow
                                 (t)]/2.

Evaluate the difference between the original time series x(t) and the mean time series to achieve the first IMF h
                                 1(t), i.e., h
                                 1(t) = x(t) − m
                                 1(t). Check whether h
                                 1(t) satisfies the two conditions of an IMF property. If they are not satisfied, repeat Steps 1 through 3 of the decomposition procedure until the first IMF is found.

The EMD extracts the next IMF by applying the above sifting procedure to the residual term r1
                                 (t) = x(t) − c1
                                 (t), where c1
                                 (t) denotes the first IMF. The decomposition process can be repeated until the last residue rn
                                 (t) has at most one local extremum or becomes a monotonic function from which no more IMFs can be extracted, at which point the decomposition procedure is halted [23].

The original time series x(t) can be reconstructed by summing all the IMF components and the one residue component as follows [22]:
                           
                              (1)
                              
                                 
                                    x
                                    
                                       (
                                       t
                                       )
                                    
                                    =
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       n
                                    
                                    
                                       
                                          c
                                          i
                                       
                                       
                                          (
                                          t
                                          )
                                       
                                       +
                                       
                                          r
                                          n
                                       
                                       
                                          (
                                          t
                                          )
                                       
                                    
                                 
                              
                           
                        where n is the number of IMFs and ci
                        (t) (i = 1, 2, … , n) are the values of each IMF, which are nearly orthogonal to each other, which are different in each frequency band, and which change with variation of time series signal x(t). The rn
                        (t) is the final residue, which represents the central trend of time series signal x(t). Thus, every signal can accomplish decomposition of the data series into n-empirical mode functions and one residue.

The back-propagation neural network is a supervised learning technique [24], and it has recently been used to deal with approximation of nonlinear maps. In the network, there is an input layer, an output layer, and one or more hidden layers between them. The topology of the BPNN is shown in Fig. 1.
                        
                     

During training, an input pattern is given to the input layer of the network. Based on the given input pattern, the network will compute the output in the output layer. This network output is then compared with the desired output pattern. The aim of the back-propagation learning rule is to define a method of adjusting the weights of the networks. Eventually, the network will give the output that matches the desired output pattern given any input pattern in the training set.

The input Ik
                         and output Ok
                         to the kth neuron are determined by the following equations:
                           
                              (2)
                              
                                 
                                    
                                       I
                                       k
                                    
                                    =
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       n
                                    
                                    
                                       
                                          μ
                                          
                                             i
                                             ,
                                             k
                                          
                                       
                                       
                                          O
                                          i
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (3)
                              
                                 
                                    
                                       O
                                       k
                                    
                                    =
                                    f
                                    
                                       (
                                       
                                          I
                                          k
                                       
                                       +
                                       
                                          ϑ
                                          k
                                       
                                       )
                                    
                                 
                              
                           
                        where μi,k
                         is the weight of the connection from the ith neuron in the previous layer to the kth neuron, f(Ik
                         + ϑk
                        ) represents the activation function of the neurons, Ok
                         is the output of neuron k, and ϑk
                         is the biases input to the neuron [25]. In order to improve performance, we adopted the bipolar sigmoid activation function, which is defined as follows:
                           
                              (4)
                              
                                 
                                    f
                                    
                                       (
                                       x
                                       )
                                    
                                    =
                                    
                                       2
                                       
                                          1
                                          +
                                          exp
                                          (
                                          −
                                          x
                                          )
                                       
                                    
                                    −
                                    1
                                 
                              
                           
                        
                     

Additionally, we used the mean square error (MSE) to evaluate the learning effects of BPNN. The training continues until the MSE falls below some threshold or tolerance level. The MSE is defined as follows:
                           
                              (5)
                              
                                 
                                    MSE
                                    =
                                    
                                       1
                                       n
                                    
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       n
                                    
                                    
                                       
                                          (
                                          y
                                          
                                             
                                             i
                                          
                                          −
                                          
                                             
                                                y
                                                ^
                                             
                                             i
                                          
                                          )
                                       
                                       2
                                    
                                 
                              
                           
                        where n is the number of training patterns, and yi
                         and ŷi
                         are actual and prediction values, respectively.

The framework of the water quality forecasting based on EMD–BPNN is shown in Fig. 2
                     . The detailed calculation steps are described as follows:
                        
                           Step 1.
                           Use the EMD to decompose the original water quality into a finite set of different sub-series which can be identified, separately predicted and recombined to achieve aggregate forecasting.

Apply the BPNN. Building a forecasting model for the each subseries, the BPNN models are then applied to forecast values of these sub-series for the next ten minutes.

The final forecasting value is obtained by the sum of the above predicted results for the original time series.

Through the EMD method, raw water quality data having dissimilar characteristics can be displayed on different scales, which allows for more fully capturing the local fluctuations of raw water quality data. Moreover, each IMF (here the different sub-series are considered one IMF) has similar frequency characteristics, simple frequency components and strong regularity, therefore allowing this model to reduce the complexity of BPNN modeling and further improve BPNN forecast efficiency and accuracy.

@&#RESULTS AND DISCUSSION@&#

For evaluating the performance of the proposed EMD–BPNN prediction model, the water quality data in prawn culture ponds were collected at ten-minute intervals from July 1, 2013 to July 8, 2013. The 144 sets of data collected daily yielded a total of 1152 observation samples. All data employed in this study were obtained by a digital wireless monitoring system for aquaculture water quality. The system had been installed at prawn culture ponds in Panyu district, Guangzhou city, Guangdong province, China. The original daily water quality time series is plotted in Fig. 3
                        . The plot exhibits a permanent deterministic pattern of long-term upward trend with short-term fluctuations that are independent from one time period to the next. Fig. 3 shows that the water temperature series have non-stationary character and the distribution of wind speed series is not Gaussian distribution. At the same time, Fig. 3 suggests a cyclical pattern with a mean cycle of about one day.

In order to further test the performance of the suggested forecasting methods, the collected data was divided into two sets: training data and testing data. To achieve a more reliable and accurate result, a longer period served as the training period. Based on these considerations, the first 1010 data points (87.5% of the total) were used as the training sample while the remaining 144 data points (12.5% of the total) were used as the testing sample. Therefore, the period from July 1 to July 7 was selected as the training period, while July 8 was selected as the testing period. Additionally, one-step-ahead prediction was performed.

In order to analyze and evaluate the prediction performance of EMD–BPNN, the root mean square error (RMSE), the mean absolute percentage error (MAPE) and the mean squared error (MSE) were used as performance indicators to evaluate the prediction capability. Eqs. (6), (7) and (8) illustrate the expression of MAPE (%), MAD and RMSE, respectively:
                           
                              (6)
                              
                                 
                                    MAPE
                                    
                                    
                                       (
                                       %
                                       )
                                    
                                    =
                                    
                                       100
                                       n
                                    
                                    
                                       ∑
                                       
                                          I
                                          =
                                          1
                                       
                                       n
                                    
                                    
                                       |
                                       
                                          
                                             
                                                y
                                                i
                                             
                                             −
                                             
                                                
                                                   y
                                                   ^
                                                
                                                i
                                             
                                          
                                          
                                             y
                                             i
                                          
                                       
                                       |
                                    
                                 
                              
                           
                        
                        
                           
                              (7)
                              
                                 
                                    MAD
                                    =
                                    
                                       1
                                       n
                                    
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       n
                                    
                                    
                                       
                                          |
                                       
                                       
                                          y
                                          i
                                       
                                       −
                                       
                                          
                                             y
                                             ^
                                          
                                          i
                                       
                                    
                                    
                                       |
                                    
                                 
                              
                           
                        
                        
                           
                              (8)
                              
                                 
                                    RMSE
                                    =
                                    
                                       
                                          
                                             1
                                             n
                                          
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             n
                                          
                                          
                                             
                                                (
                                                
                                                   
                                                      y
                                                      i
                                                   
                                                   −
                                                   
                                                      
                                                         y
                                                         ^
                                                      
                                                      i
                                                   
                                                
                                                )
                                             
                                             2
                                          
                                       
                                    
                                 
                              
                           
                        where n is the total number of actual data points in the data set and yi
                         and ŷi
                         are original and prediction values, respectively.

Most previous prediction studies only directly attempt to bring the fitting values as close as possible to the real values through the experimental method, even though the experimental method is repetitive and time consuming. However, due to the non-stationary nature and intrinsic complexity of the original water temperature datasets, it is very difficult to directly describe the moving tendency of water temperature directly by using the proposed prediction models. In order to improve the prediction accuracy and obtain decision information for water quality management, we have applied the EMD method to decompose water temperature in the prawn culture ponds.

IMFs are obtained from the original water temperature datasets by the EMD. Fig. 4
                        
                         shows the decomposition process of the original water temperature datasets. Clearly, the number of water temperature series is decomposed into ten IMFs and one residue, which exhibits a stable and regular variation. This means that the interruption and coupling between information having different characteristics embedded in the original data have been weakened to some extent. Thus, the forecasting model is easier to build. After using EMD to decompose the original water temperature datasets into ten IMFs and a residue, these were then used to build the corresponding BPNN forecasting model.

In the previous section, the original water temperature datasets are decomposed into ten IMFs and a residue by EMD, and each sub-series is used to build the corresponding BPNN forecasting model. In this section, each BPNN is applied to forecast the corresponding sub-series, and the final prediction of the original water temperature is obtained by aggregating the prediction results of each sub-series. The one-step-ahead forecast is adopted in this study, namely, it used the previous ten minutes’ water temperature to predict that of the next ten minutes.

In this study, the proposed EMD–BPNN algorithm was implemented in Matlab7.12.0 programming language. The experiment was performed on a 2.50 GHz Core(TM) i5-3210M CPU personal computer with 4.0 G memory under Microsoft Windows 7 Ultimate edition. The initial architecture of the BP neural network consisted of 11 input variables, one output variable, the hidden layer with seven initial neurons, and a learning rate of 0.03. The stimulating function was bipolar sigmoid; a maximum of 2000 iterations or an MSE less than or equal to 0.0001 were adopted as the termination criterion. The network topology with the minimum testing MSE was considered the optimal network. In modeling the standard BPNN model after the EMD–BPNN model, the hidden layer consisted of six initial neurons, whereas the input and output layers had one node each. Fig. 5 illuminates the water temperature prediction result given by EMD–BPNN. From Fig. 5 it is clear that the proposed model of EMD-BPNN achieves excellent generalization performance since the EMD–BPNN is a good compromise for guaranteeing improvement in both stability and accuracy and is a suitable and effective method for predicting the water temperature of water quality in in modern intensive prawn culture.

To analyze and compare prediction performance, a standard BPNN model with the same structure (referred to as standard BPNN model) and a standard support vector regression model (referred to as standard SVR model) were used to forecast the sequence for the same period of time, while the sample data were not processed by EMD. The standard SVR parameters were found using a 5-fold cross-validation method, and the selected optimal values of C, ε and δ were 28.526, 0.041 and 0.139, respectively. The initial architecture of the BPNN consisted of eleven input variables, one output variable, a hidden layer with seven initial neurons, a learning rate of 0.03, and the maximum training steps are 2000. Fig. 6
                         illuminates the water temperature series forecasting result. It can be observed from Fig. 6 that the predicted values obtained from the proposed model of EMD–BPNN are closer to the actual data than those obtained from the standard BPNN model and standard SVR model. Table 1
                         compares the results obtained with the EMD–BPNN, standard BPNN and standard SVR models for water temperature.

For the same BPNN, the relative MAPE, MAD, RMSE and run time t differences between the EMD–BPNN and standard BPNN model were 55.33%, 49.05%, 58.38%, and 0.51 s in the test period, respectively. Fig. 6 and Table 1 depicts our hybrid model has yielded significantly more reliable performance and generalization ability and higher prediction precision than standard BPNN mode. The relative MAPE, MAD, RMSE and run time t differences between the EMD–BPNN and standard SVR model were 32.80%, 30.24%, 48.96%, and 0.22 s in the test period, respectively. It is clear that EMD–BPNN also has more accurate results than standard SVR. For the EMD approach has more capability to capture the non-stationary characteristics of the water temperature, the complex water temperature time-series are decomposed into simple time-series of different resolution intervals by EMD. Thus, some features of the sub-series can be seen more clearly than the water temperature signal. The BPNN model is constructed with appropriate sub-series of different scales. The forecasts are more accurate than that obtained directly by original water temperature signals due to the fact that the features of the water temperature sub-series are obvious. This study proposes the hybrid model algorithm applied to predict water temperature in prawn culture ponds. The result of application demonstrates that the EMD–BPNN model is a suitable and effective method for water quality management in modern intensive prawn cultures.

@&#CONCLUSION@&#

Grasping the trend of the water temperature timely and accurately and regulating water quality dynamics are the key for healthy growth in the non-stress environment of aquatic products. In this paper, an efficient water temperature forecasting model based on EMD and BPNN techniques. The proposed EMD–BPNN model exploits the learning and generalization capability of BPNN for forecasting water temperature in prawn engineering culture ponds. It also uses EMD for selecting apposite water temperature properties. Using actual monitored aquaculture water quality data of prawn culture factories of Panyu district, Guangzhou city, Guangdong province, China, the experimental results clearly show that compared with standard BPNN and standard SVR, the proposed hybrid model of EMD–BPNN has better prediction performance in MAPE, MAD and RMSE. Therefore, the proposed EMD–BPNN method is both highly suitable and efficient for water quality prediction in large-scale intensive aquacultures with nonlinear, non-stationary and strong complexity water quality data.

@&#ACKNOWLEDGMENTS@&#

The authors would like to thank native English speaking expert Laurie Schiller to polish our paper. This research was supported by the Special Fund for Agro-scientific Research in the Public Interest (no. 201203017), National Natural Science Foundation Framework Project (no. 61471133), National Science and Technology Supporting Plan Project (no. 2012BAD35B07), Guangdong Science and Technology Plan Project (nos. 2013B090500127, 2013B021600014, 2015A070709015 and 2015A020209171), and Guangdong Natural Science Foundation Project (no. S2013010014629).

@&#REFERENCES@&#

