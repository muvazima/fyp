@&#MAIN-TITLE@&#Notes on ‘Hit-And-Run enables efficient weight generation for simulation-based multiple criteria decision analysis’

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Hit-And-Run enables efficient sampling of weights with linear inequality constraints.


                        
                        
                           
                           A generalized change-of-basis can extend the method to handling equality constraints.


                        
                        
                           
                           A flawed seedpoint generating method should be replaced by a linear program.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Multiple criteria analysis

Simulation

Uncertainty modeling

@&#ABSTRACT@&#


               
               
                  In our previous work published in this journal, we showed how the Hit-And-Run (HAR) procedure enables efficient sampling of criteria weights from a space formed by restricting a simplex with arbitrary linear inequality constraints. In this short communication, we note that the method for generating a basis of the sampling space can be generalized to also handle arbitrary linear equality constraints. This enables the application of HAR to sampling spaces that do not coincide with the simplex, thereby allowing the combined use of imprecise and precise preference statements. In addition, it has come to our attention that one of the methods we proposed for generating a starting point for the Markov chain was flawed. To correct this, we provide an alternative method that is guaranteed to produce a starting point that lies within the interior of the sampling space.
               
            

@&#INTRODUCTION@&#

Multi-Attribute Value Theory (MAVT) is widely applied in multi-criteria decision making contexts to make a choice from a set of alternatives, or to rank them from the best to the worst. This is achieved by constructing a value function that associates with each alternative a real number indicating the total value, or utility, of this alternative to the decision maker. In practical applications of MAVT, it is usually assumed that the decision makers’ preference structures satisfy the preferential independence conditions necessary for the application of the additive value model. The problem of eliciting preferences then reduces to specifying a set of single-attribute value functions and a set of weights that reflect the relative importances of unit increases in these functions (Keeney & Raiffa, 1976).

Traditional preference elicitation approaches rely on relatively complex questioning techniques to elicit the required model parameters from a decision maker, such as the bisection method for specifying the partial value functions, and swing weighting for determining the scaling constants (Belton & Stewart, 2002). In practice, however, decision makers are not always able or willing to provide precise preference information, meaning that their preference structures cannot be uniquely specified. In response to this problem, three generic strategies have been developed to extend the use of MAVT to settings where only imprecise preference information is available, or where a mixture of precise and imprecise preference information can be obtained: (i) inference of a single representative value function that is in some way the most consistent one with the provided value judgments (Greco, Kadziński, & Słowiński, 2011; Kadziński, Greco, & Słowiński, 2012), (ii) random sampling of a set of value functions from the restricted parameter space defined by these judgments (Kadziński & Tervonen, 2013a, 2013b), and (iii) exploring the whole set of value functions compatible with the preferences of the decision maker as in the Robust Ordinal Regression (ROR) approach (Greco, Mousseau, & Słowiński, 2008).

As both ROR and the inferential approach for handling imprecise preference information are based on linear programming, they can be used with an arbitrary combination of ordinal, categorical, and imprecise numerical judgments provided that each of these judgments can be expressed as linear constraints on the parameter space (Belton & Stewart, 2002). However, due to the lack of efficient sampling algorithms, the practical application of the simulation-based approach was until recently restricted to specific parameter spaces, such as the 
                        
                           (
                           n
                           -
                           1
                           )
                        
                     -simplex in n-dimensional space, which reflects complete ignorance of the weights for the additive value function (Tervonen & Lahdelma, 2007). In our previous work (Tervonen, van Valkenhoef, Baştürk, & Postmus, 2013), we provided a partial solution to this problem by showing how the HAR sampler (Smith, 1984) can be applied to enable efficient sampling of the weights from a convex polytope constructed by restricting a simplex with a set of linear inequality constraints. In this short communication, we describe how our previously proposed method for transforming the n-dimensional weight space to an 
                        
                           (
                           n
                           -
                           1
                           )
                        
                     -dimensional sampling space can be generalized to allow mixing imprecise and precise preference statements, which provides greater flexibility for the types of statement that can be elicited from the decision maker. In addition, it has come to our attention that one of the methods we proposed to generate the starting point for the Markov chain was flawed. As a remedy to this issue, we describe how the inferential approach for handling imprecise preference information can be utilized to generate a starting point that is guaranteed to lie within the interior of the sampling space.

All possible n-dimensional weight vectors lie on the hyperplane defined by the constraint 
                        
                           
                              
                                 ∑
                              
                              
                                 i
                                 =
                                 1
                              
                              
                                 n
                              
                           
                           
                              
                                 w
                              
                              
                                 i
                              
                           
                           =
                           1
                        
                     . To apply HAR to weight generation, the n-dimensional weight space must be reduced to an 
                        
                           (
                           n
                           -
                           1
                           )
                        
                     -dimensional sampling space because the hyperplane corresponding to the normalization constraint has zero volume and thus the probability of a random walk staying within the hyperplane is also zero. In our previous work (Tervonen et al., 2013, Section 3.1), we indicated how the normalization constraint 
                        
                           
                              
                                 ∑
                              
                              
                                 i
                              
                           
                           
                              
                                 w
                              
                              
                                 i
                              
                           
                           =
                           1
                        
                      can be eliminated by the composition of a change of basis and a translation. This can be generalized to enable weight sampling with any consistent set of linear constraints. For example, in a problem with three criteria (see Fig. 1
                     ), the transformation suggested in our previous paper allows the calculation of intermediate results during ordinal preference elicitation, first with no preferences 
                        
                           {
                           }
                        
                     , then with a single statement, e.g. 
                        
                           {
                           
                              
                                 w
                              
                              
                                 1
                              
                           
                           ⩾
                           
                              
                                 w
                              
                              
                                 2
                              
                           
                           ,
                           
                              
                                 w
                              
                              
                                 1
                              
                           
                           ⩾
                           
                              
                                 w
                              
                              
                                 3
                              
                           
                           }
                        
                     , and finally with the full ranking, e.g. 
                        
                           {
                           
                              
                                 w
                              
                              
                                 1
                              
                           
                           ⩾
                           
                              
                                 w
                              
                              
                                 2
                              
                           
                           ,
                           
                              
                                 w
                              
                              
                                 2
                              
                           
                           ⩾
                           
                              
                                 w
                              
                              
                                 3
                              
                           
                           }
                        
                     . The generalization below allows the calculation of intermediate results during the subsequent trade-off weighting, e.g. with preferences such as 
                        
                           {
                           
                              
                                 w
                              
                              
                                 1
                              
                           
                           =
                           1.3
                           
                              
                                 w
                              
                              
                                 2
                              
                           
                           ,
                           
                              
                                 w
                              
                              
                                 2
                              
                           
                           ⩾
                           
                              
                                 w
                              
                              
                                 3
                              
                           
                           }
                        
                     . This allows for greater flexibility in determining when the preference elicitation process can be terminated, for example due to the alternatives becoming sufficiently discriminated with the given preference statements.

Let the sampling space be embedded in 
                        
                           
                              
                                 R
                              
                              
                                 n
                              
                           
                        
                      and defined with the following constraints:
                        
                           
                              Cw
                              ⩽
                              b
                              ;
                              
                              Fw
                              =
                              g
                              .
                           
                        
                     The solution space of the equality constraints is then given by
                        
                           
                              w
                              =
                              
                                 
                                    F
                                 
                                 
                                    †
                                 
                              
                              g
                              +
                              (
                              I
                              -
                              
                                 
                                    F
                                 
                                 
                                    †
                                 
                              
                              F
                              )
                              y
                              ,
                           
                        
                     where 
                        
                           
                              
                                 F
                              
                              
                                 †
                              
                           
                        
                      is the Moore–Penrose pseudoinverse of F (Penrose, 1955; Penrose & Todd, 1956) and y is an n-vector representing the transformed weights. If 
                        
                           F
                           =
                           U
                           Σ
                           
                              
                                 V
                              
                              
                                 T
                              
                           
                        
                      is the singular value decomposition of F, then the pseudo-inverse is given by 
                        
                           
                              
                                 F
                              
                              
                                 †
                              
                           
                           =
                           V
                           
                              
                                 Σ
                              
                              
                                 †
                              
                           
                           
                              
                                 U
                              
                              
                                 T
                              
                           
                        
                     , and 
                        
                           
                              
                                 Σ
                              
                              
                                 i
                                 ,
                                 i
                              
                              
                                 †
                              
                           
                           =
                           1
                           /
                           
                              
                                 Σ
                              
                              
                                 i
                                 ,
                                 i
                              
                           
                        
                      where 
                        
                           
                              
                                 Σ
                              
                              
                                 i
                                 ,
                                 i
                              
                           
                           
                           ≠
                           
                           0
                        
                      and 
                        
                           
                              
                                 Σ
                              
                              
                                 i
                                 ,
                                 j
                              
                              
                                 †
                              
                           
                           =
                           0
                        
                      everywhere else. When 
                        
                           
                              
                                 F
                              
                              
                                 †
                              
                           
                           F
                           =
                           I
                        
                     , the solutions space consists of a single point 
                        
                           x
                           =
                           
                              
                                 F
                              
                              
                                 †
                              
                           
                           g
                        
                     . In other cases, the dimension of the solution space is 
                        
                           d
                           =
                           rank
                           (
                           I
                           -
                           
                              
                                 F
                              
                              
                                 †
                              
                           
                           F
                           )
                           <
                           n
                        
                     , and some components of the n-vector y are redundant. A non-redundant orthonormal basis D can be constructed from 
                        
                           (
                           I
                           -
                           
                              
                                 F
                              
                              
                                 †
                              
                           
                           F
                           )
                        
                      by taking the first d columns of the Q component of the QR decomposition of 
                        
                           (
                           I
                           -
                           
                              
                                 F
                              
                              
                                 †
                              
                           
                           F
                           )
                        
                     . The solution space is then given by:
                        
                           
                              w
                              =
                              
                                 
                                    F
                                 
                                 
                                    †
                                 
                              
                              g
                              +
                              
                                 
                                    Dy
                                 
                                 
                                    ′
                                 
                              
                              ,
                           
                        
                     where 
                        
                           
                              
                                 y
                              
                              
                                 ′
                              
                           
                        
                      is a d-vector. Because it is just a change of basis followed by a translation, this relation can be used to transform the inequality constraints defined on the n-dimensional space to the d-dimensional space in which HAR is applied, and its inverse to transform the generated samples to the original weight space (Tervonen et al., 2013).

In our previous work (Tervonen et al., 2013), we claimed that a starting point for the Markov chain can be generated by first determining the extreme points along each dimension and then taking a weighted average of those points. If the inequality constraints are given (in the sampling space) by 
                        
                           Ax
                           ⩽
                           b
                        
                     , the extreme points along the k-th dimension can be obtained by solving the following two linear programs (LPs):
                        
                           
                              
                                 
                                    
                                       
                                          maximize
                                       
                                    
                                    
                                       
                                          
                                          
                                             
                                                x
                                             
                                             
                                                k
                                             
                                          
                                          
                                          
                                          
                                          
                                          minimize
                                          
                                          
                                          
                                             
                                                x
                                             
                                             
                                                k
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       
                                          subject
                                          
                                          to
                                       
                                    
                                    
                                       
                                          
                                          Ax
                                          ⩽
                                          b
                                          
                                          
                                          subject
                                          
                                          to
                                          
                                          
                                          Ax
                                          ⩽
                                          b
                                       
                                    
                                 
                              
                           
                        
                     However, if the polytope is oriented in a certain way relative to the axes along which the LP are solved, the extreme points may all lie on a single (hyper) face of the polytope. For example, given the polytope with vertices 
                        
                           {
                           (
                           0
                           ,
                           0
                           )
                           ,
                           (
                           1
                           ,
                           1
                           )
                           ,
                           
                              
                                 
                                    
                                       
                                          1
                                       
                                       
                                          4
                                       
                                    
                                    ,
                                    
                                       
                                          3
                                       
                                       
                                          4
                                       
                                    
                                 
                              
                           
                           }
                        
                     , if we solve the LP along the natural axes we only get the points 
                        
                           (
                           0
                           ,
                           0
                           )
                        
                      and 
                        
                           (
                           1
                           ,
                           1
                           )
                        
                     , which thus span a space of only one dimension while the polytope itself is two-dimensional (Fig. 2
                     ). In that case the generated starting point will lie the boundary of the polytope rather than in the interior. Due to numerical accuracy issues, the starting point may even lie slightly outside of the polytope, leading to further problems. Replacing the set of extreme points by the vertices of the polytope (as we described in our previous work) would solve this problem, but enumerating the vertices is computationally expensive.

Here, we propose a simple, less expensive solution based on the slack-maximizing LP formulation underlying the inferential approach for handling imprecise preference information. If the inequality constraints are given (in sampling space) by 
                        
                           Ax
                           ⩽
                           b
                        
                     , the following LP results in an interior point that maximizes the minimum slack on each constraint defining the polytope:
                        
                           
                              
                                 
                                    
                                       
                                          maximize
                                       
                                    
                                    
                                       
                                          
                                          δ
                                       
                                    
                                 
                                 
                                    
                                       
                                          subject
                                          
                                          to
                                       
                                    
                                    
                                       
                                          
                                          Ax
                                          +
                                          e
                                          =
                                          b
                                       
                                    
                                 
                                 
                                    
                                    
                                       
                                          
                                          
                                             
                                                ∀
                                             
                                             
                                                i
                                             
                                          
                                          
                                          δ
                                          ⩽
                                          
                                             
                                                e
                                             
                                             
                                                i
                                             
                                          
                                          .
                                       
                                    
                                 
                              
                           
                        
                     
                  

A randomized interior point can then be generated by randomly rescaling the slack on each constraint:
                        
                           
                              
                                 
                                    
                                       
                                          maximize
                                       
                                    
                                    
                                       
                                          
                                          δ
                                       
                                    
                                 
                                 
                                    
                                       
                                          subject
                                          
                                          to
                                       
                                    
                                    
                                       
                                          
                                          Ax
                                          +
                                          e
                                          =
                                          b
                                       
                                    
                                 
                                 
                                    
                                    
                                       
                                          
                                          
                                             
                                                ∀
                                             
                                             
                                                i
                                             
                                          
                                          
                                          δ
                                          ⩽
                                          
                                             
                                                c
                                             
                                             
                                                i
                                             
                                          
                                          
                                             
                                                e
                                             
                                             
                                                i
                                             
                                          
                                          ,
                                       
                                    
                                 
                              
                           
                        
                     where the scaling factors 
                        
                           
                              
                                 c
                              
                              
                                 i
                              
                           
                           ∈
                           (
                           0
                           ,
                           1
                           ]
                        
                      can be drawn from independent uniform distributions. Before solving this program, we eliminate redundant constraints from 
                        
                           Ax
                           ⩽
                           b
                        
                      to prevent these constraints from biasing the starting point towards a specific region of the polytope. However, even with this precaution, the distribution obtained from this procedure is far from uniform, meaning that it should not be used in place of HAR for the actual weight sampling.

@&#IMPLEMENTATION@&#

The updated methods presented here have been implemented in version 0.4 of the hitandrun package for R, available from http://cran.r-project.org/package=hitandrun. Full source code and the change history is available at https://github.com/gertvv/hitandrun.

@&#REFERENCES@&#

