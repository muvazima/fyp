@&#MAIN-TITLE@&#Automatic usability and stress analysis in mobile biometrics

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We made usability stress tests on mobile biometrics successfully.


                        
                        
                           
                           We optimized resources through automation.


                        
                        
                           
                           Stress is not a major drawback for handwritten signature recognition.


                        
                        
                           
                           The use of colours as a feedback of the recognition benefits usability and performance.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Usability

Mobile devices

Biometrics

Stress

Handwritten signature recognition

@&#ABSTRACT@&#


               Graphical abstract
               
                  
                     
                        
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

One of the main drawbacks users find in biometric recognition [1] systems is the lack of usability. Almost all the work done in biometrics is devoted to improving algorithm performance and bringing the Equal Error Rate (EER) close to zero. But while this kind of research is necessary, working on improving user interaction with systems is also extremely important, as a lack of usability could mean not only the rejection of the system by the users, but also a reduction in the expected performance of the biometric system. There are previous usability works in biometrics in the literature [2] and most of them come from the usability definition given by the ISO 13407:1999 [3]: “the extent to which a product can be used by specified users to achieve specified goals with effectiveness, efficiency, and satisfaction in a specified context of use”. One of the most complete models published up to now is the Human Biometric System Interaction (HBSI) [4], which proposes methods and measures (including the ones recommended by ISO 13407:1999) to analyse the user–sensor interaction deeply. As this model has not yet been tested empirically in dynamic modalities [5], our work goes a step further, proposing some modifications to it, and therefore, the results obtained can be considered a novelty. Furthermore, this study includes stress tests where users sign under pressure conditions. The inclusion of these tests in the evaluation is motivated by some common scenarios where users are indirectly encouraged to sign quickly and carelessly (e.g. post offices, banks or supermarkets). Therefore, the main intention in this study is to measure the influence of stress in the recognition process, as this is one of the major concerns regarding usability and performance. These tests mean a novelty and an important advance in the improvement of security in mobile environments. Another relevant factor recently studied in handwritten signature recognition is the effect of ageing, which has been demonstrated to decrease the performance [6] [7]. In those works authors suggest different strategies to maximize the system accuracy over time, making the template updating less critical than expected.

It is important to note that the current tendency is to move from desktop computers to mobile devices, using them in mobile scenarios. Therefore the migration of biometrics to these scenarios has become an important topic nowadays. There are several published works focusing on the adaptation of biometrics to mobile devices, using different modalities such as the iris [8], hand [9] or fingerprint [10]. In our previous works with mobile devices and dynamic handwritten signature recognition [11], the algorithm applied was tested under different conditions but the evaluation of its usability was left for a future work, being covered by this paper.

In this experiment, 56 users (54 finished the whole process) signed in 2 sessions on a Samsung Galaxy Note [12] using a stylus. The process was split into user training, enrolment, verification and stress tests. Finally, the users had to complete a satisfaction questionnaire where they were asked about various usability aspects of the evaluation such as easiness or comfort. All signatures captured were real signatures (i.e. not invented, as they are the same ones that the users write when shopping with a credit card).

This paper is divided into 6 sections: in Section 2 the state-of-the-art is presented, while Section 3 explains the evaluation set-up, followed by a description of the experiments performed (Section 4). Results are shown and discussed in Section 5, finishing the paper with Section 6, where the conclusions obtained and the proposed future works are explained.

The three best known first usability studies in biometrics were: an enrolment trial in the UK [13] conducted by Atos, the guidelines drawn up by NIST [14] and the HBSI model [4]. All of these measure the efficiency, effectiveness and satisfaction as defined by ISO 13407:1999, but HBSI goes further, focussing on the human–system interaction and the potential errors this relationship involves. Therefore, this is the baseline model applied to this work. The following subsections provide an explanation of the HBSI model and its application to handwritten signature recognition.

This model includes several measurements in order to complete a full usability analysis covering ergonomics and signal processing, as shown in Fig. 1
                        . The HBSI was applied to various usability evaluations including most of the best known biometric modalities such as fingerprints [15] or hand geometry [16]. Nevertheless, the approaches to dynamic modalities were only theoretical [3], so this work proves the practical viability of the HBSI in behavioural biometrics. Additionally, this model incorporates interaction metrics in order to categorize the FTA (Failure to Acquire) errors during the sample acquisition process [3].

There are previous relevant studies in the literature about handwritten signature recognition in mobile devices [17] [18] and there are also commercial products using these kinds of algorithms [19]. Works previously carried out by authors regarding usability show interesting outcomes that have been applied to this experiment. These conclusions [8] are:
                           
                              
                                 Mobile device
                              
                              Regarding performance and users' opinions, capacitive devices are the preferred ones. Moreover, users prefer signing with a stylus to using the fingertip. The screen size and the device operative system were not considered as influential parameters.

None of the tested user positions for signing (seated or standing up; with the device resting on a table or held by the user) involves better performance results than the others and neither does the device situation. The users' training process has been demonstrated to be a highly influential factor for both effectiveness and efficiency [20], so we have considered this process indispensable. The expected time for users to complete the evaluation was reduced in this experiment (fewer signatures and sessions) with respect to previous usability evaluations [21] as many users complained about it.

This evaluation set-up is in accordance with the conclusions obtained in previous works and the necessity to accomplish users' requirements in order to develop usable systems.

The experiment was divided into 2 sessions one week apart [22]. Training, enrolment and Verification 1 (V1) were done during session 1 (with pauses between them). Verification 2 (V2) and Stress-Influence Tests (SIT) were done in session 2 (also including pauses between them). Users could pause at any time to rest, except during the stress-influence testing.

The evaluation crew was composed of 56 users (37 men and 19 women) chosen without any special requirement. The only condition for taking part in the evaluation was age (over 16years old as it was demonstrated that the handwritten signature is not stable in children [23]). There were 54 right-handed and 2 left-handed. Most of them (41) were between 18 and 35years old, 8 were between 35 and 50, and 7 were older than 50. Regarding the level of studies, only one user did not have a minimum qualification. The rest of them had primary education studies (2), secondary (2), high school (13) and university degrees (38). Several (29) did not have previous experience of signing on mobile devices. Almost all the users completed the evaluation successfully (54).

Currently one of the leading representatives in the smartphones market, the Samsung Galaxy Note,
                           1
                        
                        
                           1
                           
                              http://www.telegraph.co.uk/technology/picture-galleries/9818080/The-20-bestselling-mobile-phones-of-all-time.html.
                         was chosen for this experiment. Its capacitive screen has 5.3-inches and 1280×800 pixels WXGA at 285ppi (HD Super AMOLED). Its processor is a Dual Core at 1.4GHz and it has 1GB RAM. The Android version at the time of the evaluation was 4.1 (Ice Cream Sandwich). One of the reasons for using this smartphone is its proprietary stylus used in the signing evaluation. It also covers the initial requirements fixed by previous works: capacitive screen, stylus and considered comfortable by users.

None of the participants received any previous information from the operator before starting the experiment. Nevertheless, the application offered guidance during the process. At the beginning, a video explaining the whole process was shown. In addition, reminders (e.g. text messages) were shown at all the stages. The evaluation was supposed to be completed without an operator, so users were encouraged not to ask for help unless they could not continue with the evaluation.

Before starting the enrolment, a training process was to be completed by users. The application required at least one accepted signature to move forward to the next stage (interface details are explained in 3.5), although the user could stay in training as long as desired. During training, all deleted and user-accepted signatures were accounted for to measure the training that each of the users needed.

A dynamic handwritten signature recognition algorithm based on DTW [24] was applied in this evaluation. The input signals used were X and Y time series coordinates from two samples and the returned value was a similarity score between those two samples. That similarity score was normalized from 0 (totally different from the original) to 5 (the same signature), and this score was compared with a pre-defined threshold so as to accept the signature only if the score was higher than that threshold. The algorithm was used in two different situations. First, it was embedded in the mobile device application for giving feedback to users in real time (only genuine comparisons); this is explained in Section 3.4.1. Then, once all signatures had been acquired from all the subjects in the test crew, the algorithm was also used to provide the error rates FRR and FAR; this is explained in Section 4.

The algorithm applied was translated into Java for Android [25] and embedded in the smartphone. It was used in the mobiles' application to calculate similarities between the signatures as feedback for users just after performing a signature and pressing the “Accept” button. The use of the algorithm differs from the enrolment in the verification.

During enrolment, the algorithm compares each signature with all of those previously acquired, starting with the second one (2nd vs 1st; 3rd vs 2nd and 1st; 4th vs 3rd, 2nd and 1st; and 5th vs 4th, 3rd, 2nd and 1st) and returns the best result. This verification is performed in order to check if the user is making a different signature or if the acquisition process has not captured a significant number of sample points. As a result, the application shows a red or green square above the signing space as feedback to users. The feedback square becomes red if the similarity score is under the threshold 3; otherwise it becomes green. If the provided signature returns a similarity score below 3, users are required to repeat the process. If after the third attempt to acquire a signature for the enrolment, the similarity score is not at least 3, the enrolment ends and the user is not allowed to complete the evaluation, increasing the rate of Failure to Enrol (FTE).

After enrolment, the verification stages occur. In these phases, the algorithm compares each signature with the five signatures obtained through the enrolment and returns the best result. In this case, if the signature is under the threshold, the error is stored and the user shall not repeat the attempt, but continues with the next one.

The application guides the user through the different menus intuitively, so the user is supposed to complete the experiment without additional help. As the application has been executed in Spain, all guidance is written in Spanish, and therefore the captured screens are in that language. These screens fit with the evaluation roadmap pictured in Fig. 2
                        . All the signatures and data gathered were locally stored in the mobile device until the end of the evaluation. Once the experiment was finished they were transferred to a PC in order to obtain the final performance and usability rates. As in a previous approach to a real application, critical concerns for a final implementation such as security have been untreated because the main target is the usability analysis.

As previously mentioned, the first session starts with a video explaining the whole process in detail. Then, users have to accept the evaluation conditions, covering National Data Protection Law requirements, as well as a participation agreement. After this, the user introduces his personal data, including name, surname, age range, profession, gender and laterality. A personal number is generated for each user, keeping personal data unlinked to signatures.

The next step is the training process where the user can practise until he feels ready to start the evaluation. The training screen has 3 buttons (accept, delete and continue) and a blank space to sign. All the deleted and accepted signatures are counted at this point. Once the user feels comfortable with the system, the enrolment starts and he is required to provide 5 signatures correctly (as mentioned above).

Next, V1 starts and 12 signatures are requested. The verification screen is similar to the enrolment one: 2 buttons (accept and delete) and a blank space for signing. A screenshot is shown in Fig. 3
                           . This is the end of session 1.

This session starts with the V2 phase, which follows the same process as the V1. Once the user completes 12 signatures again, the SIT starts, by showing a new screen with the message “the interface will change slightly, please keep signing normally like up to now”. Then, the interface starts blinking, changing between yellow and red. At the same time, an intermittent annoying sound is played loudly and a countdown from 5 to 0 starts. When the countdown reaches 0 a text message appears saying “you are so slow, please go faster”. All these signals are intended to provoke stress in the users. At this stage, no feedback about the signature similarity is provided and only when the user completes 12 signatures does the stress test finish. When all the signatures are completed, a satisfaction questionnaire is provided to the user to express his opinion about several aspects of the test and the evaluation ends.

A user can finalize each signing attempt at any time either in V1 or in V2. The signing process finishes by accepting or deleting the signature performed. Once the delete button is pressed, a pop-up window with the following options is shown:
                              
                                 –
                                 
                                    Why did you delete the signature?
                                    
                                       
                                          o
                                          
                                             I did not like it
                                          


                                             I repeated strokes
                                          


                                             I made it partially in the air
                                          


                                             I placed my wrist on the screen
                                          


                                             I made it out of the blank space
                                          

If the user presses the accept button, the algorithm works as explained in Section 3.4.1. Furthermore, if the similarity score obtained is 1 or less (in the 0–5 scale), a new pop-up window appears, asking the user whether the signature is correct or not. By pressing yes, the user continues with the process and pressing no means repeating the signature. These steps were made in order to categorise both errors and deleted signatures into the HBSI metrics.

The intention of this work was to save resources during evaluation, such as video recording and data processing, making the system automatic. Recording the whole evaluation on video is a method used in the HBSI to better understand the user–system interactions. In such a case, at the end of the evaluation, operators have to review the videos carefully to categorise interaction errors. This process takes a very long time and requires several personnel to minimize categorisation errors during video replaying. In this work, it is the user who decides whether a signature is correct or not (deleting or accepting the signature), and also the reason for deleting. Therefore, the categorisation is automatic. This decision process proposed by the HBSI was modified and it is shown in Fig. 4
                           .

The experiment involves measuring several usability parameters. This evaluation returns three kinds of outcomes: HBSI rates, stress-influence and system performance. As these outcomes are quite inter-related, this analysis is done separately first and the correlations found are then analysed in the conclusions section.

Following the diagram shown in Fig. 1, the HBSI model is divided into usability, ergonomics and signal processing. Therefore, the test done is in accordance with these three categories.

The usability analysis included in HBSI considers the three main parameters proposed by the ISO 13407:1999, satisfaction, efficiency and effectiveness:
                              
                                 –
                                 
                                    Satisfaction: Defined as the percentage of satisfied users. This parameter is measured and studied through the satisfaction questionnaires. The questions concerning satisfaction included in the experiment questionnaire are:
                                       
                                          •
                                          
                                             Would you use this system in your daily life?
                                          


                                             Do you consider the received instructions enough?
                                          


                                             Time spent. Score 0–5 (0—very annoying to 5—very satisfactory)
                                          


                                             Easiness. Score 0–5 (0—very annoying to 5—very satisfactory)
                                          


                                             Privacy. Score 0–5 (0—very annoying to 5—very satisfactory)
                                          


                                             Global opinion of the evaluation. Score 0–5 (0—very annoying to 5—very satisfactory)
                                          


                                             Intrusion (how intrusive the application is). Score 0–5 (0—very annoying to 5—very satisfactory)
                                          


                                    Efficiency: The time spent on performing tasks. When a user deletes a signature, it also decreases the efficiency as the time performing that task is increased. For that reason, in this particular case the efficiency is calculated with two parameters: the time spent signing and the rate of non-deletions:
                                       
                                          
                                             N
                                             o
                                             n
                                             ‐
                                             Deletions
                                             
                                             Rate
                                             =
                                             
                                                
                                                   1
                                                   −
                                                   
                                                      
                                                         Number
                                                         
                                                         of
                                                         
                                                         deleted
                                                         
                                                         signatures
                                                      
                                                      
                                                         Total
                                                         
                                                         number
                                                         
                                                         of
                                                         
                                                         signatures
                                                         
                                                         ×
                                                         
                                                         Users
                                                      
                                                   
                                                
                                             
                                             
                                             ×
                                             
                                             100
                                             .
                                          
                                       
                                    
                                 

This involves a change to the HBSI proposal, as the possibility of deleting an acquired sample is not considered in HBSI.


                                    Effectiveness: Defined as the task completion rate by users.
                                       
                                          
                                             Task
                                             
                                             Completion
                                             =
                                             
                                                
                                                   
                                                      Number
                                                      
                                                      of
                                                      
                                                      users
                                                      
                                                      able
                                                      
                                                      t
                                                      o
                                                      
                                                      complete
                                                      
                                                      the
                                                      
                                                      process
                                                   
                                                   
                                                      Total
                                                      
                                                      number
                                                      
                                                      of
                                                      
                                                      Users
                                                   
                                                
                                             
                                             
                                             ×
                                             100
                                             .
                                          
                                       
                                    
                                 

These three measurements are inter-related as the decrease of any of them usually involves the decrease of the rest of them (i.e. when a user deletes a signature several times the efficiency decreases and at the same time the user feels annoyance so the satisfaction decreases too).

Ergonomics in the HBSI includes cognitive and physical categories.
                              
                                 –
                                 
                                    Cognitive: Defined as the percentage of users that
                                       
                                          1.
                                          Know how to use the capture sensor. Obtained in session 1

Learn how to use the capture sensor (also known as learnability). This is obtained in session 2 once the users have acquired skills previously in session 1

Remember how to use the capture sensor. This parameter is also observed in session 2.


                                    Physical: The percentage of users that can use the capture sensor.

This measurement includes sample quality metrics and processing capability (number of segmentations and feature extraction errors). In the handwritten signature recognition state-of-the-art there is no method to measure the sample quality, so this parameter was not measured in this evaluation. However, as aforementioned, a similarity metric between signatures was given to users as feedback by the mobile application. There are only a few works using similarity metrics as a measure of quality in the state-of-the-art [26].

The HBSI metrics are divided into two types depending on whether the presentation is successful or not [3]. As already mentioned in the previous section, the application allows the user to delete signatures, and also asks for a reason for doing so. This, together with the newly defined flow chart, limits the possibilities of HBSI metrics to the following two:
                              
                                 –
                                 
                                    FI (False Interaction): “Incorrect presentation is detected by the system and classified as correct”.
                                 


                                    CI (Concealed Interaction): “Incorrect presentation is detected by the system, but not classified as correct”.
                                 

In the handwritten signature recognition state-of-the-art there is no method to measure the sample quality to determine its correctness, so new definitions for FI and CI are needed. It is important to note that the user is aware of whether the signature meets the similarity requirements or not through the feedback provided by the application. Therefore we propose the following definitions:
                              
                                 –
                                 
                                    FI (False Interaction): “Presentation with a similarity score below the defined threshold that is classified by the user as correct (accepted)”.
                                 


                                    CI (Concealed Interaction): “Presentation with a similarity score below the defined threshold that is classified by the user as incorrect (deleted)”.
                                 

This kind of measurement is not included in the HBSI model, as there are no measurements related to the user's mood, so it is necessary to include them in the evaluation.

It is important to remember that during the acquisition of signatures under stressful conditions, no feedback on the similarity of the signatures is provided. Then, these signatures are used to calculate performance, being compared with the ones obtained in V1 and V2, extracting from these the new metrics for the influence of stress on the biometric behaviour.

Most of the main outcomes of the evaluation are extracted from the performance results, which are the following:

–Error rates in V1, obtaining the FRR when comparing each of the 12 genuine samples with the corresponding template, and doing this with each of the users enrolled in the system. The FAR is obtained by comparing the template from each user with the whole set of 12 signatures of the other 53 users, which are considered here as impostor signatures.
                           
                              –
                              Error rates in V2. Learnability information is obtained by comparing these rates with the ones obtained during the V1. The FAR and FRR are calculated in the same way as in V1.

Error rates under stress conditions. Information about the influence of stress is obtained by comparing these rates with the two previous rates. The FAR and FRR are calculated in the same way as in V1 and V2.

Error rates of the whole system in normal conditions: considering the signatures of the V1 and the V2 jointly. The FRR and the FAR are obtained in the same way as the V1, V2 and SIT, but considering 24 signatures instead of 12.

Error rates considering the signatures from the V1, V2 and SIT jointly. This reports a result of a possible real environment where the stress factor is present sometimes. The FRR and the FAR are obtained in the same way as the V1, V2 and SIT but using 36 signatures instead of 12. The complete set of error rates is summarized in Table 2.

@&#RESULTS AND DISCUSSION@&#

In this section all the results are shown and discussed. The outcomes are presented here split into HBSI results and performance results.

One of the strong points of this research has been to design a usability evaluation where users could complete the whole process with the minimum external help. Then, the aim at the development phase was to include several items of information in the application, including video and text guides. Most of the users considered the information received enough, but two of them (over 60years old) could not complete the experiment, indicating that either usability or accessibility for elders may currently be one of the major weaknesses (i.e. text messages are not big enough). These two users also found complications when introducing their personal data due to their lack of skills in the new technology. In addition, six users asked for help during the process, having doubts related also to the smartphone accessibility (keyboard and writing) rather than to the main application objective.

After the evaluation of the different responses to the questionnaires filled in by the participants, the overall satisfaction can be considered successful as the quantitative parameters measured were all close to the highest possible values. The parameters, scored from 0 to 5, are shown in Fig. 5
                           . The global opinion score of the evaluation (from 0 to 5) was 4.13. The best scored feature by the users was easiness (4.38), while the worst one was intrusion (3.72). Furthermore, 87% of the users would use this kind of system daily. Half of the evaluation crew had not had experience of handwritten signatures in mobile devices and only 12.5% would not use it daily. Only one user considered that the instructions given were not clear enough.

Efficiency is measured through the time spent signing and the number of deleted signatures. As shown in Fig. 6
                           , the time employed to sign decreased from one phase to another, with the stress section being fastest. The time spent signing was not high on average (although it is user-dependent) and it was considered good by users (3.96/5) and it decreased also with repetition.

The numbers of deleted signatures are shown in Fig. 7
                           , divided into the different phases of the evaluation where users could delete their signatures, i.e. enrolment, V1 and V2. Accounting for all the signatures made in these three phases (1537) and the number of deletions (119), the non-deletions rate was 92.26%.

The results show that most of the deleted signatures were due to disliking the signature performed (74.29% of all deletions) and that this tendency did not noticeably decrease throughout each of the phases. After interviewing the participants, most of them commented that they decided to delete the signature because it was slightly different from the original one made on paper. The authors consider as a hypothesis for a future study that the number of deleted signatures will decrease once users get used to the application, the way the signature is shown in a digital device, the improvement of those devices, and its daily use.

Effectiveness was computed using the number of errors, assists and the percentage of task completion. Two fatal errors occurred, meaning that 2 users could not complete the evaluation (at the beginning of the evaluation there were 56 users and 54 finished it): one user (over 65years old) did not manage to follow the instructions and complete the process by himself and another user introduced a wrong identity at the beginning of the second session. Thus the task completion factor was 96.43%. Regarding the errors made when signing, they did not increase significantly with the reduction of time needed for signing: by the end of the evaluation users were signing faster than at the beginning but the number of errors remained constant. This means the users' training was successful and they learnt quickly to use the application. We conclude that efficiency was satisfactory as the measured factors rates did not have a negative influence in the experiment.

Regarding ergonomics, the percentage of users that knew how to use the sensor equated to the users who did not need help. This was 87.3% because there were 6 users (10.7%) who needed help to complete the experiment due to misunderstandings with the information received. The percentage of users that learned how to use the capture sensor was the same because the two people who failed the evaluation did not start the second session. This is also applicable to the percentage of users that remembered how to use the capture sensor: 87.3% of users showed good skills at completing the second session. There was no user who was unable (physically) to use the capture sensor. As a complement to the ergonomics measures, most of the users (87.50%) signed seated and only 16.07% decided not to place the smartphone on a Table. 7.14% signed while standing and 5.35% held the device themselves, which indicates that the users' preferred position was to be seated with the device resting on a table.

Regarding signal processing, there was no segmentation or feature extraction errors. The scores obtained from the users' feedback as a similarity measure between samples were high, as shown in Table 1
                           .

There were only two similarity scores under 3 (red squares) in the V1 and three in the V2. All the squares were green during the enrolment. The SIT scores (calculated but not shown to users as feedback) were worse than the V1 and V2 similarity scores as expected, but only four signatures obtained similarity scores under 3 (the similarity threshold) in the stress section.

Only two signatures had similarity scores under 1 during the evaluation (in V2) and those users did not decide to delete them, increasing the FI rate. Thus the obtained rates were CI=0% and FI=0.10%, showing that the HBSI error rates were not influential.

Once all the signatures were gathered, the algorithm was applied to process them in order to obtain performance results. In Fig. 8
                         various Detection Error Trade-off (DET) curves are shown: V1, V2, stress, AS (all including stress) and AWS (all without stress). A summary of the details including EERs is in Table 2
                        .

The best performance was reached during V1, while in V2 the EER was clearly worse. The possible explanation is that several days had passed since the first session and there were no training sessions on the V2 day. Two solutions arise at this point: to include another session to better analyse the learnability or to extend the training session.

Those tests where the stress signatures were included returned the worst results, as noted in 5.2. The results with the whole data set except for stress (AWS) were not far from the V1 in terms of EER, but according to a wider view of the whole performance (DET curves in Fig. 8; the dashed black line is the EER) there was a clear difference. The better results in V1 may be due to the several signatures that users provided just before starting that phase (training and enrolment), thus acquiring habituation in terms of the consistency of the signature and therefore reducing the intra-class variation, as can be seen in Fig. 8 (FRR decreased much faster in the case of V1).

During the SIT, users tended to sign faster than in a regular environment, as shown in Fig. 6. The resulting similarity score in the SIT was worst on average and the number of signatures under the similarity threshold was higher (Table 1). Also, the performance obtained from the signatures of the SIT was worse than the rest (EER=1.26% with SIT signatures isolated, EER=0.83% with the complete set (including SIT), and EER=0.55% considering only non-stress situations). Even though the results under stress proved to be worse, the outcomes obtained from this test show an acceptable system performance under stressed scenarios. The average of the similarity score when providing feedback was 4.44 and this fell to 4.01 when removing feedback and subjecting the users to stress conditions. Furthermore, as can be seen in Fig. 8, the DET curve of the SIT is below the V2 curve and close to the AS for several threshold values, indicating that stress in handwritten signatures is a non-major influential parameter on performance. Nevertheless, it is important to remark that the SIT was carried out a few minutes after the V2, so the users had been signing just before starting the SIT. This suggests that the order in the procedure is another influential parameter that should be measured in future experiments.

@&#CONCLUSIONS AND FUTURE WORK@&#

Several conclusions have been extracted from this experiment. As the HBSI model was the starting point to design the usability evaluation, let us proceed with the conclusions related to the HBSI first.

The whole model was applied to this evaluation including the proposed metrics. However, because this is the first empirical approach to the analysis of a behavioural modality, the metrics adopted had to be adapted to this new context. These changes are impacted by the fact that the application allowed users to interact freely with the system (i.e. deleting signatures) and by the impossibility of knowing whether a signature is correct or not. Then, it is noticeable that the HBSI flow chart to decide the result metric is highly dependent not only on the modality but also on the characteristics of the evaluation. The ergonomic results demonstrate that the application is easy to use and the procedure is easy to learn and remember, as only two users could not complete the experiment. The sample similarity metrics applied as users' feedback provided some interesting outcomes in various aspects: users proved to be comfortable with an acknowledgement of their signature and there were not too many errors. Furthermore, similarity scores on average were quite good in all the phases.

The aim of causing stress in users was achieved, as all of them were anxious to finish this phase (as they expressed at the end of the experiment). In addition, the time spent completing the signatures was reduced ostensibly and the similarity scores were worse than in other phases. Nevertheless, the error rates obtained in the SIT reveal better performance than the V2 at several operating points in the DET curves, indicating that the stress factor is not a major drawback for recognition. Regarding general performance results, these are similar to those obtained in our previous works [8] with the same mobile device (EER=0.17%). Note that, in this case, most of the users had no previous knowledge of signing on mobile devices. According to the good results obtained, both in usability and performance, authors recommend the use of handwritten signature recognition in mobile devices as a trustable and comfortable method for biometric recognition.

@&#FUTURE WORK@&#

Various future works arise in the line of this research. First, an accessibility study of mobile devices should be done in order to correct the drawbacks and misuses found in the experiment. Second, carrying out a similar evaluation in a real scenario (such as a post office or a bank) to better understand the user–sensor interaction during the biometric recognition process is desirable. At the same time, such a study will mean gathering data over a longer period and with a larger testing crew. Another possible improvement could be to analyse the usability in biometric systems when users are in different moods, such as angry, apathetic or tired. Adding signatures from skilled forgers could be a significant approach to a real situation where the system could be exposed to malicious users.

@&#REFERENCES@&#

