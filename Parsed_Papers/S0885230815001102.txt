@&#MAIN-TITLE@&#Impact of Word Error Rate on theme identification task of highly imperfect human–human conversations

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           Review of the impact of dialogue representations and classification methods.


                        
                        
                           
                           We discuss the impact of discriminative words in terms of transcription accuracy.


                        
                        
                           
                           Original study evaluating the impact of the WER in the LDA topic space.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Speech analytics

Human–human dialogue

Latent Dirichlet allocation

Topic representation

Principal component analysis

Classification performance study

@&#ABSTRACT@&#


               
               
                  A review is proposed of the impact of word representations and classification methods in the task of theme identification of telephone conversation services having highly imperfect automatic transcriptions. We firstly compare two word-based representations using the classical Term Frequency-Inverse Document Frequency with Gini purity criteria (TF-IDF-Gini) method and the latent Dirichlet allocation (LDA) approach. We then introduce a classification method that takes advantage of the LDA topic space representation, highlighted as the best word representation. To do so, two assumptions about topic representation led us to choose a Gaussian Process (GP) based method. Its performance is compared with a classical Support Vector Machine (SVM) classification method. Experiments showed that the GP approach is a better solution to deal with the multiple theme complexity of a dialogue, no matter the conditions studied (manual or automatic transcriptions) (Morchid et al., 2014). In order to better understand results obtained using different word representation methods and classification approaches, we then discuss the impact of discriminative and non-discriminative words extracted by both word representations methods in terms of transcription accuracy (Morchid et al., 2014). Finally, we propose a novel study that evaluates the impact of the Word Error Rate (WER) in the LDA topic space learning process as well as during the theme identification task. This original qualitative study points out that selecting a small subset of words having the lowest WER (instead of using all the words) allows the system to better classify automatic transcriptions with an absolute gain of 0.9 point, in comparison to the best performance achieved on this dialogue classification task (precision of 83.3%).
               
            

@&#INTRODUCTION@&#

Automatic Speech Recognition (ASR) systems frequently fail on noisy conditions and high Word Error Rates (WER) make difficult the analysis of the automatic transcriptions. Speech analytics suffer from these transcription issues that may be overcome by improving the ASR robustness and/or the tolerance of speech analytic systems to ASR errors. This paper proposes a global study to improve the robustness of speech analytics by first comparing word representations as well as classification methods and the impact of WER in topic space learning process, on the theme identification task (Bechet et al., 2012) in the application framework of the RATP call centre (Paris Public Transportation Authority).

Telephone conversation is a particular case of human–human interaction whose automatic processing encounters many difficulties, especially due to the speech recognition step required to obtain the transcription of the speech contents. First, the speaker behavior may be unexpected and the training/test mismatch may be very large. Second, speech signal may be strongly impacted by various sources of variability: environment and channel noises, acquisition devices, etc.

Themes are related to the reason why the customer called. Various classes corresponding to the main customer requests are considered (lost and founds, traffic state, timelines, etc). In addition to classical problems in such adverse conditions, the topic-identification system should face issues to classes proximity. For example, a lost and found request is related to itinerary (where was the object lost?) or timeline (when?), that could appear in most of the classes. In fact, these conversations involve a relatively small set of basic concepts related to transportation issues. Fig. 1
                      shows an example of a dialogue manually labeled by the agent as an issue related to an infraction. However, words in bold suggest that this conversation could also be related to a transportation card issue.

Agents then annotate a conversation with what they consider the major theme of the customer request: as a result, a single theme is associated for each conversation.

In the context of Information Retrieval (IR) tasks, the main feature used is the term frequency that allows to obtain a subset of discriminative
                        1
                     
                     
                        1
                        The term “discriminative” is associated to a word if it permits to discern a class from the others.
                      words for a considered class. This set of discriminative words should permit to compose a vector representation of conversation themes in the semantic space. Its application to automatic transcriptions is more difficult since transcription errors would lead to an incorrect word representation. Thereby, we assume that dialogues have to be considered in an intermediate thematic representation to fully perform this multiple themes’ complexity. For this reason, the projection of the automatically transcribed words in a more abstracted space could increase the robustness to the Automatic Speech Recognition (ASR) errors.

Thus, we propose to first explore a term frequency representation, with the TF-IDF-Gini method, and a topic space representation, with a latent Dirichlet allocation (LDA) approach (Blei et al., 2003), coupled with a classification method to automatically identify themes from highly imperfect automatic transcriptions. The other main issue is the choice of the best classification method that does not modify the dialogue topic representation.

In the second part of this paper, the classical SVM method (Yuan et al., 2012), that modifies the word representation with a kernel function, is compared with a Naive Bayesian classifier, that does not modify it. We assume that this study will highlight the fact that these two assumptions are relevant: the Gaussianity of the theme classes and the equality of the class covariances.

The task is in the context of automatic transcriptions from an Automatic Speech Recognition (ASR) system. Thus, the impact of the transcription performance has to be evaluated. This article then discusses about the impact of the Word Error Rate (WER) for discriminative and non-discriminative words chosen by both methods in terms of transcription accuracy (Morchid et al., 2014). This part leads to consider the WER during the theme identification task itself. For this reason, an original study of the impact of the WER during the learning process of the LDA topic space as well as during the theme identification task is proposed.

This paper resumes the work carried out on a classification task using highly imperfect transcriptions. First of all, results obtained comparing two word representation methods with the same classification algorithm are described in (Morchid et al., 2014). Secondly, a comparison of performance between two classification methods is performed as reported in (Morchid et al., 2014). Finally, contribution of this paper is an original qualitative study that aims at highlighting the link between word transcription quality and dialogue classification performance. We propose to evaluate the impact of selecting a small subset of words having the lowest WER, instead of using all the words, for the topic-based representation allows the system to better classify automatic transcriptions.

The paper is organized as follows. Section 2 presents the related work. The dialogue representation approaches and the classification methods are described in Sections 3 and 4. Sections 5 and 6 report experimental results and a classification performance analysis, before concluding in Section 7.

@&#RELATED WORK@&#

Customer care services (CCS) have been the subject of various studies in the context of natural language processing. First reported studies concern the classification of electronic messages sent to a call-center, such as (Busemann et al., 2000). The objective of this work was to identify the customer's problem to increase the quality and the efficiency of the response. Authors proposed to compare various machine learning techniques, experiments have shown that the Support Vector Machine classification technique was the most effective (accuracy of 56.23% on the Reuters-21578 corpus). Globally, many works have been proposed to deal with the Reuters-21578 corpus, the e-mail classification and subject detection being a part of the corpus, such as (Tam et al., 2002; Zaï ane and Antonie, 2002; Jiang et al., 2012). Last results report a classification a F1-measure of around 90% on the Reuters-21578 corpus (Tam et al., 2002).

Nonetheless, this classification task is only a particular media of the customer care services, which globally answer to customer's requests by phone calls. Identifying the main theme of a telephone conversation then appears to be more challenging, since it combines the challenges of natural language processing problematics with speech transcription difficulties, which can contain various transcription errors in such conditions. Spoken dialog classification has been studied in the last years, globally identifying difficulties when many classes exist (Cortes et al., 2003). Recently, automatic dialog act segmentation and classification in multiparty meetings have been studied (Ang et al., 2005) using automatic transcriptions, where authors highlighted the difficulty of a fully automatic system with high-level performance.

In the particular context of call services, few studies have been proposed since few corpus have been collected, as reported in (Bechet et al., 2012). Indeed, most of them are lectures or broadcast conversations, only the LUNA FP6 and CallSurf projects (Garnier-Rizet et al., 2008) being close to call-center conversations. Authors in (Clavel et al., 2013) reported a transcription accuracy (Word Error Rate) of 30.3% and mentioned the difficulty of identifying customer's reason of the call, mainly with unpredictable customers behaviors and conversational conditions (repetitions, rephrasing...).

The robustness of ASR systems may impact the classification quality. To deal with ASR errors, authors in Garnier-Rizet et al. (2008) propose for example to use a list of keywords for each class (i.e. theme to identify), which should “exclude” wrongly transcribed words which could negatively impact the classification. Nonetheless, for such difficult conditions, the transcription quality may have a limited impact on the classification accuracy: the authors in Morchid et al. (2014) reported a difference of around 4 points of accuracy when using the reference transcription for training and testing (87.4%) in comparing to the automatic transcriptions (81.6%). The impact of selected words to classify should then be more important than transcription quality, which we claim with the original qualitative study proposed in Section 6.2.

The classical Term Frequency-Inverse Document Frequency (TF-IDF) (Robertson, 2004) has been widely used for extracting discriminative words. Improvements are observed with the Gini purity criteria (Dong et al., 2011).

Other word representation approaches proposed to consider the document as a mixture of latent topics. These methods, such as Latent Semantic Analysis (LSA) (Deerwester et al., 1990; Bellegarda, 1997), Probabilistic LSA (PLSA) (Hofmann, 1999) or latent Dirichlet allocation (LDA) (Blei et al., 2003), build a higher-level representation of the document in a topic space. Documents are then considered as a bag-of-words (Salton, 1989) where the word order is not taken into account.

LDA is a generative model which considers a document, seen as a bag-of-words, as a mixture probability of latent topics. In opposition to a multinomial mixture model, LDA considers that a theme is associated to each occurrence of a word composing the document, rather than associate a topic with the complete document. Thereby, a document can change of topics from a word to another. However, the word occurrences are connected by a latent variable which controls the global respect of the distribution of the topics in the document. These latent topics are characterized by a distribution of word probabilities which are associated with them. PLSA and LDA models have been shown to generally outperform LSA on IR tasks (Hofmann, 2001). Moreover, LDA provides a direct estimate of the relevance of a topic knowing a word set.

Various classification approaches have been studied. One of the most used one is the Support Vector Machine (SVM) method. SVMs are a set of supervised learning techniques. Knowing a sample, SVMs determine a separation plan between parts of the samples called support vector. Then, a separating hyperplane that maximizes the margin between the support vectors and the hyperplane separator (Vapnik, 1963) is calculated. SVMs were used for the first time by (Boser et al., 1992) both in regression (Müller et al., 1997) and in classification (Joachims, 1999) tasks.

A LDA-based approach combined with a SVM classification process has recently been studied in various domains, such as biology (hua Yeh and hsing Chen, 2010), text classification (Zrigui et al., 2012), audio information retrieval (Kim et al., 2009), social event detection (Morchid et al., 2013) or image detection (Tang et al., 2009). A combined LDA-SVM approach has been explored in the context of keyword extraction in automatic transcriptions (Sheeba and Vivekanandan, 2012), and theme classification of highly imperfect automatic transcriptions (Morchid et al., 2014).

The Gaussian classifier based on a Bayes decision rule has been studied mainly in speaker identification from audio, such as (Dehak et al., 2011), where the authors use a compact version of a Gaussian Mixture Model (GMM) super-vector (named i-vector), or in (Bousquet et al., 2011), where a within covariance matrix of normalized data to represent the intersession variability is proposed. The Mahalanobis (Xing et al., 2002) metric distance is generally used to evaluate this particular task. A combined LDA-Gaussian-based Bayes approach has been applied for our particular multi-theme classification problem (Morchid et al., 2014).

The next sections describe two different unsupervised approaches (Morchid et al., 2014) to create a vector representation of words: a term frequency Okapi/BM25 vector (Robertson, 2004) with the TF-IDF-Gini method (Dong et al., 2011) and a topic space representation with the LDA approach (Blei et al., 2003).

Let's consider a corpus D of dialogues d with a word vocabulary 
                           
                              
                                 V
                              
                           
                           =
                           
                              
                                 {
                                 
                                    w
                                    m
                                 
                                 }
                              
                              
                                 m
                                 =
                                 1
                              
                              N
                           
                         of size N where d is seen as a bag-of-words (Salton, 1989). A term 
                           w
                         of V is chosen from its importance δ (see example in Fig. 2
                        ) in the theme t defined as:
                           
                              (1)
                              
                                 
                                    δ
                                    t
                                    w
                                 
                                 =
                                 
                                    tf
                                    t
                                 
                                 (
                                 w
                                 )
                                 idf
                                 (
                                 w
                                 )
                                 gini
                                 (
                                 w
                                 )
                                 .
                              
                           
                        The Gini purity criteria 
                           gini
                           (
                           w
                           )
                        , is common for all the themes with:
                           
                              (2)
                              
                                 
                                    gini
                                    t
                                 
                                 (
                                 w
                                 )
                                 =
                                 1
                                 −
                                 
                                    
                                       
                                          ∑
                                          
                                             t
                                             ∈
                                             
                                                
                                                   T
                                                
                                             
                                          
                                       
                                       P
                                       
                                          
                                             (
                                             w
                                             |
                                             t
                                             )
                                          
                                          2
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

Then the words having the highest scores Δ for all the themes T constitute a discriminative word subset VΔ
                        . Each theme t
                        ∈
                        T has its own score δ
                        
                           t
                         and its own frequency γ
                        
                           t
                        
                        =
                        P(t) which is the frequency of the dialogues d
                        ∈
                        t in the corpus D. Note that a same word 
                           w
                         can be present in different themes, but with different scores depending of its relevance in the theme:
                           
                              (3)
                              
                                 Δ
                                 (
                                 w
                                 )
                                 =
                                 P
                                 (
                                 w
                                 |
                                 t
                                 ,
                                 t
                                 ∈
                                 
                                    
                                       T
                                    
                                 
                                 )
                                 =
                                 
                                    ∑
                                    
                                       t
                                       ∈
                                       
                                          
                                             T
                                          
                                       
                                    
                                 
                                 
                                    P
                                    (
                                    w
                                    |
                                    t
                                    )
                                    P
                                    (
                                    t
                                    )
                                 
                                 =
                                 〈
                                 
                                    
                                       
                                          δ
                                          w
                                       
                                    
                                    →
                                 
                                 ,
                                 
                                    γ
                                    →
                                 
                                 
                                    〉
                                    
                                       t
                                       ∈
                                       
                                          
                                             T
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

For each dialogue d
                        ∈
                        D, a semantic feature vector 
                           
                              V
                              d
                              s
                           
                         is determined. The nth (1≤
                        n
                        ≤|VΔ
                        |) feature 
                           
                              V
                              d
                              s
                           
                           [
                           n
                           ]
                         is composed with the number of occurrences of the word 
                           
                              w
                              n
                           
                         (
                           |
                           
                              w
                              n
                           
                           |
                        ) in d and the score Δ of 
                           
                              w
                              n
                           
                         (see Eq. (3)) in the discriminative word set VΔ
                         defined as:
                           
                              (4)
                              
                                 
                                    V
                                    d
                                    s
                                 
                                 [
                                 n
                                 ]
                                 =
                                 |
                                 
                                    w
                                    n
                                 
                                 |
                                 ×
                                 Δ
                                 (
                                 
                                    w
                                    n
                                 
                                 )
                                 .
                              
                           
                        
                     

The topic-based representation is performed using a latent Dirichlet allocation (LDA) approach. The LDA parameters are estimated with the Gibbs sampling technique. This is due to the difficulty to directly and exactly estimate parameters that maximize the likelihood of the whole data collection defined as:
                           
                              (5)
                              
                                 P
                                 (
                                 W
                                 |
                                 
                                    α
                                    →
                                 
                                 ,
                                 
                                    β
                                    →
                                 
                                 )
                                 =
                                 
                                    ∏
                                    
                                       m
                                       =
                                       1
                                    
                                    M
                                 
                                 P
                                 (
                                 
                                    
                                       w
                                       →
                                    
                                    m
                                 
                                 |
                                 
                                    α
                                    →
                                 
                                 ,
                                 
                                    β
                                    →
                                 
                                 )
                                 
                              
                           
                        for the whole data collection 
                           W
                           =
                           
                              
                                 {
                                 
                                    
                                       w
                                       →
                                    
                                    m
                                 
                                 }
                              
                              
                                 m
                                 =
                                 1
                              
                              M
                           
                         knowing the Dirichlet parameters 
                           
                              α
                              →
                           
                         and 
                           
                              β
                              →
                           
                        . Several techniques to estimate the LDA parameters exist, such as Variational Methods (Blei et al., 2003), Expectation-propagation (Minka and Lafferty, 2002) or Gibbs sampling (Griffiths and Steyvers, 2004). Gibbs sampling is a special case of Markov-chain Monte Carlo (MCMC) (Geman and Geman, 1984) and gives a simple algorithm to approximate inference in high-dimensional models such as LDA (Heinrich, 2005). The first use of Gibbs sampling for estimating LDA is reported in Griffiths and Steyvers (2004) and a more comprehensive description of this method is available in the technical report (Heinrich, 2005).

This method is used both to estimate parameters of LDA and to infer an unseen dialogue with the topic space. Gibbs Sampling makes it possible to estimate the LDA parameters in order to represent a new document d of size N
                        
                           d
                        
                        
                           2
                        
                        
                           2
                           
                              N
                              
                                 d
                               is the number of words contained into the document d.
                         with a topic space of size q, and to obtain a feature vector 
                           
                              θ
                              d
                              z
                           
                         of the topic representation of d. The kth feature 
                           
                              θ
                              d
                              
                                 
                                    z
                                    k
                                 
                              
                           
                           =
                           P
                           (
                           
                              z
                              k
                           
                           |
                           d
                           )
                         (where 1≤
                        k
                        ≤
                        q) is the probability of topic z
                        
                           k
                         to be generated by the unseen document d in the topic space of size q (see Figs. 3 and 4
                        
                        ) and 
                           
                              ϕ
                              
                                 
                                    z
                                    k
                                 
                              
                              w
                           
                           =
                           P
                           (
                           w
                           |
                           
                              z
                              k
                           
                           )
                         is the vector representation of a word 
                           w
                         (
                           w
                           ∈
                           
                              N
                              d
                           
                           )
                        :
                           
                              (6)
                              
                                 
                                    θ
                                    d
                                    z
                                 
                                 [
                                 k
                                 ]
                                 =
                                 P
                                 (
                                 
                                    z
                                    k
                                 
                                 |
                                 d
                                 )
                                 .
                              
                           
                        
                     

This section presents the proposed theme classification approaches (Morchid et al., 2014) that use the extracted vectors 
                        
                           V
                           d
                           z
                        
                      to learn a classifier (SVM or Gaussian-based approaches).

This probabilistic approach ignores the process by which vectors were extracted. Instead, they pretend they were generated by a prescribed generative model. Once a topic vector is obtained from a dialogue, the LDA mechanism is ignored and is considered as an observation from a probabilistic generative model. The two most simple assumptions are those of the homoscedastic Gaussian-based Bayes classifier (Petridis and Perantonis, 2004): (i) the Gaussianity of the theme classes and (ii) the equality of the class covariances.

The Gaussian classifier is based on the Bayes decision rule and is combined with a scoring metric to assign the most likely theme 
                           
                              t
                              ˆ
                           
                         to a dialogue d. Given a training dataset D of dialogues, let W denote the within dialogue covariance matrix defined by:
                           
                              (7)
                              
                                 
                                    
                                       W
                                    
                                 
                                 =
                                 
                                    ∑
                                    
                                       k
                                       =
                                       1
                                    
                                    K
                                 
                                 
                                    
                                       
                                          n
                                          t
                                       
                                    
                                    n
                                 
                                 
                                    
                                       
                                          
                                             W
                                             k
                                          
                                       
                                    
                                 
                                 =
                                 
                                    1
                                    n
                                 
                                 
                                    ∑
                                    
                                       k
                                       =
                                       1
                                    
                                    K
                                 
                                 
                                    ∑
                                    
                                       i
                                       =
                                       0
                                    
                                    
                                       
                                          n
                                          t
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             x
                                             k
                                             i
                                          
                                          −
                                          
                                             
                                                
                                                   x
                                                   k
                                                
                                             
                                             ¯
                                          
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   x
                                                   k
                                                   i
                                                
                                                −
                                                
                                                   
                                                      
                                                         x
                                                         k
                                                      
                                                   
                                                   ¯
                                                
                                             
                                          
                                       
                                    
                                    t
                                 
                                 
                                 ,
                              
                           
                        where K is the number of themes, Wk
                         is the covariance matrix of the kth theme C
                        
                           k
                        , n
                        
                           t
                         is the number of dialogues annotated with for the theme t
                        
                           k
                        , n is the total number of dialogues in the training set, 
                           
                              x
                              k
                              i
                           
                         is the vector of features for the ith dialogue annotated with the kth theme and 
                           
                              
                                 
                                    x
                                    k
                                 
                              
                              ¯
                           
                         is the centroid of all vectors 
                           
                              x
                              k
                              i
                           
                         describing the features of dialogues annotated with the kth theme. Each dialogue does not contribute to the covariance in an equivalent way. For this reason, the term 
                           
                              
                                 
                                    n
                                    t
                                 
                              
                              n
                           
                         is introduced in Eq. (7).

If homoscedasticity (equality of the class covariances) and Gaussian conditional density models are assumed, a new observation x from the test dataset can be assigned to the most likely theme k
                        Bayes using the Gaussian classifier based on the Bayes decision rule:
                           
                              (8)
                              
                                 
                                    k
                                    Bayes
                                 
                                 =
                                 arg
                                 
                                    max
                                    k
                                 
                                    
                                    
                                 N
                                 (
                                 x
                                 |
                                 
                                    
                                       
                                          x
                                          k
                                       
                                    
                                    ¯
                                 
                                 ,
                                 
                                    
                                       W
                                    
                                 
                                 )
                                 =
                                 arg
                                 
                                    max
                                    k
                                 
                                 
                                    
                                       
                                          −
                                          
                                             1
                                             2
                                          
                                          
                                             
                                                
                                                   
                                                      x
                                                      −
                                                      
                                                         
                                                            
                                                               x
                                                               k
                                                            
                                                         
                                                         ¯
                                                      
                                                   
                                                
                                             
                                             t
                                          
                                          
                                             
                                                
                                                   W
                                                
                                             
                                             
                                                −
                                                1
                                             
                                          
                                          
                                             
                                                
                                                   x
                                                   −
                                                   
                                                      
                                                         
                                                            x
                                                            k
                                                         
                                                      
                                                      ¯
                                                   
                                                
                                             
                                          
                                          +
                                          
                                             a
                                             k
                                          
                                       
                                    
                                 
                              
                           
                        with
                           
                              
                                 
                                    a
                                    k
                                 
                                 =
                                 log
                                 (
                                 P
                                 (
                                 
                                    C
                                    k
                                 
                                 )
                                 )
                              
                           
                        where x is the feature vector of a document d, W is the within theme covariance matrix defined in Eq. (7), 
                           N
                         denotes the normal distribution and a
                        
                           k
                         is the log prior probability of the theme membership (
                           
                              a
                              k
                           
                           =
                           log
                           
                              
                                 
                                    P
                                    (
                                    
                                       C
                                       k
                                    
                                    )
                                 
                              
                           
                        ). It is worth noting that, with these assumptions, the Bayesian approach is similar to the Fisher's geometric approach: x is assigned to the nearest centroid's class, according to the Mahalanobis (Xing et al., 2002) metric of W
                        −1:
                           
                              (9)
                              
                                 
                                    
                                       
                                          t
                                          ˆ
                                       
                                    
                                    Bayes
                                 
                                 =
                                 arg
                                 
                                    max
                                    k
                                 
                                 
                                    
                                       
                                          −
                                          
                                             1
                                             2
                                          
                                          |
                                          |
                                          x
                                          −
                                          
                                             
                                                
                                                   x
                                                   k
                                                
                                             
                                             ¯
                                          
                                          |
                                          
                                             |
                                             
                                                
                                                   
                                                      
                                                         W
                                                      
                                                   
                                                   
                                                      −
                                                      1
                                                   
                                                
                                             
                                             2
                                          
                                          +
                                          
                                             a
                                             k
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

This classifier modifies the representation of dialogues, which are mapped into a space of higher dimension. This goes against the assumptions previously defined in Section 4.1.

More formally, a support vector machine (SVM) constructs a hyperplane or set of hyperplanes x in a high (infinite) dimensional space, which can be used for classification. Intuitively, a good separation is achieved by the hyperplane that has the largest distance 
                           
                              2
                              
                                 |
                                 |
                                 
                                    
                                       w
                                    
                                 
                                 |
                                 |
                              
                           
                         to the nearest training data point w of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier (see Fig. 5
                        ).

As the classification of dialogues requires a multi-class classifier, the SVM one-against-one method is chosen with a linear kernel. This method gives a better accuracy than the one-against-rest (Yuan et al., 2012). In this multi-theme problem, T denotes the number of themes and t
                        
                           i
                        , i
                        =1, …, T denotes the T themes. A binary classifier is used with a linear kernel for every pair of distinct theme. As a result, binary classifiers T(T
                        −1)/2 are constructed all together. The binary classifier C
                        
                           i,j
                         is trained from example data where t
                        
                           i
                         is a positive class (wx
                        −
                        b
                        =1) and t
                        
                           j
                         a negative (wx
                        −
                        b
                        =−1) one (i
                        ≠
                        j).

For a vector representation of an unseen dialogue d, if C
                        
                           i,j
                         means that d is in the theme t
                        
                           i
                        , then the vote for the class t
                        
                           i
                         is added by one. Otherwise, the vote for the theme t
                        
                           j
                         is increased by one. After the vote of all classifiers, the dialogue d is assigned to the theme having the highest number of votes.

Experiments are performed using The DECODA project corpus (Bechet et al., 2012). This corpus is composed of 1514 telephone conversations split into a train (740 dialogues) and a test (327 dialogues) set, and manually annotated with 8 conversation themes: problems of itinerary, lost and found, time schedules, transportation cards, state of the traffic, fares, infractions and special offers.

The corpus is a set of human–human telephone conversations from the customer care service (CCS) of the RATP Paris transportation system. This corpus comes from the DECODA project (Bechet et al., 2012) and is used to perform experiments on the conversation theme identification. It is composed of 1242 telephone conversations, corresponding to about 74h of signal, split as described in Table 1
                        . Conversations have been manually transcribed and labeled with one theme label corresponding to the principal concern mentioned by the customer. The semantic annotation consists in 8 conversation themes: problems of itinerary, lost and found, time schedules, transportation cards, state of the traffic, fares, infractions and special offers.

The number of turns in a conversation and the number of words in a turn are highly variable. The majority of the conversations have more than 10 turns. The turns of the customer tend to be longer (>20 words) than those of the agent and are more likely to contain out-of-vocabulary words that are often irrelevant for the task. Additionally to the classical problems in such adverse conditions, the topic identification system has also to face issues due to class proximity. Indeed, a conversation may contain more than one semantically related theme, but not all of them are relevant for the application task. For example, a customer may inquiry about an object lost on a transportation that was late. In such a case, the loss is a much more relevant theme than the traffic state. In this situation, an agent annotates a conversation with what he considers the major theme of the customer request. This leads to annotate a theme for each conversation, the task being to find out this main theme.

As described in Bechet et al. (2012), each dialog has been manually tagged according to the RATP ontology. The repartition of the 10 top call-types is given in Table 1. As we can see, 4 call-types represent 67% of the dialogs. The top call-type is “state of the traffic” which contains requests about delays, roadworks and strikes in the transportation network. This is an interesting category as a large variety of customers behaviors occur, like for example stress and angriness sentiments from people being late because of a problem on the transportation network.

The LIA-Speeral ASR system (Linarès et al., 2007) with 230,000 Gaussians in the triphone acoustic models has been used for the experiments. Model parameters were estimated with maximum a-posteriori probability (MAP) adaptation from 150h of speech in telephone condition. The vocabulary contains 5782 words. A 3-gram language model (LM) was obtained by adapting with the transcriptions of the train set a basic LM. A “stop list” of 126 words
                           3
                        
                        
                           3
                           
                              http://code.google.com/p/stop-words/.
                         was used to remove unnecessary words. The ASR system performance is reported in Morchid et al. (2014), which results in a WER of 33.8% on the train, 45.2% on the development, and 49.5% on the test. These high error rates are mainly due to speech disfluencies and to adverse acoustic environments for some dialogues when, for example, users are calling from train stations, or noisy and crowded streets with mobile phones.

No other transcription performance has been reported so far using another ASR system on this particular corpus, but the impact would have been minor in our experiments. Indeed, previous evaluation campaigns highlighted that other state-of-the-art ASR systems can achieve better word transcription accuracy, without seeing any impact on speech analytics task performance, as claimed, for example, by equivalent results obtained on the person name retrieval task of the REPERE evaluation campaign with different ASR systems (including LIA-Speeral) (Galibert and Kahn, 2013). Moreover, the proposed work seeks to evaluate the impact of words correctly and wrongly transcribed on a classification task, the ASR transcription performance being not the purpose of this work. But the manual transcriptions are also used to show the maximum classification performance that could be achieved (i.e. best condition if no transcription error has been made by an ASR system). Also, a close WER has been reported on a similar corpus type (conversations from call-center) (Garnier-Rizet et al., 2008).

Theme identification task in the DECODA project consists in associating the most likely theme to a dialogue between an agent and a customer. To evaluate the effectiveness of the proposed method, the authors in Morchid et al. (2013, 2014, 2014), Bouallegue et al. (2014, 2014) used only the accuracy defined as:
                           
                              (10)
                              
                                 
                                    r
                                    k
                                 
                                 =
                                 
                                    
                                       
                                          TP
                                          k
                                       
                                    
                                    
                                       
                                          TP
                                          k
                                       
                                       +
                                       
                                          FN
                                          k
                                       
                                    
                                 
                                 .
                              
                           
                        and then,
                           
                              (11)
                              
                                 accuracy
                                 (
                                 d
                                 )
                                 =
                                 
                                    1
                                    K
                                 
                                 
                                    ∑
                                    
                                       k
                                       =
                                       1
                                    
                                    K
                                 
                                 
                                    r
                                    k
                                 
                                 =
                                 
                                    1
                                    K
                                 
                                 
                                    ∑
                                    
                                       k
                                       =
                                       1
                                    
                                    K
                                 
                                 
                                    
                                       
                                          TP
                                          k
                                       
                                    
                                    
                                       
                                          TP
                                          k
                                       
                                       +
                                       
                                          FN
                                          k
                                       
                                    
                                 
                              
                           
                        
                     


                        TP
                        
                           k
                         is the number of documents correctly classified as class k (i.e. true positives); FN
                        
                           k
                         represents the number of documents that belong to class k but which are not classified to this class (i.e. false negatives).

One can find more about evaluation metrics including macro- and micro-metrics in Van Asch (2013).

The train set is used to compose a subset of discriminative words to elaborate a semantic space for each conversation of the test corpus with the basic TF-IDF-Gini method. In the experiments, the number of discriminative words has been varied from 800 to the total number of words contained in the train corpus (7920 words). The test corpus contains 3806 words (70.8% occur in the train corpus).

A set of 19 topic spaces with a different topic number ({5, …, 300}) is also elaborated on the train corpus by using a LDA model made with the LDA Mallet Java implementation (McCallum, 2002).

Then, for both configurations (semantic or topic vectors), two unsupervised classification methods are applied. A SVM classifier is learnt with the LIBSVM library (Chang and Lin, 2011) using the one-against-one method with a linear kernel. SVM parameters are optimized by cross validation on the train corpus. This method gives a better accuracy than the one-against-rest (Yuan et al., 2012). Finally, the SVM classifier is compared with a Gaussian classifier trained based on the Bayes decision rule.

For sake of comparison, experiments are conducted using the manual transcriptions only (TRS) and the automatic transcriptions only (ASR). The conditions indicated by the abbreviations between parentheses are considered for the development (Dev) and the test (Test) sets.

Only homogenous conditions (TRS or ASR for both training and validations sets) are considered in this study. Authors in Morchid et al. (2014) notice that results collapse dramatically when heterogenous conditions are employed (TRS or TRS+ASR for training set and ASR for validation set).

@&#EXPERIMENTS AND RESULTS@&#

In this section, results of two different word representations and two different classification methods are shown (see Section 6.1). Finally, we propose to discuss the impact of discriminative and non-discriminative words extracted by both word representation methods in terms of transcription accuracy in Section 6.2.


                        Figs. 6 and 7
                        
                         present the theme classification accuracies obtained by the TF-IDF-Gini and the LDA approaches on the development and test corpus for all transcription configurations (TRS/ASR) when varying the word extraction conditions (number of discriminative words and number of topics). We can note that results obtained on development and test data are consistent: curves globally follow the same trend. Results presented in Table 2
                         summarize the theme identification accuracies obtained on the DECODA dataset using the TF-IDF-Gini and the LDA approaches. The columns 3 and 4 present best accuracies obtained with both development and test sets independently, while the last column presents results obtained when the best configuration found with the development set is applied to the test set (real configuration since, in a real case of dialogue categorization, the test label is unknown). As reported in Table 2, we can see that the LDA-based method outperforms the best results obtained by the TF-IDF-Gini approach.

As expected, the best classification results are obtained by the TRS configuration with a gain of 6.8 points with the LDA method in comparison to the TF-IDF-Gini approach using real conditions (ı.e. optimized using the development corpus). A gain of 7.9 points is noted with the LDA method compared to the TF-IDF-Gini approach on the automatic transcriptions (ASR).

We can finally note that the LDA approach performance has a tendency to fluctuate when varying the number of topics (see Fig. 7). This could be explained by the high Word Error Rate (WER) of the targeted corpus: indeed, the words chosen as discriminative in particular topic number conditions could be wrongly transcribed in a high proportion. This assumption can be supported by analyzing the 90 topics condition (see Fig. 7). An important performance drop is observed for the ASR training conditions while a smaller performance lost is seen when using the reference transcriptions (TRS).


                        Figs. 7 and 8
                         show the theme classification accuracies obtained by the SVM and the Gaussian approaches on the test corpus for all transcription configurations (TRS/ASR) using the LDA topic-based representation (best dialogue word content representation). We can see that the Gaussian method outperforms the results obtained by the SVM approach no matter the condition studied (see Table 2). As already seen when comparing word representations, the TRS configuration achieves the best results with a gain of 0.5 point using the Gaussian classifier method on real conditions. If focusing on the ASR experience, the Gaussian method obtains a gain of 1.9 points in comparison to the SVM approach. We can finally notice that above 100 topics, the accuracy of the Gaussian classifier decreases. It could be explained by a lack of training data to correctly estimate a topic space of such size.

While performance with the TF-IDF-Gini approach is clearly better on manual transcriptions, as shown in Table 2, it is almost identical on manual and on automatic transcriptions with the LDA method (respectively 86.5% and 81.4% of classification accuracy). This section studies the impact of Word Error Rate (WER) for the most representative words of both word-frequency and topic-based representations.

The Word Error Rate (WER) is the most common metric used to evaluate the performance of a speech recognition system. The WER is a derivation of the Levenshtein distance (Kessler, 1995) and allows us to compare different systems as well as for evaluating improvements within one system. The WER is computed as:
                              
                                 (12)
                                 
                                    WER
                                    =
                                    
                                       
                                          S
                                          +
                                          D
                                          +
                                          I
                                       
                                       
                                          S
                                          +
                                          D
                                          +
                                          C
                                       
                                    
                                    
                                 
                              
                           where S, D, I, and C are respectively the number of substitutions, deletions, insertions, and corrects.

Since the LDA-based approach outperforms results obtained with the TF-IDF-Gini method on automatic transcriptions, we assume that LDA can better manage the transcription errors by choosing discriminative words depending on their transcription accuracy (Morchid et al., 2014). In other words, the topic-based approach should better represent the dialogs content by selecting the words having the lower WER (i.e. better transcribed) while the TF-IDF-Gini approach will select all the words including the ones wrongly transcribed, that would have a negative impact on the classification accuracy.

In a first analysis, we propose to highlight the link between the transcription performance and the quality of the most discriminative words chosen by each method (LDA and TF-IDF-Gini). Each word is associated with a score computed by each method. For the TF-IDF-Gini method, the score 
                              Δ
                              (
                              w
                              )
                            associated to each word 
                              w
                            will allow to rank the words according to their importance (see Section 3.1). The score 
                              s
                              (
                              w
                              )
                            used to find the most relevant words for the LDA approach is meanwhile computed with:
                              
                                 
                                    s
                                    (
                                    w
                                    )
                                    =
                                    P
                                    (
                                    w
                                    |
                                    m
                                    )
                                    =
                                    
                                       ∫
                                       z
                                    
                                    P
                                    (
                                    w
                                    |
                                    z
                                    )
                                    P
                                    (
                                    z
                                    |
                                    m
                                    )
                                    
                                    dz
                                    =
                                    
                                       ∑
                                       
                                          z
                                          ∈
                                          m
                                       
                                    
                                    
                                       P
                                       (
                                       w
                                       |
                                       z
                                       )
                                       P
                                       (
                                       z
                                       |
                                       m
                                       )
                                    
                                    =
                                    
                                       ∑
                                       
                                          z
                                          ∈
                                          m
                                       
                                    
                                    
                                       
                                          V
                                          z
                                          w
                                       
                                       ×
                                       
                                          V
                                          m
                                          z
                                       
                                    
                                    =
                                    〈
                                    
                                       
                                          
                                             V
                                             w
                                          
                                       
                                       →
                                    
                                    ,
                                    
                                       
                                          
                                             V
                                             m
                                          
                                       
                                       →
                                    
                                    〉
                                 
                              
                           where 
                              
                                 
                                    
                                       V
                                       w
                                    
                                 
                                 →
                              
                            is the vector representation of a word 
                              w
                            in all topics z of the topic space m, 
                              
                                 
                                    
                                       V
                                       m
                                    
                                 
                                 →
                              
                            is the vector representation of all the topics z in m and 〈·, ·〉 is the inner product. The WER is then classically computed on the n most discriminative words (weight of 1 for each word).


                           Figs. 9 and 10
                           
                            compare the Word Error Rates (WER) of the n most discriminative words using TF-IDF-Gini and LDA approaches on all the configurations (TRS/ASR) on the test set. The number of discriminative words (n) has been varied from 50 (i.e. the 50 words having the higher scores) to the total number of words (3806 words). We can firstly see that the WER obtained with the LDA approach is slightly lower than the one obtained with the TF-IDF-Gini method, no matter the number of discriminative words considered. This means that a better transcription accuracy is associated to the discriminative words extracted with the LDA approach in comparison to the one obtained with the TF-IDF-Gini method, which could explain the higher classification performance reached by the LDA-based configuration (see Table 2). We can finally conclude that, for both methods, that similar conditions allow to select words having the lower WER: indeed, using manual transcriptions for train and automatic transcriptions for test (TRS → ASR) make the WER of selected words higher than those obtained with automatic conditions (ASR → ASR).

The curves presented in Fig. 10 show that the most relevant words for a given LDA topic space are better transcribed by the ASR system (i.e. low WER). From this first analysis (Morchid et al., 2014), we propose an original study to verify if the selection of discriminative words may impact classification performance. To do so, different topic spaces will be trained with the n most discriminative words (i.e. having the lowest WER) and see if this topic-based representation allows the system to achieve better theme identification accuracy.

Thus, the next section proposes an original study that seeks to highlight the fact that selecting words having a low WER impacts the quality of the topic spaces and therefore, the theme identification task.

The previous section points out that the most relevant words contained into a topic space are the ones that are better transcribed (i.e. lower WER). This section proposes an original study to evaluate the classification accuracy impact when training topic-based models using the words having lowest WER as well as the impact of these most discriminative words on the LDA topic space quality.


                           Fig. 11
                            shows the numbers of words 
                              
                                 n
                                 w
                              
                            with a WER up to a given value (
                              w
                           ). For example, there is roughly 4000 words with a WER up to 50% which corresponds to 58% of the vocabulary of the automatic transcriptions from the training corpus.
                              4
                           
                           
                              4
                              The vocabulary size of the training corpus of automatic transcriptions is of 6928 words which differs from the vocabulary of the manual transcriptions (7920 words). This is due to the insertion and deletion errors of the ASR system.
                            We can point out that there is a significant difference (>1000 words ≃14.4%) between the vocabulary size for a WER≤50% and WER>50%. Sizes of vocabularies composed with words having at least a WER equals to 70% are similar (
                              
                                 n
                                 w
                              
                              (
                              70
                              )
                              −
                              
                                 n
                                 w
                              
                              (
                              100
                              )
                              =
                              229
                            words ≃3.3%). The next step is now to evaluate the impact of selecting well transcribed words to learn a LDA topic space.


                           Fig. 12
                            shows the mean perplexity (between 60, 80 and 100 classes in the topic space) as well as the log-likelihood 
                              L
                            of topic spaces from LDA built with words having a Word Error Rate (WER) lower than a threshold (
                              w
                           ). To evaluate these quality metrics, the test set collection D
                           test of unseen transcriptions d is used as well as the topic matrix ϕ and the hyper-parameter α for topic-distribution of documents. The LDA parameter θ is not taken into consideration as it represents the topic-distribution for the documents contained in the training corpus, and can therefore be ignored to compute the likelihood of unseen documents as:
                              
                                 (13)
                                 
                                    L
                                    (
                                    
                                       D
                                       test
                                    
                                    )
                                    =
                                    logP
                                    (
                                    D
                                    |
                                    ϕ
                                    ,
                                    α
                                    )
                                    =
                                    
                                       ∑
                                       
                                          d
                                          ∈
                                          
                                             D
                                             test
                                          
                                       
                                    
                                    logP
                                    (
                                    d
                                    |
                                    ϕ
                                    ,
                                    α
                                    )
                                    .
                                 
                              
                           
                        

Likelihood of a set of unseen documents can be used to compare models: higher likelihood is better the model is. The most used measure of quality of held-out documents d in the case of topic modeling is the perplexity defined as:
                              
                                 (14)
                                 
                                    perplexity
                                    (
                                    
                                       D
                                       test
                                    
                                    )
                                    =
                                    
                                       e
                                       
                                          −
                                          
                                             
                                                L
                                                (
                                                w
                                                )
                                             
                                             N
                                          
                                       
                                    
                                       
                                       
                                    with
                                       
                                       
                                    N
                                    =
                                    
                                       ∑
                                       
                                          d
                                          ∈
                                          
                                             D
                                             test
                                          
                                       
                                    
                                    
                                       N
                                       d
                                    
                                    
                                 
                              
                           with N the total number of tokens (or words) contained in the test set collection of automatic transcriptions. The perplexity is a decreasing function of the log-likelihood 
                              L
                              (
                              
                                 D
                                 test
                              
                              )
                            of the unseen documents d
                           ∈
                           D
                           test: the lower the perplexity the better the topic model.

We can note that the curves of Fig. 12 are quite correlated with the Fig. 11 presenting the number of words depending their WER. Indeed, Fig. 12 shows that the quality of the topic space (log-likelihood or perplexity) stagnates when the vocabulary contains words with WER up to 50% which means that the “worst” point is achieved. From 10% to 50%, the quality of the LDA topic space moves down. The best quality is observed when the WER is in the gap of 10% to 25%. This clearly shows that the LDA quality is correlated with the quality of words used to build the models.

Thus, one can select a set of words to compose the vocabulary of words with a WER up to 25%. Nonetheless, the objective is a theme identification task. This task requires enough words to compose the vocabulary in order to robustly represent each theme contained in the transcriptions. For this reason, a trade-off between a low WER and a sufficient number of words in the vocabulary should be found to achieve best classification performance, which we demonstrate in next experiments.

Curves presented in Fig. 13
                            show the theme identification accuracy with topic spaces having different numbers of classes (60, 80 and 100). The first remark is that the accuracy moves up until a WER of roughly 40%. Then, these accuracies stagnate and reach a maximum accuracy of 84.2% for the topic space containing 80 classes. In comparison to Table 2, where the best precision reached 83.3% using the best classification method (Gaussian approach) on the automatic transcriptions (ASR), the observed gain is of 0.9 point. We can also point out that a topic space with small number of classes (60) is more robust to WER variation than topic spaces with large number of classes (80 or 100). Indeed, one can see in the WER gap between 60% and 100% that the curve representing the accuracy of a topic space of 100 classes is quite unstable. It is finally difficult to clearly highlight the link between topic space quality (see Fig. 12) and classification performance (see Fig. 13). Nonetheless, this original qualitative study highlighted the fact that selecting the correctly transcribed words is essential to obtain robust models, an improvement of 0.9 point being observed by excluding the words having high WER (wrongly transcribed in more than 50% of the time).

@&#CONCLUSIONS@&#

In this paper, we first resumed an architecture to identify conversation themes using two different word representations and two different classification methods. We showed that the proposed topic-based representation using a LDA-based method outperforms the classification results obtained by the classical TF-IDF-Gini approach. The classification accuracy reaches 86.6% on manual transcriptions and 81.4% on automatic transcriptions with a respective gain of 6.8 and 7.9 points.

The second part of the work focused on choosing the best classification method. We highlighted that the intuitions about the Gaussianity of the theme classes and the equality of the class covariances discussed in this paper are effective. Thus, the topic representation using a Gaussian classifier method outperforms the classification results obtained by the classical SVM approach. The accuracy reaches 87.4% on manual transcriptions and 83.3% on highly imperfect automatic transcriptions with a first respective gain of 0.8 and 1.9 points.

We also discussed the possible link between classification performance and transcription accuracy. The proposed analysis showed that the best classification results are obtained on configurations which extract the discriminative words having a lower Word Error Rate (WER). Overall, the Section 6.2 points out that the WER is inextricably related to the topic model quality and therefore, to the theme identification accuracies. The analyse in Section 6.2.3 demonstrates that a trade-off between WER and number of words has to be found. These two remarks are underlined in Fig. 13 where a choice of a vocabulary of words with a WER≤50% is a good trade-off between enough vocabulary size to describe each theme and a set of robust (to automatic transcriptions) set of words. We therefore observe a better accuracy in the theme identification task (84.2) with a significant gain of 0.9 points (see Table 2) when a subset of words with low WER are selected to compose the vocabulary.

A perspective would be to propose a solution to estimate the classification performance depending on the transcription accuracy. In the context of evaluation metrics, it would also be interesting to find another way to estimate the accuracy of automatic transcriptions in the context of a specific task since the classical WER is not a good indicator of transcription quality in an applicative context.

@&#REFERENCES@&#

