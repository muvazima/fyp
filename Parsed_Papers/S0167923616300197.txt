@&#MAIN-TITLE@&#Man vs. machine: Investigating the effects of adversarial system use on end-user behavior in automated deception detection interviews

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We present adversarial systems as a novel/growing area of IS research.


                        
                        
                           
                           Knowledge of a deception system's operations increases countermeasure use.


                        
                        
                           
                           Presenting deceivers with relevant stimuli increases countermeasure use.


                        
                        
                           
                           Truth tellers use countermeasures when aware of the system's functionality.


                        
                        
                           
                           An extensive set of novel countermeasures is identified.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Deception

Credibility assessment

Adversarial system

Countermeasures

Mandatory technology adoption

Concealed information test (CIT)

@&#ABSTRACT@&#


               
               
                  Deception is an inevitable component of human interaction. Researchers and practitioners are developing information systems to aid in the detection of deceptive communication. Information systems are typically adopted by end users to aid in completing a goal or objective (e.g., increasing the efficiency of a business process). However, end-user interactions with deception detection systems (adversarial systems) are unique because the goals of the system and the user are orthogonal. Prior work investigating systems-based deception detection has focused on the identification of reliable deception indicators. This research extends extant work by looking at how users of deception detection systems alter their behavior in response to the presence of guilty knowledge, relevant stimuli, and system knowledge. An analysis of data collected during two laboratory experiments reveals that guilty knowledge, relevant stimuli, and system knowledge all lead to increased use of countermeasures. The implications and limitations of this research are discussed and avenues for future research are outlined.
               
            

@&#INTRODUCTION@&#

A vital consideration of information systems research is the growing use of mandatory systems. These systems have the capacity to measure user behavior without the express consent or instigation of the user. Traditional technology adoption research has been conducted from the perspective that use is voluntary and focused on a reward or positive outcome for the user. Primarily, this research has focused on systems interaction contexts in which users want to use the system to help them accomplish certain tasks, or make them more effective in their work [1–3]. Prior efforts have focused on user perceptions of system usefulness, ease of use, job relevance, image, output quality, computer self-efficacy, perception of external control, computer playfulness, enjoyment, and usability [4]. Most system interactions today are of this type—voluntary and reward-focused [5,6]. While some research has looked at the involuntary adoption of systems, the outcome was still focused on task effectiveness and the ability of the system to improve overall organizational effectiveness [7,8]. However, many of these factors are not relevant to interactions with systems in which the interaction is compulsory (e.g., a full-body scanner at an airport), and could result in a punitive outcome for the user (e.g., being detained at the airport). Systems of this nature, hereafter referred to as adversarial systems, introduce a new context of research where users are placed in situations in which they must interact with the system, have no control over the data that are collected, and could be subject to a punitive outcome (see Table 1
                      for definitions of key terms used in this paper).

Deception detection is one context in which a user and a system may be working in opposition [9]. In this context, the human–computer interaction principle of a system supporting the user—or the system and user complementing one another [13]—is violated. Traditional computer-aided deception detection often includes the use of a polygraph device coupled with accompanying sensors to aid in determining the veracity of a person's statements. A polygraph device requires the direct measurement of a person's heart rate, skin conductance, respiration, and blood pressure by a trained polygraph examiner [14]. This process is expensive, obtrusive, and not easily scalable to a large number of interactions. A growing body of information systems research addresses the development of computing devices that will permit deception detection to be automated, unobtrusive, cost effective, and potentially more accurate and scientifically valid [15–20]. A system capable of conducting automated deception detection interviews has the potential to be utilized in any number of government or organizational contexts and applications. These include employment screening and the identification of insider security threats, a key concern of information security researchers [21–25]. Despite recent progress in the development of deception detection systems, several elements of users' interactions with such systems have yet to be investigated. Specifically, three key areas related to user behavior with adversarial systems that information systems researchers have yet to address include: (1) the impact of guilty knowledge, (2) the impact of relevant stimuli being presented during the interaction and (3) the impact of increasing a user's knowledge about the system.

First, deceptive users working to avoid detection by a deception detection system would perceive the system to be adversarial. Accordingly, users would likely attempt to mitigate the system's effectiveness by altering their behavior to appear innocent. Actions taken to mitigate the effectiveness of a detection system are called countermeasures. The practice of using countermeasures to appear innocent has been witnessed and studied extensively in polygraph examinations [26]. Extant work on countermeasures has been limited to the investigation of (a) countermeasures employed against the polygraph [27], and (b) the impact of traditional polygraph countermeasures on newly developed information systems designed to detect deception [9]. Most deception indicators targeted by new deception systems are different from those targeted by the polygraph; accordingly, researchers must explore novel ways in which users will manipulate their behavior to appear truthful.

Second, all forms of deception detection interactions require the selection of questions or stimuli that will elicit deception indicators from users. Even the most valid deception interaction formats can be difficult to administer due to limitations in selecting relevant questions/stimuli to be used during the interaction [28]. Research is needed to explore how a lack of relevant stimuli during a deception detection interaction will influence countermeasure use.

Third, deception researchers developing new systems often conduct studies in which participants have no concept of the purpose of the system or any concept of its operations [15–17]. This limits ecological validity as real-world users—especially those with a vested interest in deceiving the system—would have a substantial amount of knowledge about the functionality of a real-world system when it is deployed for use. We see this currently with the polygraph, with widely available resources teaching how to “beat” a polygraph examination. Understanding how increased system knowledge will affect countermeasure use warrants further investigation.

This research investigates variations in behavior that occur when users interact with an adversarial deception detection system. These variations are manifested as countermeasures. The use of countermeasures is predicted to vary in response to the following three manipulations: (1) the presence or lack of guilty knowledge in system users, (2) the system's inclusion or omission of relevant stimuli during the interaction, and (3) the user being aware or unaware of the capabilities/functionality of the system. This research contributes to existing knowledge by demonstrating that there are substantial differences in the way users interact with adversarial deception detection systems based on a presence or lack of: guilty knowledge, relevant stimuli, and system knowledge. The remainder of the paper is organized as follows. First, we discuss relevant literature. Next, we specify hypotheses based on relevant theory. Third, we outline the methodology used for two data collections. Fourth, we provide an analysis of the data and discuss the implications of our work. Finally, we present limitations and avenues for future research.

@&#LITERATURE REVIEW@&#

We have identified countermeasures as a strategy that users can employ to mitigate the accuracy of adversarial deception detection systems. We now examine the use of adversarial deception detection systems by drawing from three key areas of literature: technology acceptance and adversarial systems, automated deception detection systems, and deception countermeasures.

One of the most widely studied theoretical models in the field of information systems is the Technology Acceptance Model (TAM) [29]. This model attempts to predict system adoption by measuring a system's perceived usefulness and perceived ease of use 
                        [6]. Evaluations of a variety of system types have used the TAM model to predict system adoption, including email, ecommerce, and executive information systems [4,30,31]. Fundamentally, TAM suggests that a user will adopt a system if it enhances his or her job performance, and that using it will be free of effort [2]. TAM has been an effective theoretical model for studying adoption of systems used in the workplace that attempt to increase productivity, effectiveness, or produce a positive outcome.

However, TAM may not be useful for understanding the adoption and use of new types of systems, which we have termed adversarial systems. For example, a user may be required to submit to a polygraph examination as part of the pre-employment screening process with a new employer [32,33]. Interacting with this system would be compulsory and not directly related to the work the prospective employee will be doing. It is also unlikely that interacting with a polygraph system will affect long-term job performance after being hired. Users have very little control over the examination process, structure, or the data that are collected [14,34].

The ways users interact with adversarial systems are fundamentally different from the ways users interact with traditional information systems. Instead of wanting to use the system to improve their own productivity, users may choose to actively work against the adversarial system. This shift in users' perceptions of such systems requires a new theoretical understanding of how users will interact with these systems. For example, Venkatesh and Bala [4] found that user experience had a moderating effect on the relationships between perceived ease of use on behavioral intention, and perceived ease of use on perceived usefulness. However, within the context of an adversarial system, there is no system adoption. The behavioral intention may be to circumvent the system, not use the system. Insufficient theoretical development has been done to understand how increasing experience may affect the way users interact with adversarial systems. In fact, due to the fundamentally different nature of adversarial systems, an entirely new theoretical framework of systems use may be warranted. The findings presented herein can be used to guide the future development of such a framework.

Deception is a persisting element of interpersonal communication. However, detecting deception is notoriously difficult for humans. Reliable deception identification rates hover around 54% [35]. Innovators have long sought information systems that can be used to augment or replace the human element in this interaction context. The polygraph is the most widely recognized and used technology for veracity assessments. Despite decades of empirical research and extensive laboratory and field testing, its validity and accuracy remain a point of uncertainty and debate [36–38]. Exacerbating the questionable utility of polygraph use is a problem of scalability. Traditional polygraph interviews require skilled criminal examiners to conduct time-consuming multiphasic interviews with specialized sensors attached to the interviewee. Government agencies, private organizations, and researchers are all seeking to develop more robust, scalable, and automated technological solutions in response to the limitations of the polygraph [15]. An optimal solution would be a system capable of conducting an automated non-intrusive interview measuring behavioral and physiological responses of the interviewee. Responses could be analyzed in real time and used to support a human decision maker. A wealth of research has already been done with the aim of creating a system capable of conducting automated deception detection interviews. The following list includes examples of topics that have already been explored in this context:
                           
                              •
                              System use for decision support [19,20]
                              

Incorporation of an automated embodied conversational agent (ECA) [15]
                              

Identification and validation of sensors used to collect/interpret verbal, nonverbal, and physiological responses [16,39–41]
                              

Identification of an optimal interviewing protocol [42,43]
                              

Design of an optimal interface and form factor [17,44]
                              

System acceptance and use by human operators [18]
                              

One of the most important aspects in developing such a system is the nature and structure of the interaction. A majority of the studies listed in the previous paragraph have used the Concealed Information Test (CIT) as the governing framework for the interaction. The CIT is a recognition-based criminal interviewing technique utilized sparsely by criminal examiners and law enforcement agencies [45]. Its limited use persists despite a wealth of scientific evidence grounded in theoretical support and extensive empirical testing pointing to its validity [46–48].

In a CIT, groups of stimuli called foils are presented to the examinee with one stimulus in each foil considered ‘crime-relevant.’ This item is referred to as the target item. The remaining stimuli in the foil serve as a baseline of behavior to which responses associated with the target item can be compared. These stimuli are named non-target items. A CIT is often comprised of several foils as the statistical likelihood of an erroneous classification diminishes as the number of foils increases. Electrodermal activity (EDA), or skin conductivity, is the dominant physiological response measured during a CIT. However, other measures can be used in conjunction with EDA [10]. The CIT interview is shorter and more adaptable than a standard polygraph examination. This makes it a good candidate for automation, especially with the use of new sensors that can remotely measure behavioral and physiological activity [28]. However, the accuracy of the CIT and other interviewing techniques can be reduced if countermeasures are successfully used by the interviewee. The following section provides an overview of various countermeasure techniques utilized to thwart the polygraph and other deception detection systems.

Psychophysiological deception detection is based on detecting a physiological response that is linked to psychological processes. For decades, research in deception detection involving the polygraph has investigated the effectiveness of countermeasures at evading detection [49,50]. Unfortunately, advances on the part of law enforcement or research have been met by efforts on the part of criminals to circumvent or thwart those advances. In the case of the polygraph, entire books and websites have been devoted to teaching people how to beat a polygraph examination [26].

There are several ways deceivers can increase their chances of passing a polygraph exam undetected. Polygraph methods center on creating an individual baseline for truthful responses that is then used to detect aberrations when the individual is lying. Countermeasures are generally used to manipulate the baseline so the baseline and deceptive responses are indistinguishable. By increasing arousal during truthful questions, liars are able to muddy their results and receive a truthful judgment [e.g., 51]. Some of the physical countermeasures studied for the polygraph are tongue biting [52] and pressing toes against the floor [50,52]. During a deception detection interaction like the CIT, countermeasures are employed during the presentation of several non-relevant items to increase physiological responses, thus reducing the reliability of the scoring system of these tests.

Mental countermeasures are employed throughout the interview, rather than just when the baseline questions are asked [51]. Some mental countermeasures function by increasing the cognitive demands on deceivers, thus distracting them from the examination and suppressing their responses [50,51]. One common mental countermeasure of this type is mental arithmetic. Simply counting backward by 7 from any large number is an effective method of passing an exam [50]. Other mental countermeasures are recalling past emotional events [49] or mentally repeating your name [53].

Of course, the effectiveness of many of these countermeasures is dependent on the type of the exam and the sensors being used to detect deception. The vast majority of countermeasures research has focused on sensors used with the polygraph, including the pneumograph and finger electrodes. As innovative sensors and novel testing strategies are employed, new types of countermeasures will emerge in an attempt to circumvent those tests. For example, the P300 is an electroencephalography-based test using electrical impulses in the brain to detect the arousal associated with deception. Despite its relatively new development, the effectiveness of countermeasures against this test has already been established [53,54]. Additionally, functional Magnetic Resonance Imaging (fMRI) is being investigated for deception detection, while research on countermeasures effectiveness proceeds in parallel [55].

Due to the nature of deception detection, it is clear that the study of mechanisms to detect deception must proceed hand-in-hand with the investigation of methods to avoid detection. An approach to deception detection involving the fusion of multiple non-contact sensors has recently shown promise [15,16]. Previous research on this type of deception detection has obtained accuracy rates comparable to the polygraph. A recent investigation of polygraph countermeasures showed limited effectiveness against a combination of sensors, supporting the multi-sensor fusion approach to detection [9]. However, the results of that research were limited to polygraph countermeasures. Because of the novel nature of this suite of sensors, we must also determine if the use of new types of countermeasures may allow deceivers to more effectively evade detection. Furthermore, understanding how certain variables (e.g., relevant stimuli and system knowledge) influence countermeasure use is of critical importance to the development and use of these systems.

Deception is a complicated process, through which one party deliberately attempts to manipulate the beliefs of another, often for personal gain. Many different theories have been proposed to cover the breadth of phenomena observed during deception. Cognitive load theory proposes, for example, that cues to deception are caused by the increased cognitive load carried by deceivers who must simultaneously recall the truth and the deception [56]. Interpersonal deception theory (IDT) proposes that deception is a complex process involving strategic use of behaviors by the deceiver in order to appear truthful [56]. These strategic behaviors are deliberately selected to be congruent with what the deceiver thinks the target of their deception would be expecting from someone truthful. In a broader sense, the strategic aspects of deceptive behavior fall into the category of impression management.

Impression management is the process through which people attempt to control others' impressions of them [57]. In the case of deception, the deceiver is attempting to create a truthful impression in his or her targets. Impression management theory breaks down the process into two components: impression motivation and impression construction [57]. Impression motivation is the reason the impression management takes place. People may have many different reasons and levels of motivation to manipulate the impressions others have of them. A job candidate has a strong desire for the interviewer to perceive him or her as a good fit for the position. A politician wants to appear trustworthy and honest when interacting with voters. These motivations cause individuals in such positions to engage in strategic behaviors of impression construction in order to engender the desired response from their audience. Impression construction consists of the verbal and nonverbal behaviors associated with creating the desired impression [58].

During a CIT, both guilty and innocent individuals are motivated to create an impression of innocence. The findings of several studies using a CIT-based interviewing format confirm that individuals deceiving during a systems-based deception detection interaction exhibit strategic behaviors in an effort to avoid detection [17,40,43]. The results of these studies indicate that there are differences between the behavior of deceivers and truth tellers. Specifically, these differences are as follows: (1) deceivers fixate on the center of the screen longer than truth tellers [43], (2) deceivers experience longer vocal response latencies relative to truth tellers [40], and (3) deceivers exhibit increased stimuli avoidance when questions relevant to their deception are present (even during repeat screenings) [17]. In order to counter natural responses to deception, individuals may engage in deliberate countermeasures. If the individual is in an unknown situation or examination, these countermeasures are termed spontaneous countermeasures [59]. Both guilty and innocent people can and do engage in spontaneous countermeasures [59]. However, the impression motivation should be much stronger in guilty individuals because they have both a more difficult task and more at stake if the test classifies them as deceptive. We propose the following hypothesis:
                        H1
                        Participants with guilty knowledge have a higher propensity to use countermeasures than those without guilty knowledge.

It is important to note that the deception literature referenced previously is based on experiments in which deceivers are presented with stimuli relevant to their deception. Relevant stimuli are presented as a means of triggering behavioral or physiological responses indicative of deceit. A hindrance to effective CIT interviewing is the necessity to identify relevant target items that can trigger deception indicators [28]. What has yet to be explored is the presence of behavioral differences between deceivers who are exposed to relevant stimuli and those who are not exposed to relevant stimuli. This issue is of critical importance for practical reasons, as persons completing real-life screening interviews could be deceiving but may not encounter relevant questions during the interaction, and thereby appear truthful.

It is the contention of this research that deceivers who are not exposed to relevant stimuli will continue to act strategically in order to appear truthful, thus differentiating themselves from truth tellers. However, the propensity for persons not exposed to relevant stimuli to act strategically will not be as persistent as persons encountering relevant stimuli. This contention is grounded in defensive response theory, often referred to as the fight-or-flight response [60], which states that a person's perception of threatening stimuli will result in a defensive behavior (i.e., a reaction to the threat) [61,62]. This sequence can be broken into three distinct components: a perceived threat, a defensive reflex, and a form of behavior modification [17]. Relevant literature investigating automated deception detection screening systems has stated that “defensive behaviors are driven by a perceived threat and therefore can be different from behavioral reactions to stimuli perceived to be non-threatening” [17,63]. In the context of using an automated system to identify deception, a sender of deceptive messages will be much more threatened by the system if the system presents stimuli that are relevant to the user's deception. This increase in perceived threat will trigger defensive responses (i.e., the use of strategic behaviors designed to mitigate the system). The following hypothesis is proposed:
                        H2
                        Participants seeing relevant stimuli have a higher propensity to use countermeasures than participants who do not see relevant stimuli.

Furthermore, experience with a system can change the way a user views and uses a system [64]. Substantial experience with an adversarial system may decrease the effectiveness of the system (i.e., reduce its ability to detect deception) as users become more confident using the system [9]. At point is the difference between experience with a system and knowledge about a system. For example, a user may have little experience with a system (i.e., never participated in a deception detection interview), yet know a lot about how a system works by reading about it online [26]. Users may also lack experience and knowledge about how a system works. However, as participants gain knowledge about the system, their behavior is likely to change, regardless of whether they have anything to hide. The Hawthorne Effect proposes that the mere act of observation may modify behavior [65,66]. In the case of a deception detection system, that behavioral modification should manifest itself by increasing behaviors (i.e., countermeasures) the system views as innocent, and decreasing those viewed as guilty [67]. Accordingly, the following hypothesis is proposed:
                        H3
                        Participants with knowledge of the system have a higher propensity to use countermeasures than participants without that knowledge.

The effect of system knowledge on countermeasure use is expected to be stronger for participants in the guilty condition. Without any guilty knowledge, participants have marginal emotional investment in the outcome of the interview, or in the functioning of the system, and thus low impression motivation. Without impression motivation, system knowledge should have a minimal effect on impression construction behaviors. With guilty knowledge, however, participants can be expected to use their knowledge of the system and of their crime to devise countermeasures to improve their chances of being deemed innocent. In this way both the impression motivation and the impression construction processes are affected. The following hypothesis is proposed:
                        H4
                        System knowledge strengthens the relationship between guilty knowledge and countermeasure use.

The complete research model for this study is presented in Fig. 1
                     .

@&#METHODOLOGY@&#

Two laboratory experiments were conducted to test the specified hypotheses. Participants from both studies were undergraduate students recruited from business courses at a large western university. No participant reported any previous experience with law enforcement, criminal investigations, or deception detection. One participant reported hearing information about the experiment prior to participating; data from that participant were discarded. The average age of participants in the first study (N=77) was 23.7years; 78.1 percent were male. The average age of participants in the second study (N=114) was 21.2years; fifty-seven percent were male. Three experimental manipulations were used to test the hypotheses: (1) the presence or lack of guilty knowledge in system users, (2) the system's inclusion or omission of relevant stimuli during the interaction, and (3) the user being aware or unaware of the capabilities/functionality of the system. Refer to Table 2
                      for a concise overview of the treatments constituting our experimental design.

Participants were randomly assigned to one of the possible conditions outlined in Table 2. Participants in the control groups were told to pack a bag with benign items and pass through a screening interview. Control group members possessed no knowledge of any criminal activity and had no reason to deceive. Guilty knowledge was manipulated by having members of the three manipulation groups smuggle a banned item through the screening interview. Participants in the manipulation groups were instructed that they were associates of a well-known criminal enterprise operating in the area. They were given instructions to pack the bag and deliver it to one of three possible criminal collaborators waiting for them in the atrium of the business school building. Participants were shown pictures of the collaborators and told to memorize their faces so that they could successfully hand off the bag to the correct individual once their task was complete. One of the items they were told to place in the bag was a simulated improvised explosive device (IED, see Fig. 2
                     ). Clothing, books, and other items were provided to help conceal the IED in the bag. The IED, the faces of the criminal collaborators, and the name of the criminal organization would serve as the guilty knowledge that manipulation group members would have to conceal to pass the screening interview (i.e., they were used as target items during the CIT).

After packing the bag, all participants continued to the screening area. Upon entering the screening area, participants were led by one of the researchers to an interviewing kiosk (see Fig. 3
                     ) and positioned in front of it. Participants were instructed to place their bag on the floor next to the kiosk during the interview. The interviewing kiosk consisted of a computer monitor and several sensors attached to a central computer. The height of the computer screen was adjusted to account for the height of each participant. To manipulate system knowledge, participants in the second study were informed about the format of the interview (a CIT) and the measurement capabilities of its sensors. Participants given system knowledge were specifically told that they would be presented with images of faces, banned items, and the names of criminal organizations. They were further told that the system would be measuring their eye movements and speech while they viewed these images. This information was conveyed to participants prior to the deception detection interview by displaying a series of seven slides on the screen (listed in Table 3
                      as ‘Automated Overview of System Functionality’).

During the CIT, several foils of image groups appeared on the screen. Each foil contained image groups of either faces, banned items, or the names of criminal organizations. Refer to Fig. 4
                      for an example of an image group of banned items displayed by the system during one of the foils. For each of the image groups presented, participants were asked the same question: “Are you familiar with any of these faces/items/criminal organizations?” Participants were required to respond verbally to each question by stating “Yes” or “No”. The presence or lack of relevant stimuli was manipulated by configuring the set of images displayed during each interaction. An interaction containing relevant stimuli consisted of images of the IED, the three criminal collaborators, and the name of the criminal organization all appearing randomly during the interaction. The set of non-relevant stimuli contained face, banned item, and criminal organization images not relevant to the mock crime.

After completing the interview, participants were informed that the bag did not need to be delivered to the criminal collaborators. Participants were then ushered into a separate room to take a post-test survey wherein they answered questions about their experience during the interview. It was during this survey that participants reported on their behavior and the use of countermeasures during the interaction. Each participant was asked if he or she used any tactics to appear truthful. If the participant answered in the affirmative, they were asked to identify the countermeasures that they employed.

There were no significant differences between the procedures in the first and second studies aside from the intended manipulations. The differences in experimental conditions for each group are summarized in Table 3. Bolded items indicate how each manipulation group differed from the control group.

@&#RESULTS@&#

The frequencies of self-reported countermeasure use associated with the three manipulations employed in this research are as follows: guilty knowledge (49.2%), no guilty knowledge (14.5%), relevant stimuli (53.2%), no relevant stimuli (25.0%), system knowledge (44.7%), and no system knowledge (24.7%) (depicted in Table 4
                     ). The percentage of participants using countermeasures in the no guilty knowledge group, who had no motivation to deceive, was surprisingly high (14.5%).

To test H1 through H3, a multiple logistic regression model using multiple predictors was fitted to the data to test the occurrence of countermeasures. The outcome variable was countermeasure use (1=yes, 0=no), and the predictors were guilty knowledge (1=yes, 0=no), system knowledge (1=yes, 0=no), and relevant stimuli (1=yes, 0=no). A multiple logistic regression test was chosen because all of the independent variables and the dependent variable are dichotomous. The minimum sample size for a multiple logistic regression is 10 cases per independent variable [68] and it does not assume normality, linearity, or homoscedasticity [69]. Results from a power analysis (α=0.05, power=0.80, odds ratio=2.6, and medium effect size) indicate that the minimum sample size is 117. This study has a sample size of 191, resulting in power=0.94.

The results shown in Table 5
                      indicate significant support for H1 (Wald's χ2
                     =3.891, p=0.049). Participants with guilty knowledge were 2.6 times more likely to use countermeasures than participants without. Support was also found for H2 (Wald's χ2
                     =5.905, p=0.015). Participants exposed to relevant stimuli were 2.9 times more likely to use countermeasures than participants who were not exposed to relevant stimuli. H3 was also supported (Wald's χ2
                     =8.737, p=0.003), as participants with knowledge of the system were 3.3 times more likely to use countermeasures than participants without. The specified logistic regression model exhibited good fit against the data (Cox and Snell R2
                     =0.170. Nagelkerke R2
                     =0.232).

To test H4, scores for guilty knowledge and system knowledge were standardized by calculating z-scores for each. These standardized scores were then multiplied together to create the moderating variable. The moderating variable was then added to the existing model. Results indicate no support for H4. Addition of the moderating variable had no substantial effect on the amount of variance explained by the model. Cox and Snell R2 changed from R2
                     =0.170 to R2
                     =0.172, and Nagelkerke R2 changed from R2
                     =0.232 to R2
                     =0.235. Results also indicate that the variable itself was not a significant predictor in the model (Wald's χ2
                     =0.530, p=0.467, β=−0.335, SE β=0.460). Thus, we find that system knowledge did not strengthen the relationship between guilty knowledge and countermeasure use. The results from hypothesis testing are summarized in Table 6
                     .

In addition to the formal hypotheses summarized in Table 6, post-hoc exploratory analysis was done on the type and variety of countermeasures reported in Study 2 in an effort to better understand how countermeasures are used by each experimental group. Countermeasures were categorized to group similar measures together; ultimately, ten categories were formed with one category labeled ‘Other’ for countermeasures used by only one participant. A listing of the category names, each with an accompanying description, is provided in Table 7
                     .

When participants learned about the functionality of the deception detection system they were told that the system would be monitoring their eye movements and speech. Five of the ten categories listed in Table 7 describe manipulations of eye behavior, namely: blurred viewing, center of screen, consistent viewing, equal viewing, and haphazard viewing. Only two of the ten categories are associated with speech, specifically: temporal response control and tone control. It is interesting to note that some participants reported using physiological pain manipulation (e.g., pinching oneself to elicit a fabricated physiological response); this type of tactic is often employed to thwart the accuracy of sensors used for polygraph interviews.

The relative use of each countermeasure was calculated for each condition; an aggregate value for all three conditions combined was also calculated. Refer to Table 8
                      for a listing of these values and a graphical representation in Fig. 5
                     . The most frequently used countermeasure in any condition was the use of equal viewing behavior employed by participants in the control group. Members of the control group largely used eye-based countermeasures but also had the highest percentage of countermeasures allocated to the ‘Other’ category. ‘Consistent Viewing’ is the highest-scoring category for both manipulation groups. The participants with guilty knowledge who did not see relevant stimuli had the most variety in the types of countermeasures that were used, while participants with guilty knowledge who did see relevant stimuli primarily employed eye-based and voice-based countermeasures.

@&#DISCUSSION@&#

This research looked at variations in behavior that occur when users interact with an adversarial deception detection system. These variations manifested as countermeasures. Countermeasures were expected to increase in response to the following three manipulations: (1) the presence of guilty knowledge in system users, (2) the system's inclusion of relevant stimuli during the interaction, and (3) the user being aware of the functionality of the system. The results of this research make important contributions to a variety of research streams. We discuss the implications of these contributions, along with limitations and avenues for future research, in the following sections.

The hypothesis test for H1 revealed that participants possessing guilty knowledge had a higher propensity to use countermeasures (2.6 times) than truth tellers. Forty-nine percent of participants with guilty knowledge used countermeasures; thus, the effect of guilt or attempted concealment on countermeasure use was substantial. However, fifteen percent of participants lacking a need to be deceptive also used countermeasures. This finding reveals a substantial pitfall associated with trying to use the presence of countermeasures as a means of identifying deceivers. Further, this finding supports the impression motivation component of deception. Even innocent participants have some motivation to appear innocent, and thus employed impression construction techniques to create the impression of truthfulness. Based on the results of this study, it can be inferred that some users will actively try to circumvent any adversarial system, even if they are innocent. Researchers and system designers must account for these attempts at circumvention when developing deception detection systems.

Formulating interactions and sets of interview questions that are constituted of information relevant to a past crime is one of the most challenging aspects of identifying deception [10,28]. Past research studies have had distinct truthful and deceptive groups to simplify the research design and clearly quantify the ability of the system to effectively identify deception [9,15,16]. This research included a treatment in which deceivers were not asked by the system about their deception. This manipulation (tested by H2) was an effort to reveal how the presence of relevant questions influences user behavior. In congruence with defensive response theory, H2 was supported as participants presented with threatening/relevant stimuli were more likely to use behavior modifications/countermeasures (2.9 times) than participants who were not presented with relevant stimuli during the interaction. Fifty-three percent of participants shown relevant items used countermeasures. Surprisingly, twenty-five percent of participants who were not shown relevant items still used countermeasures. This result demonstrates that deception detection systems are effective at eliciting countermeasure use even if stimuli relevant to the deception are not shown. Exploratory analysis of the data indicated that the types of countermeasures used by members of each experimental group was quite different. Accordingly, an analysis focusing on the types of countermeasures used could provide a means of differentiating between truthful and deceptive individuals.

Third, prior research has looked at how experience with a system changes how a user interacts with and uses that system [4,6,70]. This research demonstrates that the mere knowledge of how a deception detection system works, even with no prior experience, significantly changes how users interact with the system. Knowledge of how a deception detection system works may reduce its effectiveness. The significant test for H3 supports prior findings. Providing users with knowledge about the deception detection system led to an increase in countermeasure use by deceivers from twenty-five percent to forty-five percent. More knowledge of how a system works will lead to significantly greater attempts (3.3 times) to circumvent the system. Results from this study show that when studying deception detection systems, or other types of adversarial systems, effort must be taken to understand the impact of users' knowledge of the system on the study's results. Performing research using only participants with little or no knowledge of the system may inadvertently harm the external validity of the study. With regard to adversarial systems, it is reasonable to believe that users may seek out information about the system before using it (e.g., buy a book on how to pass a lie detector test before an assessment [26]). It is also likely that information about the system will be disseminated to the public. As such, simulating interviewee knowledge of the system in a laboratory setting will yield more ecologically valid data and result in a system that is more robust and ready for use in real-world applications.


                        H4 was not supported as there was no moderating effect of system knowledge on the relationship between guilty knowledge and countermeasure use. While both guilty knowledge and system knowledge have significant direct effects (H1 and H3 respectively), having prior knowledge about the system did not strengthen the relationship between guilty knowledge and countermeasure use. Researchers looking at repeated use of adversarial systems (i.e., biannual polygraph exams) can be confident that prior knowledge about the system will not differentially affect countermeasure use among participants with guilty knowledge.

Finally, this study serves as a reminder that experimental participants are not passive observers, or even passive participants in any interactive sense. This is illustrated by the fact that a sizeable portion of members of the control group utilized countermeasures despite having no information to conceal or need to be deceptive. Just as survey questions or observation can modify behavior [65,66,71], so too can the measurement of behavior in sensor-driven studies. Researchers must consider the implications of their measurement and understand that members in all treatments (including the control group) are influenced by the very act of being measured, thereby potentially dampening the generalizability of empirical work to the real world.

The results of this study offer several valuable insights for practice. First, we find that while participants with guilty knowledge are more likely to use countermeasures, there is still a strong percentage of innocent people who employed countermeasures. This effect is magnified by the introduction of system knowledge. For the deployment of automated deception detection systems in the field, it is important to consider how people will react to the system when they know what is being measured. It would be unrealistic to assume naïveté on the part of all persons who interact with a system, even among those who are innocent. Out of curiosity it is possible, even likely, that an interested person may investigate the technology behind a deception detection system. System designers must understand this and incorporate it into the algorithms they employ. For example, attempting to view all items on the screen consistently could be a countermeasure employed either due to deception or because an innocent person presumes that is the expected innocent behavior. The mere detection of employed countermeasures is not sufficient to establish deception.

Additionally, the exploratory analyses confirm that participants employed an extensive set of countermeasures during the interview, many of which have not been previously identified or reported in extant literature and are thus considered novel. These countermeasures varied by experimental condition. For example, twenty-eight percent of the countermeasures used by the control group consisted of equal viewing, but only five percent of the countermeasures used by the group not exposed to relevant stimuli consisted of equal viewing. Similarly, nineteen percent of the countermeasures used by the group not exposed to relevant stimuli were attempts at vocal temporal response control, but the control group reported zero attempts at vocal temporal response control. Practitioners both developing and using new forms of deception detection systems need to account for the inevitability that new types of countermeasures will constantly be developed and employed by users, regardless of how novel the systems or sensors used for data collection may be.

One of the contributions of this research is the investigation of how knowledge of a screening system affects countermeasure use. Providing participants with knowledge of a deception detection system in a laboratory setting could be perceived as a limitation of the study as system knowledge could alter user perceptions and behavior. However, it is our contention that increasing system knowledge improves realism and generalizability as the use of these systems in the real world would result in public knowledge concerning their operations. In light of this inevitability, research investigating how users interact with screening systems when they are aware of their operations should help to bridge the ecological validity gap between laboratory and real-world interactions. It is also worth noting that providing an overview of the CIT interview format/sensors prior to the administration of a traditional CIT is a standard protocol [10]. Adopting this protocol in an automated screening system context more closely adheres to CIT best practices (a practice largely ignored in prior automated screening research). It should be noted that additional knowledge and experience with the system might reduce the effectiveness of the system as users become more comfortable and confident in their ability to manipulate the system. Additional studies looking at repeated exposure to the deception detection system and the CIT are warranted.

Relatedly, this study is limited somewhat by the use of a mock crime experimental task with student participants rather than real criminals. While there are certainly differences between a student participant smuggling a banned item in a laboratory and a criminal smuggling illegal contraband through an actual law enforcement checkpoint, this limitation may not be as impactful as it initially seems. Throughout the history of the study of deception, laboratory experiments have been used as proxies for criminal interviews to understand the mechanisms behind deception and its detection [72–74]. Questions are often raised about the validity of these experiments when generalizing to real-world applications [73,75]. Several studies comparing field studies to well-designed laboratory experiments have found the results to be generalizable [75,76], and deception effects have even been shown to be greater (and thus more easily detectable) in the field than in a laboratory [75]. Thus, the results of mock crime experiments may be generalized to a broader context, though further study will be required to confirm the model with a sample from the field [73].

Future research should also examine the impact of countermeasures found in this study on the accuracy of automated deception detection systems. The exploratory part of Study 2 found that the type and quantity of countermeasure use varied across experimental conditions. A separate study will be required to more fully explore how and why specific countermeasures are used by truthful and deceptive users engaged in a deception detection interaction. Furthermore, the impact of individual countermeasures on Type I and Type II errors will provide a rich area of study. Depending on the application of a system (e.g., border screening), an increase in misclassifications could prove to be a limiting factor in the system's real-world capabilities. It is, however, doubtful that an automated screening system will be used to remove humans completely from the decision-making process. Rather, a successive hurdles approach could be used wherein an automated system provides an initial classification, and a human passes the final judgment. Additionally, previous research has examined polygraph-style mental and physical countermeasures in a CIT [9], but the effectiveness of the spontaneous countermeasures employed by participants in this experiment would require further study.

Finally, while the operationalization of this research focused on user interactions with an automated deception detection system, the broader context of this work is the exploration of user interactions with adversarial systems. The lion's share of technology adoption and use research assumes cooperation between system and user, however, new systems and novel interaction formats are creating exceptions to this traditional view. Within the context of interaction with an adversarial system there is no system adoption. The ways users interact with adversarial systems are fundamentally different from the ways users interact with traditional information systems. Instead of wanting to use the system to improve their own productivity or achieve an objective, users may choose to actively work against an adversarial system. In other words, the behavioral intention may be to circumvent the system, not to use the system. This shift requires a new theoretical understanding of how users will perceive such systems and how users will choose to interact with such systems. Future research can build on this work to help illuminate technology use when the goals of the system and the user are in opposition.

@&#CONCLUSION@&#

The advancement of technology is resulting in a preponderance of new devices and systems that can be used in entirely novel ways. One such area is the introduction of systems and sensors that can be used to measure user behavior and physiology in an interaction not instigated or wanted by the user. Such adversarial systems are typified by involuntary use, little or no user control, and potentially punitive outcomes. The purpose of this research was to investigate user behavior in such a context; this context was operationalized using a deception detection interaction leveraging an automated interviewing system. Theoretically-grounded hypotheses were specified and two laboratory studies were performed to measure the influence of the following three variables on the use of behavioral countermeasures: (1) guilty knowledge, (2) relevant stimuli, and (3) system knowledge. Analysis of the data revealed that guilty knowledge, exposure to relevant stimuli during the system interaction, and increased system knowledge before the interaction all contributed to the use of countermeasures. These findings have important implications for researchers and practitioners (1) developing deception detection systems and (2) seeking to better understand how users interact with adversarial systems.

@&#ACKNOWLEDGEMENTS@&#

This research was supported by the U.S. Department of Homeland Security, through the National Center for Border Security and Immigration (Grant # 2008-ST-061-BS0002), and the Center for Identification Technology Research (CITeR), a National Science Foundation (NSF) Industry/University Cooperative Research Center (I/UCRC) (Project #12F-13W-12). However, any opinions, findings, and conclusions or recommendations herein are those of the authors and do not necessarily reflect views of the U.S. Department of Homeland Security or the Center for Identification Technology Research. The views, opinions, and/or findings in this document are those of the authors. We also acknowledge contributions made by Jay F. Nunamaker, Jr. and Judee K. Burgoon in advising this research, as well as support provided by a number of researchers affiliated with the Center for the Management of Information (CMI).

@&#REFERENCES@&#

