@&#MAIN-TITLE@&#An automated vision-based method for rapid 3D energy performance modeling of existing buildings using thermal and digital imagery

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Image-based 3D modeling method of actual building energy performance is presented.


                        
                        
                           
                           Building environments and their energy performances are jointly visualized in 3D.


                        
                        
                           
                           Automatically 3D-registered thermal images assist with localizing energy problems.


                        
                        
                           
                           3D spatio-thermal models can facilitate energy building diagnostics and retrofit analysis.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Energy performance modeling

3D reconstruction

Structure-from-Motion

Thermography

Building retrofit

@&#ABSTRACT@&#


               
               
                  Modeling the energy performance of existing buildings enables quick identification and reporting of potential areas for building retrofit. However, current modeling practices of using energy simulation tools do not model the energy performance of buildings at their element level. As a result, potential retrofit candidates caused by construction defects and degradations are not represented. Furthermore, due to manual modeling and calibration processes, their application is often time-consuming. Current application of 2D thermography for building diagnostics is also facing several challenges due to a large number of unordered and non-geo-tagged images. To address these limitations, this paper presents a new computer vision-based method for automated 3D energy performance modeling of existing buildings using thermal and digital imagery captured by a single thermal camera. First, using a new image-based 3D reconstruction pipeline which consists of Graphic Processing Unit (GPU)-based Structure-from-Motion (SfM) and Multi-View Stereo (MVS) algorithms, the geometrical conditions of an existing building is reconstructed in 3D. Next, a 3D thermal point cloud model of the building is generated by using a new 3D thermal modeling algorithm. This algorithm involves a one-time thermal camera calibration, deriving the relative transformation by forming the Epipolar geometry between thermal and digital images, and the MVS algorithm for dense reconstruction. By automatically superimposing the 3D building and thermal point cloud models, 3D spatio-thermal models are formed, which enable the users to visualize, query, and analyze temperatures at the level of 3D points. The underlying algorithms for generating and visualizing the 3D spatio-thermal models and the 3D-registered digital and thermal images are presented in detail. The proposed method is validated for several interior and exterior locations of a typical residential building and an instructional facility. The experimental results show that inexpensive digital and thermal imagery can be converted into ubiquitous reporters of the actual energy performance of existing buildings. The proposed method expedites the modeling process and has the potential to be used as a rapid and robust building diagnostic tool.
               
            

@&#INTRODUCTION@&#

The building sector accounts for approximately 24% of global energy consumptions and up to 41% in developed countries [1]. Looking at a building’s life-cycle energy consumptions, up to 84% of the total energy consumed is used during the operation phase [1]. As a result, government agencies are proposing several new mandates to improve the efficiency of building energy consumption and minimize their environmental impacts. The problem of energy efficiency is not only limited to how new buildings should be designed. Most existing buildings also do not meet these rigorous energy standards. Faulty behaviors in existing buildings (e.g., poor insulations) can alone account for 2–11% of the total building energy consumption [2].

The age of existing buildings is also rising, and as a result they continuously undergo degradation over the remainder of their service life. Currently 87% of overall residential buildings and 74% of total commercial buildings were built before year 2000 when the current rigorous energy regulations were not established [1,3]. This is almost about 150 billion S.F., roughly half of the entire building stock in the US which over the next 30years will require retrofit to meet the new rigorous energy standards [4]. Non-compliance with the new energy standards is not limited to the existing buildings. A quarter of the newly constructed and certified buildings do not also save as much energy as their designs had originally predicted [5].

In these conditions, retrofitting is imperative for most buildings as it can help minimize the cost associated with excessive energy consumptions. Thus, about 86% of the overall building construction expenditures is now being committed to the renovation of existing buildings as opposed to new construction [6]. As any retrofit decision directly translates into cost, building owners and occupants are increasingly interested in identifying potential retrofit alternatives so that they can take timely corrective actions. To that end, rapid and accurate modeling of the current energy performance status of their buildings is an essential prerequisite for proper retrofit.

Currently, it is common practice to model building energy performance using simulation software such as EnergyPlus, Ecotect, and eQuest. Despite the effectiveness, there are several main barriers facing the current practice of energy modeling. Current methods:
                        
                           1.
                           are time-consuming and labor-intensive due to the manual modeling and calibration processes [7,8]. They also delay the decision-making process by the late delivery of information necessary to guide the retrofit process;

cannot determine the performance deviations caused by construction defects or degradations. This is mainly because the simulation tools only provide static representations on a building’s energy performance within a given space (e.g., average temperature of each wall) [9]. Thus, they cannot detect the specific building areas in need for retrofit; and

often do not accurately represent the actual energy performances, once buildings are operating. This is due to the difficulties in the selection of accurate simulation inputs and the rather subjective calibrations of the energy models [10–13].

There is a need for a robust and easy-to-use energy performance modeling method that allows rapid and accurate diagnostics of the dynamic conditions of a building at its element level. Over the past few years, thermographic inspection using thermal cameras has become prevalent for the detection of thermal defects and air leakages in existing buildings [14]. Relatively cheap consumer-level thermal cameras (in range of $3–7k) enable acquisition of high quality thermal images without any need for special trainings. Thermography is defined as the process of detecting and measuring heat variations emitted by an object under inspection and transforming identified changes into visible imagery [15]. In the context of building energy performance modeling, thermography can be a robust tool in recording, analyzing, and reporting actual energy performance of existing buildings. Thermal images captured from buildings are directly influenced by energy performance deviations caused by insulation voids or thermal bridges. Since building energy performance is largely related to the space heating and cooling which account for 48.8% and 24.8% of the total usage in residential and commercial buildings respectively [1], monitoring such thermal performance can significantly support building diagnostics and retrofit.

Currently the majority of basic and applied research on thermography is based on raw 2D thermal images. Nonetheless, application of raw thermal imagery for energy performance modeling of existing buildings is challenging. Because the spatial resolution and field-of-view of thermal images are much lower than that of the digital imagery (e.g., 320×240 pixels for thermal images compared to 2048×1356 for their digital counterparts, as shown in Fig. 1
                     ), there is a need for a large number of images to represent the energy performance in a given building space. Not only many thermal images need to be captured, but also they are often unordered, un-calibrated, and not geo-tagged. Hence it is difficult in the post-processing stage to figure out where these images were actually captured from or what building components and systems they are representing. Several current practices involve integration of a pair of digital and thermal images (picture-in-picture overlay) or voice recording to facilitate data collection on location of these imagery or building elements they represent. Given the significant number of thermal imagery, examining these large numbers of the 2D overlaid images or recorded voices for the purpose of entire building assessment can be time-consuming.

To address these limitations and facilitate energy performance modeling, this paper proposes a new vision-based method for reconstructing actual 3D spatio-thermal models of buildings, registering thermal and digital imagery in a common 3D environment, and visualizing the energy performance data at the level of 3D points. The integrated visualization of 3D building and thermal models, in addition to 3D-registered imagery, enable the auditors to remotely analyze how temperature values are spatially distributed within a built environment. The proposed method benefits from the new thermal cameras which have built-in digital lenses. The digital and thermal images that are simultaneously captured are used to document the thermal distribution of both interior and exterior building spaces at a high sensitivity rate. Fig. 1 shows examples of these thermal and digital images.

The proposed method for generating 3D spatio-thermal models is validated for several interior and exterior built environments of a residential building and an instructional facility. In the following sections, first the challenges of the current practice for energy performance modeling and the state-of-the-art in 3D thermal modeling of existing buildings are reviewed. A set of open problems are then presented for the field. Next, the proposed methods and the experimental results on several datasets are presented. Video demos along with additional supplementary materials including data can be found at www.raamac.cee.illinois.edu.

Energy performance modeling using energy simulation tools is essentially an empirical exercise that relies on good observations to capture the current conditions of existing buildings. First, building geometrical information are collected using traditional field surveying techniques such as tape measurement, Total Stations, terrestrial laser scanning, or from 2D CAD models. Currently, 2D images are sometimes used to manually extract the geometrical information from buildings (e.g., OpenStudio SketchUp). Based on the collected geometrical information, modelers manually develop 3D energy models of existing buildings with the resulting geometry and simulation inputs (e.g., weather data and occupancy schedules).

However, due to the assumptions typically made during the modeling stage, once buildings are operating, the actual energy performance deviates from the baseline models. For example, during each model set-up, all areas in a building are assumed to have similar insulation and degradation conditions. Due to these assumptions, this approach may not provide an accurate representation of defects and the as-is energy performance of a building. If the baseline model is closely representing the as-is performance, it is more likely to provide a reliable source for the analysis of retrofit alternatives. Thus, to form an accurate baseline model, modelers need to manually calibrate model parameters to minimize the deviations between baseline and actual energy performances. The outcome of this process will be the actual building energy performance model within an acceptable tolerance in accuracy. This is currently a widely accepted energy performance modeling process for existing buildings [16–18]. Despite the benefits of the energy simulation tools, there are a number of challenges associated with their application for building retrofit which include:
                           
                              1.
                              Application of existing energy performance simulation tools is time-consuming and labor-intensive. It often requires several weeks to a few months to manually build a 3D energy model, and therefore, current approaches are restricted to only high-profile and high budget projects [7,8]. Moreover, due to the significant time and effort required for the energy modeling processes including the manual modeling and calibrations, practitioners have to spend most of their time on modeling as opposed to more valuable tasks such as decision-making on the best retrofit alternatives.

Current building energy performance simulation tools can only provide general indication of thermal performance within a given space. According to a recent report by the National Institute of Standards and Technology (NIST) [19], one of the top measurement challenges for energy-efficient building is measuring material aging (i.e., the effect of aging on insulation efficiency). However, current energy modeling tools do not provide detailed information on the spatial distribution of thermal performance (e.g., specific building areas in need for retrofit due to construction defects or building degradation over time), and

The results of existing energy modeling tools typically do not reflect a realistic picture of the actual building energy performances [10–13]. Identifying a particular combination of model parameters which results in a good fit between the baseline models and the measured energy performance does not always guarantee an accurate actual energy performance model. There can be other combinations of model parameters which yield the similar reasonable results. Consequently, the quality of models relies heavily on the subjective judgment of the modelers. Even if the same modeling tools and building characterization data are used, a modeler may not produce the same energy modeling results [10,11]. Thus, during the manual model calibration process, modelers iteratively create new energy models with different parameter values to find the most proper combination until the difference between actual and computed energy performance is minimal. According to a recent survey from a wide range of energy modeling stakeholders [10], one of the major bottlenecks in the current modeling practice was identified as the difficulty in translating building geometrical and environmental information into right inputs for energy modeling. Since the modeling process requires the selection and adjustment of a large number of model parameters, the process will be vulnerable to the subjective analysis of the experts and may not result in consistent outcomes. Overall, although current building simulation tools are typically suitable for comparison of design alternatives, they may not directly predict the actual energy performance of buildings [12,20].

The state-of-the-art practice for building thermal modeling is based on application of 2D thermal imagery. For example, the GeoInformation Group [21] developed a method to identify relative building roof heat loss properties using a color-coded thermal-mapping. Similarly, Infrared Concepts Corporation [22] proposed a roof thermal mapping methodology through aerial surveying to gauge repair needs of flat-roofed structures. These methods require manual and sequential texture-mapping of all pairs of thermal and digital images. As a result, their application for both interior and exterior built environments can be time-consuming.

Nowadays, most thermal cameras have the capability to capture both digital and thermal images simultaneously. This has created an unprecedented opportunity, and as a result several researchers have looked to fusing thermal and digital imagery to put thermal images into the building context and obtain more informative scene representations. For example, HomeInspex [23] used an overlay image which consists of a thermal and digital imagery from the same scene for building retrofit. Nonetheless, the 3D registration of one pair of 2D digital and thermal image is still not sufficient to provide a whole representation of the energy performance in a given space.

Several researchers in both computer science and building construction communities have proposed new methods for 3D thermal modeling of existing buildings. Stockton [24] presented a 3D thermal monitoring system to sense, analyze and manage power and cooling operations in a data center. The implementation involves manually creating image mosaics and performing texture-mappings. The results of this work show the effectiveness of the 3D thermal mapping for building retrofit purposes.

In the Architecture/Engineering/Construction community, Cho and Wang [3] are among the first to introduce a 3D thermal modeling method of an residential building façade using a Hybrid LIDAR system. The proposed system which consists of a laser scanner and a thermal camera can generate 3D point cloud models for building envelops with the corresponding temperature values at their individual point level. The developed method has been extensively tested in both lab settings, and also on the façade of a residential building, and promising results are presented. In a more recent work [25], a new methodology for visualizing 3D thermal models of building envelops on web-based geospatial systems such as Google Earth is proposed and promising results are presented. This approach in particular can be very useful where city-level energy modeling is needed.

Lagüela et al. [26] introduced a methodology for registering temperature data in the 3D building point clouds produced by a terrestrial laser scanner. Nüchter [27] also proposed generating 3D thermal models of indoor environments using a similar approach. The work utilizes a robot equipped with a Reigl VZ-400 laser scanner and an Optris thermal camera. The initial experiments were conducted in indoor lab spaces. Laser scanners can provide useful visual information; nonetheless, confined indoor building environments, in addition to the lack of semantics in raw 3D laser scanning point clouds can still create several challenges in their applicability for energy modeling and particularly for automated identification and characterization of the as-built information. Ongoing research on laser scanners and thermal imagery such as [28] is focusing on addressing these issues for residential buildings.

In the context of jointly utilizing digital and thermal imagery, Lagüela et al. [29] proposed a new method which benefits from several semi-automated feature selection and image registration techniques to form digital–thermal image mosaics and also sparse 3D thermal models. In this work, the process of area-based image registration involves manual selection of overlapping regions among consecutive thermal images, followed up with a visual verification to minimize false matches. In the computer vision community, several researchers have also addressed the need for 3D thermal modeling [30–35]. Although 3D thermal models resulted from the above methods are generally accurate, these researches are primarily focusing on confined and isolated objects (e.g., cylinder). Also these methods do not directly work with the typical thermal cameras that are used in the current building auditing practices. Rather they propose hybrid systems which consist of multiple thermal cameras (e.g., a combination of one digital and two thermal cameras or a thermal camera stereo system) to build the 3D thermal models. More research needs to be done to directly utilize a simple thermal camera for the purpose of 3D energy performance modeling.

Given a collection of 2D digital and thermal imagery from a single thermal camera, our goal is to automatically model the as-is geometry of existing buildings in 3D and visualize the actual energy performance at building element level. In our study, digital images are capturing the actual geometrical information of existing buildings, and the thermal images are capturing the instantaneous energy performance. The data and process in the proposed approach is illustrated in Fig. 2
                     .

As observed in Fig. 2, once the thermal and digital images are captured simultaneously using a single thermal camera, the following steps are conducted: At first, the digital images are placed into an image-based 3D reconstruction pipeline which consists of a new Graphic Processing Unit (GPU)-based Structure-from-Motion (SfM) and Multi-View Stereo (MVS) algorithms to automatically generate a dense 3D point cloud of the existing building. Next step involves application of thermal imagery for generating a dense 3D thermal point cloud. The thermal camera is internally calibrated, and as a result the focal length and radial distortion parameters of the thermal lens are calculated. By forming the Essential matrix, the Euclidean transformation between thermal and digital cameras is also calculated, and the extrinsic thermal camera parameters are derived. Based on the intrinsic and extrinsic thermal camera parameters, the perspective projection matrices for all thermal images are formed. Using the MVS algorithm, a dense 3D thermal point cloud is then generated. Finally to form the actual 3D spatio-thermal models, the 3D building and thermal point clouds are automatically superimposed along with the 3D-registered digital and thermal images. The result is a 3D environment in which the spatio-thermal models along with digital and thermal images are jointly integrated and visualized. The following subsections describe each of these steps in detail.

A camera maps the contents of the 3D world into 2D imagery. Such transformation is represented by a 3×4 camera perspective projection matrix (MT
                        ) which maps from homogeneous coordinates of a world point in the 3D-space to homogeneous coordinates of the images point on a 2D image plane. MT
                         consists of camera intrinsic and extrinsic parameters [36]. Camera intrinsic parameters (K) include the focal length, skew factors, lens distortion, and the position of an image’s center point. Camera extrinsic parameters include pose and location of the camera which indicate translation (T) and rotation (R). This represents the transformation from the origin of the scene’s 3D coordinate system to the origin of each camera’s local 3D coordinate system. Assuming a pinhole camera model, MT
                         can be formulated as:
                           
                              (1)
                              
                                 x
                                 =
                                 
                                    
                                       K
                                    
                                    
                                       3
                                       ×
                                       3
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                R
                                             
                                             
                                                3
                                                ×
                                                3
                                             
                                          
                                          |
                                          
                                             
                                                T
                                             
                                             
                                                3
                                                ×
                                                1
                                             
                                          
                                       
                                    
                                 
                                 X
                                 =
                                 
                                    
                                       M
                                    
                                    
                                       T
                                       3
                                       ×
                                       4
                                    
                                 
                                 X
                              
                           
                        where X is a 3D point in a world coordinate frame (in form of a homogeneous 4-vector [x,
                        y,
                        z,1]
                           T
                        ), x is a 2D point in a camera coordinate frame (in form of a homogeneous 3-vector [x,
                        y,1]
                           T
                        ). The resulting MT
                         is used to reconstruct the as-is 3D building model.

First, using an image-based 3D reconstruction method known as Structure-from-Motion (SfM) [37,38], we compute intrinsic and extrinsic camera parameters from the given set of digital images. To reduce computational cost, a new GPU-based Structure-from-Motion (SfM) algorithm is used: (1) distinct visual features are automatically detected and matched across all digital image pairs. Here, we use the GPU-based implementation [39] of the Scale-invariant feature transform (SIFT) keypoint detection [40]. This implementation rapidly finds distinctive feature points which are invariant to changes in location, rotation, scale, and scene illumination; (2) once these features are detected independently in each image, their descriptors are matched across each image pair using a nearest neighborhood matching algorithm to further expedite the matching process. We then form the Epipolar geometry between each digital image pair by estimating the Fundamental matrix (F) within a RANdom SAmple Consensus (RANSAC) algorithm [41]. Fitting F in the RANSAC loop helps remove false matches as it enforces the corresponding features to have consistent transformation:
                              
                                 (2)
                                 
                                    
                                       
                                          p
                                       
                                       
                                          T
                                       
                                    
                                    
                                       
                                          Fp
                                       
                                       
                                          ′
                                       
                                    
                                    =
                                    0
                                 
                              
                           where p and p′ are homogeneous coordinates of the corresponding feature points in each image pair. This is a key component as many building elements might have similar visual features. This process can eliminate the mismatches that are not consistent by the general transformation from one camera location to another. If more than one features in image i match the same feature in image j, both of such matches are removed, as one is a false match [36]; (3) Once the feature matches are formed, an initial pair of matching features will be selected for initializing the 3D reconstruction process. For selection of the initial pair, we follow the selection heuristics proposed in [38]. Starting from the initial 3D reconstruction, additional camera poses and locations of the 3D points are incrementally computed. Each time a new camera is added, the new camera’s parameters are also estimated using DLT inside the RANSAC loop. Once each camera is added, the bundle adjustment algorithm [43] automatically optimizes the camera parameters and the locations of the points in 3D. This process is repeated for all cameras until no cameras which observe any reconstructed 3D point remain. In this work, we use the GPU-based bundle adjustment library [44] which significantly cuts the computation time of our previous implementation of [37]. For more details, the readers are recommended to look into [37,38]. The outcome of this process is a sparse 3D point cloud along with intrinsic and extrinsic digital camera parameters.

Our approach in generating dense 3D building point cloud models is similar to [45] wherein Multi-View Stereo (MVS) algorithm [46] is used. The following two data structures are used: (1) camera projection matrices are derived from SfM algorithm mentioned in the previous subsection; and (2) undistorted images that radial distortions are minimized. There are three components to this algorithm: matching, expanding, and filtering. First, features found by Harris and Difference-of-Gaussians operators are primary matched for all images, yielding a sparse set of 2D patches. Then, expansion and filtering steps are iterated to make patches dense and remove erroneous matches by using visibility constraints. The size of the patch controls the density of reconstructions. Since this algorithm intends to reconstruct at least one patch in every square region in all the registered images, the increase of the patch size results in sparser 3D reconstruction. The final outcome is a dense 3D building point cloud which represents the geometrical conditions of the building, plus the 3D-registered digital images within the underlying reconstructed scene.

To generate 3D spatio-thermal models, at first digital and thermal images need to be co-registered in a common 3D environment to map the thermal values to the reconstructed building elements. To that end, our initial approach was to directly use the SfM algorithm by forming the Epipolar geometry [36] between (1) every pair of digital and thermal images or (2) every two thermal images, and computing the F within the RANSAC loop. For this purpose, we conducted exhaustive experiments on the state-of-the-art invariant feature detection and matching algorithms to initiate the SfM process for unordered images. These methods included SIFT, Affine-SIFT (ASIFT), and Speeded Up Robust Feature (SURF). In all of our experiments, these feature detection and matching algorithms worked very poorly or did not work at all. In most cases, the number of matching feature points before fitting F in the RANSAC loop where less than 10 features which suggested a strong probability of significant mismatches among these paired feature points. Fig. 3
                        a and c illustrate an example of the thermal and digital images simultaneously captured from the same camera location; (b) and (d) highlight the detected SIFT feature points, and (e) displays the matched feature points. As observed, not only the number of matching points is minimal (<10 features), but also most of these feature points are incorrectly matching. Fig. 3f–j further shows the same limitation of the feature detection and matching algorithms between a pair of thermal images from the same scene.

Such performance is primarily caused by the inherent characteristics of the thermal imagery:
                           
                              (1)
                              Due to the current limitation of thermal lens, the spatial resolution of thermal cameras is considerably lower than the optical ones (most consumer-level thermal cameras have a spatial resolution of 160×120 or 320×240 pixels); and

Since thermal cameras use gradient color coding to capture thermal performance of a surface, typical points that are detected as features in digital imagery cannot be detected. This is primarily because feature detection algorithms look for those pixels that after initial convolution processes, still have the maximum intensity with respect to their surrounding scale-space environments. Since thermal color coding smoothens such sharp changes in the intensity map, these pixels cannot be detected as feature points in thermal images.

Hence, currently there is a limitation to directly use SfM algorithms on unordered thermal imagery and generate 3D thermal point cloud models. To overcome this challenge, we propose a new 3D thermal modeling method which includes the following steps:

Prior to calibration, the varying range of temperatures in thermal images are normalized to a fixed spectrum of colors so that the RGB color of each point corresponds to an absolute temperature value. Next, through an internal calibration process, the intrinsic parameters of the thermal camera (i.e., the focal length and radial distortion parameter) are computed. To calibrate general digital cameras, calibration checkerboards are typically used [47]. However, because the thermal cameras are only capable of detecting thermal variations emitted by the objects in a scene, it is not possible to clearly distinguish intersecting corners and lines of the conventional calibration rig. Hence, we design a thermal calibration rig (550×700mm) with 42 small LED lights located on the intersections of the conventional checkerboard (δ
                           =10cm) to calibrate the thermal camera similar to [26,29] (Fig. 4
                           ). Once the LED lights are on, they generate enough heat that they easily distinct themselves in color from their surrounding environment.

We use the camera calibration tool box [47] to internally calibrate the thermal images (i.e., the focal length (fT
                           ) and radial distortion parameters (
                              
                                 
                                    
                                       k
                                    
                                    
                                       T
                                    
                                    
                                       1
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       k
                                    
                                    
                                       T
                                    
                                    
                                       2
                                    
                                 
                              
                           ) of the thermal camera are calculated). Next, assuming a pinhole camera model, the radial distortion of thermal imagery is removed. The thermal images with pixels denoted as p
                           =[u,
                           v,1]
                              T
                            are undistorted by using the following equation:
                              
                                 (3)
                                 
                                    
                                       
                                          p
                                       
                                       
                                          undistorted
                                       
                                    
                                    =
                                    
                                       
                                          f
                                       
                                       
                                          T
                                       
                                    
                                    ×
                                    r
                                    (
                                    
                                       
                                          p
                                       
                                       
                                          distorted
                                       
                                    
                                    )
                                    ×
                                    
                                       
                                          p
                                       
                                       
                                          distorted
                                       
                                    
                                 
                              
                           In this equation, r(p) is a function that computes a scaling radial factor to undo the distortion:
                              
                                 (4)
                                 
                                    r
                                    (
                                    p
                                    )
                                    =
                                    1.0
                                    +
                                    
                                       
                                          k
                                       
                                       
                                          T
                                       
                                       
                                          1
                                       
                                    
                                    ×
                                    ‖
                                    p
                                    
                                       
                                          ‖
                                       
                                       
                                          2
                                       
                                    
                                    +
                                    
                                       
                                          k
                                       
                                       
                                          T
                                       
                                       
                                          2
                                       
                                    
                                    ×
                                    ‖
                                    p
                                    
                                       
                                          ‖
                                       
                                       
                                          4
                                       
                                    
                                 
                              
                           
                        

In our formulation of the pinhole camera projection we assumed 2D images have no radial distortion. Nonetheless, thermal camera lenses create significant radial distortions and require correction. Since most cameras have a fixed thermal lens, this process only needs to be conducted once per camera and does not need to be repeated per data collection process. Hence, it does not add any burden to the auditors as this information can be precompiled in the implementation of the proposed system.

The next step is to calculate the extrinsic parameters of the thermal camera (i.e., location and orientation). For this purpose, we first extract the extrinsic parameters of the digital camera from the outcome of the GPU-based SfM algorithm as explained in Section 3.1.1. To find the extrinsic parameters of the thermal camera, we derive the relative pose of the thermal lens with respect to the digital lens. In the proposed method, we first form the Epipolar geometry between matching features of a pair of thermal and digital images. These feature points are selected in a supervised manner. To find the relative pose between two cameras, using Nistér’s five point algorithm [42], the Essential matrix is computed based on calculating coefficients of a tenth degree polynomial and extracting the roots. The Essential Matrix encapsulates the Epipolar geometry between the two cameras (see Fig. 5
                           ). It is assumed that the digital camera located at the origin of the Cartesian coordinate system has an identity rotation and zero translation (i.e., the digital camera extrinsic matrix=[I|0]). The relative rotation and translation of the thermal camera is determined through the Singular Value Decomposition of the Essential Matrix.

The outcomes of this step are four possible solutions for the thermal camera extrinsic parameters. Among them, only one set corresponds to the true configuration. The other sets correspond to twisted pairs obtained by rotating the views by 180° around the baseline, and the remaining two parameters correspond to reflections of the true and the twisted configurations. In order to determine which extrinsic parameter corresponds to the true configuration, the Cheirality constraint [36] is imposed. Only one pair of matching feature points among the five candidates is enough to check for the constraint and resolve the ambiguity. First, the selected matching points are triangulated using the DLT algorithm assuming [I|0] for the digital camera and using the four candidates for the Rrel
                            and Trel
                           . For each of the four candidates, this process yields a 3D point Q. This point is tested using the Cheirality constraint; i.e.,
                              
                                 (5)
                                 
                                    If
                                    
                                    
                                       
                                          c
                                       
                                       
                                          1
                                       
                                    
                                    ≡
                                    
                                       
                                          Q
                                       
                                       
                                          3
                                       
                                    
                                    
                                       
                                          Q
                                       
                                       
                                          4
                                       
                                    
                                    >
                                    0
                                    
                                    and
                                    
                                    
                                       
                                          c
                                       
                                       
                                          2
                                       
                                    
                                    ≡
                                    
                                       
                                          (
                                          
                                             
                                                P
                                             
                                             
                                                i
                                             
                                          
                                          Q
                                          )
                                       
                                       
                                          3
                                       
                                    
                                    
                                       
                                          Q
                                       
                                       
                                          4
                                       
                                    
                                    >
                                    0
                                    
                                    then
                                    
                                    return
                                    
                                    
                                       
                                          P
                                       
                                       
                                          i
                                       
                                    
                                    
                                    and
                                    
                                    Q
                                 
                              
                           where Q
                           3 and Q
                           4 are the third and fourth components of the Homogenous coordinates of the point Q, and Pi
                           
                           =[Rrel
                           |Trel
                           ] is the relative pose with respect to the digital camera. Based on the rotation and translation of the digital camera (RD
                            and TD
                           ) along with the transformation matrix between the thermal and digital lens [Rrel
                           |Trel
                           ], the extrinsic parameters of thermal camera are calculated according to Eq. (6). Fig. 6
                            summarizes the proposed algorithm.
                              
                                 (6)
                                 
                                    [
                                    
                                       
                                          R
                                       
                                       
                                          T
                                       
                                    
                                    |
                                    
                                       
                                          T
                                       
                                       
                                          T
                                       
                                    
                                    ]
                                    =
                                    [
                                    
                                       
                                          R
                                       
                                       
                                          D
                                       
                                    
                                    
                                       
                                          R
                                       
                                       
                                          rel
                                       
                                    
                                    |
                                    
                                       
                                          R
                                       
                                       
                                          D
                                       
                                    
                                    
                                       
                                          T
                                       
                                       
                                          rel
                                       
                                    
                                    +
                                    
                                       
                                          T
                                       
                                       
                                          D
                                       
                                    
                                    ]
                                 
                              
                           
                        

Once the thermal camera intrinsic parameters are derived from the calibration process and the extrinsic parameters are automatically calculated, generating the 3D thermal point cloud model converts into the classical problem of reconstructing a 3D scene using calibrated thermal imagery. Because our thermal images are now fully calibrated, for the purpose of generating a dense point cloud model, we place the derived thermal camera projection matrixes and undistorted thermal images into the MVS algorithm presented in Section 3.1.2. The outcome of this process is a dense thermal point cloud model in a 3D environment wherein the thermal cameras are also spatially registered.

The last step is to align the reconstructed thermal and building point cloud models and ultimately form the actual 3D spatio-thermal models. Since the 3D thermal model was reconstructed by estimating the relative pose of the thermal camera with respect to the digital camera, both of these models are in the same coordinate system. Thus, superimposing these two models simply involves bringing both point cloud models into the same 3D environment. This also enables both digital and thermal images captured to be 3D-registered within the same 3D virtual environment.

The final step involves transforming the 3D spatio-thermal model into the site coordinate system. This is a critical step since the scale of image-based point cloud models may be unknown. To do so, we use the closed-form solution of absolute orientation using unit quaternions [48] similar to [37,49]. Here, the transformation has 7 degrees of freedom. Thus, theoretically we need to have the 3D coordinates of a minimum of three control points from both the 3D spatio-thermal model and the real-world. These points can either be pre-located before conducting the data collection, or could simply be extracted from known dimensions such as two dimension of a room (e.g., height and width). Let n be the number of the matching points between the 3D spatio-thermal model and the actual site coordinate system. The two sets of Cartesian points are denoted by 
                           
                              
                                 
                                    r
                                 
                                 
                                    Model
                                 
                                 
                                    i
                                 
                              
                           
                         and 
                           
                              
                                 
                                    r
                                 
                                 
                                    Site
                                 
                                 
                                    i
                                 
                              
                           
                         respectively, where i is a corresponding point (i
                        ∊[1,
                        n]) and Model and Site represent the spatio-thermal model and site coordinates respectively. The Euclidean transformation between each corresponding pair can be formulated as:
                           
                              (7)
                              
                                 
                                    
                                       r
                                    
                                    
                                       Site
                                    
                                    
                                       i
                                    
                                 
                                 =
                                 sR
                                 
                                    
                                       
                                          
                                             
                                                r
                                             
                                             
                                                Model
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                 
                                 +
                                 T
                              
                           
                        where s is the uniform scaling factor, T is the translational offset, and 
                           
                              R
                              (
                              
                                 
                                    r
                                 
                                 
                                    Model
                                 
                                 
                                    i
                                 
                              
                              )
                           
                         is the rotation of the 3D spatio-thermal model. To minimize user selection errors for finding correspondence between 3D spatio-thermal model and the real-world coordinate system, the estimation of the accurate registration of point coordinates from one system into another is approximated with minimization of the sum of squared residual errors between these n corresponding pairs. This further helps minimize the errors caused by user selection or poor measurements of the actual scene. Hence, the minimization of the errors can be formulated using Eq. (8). This formulation results in the 3D spatio-thermal model to be transformed into the site coordinates and also measures the accuracy of registration.
                           
                              (8)
                              
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          1
                                       
                                       
                                          n
                                       
                                    
                                 
                                 ‖
                                 
                                    
                                       e
                                    
                                    
                                       i
                                    
                                 
                                 
                                    
                                       ‖
                                    
                                    
                                       2
                                    
                                 
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          1
                                       
                                       
                                          n
                                       
                                    
                                 
                                 ‖
                                 
                                    
                                       r
                                    
                                    
                                       Site
                                    
                                    
                                       i
                                    
                                 
                                 -
                                 sR
                                 
                                    
                                       
                                          
                                             
                                                r
                                             
                                             
                                                Model
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                 
                                 -
                                 T
                                 
                                    
                                       ‖
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        To enable integrated visualization of digital and thermal 3D point cloud models and 3D-registered imagery, we created a new Energy Performance Augmented Reality environment (EPAR) based on our previous work [37]. The following data structure is used in the EPAR environment:


                        
                           
                              •
                              
                                 
                                    
                                       (
                                       
                                          
                                             P
                                          
                                          
                                             D
                                          
                                          
                                             1
                                          
                                       
                                       ,
                                       
                                          
                                             P
                                          
                                          
                                             D
                                          
                                          
                                             2
                                          
                                       
                                       ,
                                       …
                                       ,
                                       
                                          
                                             P
                                          
                                          
                                             D
                                          
                                          
                                             n
                                          
                                       
                                       )
                                    
                                 : a set of 3D points where 
                                    
                                       
                                          
                                             P
                                          
                                          
                                             D
                                          
                                          
                                             i
                                          
                                       
                                       =
                                       〈
                                       
                                          
                                             X
                                          
                                          
                                             D
                                          
                                          
                                             i
                                          
                                       
                                       ,
                                       
                                          
                                             RGB
                                          
                                          
                                             D
                                          
                                          
                                             i
                                          
                                       
                                       〉
                                    
                                  encapsulates the 3D location (
                                    
                                       
                                          
                                             X
                                          
                                          
                                             D
                                          
                                          
                                             i
                                          
                                       
                                    
                                 ) and the color (
                                    
                                       
                                          
                                             RGB
                                          
                                          
                                             D
                                          
                                          
                                             i
                                          
                                       
                                    
                                 ) of each point which is the average from the colors of all cameras that observe it.


                                 
                                    
                                       (
                                       
                                          
                                             M
                                          
                                          
                                             D
                                          
                                          
                                             1
                                          
                                       
                                       ,
                                       
                                          
                                             M
                                          
                                          
                                             D
                                          
                                          
                                             2
                                          
                                       
                                       ,
                                       …
                                       ,
                                       
                                          
                                             M
                                          
                                          
                                             D
                                          
                                          
                                             n
                                          
                                       
                                       )
                                    
                                 : a set of digital cameras where 
                                    
                                       
                                          
                                             M
                                          
                                          
                                             D
                                          
                                          
                                             i
                                          
                                       
                                       =
                                       
                                          
                                             K
                                          
                                          
                                             D
                                          
                                          
                                             i
                                          
                                       
                                       [
                                       
                                          
                                             R
                                          
                                          
                                             D
                                          
                                          
                                             i
                                          
                                       
                                       |
                                       
                                          
                                             T
                                          
                                          
                                             D
                                          
                                          
                                             i
                                          
                                       
                                       ]
                                    
                                  represents the digital camera projection matrix for camera i.


                                 
                                    
                                       (
                                       
                                          
                                             P
                                          
                                          
                                             T
                                          
                                          
                                             1
                                          
                                       
                                       ,
                                       
                                          
                                             P
                                          
                                          
                                             T
                                          
                                          
                                             2
                                          
                                       
                                       ,
                                       …
                                       ,
                                       
                                          
                                             P
                                          
                                          
                                             T
                                          
                                          
                                             n
                                          
                                       
                                       )
                                    
                                 : a set of 3D thermal points where 
                                    
                                       
                                          
                                             P
                                          
                                          
                                             T
                                          
                                          
                                             i
                                          
                                       
                                       =
                                       〈
                                       
                                          
                                             X
                                          
                                          
                                             T
                                          
                                          
                                             i
                                          
                                       
                                       ,
                                       
                                          
                                             RGB
                                          
                                          
                                             T
                                          
                                          
                                             i
                                          
                                       
                                       〉
                                    
                                  encapsulating the 3D location (
                                    
                                       
                                          
                                             X
                                          
                                          
                                             T
                                          
                                          
                                             i
                                          
                                       
                                    
                                 ) and the color (
                                    
                                       
                                          
                                             RGB
                                          
                                          
                                             T
                                          
                                          
                                             i
                                          
                                       
                                    
                                 ) of each 3D thermal point. Here the color is corresponding to the normalized color spectrum as introduced in Section 3.2.1.


                                 
                                    
                                       
                                       (
                                       
                                          
                                             M
                                          
                                          
                                             T
                                          
                                          
                                             1
                                          
                                       
                                       ,
                                       
                                          
                                             M
                                          
                                          
                                             T
                                          
                                          
                                             2
                                          
                                       
                                       ,
                                       …
                                       ,
                                       
                                          
                                             M
                                          
                                          
                                             T
                                          
                                          
                                             n
                                          
                                       
                                       )
                                    
                                 : a set of digital cameras where 
                                    
                                       
                                          
                                             M
                                          
                                          
                                             T
                                          
                                          
                                             i
                                          
                                       
                                       =
                                       
                                          
                                             K
                                          
                                          
                                             T
                                          
                                          
                                             i
                                          
                                       
                                       [
                                       
                                          
                                             R
                                          
                                          
                                             T
                                          
                                          
                                             i
                                          
                                       
                                       |
                                       
                                          
                                             T
                                          
                                          
                                             T
                                          
                                          
                                             i
                                          
                                       
                                       ]
                                    
                                  represents the thermal camera projection matrix for camera i.

A mapping relationship between the points and the cameras that observe each points.

The x and y coordinates of the points in local coordinate system of each image.

The number of feature points and cameras which observe the points.

In our visualization interface (which is shown later in Figs. 7–12
                        
                        
                        
                        
                        
                        ), the digital and thermal cameras are rendered in form of pyramid-shape camera frusta. Once a camera frustum is visited in the reconstructed 3D scene, the frustum is automatically texture-mapped with the full-resolution images. Here, the user can either select to view the digital or thermal image captured from the particular camera location. The user location can also be preserved, while the viewpoint is changing to jointly visualize and study the 3D point cloud models and the 3D-registered imagery.

In order to validate the proposed method, a series of experiments were conducted on both residential and instructional facilities. The image datasets for these experiments were collected from several interior and exterior locations of a typical residential building and an instructional facility. For the instructional facility, we considered experimenting on an office room, a main hall, a corridor, and a façade. A kitchen room and the façade of a residential building were also studied. In these experiments, digital and thermal images were simultaneously captured using an FLIR E60 thermal camera which has a built-in digital camera. Table 1
                         shows the technical specifications of the thermal camera.

In our experiments, we evaluated the image-based reconstruction process based on completeness (i.e., the density of the point clouds, and percentage of the images that were successfully registered) and also the computational time required for 3D modeling (both CPU and GPU-based implementations). The accuracy of registering the 3D spatio-thermal point cloud into the site coordinate system was also measured using the Eq. (8). This accuracy gives an indication of how precisely the task of measurement can be conducted on the spatio-thermal modeling.


                        Figs. 7–12 present several snapshots from the outcome of the 3D spatio-thermal modeling on each of our experiments. Particularly Fig. 7 and 8 show results on the façade of the instructional facility and a kitchen of the residential building respectively. As observed, the resulting building and thermal point cloud models are shown for each case. Also one example on the location of the camera and rendering of the related 3D-registered thermal and digital imagery is shown. Figs. 9 and 10 show the outcomes for the other indoor and outdoor environments. In the second row for each case, the thermal point cloud models as well as the registration of the thermal and digital imagery with respect to the thermal point clouds are further visualized. Figs. 11 and 12 show the 3D spatio-thermal models for both outdoor and indoor environments based on the automated superimposition of both the digital and thermal models. Also the 3D registration of the thermal and digital imagery is shown in various cases.

Our EPAR visualization system supports several modes for navigating through the 3D spatio-thermal models and thermal and digital imagery. These modes include free-flight navigation, finding related building views, area-based navigation, and also viewing 3D-registered imagery. The navigation controls is similar to most engineering 3D viewers. For example, the user can move the virtual camera in 3D and pan, tilt, and zoom it. This allows the auditors to move around the reconstructed scene and provides a simple way to find proper viewpoints, or related digital and thermal imagery. At any time, the user can also choose a frustum and the virtual camera will divert into the selected camera. In this case, the frustum is rendered by full-resolution thermal or digital imagery. The virtual camera pans so that both selected images as well as the 3D environment can be jointly visualized. Videos of these experiments can be found at www.raamac.cee.vt.edu/epar.


                        Table 2
                         presents detailed information on the conducted experiments and shows the results for validating each experiment based on the metrics presented in the previous subsection. As observed, for the six experiments that were conducted, the numbers of digital and thermal images used are presented. For each experiment, the success ratio for registration of the digital images to the underlying reconstructed point cloud is also shown. In almost all of our experiments, high registration success ratios are observed. Nonetheless, in RB#2 experiment where a kitchen was being reconstructed, the wide baseline among digital images and the lack of distinct features in most surfaces resulted in poor 3D reconstruction outcomes. This table further presents the density of the reconstructed point cloud models. As observed, the density of the thermal models is considerably smaller than that of the building point cloud models. This is due to the low spatial resolution of the thermal images. The results on the computational time of our 3D modeling process are also presented. Our new GPU-based platform significantly cuts down in the computational time for totally unordered images. Poor performance is observed on the IF#1 experiment where 678 images were used. Currently using our GPU-based implementation, the entire model of a building with approximately 3528 S.F. of the façade can be reconstructed in about 11 hours using a regular engineering workstation (Benchmarked on an Intel i7 960 core with 24GBs RAM and NVIDIA GeForce GTX 400). The results in all cases are promising, though still more developments need to be done to minimize the overall computation time.

The accuracies of registration of the 3D spatio-thermal model with the site coordinate system are also analyzed based on the Eq. (8), and the results are presented in Table 3
                        . As observed, despite testing our method on different areas with varying footprints, the accuracy of registration is within a reasonable practical range. Undoubtedly, the accuracy is sensitive to how the control points are selected by the user. Since more than the minimum numbers of the required control points (three) were selected in these experiments, such errors are minimized.

The correlation between the number of images per 100 S.F. of visible areas and the density of the point cloud is also studied. Fig. 13
                         shows the results of an experiment on the exterior of the instructional facility (IF#1). The surface area (excluding the roof) was 3528 SF. We conducted seven experiments with various numbers of images decreasing from 672 to 72. For each case, the reduction on images was almost evenly distributed; i.e., only one image from every pair of image with almost similar contribution to the outcome of the 3D reconstruction was gradually removed. The density of the point cloud for the numbers of images per 100 S.F. is mapped and the results are presented in Fig. 13. In almost all cases, an average number of 10 images per 100 S.F. seems to be a practical number. Given different building surface materials, this number may vary. Ongoing research is looking into practical number of images for different types of building surfaces.

This study presented a new method for rapid energy performance modeling of existing buildings from a large number of unordered digital and thermal images. Compared to the previous 3D thermal modeling approaches such as the manual integration of digital and thermal images or application of laser scanning, the proposed method can automatically generate 3D spatio-thermal models in both interior and exterior built environments without any space limitations (e.g., confined indoor environments) using an inexpensive consumer-level single thermal camera. The automatically geo-tagging thermal performance data can further minimize significant effort and time to find ‘where the thermal images were captured from’ or ‘what building components associated with’ during the post-processing stage.

The proposed approach has the potential to reduce the time and effort required for collecting both building geometrical and energy performance data, as well as generating energy models especially because it works with existing auditing tools. As a result, building auditors and other practitioners would spend less time on modeling; rather they can focus on more value-adding tasks such as comparative analysis of possible retrofit alternatives. Streamlining modeling process would provide more timely and informative feedback necessary to guide the building retrofit process, and ultimately supports wide implementation of retrofit assessments. The integrated visualization of spatio-thermal models in addition to digital and thermal imagery also enables owners, facility managers, and auditors to remotely access and virtually walk-through in the existing buildings under energy inspection, especially if this method is implemented in a web-based platform. Without spending significant amount of time and effort to visit or travel to the building for auditing purposes, they can remotely navigate through the 3D spatio-thermal models and browse the collection of thermal and digital images with the corresponding building geometrical information. Thus, the term ‘rapid’ in this study not only refers to the streamlined modeling process itself, but also supporting proactive and quick building diagnostics to identify potential areas required for more detailed energy analysis. While this paper presented a method for 3D spatio-thermal modeling and integrated visualization of 3D point cloud with digital and thermal imagery, several critical challenges remain. Some of the open research problems include:
                        
                           •
                           
                              Integrated visualization of actual 3D spatio-thermal models with expected energy performance models. Integrating actual and expected energy performance models (i.e., joint representation of 3D spatio-thermal model with 3D geometrical models that present expected energy performance of the building) in an augmented reality environment can provide opportunities for (1) creating visualizations that can further facilitate retrofit decision-making analysis by enabling auditors to know how the building is expected to perform in various spaces, and (2) development of methods that can automatically sense and analyze performance deviations and ultimately identify potential areas for retrofit.


                              Generating 4-dimensional energy performance models (3D+time). The proposed method in this paper is capable of generating 3D models that integrate imagery taken in a rather short period of time. Integrating several 3D models can enable the impact of retrofit alternatives to be studied over time (e.g., generating models before and after retrofit or studying the energy performance over a time span). More research needs to be done by techniques that can automatically superimpose several 3D spatio-thermal models.


                              Practical number of images required for various types of surfaces. Since different surfaces generate different number of visual features, more research needs to be done to investigate how many images per S.F. of the area needs to be collected for robust 3D modeling of different environments.


                              Extracting semantic information from the underlying 3D model in support of retrofit analysis. Despite the benefits of visualizing the entirety of an environment using a point cloud representation, representing large areas with millions of point clouds can be visually and technically challenging. More work needs to be done on shape modeling algorithms that can extract 3D elements from the scene. Also more research is needed on appearance-based analysis of digital images to identify surface materials.


                              Completed modeling of interior spaces. The proposed method showed the potential of forming spatio-thermal models in confined spaces. More experiments needs to be conducted to structure entire models, particularly in areas where reflective or specular surfaces exist (e.g., windows, mirrors, and curtain walls).

@&#CONCLUSION@&#

Timely and accurate assessment of existing building energy performance helps owners and facility managers identify potential areas for better retrofit, and meet environmental goals. To that end, a rapid 3D energy performance modeling and visualization method of existing buildings using an image-based 3D reconstruction algorithm on both thermal and digital images is presented. We evaluated the performance of the proposed method in six interior and exterior residential and instructional built environments in term of the completeness of the modeling, the accuracy in registration for measurement purposes, and finally the computational time. Our experimental results indicate the applicability of 3D spatio-thermal models and show the perceived benefits in providing an actual representation of the current energy performance status of both interior and exterior of existing buildings as a robust tool for rapid building diagnostics. Future work involves using geometrical conditions extracted from the 3D building models, and assigning thermal values as boundary conditions to generate expected energy models and integrating those with proposed 3D spatio-thermal models. Also, more experiments need to be conducted on reconstructing closed-models including ceilings and floors, and analyzing the energy performance variations over time in interior and exterior built environments. The accuracy and completeness of generating these models still needs to be benchmarked against laser scanning based solutions. These areas are currently being explored as part of our ongoing research.

@&#REFERENCES@&#

