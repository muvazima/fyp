@&#MAIN-TITLE@&#Automatic scoring for answers to Arabic test questions

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           The research deals with responses written in Arabic.


                        
                        
                           
                           The research presents a new benchmark Arabic dataset that contains 610 answers.


                        
                        
                           
                           The system gets a model response from an already built database for specific curriculum.


                        
                        
                           
                           Responses are translated into English to overcome the lack of NLP resources in Arabic.


                        
                        
                           
                           Different methods of scaling the similarity values to be in the same range as the manual scores are presented.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Short answer scoring

Text similarity

Semantic similarity

Arabic corpus

@&#ABSTRACT@&#


               
               
                  Most research in the automatic assessment of free text answers written by students address English language. This paper handles the assessment task in Arabic language. This research focuses on applying multiple similarity measures separately and in combination. Many aspects are introduced that depend on translation to overcome the lack of text processing resources in Arabic, such as extracting model answers automatically from an already built database and applying K-means clustering to scale the obtained similarity values. Additionally, this research presents the first benchmark Arabic data set that contains 610 students’ short answers together with their English translations.
               
            

@&#INTRODUCTION@&#

The rapidly growing educational community, both electronic and traditional, with an enormous number of tests has caused a need for automatic scoring systems. Automatic Scoring (AS) systems address evaluating a student's answer by comparing it to model answer(s). AS technology handles different types of students’ responses, such as writing, speaking and mathematics. Writing assessment comes in two forms: Automatic Essay Scoring (AES) and Short-Answer Scoring. Speaking assessment includes low and high entropy spoken responses, while mathematical assessments include textual, numeric or graphical responses. AS Systems are easily implemented for certain types of questions, such as Multiple Choice, True–False, Matching and Fill-in-the-Blank. Implementing an automatic scoring system for questions that require free text answers is more difficult because students’ answers require complicated text understanding and analysis. In this research, short-answer scoring is handled through an approach that addresses students’ answers holistically and depends on text similarity measures (Mohler and Mihalcea, 2009; Mohler et al., 2011). Three types of text similarity measures are handled: String similarity, Corpus-based similarity and Knowledge-based similarity. String similarity measures operate on string sequences and character composition. Corpus-based depends on information derived from large corpora. Knowledge-based uses semantic networks (Mihalcea et al., 2006; Budanitsky and Hirst, 2001; Gomaa and Fahmy, 2013).

This research presents a system for short-answer scoring in the Arabic language. Arabic is a widespread language that is spoken by approximately 300 million people around the world. From a natural language point of view, the Arabic language is characterized by high ambiguity, rich morphology, complex morpho-syntactic agreement rules and a large number of irregular forms (Habash, 2010). Our system focuses mainly on measuring the similarity between the student and the model answers using a bag of words (BOW) model and disregarding complex Arabic computational linguistics tasks.

The system translates students’ responses into English to overcome the lack of text processing resources in the Arabic language. Acknowledging that machine translation is sub-optimal, but it is still helpful for the scoring task as experiments will explain in the next sections. Different methods of scaling the similarity values to be in the same range as the manual scores are presented and tested. Multiple text similarity measures were combined using supervised and unsupervised methods; this combination affected the obtained results positively.

Additionally, the system presents a module that searches for a model answer from an already built database that is aligned with the curriculum.

This paper is organized as follows: Section 2 presents related work on automatic short-answer scoring systems. Section 3 introduces the three main categories of Similarity Algorithms used in this research. Section 4 presents the first Arabic data set to be used for benchmarking short-answer scoring systems. In Section 5, the proposed system is illustrated with a walk-through example. Section 6 shows the experiment results, and finally, Section 7 presents the conclusions of the research.

@&#RELATED WORK@&#

A substantial amount of work has recently been performed in short-answer grading at the SemEval-2013 task #7: The Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge (Dzikovska et al., 2013). This task offered three problems: a 5-way task, with 5 different answer judgments, and 3-way and 2-way tasks, which conflate more judgment categories each time. Two different corpora, Beetle and SciEntsBank, were labeled with the 5 following labels: Correct, Partially correct incomplete, Contradictory, Irrelevant and Non Domain, as described in Dzikovska et al. (2012). A system called ETS (Heilman and Madnani, 2013) was presented through a short-answer grading approach that uses stacking (Wolpert, 1992) and domain adaptation (Daumé and Marcu, 2007) to support the integration of various types of task specific and general features. The full system included many features, such as baseline, intercept, Word-based N-gram, character-based N gram and text similarity features. Evaluation results indicate that the system achieves relatively high levels of agreement with human scores, compared to other systems that were submitted to the shared task.

The SOFTCARDINALITY (Jimenez et al., 2013) system utilized text overlap based on soft cardinality (Jimenez et al., 2010) plus a machine learning classifier. Soft cardinality is a general model for object comparison that has been tested on text applications. The system performed well, especially with “unseen domain” instances, which was the more challenging test set. Additionally, it obtained 1st place in a 2-way task and 2nd place in the 3-way and 5-way tasks, considering the overall accuracy across all of the data sets and test sets. The CNGL (Biçici and van Genabith, 2013) system was based on referential translation machines (RTMs), a computational model for identifying translation acts between any two data sets with respect to a reference corpus selected on the same domain, which can be used for automatically grading student answers. RTMs provide a clean and intuitive computational model for automatically grading student answers by measuring the acts of translation that are involved, and it was found to be the 2nd best system on some tasks in the Student Response Analysis challenge. EHU-ALM (Aldabe et al., 2013) is a 5-way supervised system that is based on syntactic-semantic similarity features. The model deploys the following: Text overlap measures, WordNet-based lexical similarities, graph-based similarities, corpus based similarities, syntactic structure overlap and predicate argument overlap measures. The results showed that the system is above the median and mean on all of the evaluation scenarios of the task. The UKP-BIU (Torsten Zesch et al., 2013) system was based on training a supervised model (Naive Bayes) using Weka (Hall et al., 2009), with feature extraction based on clearTK (Ogren et al., 2008). The features used were BOW, syntactic, basic similarity, semantic similarity, spelling and entailment features. The UKP-BIU results summarized that the Correct category was classified quite reliably but the Irrelevant category was especially hard. The LIMSIILES (Gleize and Grau, 2013) system was modeled as a paraphrase identification problem, based on substitution by basic English variants. Basic English paraphrases were acquired from the Simple English Wiktionary. Substitutions are applied on both the model and student answers to reduce the diversity of their vocabulary and map them to a common vocabulary. The evaluation showed promising results, and this work is a first step toward an open domain system that would be able to exhibit deep text understanding capabilities.

In addition to the systems described in SemEval-2013 task #7, an excellent and more detailed overview of related work can be found in Ziai et al. (2012), such as CarmelTC, C-Rater, Intelligent Assessment Technologies (IAT), Oxford-UCLES and Texas systems. CarmelTC (Rosé et al., 2003) is a Virtual Learning Environment system that has been developed at the University of Pittsburgh. It has been used in the tutorial dialog system Why2-Atlas (VanLehn et al., 2002). This system has the ability to assign scores for students’ answers and detect which set of correct features are present in the student essays. The system combines machine learning classification methods using the features extracted from both Carmel's linguistic analysis of the text and the Rainbow Naive Bayes classification. The student's answer is first broken into a set of sentences that are passed to a Bayesian network to extract the correct features that represent each sentence. These features are used to generate a vector that indicates the presence or absence of each correct feature. Finally, the ID3 tree learning algorithm is applied to feature vectors to create the rules for identifying the sentence classes. The system was tested with 126 physics essays, and the results were 90% precision, 80% recall and an 8% false alarm rate.

C-Rater was released by the Educational Testing System (ETS) (Leacock and Chodorow, 2003). It used gold-standard model patterns to score student answers according to their syntactical structure. These patterns are built semi-automatically by converting each answer into a set of one or more predicate-argument tuples. C-Rater reported having an accuracy of between 81% and 90% when used by The National Assessment of Education Progress agency. Modern work on C-Rater (Sukkarieh and Blackmore, 2009; Sukkarieh and Stoyanchev, 2009) treats the grading task more similar to a textual entailment task. It analyzed 100–150 graded student answers to create a set of concepts for which each is represented by a set of sentences supplemented by a lexicon. Scoring is based on the presence or absence of these concepts. For more development of C-Rater, the student answers are parsed, to extract a predicate argument structure that is then categorized as absent, present, or negated for each concept, using a maximum entropy-based matching algorithm. The reported agreement (per concept-math) was 84.8% compared to an annotator agreement of 90.3%.

Intelligent Assessment Technologies (IAT) is a scoring system that is presented in Mitchell et al. (2002). This system depends on information extraction templates that are manually created by a special-purpose authoring tool that explores a sample of student's responses. A student's answer is then compared to the templates that correspond to a question. This system was applied to a progress test that had to be taken by medical students in which 800 students went through 270 test items. According to the authors, their system reached 99.4% accuracy on the full data set after the manual adjustment of the templates via the moderation process. They reported an error of “between 5 and 5.5%” in inter-grader agreement.

Oxford-UCLES (Pulman and Sukkarieh, 2005) is an information-extraction short-answer scoring system developed at Oxford University to fulfill the needs of the University of Cambridge Local Examination Syndicate (UCLES). This system utilizes pattern matching for scoring. A human expert discovers information extraction patterns so that each set of patterns is associated with a corresponding question. This set is then split into a group of equivalence classes for which the members of the same equivalence class deliver the same message and/or information. The scoring algorithm matches the student answers to the equivalence classes and assigns scores according to the number of matches. The evaluation of the latest version of the Oxford-UCLES system was conducted by using approximately 260 answers for each of the nine questions that were taken from a UCLES GCSE biology exam. The full mark for these questions ranged from 1 to 4. Two hundred marked answers were used as the training set to extract the patterns, while 60 unmarked answers were kept for the testing phase. The average percentage agreement between the system's grade and the human expert's grade was 84%.

A Text-to-Text (Texas) system was introduced in Mohler and Mihalcea (2009). Here, the score is assigned according to a measure of the semantic similarity between a student answer and a model answer when using several measures, including knowledge-based and corpus-based. The system was applied to a computer science data set that contains 21 questions and 610 student responses, where the best Pearson correlation value between the automatic and manual scores was r
                     =0.47. An enhanced version of the Texas system was introduced in Mohler et al. (2011), which used dependency graph alignments that were generated by machine learning. The data set used in this version contained 80 questions and 2273 responses. The best Pearson correlation value was 0.518, and the best Root Mean Square Error (RMSE) was 0.978. The Texas system is the most closely related research to our work.

There are few published research studies in the area of automatic short-answer grading for the Arabic language. One of them, which was presented in Khalid and Izzat (2009), was theoretical research that did not use a data set and did not have experimental results. It presented a model that assigned weights for selected keywords from students and a model answer and used a stemming process and word synonyms that were manually predefined.

Another research study applied the Soundex phonetic algorithm for an Arabic “Complete” type question to facilitate an automatic intelligent marking method (Howida, 2011). It presented a model that required one word answer provided that the student can respond with the correct spelling or correct sounding word.

This section presents a review of the commonly used text similarity measures, shown in Fig. 1
                     , and their combination methods, as reported in recent publications. The proposed combination method is presented in Section 6.

String similarity measures operate on string sequences and character composition to judge the similarity between two text strings. There are several string metrics, such as the Hamming distance (Hamming, 1950), Levenshtien distance (Levenshtein, 1966), Damerau-Levenshtein (DL) distance (Hall and Dowling, 1980; Peterson, 1980), Needleman-Wunsch distance (Needleman and Wunsch, 1970), Smith-Waterman distance (Smith and Waterman, 1981), Jaro Winkler distance (Winkler, 1990), Dice's coefficient (Dice, 1945), Jaccard similarity (Jaccard, 1901), Longest common substring (Gusfield, 1997) and N-gram similarity (Barrón-Cedeno et al., 2010).

This research handles three types of String-based text similarity: Damerau-Levenshtein (DL) distance, Longest Common Substring (LCS), and N-gram Similarity. The DL distance determines the distance between two strings S1 and S2 according to the minimum number of operations that are required to transform one string into the other. Operations can be either insertion, deletion, or substitution of a single character, or the transposition of two adjacent characters (Hall and Dowling, 1980; Peterson, 1980). To compute the DL similarity value (DLSim), the DL distance is normalized through the following equation:
                           
                              (1)
                              
                                 
                                    D
                                    L
                                    S
                                    i
                                    m
                                    (
                                    
                                       S
                                       1
                                    
                                    ,
                                    
                                       S
                                       2
                                    
                                    )
                                    =
                                    
                                       
                                          M
                                          a
                                          x
                                          L
                                          e
                                          n
                                          g
                                          t
                                          h
                                          −
                                          D
                                          L
                                          D
                                          i
                                          s
                                          t
                                          a
                                          n
                                          c
                                          e
                                       
                                       
                                          M
                                          a
                                          x
                                          L
                                          e
                                          n
                                          g
                                          t
                                          h
                                       
                                    
                                 
                              
                           
                        where MaxLength represents the maximum length of the given two strings and DLDistance is the obtained DL distance between these two strings.

The LCS approach considers that the similarity between the two strings is based on the length of the longest contiguous chain of characters that exists in both strings. The main drawback of this approach is that the position of an error affects the computed similarity. For a word such as “PINEAPPLE”, typing ‘PINESPPLE’ gives a longest common substring of length 4, whereas “OINEAPPLE” gives a value of 8 (Gusfield, 1997). To compute the LCS Similarity value, a normalization task was performed by dividing the obtained LCS length by the average length of the student and model answers.

N-gram similarity is used for comparing two given strings by sliding a window of length N over the characters or words of a string to create a number of ‘N’ length grams that match. A match is then rated as the number of N-gram matches within the second string over the total number of possible N-grams. The reason behind using N-grams as a foundation for approximate string processing is that when two strings S1 and S2 are within a small edit distance of each other, they share a large number of N-grams in common. To estimate the similarity of the strings, Dice's similarity coefficient is used and calculated according to the following formula:
                           
                              (2)
                              
                                 
                                    N
                                    g
                                    r
                                    a
                                    m
                                    S
                                    i
                                    m
                                    (
                                    
                                       S
                                       1
                                    
                                    ,
                                    
                                       S
                                       2
                                    
                                    )
                                    =
                                    
                                       
                                          2
                                          C
                                       
                                       
                                          A
                                          +
                                          B
                                       
                                    
                                 
                              
                           
                        where NgramSim is the similarity value, A and B are the respective numbers of unique N-grams in S1 and S2, and C is the total number of unique N-grams that are common for both strings being compared.

As a preprocessing step for all of the three string similarity measures, non-words such as Numbers and punctuation have been removed, and all white spaces, such as tabs, newlines and spaces, have been trimmed to a single space character. Another optional preprocessing step is the Stop Words Removing (SWR) task, which filters out common words that do not have as much meaning prior to measuring the similarity. In our system, the stop words are removed according to a predefined list that has 429 words in Arabic and 659 words in English.

Corpus-based similarity detects the similarity between words according to the information gained from a large corpora. There are many corpus-based similarity techniques, such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Pointwise Mutual Information-Information Retrieval (PMI-IR) (Turney, 2001), and finally, Extracting Distributionally similar words using CO-occurrences (DISCO http://www.linguatools.de/disco/disco_en.html) (Kolb, 2008, 2009). DISCO supports nine languages, which are Arabic, Czech, Dutch, English, French, German, Italian, Russian and Spanish. It computes the distributional similarity between words while considering that the words that are similar in meaning occur in similar context. Similarity is based on the statistical analysis of very large text collections. To determine the similarity between two words, the Lin measure (Lin, 1998b) is applied to words that were retrieved from vectors of the indexed data. The two main similarity measures in DISCO are DISCO1 and DISCO2:
                           
                              -
                              DISCO1: Computes the first order similarity between two input words based on their collocation sets.

DISCO2: Computes the second order similarity between two input words based on their sets of distributionally similar words.

Our research will handle the corpus based approach via DISCO using DISCO1 and DISCO2 similarity measures. Table 1
                         shows features of Arabic and English DISCO data packets that were used in this research.

Knowledge-based similarity is a semantic similarity measure that determines the degree of similarity between words using information derived from semantic networks (Mihalcea et al., 2006). WordNet (Miller, 1995) is the most popular semantic network in the area of measuring Knowledge-Based similarity between words. Although there exists an Arabic Version of WordNet (Black et al., 2006) this research depends on English WordNet. Arabic WordNet (Rodríguez et al., 2008) is still suffering from weak word coverage which was clear in our research; as about 2000 words from a total of 9000 words used in the data set were missing from Arabic WordNet with a rate of more than three main words missing in each answer. Arabic version of WordNet needs a lot of work to be as reliable as English version; this can be clear when comparing the two versions in terms of number of words, synsets and relations as shown in Table 2
                        .

We decided to translate our data set to English to benefit from all of the advantages of English WordNet. The data set was both human and machine translated. Machine translation was conducted by the two online translators, Google and Bing. Translation from any language to English helps when targeting a language that does not have as many resources as English.

WordNet::Similarity http://wn-similarity.sourceforge.net/ is a Perl module that implements measures of similarity and relatedness that are all in some way based on the structure and content of English WordNet (Pedersen et al., 2004). Semantic similarity is a type of relatedness between two words. It covers a broader range of relationships between concepts, such as is-a-type-of, is-a-specific-example-of, is-a-part-of, and is-the-opposite-of (Patwardhan et al., 2003). WordNet::Similarity implements six measures of semantic similarity. Three of these measures are based on Information content: Res (Resnik, 1995), Lin (Lin, 1998a) and Jcn (Jiang and Conrath, 1997). The other three measures are based on the path length: Lch (Leacock and Chodorow, 1998), Wup (Wu and Palmer, 1994) and Path. WordNet also implements three measures of semantic relatedness: Hso (Hirst and St-Onge, 1998), Lesk (Banerjee and Pedersen, 2002) and Vector (Patwardhan, 2003).

Hybrid similarity measures were covered in many research studies where multiple similarity measures were used. In Mihalcea et al. (2006) eight semantic similarity measures were evaluated, both individually and combined; the best performance was achieved with combined similarity measures. A method for measuring the semantic similarity between the sentences or very short texts, based on semantic and word order information, was presented in Li et al. (2006). The authors of Islam and Inkpen (2008) presented a method called Semantic Text Similarity, which determines similarity between two texts according to semantic and syntactic information. The approach presented in Aggarwal et al. (2012) combined a corpus-based semantic relatedness measure with knowledge-based semantic similarity scores, which showed a significant improvement in calculating the semantic similarity between sentences. Promising correlation results were achieved in Buscaldi et al. (2012), where two modules were combined. The first calculates the similarity between the sentences using N-gram based similarity, and the second calculates the similarity between the concepts in the two sentences, using a concept similarity measure and WordNet. UKP was the system introduced in Bär et al. (2012); it used a simple log-linear regression model based on training data to combine multiple text similarity measures. A substantial amount of work has recently been performed on applying the idea of combining multiple similarity measures at the *SEM 2013 shared task on Semantic Textual Similarity (STS). The DeepPurple (Malandrakis et al., 2013) system was implemented by estimating the semantic similarity between two sentences, using regression models with features of n-gram hit rates, lexical semantic similarity between non-matching words, string similarity metrics, effective content similarity and sentence length. UNITOR-CORE TYPED (Croce et al., 2013) combined text similarity and semantic filters through support vector regression. The basic similarity functions were lexical overlap, compositional distributional semantics and convolution kernel-based similarity. A similar system called ECNUCS (Zhu and Man, 2013) used a support vector regression model to measure sentence semantic similarity by integrating multiple measurements, i.e., string similarity, knowledge-based similarity, corpus-based similarity and number similarity.

All of the measures described previously in Sections 3.2 and 3.3 work on a word-to-word similarity basis. This section presents the computation of semantic similarity at the sentence level. Our system relies on the BOW model; in this model, a sentence is represented as an unordered collection of words, disregarding the grammar and even the word order. To compute a similarity score between the student and model answers, the similarity scores for each word pair must be collected in a structure, to allow an overall score to be computed for the pairs. Because every word in a student answer is compared with every word in the model answer, the solution is to create a similarity matrix of size N*M, where N is the number of words in the model answer and M is the number of words in the student answer. In the matrix, each row represents a word in the model answer, while each column represents a word in the student answer.

After constructing the similarity matrix, the similarity between the student answer (SA) and the model answer (MA) is therefore determined according to the following bi-directional scoring equation, which was proposed in Mihalcea et al. (2006):
                           
                              (3)
                              
                                 
                                    s
                                    i
                                    m
                                    (
                                    M
                                    A
                                    ,
                                    S
                                    A
                                    )
                                    =
                                    
                                       1
                                       2
                                    
                                    
                                       
                                          
                                             
                                                
                                                   ∑
                                                   
                                                      W
                                                      ∈
                                                      {
                                                      M
                                                      A
                                                      }
                                                   
                                                
                                                
                                                   (
                                                   S
                                                   i
                                                   m
                                                   (
                                                   w
                                                   ,
                                                   S
                                                   A
                                                   )
                                                   *
                                                   f
                                                   (
                                                   w
                                                   )
                                                   )
                                                
                                             
                                             
                                                
                                                   ∑
                                                   
                                                      W
                                                      ∈
                                                      {
                                                      M
                                                      A
                                                      }
                                                   
                                                
                                                
                                                   f
                                                   (
                                                   w
                                                   )
                                                
                                             
                                          
                                          +
                                          
                                             
                                                
                                                   ∑
                                                   
                                                      W
                                                      ∈
                                                      {
                                                      S
                                                      A
                                                      }
                                                   
                                                
                                                
                                                   (
                                                   S
                                                   i
                                                   m
                                                   (
                                                   w
                                                   ,
                                                   M
                                                   A
                                                   )
                                                   *
                                                   f
                                                   (
                                                   w
                                                   )
                                                   )
                                                
                                             
                                             
                                                
                                                   ∑
                                                   
                                                      W
                                                      ∈
                                                      {
                                                      S
                                                      A
                                                      }
                                                   
                                                
                                                
                                                   f
                                                   (
                                                   w
                                                   )
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where f(w) is the relative frequency of a word in a model answer or a student answer. Sim(w, SA) is calculated either by Max similarity (MaxSim) or Average similarity (AvgSim). MaxSim is the highest similarity value between a given word w and all of the words in the student answer. AvgSim is calculated by dividing the sum of the similarity values of a given word w by the number of words in the student answer. The same goes for Sim(w, MA).

The obtained similarity score sim(MA, SA) has a value between 0 and 1; a score of 1 indicates identical answers, and 0 indicates no semantic similarity. Once the similarity score is computed, the score is normalized considering the length of the two answers. An illustrative example of these calculations will be presented in Section 5.2.

This research presents the first Arabic data set that can be used as a benchmark for automatic Arabic short-answer grading. Questions presented in the data set cover one chapter of the official Egyptian curriculum for Environmental Science (ES) course, which represents 25% of the overall curriculum. The data set contains 61 questions, 10 answers for each, with a total number of 610 answers. The average length of a student's answer is 2.2 sentences, 20 words or 103 characters. The data set contains a collection of students’ answers and grades, which were scored by two annotators who gave marks with values between 0 and 5 and obtained a Pearson correlation coefficient (r) and Root Mean Square Error (RMSE) of 0.86 and 0.69, respectively. The data set supports 4 types of short-answer questions: “Define the scientific term”, “Explain”, “What are the consequences of” and “Why”. Table 3
                      shows the r and RMSE values between the scores of the two annotators for each question type along with the number of students’ answers. All of the questions, model answers and students’ answers were both human translated and machine translated (via Google and Bing translation) to support both the Arabic and English languages. To evaluate the accuracy of the machine translators, they were compared to human translations using the BiLingual Evaluation Understudy (BLEU) method (Papineni et al., 2002). Considering human translation as a reference, the BLEU scores of the Google and Bing translators were 0.31 and 0.29, respectively.

The data set is available in three formats: Microsoft Access Database, MySQL Database, and XML. Fig. 2
                      shows the distribution of the students’ marks. Tables 4A and 4B
                     
                      represent sample questions, model answers, and short answers provided by two students, and grades assigned by two human judges.

The presented system aims to measure the similarity between the student's answer and the model answer to produce the final automatic score for the student response. Fig. 3
                         shows the steps of the system. The specialist manually converts the curriculum into a list of questions together with their model answers provided that the questions cover all parts of the curriculum. This step increases the system scalability and enables the examiners to freely enter their conceived questions without restriction to the questions that were previously saved in the data set. The system automatically selects the best match for the examiner-suggested question among the saved questions; More details about the selection methodology will be explained in Section 6.4.

Two specialist evaluators manually score the student answers while considering the model answers; then, the average of their scores represents the manual score.

The automatic score is then calculated in two steps. First, the similarity between the student and model answers is measured using the text similarity measures described above. Second, the obtained similarity values [0–1] are scaled onto the original scale [0–5] for ease of comparison. The scaling step is essential and is highly related to the system accuracy evaluation. The system accuracy is evaluated by comparing the manual and automatic scores while considering two factors, which are the association and the error size. The association is measured by the Pearson correlation coefficient (r) to indicate the correctness of the score ranking. The error size is measured by the Root Mean Square Error (RMSE) to characterize the precision of automatic score prediction. To evaluate the system output, especially using RMSE, it is necessary for the obtained similarity values and the annotators’ marks to be on the same [0–5] scale.

In the proposed system, four scaling methods are introduced and were named according to our own conception. The first method is called “SimpleScale” and is performed by multiplying the similarity values by 5 and rounding the scaled values to the nearest score awarded by the system (0, 0.5, …, 5). The second method is “IsotonicScale”, which was previously introduced in Mohler et al. (2011). This method simply trains an isotonic regression model (Zadrozny and Elkan, 2002) on each type of system output. We use Weka (Hall et al., 2009) to perform this task, and 10-fold cross validation is used for all of the experiments; 9/10th of the average marks given by the two annotators on the ES data set were used to acquire the training data, and 1/10th was used for evaluation. The average performance over the 10 experiments is reported.

The third and fourth scaling methods introduce a new unsupervised scaling model that depends on clustering the similarity values using the simple and accurate one-dimensional k-means clustering (Wang and Song, 2011). The third method was named “Clust6” because we set the number of clusters to 6. Similarity values are updated according to the clustering results, to be an integer value of 0, 1, 2, 3, 4 and 5, to represent the Very Poor, Poor, Fair, Good, Very Good and Excellent student's grades, respectively. The fourth method was called “Clust11”, in which 11 clusters were determined. Each cluster refers to one score from all possible 11 scores obtained by the system (0, 0.5, …, 5).

This section states the steps that are performed to measure the similarity between the model answer and the student's answer no. 1, which was previously presented in Tables 4A and 4B.

The question is: “
                           
                        ”, which literally means: “Define Plankton.”

The model answer is: “
                           
                        ”, which literally means: “Small-size plant or animal organisms that are carried by waves without resistance due to their small bodies.”.

The student's answer no. 1 is: “
                           
                         
                        
                           
                        ”, which literally means “Tiny microscopic plant or animal organisms that live in the surface water of the seas and that are carried by waves without resistance.”

The system automatically detects the type of question by comparing the question's header “
                           
                        ” (which literally means “Define:”) to all of the saved patterns. If the saved patterns do not contain this header, then we measure the similarity between the question's header and each group of saved patterns to obtain the correct question type. After detecting the question's type, in this example, “Define the scientific term”, the similarity between the question's text and all of the saved questions’ texts that cover the detected type is measured, to select the correct question; then, the model answer is retrieved.

According to the above example “Define Plankton.”; three string-based similarity algorithms – DL, LCS and N-gram – were tested to measures the similarity between the model and student answers over both Arabic and English, while considering SWR to be an optional task. Table 5
                         introduces the obtained string-based similarity values that are normalized between 0 and 1 and that will be scaled later, using the scaling methods described previously in the system architecture section.

For both Corpus-based and Knowledge-based Measures, the similarity matrix previously described in Section 3.5 was constructed to represent the similarity between the model answer and the student answer. The walk-through example shows calculations that were applied to Table 6A, and the same process is applied to the remainder of the tables in this section. DISCO1 and DISCO2 Corpus-Based similarity measures were applied over the Arabic and English languages after applying the SWR task because there is no need to measure the semantic similarity between the Stop Words. Tables 6A–6C
                        
                        
                         represent the similarity matrix using DISCO2 Corpus-Based similarity applied to the original Arabic text, Human translated text and Google translated text, respectively.

Applying Eq. (3) to Table 6A using MaxSim, we obtain the following similarity score between the model answer and student answer:
                           
                              
                                 
                                    
                                       
                                          
                                             
                                                s
                                                i
                                                m
                                                (
                                                M
                                                A
                                                ,
                                                S
                                                A
                                                )
                                                =
                                                
                                                   1
                                                   2
                                                
                                                
                                                   
                                                      
                                                         
                                                            (
                                                            (
                                                            1
                                                            *
                                                            1
                                                            )
                                                            +
                                                            (
                                                            1
                                                            *
                                                            1
                                                            )
                                                            +
                                                            (
                                                            1
                                                            *
                                                            1
                                                            )
                                                            +
                                                            (
                                                            1
                                                            *
                                                            1
                                                            )
                                                            +
                                                            (
                                                            0.408
                                                            *
                                                            1
                                                            )
                                                            +
                                                            (
                                                            1
                                                            *
                                                            1
                                                            )
                                                            +
                                                            (
                                                            1
                                                            *
                                                            1
                                                            )
                                                            +
                                                            (
                                                            1
                                                            *
                                                            1
                                                            )
                                                            +
                                                            (
                                                            0.01
                                                            *
                                                            1
                                                            )
                                                            +
                                                            (
                                                            0.44
                                                            *
                                                            1
                                                            )
                                                            )
                                                         
                                                         
                                                            10
                                                         
                                                      
                                                      +
                                                      
                                                         
                                                            (
                                                            (
                                                            1
                                                            *
                                                            1
                                                            )
                                                            +
                                                            (
                                                            1
                                                            *
                                                            1
                                                            )
                                                            +
                                                            (
                                                            0.24
                                                            *
                                                            1
                                                            )
                                                            +
                                                            (
                                                            1
                                                            *
                                                            1
                                                            )
                                                            +
                                                            (
                                                            1
                                                            *
                                                            1
                                                            )
                                                            +
                                                            (
                                                            0.3
                                                            *
                                                            1
                                                            )
                                                            +
                                                            (
                                                            0.408
                                                            *
                                                            1
                                                            )
                                                            +
                                                            (
                                                            0.334
                                                            *
                                                            1
                                                            )
                                                            +
                                                            (
                                                            0.092
                                                            *
                                                            1
                                                            )
                                                            +
                                                            (
                                                            1
                                                            *
                                                            1
                                                            )
                                                            +
                                                            (
                                                            1
                                                            *
                                                            1
                                                            )
                                                            +
                                                            (
                                                            1
                                                            *
                                                            1
                                                            )
                                                            )
                                                         
                                                         
                                                            12
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                s
                                                i
                                                m
                                                (
                                                M
                                                A
                                                ,
                                                S
                                                A
                                                )
                                                =
                                                
                                                   1
                                                   2
                                                
                                                
                                                   
                                                      
                                                         
                                                            7.858
                                                         
                                                         
                                                            10
                                                         
                                                      
                                                      +
                                                      
                                                         
                                                            8.374
                                                         
                                                         
                                                            12
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

The obtained score is then normalized while considering the length of the two answers, as follows: 0.741*(10/12)=0.618, which represents the final overall similarity for MaxSim.

Additionally, applying Eq. (3) to Table 6A using AvgSim, we obtain the following similarity score between the model answer and student answer:
                           
                              
                                 
                                    
                                       
                                          
                                             
                                                s
                                                i
                                                m
                                                (
                                                M
                                                A
                                                ,
                                                S
                                                A
                                                )
                                                =
                                                
                                                   1
                                                   2
                                                
                                                
                                                   
                                                      
                                                         
                                                            1.702
                                                         
                                                         
                                                            10
                                                         
                                                      
                                                      +
                                                      
                                                         
                                                            2.043
                                                         
                                                         
                                                            12
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                s
                                                i
                                                m
                                                (
                                                M
                                                A
                                                ,
                                                S
                                                A
                                                )
                                                =
                                                0.17
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

The obtained score is then normalized while considering the length of the two answers, as follows: 0.17*(10/12)=0.142, which represents the final overall similarity for AvgSim.

Each final overall similarity will be scaled according to the scaling methods described above, to represent the automatic student's mark.

For Knowledge-based measures, nine similarity or relatedness measures were applied after translating the student and model answers from Arabic to English. The SWR task was performed before constructing the similarity matrix. Normalization was performed by dividing the similarity value by the maximum possible similarity value for any given algorithm. Tables 7A–7C
                        
                        
                         represent the similarity matrices based on only one knowledge-based similarity algorithm, which is Vector, via the three different translations.

This research presents a system that automatically scores each student's answer (for 610 answers) with 536 different runs: 256 of the runs used String-Based Similarity, 64 used Corpus-Based Similarity, and the other 216 used Knowledge-Based Similarity measures. For each run, the Pearson Correlation Coefficient (r) and the Root Mean Square Error (RMSE) were computed.

This section presents an analysis and comparison of the results obtained with 4 perspectives:
                        
                           •
                           Experiment results for the original Arabic text.

Impact of the translation process.

The proposed hybrid approach.

Learning technology aspects.

For the string similarity measures, Tables 8 and 9
                        
                         represent the experimental results applied to the original Arabic text without SWR and with SWR, respectively. Each table represents the r and RMSE values using the previously explained four scaling methods.

The experiments handled four types of string-based similarity, DL, LCS, Character-based N-gram and Word-based N-gram. The character-based N-gram approach achieved better r and RMSE results than the other three types. In general, the character-based N-gram approach has many advantages, such as: simplicity; it is more reliable for noisy data such as misspellings and grammatical errors; and it outputs more N-grams in given strings than N-grams resulting from a Word-based approach, which leads to collecting a sufficient number of N-grams that are significant for measuring the similarity. According to our system, we can define two benefits of the character-based approach. The first benefit is ignoring the common-word order similarity due to applying BOW as shown in Eq. (3), considering that the word order could be more effective for other string-based similarities. The second benefit is disregarding stemming and other complex linguistic computational tasks. Other string-based similarity measures require at least the stemming task as it is rare to find three or four consecutive words with the same form in both the student's answer and the model answer.

For the scaling methods, the three used methods, IsotonicScale, Clust6 and Clust11, strongly enhanced the r and RMSE results when compared with the results of the simpleScale method. The two unsupervised methods, Clust6 and Clust11, obtained higher results than the supervised method, IsotonicScale, in most of the cases. An interesting point in our research is that we depend on the simple unsupervised K-means clustering algorithm to scale the similarity values without regard to the supervised learning task on the annotators’ marks. The SWR task was performed on both the student and model answers to remove words that were not useful and that would not affect the meaning. The results showed that the SWR task did not affect the results significantly for the string similarity measures because the results were in the same range regardless of whether the SWR was applied or not.

Additionally, the results showed that several measures appear to be better when evaluating with the r correlation measure while others appear to be better when measuring with the RMSE. The best value of the r correlation was 0.73, which resulted from the Bi-gram character-based approach using the Clust11 scaling method. Although this correlated value is very promising, by scanning the error rate, we found that the best RMSE 1.07 resulted from the Tri-gram character-based method, and it is not promising to rely on string-based similarity in a real scoring system.

Considering Corpus-based similarity measures applied to the original Arabic text, the experiment results emphasize the benefits of scaling similarity values using the regression and clustering methods. This finding was clear, especially when investigating the results of AvgSim method shown in Table 10
                        . For example, the r and RMSE values resulting from applying DISCO1 using SimpleScale were 0.39 and 1.42 respectively which were improved to 0.50 and 1.10 using CLust11. Using the MaxSim method clearly enhanced the correlation and error rate results in all of the cases in Corpus-Based measures. DISCO1 achieved the best correlation value, 0.63, using the Clust6 and Clust11 scaling methods, and DISCO2 resulted in the best and most promising RMSE value, which is 0.86. These results confirm again that some measures appear to be better when evaluating with the r correlation measure, while others appear to be better when measuring with the RMSE. Although correlation values that result from string-based measures are higher than the values that result from corpus-based measures, the RMSE values for the corpus-based measures were much less than the string-based measure values. This finding confirms the important role of corpus-based measures as a type of semantic similarity approach.

For the String-Based Similarity, the correlation and error rate results are almost the same when translated text is used instead of the original Arabic text because String-Based similarity measures have the same behavior with different languages, which is clear from the very similar values listed in Tables 11–16
                        
                        
                        
                        
                        
                        . Although the BLEU values of the Google and Bing translators mentioned in Section 4 confirmed that the machine translation task was sub-optimal;

The scoring results of Google and Bing translated text were very close to a human translated text. Two main factors support the idea of using machine translation instead of human translation. First, most of the translation errors are related to grammar, which is not involved in our system and which adopts the BOW model. Second is unifying the translation source for both the student and model answers, which will result in the same translation errors for both of the answers.

For Corpus-Based Similarity, the human or machine translation from Arabic to English greatly affected the results that used the Corpus-Based similarity measures. The main reason for the enhancement of the correlation and error rate values is the impact of the corpus sizes, the number of tokens and the number of queriable words, as shown in Table 1. The best correlation r value was increased from 0.63 to 0.70; also, the best RMSE value was decreased from 0.88 to 0.82, as shown in Table 17. These two values resulted from applying the MaxSim and Clust11 scaling method to human-translated text. Again, Tables 17–19
                        
                        
                         emphasize the importance of using MaxSim and applying supervised and unsupervised scaling methods, such as IsotonicScale, Clust6, and Clust11. In other words, the results showed that the worst results were from using the AvgSim and SimpleScale methods. Additionally, the results proved that machine translation can be relied upon for building a fully automatic application. The best correlation and error rate values that result from the Google and Bing translators (r
                        =0.68, RMSE=0.84, resulting from the Google translators) (r
                        =0.66, RMSE=0.86, resulting from the Bing translators) are very similar to values that result from human translators.

For Knowledge-Based Similarity, knowledge-based measures were applied over the translated text to benefit the features of the English WordNet, as explained in Table 2. Tables 20–22
                        
                        
                         show the experiment results based on Knowledge-Based similarity; the best correlation value over all of the Knowledge-Based measures was 0.64, which resulted from using human translation via the Vector algorithm and Clust6 scaling method, and the best RMSE value in all was 0.85, which resulted from using human translation via the Lesk algorithm and the Clust6 scaling method. Using the Max Overall Similarity method clearly enhanced the correlation and error rate results in all of the cases in Knowledge-Based measures. Knowledge-Based results indicate that we can depend on the machine translation because the difference between the best RMSE values in human and automatic translation is 0.89 minus 0.85, which equals 0.04. This value is very small if it is compared to the benefits of automatic translation in building a fully automatic scoring application.

As previously mentioned in Section 3.4, many researchers agreed to the idea of mixing the results of the different measures to enhance the overall similarity. We performed this task in a supervised way by learning the obtained student marks through three models using Weka (Hall et al., 2009). These models are Simple Linear Regression, Linear Regression and SMOreg (Shevade et al., 2000). The Simple Linear Regression model performs regression based on a single attribute, choosing the attribute that yields the lowest squared error, while the Linear Regression Model is based on multiple attributes.

SMOreg is a sequential minimal optimization algorithm for training a support vector regression. This implementation globally replaces all of the missing values and transforms nominal attributes into binary attributes; it also normalizes all of the attributes by default (Shevade et al., 2000).

To perform the training and testing tasks, 10-fold cross-validation is used for all of the experiments. We have submitted two methods, called CombineALL and CombineBest, for a hybrid task. The CombineALL method is performed by training on all of the obtained marks from all of the 536 runs that were tested separately. In the CombineBest method, we choose the best measures that outputted best correlation and error rate results. For the String-based method, the three character-based N-gram measures, Bi-gram, Tri-gram and Quad-gram, applied to the original Arabic text using the three scaling methods IsotonicScale, Clust6 and Clust11, are selected. Additionally, both of the methods of applying SWR (or not) were selected. These selections output 18 features of String-based similarity measures.

For Corpus-based similarity measures, DISCO1 and DISCO2 were selected while Lesk and Vector were selected to represent the Knowledge-based measures. Disregarding the runs using SimpleScale and AvgSim methods due to their unsatisfactory results, the remaining runs represent respectively 24 and 18 features of Corpus-based and Knowledge-based similarity measures. To conclude, the idea of using CombineBest to obtain 60 features from the 536 was based on selecting according to the correlation and error rate results. Table 23
                         represents the experiment results of the proposed hybrid models. These results indicate the benefit of combining multiple similarity measures by enhancing both the correlation and the error rate values. The best r value, 0.83, resulted from both Linear Regression and SMOreg models applied to the CombineBest method. The best RMSE value, 0.75, resulted from the SMOreg model applied to the CombineBest method. These resulting values are very close to the values that were scored manually by the two annotators, which were r
                        =0.86 and RMSE=0.69.

These values indicate that the presented system walks along the same path as manual evaluation. Fig. 4
                         shows a noticeable similarity between the manual and automatic scoring after applying the hybrid approach, which is clear from the distribution between the number of students and the students’ marks. Finally, Table 24
                         shows the r and RMSE values between the system scores resulting from applying the hybrid approach and the average scores of the two annotators for each question type. These values show that the presented methodologies can handle different question types and especially “Define the scientific term” which got the least RMSE 0.71.

The main goal of this research is to enhance an e-learning environment. Many e-learning technology factors were considered in this research, such as language independence, ease of use, scalability and system re-usability. The proposed system comes in a simple and easy-to-use user interface that does not require a specialist or training. Additionally, it can be easily integrated and re-used as a sub-program in any e-learning application. The system is highly dynamic and can be used in a simple manner with any curriculum and with any form or type of short-answer system. Additionally, the system is suitable for any language that suffers from a lack of resources, and it has no restrictions over the curriculum language provided that a suitable corpus and machine translation are used. The examiners are free to enter their conceived questions in an unrestricted manner to a fixed domain of questions that were previously saved in a data set. As previously mentioned, the examiner suggests any question within the domain, and the system automatically detects the best match question and its model answer according to the conceived question's type and text in two steps. First, the system detects the question type based on simple patterns in the question header that were saved in the data set. For example, if the examiner wants to ask about the reason “Why Question”, he/she can use many headers, such as 
                           
                         
                        
                           
                        , which are literally translated into “What are the causes of”, “How do you explain”, “State the reasons”, “What are the reasons”, “Deduct the reasons” and “Why”, respectively. Second, the question text is compared to all previously saved questions that follows the detected question type. A total of 60 questions (15 for each type) from previous Egyptian official exams in Environmental Science were compared with questions that were stored in the data set. The question forms of these questions are different from the saved questions in the data set. Using the proposed hybrid approach, all of the questions were correctly selected, with an accuracy of 100%. This result emphasizes the idea of building a bank of questions that covers every part of the curriculum, which allows the examiner to suggest any question.

@&#CONCLUSIONS AND FUTURE WORK@&#

This paper examines an important research area, which is short-answer scoring for non-Latin languages, especially Arabic.

Our research goes through five stages. The First stage was building a data set that supports the Arabic language due to the unavailability of Arabic data sets. While building the data set, three aspects were considered: the variety of question types, the assessment process and the scoring quality. The variety of short-answer question types provided by the specialist cover every part of the curriculum, which increases the system scalability and allows the examiner to suggest any question within the domain. The assessment process was performed by two specialists, to improve the scoring quality.

The Second stage was scaling the similarity values that were obtained from any similarity algorithm, to make them in the same range of the manual scores, which was accomplished through 4 different methods; SimpleScale, IsotonicScale, Clust6 and Clust11. The IsotonicScale, Clust6 and Clust11 methods have positively affected the correlation and error rate results. Using the simple unsupervised K-means algorithm to scale the similarity value is one of the important contributions of our system.

The third stage was applying the similarity algorithms to the original Arabic text; Three string-based similarity algorithms were tested: DL, LCS and N-gram, and the N-gram character-based was found to be the best. It achieved promising correlation but disappointing RMSE results. The two Corpus-Based Similarity algorithms, DISCO1 and DISCO2 were tested and both achieved promising r and RMSE results.

The Fourth stage was translating from Arabic to English. Two reasons were behind using translated texts: First, comparing the behavior of String-Based and Corpus-Based methods for different languages. Second, testing the Knowledge-Based measures using the English WordNet to benefit from its wide word coverage. Human translation and two machine translations (Google and Bing) were used. String-Based Similarity measures have the same behavior for different languages, while Corpus-Based Similarity measures for English obtained a higher correlation and a lower RMSE than for Arabic due to the role of the corpus size and other features, which are shown in Table 1. The experiments proved that machine translation can be relied upon for building a fully automatic application.

The Fifth and final stage was combining the measures from different categories, which raised the correlation results to be 0.83 and decreased the RMSE to be 0.75. These resulting values are very close to values that were scored manually by the two annotators, which were r
                     =0.86 and RMSE=0.69. A combination task is performed using Weka through examining three supervised models: Simple Linear Regression, Linear Regression and SMOreg models. The three models were applied with two methods: one method collects all of the measures on the runs, and the second method is applied to the best runs. The experiments emphasized the idea of mixing measures not only from different categories but also from different languages as well.

In conclusion, we presented a study whose results can enhance the learning of environment tasks, especially those that cover Arabic language. The first benchmark Arabic data set was introduced and contained 610 students’ short answers together with their English translations. The obtained results prove that the presented system performs well enough for deployment in a real scoring environment. The application is suitable for any language by providing a corpus in a certain language and a machine translation. The application ensures the benefits of mixing different similarity approaches and similarity value scaling methods. A final advantage of the presented system is its usability for the examiners and the course's authors because they are able to enter their conceived questions without restriction to the questions’ forms that were previously saved in the data set.

The future work will cover three main points. The first point is comparing the usage and training of generic and domain-specific corpora in corpus-based similarity measures. The second point is enriching the Arabic WordNet data to be suitable for knowledge-based measures. The final point is providing the student with useful feedback that contains comments on the students’ answers and an explanation for the assigned automatic score.

Supplementary material related to this article can be found, in the online version, at http://dx.doi.org/10.1016/j.csl.2013.10.005.


                     
                        
                           
                        
                     
                  

@&#REFERENCES@&#

