@&#MAIN-TITLE@&#A bioinformatics approach to 2D shape classification

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           An alternative interaction between Pattern Recognition and Bioinformatics is studied.


                        
                        
                           
                           2D shape classification is faced using biological sequence analysis approaches.


                        
                        
                           
                           Classification results are competitive with literature.


                        
                        
                           
                           Other bioinformatics tools are used for understanding and interpretation.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

2D shape classification

Bioinformatics

Sequence alignment

Visualization

@&#ABSTRACT@&#


               
               
                  In the past, the huge and profitable interaction between Pattern Recognition and biology/bioinformatics was mainly unidirectional, namely targeted at applying PR tools and ideas to analyse biological data. In this paper we investigate an alternative approach, which exploits bioinformatics solutions to solve PR problems: in particular, we address the 2D shape classification problem using classical biological sequence analysis approaches – for which a vast amount of tools and solutions have been developed and improved in more than 40 years of research. First, we highlight the similarities between 2D shapes and biological sequences, then we propose three methods to encode a shape as a biological sequence. Given the encoding, we can employ standard biological sequence analysis tools to derive a similarity, which can be exploited in a nearest neighbor framework. Classification results, obtained on 5 standard datasets, confirm the potentials of the proposed unconventional interaction between PR and bioinformatics. Moreover, we provide some evidences of how it is possible to exploit other bioinformatics concepts and tools to interpret data and results, confirming the flexibility of the proposed framework.
               
            

@&#INTRODUCTION@&#

Research in Computational Biology and Bioinformatics experienced an unprecedented growth in the last years, mainly due to the fruitful interaction with many disciplines and fields of computer science. Among others, Pattern Recognition/Machine Learning techniques have been successfully exploited in this context [1], for many different reasons: it is possible to “learn from examples”, derive quantitative models, handle non vectorial data, and deal with many classification, clustering and detection problems commonly encountered in life sciences. In many cases the particular Pattern Recognition model has not been applied “as is”, but has been adapted and modified to take into account biological constraints and needs. Sometimes, this produced approaches that are very different from original methodology – a clear example is the profile-HMMs [2].

To some extent, it can be stated that this tight interaction has been mainly unidirectional, with biology/life science gaining the largest benefit
                        1
                     
                     
                        1
                        In different cases bioinformatics issues have led to novel pattern recognition methodological challenges – the most famous example being the biclustering problem [3].
                     . In this paper, we explore an alternative direction, trying to answer the following question: can we reverse the typical direction of interaction between Pattern Recognition and Bioinformatics? Or, in other words, can we exploit advanced bioinformatics models and solutions to solve pattern recognition tasks?.

To the best of our knowledge, this perspective is rather new in the literature – the only relevant example is the video-genome project
                        2
                     
                     
                        2
                        See http://v-nome.org/about.html
                        
                      
                     [4] – and it seems a promising direction for two different reasons. First, if we are able to encode the Pattern Recognition problem in biological terms then we can exploit the huge range of effective, optimized, and interpretable bioinformatics tools developed by more than 40 years of research. These tools heavily rely on the solution of general pattern recognition tasks such as matching, classification, retrieval, clustering, distance computation and so on. For example, in the video-genome project [4], authors established an analogy between biological sequences and videos, defining the so called “video-DNA”, a way to map features extracted from video frames into nucleotidic biological sequences. Having encoded the problem in biological terms, authors were then able to address the video retrieval task by using the famous BLAST [5] – an extremely fast and effective heuristic-driven algorithm for biological sequence retrieval. Second, and more important, the main goal in bioinformatics research is to derive knowledge from biological data: therefore, the interpretability of methods and solutions is a key feature, and many visualization, inspection and interpretation tools are available in the literature. These tools may be very useful also in the Pattern recognition scenarios, to better understand the different aspects of the data for a given problem: actually, in recent years interpretability has become a stringent need in Pattern Recognition [6].

This paper makes another step in this direction, providing some further evidence on the effectiveness and interpretability of bioinformatics approaches for Pattern Recognition problems. In particular, in this paper, we propose and discuss a bioinformatics approach to 2D shape classification. Analysis of 2D shapes represents an important and vibrant research area (often paving the way for 3D object classification). Many approaches appeared in the literature (see for example the reviews [7,8]): very often, the 2D shape is encoded by the contour, which proved to be an effective and natural choice in many applications. Here we propose some methods to encode the shape contour as a biological sequence, employing tailored bioinformatics tools to perform classification. In the huge literature related to 2D shape analysis, many approaches exploit sequence alignments tools to perform shape matching ([9–13], just to cite a few) – some sequence matching-based approaches which start from shape-skeletons have also been proposed [14–16]. Focusing on our main target, i.e. to use biological sequence alignment tools, it should be noted that few approaches exist that employ techniques developed for biological sequences to perform shape classification or matching [17,18]. Nevertheless, these approaches propose a very different perspective with respect to our approach (and the video genome project), where the main goal is to encode the PR problem in biological terms, hence exploiting tools developed for biological sequence analysis. In other words, to exploit Bioinformatics tools for Pattern Recognition, one can consider two main steps: (i) encoding the PR problem in biological terms; (ii) applying bioinformatics tools to solve the problem. From this point of view, the approaches in [17,18] are rather poor, employing one particular technique for one particular purpose, and not considering a biological encoding which would allow the use of a wide class of algorithms for sequence analysis.

In this paper we do explicitly consider this aspect: first, we establish an analogy between 2D shapes and biological sequences, this motivating the employment of bioinformatics tools. Then we propose three ways for transforming a silhouette, encoded with the 8-directional chain code [19], into an aminoacidic sequence; given that, we can compute the similarity between shapes by using established biological sequence alignment tools. Such similarity is then exploited for classification in a K-nearest-neighbor setting. Finally, we show that other biological tools and concepts (such as multiple sequence alignment, conserved domains and locality and quality of alignment) can be used for a deeper analysis of the results. We performed different experiments with five standard shape datasets; on one hand, we show that classification results are very competitive with the state-of-the art. On the other hand, we show that poor results we obtained on a retrieval case can be analysed in a deeper way by exploiting other biological sequence mining tools.

@&#BACKGROUND@&#

This section briefly summarizes the bioinformatics tools exploited in our analysis. First, we present a preliminary overview of biological sequence alignment, so to clarify notations and terminology. Then, we present the tools employed for pairwise sequence alignment and multiple sequence alignment, trying to highlight specific aspects which are useful for our task.

Understanding and modelling the behavior of living cells is strongly dependent on the analysis of biological sequences, both nucleotide sequences – i.e. strings made with the 4 symbols of DNA, namely ATCG – and aminoacid sequences – i.e. strings with symbols coming from a 20 letter alphabet. The most important basic operation is sequence alignment, which is a crucial step in many computational biology and bioinformatics analyses. The alignment of a pair of sequences aims at finding the best registration between them. This is done by taking into account the biological nature of the input sequences, so that biological (usually evolutionary) events, such as mutations and rearrangements, are clearly expressed [20].

From a practical point of view, alignment is obtained by inserting spaces inside the sequences (the so called gaps) so to maximise the point-wise similarity between them – a graphical example can be seen in Fig. 1
                        . Such maximization relies on two important parameters. The first one is the so-called substitution matrix B(i, j) which indicates the penalty to be paid for a mismatch between symbols i and j. This encodes the fact that in nature substitutions between aminoacids/nucleotides are not all equally likely. Different alternatives exist (such as the PAM [21] and the BLOSUM [22] matrices), each one exploiting biological a priori knowledge such as chemical properties of aminoacids. The second parameter is called the gap penalty pair, which is a pair of numbers specifying the cost of inserting and the cost of extending a gap in one of the sequences (in biology, inserting a new gap has a different impact with respect to extending an existing one).

The simplest instance of sequence alignment aims at finding the best registration between two sequences. In this case the approaches can be divided into global and local: global methods try to find an alignment between the entire strings, whereas local approaches aim at finding short regions of high similarity. Historically, the most famous pairwise sequence alignment algorithms are the Needleman-Wunsch [23] (which operates globally) and the Smith-Waterman [24] (which is local); both methods rely on dynamic programming to solve the problem efficiently. In particular, they both have a time complexity of O(MN), with M and N being the lengths of the two sequences. We chose these two established tools, dating back to 70s/80s, in order to be as basic as possible; however large margins of improvements exist, since many advanced algorithms appeared in the last 30 years; one clear example is the popular BLAST (Basic Local Alignment Search Tool) [5], which implements a set of simple but effective heuristics to drastically reduce the time complexity of the alignment.

A by-product of the alignment process is the alignment similarity score: such quantity measures how “well aligned” the two sequences are. This score can be reasonably intended as a similarity measure between two sequences.

When the goal of sequence analysis is to infer evolutionary events from a set of sequences, rather than reasoning in terms of pairwise alignments, the best option is to simultaneously align all the sequences, performing the so called multiple sequence alignment (MSA - [25]). In this context, the most widely used approach employs a heuristic search known as progressive technique, which builds up the final alignment by combining pairwise alignments – starting from the most similar pair and progressing to the most unrelated. In this scenario, the most famous tool employed by researchers is ClustalW
                           3
                        
                        
                           3
                           
                              http://www.ebi.ac.uk/Tools/msa/clustalw2/
                           
                         
                        [26].

Given a multiple alignment, different information can be inferred. For our scope, we will exploit two aspects:

                           
                              1.
                              The quality of the multiple alignment, which can be used to understand the local reliability of the sequence alignment (i.e. where and how well the sequences are aligned). This can be assessed with publicly available tools such as the CORE web service
                                    4
                                 
                                 
                                    4
                                    
                                       http://www.igs.cnrs-mrs.fr/Tcoffee/tcoffee_cgi/index.cgi?stage1=1&daction=CORE::Regular
                                    
                                  
                                 [27].

The conserved domains, namely regions which are very conserved in the whole set of sequences: in biology, these regions are of crucial importance for the correct function of the molecules, since a mutation in such zone can have drastic effects. Different simple or advanced tools are available to discover such conserved domains, such as those implemented in the software ConFind
                                    5
                                 
                                 
                                    5
                                    
                                       http://www.colorado.edu/chemistry/RGHP/software/
                                    
                                  
                                 [28]: in this case conserved domains are defined in terms of the maximum entropy allowed per position, the number of exceptions to the maximum entropy allowed and the minimum region length.

In this section we present our approach: in particular, we first link 2D shapes and biological sequences, which may motivate the employment of bioinformatics tools in this context. Then we introduce the three methods used to encode shapes into biological sequences; finally, we detail how to transform alignments into a classification scheme.

In this part we will describe some characteristics of biological sequences which can be present also in 2D silhouettes; such aspects are explicitly or implicitly taken into account by many biological sequence analysis tools, and can motivate and support the usage of such tools for the analysis of 2D shapes.

The first and most obvious similarity concerns the observation that 2D shapes can be effectively represented as sequences, similar to what is present in the biological case. From a general point of view, proteins represent the basic elements of every living cell, each one responsible for a different function of the cell. Proteins are 3D structures, which are directly mapped (through a projection) to a 2D discrete string (the aminoacid sequence). In the same way, a 3D object can be projected to the contour of one of its corresponding 2D aspects. In both cases there is a loss of information: it is difficult to recover the 3D object from the contour, as well as in biology it is difficult to recover the 3D structure of the protein from the sequence. Nevertheless, such projections are very useful: in both domains, similarities between 3D structures can be inferred by looking at similarities in the sequence domain.

In the 2D shape domain a classical contour representation exists, which shares many similarities with the biological representation: the chain code scheme [19]. This approach describes each point of the contour with a symbol taken from a 4- or 8-symbols alphabet, similarly to the biological case which uses a 4- or 20-letter alphabet. While being simple, such representation is very descriptive, allowing to encode all possible contours of all possible 3D objects – as the biological sequence can describe all possible proteins. Moreover, such representation is also useful to discriminate between shapes, as it has been shown in different successful cases [8,29].

Another aspect concerns occlusions, which can represent a problem when analyzing 2D shapes: part of the object is not visible or, worse, is covered by another object. This results in a contour which may have missing parts or “replaced” parts. This situation has a clear counterpart in biological sequences, and corresponds to sequence insertions, deletions or mutations. Many sequence analysis tools have to deal with this problem. Actually, local alignment tools (such as the classic Smith-Waterman [24]) rank alignments not on the basis of the whole sequence, but only using a set of highly similar small segments, which can prevent small occlusions from becoming problematic.

It is also important to note that there is a crucial difference between the two domains: 2D shapes tend to be closed, whereas biological sequences are not concerned with periodicity. This aspect is closely related to the choice of the starting point, which often represents an issue in the contour-based 2D shape classification. In all our experiments, a good starting point was reasonably determined by a simple pre-alignment of the shapes.

A final comment: as already mentioned in the introduction, it is worth considering that the main goal in biology/bioinformatics is to recover information which can help the understanding of the complex behavior of living organisms: therefore, a great importance has been always given to the interpretability of methods and solutions – with many visualization, inspection and interpretation tools available in the literature. These tools may be very useful also in Pattern Recognition, where interpretability has become a stringent need in recent years [6]. This aspect has been largely investigated in this paper: by linking 2D shapes and biological sequences we can then leverage a wide variety of tools, developed to address the great amount of different problems faced by Bioinformatics tools, where matching is only one of the possible tasks. In fact, more than showing that a very competitive shape classification tool can be devised, we provide evidence on possible exploitations of other bioinformatics tools, not strictly devoted to matching, usable to perform general mining and knowledge extraction from 2D shapes. Exploiting concepts such as high similarity segments, conserved domains and quality of multiple sequence alignments, we were able to show the effectiveness of our approach, providing a better understanding of some of the results. Remarkably, all these analyses have been carried out by using classical web tools that bioinformatics experts are using every day (ClustalW, Tcoffee, ConFind).

In the following we describe some methods to encode 2D shapes into biological sequences, in particular exploiting the 20-symbols alphabet of aminoacids.


                        Encoding scheme 1: Single. We start by describing every 2D shape with the chain code representation: such codes are used to represent a boundary by a connected sequence of straight-line segments with a specified length and direction [19]. Typically, this representation is based on 4- or 8- connectivity of the segments, as shown in the left part of Fig. 2
                        : first we select an initial point, then we follow the contour in a clockwise order, and we select the code based on the direction taken towards the next pixel. Clearly, this descriptor is invariant to translation, but it is not invariant to rotations and scale changes
                           6
                        
                        
                           6
                           A well known extension of the chain code, called differential chain code, is invariant to rotation; however, a preliminary set of experiments revealed a degradation in performances. This may be related to the loss of information due to the use of derivative operation.
                        . Even if we are aware that many more complex descriptors are available (such as Multi-scale contour fragments [11,30]), this simple approach allows an easy encoding of a shape into a biological sequence; it also facilitates the exploitation of visualization tools. Remarkably, even with this simple scheme, we already achieve satisfactory results. Since Chain Codes are not invariant to rotation, the initial point should be carefully selected. In particular, in all our experiments, we determine such initial point by first pre-aligning the shapes using a Procrustes analysis, and then selecting the most upper left point of the contour.

The first strategy to encode 2D shapes into biological sequences is rather simple, in order to analyse the basic potential of our approach. In particular, each chain code value is directly mapped onto an aminoacid. More in detail, the chain codes are mapped onto the first 8 aminoacids, as given in the IUPAC coding
                           7
                        
                        
                           7
                           
                              http://www.iupac.org/publications/pac/1984/pdf/5605x0595.pdf
                           
                        : A, R, N, D, C, Q, E, and G. Even if this scheme is very simple, some good results have already been obtained – see our preliminary works [31,32].


                        Encoding scheme 2: Triplets. The second encoding scheme is based on two ideas, both aimed at exploiting the whole set of 20 aminoacids available in nature. Given the chain code representation, we consider the shape as composed by many short subsequences of length 3 – also called k-mers or N-grams in some papers. This idea of analysing short fragments of contiguous symbols – which is not new in the 2D shape classification scenario [29,30] – seems to be a reasonable choice also in biology: it has been recently shown that different information can be extracted, at the genomic level, from the analysis of the distribution of such words [33,34].

If we consider subsequences/fragments of length 3 (i.e. triplets), then we have 83 possible values (when using 8 directional chaincode), i.e. every sequence can be described using words coming from a dictionary of 83 entries; the main idea of the triplet encoding is to find a mapping between this dictionary and the aminoacidic alphabet; once given this mapping we can substitute every occurrence of a given fragment in a sequence with the corresponding aminoacid. To find this mapping, we adopt a way of reasoning similar to the one used by nature, where specialized molecules read a triplet of DNA (also called codon) and translate it into an aminoacid. This code, which starts from a low cardinality alphabet (A,T,C,G), is redundant: there are 
                           
                              
                                 4
                                 3
                              
                              =
                              64
                           
                         possible codons, which are mapped on 20 aminoacids by means of a lookup table. In our mapping we exploited directly this scheme of nature: we considered triplets of chain codes, and we found a mapping between each of them and a codon (i.e. a triplet of DNA characters). Since we used a 8-directional chain code, there will be more triplets mapped into a single codon.

More in detail, the mapping is performed in two steps:

                           
                              1.
                              
                                 Chain code triplets clustering: in this phase all the chain code triplets
                                    8
                                 
                                 
                                    8
                                    We discard from the clustering the triplets which are meaningless (such as those containing “40”, which indicates a “move to right” followed by a “move to left”) and those not present in the considered dataset – in average about 300 triplets were considered.
                                  are clustered into 61 groups, in order to allow, in the second step, to compute a one to one mapping with codons (we used 61 codons, not considering stop codons, which are particular triplets which are used in nature to depict the end of the sequence). The clustering is performed using an agglomerative hierarchical scheme, with the average link rule (as in the UPGMA algorithm, a widely used technique for phylogeny). The distance between two triplets of chain code 
                                    
                                       x
                                       =
                                       
                                          x
                                          1
                                       
                                       
                                          x
                                          2
                                       
                                       
                                          x
                                          3
                                       
                                    
                                  and 
                                    
                                       y
                                       =
                                       
                                          y
                                          1
                                       
                                       
                                          y
                                          2
                                       
                                       
                                          y
                                          3
                                       
                                    
                                  is defined as

                                    
                                       (1)
                                       
                                          
                                             d
                                             
                                                (
                                                x
                                                ,
                                                y
                                                )
                                             
                                             =
                                             
                                                ∑
                                                i
                                             
                                             min
                                             
                                                [
                                                mod
                                                
                                                   (
                                                   
                                                      x
                                                      i
                                                   
                                                   −
                                                   
                                                      y
                                                      i
                                                   
                                                   ,
                                                   
                                                   8
                                                   )
                                                
                                                ,
                                                mod
                                                
                                                   (
                                                   
                                                      y
                                                      i
                                                   
                                                   −
                                                   
                                                      x
                                                      i
                                                   
                                                   ,
                                                   
                                                   8
                                                   )
                                                
                                                ]
                                             
                                          
                                       
                                    
                                 We chose this distance as it allows to group together parts of contours with very small perturbations
                                    9
                                 
                                 
                                    9
                                    For example, a cluster is composed by 000, 001, 010, and 011.
                                 .


                                 Mapping codons to clusters of chain code triplets: this second phase aims at finding a direct correspondence between every cluster (group of chain code triplets) and every codon. In this case we adopt two approaches: the former starts from the observation that aminoacids (and therefore codons) do not appear in nature with the same frequency; since the same holds for chain code triplets in a given dataset, a reasonable choice is to map triplets to codons by aligning the frequencies (we employed the codon frequencies of the Homo sapiens
                                    10
                                 
                                 
                                    10
                                    Available at http://www.kazusa.or.jp/codon/cgi-bin/showcodon.cgi?species=9606.
                                 ), so that a triplet which appears often in a dataset is mapped to a DNA triplet widely present in nature – we call this scheme Triplets-Freq. The second method tries to align pairwise distances between codons and chaincode triplets. Specifically, we try to preserve the relation every codon/triplet has with all the other codons/triplets. To do that, we compute for every codon its averaged distance to all the other codons: this represents a measure of how “central” is the codon in the population. We repeated this operation for groups of chaincode triplets. Finally, the mapping is carried out by aligning these two measures of centrality: as before, the ordering of codon and triplet centralities returns the direct mapping – we call this scheme Triplets-Dist, summarized in Fig. 3
                                 . Distances between triplets of chain codes have been obtained as described before (i.e. as a result of the the average link scheme), whereas distances between codons have been computed with the Tajima–Nei function. This function, introduced in [35], computes the distance between two nucleotidic sequences by counting the mismatches which occur between them, each one weighted by the frequency of each symbol in the sequence. Even if this method was proposed several years ago, this distance is still largely applied when comparing DNA sequences.

Once we established the mapping between chain code triplets and codons, we use the standard biological lookup table for translating every codon into an aminoacid, hence obtaining the final encoding.

Summarizing, a given 2D shape is encoded into a biological sequence by scanning its chain code sequence and extracting triplets: we considered overlapped triplets, in order to remove the dependence from the reading frame (right part of Fig. 2). After extraction, every triplet is mapped to the aminoacid via one of the two mappings described above (Triplets-Freq or Triplets-Dist); as a result the sequence of aminoacids is obtained.

Given the encoding, it is now straightforward to define a classification strategy based on standard K-nearest neighbor classifier [36]
                        
                           11
                        
                        
                           11
                           We are aware that, given a similarity matrix, interesting alternatives to K-NN exist (e.g. the dissimilarity-based representation paradigm [37]). However, KNN is accurate and simple enough to demonstrate the suitability of our proposed approach – in this paper we are more interested in showing the feasibility of our perspective, rather than reaching state of the art results. Moreover, this technique is very easy to interpret, since it gives an intuitive motivation of the assigned class label by showing the nearest neighbor to the user.
                        . Given an unknown object X and a distance, this classifier finds the K points in the training set which are nearest to X, assigning X to the most frequent class in that set. This is a natural choice because, given our framework, it is straightforward to define a distance between 2D shapes: after encoding the two shapes into aminoacid sequences, we can align them and use the alignment error as a measure of distance.

In our experiments we used both local and global alignment tools: in particular, we employed the two historical approaches described in previous section, namely the Needleman-Wunsch [23] and the Smith-Waterman [24] algorithms. We used, as substitution matrix, the classical family of BLOSUM matrices [38]
                        
                           12
                        
                        
                           12
                           Recently we investigated the possibility of employing substitution matrices which are learnt from shape datasets, see [39].
                        .

In this section we evaluate the proposed framework in the context of shape classification. In particular, we first describe the datasets we used and the corresponding evaluation protocols; then we provide some details on the parameters of the proposed framework; finally we present and discuss our classification results, putting them in perspective with respect to the state of the art.

The proposed framework has been analysed using five different datasets:

                           
                              •
                              The Vehicle Shape dataset (Vehicle)
                                    13
                                 
                                 
                                    13
                                    
                                       http://visionlab.uta.edu/shape_data.htm.
                                  
                                 [40], which contains 120 vehicle shapes classified in 4 classes; following [40], classification accuracies have been determined using 10-fold cross validation.

The Chicken Pieces dataset (Chicken)
                                    14
                                 
                                 
                                    14
                                    
                                       http://algoval.essex.ac.uk:8080/data/sequence/chicken/.
                                  
                                 [41], containing 446 shapes of chicken pieces, divided in 5 classes; in this case we employed the the Leave-One-Out (LOO) protocol, since most of the literature methods tested on this dataset in the past have been evaluated using this protocol.

The MPEG-7 CE-Shape-1 dataset (MPEG-7)
                                    15
                                 
                                 
                                    15
                                    
                                       http://www.dabi.temple.edu/~shape/MPEG7/dataset.html
                                    
                                  
                                 [42], representing a widely employed dataset used as reference in many 2D shape classification analysis; it contains 1400 shapes, divided in 70 classes. In this case we used both the Leave One Out and the Averaged Holdout Cross Validation (with 10 repetitions) – these being the two most common choices for classification
                                    16
                                 
                                 
                                    16
                                    On the contrary, the Bull’s eye test [9] is typically employed to measure the retrieval performances.
                                  
                                 [30].

The ETH80 dataset (ETH80)
                                    17
                                 
                                 
                                    17
                                    
                                       http://www.d2.mpi-inf.mpg.de/Datasets/ETH80
                                    
                                  
                                 [43], which contains 80 high-resolution color images of 3D objects from 8 categories: each object is represented by 41 images taken from different points of view, leading to a total of 3280 images. For this dataset we used the leave-one-object-out protocol, as detailed in the original paper [43].

The Animal dataset (Animal)
                                    18
                                 
                                 
                                    18
                                    
                                       https://sites.google.com/site/xiangbai/animaldataset
                                    
                                  
                                 [44], which contains 2000 shapes describing 20 kinds of animals — each category has 100 animal images. The dataset is very challenging, containing articulation changes, part missing, large-scale intra-class variation, and noisy shapes. Following [30], we compute the classification accuracy with the averaged holdout protocol (with 10 repetitions).

Some examples of the various datasets are highlighted in Fig. 4
                        . For all datasets, as in other approaches (e.g. [12]), 2D shapes have been pre-aligned using a classic Procrustes analysis. This allows to select a good starting point for the contour. Moreover, following [45], in the ETH80 and MPEG-7 case we sub-sampled the contour retaining only 200 points.

For all datasets we apply the proposed framework: we extract the shape chain codes and we encoded them using the 3 coding strategies described in the previous section (Single, Triplets-Freq and Triplets-Dist). We compute similarities between sequences using two alignment schemes: the Needleman–Wunsch (NW) and the Smith-Waterman (SW), as implemented in the MATLAB Bioinformatics toolbox (the alignment score has been normalized by the length of the two compared sequences). Finally, we perform the classification with the K-NN rule (with K automatically selected with Leave One Out cross-validation on the training set).

As explained in the Section 2, different parameters can be set within the bioinformatics tools; as a general rule, for most of them, we used the default values which are typically offered to the user – this represents the typical choice in biology when no extra information is available. However, for what concerns the two most important parameters of sequence alignment (the substitution matrix and the gap opening/extending penalty), we tested two possible configurations: in the first one, we used the default configuration: BLOSUM62 as substitution matrix and the pair (11,1) for gap penalties – where the former number represents the gap opening penalty, and the latter the gap extending penalty. In the second configuration, we adjusted the biological parameters in order to take into account the specific scenario (the shape classification context) – also in biology the tuning of parameters represents the best way to insert a priori knowledge into the process. In particular, in biology, the default gap penalty is rather high, since in this context it is not usually desirable to break a biological sequence. In the shape context, however, such a strong constraint does not hold: actually, gaps can significantly help in dealing with occlusions and scale changes. Therefore we changed the gap penalty from (11,1) to (6,2) – we restricted our choices to standard gap costs implemented within the BLAST algorithm. Furthermore, we used a substitution matrix which highly penalizes changes in the sequences (namely, the algorithm is forced to try to align the sequences in the best possible way). The idea here is that whereas there are cases in biology where it is preferable to allow a high degree of mutation (i.e. tolerating mismatches), in the 2D shapes context an exact matching may be preferable. In particular we used a BLOSUM90 matrix (the higher the number after the word “BLOSUM” the more “conservative” the substitution matrix) – we called this Advanced configuration.

@&#RESULTS AND DISCUSSION@&#

All results, for all datasets, all coding strategies, all alignment algorithms and parameters are presented in Table 1
                        .

In order to get an idea of the statistical significance of the results, we computed for all experiments the standard errors of the mean, a common way of estimating the variance of the accuracy [46], computed by dividing the averaged accuracy by the squared root of the number of objects. In all experiments concerning the five datasets such errors were all less than 0.0077, 0.0008, 0.0001, 0.0023, 0.0004 for the Vehicle, the Chicken, the MPEG, the ETH80, and the Animal, respectively.

From the table it is evident that the proposed framework performs reasonably well, being also very competitive with respect to the state of the art — reported in Tables 2
                        , 3
                         and 4
                        .

Going more into the details, it is interesting to note that for the chicken, the MPEG, the ETH80 and the Animal datasets the Simple encoding represents the best choice, whereas for vehicle a substantial improvement is obtained when using the two triplets codings (especially with the default configuration of the alignment parameters). At the same time, in the vehicle dataset the Smith-Waterman algorithm provides better results, whereas for the remaining four the Needleman–Wunsch gives superior performances. This may be due to the low between class variability in the vehicle dataset, hence it is more convenient to employ a local algorithm such as SW, which can better align local parts, namely details – which are also better captured by the richer Triplets coding. On the contrary, in the other datasets, the differences between the classes is more pronounced (in such datasets the challenge is represented by the intra-class variability), therefore a global alignment method can be more appropriate. In any case, in all configurations a proper tailoring of the alignment parameters led to an improvement of the performances.

In this part we provide an example of how it is possible to exploit the huge amount of bioinformatics tools to have a deeper understanding of the results. To do that, we evaluated our framework in a slightly different task (the retrieval task), trying to exploit bioinformatics tools and concepts to better understand results that were not satisfactory. Even if related, the retrieval task is slightly different from classification: given a testing object, the goal is to retrieve as many shapes as possible from the same class. Clearly, given our framework, a retrieval system can be easily defined: given an unknown shape encoded in an aminoacid sequence, its alignment with respect to all sequences of the dataset can induce a ranking which allows to extract the most similar shapes.

The MPEG-7 dataset has been widely applied in the retrieval task, where the performances are compared using the well established protocol called bull’s eye test [9]: for every query shape, we count the number of objects that belong to the same class in the top 40 matches; the retrieval rate is then measured as the ratio of the total number of shapes from the same class (which at most is 20) to the highest possible number (which is 
                        
                           20
                           ·
                           1400
                           =
                           28
                           ,
                           000
                        
                     ). A large amount of results appeared in the literature on this task, with some very advanced techniques reaching impressive retrieval rates – for example, an almost perfect retrieval accuracy was given in [70], where
                     
                      many different distance measures are fused using co-transduction.

Using our proposed framework, the best result we achieved was a bulls eye score of 77.24%: in this case, our algorithm does not represent a valuable competitor of other specific algorithms, probably because it is too general to face the larger complexity of the retrieval task with respect to the classification task. Let us go deeper on this aspect: within our framework, which exploits KNN for classification, the starting point is the same for both classification and retrieval, namely the similarity of the query (testing shape) with all objects in the dataset. Roughly speaking, it can be said that here good classification performances can be obtained if the query shape has a very good match with some of the correct training shapes (i.e. that belong to the same class) – in the extreme case of Nearest Neighbor, it is enough that the query has the highest match with only one correct shape, no matter how low are the matching scores with all the remaining correct shapes. On the contrary, in order to have a good retrieval rate, the query shape should have a rather good match with all the shapes of the same class – in MPEG a perfect retrieval is obtained when all the correct shapes are retrieved among the first 40 hits (over 1400). Therefore, in our case, the retrieval task is definitely more difficult than the classification task: within the proposed framework, it is evident that it is possible to derive good matches of the query shape with many correct shapes, but not with all.

In the last part of this section we will provide some more complex versions of the framework which are able to partially alleviate this problem. Before that, however, we would like to show an example of how it is possible to exploit some other standard bioinformatics tools to qualitatively investigate the intuitions sketched before. In particular, we started by visually analysing the alignment results, in particular inspecting and visualizing the regions of high similarity derived from the application of the local Smith Waterman algorithm. Visual analysis of alignments represents a standard tool employed by bioinformaticians, which allows to recover from alignment errors as well as to detect interesting parts of the paired sequences. In our case, we visualized classes where classification and retrieval were both satisfactory, as well as classes where classification was appropriate but retrieval failed. As an example, we report in Fig. 5 two query shapes, together with the corresponding high similarity regions obtained after aligning them with all the other shapes of the same class. The first example refers to a class where both classification and retrieval are good. It shows that the regions of high similarities are (i) long and, more importantly, (ii) mainly involved with the same part of the shape. In contrast, in the second example, only the first matches are with long segments; moreover, the matched parts are different – compare for example the first match with the fourth. Clearly, since there are different shapes in the same class with a good match, satisfactory classification results can be obtained when using the K-NN. Nevertheless, as mentioned before, for retrieval, a given shape should have a good match with all the shapes of the same class, i.e. there should be a unique shared region characterizing the whole class, which should be retrieved in all shapes. Clearly, this is not the case for the example shown in figure – indeed, for the ray class the retrieval rate was really poor.

Another confirmation of this fact can be found by visually inspecting the so-called conserved domains; as described in the previous section, such regions represent parts of the biological sequences which are present in all the sequences belonging to a given set; they are key elements for the correct functioning of the molecules, with many algorithms and tools developed in last 20 years to find them. Here, to extract them, we performed a multiple sequence alignment using the ClustalW online software
                        19
                     
                     
                        19
                        freely available at www.ebi.ac.uk/clustalw2
                        
                     , and we then employ the ConFind program [28] on the multiple alignment. For illustrative purposes, the conserved domains of the ray class are shown in Fig. 6 (in solid red), superimposed to the original 20 shapes. From this figure it is evident that there is not a truly and trustable conserved domain related to a particular peculiar part of the shape, thus the retrieval rate in this case cannot be satisfactory.

A further and final confirmation can be extracted by directly inspecting and visualizing the quality of the multiple sequence alignment of all the sequences belonging to a given class, for example employing the Tcoffee core evaluation program [27] on the multiple sequence alignment given by ClustalW. We report two examples in Fig. 7 again with the hearts and the rays (only part of the visualization is displayed): it is evident that it is impossible to find a reliable and high quality multiple alignment for the right class (rays) – while this is necessary to solve the retrieval task. In contrast, for the class on the left (hearts), the multiple sequence alignment turned out to be very accurate, this is reflected on the retrieval accuracy of the system on this class.

As a final contribution, we investigated some variants of the proposed framework, to provide a more robust distance matrix. In particular, we investigated three lines:

                        
                           1.
                           
                              Alternative Boundary representations. We investigated different contour-based representations for the shape, such as the generalized chain code, the curvature [51], the Inner-Distance Shape Context [9], and the Contour Fragments [30]. For continuous representations (such as curvature, IDSC and CF), we had to perform a vector quantization in order to obtain discrete aminoacid symbols: in particular we quantized the signals to 400 levels, encoding each level with a pair of aminoacids. In all cases, retrieval rates were again not so impressive, where the best result (80.4%) was obtained with the Contour Fragments.


                              Alternative alignment schemes. Following the spirit of the manuscript, we also tried to exploit another bioinformatics algorithm in order to improve the retrieval accuracy. In particular, instead of repeating pairwise alignments of the query sequence to every single training sequence, we tried to align such query simultaneously to all the training sequences, in order to obtain a global and robust alignment. The results confirmed the feasibility of our choice, with the retrieval rate reaching a value of 85%; we consider this a significant result: looking at the state of the art results sketched in the bottom part of Table 1 of [70] (matching algorithms without distance-matrix post processing), we can observe that recent techniques obtained retrieval rates from 80% to 89%
                                 20
                              
                              
                                 20
                                 One approach reached 93%, but it is not based on the contour of the shapes.
                              .


                              Post-processing of distance matrix. Finally, we also tried to post-process the distance matrix, motivated by the fact that many of the most effective approaches to shape retrieval adopted this scheme [70]. In particular we applied the scheme proposed in [60] to the distance matrix obtained with our proposed approach. That scheme learns context-sensitive shape similarity by graph transduction; by using such approach we achieved a 89.1% retrieval rate.

@&#CONCLUSIONS@&#

In this paper we explored the possibility of exploiting bioinformatics concepts, tools and solutions to address the 2D shape classification problem. In our framework, the contour of a 2D shape is encoded using the chain code, and then transformed into biological sequences through three encoding strategies. We then employ biological sequence alignment tools to compute a similarity measure between sequences/shapes, and we use a KNN classification approach. We also proposed some tailoring of the biological sequence alignment tools, which take into account the specific application scenario. Experimental results, on five benchmark datasets, confirm the potentials of the proposed scheme. We also carried out a further analysis to investigate the use of other bioinformatics tools and concepts to deeper inspect the unsatisfactory results obtained in the retrieval case.

@&#ACKNOWLEDGMENTS@&#

Authors would like to thank Nebojsa Jojic and Alessandro Farinelli for helpful discussions and suggestions. Authors are also grateful to the anonymous reviewers for their precious comments.

@&#REFERENCES@&#

