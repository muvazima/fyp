@&#MAIN-TITLE@&#Interactive multiple object learning with scanty human supervision

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Efficient approach for learning and detecting multiple objects in image sequences.


                        
                        
                           
                           Interactive object learning using scanty human supervision.


                        
                        
                           
                           Computation of multiple online classifiers using human-robot interaction.


                        
                        
                           
                           Real-time performance in diverse recognition problems.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Object recognition

Interactive learning

Online classifier

Human-robot interaction

@&#ABSTRACT@&#


               
               
                  We present a fast and online human-robot interaction approach that progressively learns multiple object classifiers using scanty human supervision. Given an input video stream recorded during the human-robot interaction, the user just needs to annotate a small fraction of frames to compute object specific classifiers based on random ferns which share the same features. The resulting methodology is fast (in a few seconds, complex object appearances can be learned), versatile (it can be applied to unconstrained scenarios), scalable (real experiments show we can model up to 30 different object classes), and minimizes the amount of human intervention by leveraging the uncertainty measures associated to each classifier.
                  We thoroughly validate the approach on synthetic data and on real sequences acquired with a mobile platform in indoor and outdoor scenarios containing a multitude of different objects. We show that with little human assistance, we are able to build object classifiers robust to viewpoint changes, partial occlusions, varying lighting and cluttered backgrounds.
               
            

@&#INTRODUCTION@&#

Over the last decade, we have witnessed the enormous progress in the field of object recognition and classification in images and video sequences. At present, there are methods that produce impressive results in a wide variety of challenging scenarios corrupted by lighting changes, cluttered backgrounds, partial occlusions, viewpoint and scale changes, and large intra-class variations (Ali and Saenko, 2014; Felzenszwalb et al., 2010; Hinterstoisser et al., 2011; Malisiewicz et al., 2011; Schulter et al., 2014; Tang et al., 2012; Villamizar et al., 2012a).

This progress in object recognition has had a positive impact in many application fields such as robotics, where computer vision algorithms have been used for diverse robotics tasks such as object recognition and grasping (Alenyà et al., 2014; Amor-Martinez et al., 2014), detection and tracking of people in urban settings (Bellotto and Hu, 2009; Merino et al., 2012; Portmann et al., 2014), human-robot interaction (Ragaglia et al., 2014; Tamura et al., 2014), and robot localization and navigation (Corominas et al., 2008; Ferrer et al., 2013; Hornung et al., 2010).

The standard method for recognizing objects in images consists in computing object specific classifiers during an offline and time-consuming training step, where large amounts of annotated data are used to build discriminative and robust object detectors (Felzenszwalb et al., 2010; Malisiewicz et al., 2011). However, there are situations in which offline learning is not feasible, either because the training data is obtained continuously, or because the size of the training data is very cumbersome, and a batch processing becomes impractical. This is particularly critical in some robotics applications, specially those related to human-robot interaction, where the robots need to compute object detectors on the fly, in real time, and with very little training data.

In these cases, online learning methods which use their own predictions to compute and update a classifier have been proposed (Godec et al., 2010; Grabner and Bischof, 2006; Moreno-Noguer et al., 2008). Yet, although these approaches have shown great adaptation capabilities, they are prone to suffer from drifting when the classifier is updated with wrong predictions. This has been recently addressed by combining offline and online strategies (Gall et al., 2010; Kalal et al., 2010), semi-supervised boosting methods (Grabner et al., 2008), or by using human intervention during learning so as to assist the classifier in uncertain classification cases (Villamizar et al., 2012b; Yao et al., 2012).

In preliminary versions of this work, we already proposed online object detection approaches in which the human assistance is integrated within the learning loop in an active and efficient manner (Garrell et al., 2013; Villamizar et al., 2012b, 2015). In Garrell et al. (2013); Villamizar et al. (2012b) the proposed approach was focused for single object detection, and further extended in (Villamizar et al., 2015) to multiple instances using an adaptive uncertainty classification threshold that reduces the human supervision. In this paper, we unify the formulation of these previous works and perform a more in-depth analysis of the method presented in Villamizar et al. (2015) through additional experiments in synthetic and real scenarios, while providing more comparisons against competing approaches.

More precisely, we propose a fast and online approach that interactively models several object appearances on the fly, using as few human interventions as possible, and still keeping the real-time efficiency (Villamizar et al., 2015). At the core of our approach, there is a randomized tree classifier (Criminisi et al., 2011; Ozuysal et al., 2010; P. Geurts and Wehenkel, 2006) that is progressively computed using its own detection predictions. Yet, to avoid feeding the classifier with false positive samples (i.e, drifting), we propose an uncertainty-based active learning strategy (Lewis and Gale, 1994; Settles, 2010) that gradually minimizes the amount of human supervision and keeps high classification rates. Note that this issue is critical in order to maintain long-term interactions with robots, as if the robot keeps asking for annotating images insistently, people tend to quickly give up the interaction (Garrell et al., 2013; Rani et al., 2006).

To make the proposed approach scalable for various object instances, multiple object specific classifiers are computed in parallel, but sharing the same features in order to maintain the efficiency of the method and to reduce the computational complexity in run time Villamizar et al. (2010).

As an illustrative example, Fig. 1
                      shows the operation of the proposed interactive method to learn and detect multiple object instances through human-robot interaction (Fig. 1a). Each time the human user seeks to model a new object of interest, he/she marks a bounding box around the object in the input image, via a mouse, keyboard or touchscreen (see Fig. 1b). The robot initializes a model for this new object and runs a detector on subsequent frames for this, and the rest of objects in the database ( Fig. 1c). When the robot is not confident enough about the detections and class predictions, it requests the human assistance to provide the true class labels, which, in turn, are used to update the classifier, observe Fig. 1d. This procedure is performed continuously, and at each iteration, the performance and confidence of the classifier is increased whereas the degree of human intervention is reduced significantly.

The remainder of the paper is organized as follows: Section 2 describes the related work and our contributions, while Section 3 explains the proposed approach with all its main ingredients. Section 4 describes the experiments conducted to evaluate the proposed learning approach. We report results using synthetic an real data. The former are used to thoroughly assess the limits of the method in terms of number of classes it can handle or classification rate. Real experiments demonstrate the applicability of the method for diverse perception tasks in challenging scenarios.

In this section, we show and discuss our contributions along with the related work on the three main topics concerned with the proposed approach: human-robot interaction, the computation of online classifiers, and interactive learning techniques.

Computer vision techniques for human-robot interaction have been mainly focused on recognizing people in urban scenarios (Bellotto and Hu, 2009; Merino et al., 2012; Portmann et al., 2014) as well as identifying human gestures and activities (den Bergh et al., 2011; Nickel and Stiefelhagen, 2007) to establish contact with people and perform particular robotics tasks such as guiding people in museums and urban areas (Garrell and Sanfeliu, 2010; Rashed et al., 2015; Thrun, 2000), providing information in shopping malls (Gross et al., 2002), or recognizing human emotions through classifying facial gestures (Bartlett et al., 2003). Although, these techniques have endowed the robot with remarkable interaction skills, they are commonly computed offline and using a potentially large training time. As a result, the robot is limited to perform tasks only for which it has been trained previously, missing the opportunity to learn and improve its perception skills through the interaction with humans.

Conversely, in this work we propose a very efficient interactive approach that combines human assistance and robot’s detection predictions so as to build object detectors which can be applied for a wide range of robotics tasks. The approach exploits the interplay between robots and humans in order to compute and improve progressively the robot’s perception capabilities.

Despite showing impressive results, standard methods for object detection compute the classifiers using intensive and offline learning approaches applied to large annotated datasets (Felzenszwalb et al., 2010; Malisiewicz et al., 2011; Ozuysal et al., 2010; Villamizar et al., 2012a). Therefore, most of these offline approaches are not suitable for some particular applications requiring computing the classifier on the fly, either because the training data is obtained continuously, or the size of the training data is so large that it needs to be loaded progressively. To handle these situations, several online alternatives allowing to sequentially train the classifiers have been proposed (Avidan, 2007; Babenko et al., 2011; Grabner and Bischof, 2006; Hall and Perona, 2014; Santner et al., 2010).

In this work, the classifier we use is based on an online random ferns formulation (Kalal et al., 2010; Krupka et al., 2014; Ozuysal et al., 2010; Villamizar et al., 2012b, 2015), which has been showing excellent results, both in terms of classification rates as computational efficiency. In Fig. 2
                        , we show the overall schemes of the proposed interactive method and the online classifier. In essence, this classifier computes several sets of intensity-based pixel comparisons (see Fig. 2b) to build the randomized trees which are then used to estimate the posterior class probabilities.

Most previous online versions are focused to single object modeling and tracking (Avidan, 2007; Babenko et al., 2011; Grabner and Bischof, 2006; Villamizar et al., 2012b). In order to learn multiple models we propose computing simultaneously and in parallel multiple classifiers, one for each object class (Fig. 2a), and with specific configurations like the spatial distribution of ferns or the particular object size (observe Fig. 2c). This also differs from other state of the art classifiers, that when applied to multiclass problems they require objects with constant aspect ratios and to know the number of object classes in advance (Torralba et al., 2007). In this respect, our method scales better for multiple objects since each object is learned by separated at the time in which the user selects a new object model in the video stream. This allows learning and detecting up to 30 object classes in an efficient and dynamic manner.

Active learning techniques have been extensively used in computer vision to reduce the number of training samples that need to be annotated when building a classifier (Settles, 2010). Approaches such as “query by committee” (Abe and Mamitsuka, 1998; H.S. Seung and Sompolinsky, 1992), and “uncertainty-based sampling” (Lewis and Gale, 1994) close the learning loop using human assistance. In these works, the human user acts as an oracle that annotates/labels those samples that the classifier is not quite confident about their class prediction.

In this paper, we propose an interactive learning strategy in which the robot plays a more active role, that is, the discriminative classifiers are built using a combination of the robot predictions with the human assistance (see Fig. 2f–h). Additionally, we also propose a methodology based on an adaptive uncertainty threshold that progressively reduces the amount of human assistance, making a more ”enjoyable” human-robot interaction. This is also another difference with respect to our own previous works (Garrell et al., 2013; Villamizar et al., 2012b). As it will be shown in the experimental section, using an adaptive threshold we can learn and detect several object instances without decrementing the intra-class classification rates.

After having discussed the related work, we can summarize the main contributions of our approach as follows: (1) Proposing an online approach to learn and detect multiple object instances in images; (2) Designing an interactive learning strategy that incrementally improves the discrimination power of the classifiers using human assistance; (3) An adaptive learning scheme to reduce gradually the human interventions; and (4) A real-time implementation of the algorithm, which can cope with multiple objects at several frames per second.

In this section, the main components of the proposed learning and detection strategy are described in more detail. Fig. 2 shows an schematic of how these elements are related.

To perform online learning of object instances, we consider a scenario in which the classifiers are learned using a computer onboard a mobile robot, equipped with devices such as a keyboard, mouse, and a screen that enable the interaction with the human. We refer the reader to Fig. 1 for an illustration. Specifically, the keyboard and mouse are used to annotate the object of interest that the user wants to learn, and also to attend the robot in situations where the robot is not confident in its predictions. On the other hand, the touch screen is used to display the output of the detector and to show the performance of the detection system.

In those situations where the robot is uncertain about its detection hypotheses, the robot will formulate to the user a set of concise questions, that expect for a ‘yes’ or ‘no’ answer. Table 1
                         shows a few examples of such questions that are used to label the detection hypotheses and to update the classifiers with them. Note that the classifiers are computed only with difficult samples which require human assistance (i.e, active learning). In the experiments section, we will show that this strategy improves the classification performance using less human annotations.

In order to make the human-robot interaction efficient and dynamic, the robot has been programmed with behaviors that avoid having large latency times (Table 1), specially when the human does not know exactly how to proceed. Strategies for approaching the person in a safe and social manner, or attracting people’s attention have been designed for this purpose (Feil-Seifer and Mataric, 2005; Garrell et al., 2013; Villamizar et al., 2012b; Wilkes et al., 1997).

The proposed approach performs object detection by scanning a fixed-size sliding window over an input image I, observe Fig. 2d. At every image location, the classifier is tested over an image sample x (local image window defined by the object size Bu
                         × Bv
                        ) and returns the probability that such window contains a particular object instance (Fig. 2e and f). The size of the object is defined at the time of selecting the object of interest by the user using the computer mouse. This scanning procedure is carried out over the entire image, and once it is finished non-maximal neighborhood suppression is applied to remove multiple overlapped detections. Observe that Fig. 2i shows just a single detection (green bounding box). Additionally, the scanning process is repeated for different window sizes so as to deal with scale changes.

Next, we describe each of the main ingredients of the online classifier used for object detection: the computation of random ferns on pixel intensities, the shared pool of ferns parameters to reduce the computational complexity of the method, the building of the classifier and the process to update this classifier with new samples.

We compute object classifiers using a particular version of the extremely randomized trees (Criminisi et al., 2011; Ozuysal et al., 2010; P. Geurts and Wehenkel, 2006), which are the so-called random ferns (Kalal et al., 2010; Krupka et al., 2014; Ozuysal et al., 2010; Villamizar et al., 2012b). Specifically, random ferns consist of sets of random and simple binary features computed over image pixel intensities Ozuysal et al. (2010). Fig. 2b shows for instance three different random ferns, each with three binary features (i.e., pairs of colored dots).

More formally, each feature f corresponds to a Boolean comparison of intensity image values at two pixel locations a and b within a square subwindow s of size S × S where the fern is computed. Refer to Fig. 3
                            for a descriptive example showing the computation of a random fern and its binary features. Then, a binary feature can be written as:

                              
                                 (1)
                                 
                                    
                                       f
                                       (
                                       x
                                       ;
                                       u
                                       ,
                                       ω
                                       )
                                       =
                                       I
                                       (
                                       x
                                       (
                                       a
                                       )
                                       >
                                       x
                                       (
                                       b
                                       )
                                       )
                                    
                                 
                              
                           where 
                              
                                 I
                                 (
                                 e
                                 )
                              
                            is the indicator function
                              1
                           
                           
                              1
                              The indicator function is defined by: 
                                    
                                       I
                                       (
                                       e
                                       )
                                       =
                                       1
                                    
                                  if e is true, and 0 otherwise.
                           , x is the image sample or window, u is the center position of the subwindow s inside the image window, 
                              
                                 ω
                                 =
                                 {
                                 a
                                 ,
                                 b
                                 }
                              
                            are two randomly chosen pixel locations, and x(a) indicates the intensity-pixel value at location a.

Following the same spirit of the random ferns Ozuysal et al. (2010), we define a fern f as the combination of M different binary features in the image subwindow s. This can be formulated as,

                              
                                 (2)
                                 
                                    
                                       f
                                       
                                          (
                                          x
                                          ;
                                          u
                                          ,
                                          
                                             ω
                                          
                                          )
                                       
                                       =
                                       [
                                       f
                                       
                                          (
                                          x
                                          ;
                                          u
                                          ,
                                          
                                             ω
                                             1
                                          
                                          )
                                       
                                       ,
                                       f
                                       
                                          (
                                          x
                                          ;
                                          u
                                          ,
                                          
                                             ω
                                             2
                                          
                                          )
                                       
                                       ,
                                       ⋯
                                       ,
                                       f
                                       
                                          (
                                          x
                                          ;
                                          u
                                          ,
                                          
                                             ω
                                             M
                                          
                                          )
                                       
                                       ]
                                    
                                 
                              
                           where 
                              
                                 
                                    ω
                                 
                                 =
                                 
                                    {
                                    
                                       ω
                                       1
                                    
                                    ,
                                    …
                                    ,
                                    
                                       ω
                                       M
                                    
                                    }
                                 
                              
                            is the set of parameters (pixel locations) for M different binary features.

The fern output f(x; u, ω
                           ) is an M-dimensional binary vector, which at the implementation level, is represented by an integer value 
                              
                                 z
                                 ∈
                                 {
                                 1
                                 ,
                                 …
                                 ,
                                 
                                    2
                                    M
                                 
                                 }
                              
                           . Fig. 3c and d shows an example of how a fern is computed on a subwindow s centered at u. In this case, 
                              
                                 M
                                 =
                                 3
                              
                            intensity-based pixel comparisons are considered, with individual outputs 1,1,0. The combination of these Boolean features responses determines the fern output 
                              
                                 z
                                 =
                                 
                                    
                                       (
                                       110
                                       )
                                    
                                    2
                                 
                                 +
                                 1
                                 =
                                 7
                              
                           .

In order to compute efficiently multiple online object classifiers from scratch, we propose to use recursively a small set of fern features among all classifiers. By doing this, the computation of the fern features, which is the most computation costly part of the algorithm, is shared by all classifiers. This provides a remarkable speed up compared to when we train each classifier with a different subset of ferns, while classification rates are shown to remain high Villamizar et al. (2012a, 2010).

To this end, a small set of R random ferns parameters, 
                              
                                 Ω
                                 =
                                 
                                    {
                                    
                                       
                                          ω
                                       
                                       1
                                    
                                    ,
                                    ⋯
                                    ,
                                    
                                       
                                          ω
                                       
                                       R
                                    
                                    }
                                 
                                 ,
                              
                            is computed in advance, such that each object classifier can then be computed as a combination of ferns evaluated at different image locations but sharing the same parameters. Again in Fig. 2c, we show an example where two different object classifiers are trained for two object instances. Note that each classifier is computed using five random ferns with only 
                              
                                 R
                                 =
                                 3
                              
                            features parameters, but with a specific spatial distribution and ferns probabilities which makes it discriminative for that object class.

Since both classifiers use the same fern features parameters Ω, the feature computation is shared and done in advance to testing the object classifiers. This results in a U × V × R lookup table with a dense sampling of all possible fern responses, being U × V the size of the input image I. Consequently, the sharing strategy makes the overall computational cost to be just a function of the number of ferns parameters R (a typical value is 
                              
                                 R
                                 =
                                 10
                              
                           ), and to be independent on the number of classifiers (K) and the amount of ferns in each classifier (J). In fact, since the complexity of the rest of computations involved in the test is negligible compared to the cost of these convolutions, we can roughly approximate a 
                              
                                 
                                    K
                                    J
                                 
                                 R
                              
                           -fold speed-up achieved by the sharing scheme Villamizar et al. (2010). With this, two classifiers with 
                              
                                 J
                                 =
                                 200
                              
                            ferns and 
                              
                                 R
                                 =
                                 10
                              
                            distinct features parameters, yields speed-ups of up to 40×.

The classifier Hk
                           (x) for an object instance k is built by random sampling with replacement of J random ferns, from the shared pool Ω, computed at multiple image locations. The response of this classifier Hk
                           (x) over the sample x is:

                              
                                 (3)
                                 
                                    
                                       
                                          H
                                          k
                                       
                                       
                                          (
                                          x
                                          )
                                       
                                       =
                                       
                                          {
                                          
                                             
                                                
                                                   
                                                      +
                                                      1
                                                   
                                                
                                                
                                                   
                                                      if
                                                      
                                                      
                                                         conf
                                                         k
                                                      
                                                      
                                                         (
                                                         x
                                                         )
                                                      
                                                      >
                                                      β
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      −
                                                      1
                                                   
                                                
                                                
                                                   
                                                      otherwise
                                                      ,
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where conf
                              k
                           (x) is the confidence of the classifier on predicting that x belongs to the object k, and β is a confidence threshold whose default value is 0.5. Thus, if the output of the classifier is 
                              
                                 H
                                 (
                                 x
                                 )
                                 =
                                 +
                                 1
                                 ,
                              
                            the sample x is considered as an object or positive sample. Otherwise, this sample is assigned to the background or negative class.

The confidence of the classifier is defined according to the following posterior:

                              
                                 (4)
                                 
                                    
                                       
                                          conf
                                          k
                                       
                                       
                                          (
                                          x
                                          )
                                       
                                       =
                                       p
                                       
                                          (
                                          y
                                          =
                                          +
                                          1
                                          |
                                          
                                             F
                                             k
                                          
                                          
                                             (
                                             x
                                             )
                                          
                                          ,
                                          
                                             
                                                η
                                             
                                             k
                                          
                                          )
                                       
                                       ,
                                    
                                 
                              
                           where 
                              
                                 
                                    F
                                    k
                                 
                                 
                                    (
                                    x
                                    )
                                 
                                 =
                                 
                                    
                                       {
                                       f
                                       
                                          (
                                          x
                                          ;
                                          
                                             u
                                             j
                                          
                                          ,
                                          
                                             
                                                ω
                                             
                                             j
                                          
                                          )
                                       
                                       }
                                    
                                    
                                       j
                                       =
                                       1
                                    
                                    J
                                 
                                 ,
                              
                            with 
                              
                                 
                                    
                                       ω
                                    
                                    j
                                 
                                 ∈
                                 
                                    {
                                    
                                       
                                          ω
                                       
                                       1
                                    
                                    ,
                                    ⋯
                                    ,
                                    
                                       
                                          ω
                                       
                                       R
                                    
                                    }
                                 
                              
                            and 
                              
                                 
                                    u
                                    j
                                 
                                 ∈
                                 
                                    {
                                    
                                       u
                                       1
                                    
                                    ,
                                    …
                                    ,
                                    
                                       u
                                       L
                                    
                                    }
                                 
                                 ,
                              
                            makes reference to the set of J ferns for the classifier k, 
                              
                                 y
                                 =
                                 {
                                 +
                                 1
                                 ,
                                 −
                                 1
                                 }
                              
                            indicates the class label, and 
                              η
                           
                           
                              k
                            are parameters of the classifier. The set 
                              
                                 {
                                 
                                    u
                                    1
                                 
                                 ,
                                 …
                                 ,
                                 
                                    u
                                    L
                                 
                                 }
                              
                            corresponds to all possible 2D pixel coordinates within a window x where a fern can be tested. For each fern j, the binary features parameters 
                              ω
                           
                           
                              j
                            and fern location u
                           
                              j
                            are chosen at random and kept constant during the learning and run-time steps.

In turn, the posterior probability 
                              
                                 p
                                 (
                                 y
                                 =
                                 +
                                 1
                                 |
                                 
                                    F
                                    k
                                 
                                 
                                    (
                                    x
                                    )
                                 
                                 ,
                                 
                                    
                                       η
                                    
                                    k
                                 
                                 )
                              
                            is computed by combining the posterior of the J ferns:

                              
                                 (5)
                                 
                                    
                                       p
                                       
                                          (
                                          y
                                          =
                                          +
                                          1
                                          |
                                          
                                             F
                                             k
                                          
                                          
                                             (
                                             x
                                             )
                                          
                                          ,
                                          
                                             
                                                η
                                             
                                             k
                                          
                                          )
                                       
                                       =
                                       
                                          1
                                          J
                                       
                                       
                                          ∑
                                          
                                             j
                                             =
                                             1
                                          
                                          J
                                       
                                       p
                                       
                                          (
                                          y
                                          =
                                          +
                                          1
                                          |
                                          f
                                          
                                             (
                                             x
                                             ;
                                             
                                                u
                                                j
                                             
                                             ,
                                             
                                                
                                                   ω
                                                
                                                j
                                             
                                             )
                                          
                                          =
                                          z
                                          ,
                                          
                                             η
                                             k
                                             
                                                j
                                                ,
                                                z
                                             
                                          
                                          )
                                       
                                       ,
                                    
                                 
                              
                           where z is the fern output and 
                              
                                 η
                                 k
                                 
                                    j
                                    ,
                                    z
                                 
                              
                            is the probability that the sample x belongs to the positive class in the k-th classifier with output z in the fern j. Since these posterior probabilities follow a Bernoulli distribution, 
                              
                                 p
                                 
                                    (
                                    
                                       y
                                       |
                                       f
                                    
                                    
                                       (
                                       x
                                       ;
                                       
                                          u
                                          j
                                       
                                       ,
                                       
                                          
                                             ω
                                          
                                          j
                                       
                                       )
                                    
                                    =
                                    z
                                    ,
                                    
                                       η
                                       k
                                       
                                          j
                                          ,
                                          z
                                       
                                    
                                    )
                                 
                                 ∼
                                 Ber
                                 
                                    (
                                    y
                                    |
                                    
                                       η
                                       k
                                       
                                          j
                                          ,
                                          z
                                       
                                    
                                    )
                                 
                                 ,
                              
                            we can write that

                              
                                 (6)
                                 
                                    
                                       p
                                       
                                          (
                                          
                                             y
                                             =
                                             +
                                             1
                                             |
                                             f
                                          
                                          
                                             (
                                             x
                                             ;
                                             
                                                u
                                                j
                                             
                                             ,
                                             
                                                
                                                   ω
                                                
                                                j
                                             
                                             )
                                          
                                          =
                                          z
                                          ,
                                          
                                             η
                                             k
                                             
                                                j
                                                ,
                                                z
                                             
                                          
                                          )
                                       
                                       =
                                       
                                          η
                                          k
                                          
                                             j
                                             ,
                                             z
                                          
                                       
                                       .
                                    
                                 
                              
                           
                        

The parameters of these distributions are computed through a Maximum Likelihood Estimate (MLE) over the input samples and their corresponding labels, provided by the human user during the interaction with the robot. That is,

                              
                                 (7)
                                 
                                    
                                       
                                          η
                                          k
                                          
                                             j
                                             ,
                                             z
                                          
                                       
                                       =
                                       
                                          
                                             N
                                             
                                                k
                                                ,
                                                +
                                                1
                                             
                                             
                                                j
                                                ,
                                                z
                                             
                                          
                                          
                                             
                                                N
                                                
                                                   k
                                                   ,
                                                   +
                                                   1
                                                
                                                
                                                   j
                                                   ,
                                                   z
                                                
                                             
                                             +
                                             
                                                N
                                                
                                                   k
                                                   ,
                                                   −
                                                   1
                                                
                                                
                                                   j
                                                   ,
                                                   z
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where 
                              
                                 N
                                 
                                    k
                                    ,
                                    +
                                    1
                                 
                                 
                                    j
                                    ,
                                    z
                                 
                              
                            is the number of positive (object) samples with output z for fern j. Similarly, 
                              
                                 N
                                 
                                    k
                                    ,
                                    −
                                    1
                                 
                                 
                                    j
                                    ,
                                    z
                                 
                              
                            corresponds to the number of negative samples for the fern j with output z in the classifier k.

During the learning stage, once a sample has been labeled by the human user, this sample is used to recompute the probabilities 
                              
                                 η
                                 k
                                 
                                    j
                                    ,
                                    z
                                 
                              
                            of Eq. (7), and update the online classifier. For instance, let us assume that a sample x is labeled as 
                              
                                 y
                                 =
                                 +
                                 1
                                 ,
                              
                            and it activates the output z of the fern f(x; u
                           
                              j
                           , 
                              ω
                           
                           
                              j
                           ). We will then update the classifier by adding one unit to the z-th bin of the histogram of 
                              
                                 N
                                 
                                    k
                                    ,
                                    +
                                    1
                                 
                                 
                                    j
                                    ,
                                    z
                                 
                              
                           . This process is repeated for all J ferns of the classifier k. With these new distributions, we can recompute the priors 
                              
                                 η
                                 k
                                 
                                    j
                                    ,
                                    z
                                 
                              
                            and update the classifier.

The computation of the online classifier is summarized in Algorithm 1. Note that the first part of the algorithm (lines 1–3) corresponds to convolve the ferns, with the shared pool of fern features parameters Ω, on the input sample x. This process is done in advance to updating the classifier, and hence, it reduces drastically the computational cost since the size of Ω is much lower than the number of ferns (R < <J).
                        

The online learning strategy to train a specific classifier k is shown again in Fig. 2d. As mentioned before, the object detection is carried out using a sliding window approach Viola and Jones (2001), where the classifier Hk
                        (x) is tested at every image location and multiple scales over an input image I. At each location, the image sample x is evaluated on all J ferns of the classifier to obtain the confidence conf
                           k
                        (x) (Eq. (4)). Subsequently, the class label for this sample, 
                           
                              y
                              =
                              {
                              +
                              1
                              ,
                              −
                              1
                              }
                              ,
                           
                         is estimated according to the classifier response and the threshold β (Eq. (3)).

To reduce the number of false positives and avoid drifting problems (produced when updating the classifier with erroneously labeled samples), frequent in non- and semi-supervised learning approaches Avidan (2007); Grabner and Bischof (2006), we use an uncertainty-based active learning strategy Lewis and Gale (1994); Settles (2010) that reduces gradually the amount of human assistance in combination with an adaptive uncertainty threshold. Active learning minimizes the risk of misclassification by updating the classifier only with samples which have been annotated/labeled by the user.

Therefore, in situations where the classifier is not certain about the class estimate y, because the confidence over the sample x is ambiguous (close to the threshold β), the system opts for requiring the human help so as annotate the true class of the sample. This request q can be written as:

                           
                              (8)
                              
                                 
                                    q
                                    
                                       (
                                       x
                                       )
                                    
                                    =
                                    I
                                    (
                                    β
                                    +
                                    
                                       θ
                                       k
                                    
                                    /
                                    2
                                    >
                                    
                                       conf
                                       k
                                    
                                    
                                       (
                                       x
                                       )
                                    
                                    >
                                    β
                                    −
                                    
                                       θ
                                       k
                                    
                                    /
                                    2
                                    )
                                    ,
                                 
                              
                           
                        where θk
                         corresponds to the uncertainty threshold for the classifier k. Fig. 4
                         displays a clarifying example. If q(x) is true the system asks for human assistance. Otherwise, this sample is discarded and not used to update the classifier. Note that by doing this we are just feeding the classifier with labeled samples that are close to the decision boundary, improving thus, its discriminability power.

With the aim of adapting the human assistance in accordance to the performance of the classifier, we define an adaptive threshold that depends on the incremental classification rate over the requested samples. That is,

                           
                              (9)
                              
                                 
                                    
                                       θ
                                       k
                                    
                                    =
                                    1
                                    −
                                    ξ
                                    
                                       λ
                                       k
                                    
                                    ,
                                 
                              
                           
                        where ξ is a sensitivity parameter assigned by the user, and λk
                         measures the performance of the classifier k. In turn, this performance rate can be computed by

                           
                              (10)
                              
                                 
                                    
                                       λ
                                       k
                                    
                                    =
                                    
                                       M
                                       k
                                       c
                                    
                                    /
                                    
                                       M
                                       k
                                       q
                                    
                                 
                              
                           
                        being 
                           
                              M
                              k
                              q
                           
                         and 
                           
                              M
                              k
                              c
                           
                         the numbers of requested samples and correctly classified samples, respectively. A sample x is correctly classified when the class label y coming from the classifier agrees with the true class label given by the user.


                        Algorithm 2
                         shows the proposed interactive learning approach to compute online classifiers with small human supervision. Observe that each classifier is only computed with difficult samples falling close to the decision boundary β and which require human assistance.

@&#EXPERIMENTAL RESULTS@&#

In this section, the proposed approach is evaluated and analyzed using diverse synthetic and real experiments. Synthetic experiments are used to accurately assess the limits of the learning approach in terms of the potential number of classes it can handle or the amount of human assistance required by ours and alternative learning strategies. Real experiments in indoor and outdoor scenarios are used in order to demonstrate the applicability of the system in our robotic platform. The system is used to perform some particular robotics tasks such as detecting multiple objects in urban environments and recognizing faces during human-robot interaction.

We initially analyze the performance of the proposed online method on a synthetic 2D classification problem, which reveals the influence of certain parameters or different learning strategies on the classification results and on the number of samples that need to be manually annotated.


                        Fig. 5
                         -left shows an example of a 2D classification problem where 10 positive classes (colored points) and one negative class (black points spread out over the feature space) are randomly and sequentially fed to the online learning system in order to compute the classifiers. In this particular scenario, the classifiers are computed using individual 2D decision stumps as binary features (Eq. (1)). That is,

                           
                              (11)
                              
                                 
                                    f
                                    
                                       (
                                       x
                                       ;
                                       ω
                                       )
                                    
                                    =
                                    I
                                    
                                       (
                                       
                                          x
                                          i
                                       
                                       >
                                       ϕ
                                       )
                                    
                                    ,
                                 
                              
                           
                        where i ∈ {1, 2} is the feature axis, ϕ ∈ (0, 1) is threshold defining the space partition, and 
                           
                              ω
                              =
                              {
                              i
                              ,
                              ϕ
                              }
                           
                         are the parameters of the binary feature.

The classification performance of the proposed approach is shown in Fig. 5-center where the classifiers computed using the training samples (left side) are evaluated on a set of test samples in order to measure the generalization capability. It can be seen that most samples are correctly classified and only a small fraction of them are misclassified, indicated in the figure through red circles. Correctly classified samples are depicted using the same color code as the training samples (left side) and without plotting red circles. Quantitatively speaking, the method obtains a F-measure
                           2
                        
                        
                           2
                           F-measure is the harmonic mean of precision and recall:

                                 
                                    
                                       
                                          F
                                          =
                                          2
                                          
                                             
                                                r
                                                e
                                                c
                                                a
                                                l
                                                l
                                                ×
                                                p
                                                r
                                                e
                                                c
                                                i
                                                s
                                                i
                                                o
                                                n
                                             
                                             
                                                r
                                                e
                                                c
                                                a
                                                l
                                                l
                                                +
                                                p
                                                r
                                                e
                                                c
                                                i
                                                s
                                                i
                                                o
                                                n
                                             
                                          
                                       
                                    
                                 
                              
                           
                         rate of 0.994 to distinguish positive samples from negative ones (two-class separability). Fig. 5-right shows the confusion matrix in specifically recognizing each of the 10 positive classes (using ground-truth sample labels). We see that the method achieves high classification rates both to separate the positive and negative classes and to correctly classify the positive subclasses.

In Fig. 6
                        -left is shown the incremental classification performance of the method as the training samples are given sequentially to the online classifiers during the learning step. Here, five different learning alternatives are evaluated:

                           
                              
                                 Supervised: The classifiers are trained with all samples and using human labels. That is, for each sample x the human user provides its class label y. They are used to train/update the classifiers.


                                 Semi-supervised: The first n samples are labeled by the human (human labels), whereas the rest ones are labeled using the classifier confidence (machine label). The latter is computed for a sample x according to 
                                    
                                       y
                                       =
                                       H
                                       (
                                       x
                                       )
                                    
                                  (see Eq. (3)). In this work we use a value of 
                                    
                                       n
                                       =
                                       500
                                    
                                 .


                                 Active: The classifiers are only trained/updated in cases of high uncertainty in their predictions. The human resolves the ambiguity by providing the sample label. Here, we use a fixed uncertainty threshold of 
                                    
                                       θ
                                       =
                                       0.2
                                    
                                  and the assistance criterion is defined in Eq. (8). Hence, samples falling within the range 
                                    
                                       [
                                       β
                                       −
                                       θ
                                       /
                                       2
                                       ,
                                       β
                                       +
                                       θ
                                       /
                                       2
                                       ]
                                    
                                  are requested for human annotation.


                                 Villamizar ICPR2012 
                                 Villamizar et al. (2012b): This approach combines active and semi-supervised learning strategies. Active learning for uncertain samples and self-learning for certain samples. Similar to the previous case, active learning uses a fixed threshold (
                                    
                                       θ
                                       =
                                       0.2
                                    
                                 ) and the assistance criterion (Eq. (8)), while the self-learning estimates the sample label using the classifier output (Eq. (3)).


                                 Proposed 
                                 Villamizar et al. (2015): The classifiers use active learning in combination with an adaptive uncertainty threshold θ defined in Eq. (9).

It can be observed that all learning approaches start with low classification scores, but then they begin to improve progressively as more samples are provided to the classifiers. However, note that at some point, the semi-supervised learning deteriorates the classifier performance. This is because the self-learning suffers from drifting problems, making the classifier to be constantly updated with erroneously labeled samples. Specifically, this classifier deteriorates after 
                           
                              n
                              =
                              500
                           
                         samples, which is the number of human labels considered in this experiment. Conversely, our proposed method and the active learning obtain the best performance since the classifiers are computed with highly informative samples (uncertain samples) and human labels. This focuses the classifier mainly on the decision boundary and makes it more discriminative that using all training samples (supervised method).

Similarly, Fig. 6-center · left shows the final classification rates (F-measure) calculated on the set of test samples (refer to Fig. 5-center). We see again that the proposed method, together with the active learning, achieves the best classification performance and generalization capability. This is evidenced in Fig. 6-center· right where the average classification scores across the different classes are plotted. Notice that all scores are above the classification threshold β, whereas the average score for the negative class is indicated in the figure by the black thick line.

As regards to the amount of human intervention, Fig. 6-right displays the percentages of human labels during the learning step as a function of the number of incoming samples. The semi-supervised learning just uses labels for the first 500 samples. Note also that the supervised learning uses all human labels (represented by a diagonal line) to compute the classifiers, whereas the proposed method reduces considerably the amount of human assistance. More precisely, the classifiers are computed by only using 22% of the training samples. By contrast, the active learning continues requiring human assistance until reaching about 38% of the samples, while the learning method presented in Villamizar et al. (2012b) reports an annotation rate of 32%, since this method uses a fixed uncertainty threshold θ. This shows that the proposed adaptive uncertainty threshold reduces the human intervention without deteriorating the classification rates.

This behavior is observed in Fig. 7
                         where our approach is evaluated in terms of the adaptive uncertainty threshold. Fig. 7-left shows the human assistance percentages for four different values of the sensitivity parameter ξ. We observe how the number of required human annotations decreases as the sensitivity parameter gets larger until obtaining less than 10% of the training samples. However, this at the expense of an important reduction in the classification rates, see Fig. 7-center · left. In Fig. 7-center · right we can see the adaptive threshold values through the incoming samples. As a general trend, the threshold decreases rapidly as the classifiers get more confident in their predictions.

We plot in Fig. 7-right the classification rates on the test samples for varying numbers of classes. Note that increasing the number of classes in the learning phase, produces a small drop in the classification rates. This is because we are considering a large number of classes for such a small feature space (2D).

With regards to other state-of-the-art classifiers, the proposed approach is evaluated against the JointBoosting classifier (JBoost), introduced in Torralba et al. (2007), using the current 2D classification problem. That is, the classification of multiple positive classes with respect to a negative class ( Fig. 8
                        -left). For JBoost, we directly use an implementation publicly available for 2D classification problems
                           3
                        
                        
                           3
                           
                              http://web.mit.edu/torralba/www/.
                        . We compare against JBoost since it is an efficient and multi-class classifier that shares weak learners across different classes, reducing thus, the overall number of features.

In Fig. 9
                        , we show the classification performance of our approach (ORFs) and JBoost in the sets of test samples. We base our analysis on two metrics: the Equal Error Rate (EER) on the precision-recall curve
                           4
                        
                        
                           4
                           The equal error rate is the point in the precision-recall curve where precision=recall.
                        ; and the Hellinger distance
                           5
                        
                        
                           5
                           The squared Hellinger distance for two distributions P and Q is defined as: 
                                 
                                    
                                       H
                                       2
                                    
                                    
                                       (
                                       P
                                       ,
                                       Q
                                       )
                                    
                                    =
                                    1
                                    −
                                    
                                       
                                          
                                             k
                                             1
                                          
                                          /
                                          
                                             k
                                             2
                                          
                                       
                                    
                                    exp
                                    
                                       (
                                       −
                                       0.25
                                       
                                          k
                                          3
                                       
                                       /
                                       
                                          k
                                          2
                                       
                                       )
                                    
                                    ,
                                 
                               with 
                                 
                                    
                                       k
                                       1
                                    
                                    =
                                    2
                                    
                                       σ
                                       P
                                    
                                    
                                       σ
                                       Q
                                    
                                    ,
                                 
                              
                              
                                 
                                    
                                       k
                                       2
                                    
                                    =
                                    
                                       σ
                                       P
                                       2
                                    
                                    +
                                    
                                       σ
                                       Q
                                       2
                                    
                                    ,
                                 
                               and 
                                 
                                    
                                       k
                                       3
                                    
                                    =
                                    
                                       
                                          (
                                          
                                             μ
                                             P
                                          
                                          −
                                          
                                             μ
                                             Q
                                          
                                          )
                                       
                                       2
                                    
                                 
                              .
                         that measures the degree of separability between the positive and negative distributions. We also report the performance in terms of the training and testing (running) times for various numbers of training classes, weak learners (WLs) for JBoost, and random ferns (J) for the proposed method. Note that the method we propose achieves remarkable classification rates (EER) and larger separability between the positive and negative classes. This is especially outstanding for large values of training classes and small numbers of random ferns. In general, the proposed approach outperforms to JBoost while keeps high efficiency. We see that the method is trained in less that one second (using 1000 positive and negative samples) and that it can be tested in the order of microseconds per test sample. This contrasts to JBoost which is very efficient, but only for small amounts of classes. Additionally, JBoost is an offline and fully supervised classifier that requires knowing the number of classes in advance so as to train all classes together.

Finally, Fig. 8 shows the classification response of ORFs and JBoost using five training classes and 50 weak learners/ferns. The left column in the figure depicts the classification output of both classifiers. We can see that our method produces a small rate of misclassified samples (indicated again with red circles). The center column shows the confidence of the classifiers on the entire feature space, where bright areas correspond to high classification scores. Here, we attribute the color cyan to the positive class, whereas red regions make reference to regions with high probability to belong to the negative class. Notice that both methods show certain correspondence with the samples in the left column. Fig. 8-right plots the score class distributions and the Hellinger distance between the positive and negative samples. Both methods obtain large distance values between classes.

The proposed approach is evaluated in this section over different object recognition scenarios and compared against some standard works devoted to object detection and tracking in video sequences
                           6
                        
                        
                           6
                           The code for the proposed method is available at http://www.iri.upc.edu/people/mvillami/code.html.
                        .

Unless otherwise stated, all classifiers are computed using the same parameters. The number of ferns used to build each classifier has been set to 
                           
                              J
                              =
                              500
                              ,
                           
                         while the amount of shared ferns parameters has been set to 
                           
                              R
                              =
                              10
                           
                        . We have utilized 
                           
                              M
                              =
                              9
                           
                         binary features to compute each random fern. Moreover, the size of the subwindow s is 
                           
                              S
                              =
                              8
                              ,
                           
                         whereas all objects are normalized by height to 30 pixels, but maintaining the aspect ratio of objects. By default, we have set a conservative sensitiviy parameter of 
                           
                              ξ
                              =
                              0.95
                           
                        .

In order to increase the robustness of the classifier and speed up the learning of object models with varying appearance, we update the classifiers not only with the object hypotheses (image windows x) given by itself, and annotated then by the human user, but that additionally we create a set of new positive and negative samples that enlarge the training data. This is done by applying small distortions like image shifts on those detection windows. For this work, we consider 10 new samples per window.


                        Incremental object learning. This first experiment consists in learning and detecting one single object in a cluttered scene. Fig. 10
                         shows some examples. Particularly, this experiment has over 2000 images containing the object (beer bottle) at diverse locations and viewpoints. The user initializes the classifier with the first frame. The top row of Fig. 10 corresponds to the output of the proposed method (for 
                           
                              ξ
                              =
                              1.1
                           
                        ). This is compared in the bottom row with the detection results obtained by the approach Villamizar et al. (2012b), that combines active learning with self-learning. Results show that both methods are able to learn and detect the object through the whole sequence. This is indicated via the green rectangles around the object. Red rectangles are detection hypotheses that the user has manually labeled as incorrect during the interaction. Yet, despite the good performance of both methods, the proposed approach has the benefit that the amount of human assistance is significantly reduced. This is shown in Fig. 12-left, where the percentages of human assistance are displayed together with the active learning approach. Our method obtains the lowest rate of human assistance while correctly detects the object in all frames.

Additionally, Fig. 12-right shows the uncertainty thresholds for the aforementioned learning approaches. Observe that our method gradually reduces the uncertainty threshold during the learning step while the active learning and Villamizar et al. (2012b) maintain a constant value (
                           
                              θ
                              =
                              0.15
                           
                        ).


                        Interactive face recognition. In this second experiment, the proposed method is focused on the detection and identification of faces for two persons interacting with our robotic platform. Fig. 11
                         shows some detection results. Similar to the previous experiment, the classifiers are interactively trained using human assistance. In this case, the method learns and detects two people simultaneously while they interact with the robot. This contrasts to Villamizar et al. (2012b) where the classifiers are independently computed and one at a time. The displayed sequence snapshots show that our method can effectively and simultaneously learn multiple faces and detect them in the subsequent frames. Besides, the approach retrieves people identity since each classifier is specialized in one person. This issue is shown by the small images beside the detection boxes. Observe that in the first video frame (at top-left of the figure), there exist several incorrect hypotheses (red boxes) that require the human intervention to label them as false positives. This occurs because the classifier is initialized in this frame and thus its confidence is very low. This issue is removed in future frames as the confidence of the classifier gets larger.

Furthermore, the method runs in real time, except for the assistance periods, where the system stops the video capture waiting for human answer. If after a while the robot does not receive response, the system resumes the video capture but without updating the classifier. Technically speaking, the method runs on about 13 fps using a C++ implementation6. It is noteworthy that the proposed method does not use neither temporal information nor tracking techniques that help to speed up the object detection.


                        Object recognition in urban settings. In our third experiment, the method is evaluated for learning and detecting multiple objects in urban scenarios. Fig. 13
                         shows two example images where the robot is moving in urban areas. During the tour the user teaches the robot some objects of interest in order to be learned and detected.


                        Fig. 14
                         depicts some sample images showing the performance of the classifiers and the ability of the proposed method to learn several urban objects in real time and interactively when the robot navigates within the environment. We can see that objects like cars, doors and buildings are easily learned and recognized by the system. They are represented in the images by colored boxes and small patches at the right side. The images also contain the percentage of human supervision (indicated by the red hand), the uncertainty threshold θ, and the incremental performance of the classifier λ. Notice that the proposed method is capable of learning and detecting multiple objects, with a few false positives (indicated by red circles), and scarce human assistance.

With regards to the computational time, Fig. 15 plots the running times (in frames per second) of the proposed method as a function of the number of object classifiers. Note that the computational cost increases as the number of objects gets larger. However, learning and detect 20 objects at four frames per second is a remarkable and promising result, especially for current robotic tasks involving online learning and real-time performance. The reason that the method slows down its recognition speed, in spite that all classifiers share the same features, is because all classifiers must be evaluated using the sliding window approach. This process depends of the number of classifiers and involves the combination of fern probabilities per each classifier (see Eq. (4)).


                        Object tracking. In this section, the approach is evaluated on public datasets focused on tracking objects under varying conditions such as illumination changes, occlusions and rotations. The objective is to compare the method with other approaches, even knowing that our method is not specifically designed for object tracking. Our method for example does not use temporal information to reduce the search space and to remove false positives (Grabner and Bischof, 2006; Kalal et al., 2010). However, we consider that object tracking is a good scenario to show the importance of the human intervention to cope with drifting problems.

First, we evaluate our approach in the BoBot object dataset (Klein et al., 2010) which contains various daily objects such as cups and toys. Here, we consider just five objects from the entire dataset: two cups, a rubikscube, a stuffed panda and a juice box. Refer to Fig. 16
                         to observe these objects under varying imagining conditions.

The detection results of our method in this dataset are summarized in Table 2
                        , where we can see that the method performs remarkably well. In most cases, the method achives a detection rate of 100% using the Equal Error Rate (EER) over the precision-recall plot. The table also includes the amounts of true positives (TPs) and false positives (FPs), the number of frames, and the percentages of human assistance in each case. Notice that these values are really small.

In the sequences of cup 2 and rubikscube, the method obtains lower detection rates because the classifiers are updated with samples including small portions of background. This occurs mainly because the object is seen from multiple views and the classifier must be adapted to new and unknown object appearance.


                        Fig. 16 shows some sample images with the output of the proposed method in each video sequence. Blue boxes correspond to the ground truth while green rectangles are the response of the method. Magenta rectangles make reference to hypotheses considered by the human as background. We see that the method is able to learn and detect the objects with a few human-labeled samples and that it is robust to cluttered backgrounds, camera viewpoint changes and lighting conditions (as shown in the stuffed panda sequence).

The proposed method has also been evaluated on the PROST dataset Santner et al. (2010). This dataset contains four different objects under difficult conditions such as occlusions, blurring, and 3D rotations. In this experiment, we evaluate the proposed approach on two object sequences: box and lemming. Some instances of these objects are shown in Fig. 17
                        
                        .


                        Table 3
                         shows the detection results of our method using the percentage of correct detections (PASCAL score) and the average euclidean distance between the detection boxes and the ground truth (Santner et al., 2010). The table also provides an extensive comparison with various recognized approaches in the state of the art for tracking purposes. Note that the proposed method outperforms these works on both object sequences. This is because our method uses human assistance to cope with the drifting problem. As a result, the classifier continues learning the object appearance with correct hypotheses and using little human intervention. Specifically, the method needs human assistances of 3.7% and 9.4% for the box and lemming objects, respectively.


                        Fig. 17 illustrates the online detection performance of our method and conventional trackers in the literature (see Table 3). Observe that our method detects the objects in almost all frames (black boxes). The PROST (Santner et al., 2010) and GRAD (Klein and Cremers, 2011) trackers also achieve high recognition rates on these sequence. However, these methods are devoted to tracking single objects while our approach is able to compute multiple object detectors on the fly. With respect to the other tracking methods, lower recognition rates are reported since they are prone to drifting, and thus, miss object instances in some frames. This is particularly evident for the lemming sequence that exhibits object rotations and occlusions.


                        People recognition under shadows. Here, the proposed approach is evaluated for people detection under difficult illumination conditions. More precisely, the method is tested on a video sequence with 348 images that include a person walking in an urban scenario with varying shadows and background. Fig. 18
                         shows some video frames. For this experiment, we use a normalized object height of 50 pixels.

For comparison purposes, we have evaluated the TLD tracker
                           7
                        
                        
                           7
                           Code available at http://personal.ee.surrey.ac.uk/Personal/Z.Kalal/tld.html.
                        on this sequence. This tracker has shown excellent results in recent years for online learning and detection Kalal et al. (2010). The recall and precision rates of this tracker and the proposed method are indicated in the Table 4
                        . We see that our method achieves perfect recognition rates (
                           
                              ξ
                              =
                              0.95
                           
                        ) but at the expense of a higher degree of human intervention. On the other hand, the TLD tracker provides small detection rates, especially in terms of recall, since the classifier is not able to adapt to the abrupt changes in the image caused by shadows and the illumination conditions. This is observed in Fig. 18, where the tracker only detects the person at the beginning and final of the video sequence (when the loop is closed). By contrast, our approach is able to detect the person in most video frames thanks to the human assistance is used to remove false positives and update the classifier.


                        Online learning with occlusions. Finally, the method is evaluated in a standard video used for object tracking under occlusions Babenko et al. (2011). This video has 886 images containing a woman under multiple occlusions with different degrees of difficulty. This experiment shows the adaptability of the method and the robustness to drifting. Fig. 19
                         shows some snapshots of this sequence and our detection results. At the top row, we see the output of the proposed approach (green boxes) along with the output of the MILTrack method Babenko et al. (2011), indicated by blue boxes. Here, we see that the latter method is prone to suffer of drifting, as shown in the fourth image. The bounding box has been displaced from the face’s center. By contrast, our method is robust to drifting since the human assistance resolves the ambiguous cases. In the middle row of the figure, the confidence of the classifier is plotted. Note that the confidence changes according to the level of difficulty. In cases with occlusions, the confidence is low and requests human intervention (snapshots 1 and 3). At the bottom row, we plot the percentage of human supervision through the sequence. Again, we see that the method reduces gradually the overall amount of human intervention.

@&#CONCLUSIONS@&#

In this work, we have presented a novel approach for human-robot interaction to interactively learn the appearance model of multiple objects in real time using scanty human supervision. The proposed method uses efficient and reliable random trees classifiers to compute object detectors on the fly and which are progressively refined with the human assistance. The proposed method also includes an uncertainty-based active learning strategy that reduces the amount of human intervention while it maintains high recognition rates. The method has been evaluated extensively in both synthetic and real-life different scenarios such as 2D classification, face recognition, and the learning and detection of contextual objects in urban settings using an autonomous mobile robot.

@&#ACKNOWLEDGMENTS@&#

This work has been partially funded by the Spanish Ministry of Economy and Competitiveness under projects ERA-Net Chistera project ViSen PCIN-2013-047, RobInstruct TIN2014-58178-R, ROBOT-INT-COOP DPI2013-42458-P, and by the EU project AEROARMS H2020-ICT-2014-1-644271.

@&#REFERENCES@&#

