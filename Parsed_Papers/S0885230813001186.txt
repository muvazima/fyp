@&#MAIN-TITLE@&#Exploring speech retrieval from meetings using the AMI corpus

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           We created an ad hoc speech retrieval test set for meeting retrieval experiments.


                        
                        
                           
                           Report retrieval results for diverse segmentations of ASR and manual transcripts.


                        
                        
                           
                           WRR, segment length and proportion of relevant content in the segment are crucial.


                        
                        
                           
                           Detailed comparative analysis of results using multiple evaluation metrics.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Speech retrieval

Recall-focused information retrieval

Informal spoken content search

Retrieval unit segmentation

@&#ABSTRACT@&#


               
               
                  Increasing amounts of informal spoken content are being collected, e.g. recordings of meetings, lectures and personal data sources. The amount of this content being captured and the difficulties of manually searching audio data mean that efficient automated search tools are of increasing importance if its full potential is to be realized. Much existing work on speech search has focused on retrieval of clearly defined document units in ad hoc search tasks. We investigate search of informal speech content using an extended version of the AMI meeting collection. A retrieval collection was constructed by augmenting the AMI corpus with a set of ad hoc search requests and manually identified relevant regions of the recorded meetings. Unlike standard ad hoc information retrieval focussing primarily on precision, we assume a recall-focused search scenario of a user seeking to retrieve a particular incident occurring within meetings relevant to the query. We explore the relationship between automatic speech recognition (ASR) accuracy, automated segmentation of the meeting into retrieval units and retrieval behaviour with respect to both precision and recall. Experimental retrieval results show that while averaged retrieval effectiveness is generally comparable in terms of precision for automatically extracted segments for manual content transcripts and ASR transcripts with high recognition accuracy, segments with poor recognition quality become very hard to retrieve and may fall below the retrieval rank position to which a user is willing search. These changes impact on system effectiveness for recall-focused search tasks. Varied ASR quality across the relevant and non-relevant data means that the rank of some well-recognized relevant segments is actually promoted for ASR transcripts compared to manual ones. This effect is not revealed by the averaged precision based retrieval evaluation metrics typically used for evaluation of speech retrieval. However such variations in the ranks of relevant segments can impact considerably on the experience of the user in terms of the order in which retrieved content is presented. Analysis of our results reveals that while relevant longer segments are generally more robust to ASR errors, and consequentially retrieved at higher ranks, this is often at the expense of the user needing to engage in longer content playback to locate the relevant content in the audio recording. Our overall conclusion being that it is desirable to minimize the length of retrieval units containing relevant content while seeking to maintain high ranking of these items.
               
            

@&#INTRODUCTION@&#

Increasing amounts of spoken content are being captured and archived from a wide variety of sources. The inefficiency of manually searching audio recordings means that if this material is to realize its full potential, effective automated retrieval techniques are required to enable users to access relevant content in an efficient manner. The form of this content and the amount of information to be retrieved to satisfy user needs vary greatly. These factors are important when considering the appropriate way to index and search this content. Retrieval from a collection of well defined spoken documents that are clearly articulated in a quiet environment, and for which high accuracy automated transcripts can be derived, can generally be handled in the same manner as a standard text document retrieval task (Garofolo et al., 2000). Other types of spoken content pose much greater challenges presenting a range of potential barriers to effective automated search.

One of the key factors is that the content may only be informally structured making the definition of the retrieval units a complex task. Spoken items may cover multiple topics, meaning that it can be hard to retrieve relevant content reliably. Such multi-topic items can sometimes be too large to audition efficiently as a single item to locate relevant regions within the audio content. In addition to the structural issues relating to the content, the speech may be casually spoken in a spontaneous manner in a noisy environment with significant amounts of cross-talk between multiple speakers. This often greatly reduces the recognition accuracy of automated speech recognition (ASR) systems leading to increased noise in the transcripts which can affect retrieval behaviour. Finally, in some situations, for example towards the end of a series of meetings working on a project or a programme of lectures, the actual spoken content may assume much knowledge of the subject under discussion. This can result in an absence of content words to facilitate reliable retrieval. The combination of these factors means that speech retrieval in some situations is a very challenging task.

Our project IISSCoS
                        1
                     
                     
                        1
                        
                           http://www.cdvp.dcu.ie/IISSCoS/
                        
                      was focused on the development of methods to improve retrieval reliability for challenging spoken data sources incorporating poor speech quality, lack of structure and informal content. A key component of achieving our project objectives was to fully understand the extent and impact of these issues on retrieval behaviour and establish search baselines against which we can demonstrate the effectiveness of novel techniques as we develop them. While existing studies of speech retrieval have worked on data and tasks which contain some or all of these problems, there is a lack of detailed analysis of the results obtained and the reasons underlying them. In particular differences between retrieval for manual and ASR transcripts have generally only been compared at a high level using standard metrics of precision and recall (Garofolo et al., 2000; Pecina et al., 2008). Other investigations using spoken corpora have looked at speech recognition quality (Renals et al., 2007), content segmentation (Hsueh and Moore, 2006; Malioutov and Barzilay, 2006) and user interaction with meeting contents (Glass et al., 2007; Repp et al., 2008). While these latter studies are clearly relevant to retrieval scenarios, their work has not been carried out and examined within experimental retrieval tasks.

In this paper we focus on a detailed examination of the task of retrieval from recordings of meetings via a study using the AMI corpus (Carletta, 2007). Search of meetings is an interesting task for speech retrieval since it incorporates all the challenging issues highlighted above. Content may be spoken in a wide range of often informal spontaneous styles, topic boundaries will generally not be clearly defined, and participants frequently know that others in the meeting are fully cognisant of the topic under discussion and will not articulate many of the details. Additionally meetings are often very long, covering multiple topics implying that identifying meaningful search units within them is needed to facilitate fine granularity retrieval and content access efficiency. The experiments described in this paper explore topical segmentation and ad hoc retrieval of the resulting segments. This investigation focuses particularly on a comparison of retrieval behaviour for different segmentations of manual and ASR transcripts.

Existing studies of speech search including Garofolo et al. (2000), Pecina et al. (2008) have focused on standard ad hoc search scenarios where a number of retrieval “document” units are regarded as relevant to the user's search query. This scenario assumes that all relevant documents are equally able to address the user's information need. In this situation since all relevant documents are deemed equally valuable for this purpose, the relevant documents are effectively assumed to be interchangeable. Comparison of retrieval effectiveness for manual and automated transcripts in these cases generally shows that the averaged ranking of relevant documents is largely robust to significant levels of error in the ASR process. However, this is only one possible retrieval scenario in terms of user information needs. In this investigation we concern ourselves with a recall-focused scenario where a user is potentially looking for all relevant topical segments. This is analogous to patent search or e-discovery where many documents may be relevant to the topic of the user's query, but the relevant information in each document may be different. For example, in the case of patent search a single document may uniquely demonstrate that an invention is not novel, or in the case of e-discovery contain some details of an illegal enterprise activity.

In the case of meeting recordings, while a number of meeting segments may be relevant to the query, only one of them may contain the key information that is needed, for example how a decision was made, and the searcher may need to locate these specific relevant details from among the relevant content. Similarly for search of lectures or other presentations, the searcher may need to locate specific details within the relevant content or view these details in the context of a sequence of relevant materials in order to properly interpret them. Part of the solution in these cases is often to arrange them in the correct temporal sequence, but efficient access also requires that relevant content be identified. Failure to retrieve all relevant segments in these cases may mean that the key information is missing or prevent the searcher from identifying or correctly interpreting this information. Our investigation focuses on exploring the impact of content segmentation methods and ASR errors on the ranking and retrievability of relevant content in this setting, and the efficiency of information access. The results of this study enable us to identify the features of retrieval units which will maximize average retrieval effectiveness and efficiency of user access to relevant content, and the challenges presented to spoken content retrieval (SCR) systems in satisfying these requirements.

This paper is structured as follows: Section 2 reviews relevant existing work in SCR, Section 3 outlines the task of retrieval from audio recordings of meetings and the background to the AMI corpus, Section 4 describes details of our meeting search test collection based on the AMI corpus, Section 5 reports results and analysis of our investigation of searching the AMI corpus, and finally Section 6 gives conclusions of our study and outlines directions for our future work.

Speech retrieval has been the subject of a number of previous experimental studies. While this work has identified some of the important challenges and features of speech retrieval, existing investigations have been limited in their scope in terms of user retrieval requirements and the depth of analysis of retrieval behaviour of automatically indexed spoken content. In this section we review key features and results from this existing work, and highlight its shortcomings which we seek to address in this paper. In particular we focus here on the relationship between speech recognition word error rate (WER) in automated transcription of spoken content and retrieval effectiveness for retrieval based on these transcripts, and the retrieval behaviour of alternative retrieval units.

Speech retrieval research has predominantly focused on search for relevant spoken content where the retrieval units are clearly defined document units. Following some early work using small private collections, e.g. a set of a few hours of radio news stories (James, 1995), this work is probably best exemplified by the Spoken Document Retrieval (SDR) track at TREC which ran for 4 years in the late 1990s (Garofolo et al., 2000). In the final two years at TREC 8 and TREC 9, the task used a collection of television and radio news broadcasts lasting several hundred hours. These were manually segmented into carefully defined story units prior to retrieval. While the SDR track had an unknown story boundaries condition to explore retrieval without fixed story boundaries, the source of this data meant that the underlying content was explicitly divided into story units. Speech retrieval experiments were carried out for a standard ah hoc retrieval task using a provided set of search topics with a known-item retrieval for TREC 6 and a varied number of relevant documents for each topic for TREC 7–9 identified by manual assessment using a pooling procedure. The main evaluation metric for these experiments was mean average precision (MAP). Examination of the results of the TREC SDR tracks showed that for this task SDR effectiveness, as measured by high ranked precision cut off and MAP, is largely robust to reasonably high levels of speech recognition errors. The results also demonstrated that by applying techniques such as relevance feedback, comparable performance can be achieved to that for accurate manual transcriptions of the speech data. At the end of four years of the TREC SDR task, speech retrieval was declared to be a largely solved problem with the remaining challenges being in tasks such as question-answering on speech data and spoken queries (Garofolo et al., 2000). However, this conclusion overlooks the fundamental difference in the nature of informal spoken content from written text or scripted speech content, that creates speech recognition challenges as the content is not well matched to the vocabulary of the ASR system, and is recorded in challenging environments.

More recent research in speech retrieval has explored search of less formally structured spoken material. One example of this work was the investigation of search of the Malach collection (Pecina et al., 2008). This data consists of interviews with survivors and witnesses of the Holocaust (Byrne et al., 2004). Interviews were manually divided into meaningful segments, and were augmented by including a number of sources of manually and automatically generated metadata for each “document” unit which provided additional description of the spoken content. This metadata included manually and automatically assigned keywords and short manually prepared expert summaries for each document. Experiments showed that even with ongoing improvements in speech recognition accuracy, speech retrieval for more complex speech sources, such as the Malach collection, still presents significant challenges. In such cases the spoken content itself may not be sufficient to enable effective search without augmentation with additional metadata. This problem arises since the speakers may discuss topics without providing details such as names of people or places required to support effective retrieval. It was found in experiments that retrieval for this task is enhanced greatly by including manually generated metadata in the search index and improved marginally by including the automatically generated metadata. In contrast to spoken content of this type, broadcast news can be referred to for retrieval purposes as “self describing,” in that the entities are referred to explicitly each time a story appears in a news broadcast.

More recently attention has begun to focus on other challenging tasks such as search of recordings of meetings or lectures. The FAME interaction system was created to serve as a potential assistant providing access to multimedia information using multi-modal cues, for example when researchers need help in their choice when seeking interesting talks at conferences or require some details contained in one of them (Metze et al., 2005). The system was able to return not only whole recordings, but also segments of the recordings, the latter were preprocessed offline. The system was demonstrated on recordings of four days of seminars on Language Technology and Language, Cognition, and Evolution. However, no information on the retrieval effectiveness of the system is available.

One of the most significant activities in the area of meeting search has been the AMI and AMIDA projects (Renals et al., 2007, 2008) which coordinated collection of the AMI meeting corpus used in our study. The AMI corpus is very carefully collected and documented, as described in Section 4.1. Research in the AMI and AMIDA projects has focused on the development of effective ASR tools for this data, and on means to provide structure to the content by automatically segmenting the content into topical segments, assigning topical labels and summarization of meetings (Renals et al., 2007). Studies of search relating to the AMI dataset have focused on search and navigation within individual or linked meeting recordings. While such strategies may be sufficient where the dataset is relatively small and the contents reasonably familiar to the users, it is unlikely to be effective for larger unknown datasets. To date no study has appeared exploring a more standard ad hoc speech retrieval task for this collection.

One application of search on meeting data is provided in Chibelushi and Thelwall (2009) which examines the mining of meeting transcripts to identify elements within the meeting associated with points related to the making of key decisions. This application is an example of the situation introduced in Section 1 where high recall is required, and all relevant content needs to be identified, since it is not clear exactly which elements of the relevant content will be required by the searcher.

Another application area for speech retrieval of increasing interest is management of lecture recordings. This work has primarily focused on the indexing and structuring of content relating mainly to recordings of the spoken content in conjunction with metadata such as slides used in the presentation (Jones and Edens, 2002). This has again explored issues such as topical segmentation of the content and user interfaces for browsing and interacting with the content (Glass et al., 2007; Repp et al., 2008). The recent SpokenDoc task at NTCIR 9 and 10 introduced an ad hoc speech retrieval task for recordings of Japanese language lectures (Akiba et al., 2011, 2013). This provided a challenging retrieval setting. Participants results suggest that this task exhibits many of the issues examined in this paper for meeting retrieval. Search of unstructured call centre recordings has been explored in Mamou et al. (2006) and a telephone conversation archive in Chia et al. (2010). Similar to Jones et al. (1996), these studies showed the effectiveness of recognition lattice structures to compensate for recognition errors in ASR. However, this work did not examine the specific factors impacting on the retrieval behaviour of individual relevant documents between manual and ASR transcripts.

The study reported in this paper extends existing work using the AMI corpus to explore an ad hoc retrieval task requiring high recall. Thus we apply segmentation to the meeting transcripts to extract retrieval units and then examine the retrieval behaviour of the relevant and non-relevant segments. An important issue in this investigation is the impact of speech recognition errors on retrieval behaviour. The next section summarizes existing work examining this topic.

ASR is an important component for most speech retrieval applications since the cost of manual transcription means that in practice speech retrieval must generally rely on automated indexing methods. An important question with respect to speech retrieval research is the effect on search behaviour of the errors made by the ASR system. Using a general purpose ASR system will generally produce speech transcripts which are inferior to those that can be created using an ASR which has been trained for the specific recognition task to be undertaken (Renals et al., 2008; Byrne et al., 2004). The basic relationship between average transcription quality and accurate (or near accurate) manual transcriptions is reported in most speech retrieval studies. For example, the TREC SDR track illustrated how the relatively low average recognition error rates on the radio and TV news material used for these studies resulted in little loss in retrieval effectiveness, as measured by MAP, for a news retrieval tasks compared to still errorful, but much more accurate manual transcripts (Garofolo et al., 2000).

An interesting and careful examination of the differences in retrieval behaviour of documents with different speech transcript accuracy levels for the results of the TREC 7 SDR task is described in (Shou et al., 2003; Sanderson and Shou, 2007). The analysis of the distribution of the error rates in the ranked lists retrieved for topics in this task shows a general tendency for documents with low WERs to be retrieved at higher ranks, independent of document relevance to the search query. A natural consequence of this observation is that relevant items with high WERs could be expected to be retrieved at low ranks or not retrieved at all, although the extent to which this occurs and the effect of ASR errors on the recall in ranked lists was not explored in this work, or in any other work of which we are aware.

The impact of the errors according to their types has been measured using different quality metrics on the level of the document or on the level of the whole collection. Such metrics as Named Entity WER and Named Entity Mean Story WER for Cross-Recognizer Results showed the best correlation with retrieval performance (Garofolo et al., 1999). The global semantic distortion metric based on the vector space model and focusing on various types of substitutions (frequent vs. infrequent, semantically similar vs dissimilar) revealed a higher impact of infrequent and semantically dissimilar substitution errors on retrieval behaviour (Larson et al., 2009). We are interested in further exploring the relationship between ASR accuracy and the retrieval behaviour in speech search for tasks. For example, non-relevant documents will have similar variations in WER to relevant ones, and this factor can be expected to interact with query-document matching scores to affect the rank of both relevant and non-relevant content. Thus, it can be expected that non-relevant content with high transcript WER may be ranked lower than that with a lower WER. Thus, changes in rank position between accurate and noisy transcripts are to be subject to a range of interacting factors. This aspect of retrieval has not previously been examined in detail.

In addition, we also explore speech retrieval where recall of relevant documents is a significant concern. We are not aware of any existing studies which have explored speech retrieval from the perspective of a recall focused task. The next section overviews archiving and accessing information in meetings, after which the following section describes the features and design of our experimental meeting search task based on the AMI corpus.

Meetings are a vital source of information within many organizations. The value of meetings often relies on the ability of individuals to access the information contained within them. Traditionally one participant in a formal meeting is assigned to take minutes which summarize the activities and conclusions of the meeting, in the case of more informal meetings there is often no record of the proceedings. Even when taken, minutes often record only the key elements of the discussions and decisions reached, as understood at the time of the meeting by the person responsible for the minutes. Thus, minutes may be deficient or inaccurate, if the minute taker misunderstands some elements of the discussion or the future significance of some part of the meeting is not apparent to the participants and no record is kept. The correctness of the minutes is often formally checked by an approval process, but this does not guarantee accuracy or impartiality, or address the problem of incompleteness. If in the future someone wants to know the process by which a decision was made or how a particular idea arose, they may find that this information is missing from the minutes. However, if recordings of the meetings have been made, participants and others can potentially play back parts of a meeting to access specific information. But in order for this to be a realistic option, an efficient mechanism for locating relevant content must exist.

As outlined in Section 1, recordings of meetings are a challenging speech search environment encapsulating many of the issues arising in search of informal unstructured content. The most simple search scenario would be simply to take transcripts of complete meetings and use these as the search unit. However, meetings are often very long, lasting anything from a few minutes to several hours, and will often cover many topics, some related to each other and some very distinct. It is more sensible to think in terms of breaking meetings into smaller semantically focused units and to use these as the search “documents” in a speech retrieval setting. We can then seek to retrieve relevant search units from this document collection and direct the user effectively to this content to efficiently address their information need using a suitable content browser application (Popescu-Belis et al., 2012).

An alternative scenario, as introduced by Popescu-Belis et al. (2009) is query-free or just-in-time retrieval (the AMIDA Automatic Content Linking Device). This system transcribes an on-going meeting automatically, and uses the transcript to perform searches of previous meetings and additional material at regular intervals. The system adopts a scenario of standard ad hoc query-based offline search to seek relevant (parts of) meetings where a certain topic was being discussed. This can be used by participants in a current meeting to look up features from earlier meetings or later to search for details in a meeting archive. This is an interesting mode of use since it means that material which may have been forgotten about, possibly since it was not regarded as of interest during previous meetings, can be made available during a current discussion. Such content would otherwise only come to the user's attention if they remembered it while searching by browsing within a meeting or stumbled across it by chance when exploring recordings of meetings. This proactive search process may result in greater efficiency in avoiding repeating arguments in subsequent meetings, discovering insights from previous discussions which were perhaps not apparent at the time of the original meeting, and may ultimately potentially lead to better decision making or management efficiency. In order to achieve objectives such as these, search of meetings needs to support high recall retrieval enabling any content which may be relevant to be located. It is the objective of recall-focused search of meetings that forms the focus of our experimental investigation described in this paper.

This section describes the construction of the search collection used for our investigation. The section begins by overviewing the AMI corpus which forms the basis of the search collection. It then describes the preprocessing of the meeting recordings within the AMI corpus to divide individual meetings into various types of potential retrieval units. The section concludes with details of the construction of search queries and corresponding relevance assessments to complete the collection.

Experimental investigation and evaluation of information retrieval applications requires a suitable search collection. For meeting search this must include a rich and well organized dataset of recorded meetings. Construction of such a meeting collection is a complex and expensive process requiring planning of the meetings which will be recorded, but also if an analysis is to be made of the impact of speech recognition errors on retrieval behaviour, a full accurate manual transcript of each meeting. Obtaining such a meeting collection and forming an accurate manual transcript of speech recordings is a very expensive process. Considering these factors our current experiments are carried out using the AMI corpus, collected as part of the AMI project and made publicly available for research purposes.

The AMI corpus
                           2
                        
                        
                           2
                           
                              http://www.amiproject.org/
                           
                         contains 100h of annotated recordings of planned meetings (Carletta, 2007). Meetings last about 30minutes each, 70% of them simulate a project meeting on product design. Meetings usually involve 4 participants, and were recorded using 6 cameras and 12 microphones: 1 headset microphone for each speaker, and an 8-element circular microphone array. For the majority of the meetings, both manual and automatic transcripts are provided, for the latter the developer of the corpus created an ASR system which makes use of a standard ASR framework employing hidden Markov model (HMM) based acoustic modelling and n-gram based language models (LMs) (Renals et al., 2007). The dataset also includes additional materials including slides projected during the meetings. In this study we use the AMI release 1.4 (automatic and manual transcripts) and release 1.5 (automatic segmentation results for ASR transcripts).

It should be noted that since the meetings contained in the AMI corpus are planned and based on predefined scenarios, they are somewhat artificial. The participants are not engaging in their real employment roles and there is no history of the issues being discussed prior to the sequence of meetings between the participants in the context of the AMI meeting sessions, or future implications of any decisions made. Also the participants did not know each other in the roles that they are taking prior to their participation in these meetings. Thus the value and significance that would be associated with decisions in a real working environment is not present, and the participants will perhaps not engage in the same way that individuals working with each other regularly on a long term basis might. While concerns such as these can be identified with regard to the AMI corpus, it is the best resource currently available to us for experimentation of meeting search from a large well documented collection, and, we believe, does capture the elements of natural meetings that we require for our investigation, In particular each meeting covers a number of topical areas with variation of speech delivery styles. Since the scenario-based meetings were recorded in four sessions (4 separate meetings) each group of participants had developed some history of interactions in their assigned roles over the course of the meetings. These resulted in natural speech behaviour including monologues, dialogues and multiparty discussions.

In order to utilize the AMI corpus for our investigation of speech search, it must be preprocessed to segment it into suitable topical search units. Our long term objective is to develop tools to support effective search of arbitrary recordings of meetings relating to discussion of multiple topics across multiple meetings. Since in this situation manual topical segmentation of meetings is unlikely to be available for financial or practical reasons, we need to use an automated segmentation process. In this section we describe the preprocessing steps applied to the AMI meeting datasets to form the retrieval collection for our experimental investigation. Since one of the key elements of our experiments is to compare retrieval of manual and automatic transcripts provided with the AMI corpus, the following procedures were applied to both the provided manual and automatic transcripts.

Since it is not practical to manually assess the relevance of all content within a collection to a query, information retrieval research typically adopts a pooling procedure, where only a subset of items deemed most likely to be relevant to the query are assessed. In this process a pool is formed by merging the highest ranking items retrieved by a number of runs using alternative retrieval methods. For our investigation the AMI collection must first be preprocessed into retrieval units prior to these retrieval runs. Thus, we begin this section with a description of our preprocessing procedure including text normalization and automatic segmentation methods.

The meeting transcripts in the AMI corpus are published separately for each speaker participating in the whole meeting. Since we are interested in exploring retrieval from the meeting, rather than information relating to each speaker, we first merged the per speaker transcripts using the time marking data provided in the corpus to form a single transcript file for each meeting. We omitted incompletely transcribed meetings, and used only the fully transcribed ones in this study, since we wished to work with only complete meeting transcripts. This gave us a total of 160 meetings for our experiments.

For the purpose of further comparison between manual and ASR transcripts, word recognition rate (WRR) was calculated as the proportion of words in the manual transcripts recognized correctly in the ASR transcripts. As the retrieval systems usually index stemmed versions of words, we also calculated the WRR after running a publicly available implementation of the Porter stemming algorithm
                              3
                           
                           
                              3
                              
                                 http://tartarus.org/~martin/PorterStemmer/python.txt
                              
                            on the transcript (Porter, 1980). Comparison of the transcripts showed that the automatic transcripts have accuracy ranging between 71–93%, averaging 85% of the manual transcripts. The stemming of the words did not significantly affect the average WRR across the corpus (87%), however it increased the WRR for some of the files resulting in a stem recognition rate (SRR) ranging between 74–94%. It can be observed that the recognition rate often varies over the length of a meeting transcript, sometimes dropping much below 45% for reasonably long sections of the transcript. While MAP has generally been shown to be robust to WRRs as low as 70%, WRRs lower than this have been shown to impact on considerably retrieval effectiveness (Garofolo et al., 2000). Thus we can note that while sections of the ASR transcripts are likely to be retrieved with good reliability, assuming that they match the content of search queries well and are sufficiently selective to distinguish them from other ones; other regions of the transcript with poor WRR are likely to present significant challenges for retrieval.

The meetings in the AMI corpus have an average duration of approximately 30-min, which is too long to expect the user to listen to them in their entirety or even to read through a transcript in their search for relevant information. Additionally since they cover multiple topics, it is likely to be difficult to distinguish between similar multi-topic meeting transcripts in retrieval to identify those containing relevant information. Thus as noted previously, we need to segment the meetings to identify suitable retrieval units. This segmentation is motivated both by the need for segment cohesion to promote the selectivity of relevant content in retrieval, and also for efficient location of specific information of interest within a relevant segment. Segments might be linked together if they are related to the same topic to form a linked data structure as discussed in Rigamonti et al. (2007), but exploring this aspect of facilitating search within a collection is beyond the scope of our current study.

The AMI collection as provided already contains manually created topic segmentations of the transcript. Topics and subtopics form a hierarchical structure, and labels have been assigned by annotators choosing from a list of suggestions. This topic segmentation was made based on the manual transcripts, but does not cover all of the meetings in the dataset. These segments are provided for only a subset of 139 out of the total of 173 meetings. However, since we wished to use as many meetings as possible for our experiments, and our long-term goal is to use speech data for retrieval in cases where manual transcripts and manual segmentation are not available, we decided to automatically segment the AMI meeting transcripts ourselves. We used implementations of several statistical algorithms based on lexical cohesion (C99, TextTiling, Minimum Cut). We give a brief description of each of these algorithms in the following section.

We consider the manual transcripts which include punctuation and capitalisation as the gold standard of recognition. However, we decided not to use these punctuation marks and the capitalisation, since these are not generally available in ASR transcripts. Segmentation of the manual transcript represents the ideal case of input for the segmentation algorithm – corresponding to perfect automatic recognition transcripts. However, since the output of segmentation on the manual transcripts was not produced or checked manually, it cannot be claimed to be the best one possible on this data. While we refer to them as manual segments in our experimental study, this refers only to the transcript input to the segmentation algorithm, and not the segmentation method. Thus, although we consider the manual transcript to be the ideal version of the speech recognition transcript, we do not suppose that segments created automatically using it form a gold standard for retrieval experiments. However, we do take these manual segments as our baseline with the best possible segmentation that could be created automatically using these algorithms in the absence of ASR errors.

Tracking the influence of the ASR performance on retrieval behaviour of the segments is not possible if the two segmented collections have different segment boundary points. Thus we projected the segment borders of the manual transcript onto the ASR transcript by using the word timing information of the transcripts. This resulted in a third collection of segments (labelled asr_man in the following study) where the only difference between this and the manual transcript segment collection is that the content of the segments is formed from the ASR transcripts. Sometimes manual transcripts in the collection do not cover the whole region of the ASR transcripts since they do not include areas regarded as not relevant to the meetings by the manual transcribers, in these cases the additional words in the ASR transcript were placed in the adjoining manual segment.

The following subsections describe the segmentation of the meeting transcripts. This considers first segmentation based on lexical cohesion, and then more simple methods to form fixed length segments.

Over the last 20 years there has been ongoing interest in the development of methods for automated text segmentation. Two of the best known algorithms are based on lexical cohesion: TextTiling (Hearst, 1993) and C99 (Choi, 2000). Based on this work further methods have been developed specifically attempting to segment spoken content, for example LCSeg (Galley et al., 2003), the method of Hsueh and Moore (2006), Utiyama and Isahara (2001), Sharp and Chibelushi (2008), and the Minimum Cut (MCut) model of Malioutov and Barzilay (2006).

For our study we performed linear lexical cohesion based segmentation using the popular TextTiling and C99 methods, and the MCut algorithm that was shown to be effective for segmentation of informally structured spoken content (Malioutov and Barzilay, 2006).
                                 4
                              
                              
                                 4
                                 We use implementations of the algorithms available at http://morphadorner.northwestern.edu/morphadorner/textsegmenter/ with their default settings as described in Choi (2000). Since our focus is on understanding the challenges and behaviour of search on an unstructured meeting collection, we made no attempt to tune parameters to our dataset. As we were creating the retrieval collection on the basis of the provided corpus, it was not possible to create a manual ground truth segmentation of the whole collection tuned to the level of detail of the topic in each of the queries, since the optimal segmentation of this informally structured content can be query dependant.
                               We applied these on both manual and ASR transcripts. TextTiling computes the cosine similarity between adjacent fixed sized blocks of sentences. The C99 algorithm also calculates the similarity between sentences using a cosine similarity measure to form similarity matrix. In C99, the cosine scores are then replaced by the rank of the score in the local region and segmentation points assigned using a clustering procedure. MCut regards the segmentation problem as a graph-partitioning task that optimizes the normalized cut criterion.

The C99, TextTiling and Minimum Cut algorithms work with the fundamental unit of the sentence, placing segment boundaries between the end of one sentence and the start of the next one. Thus since we do not have punctuation, we perform segmentation for both manual and ASR transcripts using pseudo “sentences.” We found the average length of real sentences in the manual transcripts to be 15 words. We thus used this as the length of our pseudo-sentences. After application of the C99 algorithm, the total number of segments for the 160 meetings dataset was found to be 2462 for the ASR transcripts, and very similar, 2476, for the manual ones. This yielded an average word count per segment of approximately 351 and 347 respectively for the two transcripts sets. In terms of the absolute number of segments, it seems that the algorithm behaves in the same way for the different transcripts. However when the actual segmented files are compared, many instances of, sometimes large, variations in the number of segments are revealed between different transcripts of the same meeting. Less than 10% of the meetings were found to have the same number of segments for manual and ASR transcripts, some have significantly more segments in the ASR transcript or in the manual one. Even where the same overall total number of segments is found for a meeting, this does not mean that the borders are positioned in similar locations. Manual analysis of segmented files showed that within an individual file while a number of the segments may line up well between the transcripts, in other localized regions there are significant variations in the number of segments. We examined a number of factors to explain this effect, the one that appears to correlate in many cases is the word recognition rate of content words (WRRC), i.e. words that were taken into account by the segmentation algorithm. This revealed that consistent low WRRC across a region usually corresponds to the undersegmentation of the ASR transcript, i.e. there are more segments in the manual transcript for this region; while significant local variations in WRRC for short regions between average WRRC and low WRRC are generally associated with greater numbers of segments in the ASR transcript for this region.

The MCut algorithm requires as one of its parameters the number of the segments into with the input document is to be divided. The authors of the algorithm set this based on manual analysis of examples of their input data. Since we assume that there is no manual segmentation available, and that the desired number of segments will differ for each input file, we used the number of the segments generated by one of the other algorithms for each transcript file as the input parameter for the MCut algorithm. This resulted in runs named mcut_C99 for C99, and mcut_tt for TextTiling.

In order to examine the value of lexical cohesion based segmentation in our speech retrieval task, we also carried out segmentation of the manual and ASR transcripts based only on timing information and the number of words in a segment. For the time-based segmentation, segment boundaries were placed at regular intervals of 60, 90, 120, 150, 180s. These time intervals were chosen as the range from minimal to average length of automatically created segments. The time boundary points were applied with flexibility to prevent words at the boundaries being split between segments. Segmentation based on the number of words had two variations: all the words were taken into account and the segment point placed after every 100, 200, 300, 400, 500, 600, 700, 800, 900 and 1000 words; and alternatively only non-stop words that were actually used by the segmentation and retrieval system afterwards were considered when counting the segment length (100–1000) and marking the boundary (length_non_stop_words or length_nsw).
                                 5
                              
                              
                                 5
                                 Stop words were taken from the list at http://snowball.tartarus.org/algorithms/english/stop.txt
                                 
                              
                           


                              Table 1
                               summarizes the average length of segments created using the lexical cohesion based algorithms, fixed time intervals and the number of non stop words. As would be expected, the average number of words increases for longer time segments and with the number of non-stop words counted.

The AMI corpus does not provide search requests or relevance information to support research into retrieval effectiveness for an ad hoc search task. In order to facilitate our investigations we created our own search collection based on the AMI corpus, containing separate sets of development and test queries for the collection and corresponding relevance data.

Since the AMI corpus is a specially created dataset, most of the meetings were based on prepared scenarios. Among the pre-prepared materials for the scenarios are a number of PowerPoint slides which were projected during the discussions. These were used in many meetings thus creating different instances of conversations by different participants about the same topic. The topic on the projected slide could also have been discussed at other points in the meetings while the slide was not being projected.

For our investigation of meeting retrieval we assume the scenario of a meeting participant or a project manager wanting to find locations in meetings where the topic of a particular PowerPoint slide was discussed regardless of whether it was being projected at that time or not. Fig. 1
                            shows an example of the contents of a slide used as a search topic.

For the development and test query sets we took 35 of the PowerPoint slides provided with the AMI corpus as a topic set based on the following criteria: the text of the slide should be sufficiently long (more than 15 content words), diverse in structure (lists of actions, sentences describing work to be done), diverse in situation of use (beginning of the meeting or closing), having a different number of possible relevant documents (from uniquely used ones as in a known-item search to slides that were used in almost every set of 4 meetings). Further we divided the queries into development and test sets consisting of 10 and 25 queries respectively. The development set allowed us to experiment with different settings for the length-based segmentation methods and to choose the most interesting cases for comparison. The requirement in doing this was to choose segments of meaningful length to the user, closer to the beginning of the relevant content and for which relevant documents are most highly ranked. We used the default settings for the lexical cohesion based segmentation algorithms, since in a practical setting there would not be the opportunity to tune these algorithms for a particular dataset. Therefore the development set was used only to select the segments for use with the test query set.

The search task was to retrieve all segments relevant to the topic being discussed in the slide. It thus represents a recall-focused search task which aims to support meeting participants looking to find all discussed material relevant to each query slide. This relevant material may be taken from discussions by the same participants, or by participants in other discussions examining topically related issues. As explained earlier, while all this material is relevant to the query, the searcher may be looking for a single element within this content, e.g. a decision point or the originator of a proposal, and meaning that full recall is vital in this application.

In order to carry out our retrieval experiments, corresponding manual relevance assessments identifying the relevant content for each slide topic was generated using a pooling procedure as outlined previously.

The segments obtained using each segmentation technique from the manual transcripts were indexed for search using a version of the SMART information retrieval system
                              6
                           
                           
                              6
                              
                                 ftp://ftp.cs.cornell.edu/pub/smart/
                              
                            extended to use language modelling (a multinomial model with Jelinek–Mercer smoothing) with a uniform document
                              7
                           
                           
                              7
                              In this section documents refer to indexed segments.
                            prior probability (Hiemstra, 2001). Eq. (1) shows how a query q is scored against a document d within the extended SMART framework.


                           
                              
                                 (1)
                                 
                                    P
                                    (
                                    q
                                    |
                                    d
                                    )
                                    =
                                    
                                       ∏
                                       
                                          i
                                          =
                                          1
                                       
                                       n
                                    
                                    (
                                    
                                       λ
                                       i
                                    
                                    P
                                    (
                                    
                                       q
                                       i
                                    
                                    |
                                    d
                                    )
                                    +
                                    (
                                    1
                                    −
                                    
                                       λ
                                       i
                                    
                                    )
                                    P
                                    (
                                    
                                       q
                                       i
                                    
                                    )
                                    )
                                 
                              
                           where q
                           =(q
                           1, …
                           q
                           
                              n
                           ) is the query comprising of n query terms, P(q
                           
                              i
                           |d) is the probability of generating the ith query term from a given document d being estimated by the maximum likelihood, and P(q
                           
                              i
                           ) is the probability of generating it from the collection and is estimated by document frequency. The retrieval model used λ
                           
                              i
                           
                           =0.3 for all q
                           
                              i
                           , the particular value being optimized on the TREC-8 dataset (Voorhees and Harman, 2000). Stopwords were removed using the standard SMART stopword list. Words were stemmed using a variant of the Lovins stemmer (Lovins, 1968) which is packaged in SMART by default.

Using the meeting segments and the extended SMART framework, the pooling procedure was carried out as follows:
                              
                                 1.
                                 Retrieval runs were conducted for each topic using segments created using the different segmentation schemes on the manual meeting transcripts.

The top 50 retrieved results for each run were collected and compiled into a pool for each topic.

The union of the segments from the same meeting was then formed. If one meeting had several different segments in the pool, the whole area between the beginning of the very first of the segments and the end of the very last of the segments was combined in a single potentially relevant region for assessment. This meant that we were able to inspect as much potentially relevant content as possible in each meeting.

An interactive application was developed which highlighted the union combination of the retrieved segments in the original meeting that they belong to. The highlighting of the union regions in this way meant that the manual relevance assessment was made of regions that had not being retrieved within the top 50 segments by any segmentation scheme, but were temporally close to retrieved potentially relevant content.

Relevant regions of each retrieved union grouping were marked manually by an assessor using the application.

After the relevant region for the manual transcript for each topic had been labelled, this information was projected onto each segment unit based on time correspondence between them, in order to create individual qrel relevance files for each of the segmentation techniques with the exact relevant region for each segment marked. Thus for each segment we record the beginning and end points of any relevant content within the segment for both the manual and ASR created segments.

While this pooling procedure only uses results generated using one retrieval model, the diversity of the segmentation schemes used for the content means that we get a wide variety of content originating in segments generated using different methods. Additionally since the relevance labelling tool requires the assessor to examine all data from the beginning point of the first retrieved content for a meeting to the end of the last one, the assessor actually looks at content not retrieved by any schemes. This gives a greater coverage of the relevance assessment process than would otherwise be the case, and means that we are more reliably able to comment on issues associated with recall during the analysis of our experimental results.

The next section describes out experimental investigation using this search collection.

Information retrieval experiments were carried out using the extended version of the SMART system outlined in the previous section. Separate retrieval runs were carried out for segments derived from both the manual and ASR transcripts, and for the pseudo segment sets created from the manual transcript segments by replacing the contents with words from the same time period from the ASR transcript.

Selection of useful and meaningful evaluation metrics is an important component of information retrieval experiments. The details of these evaluation metrics used for our investigation with justifications for their use are described in the next subsections. This is followed by experimental results evaluated using these metrics and a detailed analysis of the results.

Information retrieval has traditionally been evaluated using measures of precision, recall and mean average precision (MAP). These give a measurement of the extent to which a retrieval system is able to retrieve the available relevant whole documents while not retrieving non-relevant whole documents (Büttcher et al., 2010). In our scenario the user is presented with the list of retrieved speech segments that s/he will listen to in order to find the relevant content. Therefore for our analysis we go beyond traditional metrics such as MAP that evaluate only the ranking of the results. We utilize three further metrics: mean Generalized Average Precision (mGAP) that incorporates the distance from the beginning of the retrieved segment to the start of the relevant content which we refer to as the “jump-in point” (Pecina et al., 2008), Mean Average Segment Precision (MASP) which takes into account the time the user will spend on listening to both relevant and irrelevant content, and Mean Average Segment Distance-weighted Precision (MASDwP) which extends MASP to incorporate the distance to the jump-in point (Eskevich et al., 2012). Another aspect of interest for our investigation is the precision of the relevant content within the individual retrieved segments, i.e. what proportion of the segment is relevant content.

Eq. (2) shows the definition of the standard average precision (AP) metric for a single query.


                           
                              
                                 (2)
                                 
                                    AP
                                    =
                                    
                                       1
                                       n
                                    
                                    .
                                    
                                       
                                          ∑
                                          
                                             r
                                             =
                                             1
                                          
                                          N
                                       
                                       P
                                       [
                                       r
                                       ]
                                       ·
                                       rel
                                       (
                                       r
                                       )
                                    
                                 
                              
                           where n is the number of relevant documents, N is the number of retrieved documents, P[r] is the precision at rank r (the number of relevant retrieved documents divided by the total number of retrieved documents), rel(r) is the relevance of the document (rel(r)=1 if document is relevant, rel(r)=0 if not). For our search collection, a meeting segment is counted as relevant if it contains any relevant content. MAP is computed by averaging AP across the topic set.

The mean Generalized Average Precision (mGAP) measure was originally introduced to incorporate the concept of partial relevance of a document (Kekalainen and Jarvelin, 2002). It was used for evaluation of speech retrieval effectiveness in the CLEF CL-SR Czech task (Pecina et al., 2008). The metric generalizes the relevance of hypothesized jump-in points to the beginning of the relevant content in relation to ground truth jump-in points by imposing a symmetric step-wise linearly decaying penalty function within a window of tolerance (we empirically chose a 60s window for this investigation). The calculation of GAP for a single query is,


                           
                              
                                 (3)
                                 
                                    GAP
                                    =
                                    
                                       1
                                       n
                                    
                                    .
                                    
                                       
                                          ∑
                                          
                                             r
                                             =
                                             1
                                          
                                          N
                                       
                                       P
                                       [
                                       r
                                       ]
                                       ·
                                       
                                          
                                             
                                                1
                                                −
                                                
                                                   
                                                      Penalty
                                                      ·
                                                      Granularity
                                                   
                                                   Window
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where P[r] is the document precision at rank r, Granularity is the step that is used to measure how far the retrieved jump-in point is from the ground truth relevant one and to penalize the result for longer distance (Granularity = 10s in this experiment); Window is the distance before and after the beginning of a relevant segment within which the result must appear, in order to be considered correctly retrieved; Penalty is the number of times the user has to move in time within the Window with the Granularity step, in order to get to the actual relevant jump-in point. Thus segments that make the user wait for longer than Window size before or after the actual relevant jump-in point are not considered relevant.

MASP is a modification of MAP, specifically adapted to speech retrieval when no pre-defined segmentation of retrieval units exists (Eskevich et al., 2012). It measures both the ranking quality and the segmentation quality with respect to relevance in a single score. Thus, the ideal state for MASP is not only to retrieve the relevant speech segments at the top of the ranked results list, but also to have each segment consist of 100% relevant speech data without including any non-relevant material. Segment precision (SP[r]) at rank r in MASP is calculated as follows:


                           
                              
                                 (4)
                                 
                                    SP
                                    [
                                    r
                                    ]
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             r
                                          
                                          rperiod
                                          (
                                          
                                             s
                                             i
                                          
                                          )
                                       
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             r
                                          
                                          length
                                          (
                                          
                                             s
                                             i
                                          
                                          )
                                       
                                    
                                 
                              
                           where length(s
                           
                              i
                           ) is the length of segment s
                           
                              i
                            in time units (minutes or seconds), and rperiod(s
                           
                              i
                           ) is the length of the relevant period in the segment s
                           
                              i
                           . The average segment precision (ASP) is calculated at the ranks where relevant content is found as follows:


                           
                              
                                 (5)
                                 
                                    ASP
                                    =
                                    
                                       1
                                       n
                                    
                                    .
                                    
                                       
                                          ∑
                                          
                                             r
                                             =
                                             1
                                          
                                          N
                                       
                                       SP
                                       [
                                       r
                                       ]
                                       ·
                                       rel
                                       (
                                       
                                          s
                                          r
                                       
                                       )
                                    
                                 
                              
                           where n is the number of segments that contain relevant content, and rel(s
                           
                              r
                           ) is equal to 1 if s
                           
                              r
                            contains any relevant content, and 0 otherwise. MASP is defined as the mean of ASP across the topic set.

Mean average segment distance-weighted Precision is a modification of MASP, that takes into account the distance to the jump-in point using the same penalty function as mGAP in evaluation (Eq. (3)), (Eskevich et al., 2012):


                           
                              
                                 (6)
                                 
                                    ASDwP
                                    =
                                    
                                       1
                                       n
                                    
                                    .
                                    
                                       
                                          ∑
                                          
                                             r
                                             =
                                             1
                                          
                                          N
                                       
                                       SP
                                       [
                                       r
                                       ]
                                       ·
                                       rel
                                       (
                                       
                                          s
                                          r
                                       
                                       )
                                       ·
                                       
                                          
                                             
                                                1
                                                −
                                                
                                                   
                                                      Penalty
                                                      ·
                                                      Granularity
                                                   
                                                   Window
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

ASDwP thus combines segment rank, the proportion of the segment which is relevant, and the distance from the start of the segment to the beginning of the relevant content. MASDwP is again defined as the mean of ASDwP across the topic set.

While the formal metrics used for our investigation are precision based, we are interested in the implication for recall of speech recognition errors and alternative segmentation of the content. The detailed analysis of the results in terms of averaged precision metrics for each retrieval scheme enables us to determine the characteristics of relevant content retrieved at high ranks. In order to understand the implication of the findings for recall, we examine cases of failure to retrieve relevant content at a rank sufficient for it to be inspected by the searcher. In summary, we seek to understand the characteristics of items containing relevant content to be retrieved at high rank, and the recall implications for retrieval of relevant content when these characteristics are not achieved.

As introduced in Section 1, we can consider many meeting retrieval tasks to be recall focused activities where the user requires to find all relevant items. In recall focused search, the user typically goes much deeper into the ranked retrieval list than in standard precision focused search tasks. For example, patent examiners may look more than 100 items into the returned list. From a practical perspective due to the temporal nature of speech content and the time taken to review content of this type, we assume that the users can reasonably be expected not to look for relevant items as deep into the list as for a text-based patent examiner. For this study we thus chose the rank 50 as the cut off for retrieved results that a user would be ready to listen to in order to attempt to find the relevant content. We thus take retrieval at rank 50 as the cut off point for the next part of our experimental analysis. Clearly if the user is looking for a known relevant item and has not found it after inspecting the first 50 items they may be willing to continue searching. Similarly if the information that they are looking for is particularly valuable they may continue to keep searching. It should also be noted that while we have chosen a cut off of 50 retrieved items for this study, similar trends to those observed and examined here in the ranking of retrieved items would be observed if a different cut off point further up or down the list were to be used.

In this section we report retrieval results for the test set of the search collection. As mentioned above, we ran an extensive range of experiments on the development set in order to narrow the set of length based segmentation methods to be used for the main runs on the test query set. The results for the development set are consistent with those of the larger number of the queries in the test set (25 in the test set against 10 in the development). Results on this larger query set can though be expected to be more reliable. Therefore we describe the results for the test set in the following sections.

All metrics are calculated for alternative segmentation methods that produce segments of comparable average length (tt, ncut_tt, C99, ncut_C99, time_120, time_150, time_180, len_300, len_400, len_nsw_100, and len_nsw_200), we include two shorter versions of time segmentation (time_60 and time_90) for additional analysis of retrieval performance on shorter segments.


                           Figs. 2 and 3
                           
                            show MAP and mGAP respectively calculated for the different segmentation schemes with the three segment types. These figures allow evaluation of the retrieval results from the perspective of ranking of segments containing relevant content, and ranking of the segments containing the jump-in point or being within the window distance from the jump-in point. Figs. 4 and 5
                           
                            show MASP and MASDwP for the same retrieval runs. These figures show the difference between retrieval performance for the different segmentation methods where the retrieval performance metrics are based on the length of the segments in time combined with rank, and rank and distance to jump-in point. Overall, as would be expected the manual content with automatic segmentation shows better results than ASR content with both forms of segment boundaries. The only segmentation method that does not follow this pattern is len_300.

From Figs. 2 and 3 it can be seen that for segments based on time segmentation MAP is higher for the longer segment units. However, according to the mGAP metric the time_90 segmentation retrieves more segments which are close to the jump-in point. We can see from Fig. 4 that there is no significant difference between time_90, time_120, time_150 and time_180 runs in terms of MASP. This arises since they retrieve approximately the same proportion of relevant content across the length of the segments in the top 50. In general, the precision of the relevant content within the segments containing relevant content is lower for longer segments, cf Fig. 6
                           . Of course, it should be noted that for longer segments there are less relevant and irrelevant segments within collection. This leads to better MAP, but due to the lower precision of relevant content within the retrieved segments, MASP remains at the same level. In the case of time_60 segmentation, the segments have the highest precision of relevant content. However, these segments are harder to retrieve because they contain less potentially relevant content, and there are more relevant segments to be accounted for in the collection which can match with the query. Overall this means that retrieval is more difficult and that relevant segments are spread out over a longer list of retrieved items.

Looking further at these figures, retrieval of length-based segments follows the same pattern for all cases except len_300 (len_300 vs. len_400, and len_nsw_100 vs. len_nsw_200): longer segments have better MAP, but the precision of the relevant content is lower, and the retrieved segments begin further from the jump-in points. This will degrade the efficiency of user access to relevant information, since they have to rewind or listen to long parts of irrelevant content.

The lexical cohesion based methods show similar levels of performance for all four metrics. Comparing the MAP results for asr, asr_man and man runs in Fig. 2, it can be seen that as we might expect, man runs are better than asr_man runs which are better than asr runs. This corresponds to the average ranking of relevant content. While this trend is consistent for MAP, except for very long fixed length segments, looking at the mGAP results in Fig. 3 shows different behaviour where in many cases the asr ranking results is better. This can be explained by the fact that for the lexical cohesion methods, the difference in segmentation between the asr and asr_man runs is much greater than for the fixed length segments. This arises due to the difference between the segmentations for the manual and ASR content. Thus the segments that might be lower in the result list for the asr runs, as compared to the asr_man runs, as shown by the lower MAP, in a number of cases have a better starting time which is closer to the ideal jump-in point. This is reflected in the higher mGAP values for the asr results in some cases. The results in Fig. 4 for MASP are quite similar across all segmentation schemes. These results are a combination of rank and precision of relevant content in the segment at each relevant rank. The results show that the precision of the content in the individual segments can often be higher where the MAP is lower. This is most noticeable for time_60, where the MAP result in Fig. 2 is much lower, but the MASP value in Fig. 4 is comparable to that of other segmentation schemes. These relative differences are most noticeable for shorter segmentation schemes, indicating that while the query-document match may be less strong due to lower numbers of terms in short segments, the absence of non-relevant content in these segments leads to better segment precision.

Finally, Fig. 5 showing MASDwP records much greater variation when combining rank of relevant items, distance to jump-in point and segment rank precision. The fact that time_90 outperforms the other methods according to this metric shows that retrieval using these segmentation units has the best balance of the ranking of segments that start close to the actual jump-in point, while minimizing the amount of non-relevant content in each segment. This result opens another direction of discussion which is out of the scope of this research, whether users would prefer to examine a greater number of shorter segments while auditioning less non-relevant material, or to examine less segments, but to spend more time auditioning non-relevant content.

It is interesting to take a more detailed look into the changes that happen in the ranks of retrieved segments when the only difference underlying the ranking is the transcript input. We compare the asr_man and man results across all the segmentation methods. In order to compare these, we calculate the average number of changes in the rank of retrieved segments containing relevant content between the asr_man and the man runs for the 25 test queries. These results are presented in Table 2
                           . Items retrieved in the top 50 for the manual segments are divided into four groups: asr_man segment receives improved rank (a_m<m), it remains at the same level (a_m = m), it achieves a degraded rank (m<a_m≤50), it disappears from the top 50 items in asr_man (m≤50<a_m). In the last column we show the number of segments that are retrieved in the top 50 for asr_man, but fall below the cutoff of 50 in the manual transcript (a_m≤50<m).

As only a small number of relevant segments stay at the same rank, the vast majority of the relevant data changes its position. Since the errors in the ASR transcripts may cause the loss of content words important for the topic of the query, it is not surprising that half of the segments are at reduced rank in the retrieved list. However, it is important to note that most of the remaining items move up the list. If we choose a certain threshold (as we have with rank 50 throughout this investigation), certain parts of the relevant information retrieved in the top 50 for the man run drop below this threshold for the asr_man run and are lost to the user, based on our assumption that they will not look beyond the top 50 ranked items. On average for all types of segmentation the number of asr_man segments that move up the list (a_m<m) is higher than the number at reduced rank (m<a_m≤50) or are lost after the set cutoff point (m≤50<a_m). Also, is should be noted that there is a certain amount of content that appears in the top 50 ranks for asr_man while not being retrieved in top 50 man at all (a_m≤50<m). This partially compensates for the amount of relevant content present in the man ranked lists that are lost in asr_man ranked lists (m≤50<a_m) when the ranks at which relevant content is retrieved are averaged.

Overall except for len_300, the average number of segments containing relevant material present in man (m<a_m≤50 and man<50<a_m) and asr_man (a_m<m and a_m≤50<m) are roughly comparable or higher, showing that as we would anticipate, on average man runs retrieve more segments with relevant content. However in the case of len_300 there are many more relevant segments retrieved in asr_man list than lost (‘a_m≤50<m’=8.9 vs. ‘m≤50<a_m’=0.8)). Also it should be noted that the ranks of relevant segments that move up the list for asr_man are usually lower than the ones for which the rank is reduced or which are lost. Overall this provides a clear explanation for the small reduction in MAP observed between man and asr_man.

In this section we first examine the ranked list result for a single search request looking at changes in ranks between man and asr_man runs, and when different segmentation methods are used to preprocess the collection. We then show that the effects for this query can be seen over all the segmentation methods averaged across the whole test query set.

While Table 2 presents the information about rank changes on average, we next show an example of the specific changes in rank position of relevant content for a single search request. Fig. 7
                               illustrates the connection between the number of changes in the rank of relevant content and the amount of relevant content which changes place in the example case for one query (query 21) and our overall preferred segmentation method (C99). The three parts of the figure depict the changes that happen within the top 50 results for man and asr_man runs. The left side (a) shows movements up and down the list for relevant items retrieved in the top 50 for both man and asr_man, the middle section (b) shows relevant items retrieved in the top 50 for man transcripts that drop out of the top 50 for asr_man, and the left side (c) shows relevant items promoted to the top 50 for asr_man which are below the top 50 for man transcripts. In all three sections, next to the rank we provide the information that characterizes the segment: length in seconds (e.g. 105/125 if there is 105s of relevant content in a segment that is 125s long, or simply 105 if the segment contains only relevant material); WRR on this segment (e.g. 0.67); the sign “JP” means that the segment contains the jump-in point for the relevant content associated with this relevant region of a meeting.

Although the amount of relevant content with higher ranks in the asr_man run within the top 50 is higher in absolute terms (2370s vs. 1694, as shown in Fig. 7), the corresponding ranks in the manual run are higher in the list. Looking at columns (b) and (c) of Fig. 7, we can see that the loss of a number of relevant segments below rank 50 between man and asr_man is to some extent compensated for by segments that were not found within the top 50 for the man run, but are promoted for the asr_man run. However both the amount of relevant content for asr_man that moves into the top 50 is lower than the amount lost (515s vs. 810s) and the ranks at which relevant content is found are on average lower in the list.

For this example, especially in columns (b) and (c) we can see that the segments that move down the list have on average lower WRR (e.g. 0.51, 0.58, 0.47) than the WRR of the segments promoted in the top 50 of the asr_man run (0.67, 0.77, 0.82), the same trend can be distinguished in the changes that happen within column (a) for relevant segments present in both lists.

The segments that move up the list in columns (a) and (c) often contain the jump-in point for the relevant content. In the cases of the top ranks within asr_man (ranks 4, 6, 8, 9, 12, 14) the retrieved segments contain the longest part of the relevant content which is spread across several segments. Within the top 50 retrieved results of the manual run there are two cases where the relevant content is split into two parts by the segmentation methods. Both of these adjacent segments are present in the retrieved list (case 1: rank 8 (segment 2), rank 24 (segment 1), these change to the following positions in the asr_man rank 13 (segment 2), rank 9 (segment 1); case 2: rank 6 (segment 2), rank 45 (segment 1) that move to rank 4 (segment 2) and rank 48 (segment 1)). In both examples, the segments that have more relevant content and are longer (segment 1 in first case and segment 2 in the second case, 350 and 243s of relevant content respectively) move up the list in the asr_man run, whereas the shorter adjacent segments (90s and 80s respectively) move down the list, even when the WRR is relatively high (0.71 ad 0.74 respectively).


                              Fig. 8
                               allows us to compare the changes illustrated in Fig. 7 for the segmentation methods exhibiting the most interesting behaviour in the earlier analysis on the development set. The left part of the figure represents the amount of relevant content that was present in the top 50 and which changes its position or stay the same, including changes where the segment falls below rank 50. The right hand side shows the amount of relevant data that appears in the top 50 for the asr_man run which was not present in top 50 results for the man run. The numbers above each bar represent the length in seconds and the ones below, the number of segments that this content is present in for each type of rank position change.

Comparison between length-based algorithms and lexical coherence based algorithms shows that the latter outperform the former (the amount of relevant content present in the top 50 manual and not lost in the top 50 asr_man is higher for C99, mcut_C99 and mcut_tt than for the results of all length and time based runs).

In the case of the length-based segmentation, longer segment based runs contain more relevant content within the top 50 than ones with shorter segments of the same type: (len_nsw_200 (2287s) vs. len_nsw_100 (2074s), and len_400 (2242s) vs. len_300 (1507s). Although if we take all the content present in the top 50 of the man runs as 100% and look at the position changes of this content, then the shorter segments have better statistics: 56% and 75% of the content moving up the list in len_nsw_100 and len_300, against 47% and 56% for len_nsw_200 and len_400, 30% and 4% lost against 36% and 29% respectively. Only the amount of relevant content going down the list is better for longer segments: 6% and 9% for len_nsw_200 and len_400 against 14% and 21% for len_nsw_100 and len_300.

As we assume the scenario of a user looking for as much relevant information in the list as possible and not being willing to listen beyond a certain number of items in the list, we can see that segmentation runs that find more information in absolute length in time should be considered better than ones that have better scores in terms of the percentage of position changes of improved rank or smaller reduction in the ranks of relevant content present in top 50 of the manual run against the asr_man run. Comparing len_300 to len_400: 1182s of relevant content in the segments move up the list in the asr_man run vs. 1758s, this means 75 % vs 56% in percentage.

We can see that in general segments where rank is promoted in the case of asr_man are in general longer, have high segment precision and high WRR. These features of such segments are to be expected. However, this does highlight the challenges of retrieving items whose rank is reduced in the asr_man run, which are typically shorter, and have lower segment precision and WRR.


                              Table 3
                               shows the total amount of relevant content present in man and asr_man top 50 ranked segments for each segmentation type, and the number of retrieval rank positions where this relevant content is found. Only two segmentations (tt, len_300) have more relevant content in the asr_man than in the man runs in absolute numbers. This is explained by Fig. 8, where we can see that only these two runs have a larger amount of relevant content moving up from below the list for the asr_man run relative to the man run, than that which falls below rank 50 in the man run (773s vs. 633s for tt and 754s vs. 62 for len_300). This trend is consistent for most of the queries for len_300, i.e. more content moves up the list in the asr_man run than falls below rank 50 in the manual run. This is the reason for the unusual behaviour of the manual run of this segmentation when evaluated using all 4 metrics in Figs. 2–5.

The analysis so far has examined the length of the relevant content which changes position, but another feature that is important to look at when analyzing the changing of ranks is the WRR of these segments. Table 4
                               shows the average WRR for relevant segments according to the changes of ranks in asr_man compared to the man runs for the same example query. The WRR of the segments that fall in rank below the top 50 in the asr_man (m≤50<a_m) is lower than the WRR of the relevant segments that compensate for this loss (a_m≤50<m) for all the segmentation methods. For all the segmentation methods except len_400, the average WRR of the segments that move up the list is higher than the average WRR of the segments that get lower ranks within top 50 of asr_man. Examining the reasons for the differences in the lists we find that segments retrieved at high rank for the asr_man segments generally have high WRR, this observation agrees with the findings reported in (Sanderson and Shou, 2007). Consistent with this, segments which experience reduced rank generally have lower WRR. This effect is particularly noticeable when examining relevant segments which disappear from the top 50 ranked items. This poses a major challenge to achieving high recall in meeting search when some regions of the ASR transcripts of the meetings experience low WRRs.

In the previous section we focused on the details of one query taken as an example. Table 5
                               shows that the behaviour for this query is consistent across the test query set. This table shows the overall average values that correspond to the transcript quality of the segments that change ranks between asr_man and man runs within the same segmentation method. The asr_man segments present within the top 50 ranks have higher WRR values than the ones that fall below the cut off.

Throughout the investigation so far we have used the WRR metric because it better reflects the behaviour of the indexing and retrieval systems (the order of the words in the transcript is not important; only the stems of the non-stop words corresponding to the query that are recognized by the ASR system influence the results). However we also calculated word error rate (WER), the standard metric that is used to define the quality of ASR system output in speech recognition. Tables 5 and 6
                               show the WRR and WER averaged across the set of test queries. WRR of segments that stay within the top 50 rank positions (a_m<m≤50, a_m = m≤50, m<a_m≤50) is higher than that of those which fall below the top 50 cut off rank for one of the transcript types (a_m≤50<m, m≤50<a_m). In the case of m≤50<a_m, asr_man segments that fall below the top 50, do so because important content words have been misrecognized. The case of a_m≤50<m illustrates the situation where non relevant content has low WRR, thus it falls lower in the ranked list allowing the relevant asr_man content to move up the list. WER values have a more straightforward relation with the rank changes: the asr_man segments that are higher than man segments in the list (a_m<m≤50, a_m≤50<m) have lower WER than the asr_man segments that are lower in rank than their manual counterpart (m<a≤50, m≤50<asr_man). This trend illustrates that the cut off value of 50 does not affect the trend observed in the results.

This paper has described our current investigation into retrieval of transcripts of multi-party meetings based on the AMI corpus. We carried out an investigation into the segmentation of transcripts using the C99 algorithm and other methods to provide suitable retrieval units. Our examination of results of the outputs of these segmentation methods shows significant variations between segmentation behaviour for manual and ASR transcripts of the meetings. Initial investigation beyond this study reveals that these differences arise from the impact of word recognition errors on topical matching between related sentences and the resulting topic boundary decisions of the segmentation algorithms, but this issue requires further examination.

In order to undertake an investigation of segment retrieval we constructed a query set with corresponding manual relevance assessments. We used a combination of different metrics to explore the retrieval behaviour for this data. The need for the use of a combination of metrics represents an important aspect of the retrieval process evaluation for this task, since we need to pay attention to different facets in terms of efficiency in locating relevant content in temporal spoken content.

Experiments with this retrieval collection show that the lexical cohesion based segmentation methods perform consistently better compared to length- and time-based segmentation methods when the set of diverse metrics are used to evaluate the retrieved results. C99 outperforms both TextTiling and the Minimum Cut method that uses the number of segments from TextTiling as input. However our detailed analysis of the behaviour of the segments shows that the C99 segmentation method needs to be enhanced to take into account WRR errors in order achieve more reliable retrieval behaviour.

Overall we can see that longer segments achieve superior MAP at the expense of time consuming browsing of non-relevant content. By contrast, shorter segments reduce the browsing time, but more segments must be explored to find the relevant content. ASR errors make the ranking of short documents less reliable, while content redundancy can compensate for this in longer segments.

Thus, ideally segments should have the highest possible precision within the segment while being as long as possible to help compensate for word recognition errors. As our results illustrate, extracting such segments is a very challenging task, results of our investigations and other studies examining topical segmentation of standard text, show that current segmentation methods are often inaccurate.

Based on the findings in the study reported in this paper, in further work we plan to continue to explore the extraction of suitable retrieval segments based on ASR transcripts. For example, using overlapping segmentation schemes as proposed by some participants in the NTCIR-9 SpokenDoc task (Akiba et al., 2011). In addition, the identified problems with retrieval of short segments with low WRRs rates/high WERs rates, suggest that we should explore use of recognition structures containing multiple word hypotheses such as lattices as sources of recognition alternatives, as for example explored in Jones et al. (1996), Saraclar and Sproat (2004), Mamou et al. (2006), and Chia et al. (2010).

@&#ACKNOWLEDGMENTS@&#

This work is supported by Science Foundation Ireland, under the Research Frontiers Programme 2008 (Grant 08/RFP/CMS1677), and Grant 07/CE/I1142 as part of the Centre for Next Generation Localisation (CNGL) project at DCU.

The authors would like to thank Debasis Ganguly, Wei Li, Ágnes Gyarmati, Jinming Min, Lorraine Goeuriot, Rasoul Samad Zadeh Kaljahi, Liadh Kelly and Stephen Doherty for assistance with manual relevance assessment; and Debasis Ganguly for assistance with the C99 software and providing the extended version of SMART.

@&#REFERENCES@&#

