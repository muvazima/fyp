@&#MAIN-TITLE@&#Covariate adjustment of cumulative incidence functions for competing risks data using inverse probability of treatment weighting

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We use inverse probability of treatment weighting, a propensity score based technique, for covariate adjustment of the cumulative incidence functions in competing risk analysis.


                        
                        
                           
                           This method requires no assumption about the form of the cumulative incidence functions and the interpretation of the adjusted cumulative incidence functions is intuitively appealing.


                        
                        
                           
                           We developed a SAS macro to make the method readily usable.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Competing risks

Multiple-outcome data

Cumulative incidence function

Subdistribution function

Inverse probability of treatment weighting

Propensity score

@&#ABSTRACT@&#


               
               
                  In observational studies without random assignment of the treatment, the unadjusted comparison between treatment groups may be misleading due to confounding. One method to adjust for measured confounders is inverse probability of treatment weighting. This method can also be used in the analysis of time to event data with competing risks. Competing risks arise if for some individuals the event of interest is precluded by a different type of event occurring before, or if only the earliest of several times to event, corresponding to different event types, is observed or is of interest. In the presence of competing risks, time to event data are often characterized by cumulative incidence functions, one for each event type of interest. We describe the use of inverse probability of treatment weighting to create adjusted cumulative incidence functions. This method is equivalent to direct standardization when the weight model is saturated. No assumptions about the form of the cumulative incidence functions are required. The method allows studying associations between treatment and the different types of event under study, while focusing on the earliest event only. We present a SAS macro implementing this method and we provide a worked example.
               
            

@&#INTRODUCTION@&#

In observational studies without random assignment of the treatment, the unadjusted comparison between treatment groups may be misleading due to confounding. One method to adjust for measured confounders is inverse probability of treatment weighting [1,2]. This general approach has been applied in many settings, e.g., to adjust survival curves for time to event data [3].

In time to event analysis, competing risks may be a concern. They arise if for some individuals the event of interest is precluded by a different type of event occurring before, or if only the earliest of several times to event, corresponding to different event types, is observed or is of interest. In the presence of competing risks, the data may be characterized by the cumulative incidence functions, one for each event type.

In this paper, we describe the use of inverse probability of treatment weighting to create adjusted cumulative incidence functions, we present a SAS macro implementing this method and provide a worked example.

@&#METHODOLOGY@&#

As a motivating example, we consider a study on the long-term outcome in early-stage Hodgkin's disease patients. The data are from Pintilie's book on competing risks [4]. They were chosen to illustrate statistical issues that arise in practice and not for purposes of drawing medical conclusions.

The data set contains 865 patients treated for Hodgkin's disease between 1968 and 1986 at the Princess Margaret Cancer Center in Toronto [5]. All patients had early stage disease (I or II) and were treated either with radiation (only) or with radiation and chemotherapy. The recorded event types were first relapse, second malignancy (the first malignancy after diagnosis of Hodgkin's disease) and death. Time to event was calculated from the date of diagnosis (in days). The data set contains the following covariates: age (in years), sex, size of mediastinum involvement (no, small or large), extranodal disease (yes or no), and clinical stage (I or II).

Clearly, death is a competing event when interest is in relapse or second malignancy. On the other hand, in real life, relapse and second malignancy do not preclude later death and relapse does not exclude later second malignancy, and vice versa. Nevertheless, all three types of event must be considered as competing with each other when only the event occurring first, i.e., the composite event, is under study. That is, for instance, death occurring after relapse or second malignancy is not of interest to the underlying research question. If, on the contrary, interest was in mortality, regardless of relapse and second malignancy, the competing risk framework would not be helpful. In Section 4, we will be interested in the event occurring first, both in its type and in its time to event.

Survival analysis deals with the time elapsed from an initiating event, e.g., the onset of some disease, to a well-defined event. Let T denote the random variable representing the time to event of interest, considered as continuous. It is usually characterized by the survivor function, or survival, defined as the probability S(t):=
                        P(T
                        >
                        t), or, equivalently, by the distribution function F(t):=
                        P(T
                        ≤
                        t)=1−
                        S(t). If the event was observed for each individual in the study population, then F(t) can be estimated as the relative frequency of time to event less than or equal to t. However, in clinical research practice, time to event is most often observed only for a fraction of individuals included in the study. For all others, what is known is that the time to event is greater than the observation time. This is referred to as censoring. Fortunately, F(t) can also be estimated in the presence of censored observations, but under the condition that censoring is ‘independent’, that is, the average risk of event among individuals not censored is the same as what the average risk would be among all individuals if no censoring occurred. In this sense, an individual censored at time t should be representative for those still ‘at risk’ (i.e., uncensored and having not yet experienced the event) at that time. Concretely, under independent censoring, F(t) may be estimated by 1 minus the Kaplan–Meier estimator Ŝ(t). The Kaplan–Meier estimator at time t is a product with one factor for each observed time to event up to t. For time s, this factor is 1 minus the observed proportion of individuals who experienced the event at time s among all individuals still ‘at risk’ at time s.

These definitions can be extended to the competing risk situation where the event may be of any one of m
                        ≥2 distinct types or causes denoted by J 
                        [6]. The cumulative incidence function, or subdistribution, for an event of type j (j
                        =1, 2, …, m) is defined as the joint probability
                           
                              
                                 
                                    
                                       F
                                       j
                                    
                                    (
                                    t
                                    )
                                    :
                                    =
                                    P
                                    (
                                    T
                                    ≤
                                    t
                                    ,
                                    J
                                    =
                                    j
                                    )
                                    .
                                 
                              
                           
                        
                     

Analogously, the subsurvivor function is defined as S
                        
                           j
                        (t):=
                        P(T
                        >
                        t, J
                        =
                        j).

Alternatively, T may be considered as the earliest of m latent, possibly unobserved, event times T
                        1, T
                        2, …, T
                        
                           m
                        , one for each of the m event types: T
                        =min (T
                        1, T
                        2, …, T
                        
                           m
                        ). This has been called the latent failure time approach to competing risks. In this setting, F
                        
                           j
                        (t) is the probability that the event of type j occurs up to time t and this before any other type of event, or, in other words, the probability that the earliest event occurs up to time t and this event is of type j. Note that F
                        
                           j
                        (t) depends not only T
                        
                           j
                        , the time to event for type j, but also on the time to event for all other event types.

The cumulative incidence function F
                        
                           j
                         can take values only up to P(J
                        =
                        j), typically smaller than 1, hence the term ‘subdistribution’. Note that the overall distribution function is equal to the sum of the cumulative incidence functions over all event types:
                           
                              
                                 
                                    F
                                    (
                                    t
                                    )
                                    =
                                    P
                                    (
                                    T
                                    ≤
                                    t
                                    )
                                    =
                                    
                                       ∑
                                       
                                          j
                                          =
                                          1
                                       
                                       m
                                    
                                    
                                       P
                                       (
                                       T
                                       ≤
                                       t
                                       ,
                                       J
                                       =
                                       j
                                       )
                                       =
                                       
                                          ∑
                                          
                                             j
                                             =
                                             1
                                          
                                          m
                                       
                                       
                                          
                                             F
                                             j
                                          
                                          (
                                          t
                                          )
                                       
                                       .
                                    
                                 
                              
                           
                        
                     


                        F
                        
                           j
                        (t) can be estimated by a sum with one term for each observed time to event up to t. For time s, this term is the estimated probability of being free of any event just before time s (i.e., the Kaplan–Meier estimator at the preceding observed time to event) multiplied by the observed proportion of individuals who experienced the event of type j at time s among all individuals still ‘at risk’ at time s. Note that 1 minus the Kaplan–Meier estimator calculated from events of type j, while treating the events of all other types as ‘independent’ censoring, is an upwards biased estimate of F
                        
                           j
                        . As a consequence, summing up the estimated cumulated incidence (1 minus estimated survival) over the m event types yields a value which is larger than the estimated cumulated incidence for the composite event. The inappropriateness of this ‘naïve’ Kaplan–Meier estimator for competing risks data is explained in greater detail in the literature [6–8].

The method of inverse probability of treatment weighting can be used to adjust for measured confounding [1,2]. Let X be the random variable denoting treatment and Z the covariate vector. With inverse probability of treatment weighting, each individual i is assigned a weight w
                        
                           i
                        , which is equal to the inverse of the probability of receiving his or her own treatment x
                        
                           i
                         conditional on the observed covariate vector z
                        
                           i
                        , i.e., w
                        
                           i
                        
                        =1/P(X
                        =
                        x
                        
                           i
                        |Z
                        =
                        z
                        
                           i
                        ). Usually, weights are estimated from the data, most often using a logistic regression model. In practice, the weights w
                        
                           i
                         may be highly variable, with very large values for small values of P(X
                        =
                        x
                        
                           i
                        |Z
                        =
                        z
                        
                           i
                        ), and in some settings stabilization is searched for. It may be achieved by multiplying by the marginal probability of receiving the observed treatment leading to stabilized weights sw
                        
                           i
                        
                        =
                        P(X
                        =
                        x
                        
                           i
                        )/P(X
                        =
                        x
                        
                           i
                        |Z
                        =
                        z
                        
                           i
                        ), where P(X
                        =
                        x
                        
                           i
                        ) can be estimated by the frequency of treatment x
                        
                           i
                        . Weighting creates a pseudo-population in which the probability of being treated does not depend on the measured covariates. In Table 1
                        , this is illustrated for our example using the disease stage as only covariate for inverse probability of treatment weighting.

Finally, the parameter or function of interest is estimated from this pseudo-population, most often by using standard statistical methods. However, variance estimation must account for the weighted nature of the pseudo-population.

Cole and Hernán [9] discuss in detail the construction of weights, including weight stabilization and weight truncation, within the more general context of a time-varying treatment variable and with time-varying covariates. Note that the weights w
                        
                           i
                         are closely related to the propensity score, which was originally defined as P(X
                        =1|Z
                        =
                        z
                        
                           i
                        ) for a binary treatment variable with values 1 (active treatment) and 0 (control) [10,11]. We point out that for a given individual i the weight w
                        
                           i
                         depends on her/his treatment x
                        
                           i
                        , whereas the propensity score P(X
                        =1|Z
                        =
                        z
                        
                           i
                        ) does not. When the weight model is correctly specified, using estimates of the weights rather than the true weights has been shown to reduce the variance of the estimator of interest (see e.g. [1]). This is because both systematic and chance imbalances between treatment groups are corrected. Assessing whether the weight model has been adequately specified involves a comparison of the treatment groups in the pseudo-population, in particular by checking for all covariates the value of the standardized difference 
                        [11,12].

The application of inverse probability of treatment weighting to the adjustment of cumulative incidence functions is straightforward. First, the weights are calculated as indicated in Section 2.3. Second, for each treatment x and each type of event j, the cumulative incidence function for the event of type j is estimated according to Section 2.2 from the weighted sub-population of all individuals having received treatment x. Confidence bands can be estimated using bootstrap, with newly calculated weights for each bootstrap replication. Note that non-stabilized and stabilized weights result in the same estimates of the adjusted cumulative incidence functions. This is because for a given treatment x stabilization consists in multiplying the non-stabilized weight by a constant value, which does not modify the cumulative incidence.

When the weight model is saturated (i.e., the estimator of the probability P(X
                        =
                        x|Z
                        =
                        z) is identical to the frequency of treatment x observed among all individuals with the covariate vector z, for all x and y), this method is equivalent to direct standardization of the cumulative incidence functions to the overall study population. In this sense, the cumulative incidence functions estimated for treatment x represent the experience of the entire study population had all individuals received treatment x. For the graphical representation, the cumulative incidence functions may be grouped per treatment or per event type.

SAS code which implements the presented method is provided in the Supplementary Material. It is organized in three SAS macros briefly described in the following.

The SAS Institute supplies the autocall macro %CumIncid (version 9.2 and later) [13] for estimating the unadjusted cumulative incidence function for a given type of event. We generalized the estimation to weighted observations and named the resulting macro %CumIncid2. All modifications are explicitly described in comments in the SAS code.

The macro %CumIncidIPTW calculates the adjusted cumulative incidence functions. First, the (non-stabilized) weights are calculated. Second, for each type of event the macro %CumIncid2 is executed using the calculated weights and with the treatment variable as stratum variable. That is, the cumulative incidence functions are estimated separately for each treatment x based on the weighted sub-population of all individuals having received the treatment x.

Third, in order to obtain bootstrap confidence bands, this estimation procedure is repeated for each bootstrap replication. The weights are newly calculated for each bootstrap replication.

Finally, the macro %CumIncidIPTWPlot can be used to display the estimated cumulative incidence functions, grouped together either by treatment or by type of event.

The invocation of the macros %CumIncidIPTW and %CumIncidIPTWPlot is illustrated (a) at the end of the SAS code for artificial data and (b) in Appendix A for the data of the worked example.

In this section, we illustrate the proposed method by analyzing the Hodgkin's disease study data described before. Except for age and sex, all covariates were related to treatment (Table 2
                     ): patients treated with radiation and chemotherapy had more often mediastinum involvement, extranodal disease and clinical stage II. Among the 616 patients treated with radiation, 58% had at most one of the three events considered. For the 249 patients treated with chemotherapy and radiation, this proportion was 49%.

The crude cumulative incidence functions are shown in Fig. 1
                     , one for each treatment group and for each type of event. For example, for radiation the crude cumulative incidence at 15 years from diagnosis is 37%, 2%, and 10% for relapse, second malignancy, and death, respectively. Summing up these three values yields 49%, the crude cumulative incidence for the composite event. The functions in Fig. 1 were obtained using the SAS autocall macro %CumIncid.

For comparison, we calculated the naïve estimators, i.e., 1 minus the Kaplan–Meier estimator based only on the considered event type and treating the other types of event as independent censoring. For radiation, we obtained at 15 years from diagnosis 38%, 4%, and 15% for relapse, second malignancy, and death, respectively, summing up to 57%, which is actually noticeably higher than the crude cumulative incidence for the composite event (49%).

We applied inverse probability of treatment weighting to adjust the cumulative incidence functions for age, sex, size of mediastinum involvement, extranodal disease, and clinical disease stage. The invocation of the SAS macro %CumIncidIPTW is detailed in Appendix A. The non-stabilized weights varied between 1.06 and 15.6, with a mean value of 2.00. Note that the mean value of the weights is near to the number of the treatments under study, which corresponds to the requirement that the mean value of the stabilized weights is near to one [9]. In the resulting pseudo-population, the standardized difference between the two treatment groups was less than 0.1 for all covariates (Table 2), a value that has been taken by some authors to indicate a negligible difference in the mean (see reference in Ref. [11] and further discussion in Ref. [12]). The adjusted cumulative incidence functions are presented in Fig. 2
                     . Confidence bands were obtained by bootstrap with 1000 replications. For example, for relapse, adjustment increases the difference in estimated cumulative incidence between both treatment groups. After adjustment, the cumulative incidence of the composite event at 25 years is 66% (95% confidence interval: 62–70%) for radiation and 55% (47–63%) for radiation and chemotherapy. Compared to radiation, the event occurring first with radiation and chemotherapy is more often second malignancy and less often relapse. We recall that this illustration of methodological issues does not aim at conveying medical conclusions.

@&#DISCUSSION@&#

We described a straightforward method to perform covariate adjustment with time to event data in the presence of competing risks and we developed a SAS macro to make the method readily usable. With the choice of the cumulative incidence function as entity to be estimated, inference for the event of interest is made ‘in the presence of competing risks’. The presented approach requires no assumption about the form of the cumulative incidence function. When the weight model is saturated, the method is equivalent to direct standardization to the entire study population. Interpretation of the estimated cumulative incidence functions is therefore simple and intuitively appealing. The method helps to visually depict associations between treatment and the different types of event under study, while focusing on the earliest event only. This focus may be motivated by the research question under study and/or form a practical constraint in studies where follow-up of individuals is stopped at the first event, in particular if each of the considered event types virtually prevents the others from happening.

To the best of our knowledge, so far, there are no methodological papers proposing covariate adjustment of cumulative incidence functions for competing risks data with inverse probability of treatment weighting. However, the presented method could be considered as a special case of more recent, rather sophisticated approaches such as that one proposed by Bekaert et al. [14]. The general approach of inverse probability of treatment weighting is widely accepted as one method for covariate adjustment [1–3,9–11] and differences to alternative methods, in particular propensity score matching and adjustment by regression modeling, are well understood [11,15]. Direct adjustment of cumulative incidence functions has been proposed by Zhang and Zhang [16]. Their approach is based on the Fine and Gray proportional subdistribution hazards regression model [17,18] and assumes constant subdistribution hazard ratios for each covariate.

When in a comparison between treated and untreated patients the interest is in standardization to the population of treated individuals rather than in standardization to the entire study population, the presented approach can be modified by using propensity score matching instead of weighting [11,15]. However, propensity score matching involves several decisions concerning the matching method and the resulting estimates may depend on it. Also note that with matching some data are thrown away, whereas with weighting all data are used.

The presented method is subject to limitations. First, inverse probability of treatment weighting requires that, for every covariate combination, each of the treatment groups under study is chosen with strictly positive probability. Second, estimation of the confidence intervals by bootstrap may be time-consuming for large data sets. Third, we only used a logistic regression model to estimate the weights. In certain situations, other approaches such as decision trees or boosting may provide an improved specification of the weight model. Fourth, due to the non-parametric description of the time to event data, the presented approach does not provide a simple relationship between treatment and the cumulative incidence of the different types of event under study. Alternatively, the cumulative incidence function could be modeled by the Fine and Gray proportional subdistribution hazards regression model depending only on treatment and inverse probability of treatment weighting could be applied to adjust for the covariates. However, this approach comes with model assumptions.

An alternative approach in competing risk analysis is to estimate the cause-specific hazard function for each type of event, most often using Cox regression models [8]. For a given type of event j, the cause-specific hazard function at time t quantifies the instantaneous risk at t for developing an event of type j among all individuals ‘at risk’. The cumulative incidence function for event type j can be calculated as a function of the cause-specific hazard functions of all event types. Inverse probability of treatment weighting could also be applied within this approach by using a Cox regression model depending only on treatment and a weight model depending on the covariates. The discrete-time analogous to this approach is a special case of that one proposed by Moodie et al. [19], which is also suited for a time-varying treatment variable and time-varying covariates. Relying on Cox models, their approach also involves proportional hazard assumptions.

Finally, only the event which occurs first is considered with the described method. When interest is also in what happens after the first event, multi-state models could be applied [20]. These models describe the overall event history and again the use of inverse probability of treatment weighting could be considered for adjustment.

None.

In this appendix, the invocation of the SAS macros %CumIncidIPTW and %CumIncidIPTWPlot is illustrated using the data of the worked example.

The Hodgkin's disease study data from Pintilie's book on competing risks [4] are available under http://www.uhnres.utoronto.ca/labs/hill/People_Pintilie.htm.
                           
                              
                           
                        
                     


                        
                           
                              
                           
                        
                     

Here, the calculation of confidence bands is requested (ic=yes) using 100 bootstrap replications (nb_bootstrap=100). The default value of 0.05 was used for the complement of the confidence level for the calculation of the confidence bands (parameter alpha). The macro %CumIncidIPTW calculates confidence bands based on bootstrap standard deviations as well as confidence bands based on percentiles. For the latter to be valid, the number of bootstrap replications must be sufficient to give satisfactory estimates of the percentiles.


                        
                           
                              
                           
                        
                     

Here, the grouping of the adjusted cumulative incidence functions by type of event is requested (grouping=event). The inclusion of confidence bands into the figure is requested, more precisely the inclusion of the confidence bands based on bootstrap standard deviations (ic_mode=std).

The SAS code which implements the presented method is provided in the Supplementary Material associated with this article and can be found at http://dx.doi.org/10.1016/j.cmpb.2016.03.008.

The SAS code which implements the presented method is provided in the Supplementary Material associated with this article and can be found at http://dx.doi.org/10.1016/j.cmpb.2016.03.008.
                        
                           
                        
                     
                  

@&#REFERENCES@&#

