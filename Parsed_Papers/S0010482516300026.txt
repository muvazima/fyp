@&#MAIN-TITLE@&#A practical efficient human computer interface based on saccadic eye movements for people with disabilities

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A wearable EOG-based system was developed to enable people with disabilities to type.


                        
                        
                           
                           Saccadic eye movements could be detected efficiently by an adaptive algorithm.


                        
                        
                           
                           Average accuracy of the system was 84% with a typing speed of 4.5 character per min.


                        
                        
                           
                           Participants with disabilities could learn to perform necessary eye movements.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

People with disabilities

Electro-oculogram

Human computer interface

Saccadic eye movement

Wearable systems

@&#ABSTRACT@&#


               
               
                  Human computer interfaces (HCI) provide new channels of communication for people with severe motor disabilities to state their needs, and control their environment. Some HCI systems are based on eye movements detected from the electrooculogram. In this study, a wearable HCI, which implements a novel adaptive algorithm for detection of saccadic eye movements in eight directions, was developed, considering the limitations that people with disabilities have. The adaptive algorithm eliminated the need for calibration of the system for different users and in different environments. A two-stage typing environment and a simple game for training people with disabilities to work with the system were also developed. Performance of the system was evaluated in experiments with the typing environment performed by six participants without disabilities. The average accuracy of the system in detecting eye movements and blinking was 82.9% at first tries with an average typing rate of 4.5cpm. However an experienced user could achieve 96% accuracy and 7.2cpm typing rate. Moreover, the functionality of the system for people with movement disabilities was evaluated by performing experiments with the game environment. Six people with tetraplegia and significant levels of speech impairment played with the computer game several times. The average success rate in performing the necessary eye movements was 61.5%, which increased significantly with practice up to 83% for one participant. The developed system is 2.6×4.5cm in size and weighs only 15g, assuring high level of comfort for the users.
               
            

@&#INTRODUCTION@&#

Diseases or injuries in the neuromuscular system may lead to levels of motor disabilities. People with brain injury, locked-in-syndrome, cerebral palsy, muscular dystrophy or amyotrophic lateral sclerosis may have very limited movement ability in their limbs, which can also be accompanied by speech impairments. These conditions leave them completely dependent on others, leading a very poor quality of life. Assistive technology has recently had a paramount impact on the life of people with disabilities, by helping them to overcome their limitations, and to benefit from social interactions, education, and even employment.

In recent decades, human–computer interfaces (HCIs) have been developed to provide people with disabilities new channels of communication with others, as well as the means to control their environment. Different approaches have been used in these systems to detect the user׳s intention, including the use of electroencephalogram (EEG) [1–3] and electromyogram (EMG) signals [4,5] Most people with severe motor disabilities can still control their eye movements. Therefore, eye-tracking systems have also been used to detect the user’s selection among several visual options. Different techniques have been used to detect eye movements which includes the use of a search coil [6,7], infrared oculography [8,9], video-based eye tracking [10], dual Purkinje image technique [11] and electrooculography.

Electrooculography (EOG) is the technique of recording eye dipole (negatively charged retina relative to the cornea) from the skin around the eyes. The signal is usually recorded horizontally and vertically [12]. It mainly reflects the gaze direction with a practically linear relationship for gaze angles of up to ±30° [13].

In contrast to other aforementioned methods for eye tracking, systems that use EOG signal are easy to operate, more accurate and reliable [14]. The EOG signal has relatively high amplitude and a simple relationship with the eye movements, which makes its interpretation and connection with the user׳s intention simple in comparison to EEG and EMG signals. Detection of eye movements with EOG is non-invasive, inexpensive and fast which make it suitable for real-time applications. In addition, easy classification of different gestures that the eye can take, makes the use of eye movements a relatively rich channel of communication with a relatively high transfer rate [15] for people with severe motor disabilities in comparison to EEG or EMG based systems.

The EOG signal has been used in several studies to develop human computer and human machine interfaces. It is used to handle wheelchairs [13,16–18] control robots [19–21], recognize reading activity [22], move cursor position on the computer screen [23], type in a virtual keyboard [24,25], and play games [26].

There are certain challenges associated with interpreting the EOG signals, including its baseline drift which can hardly be filtered due to the low frequency nature of the EOG signal. In addition, the EOG signal is slightly different for each participant and its amplitude depends on the background light of the environment. Without using a practical automated algorithm, frequent recalibration is necessary during the EOG analysis.

Different signal processing methods have been utilized to identify eye gestures from the EOG signal. Yamagishi et al. [27] have defined eight directional eye movements and Banik et al. [28] have considered different specific sequences of horizontal eye movements as commands and used threshold analysis of the signal amplitude to detect them. Shang-lin et al. [29] have applied a four-level thresholding on the signal amplitude in order to identify eye movements in eight directions as well as blinking. Ianez et al. [30] have used thresholds on the derivatives of horizontal and vertical EOG to detect predefined eye movements to left, right, up and down. They used these movements in addition to the blink to control a robot arm. Several statistical parameters of the EOG signal were also tested for off-line saccade sequence recognition [31]. Deng et al. [26] used fuzzy distinction rules for recognition of eye movements in four directions to control an EOG-based game. Usakli and Gurkan [25] have employed the nearest neighborhood algorithm to distinguish eye movements in four directions as well as blinking for typing in a virtual keyboard. Continuous wavelet transform with a radial basis function (RBF) classifier has also been utilized [32] to detect saccadic eye movements over long periods of time. Bulling at al. [33] have used a thresholding method on Continuous Wavelet Transform of the signal to detect saccades, fixations and blinks. They have devised 90 different features based on these characteristics, and used the minimum redundancy maximum relevance feature selection method to select a subset of these features. A support vector machine (SVM) has then been used to detect five specific activity classes. Wu et al. [34] have also used wavelet transform and a SVM classifier to identify three commands of looking up, single blink and double blinks.

In recent years, wearable HCI systems have been developed to provide the users, comfort and convenience in long term use. The main features of these systems are the small size, light weight, and low power consumption. The trade-off between wearability and the performance is the main challenge in developing these systems. Bulling et al. [14] have proposed an EOG-based embedded eye tracker. This wearable device consists of a pair of goggles and a wearable EOG processing unit. The goggles include dry active electrodes, a light sensor, and an accelerometer, all attached to a frame. The Goggles are connected to the processing unit with a shielded 14-core cable. The device has been aimed for mobile HCI applications, activity recognition and context awareness. Kirbis and Kramberger [35] have designed a mobile device for electronic eye gesture recognition. It has been developed to control different computer applications and home appliances wirelessly based on the users’ eye gestures, and uses a Bluetooth or alternatively an IR module for transferring data to the computer. Barea et al. [36] have developed an EOG-based HCI system using a commercially available video eyewear, with dry electrodes installed on it. The eyewear is connected to the main desktop system via ZigBee, which controls a home automation system using a WiFi network. Recently, Wu et al [37] have proposed a new classification algorithm to detect eye movements in eight directions as well as blinking, showing its performance with a wireless EOG acquisition device.

The previous studies reviewed above have examined the developed EOG-based HCI systems on people without disabilities. Our observations show that this can be a major drawback; since people with disabilities usually have restrictions even in moving their eyes or may have involuntary movements in their head and therefore a system that demonstrates a high performance when evaluated by people without disability, may be unpractical for people with disabilities. The ease of use is a very important factor in a practical HCI for people with disabilities, depending on the physical specifications of the system, the eye gestures protocol, and the user interface. In addition, the systems developed in some previous studies are very sensitive to several threshold values and demand special calibrations for each individual user, and repeated calibrations in long runs for proper functionality [24,26,27], which makes the use of the system difficult. Moreover, the algorithms proposed in some studies [14,33] are too heavy to be used for online processing in real applications.

This paper describes the development and evaluation of a wearable EOG-based human computer interface, aimed to be practical and comfortable for people with disabilities. Eye gesture commands were defined based on the limitations observed in eye movements and blinking in people with disabilities. An adaptive algorithm was proposed to improve the system performance in detecting eye movements in eight directions as well as double blinks, and to eliminate manual calibration for each user, and repeated calibrations in long runs. An efficient typing environment is proposed that allows the user to select each desired character by only two eye movements and two double blinks. The system consists only of a miniaturized hardware, which is mounted on eyeglasses and is connected wirelessly to a laptop computer via Bluetooth. The system has been designed for low power consumption, small size and lightweight to improve the wearability. The performance of the system in detecting eye movements was evaluated by participants with severe motor disabilities using a game environment. The typing environment was also evaluated by participants without disabilities.

A novel wearable EOG-based human computer interface has been proposed. The system consists of wearable hardware mounted on a glasses frame, recording and transmitting horizontal and vertical EOG directly to a laptop, and a computer program that receives and analyses the EOG data in real time for recognizing eye movements and interpreting them as commands for an efficient user interface for typing.

The system was aimed as a wearable device with small size, lightweight and low power consumption, to guarantee the user׳s comfort in real applications. The block diagram of the hardware is shown in 
                        Fig. 1. It consists of an ADS1294 integrated analog front-end for bio-potential recording (Texas Instruments), a PIC16lf88 micro-controller (Microchip) to program the front-end chip and relay the recorded signals to a BTM182 Bluetooth module (Rayson Technology) that transmits the data to a laptop.

Ag/AgCl electrodes were used at positions shown in Fig. 1 to record two channels of horizontal and vertical EOG. The sampling rate was considered 250 sps, high enough for the low-frequency EOG signal. The data were then transmitted wirelessly with the baud rate of 19,200bps to the computer. 
                        Fig. 2 demonstrates a prototype of the developed device mounted on the temple of eyeglasses. The total weight of the hardware (excluding the weight of the eyeglasses) is 7g without battery and 15g with a 3.7V, 260mAh Li/Ion battery which is mounted on the back of the board. The electronic board is 4.5×2.5cm. The total power consumption of the hardware was measured 123mWh, and it works continuously for 6h without the need to recharge the battery. The radio works properly up to 10m from the computer.

The software was implemented in MATLAB environment version 7.10 (Mathworks Inc.), to analyze the received EOG signals, recognize the nine defined eye gestures in real time and to provide an environment for efficient typing. In another version of the software, the identified eye gestures were used to run a simple game, aimed to evaluate the ability of participants with disabilities to work with the system and help them learn to work with that efficiently.

The authors encountered problems in reading the serial port associated with the Bluetooth link directly in MATLAB. The port could not be opened most of the times or with delays up to several minutes. Therefore, a serial capture program, RealTerm (Version 2), was called from MATLAB, that received the data and passed it to the developed program.

A robust real-time algorithm was developed to process the horizontal and vertical EOG signals and to recognize the user commands based on his/her eye gestures. The pre-processing step includes an electrode-off detection algorithm, and filtering for the elimination of the noise. The processing steps are aimed to detect saccadic eye movements and blinks, and to follow their sequences to identify the predefined eye movements which a user is supposed to perform to control the interface. The protocol was defined based on preliminary evaluation of the limitations that participants with disabilities had in moving their eyes and blinking. These limitations include their difficulty in moving their eyes which makes their eye movements very slow, and difficulty in closing their eyes firmly and instantly. In participants with cerebral palsy involuntary head and body movements usually affected the EOG signal. However, since the system was wearable and wireless, these effects were minimum and participants could still manage to work with the system.

In the typing environment, the user is supposed to look straight forward at the center of the computer screen (The laptop was placed in front of his/her face). To move the cursor on the virtual keyboard, the user should move his/her eyes for more than 30° in a direction and return to the center from 0.4 to 1.5s later. This specific protocol and the 30° threshold were considered for limiting false command detections due to usual eye movements in exploring the screen. Because eye movements were slow in people with disability, a return of sight to the center of the screen was allowed for up to 3s in their experiments with the game. The eye movements are recognized in eight main and diagonal directions. Selecting a key on the keyboard was initially performed by closing the eyes for 1s. However, participants with disability usually could not firmly and timely close and open their eyes. Therefore the protocol was changed to a double blink in less than 0.8s. This also reliably prevented unwanted selections due to involuntary eye blinks. The processing steps are demonstrated as a flow chart in 
                        Fig. 3, while the command recognition algorithm is shown in 
                        Fig. 4.

In long-term recordings, the electrode connections may become weak due to the head movements and sweating, which results in an unstable baseline and high amplitude noise on the signal. This leads to undesired cursor movements or selections at the typing environment. To avoid this, the algorithm continuously monitors the standard deviation of the recorded signals. If the standard deviation in a time window exceeds an expected range (Min=0.1nV and Max=100mV), the input signal is not considered valid, and no further processing steps are taken until it comes back to the range.

A fifth order Butterworth low-pass filter with cut-off frequency of 20Hz was employed to remove almost all high frequency noises including those from the power supply and the EMG of the muscles close to the electrodes. In addition, to cancel the baseline drift of the signal, a second order Butterworth high-pass filter with cut-off frequency of 0.05Hz was implemented.

Blinks appear mostly in the vertical EOG signal and have to be detected, first because two consecutive blinks are considered the ‘select’ command, and second because the algorithm has to distinguish them from saccadic eye movements that are considered the ‘move’ commands. Here, detection of the eye blinks was accomplished by a template matching technique. A 400ms (95 samples) template was created by averaging 50 sample recorded blinks from one participant after aligning them based on the time of their peaks. The template was used for all the experiments. The detected blinks are then replaced in the vertical signal by a linear interpolation between the first and the last point of the blink.

When the user moves his/her eyes, based on the direction and the angle of the eye movement, rapid changes with different amplitudes appear in vertical and/or horizontal EOG signals. The derivative of both vertical and horizontal EOG signals is calculated to detect these saccadic eye movements. Any significant positive peak of horizontal and vertical derivative signals means right and up saccades, respectively, while the negative peaks indicate left and down saccades. The amplitude of these changes varies among individuals, especially among people with disabilities. It also depends on the environmental illumination. Therefore, the thresholds that are applied on the derivative signals to identify saccadic eye movements from noise were considered adaptive. The four thresholds (positive and negative threshold on horizontal and vertical signals) are determined based on concurrent values of peak and noise level as described in Eq. (1). In each step of the analysis, the peak or noise levels are updated based on Eqs. (2) or (3) if the derivative signal is identified as an eye movement peak or if it is not, respectively. In these equations, ‘a’ and ‘b’ are constant values between 0 and 1 (0.5 for ‘a’ and 0.825 for ‘b’ were used in this study).
                              
                                 (1)
                                 
                                    
                                       
                                          Threshold
                                       
                                       =
                                       
                                          Noise
                                       
                                       
                                       
                                          Level
                                       
                                       +
                                       
                                          a
                                       
                                       .
                                       
                                          (
                                          
                                             
                                                Peak
                                             
                                             
                                             
                                                Level
                                             
                                             −
                                             
                                                Noise
                                             
                                             
                                             
                                                Level
                                             
                                          
                                          )
                                       
                                    
                                 
                              
                           
                           
                              
                                 (2)
                                 
                                    
                                       P
                                       e
                                       a
                                       k
                                       
                                       L
                                       e
                                       v
                                       e
                                       l
                                       =
                                       
                                          (
                                          
                                             1
                                             −
                                             b
                                          
                                          )
                                       
                                       .
                                       P
                                       e
                                       a
                                       k
                                       
                                       L
                                       e
                                       v
                                       e
                                       l
                                       +
                                       b
                                       .
                                       S
                                       i
                                       g
                                       n
                                       a
                                       l
                                    
                                 
                              
                           
                           
                              
                                 (3)
                                 
                                    
                                       N
                                       o
                                       i
                                       s
                                       e
                                       
                                       L
                                       e
                                       v
                                       e
                                       l
                                       =
                                       
                                          (
                                          
                                             1
                                             −
                                             b
                                          
                                          )
                                       
                                       .
                                       N
                                       o
                                       i
                                       s
                                       e
                                       
                                       L
                                       e
                                       v
                                       e
                                       l
                                       +
                                       b
                                       .
                                       S
                                       i
                                       g
                                       n
                                       a
                                       l
                                    
                                 
                              
                           
                        

Initial system evaluations during the design process demonstrated that this adaptive algorithm significantly improves the systems performance for users with disability.


                           
                           Fig. 5 illustrates the process when the eyes moved to up-left side and returned to center and then two blinks occurred.

The sequence of detected saccadic eye movements is then analyzed to recognize eight user commands. The commands were defined as rapid movement of the eyes to one of the main (up, down, right or left) or diagonal (up-right, up-left, down-right or down-left) directions for more than about 30°, and then returning the eye gaze to the front. Fig. 4 illustrates the flow chart of recognizing the commands based on the sequence of eye movements.

In this study, two different user interfaces, a simple computer game and an efficient environment for typing, were developed. They operate based on the commands produced by the eye movements defined and extracted in the previous stages.

A simple computer game that works based on the user׳s eye movements as defined in Section 2.2 was developed. This game is considered a tool to evaluate if people with movement disabilities can work with the proposed system by performing eye movements as required by the system. Moreover, it can be used by them to improve their performance in working with the system.

In this game, a soccer player with a ball appears in a green field. The user is supposed to kick the ball into the goal area by moving his/her eyes in the appropriate direction. In the first stage of the game, the soccer player is on the right side and the goal area is on the left (
                           Fig. 6(a)). Therefore, the user should move his/her eyes to the left side and return to the center. By this action, the ball crosses the goal line, and the user is encouraged by a clapping sound effect. In this stage of the game, other commands (eye movements in other directions) would have no effect. The stage will be completed if the user can successfully kick the ball five times. The next stage of the game starts by blinking twice. The game was considered in four stages, to practice eye movements towards the four main directions.

The main user interface developed for this system for typing is based on eight identified eye movements plus double blinking. As shown in Fig. 6(b), in the developed keyboard, letters and numbers are divided into nine groups. The user can move a cursor on the keyboard by each eight predefined eye gestures, and select the desired group by a double blink. Then, the second page of the keyboard appears (Fig. 6(c) as an example) that contains all the letters and numbers in the selected group as well as the specific characters of DOT (.), DEL (for deleting the last typed character), SPACE, CLEAR ALL (for deleting all typed characters) and BACK (for going back to the main page of the keyboard). The selected letter is then typed on the white space at the bottom of the main page.

The interface is designed in such a way that the user can type each desired letter or number by at most two eye movements, and two double-blinks, which allows for a uniform and appropriate speed of typing.

To demonstrate the functionality of the proposed system, two series of experiments were performed, one by a group of able-bodied people that examined the typing environment, and the other by a group of people with disabilities that tried the game. It was not possible to test the typing environment by people with disabilities, since those that could be encouraged to participate in these experiments were either too young to use a typing environment or unfamiliar with the English alphabet. However, the signal processing part of the system is the same for typing and game environments and it is supposed that if participants with disabilities can perform well in the game environment, they also can work appropriately in the typing environment. All the participants took part in the experiments voluntarily, after getting permission from them or their parents. The procedure was described for them and an informed consent was obtained. The experiments were performed in the presence of the parents of participants with disabilities. Moreover, the experiments were ceased immediately if a participant hesitated to continue.

To start working with the system, each participant was seated in front of a laptop equipped with the Bluetooth module. The participant put on the recording eyeglasses and the electrodes were mounted at the proposed places around his/her eyes (Fig. 1). The user was asked to look straight at the center of the screen (the laptop was placed in front of the face) for the first 30s during which the system automatically detects the baseline of the signals. The measured baselines were then compensated throughout the experiment. No more calibration procedure is necessary for the system. On instruction, the participant tried to work with the system by performing the eye movement protocols. When the user was able to perform each eye movement twice without any problem, the main experiment started. In both series of experiments, the performance of the system was quantitatively evaluated.

Six volunteers without disabilities aged 16–50 (Mean±Std=32.2±12.5 years), three females and three males, were asked to participate in the experiment by typing five short sentences, “IT IS OK”, “BE FRESH”, “FIX MY PC”, “SAY YOUR MIND”, and “I HAVE A COLD”. The sentences which consisted of 51 characters were not specifically selected. Different eye movements are necessary to be performed to type the characters throughout these sentences. The participants had no previous experience of working with this system, except participant #6 that had previously performed experiments with the system for several times. The participants were supposed to use eight different eye movements and blinking according to the predefined protocols to type the characters of each sentence one by one. The experiment was repeated three times for participants 1–5, and 6 times (in 2 sections) for user 6. Repeating the experiments for participant #6 was to investigate how much the results may improve if a user works frequently with the system. The participants had 2 minutes to rest before repeating the experiment.

During the experiments, a video of the face of the participant as well as the desktop screen was recorded, and then it was carefully examined to identify the actual eye movements performed by the user and evaluate the performance of the system in detecting these eye movements. Based on the results, true positive (TP), false negative (FN) and false positive (FP) counts were taken to calculate the precision TP/(TP+FP), sensitivity TP/(TP+FN), accuracy (TP+TN)/(TP+TN+FP+FN) and f1-score (2TP/(2TP+FN+FP)) of detecting eight eye movements and double-blink by the system as tried by the participants [38].

The main goal of this study has been to develop an interface for typing for people with disabilities who are not able to speak. However, participants with disabilities in this study were not able to type in English which was not their native language, and working with the typing interface was not so easy for them due to the focus that is needed for typing, limitations they had in performing eye movements, and sometimes involuntary movements in their head. Moreover, some of the participants were too young and some suffered from depression. These factors made it very difficult to motivate them to participate in an experiment with the typing environment. Therefore the performance of the system for participants with disabilities was evaluated using simple and motivating interface of a game.

Ten people with severe movement disabilities that could not speak clearly were invited to participate in the experiments. In the first session, their general condition was evaluated and they were trained to do specific eye movements necessary for working with the system. The criteria for selecting the participants to continue performing the main experiment were the ability to move their eyes and blink. Two of the participants suffered from both physical and mental disabilities, and it was concluded that their training would take too much time. One of the participants could not move his eyes vertically, and another one could not continue to participate in the experiments. Therefore, only six participants performed the main experiments. These participants were two males aged 6 and 26 and a female aged 7 with cerebral palsy. They had involuntary head movements, could hardly move their hands and legs, and could speak very unclearly, except the 26 years old participant that could not speak at all. The other participant was a female aged 8 with traumatic brain injury, who could hardly move her hands and legs and could hardly speak. The last two participants were females aged 49 and 59. The former suffered from ALS for 6 years, and the latter for 2 years, and she was also diagnosed with depression. None of them could have any movements in their hands and legs and could not speak.

In the second session, the participants were encouraged to learn and work with the developed game. The main experiments were held in the third or later sessions. It was intended to perform the main experiment twice for each participant. However, the second experiment could not be performed for two of the participants due to the lack of enough motivation. The experiment was repeated for more several times for two of the participants in order to investigate the possible improvement in the performance by increased experience.

A video of the face of the user and the desktop screen was recorded during the experiments. The video was then carefully evaluated to calculate the number of eye movements detected correctly by the system. The total number of eye movements performed by the user until performing a successful one and the ratio of these two values, as the rate of success for each participant, were extracted from the videos.

@&#EXPERIMENTAL RESULTS@&#

The developed system was evaluated by conducting two series of experiments. Six healthy participants used the system for typing five sentences, and the system performance was assessed based on their experience. In addition, six people with disabilities used the system to play a simple game. The goal was to have a realistic assessment of the usability of the system for people with disabilities, and the associated challenges for actual users of the system.


                        
                        Fig. 7(a) demonstrates the accuracy of the system in the experiments of participants without disabilities. The average accuracy for all the experiments was 82.91%. For five of the six participants the accuracy has improved through the three runs of the experiments. User #6 achieved the maximum accuracy of 95.96% in her fifth experiment. It should be noted that the errors in recognizing eye movements might be due to the system misidentification of the eye movements and double blinks, or the user׳s errors in not complying with the protocol. Therefore, the accuracies that are reported in this section are for the whole system including the user that works with the system, and reflect both the performance of the algorithm and the eye movement protocol that is used in this system.

The ideal time to type a character with the proposed system which is the minimum time for performing two eye movements and two double blinks as required by the algorithm is 5s, equivalent to a typing rate of 12 characters per minute. This value is obtained by considering no time for the user to determine the next character and find it on the keyboard, and no errors in typing. In Fig. 7(b), the average typing rates that are obtained from the experiments are demonstrated. The typing rates are calculated by dividing the total number of characters in all five sentences typed in each experiment to the time spent to type them correctly and completely. This accounts for the time spent by the user to determine and find the next character on the keyboard and to correct any mistakes due to a system or user error in typing, and therefore provides realistic values of the rate of typing. The total average typing rate was obtained 4.52cpm (character per minute). Moreover, the communication transfer rate [39] of the system is 36.1bpm (bit per minute). It is important to notice that a character need four commands to be written.

The average typing rate of a character is increasing for five of six participants, suggesting improvements in typing speed by practice. However, a nonparametric Friedman׳s statistical test on three first experiments of six users did not revealed any significant difference between the accuracies in these experiments (p<0.05). Noticeably, the accuracies are descending for user 1. A careful review of the video demonstrated that this participant blinks too much involuntarily and her vertical eye movements are accompanied with these blinks which results in increased mistakes of the algorithm in detecting her eye movements. The blinks increased significantly when she proceeded in the experiments which leaded to lower accuracies in later experiments. Excluding the data of user 1 from Friedman׳s test resulted in statistically significant differences among accuracies in the three experiments (p<0.05). The test was also repeated for the typing rates with and without the data from user 1, and the results was the same as the accuracies.

User 6 achieved the best typing rate of 7.17cpm after six experimentsand a communication transfer rate of 77.8bpm. It should be noted that user 6 had also some previous experience in working with the system which can also describe high accuracy and high typing rate from the first experiments for this user (Fig. 7).

The confusion matrix for identification of the user commands by the proposed algorithm was obtained for all the 1071 characters typed by the participants (
                        Table 1). F1-score, accuracy, precision and sensitivity are presented for each class separately. F1-score is also calculate by macro-averaging (shown with an M index) and micro-averaging (shown with µ indices) for the whole data. F1-scoreM which shows relations between data׳s positive labels and those given by a classifier based on a per-class average was obtained 86.7%. F1-scoreµ which shows relations between data׳s positive labels and those given by a classifier based on sums of per-text decisions was gained 82.9%.


                        
                        Table 2 presents the results of the experiments performed by participants with disabilities. The average success rate of eye movements for all the participants was 61.5%. The average success rate over the experiments is presented in the last column in TABLE II. The first two participants had a small improvement in their second experiment compared to the first one. The experimenter could not gain the interest of the third and fifth participants and they left the second experiment before it was finished. However, the progress was significant in the performance of the fourth 26-year-old and sixth 49-year-old participants who achieved a performance of 83.3% and 76.9% respectively after repeating the experiment for several times. This demonstrates how well the participants with disabilities could adapt themself with the system protocol and improve the performance. However, it should be noted that a Kruskal–Wallis test on the accuracies in the first two experiments of four participants with disabilities does not revealed any significant difference.

The horizontal and vertical EOG signals recorded from one of the participants is shown in 
                        Fig. 8. This demonstrates how severely the involuntary head movements of the participant affect the signals. Despite this, it was observed that the participants were able to adapt their eye movements with involuntary movements of their head, so that the system would recognize their intentions correctly.

@&#DISCUSSION AND CONCLUSION@&#

In this paper, an efficient wireless EOG-based human–computer interface was presented that works based on saccadic eye movements in eight directions, and double-blink as user commands. The system׳s wearable hardware, efficient algorithm and the environments developed for working with the system as an interface were described, and the system׳s performance in detecting eye movements for people without and with disabilities was evaluated. Performance of the typing environment was evaluated for people without disabilities. The results suggest that the defined eye movements and double-blink could be detected with high accuracy by the proposed adaptive algorithm, enabling the participants to effectively type with the system. However, the aim of developing this system was to help people with severe motor disabilities who are not able to move their limbs and clearly speak, to communicate with others, mention their needs, intentions and even ideas, and study more easily. The experiments performed by people with severe movement disabilities suggest that if they have the ability of moving their eyes and blinking, they can learn to use the system practically and the system can adapt to detect their eye movements and blinks. Therefore the system can potentially be a practical channel of communication for people with disabilities by allowing them to type characters just by their eye movements.

The adaptive thresholding algorithm that was implemented in the proposed system has eliminated the need for any manual calibration of the system in the beginning and during the recording for different users or in different background illumination. The only calibration is automatically performed in the first 30s of the recording with the system while the user is asked to gaze straight to adjust the baseline of the signals. In contrast, the systems proposed by Yamagishi et al. [27] and Deng et al. [26] necessitates a manual testing of the eye-sight level and setting specific constant thresholds for each individual at the beginning of each test. Borghetti et al. [24] have also used an online correction algorithm for the voltage thresholds and have also provided an interface for manual adjustments.

The typing environment that was designed in this study, allows for typing each desired letter or number by just two eye movements and two double blink selections. This feature results in a high and uniform rate of typing (ideally 12cpm). Previous studies used an ordinary alphanumeric keyboard [27] or a 6×6 matrix of letters and numbers [24,25]. On these keyboards, the typing speed is not uniform and depends on the sequence of the desired characters to be typed.

The characters on the keyboard used in this study were in alphabetical order, which help the user quickly find the characters to be typed. The characters can be arranged in other ways to improve the accuracy or speed of character selection. For example, since the accuracy of the system in detecting main directions is higher that diagonal directions, characters with higher rate of usage may be placed on the former and others on the latter. Moreover, a dynamic keyboard that predicts the next character based on a dictionary or probability of letter sequences and arrange the characters accordingly on the keyboard may be used [40]. However, it should be noted that in typing each letter, some time is spent to find the next intended letter on the keyboard. An unfamiliar character order and also a dynamic keyboard increase this time significantly and even can make an unexperienced user completely confused. An autocomplete capability however may benefit the keyboard significantly.

Based on the experiments conducted in this study, the accuracy of recognizing nine defined eye gestures by the proposed system was about 83%. This sounds low in the range of accuracies (82–100%) reported in previous studies [14,20,25–32,34–37] for EOG-based recognition of eye movements. However, it should be noted that the experiments conducted in those studies are not as extensive as those conducted in this study are. Therefore the authors believe that the results may not be comparable. For example, the experiment carried out by Usakli and Gurkan [25] consisted of typing only five characters by 20 participants, by Yamagishi et al. [27] twelve characters by two participants and by Borghetti et al. [24] fourteen characters by 20 participants. On the other hand, some other studies have just evaluated the success rate of their algorithms in detecting eye movement commands not for a specific application. For example, the experiment carried out by Wu et al. [34] consisted of performing four commands of looking up, single and double blinks by 6 participants, Shang-lin et al. [29] evaluated detection of eye movements in eight directions that were repeated for ten times by 8 participants. Banik et al. [28] have also evaluated identification of performing defined sequentially horizontal eye movement by 5 participants. In comparison, in this study, fifty-one characters were typed three times or more (1071 characters) by 6 participants. In order to type these characters, more than five thousands eye movements and double blinks were processed. These extensive experiments were conducted in this study to better simulate the real application of the proposed practical system. The duration of each experiment was too longer in this study and therefore the fatigue in working with the system in order to comply with the protocol and keep the necessary continuous focus, especially for those participants who were not experienced in working with the system, have affected the resulting accuracy. One of the participants who was experienced in working with the system, achieved an accuracy of 96% in her fifth experiment in this study; an accuracy, which is at the top of the reported accuracies in the similar systems. This demonstrates the importance of the experience in efficiently working with the proposed system. However, more extensive experiments must be conducted to confirm such ideal accuracy in working with the system.

In this study, the functionality of the proposed system was also evaluated for real users with disabilities, while none of the previous studies reported the results of such experiments. The experiments showed that although the impairment and involuntary head movements of the people with disabilities significantly affected the EOG signals, they were actually able to perform the computer game with their eye movements. It was observed that they figured out how to coordinate their eye movements in the presence of involuntary head movements to achieve the desired results. The average success rate in these experiments was about 61.5% which may be considered low for a practical system. However, it should be noted that three of the participants in these experiments were too young and since controlling involuntary eye and head movements might be hard for them to work with the system, it was difficult to keep their interest to focus on the game. This was also true for another participant that was diagnosed with depression, and her interest could not be kept on the game. Although the experiment interface of the game for participants with disabilities was very simple in comparison to of the typing environment, the accuracies for participants with disabilities are significantly lower than for participants without disabilities (Mann–Whitney test, p-value=0.002<0.05). This suggests that although people with disabilities participated in the main experiments of this study can all move their eyes and blink, their performance in complying with the specific protocol of the system is significantly lower than participants without disability. This emphasizes the necessity of including experiments with people with disability in studies for development of practical HCI systems for them. On the other hand, the 26-year-old and 49-year-old participants managed to increase their success rate of doing the correct movement to 83.3% and 76.9%, respectively. This is very promising for real application of the proposed system by people with disabilities, and suggests that they can learn to improve their performance to values comparable to able-bodied participants. However, more extensive experiments are necessary to confirm this.

For most of the participants with disability, working with the system was obviously motivating. It seems interesting for them to be able to proceed in the game by their intent. This was more significant in adults who understood that the system may also let them later to type their intentions. One of the participants was not interested in the game and just participated in one round of the experiment. The specific eye movements necessary for the system may be difficult for him to learn since he suffered from cerebral palsy and was only 7 years old. The other participant with ALS who was 59 years old suffered from depression and was not interested to continue performing the experiments.

None of these participants was familiar with English alphabet and we could not evaluate their actual performance in typing with the system. The eye movement protocol defined in this system was the same for the game and the typing environments except that in the latter diagonal eye movements can also be detected and used to reach the desired characters faster. Therefore we expect that the performance in working with the typing environment can be as good as the game. However, experiments with the typing environment performed by people with disabilities are necessary to have a realistic assessment on the systems performance for target users. Moreover, the participants that could be recruited for this study was heterogeneous regarding their age and the reason for their movement disability. Experiments with more participants in each group can help to better evaluate the system performance for each group and the ways to optimize the system for them.

The hardware of the system is significantly smaller (26×45mm) with lower power consumption (123mW) in comparison to other EOG-based HCI systems found in the literature. A similar system developed by Bulling et al [14] consisted of two parts one 42×15mm and the other 82×56mm, connected by a cable, with a power consumption of 828mW. Kirbiš and Kramberger [35] developed a portable system with a power consumption of 260mW while they did not report the size of their system. The wireless EOG signal acquisition device developed by Wu et al [37], was approximately 45×32mm in size and had a power consumption of 188mW.

The performance of the developed system can be improved by optimizing both the hardware for its size and power consumption, and the algorithm for adjustability to the needs and constraints of each specific user. Using a Bluetooth 4 (low energy) module can significantly reduce the power consumption of the device as a wearable system. Using an accelerometer to detect and compensate for involuntary head movements, and simultaneous recording of the facial electromyogram and the electroencephalogram can improve the flexibility of the system and result in higher accuracy as well as the rate of communication. The software can be converted to a virtual keyboard to provide the possibility of working with other useful computer programs, e.g. for browsing the web, working with educational interactive programs, playing different games, and even controlling appliances in the user’s home. In addition to the users who cannot speak, such a system can be a computer interface for those who have limitations in using their hands for more efficient use of the computers. Moreover, such a wearable system can be considered as an extra interface for ordinary computer users for faster and easier ways of working with the computers.

The authors have a registered national patent related to the subject of this manuscript: A. Mahnam, S. Behtaj, S. Soltani, “Able Eyes: An EOG-based human computer interface for expression of intentions for people with disabilities”, patent number: 75073, 2012.

@&#ACKNOWLEDGMENT@&#

The authors would like to appreciate the kind collaborations provided by Mrs Samane Behtaj who helped partly in the implementation of the hardware.

@&#REFERENCES@&#

