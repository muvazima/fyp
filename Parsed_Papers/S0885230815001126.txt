@&#MAIN-TITLE@&#Concept-to-Speech generation with knowledge sharing for acoustic modelling and utterance filtering

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           We present two knowledge sharing approaches for CTS generation.


                        
                        
                           
                           Syntactic features replace prosody phrasing features in HMM-based speech synthesis.


                        
                        
                           
                           Acoustic features are used to filter synthetic utterances for one input concept.


                        
                        
                           
                           The HMM-based acoustic model yields comparable results without prosodic phrasing.


                        
                        
                           
                           Utterance filtering can remove inferior synthetic utterances for the input concept.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Concept-to-Speech

Speech synthesis

Hidden Markov model

Natural language generation

@&#ABSTRACT@&#


               
               
                  A Concept-to-Speech (CTS) system converts the conceptual representation of a sentence-to-be-spoken into speech. While some CTS systems consist of independently built text generation and Text-to-Speech (TTS) modules, the majority of the existing CTS systems enhance the connection between these two modules with a prosodic prediction module that utilizes linguistic knowledge from the text generator to predict prosodic features for TTS generation. However, knowledge embodied within the individual modules has the potential to be shared in more ways. This paper describes knowledge sharing for acoustic modelling and utterance filtering in a Mandarin CTS system. First, syntactic information generated by the text generator is propagated to a hidden Markov model (HMM) based acoustic model within the TTS module and replaces the symbolic prosodic phrasing features therein. Our experimental results show that this approach alleviates the local hard-decision problem in automatic prosodic phrasing for Mandarin CTS systems and achieves a comparable performance to the traditional approach without explicit prosodic phrasing. Second, the acoustic features of multiple synthetic utterances expressing the same input concept are utilized to evaluate the utterance candidates. With this ‘post-processing’ mechanism, our CTS system is able to filter out inferior synthetic utterances and find an acceptable candidate to express the input concept.
               
            

@&#INTRODUCTION@&#

The transfer of human language proficiency to machines has been explored for decades. One branch of exploration, the Concept-to-Speech (CTS) approach, endeavors to enable machines to produce speech based on abstract representations of the sentence to be spoken. Because humans articulate their ideas by translating them into syntactic, phonological and phonetic codes that guide the vocal tract articulator to produce acoustic waveforms, a similar process is to some extent adopted by CTS systems wherein the input concept undergoes syntactic and phonological processing and eventually drives the ‘articulator’ to synthesize speech. Another type of human language proficiency, reading text out loud, has also been transferred to machines. This technique is known as Text-to-Speech (TTS) synthesis. One obvious difference between CTS and TTS is that the CTS approach must convert conceptual representations into sentences before articulating them while TTS directly accepts a concrete sentence as input. Because conceptual representation is typically domain dependent, CTS is incorporated into a spoken dialogue system wherein conceptual representations can be defined and provided by front-end modules. For example, well-known CTS systems have been deployed in an inquiry system for a water-supply network (Young and Fallside, 1979) and a multimedia medical briefing system (McKeown and Pan, 1997).

Although CTS may appear to be more complex than TTS due to the additional task of sentence composition, CTS can be practically implemented by concatenating a text generator with a TTS module, as shown in Fig. 1
                     (a). This solution has been adopted by several dialogue systems that generate spoken responses (Litman et al., 1998; Rudnicky et al., 1999; Walker, 2000). However, this solution abandons the valuable linguistic knowledge that is available in the text generator, which cannot be fully retrieved by the error-prone text-analyzer in the TTS module.

Finding improvements to knowledge sharing amongst the components of a CTS system is a vital issue in CTS research (Young and Fallside, 1979). The consensus among researchers seems to be that linguistic information such as discourse, semantic and syntactic knowledge permits finer control of prosody modelling in CTS systems. Existing CTS systems based on this idea typically incorporate external prosodic prediction modules to convert linguistic information into prosodic symbols. This idea is illustrated in Fig. 1(b). For example, the system in Pan (2002) utilizes various types of linguistic information and an instance-based learning algorithm to predict ToBI symbols (Silverman et al., 1992) for CTS generation in the medical briefing domain. Other CTS systems apply similar ideas, and their performance confirms that predicting prosodic symbols based on the knowledge from text generation is effective for CTS tasks (Danlos et al., 1986; Theune et al., 1997; Dorffner et al., 1990; Nakatani and Chu-Carroll, 2000; Fawcett, 1990; Teich et al., 1997; Takada et al., 2007; Schnell and Hoffmann, 2004; Hitzeman et al., 1998; Pan, 2002).

Although the above strategy is informative, we wonder whether it represents the best way to address the CTS task, particularly for Mandarin. Initially, the pipeline structure in Fig. 1(b) only shares the linguistic information from text generation with the prosodic model or a prosodic phrasing model for Mandarin. We think that propagating the linguistic information as well as the prosodic features to the speech synthesizer is a worthy pursuit (Badino et al., 2012). However, because automatic prosodic phrasing for Mandarin is challenging, we doubt that the improvements produced by linguistic information in both automatic prosodic phrasing and acoustic modelling can prevent the unacceptable synthetic speech after an erroneous prosodic phrase boundary is predicted by the automatic prosodic phrasing component in the CTS pipeline. Thus, for our Mandarin CTS system, we replace the prosodic phrasing features with linguistic features given by text generation. These linguistic features are directly utilized to cluster the acoustic models for spectra, F0 and duration features without using prosodic phrasing features. Although these linguistic features are more ambiguous for predicting acoustic features than the hand-annotated prosodic features, experiments will demonstrate that the loss in the accuracy of the prediction is more acceptable than the degradation caused by the erroneously predicted prosodic features in the traditional approach. Additionally, our approach avoids the cost of annotating prosodic boundaries and building a prosodic phrasing module for the Mandarin CTS system.

Another imperfection of the existing CTS systems is their pipeline structure wherein the text generator makes decisions regarding text planning and sentence realization without any consideration of the capability of the speech waveform generator. Due to the inevitable sparse distribution of acoustic data, the speech generator may fail to articulate certain words or accomplish the appropriate intonation in certain contexts. One solution is to back propagate the acoustic information to help the text generator avoid these pitfalls during sentence composition. However, acoustic information cannot be collected before the sentence is generated, or at least partially processed, by the speech generator. To address this ‘surface problem’ (Inui et al., 1992), we propose utterance filtering for the CTS generation: first, several synthetic utterances that express the same input concept are generated by the text generator and speech synthesizer in the CTS system; second, the candidates’ acoustic information is collected and propagated to the utterance filter in the CTS pipeline; lastly, synthetic utterance candidates with imperfect acoustic quality can be removed and a more suitable candidate can be used to reconstruct the output speech waveform. In this manner, the quality of the output speech is expected to be improved over conventional CTS systems.

The structure of the proposed CTS method is shown in Fig. 1(c). Because this paper primarily addresses Mandarin CTS synthesis, Section 2 will introduce the baseline Mandarin CTS method, which incorporates text generation based on natural language generation (NLG), automatic prosodic phrasing and HMM-based parametric speech synthesis for Mandarin. In Section 3, the shortcomings of the baseline CTS method will be discussed. Next, the proposed method of knowledge sharing in acoustic modelling and utterance filtering will be introduced. Section 4 will detail the experiments that were performed on both the baseline and proposed methods. Section 5 presents the conclusions of the study.

Although the proposed methods do not require domain-specific knowledge, the experiments in this paper are conducted on a CTS system built in a specific domain using these methods. After all, CTS systems are domain-specific as aforementioned. But the same methods can be used to construct CTS systems for other domains. If the domain-specific CTS system based on the proposed methods has to be utilized in another domain, additional ‘domain adaptation’ techniques such as the one proposed by Tsiakoulis et al. (2014) should be utilized. This topic is not covered in this paper.

This paper focuses on Mandarin CTS systems. The baseline system follows the structure in Fig. 1(b) and will be introduced in this section; Section 2.1 will address NLG based on Komet-Peman MultiLingual (KPML) for Mandarin; Section 2.2 will discuss prosodic phrasing for Mandarin; Section 2.3 will discuss HMM-based parametric speech synthesis. Note that the domain-dependent aspects of CTS systems, primarily in NLG, will not be discussed in this paper.

Rather than using a template-based text generator, the baseline system utilizes a technique from NLG to convert the input abstract representation of the concept into text. The typical structure of an NLG module consists of a text planner, sentence planner and surface realizer (Reiter and Dale, 2000). While the first two components organize the concept and prepare for composing the sentences in a domain-dependent way, the surface realizer converts the text specifications given by the front-end into a string of surface words without using domain-dependent knowledge. Because this paper focuses on the quality of the speech generated by CTS systems, the baseline and proposed system are initiated with a surface realizer and the input to the surface realizer is assumed to be the input to the entire CTS system.

The NLG module in our Mandarin CTS system uses KPML (Bateman, 1996), a surface realizer based on systemic functional grammar (SFG) (Halliday and Matthiessen, 2004). According to SFG, language provides the resources for us to create meaning, represent knowledge and interact with one another. For example, we interact with other people via strategies such as asserting, doubting, etc. because our language provides us with the resources for expressing these speech functions. As one component of language, grammar provides the resources for creating meaning in the form of grammatical sentences (Matthiessen and Halliday, 2009). For example, if we assert or describe something in English, the grammar defines that we must compose the sentence with a subject. We generally choose different language resources to express the intended meaning and compose sentences following the grammatical constraints defined by the grammar. For the grammars defined by SFG, these resources are organized as choices. If we suppose that clauses can be either indicative (e.g., describing) or imperative (e.g., requesting), the grammar provides us with these two choices, as shown in Fig. 2
                        (a), wherein one clause must be either indicative or imperative; if it is indicative, it must contain a subjective and finite element according to the grammatical constraints +Subject (insert a subject) and +Finite (insert a finite element). In SFG, each point of choice defines a System.
                           1
                        
                        
                           1
                           We use System in Serif font to refer to the grammar system in SFG.
                         Each System reflects a minimal grammatical difference (Bateman, 1997). It also describes what choice a particular linguistic unit can choose, what the grammatical constraint for that choice is and when that choice should be chosen.


                        Fig. 2(a) defines only one System. In English, an indicative clause can be either ‘declarative’ (they ran away.) or ‘interrogative’ (did they run away?). Thus, another System for the two choices will follow the indicative path shown in Fig. 2(a). Similarly, more delicate Systems follow the more general Systems. This interrelated set of Systems is called a System Network. These principles of SFG are implemented by KPML. In KPML, the System Network is manually encoded as expert knowledge that will be called for text generation. Note that the condition for choosing either term is not shown in Fig. 2(a).

The input to KPML, which contains the semantic or other information for one potential clause, will guide KPML to make choices in the System Network. The input to KPML can be encoded in Sentence Plan Language (SPL) (Kasper, 1989). The SPL structure for the sentence ‘That balloon is rising’ is shown in Fig. 2(b). In this SPL, RISE, BALLOON and NOW specify the semantic content of the clause while THAT directly specifies the grammatical determiner that must be included in the clause. Initially, KPML begins with clause as the entry condition and then walks forward in the System Network under the guidance of SPL. For example, :tense NOW will guide KPML to choose the present tense in grammatical tense Systems. Although this SPL structure does not specify the choice in MOOD system explicitly, KPML will choose Indicative by default. In this paper, this mechanism is referred to as the default choice mechanism. When KPML reaches the end of the System Network, it can collect grammatical constraints and construct the surface text accordingly. In other words, KPML takes SPL as input and converts it into a surface sentence based on the grammatical constraints acquired from the System Network. The System Network is a language-specific resource that must be loaded prior to text generation. Due to space limitations, the lexical realization of concepts and other aspects of KPML cannot be introduced in this paper.

To utilize KPML for Mandarin, we manually augmented a System Network based on the work in Yang and Bateman (2009). Besides, the lexicon of KPML is augmented with phonemic symbols so that the initial/final and tone sequences of a Mandarin sentence can be directly generated from KPML. Thus, our baseline Mandarin CTS system does not contain a text-analyzer.

As introduced in Section 1, several existing CTS systems use manually compiled rules or statistical tools to predict prosodic symbols based on linguistic information from the text generator. In these CTS systems, typical prosodic symbols include the pitch accent and boundary tone defined in ToBI (Silverman et al., 1992). After being predicted by the prosodic module, these prosodic symbols are realized as the fundamental frequency (F0) or other acoustic parameters by the TTS modules of these CTS systems.

Our baseline CTS system adopts an identical method for prosodic modelling; however, although linguists suggest that Mandarin prosody includes stress structure, pitch variation and prosodic structure (or rhythmic structure, prosodic hierarchy) (Li, 2002), the Mandarin CTS system described in this paper only covers prosodic structure, which is similar to typical Mandarin TTS systems. The consensus among researchers of Mandarin prosody is that one of the most pronounced features of Mandarin speech flow is the chunking and grouping of smaller prosodic units into larger units. At the bottom level, Mandarin syllables generally form polysyllabic chunks or prosodic words (PWs). Next, these chunks further establish the prosodic phrase (PP) and intonation phrase (IP) (Li, 2002; Tseng and Pin, 2004; Cao, 2000). An example of this prosodic structure is shown near the bottom of Fig. 3
                        . Note that this prosodic structure assumes three tiers: the clause, prosodic phrases and prosodic words. Given that the prosodic structure has a fixed number of levels and each level has its own prosodic unit type, the prosodic structure can be equivalently represented by a sequence of the prosodic boundaries between every pair of adjacent syllables in the utterance. For example, the first three prosodic boundaries for the utterance in Fig. 3 are PW boundary, PW boundary and the boundary within PW.

Prosodic phrasing, or predicting the prosodic structure, is essential for Mandarin speech synthesis because prosodic structure correlates not only with pitch variation but also with the pre-boundary lengthening in the utterance (Tseng and Tseng, 2002; Cao, 2000). Specifically, the PP boundary signifies the location of a silent pause within a Mandarin utterance. An incorrectly located PW or PP boundary may result in an unnatural synthetic utterance. More fundamentally, because the prosodic structure influences the listener's perception of the meaning of the synthetic utterance (Cao, 2000), an inappropriate prosodic boundary may make the utterance obscure.

Researchers have proposed several methods for automatic prosodic phrasing in Mandarin speech synthesis. The studies by Cao and Zhu (2002) and Che et al. (2014) confirm that syntactic information can be used as input features for prosodic phrasing. Therefore, we utilize the syntactic information provided by KPML to predict prosodic boundary sequences in our baseline Mandarin CTS system. The syntactic information provided by KPML will be discussed in Section 3.1.2. In the baseline system, the sequence of prosodic boundaries is converted into the prosodic phrasing features listed in Section 2.3. These features are then added to contexts of the HMM-based acoustic model. The HMM-based parametric speech synthesizer then generates the acoustic parameters such as the F0 contour. Note that the prosodic phrase (PP) boundary explicitly specifies the location of a silent pause in the utterance. The HMM model for a silent pause is always inserted after a PP boundary so that a perceptible break can be generated by the HMM-based speech synthesizer.

The TTS module of our CTS system adopts an HMM-based parametric speech synthesis framework (HTS) (Zen et al., 2009) due to its flexibility in incorporating various contextual features and relatively low requirements on the size of the training data corpus. The core of HTS consists of first modelling the distribution of the acoustic features 
                           O
                         given the corresponding contexts 
                           W
                         and then predicting the acoustic features 
                           o
                         for the target context 
                           
                              w
                           
                        . These two steps are shown in (1) and (2) (Zen et al., 2009),
                           
                              (1)
                              
                                 
                                    
                                       
                                          
                                             λ
                                          
                                       
                                       ˆ
                                    
                                 
                                 =
                                 arg
                                 
                                    max
                                    
                                       
                                          λ
                                       
                                    
                                 
                                 
                                    p
                                    (
                                    
                                       
                                          O
                                       
                                    
                                    |
                                    
                                       
                                          W
                                       
                                    
                                    ;
                                    
                                       
                                          λ
                                       
                                    
                                    )
                                 
                                 ,
                              
                           
                        
                        
                           
                              (2)
                              
                                 
                                    
                                       
                                          
                                             o
                                          
                                       
                                       ˆ
                                    
                                 
                                 =
                                 arg
                                 
                                    max
                                    
                                       
                                          o
                                       
                                    
                                 
                                 
                                    p
                                    (
                                    
                                       
                                          o
                                       
                                    
                                    |
                                    
                                       
                                          w
                                       
                                    
                                    ;
                                    
                                       
                                          
                                             
                                                λ
                                             
                                          
                                          ˆ
                                       
                                    
                                    )
                                 
                                 .
                              
                           
                        where 
                           λ
                         is the model parameter set.

For Mandarin speech synthesis, the basic units of the HMM-based acoustic model are the initial/final sounds of a Mandarin syllable (Sun, 2006). For simplicity, we will refer to these initials and finals sounds as Mandarin phones. For every acoustic model, the contextual features 
                           W
                         include:
                           
                              •
                              Segmental features:
                                    
                                       –
                                       The identities of the current and surrounding Mandarin phones;

The tones of the current and surrounding Mandarin syllables;

Prosodic phrasing features:
                                    
                                       –
                                       The number of syllables in the current prosodic word (PW), the number of PWs in current prosodic phrase (PP) and the number of PPs in current utterance;

The distance between the current syllable and the head of the current PW, PP and utterance;

The distance between the current syllable and the tail of the current PW, PP and utterance;

The type of the prosodic boundary between the current and adjacent syllables.

In the baseline system, the Gaussian distribution is used to describe the duration of the Mandarin phones accompanied by model context 
                           W
                        , and another set of multi-variate Gaussian distributions is used to model the durations of the HMM states of every phone. Before training the two sets of duration models, the duration of every phone and its associated HMM states can be acquired through force-alignment of the training data. These two sets of context-dependent Gaussian distributions can then be trained. During the synthesis phase, the duration of the HMM states is predicted by maximizing the joint probability of the duration of the phone and its HMM states (Wu and Wang, 2006). Note that the silent pause following a PP boundary is treated as a phonemic unit, and the duration of the silent pause is modelled in the same manner as the other phonemic units.

The baseline Mandarin CTS system adopts the structure shown in Fig. 1(b). Although this structure is widely accepted in the CTS community, we wonder if the external prosodic phrasing model for Mandarin CTS might make full use of the linguistic knowledge from the text generator. What is more, we think the acoustic information offered by the acoustic model could also help formulate the content of the synthetic utterance. Thus, we propose a CTS method in which linguistic and acoustic knowledge can be propagated to downstream components so that linguistic and acoustic knowledge can be exploited by the system. This CTS method is illustrated in Fig. 4
                     . The use of linguistic knowledge in acoustic modelling and acoustic knowledge in sentence content planning via utterance filtering will be discussed in Sections 3.1 and 3.2 respectively.

@&#MOTIVATION@&#

Propagating the linguistic information from the text generator to the acoustic model seems to be an effective way to take advantage of linguistic knowledge (Badino et al., 2012; Yu et al., 2013), however, this solution may not work as expected for Mandarin if the prosodic phrasing features and linguistic features are simply combined as contextual features of the HMM-based acoustic model. First, according to Section 2.2, the results from automatic prosodic phrasing directly influence the naturalness and intelligibility of the synthetic utterance. Specifically, it is the PP boundary, not the newly added linguistic features, that directly decides the location of a perceptible break in the synthetic utterance. The additional linguistic features can only implicitly help the acoustic models discriminate the distribution of acoustic parameters such as the F0 variation in different model contexts. Thus, introducing additional linguistic features may not prevent the degradation caused by erroneous results from automatic prosodic phrasing. For example, in Fig. 3, if ‘
                              
                           ’ (that) is a single prosodic word while ‘
                              
                           ’ (a quantity maker) and ‘
                              
                           ’ (balloon) incorrectly form one prosodic word, listeners may find the synthetic utterance to be obscure however natural the intonation is.

Unfortunately, state-of-the-art automatic prosodic phrasing for Mandarin is subject to erroneous predictions even when then syntactic features are utilized as input information. Recently, Che et al. (2014) compared the performances of automatic prosodic phrasing on a large Mandarin corpus using different combinations of input linguistic features and three statistical classifiers. According to their results, although the syntactic features can improve the performance of automatic prosodic phrasing effectively, the greatest F
                           1 scores of PW and PP boundary prediction are less than 0.83 and 0.73 respectively. Thus, directly utilizing syntactic information may not prevent the automatic prosodic phrasing module from generating unacceptable outcomes.

The above problems may be due to the hard decision in prosodic phrasing. ‘Hard decision’ means the target of prosodic phrasing is one of the pre-defined prosodic boundary types. On one hand, different types of prosodic boundary may be realized by different acoustic features. Thus, a mis-predicted prosodic boundary may lead to an unnatural synthetic utterance. On the other hand, prosodic annotation may be ambiguous and the performance of the prosodic model trained based on this prosodic annotation may be degraded. To alleviate the problem, we think it worthy of replacing prosodic phrasing features with linguistic features completely.

In the proposed Mandarin CTS system, syntactic features offered by KPML are utilized to replace the prosodic phrasing features listed in Section 2.3. Research in psychology has suggested that syntactic structure influences the location of intonational boundaries during speech production (Watson and Gibson, 2004). Thus, it is rational to expect that a correlation exists between syntactic features and acoustic signals. Furthermore, syntactic features are by-products of the text generation process and can be cost-effectively extracted from KPML. The KPML syntactic features stem from SFG (Halliday and Matthiessen, 2004). As mentioned in Section 2.1, a syntactic tree can be generated by KPML for every generated sentence. The syntactic features for the grammatical units in the sentence can then be extracted from this tree. In the proposed Mandarin CTS system, a set of features similar to that defined in Yu et al. (2013) was designed. This set of syntactic features includes phrase functional tags and structural information about the grammatical units. The syntactic features for the unit ‘
                              
                           ’ in Fig. 3 are shown in Table 1
                           .

An overview of this method is shown in Fig. 4. Syntactic features are added to the context 
                              W
                            for HMM-based acoustic modelling. Then, the automatic prosodic phrasing component is removed. During model training, the acoustic models that share some common features in 
                              W
                            can be tied together in the decision-tree-based model clustering stage (Odell, 1995).

This method is different from the conventional approach wherein two decision-trees are constructed separately, i.e. one tree for prosodic phrasing and the other one for acoustic modelling. Although the output of the first tree is one of the compact prosodic boundary types, an incorrectly predicted prosodic boundary may result in an unacceptable synthetic utterance. For example, a silent pause model may be falsely inserted or deleted. Apart from this, given prosodic features, acoustic features are assumed to be independent from syntactic features. Thus, acoustic modelling may not take full advantage of the information in syntactic features.

In the proposed method, the decision-tree is optimized with the likelihood function of acoustic features as criterion and the syntactic features as input contexts. The size of the tree and the acoustic models tied to each leaf node are automatically determined by the model clustering algorithm. Although syntactic features may be ambiguous for directly estimating the distribution parameters of acoustic features, this issue can be alleviated by model clustering which introduces an averaging mechanism. The results of the experiments will demonstrate that this loss is more acceptable than the severe degradation caused by the incorrect prediction from the conventional prosodic phrasing.

The addition of syntactic features to the contextual features of the HMM-based acoustic models is straightforward. However, the original prosodic features can not be eliminated by simply deleting the prosodic descriptions in 
                              W
                           . As mentioned in Section 2.2, the prosodic features for Mandarin describe the timing structure of the utterance. Typically, a PP boundary always precedes a short silent pause. This segment of silent pause is assigned to an individual short pause (SP) model during the training of the acoustic model. If the prosodic features are eliminated, the explicit association between the segments of silent pauses and the SP model is lost. If the SP model is eliminated in conjunction with the prosodic features, the silent segments will be utilized to train the acoustic models of the other non-silent units, which would sabotage the acoustic models of the non-silent units. Our solution is to configure one HMM state in every non-silent acoustic model for the possible silent segment. Because the silent pause does not necessarily follow every phoneme, a left-to-right HMM structure with skip transitions is utilized, as shown in Fig. 5
                           . Despite the revision of the model structure, the original training scheme can still be utilized to train the HMM-based acoustic model (Rabiner, 1989). The silent pause is assumed to be trapped by the preceding acoustic model after initializing the last state of this model with silent data and training the model in the conventional training scheme. For example, the ‘sp’ segment is modeled by the model of phoneme ‘a’ in Fig. 5. The statistical mechanisms to decide whether ‘sp’ is skipped are different in the model training and speech synthesis stages. During the model training state, it is decided by the state transition probability and the likelihood of the distribution in each HMM state given the training data, which follows the standard training algorithm (Rabiner, 1989). In the synthesis stage, the length of a silent segment is decided primarily by the mean of the Gaussian distribution of the corresponding ‘sp’ state's duration.

Another portion of the knowledge sharing in the proposed method tries to influence the content of the output utterance by utilizing the acoustic knowledge offered by the acoustic model. A CTS input concept can be expressed by utterances with different wordings. However, the text generator in traditional CTS systems only yields one ‘best’ sentence following linguistic constraints while ignoring the quality of the corresponding synthesized utterance. Unfortunately, CTS systems with corpus-based speech synthesis modules cannot ensure properly synthesized units due to data sparsity. Regardless of how meticulous the content is, it only takes one poorly synthesized syllable to ruin the entire utterance. Thus a better strategy for CTS systems is to decide which words should be synthesized considering its ability to synthesizing them well. This strategy is similar to the strategy adopted by human speakers. For example, English learners will say ‘suitable’ rather than ‘appropriate’ if they have difficulty pronouncing the latter word.

However, acoustic knowledge about the utterance cannot be easily retrieved until one sentence is processed by the acoustic model. This is known as the ‘surface problem’ in the NLG community (Inui et al., 1992). To address the problem, NLG researchers have proposed a revision cycle wherein the generated sentence is analyzed and then revised until no further revision is required. In the proposed CTS system, the ‘surface problem’ is addressed by three steps: first, all the valid utterances expressing the same input concept are generated and synthesized; next, the candidates with inferior acoustic quality are identified and removed; finally, the output is randomly selected from the remaining candidates. We refer to these three steps as utterance filtering. The revision-based method was not adopted because utterance filtering requires the acoustic data of all the synthetic candidates.

The second step of utterance filtering is similar to the task of automatic evaluation of synthetic utterances wherein the synthetic utterances to be evaluated contain equivalent strings of words that are realized by different speech waveforms. However, in CTS, the candidate utterances for one input concept bear different word strings. As a result, the candidates cannot be directly compared with and ranked against each other. Furthermore, unlike the utterance evaluation for the HMM-based parametric approach in Do et al. (2014) and the unit-selection approach in Song et al. (2013), a reference waveform is not available for measuring the quality of the synthesized speech. Thus, in contrast with the methods in Song et al. (2013) and Do et al. (2014), we propose a two-step method for evaluating synthetic utterances: evaluate the syllables in each utterance and then evaluate all the utterances based on the syllable evaluation results.

The preliminary step of utterance filtering involves generating multiple text candidates per input concept. To fulfill this task, the default choice mechanism discussed in Section 2.1 is modified in our system. For example, if the input does not specify whether the sentence should be a tagged or untagged interrogative, both cases are generated as candidates. The direct implementation of this idea is to transform the original SPL input into two sets of specifications with explicit tagged/untagged choices. For this purpose, several grammatical rules were manually crafted to transform the SPL input. These grammatical rules cover three aspects similar to the definition of meta-functions in SFG (Halliday and Matthiessen, 2004): Interpersonal, Experiential and Logical. Interpersonal rules change the mood of the clause; Experiential rules alter the semantic role played by each item; Logical rules replace the logical structure in the original clause. Examples based on these rules are given in Table 2
                           .

Varying the text candidates is also possible at the lexical level. This can be achieved by incorporating a thesaurus and deriving multiple candidates with similar semantic meaning but different words. Currently, a simple thesaurus with 100 entries is included in our system.

The generation of multiple text candidates is indicated by the Varying the input in Fig. 4. The text candidates derived from this method have similar meanings and are expected to achieve the same communication goal. For example, the first two interrogatives in Table 2 can initiate a conversation for the user to query his/her bank account information.

The second step of utterance filtering involves rating each unit of a synthetic utterance based on its acoustic information. The syllable is chosen as the basic unit because it has a regular structure and conveys the tones of Mandarin (Sun, 2006). In our system, syllable evaluation is a binary (‘acceptable’ or ‘unacceptable’) classification task and is implemented by the k-Nearest Neighbors (k-NN) algorithm. Specifically, a score P(
                              s
                           )=
                           N
                           
                              aNN
                           /T
                           
                              NN
                            is calculated for each synthetic syllable 
                              s
                           , where T
                           
                              NN
                            is the number of nearest neighbors in the training set and will be tuned in the experiment, and N
                           
                              aNN
                            is the number of acceptable instances among the T
                           
                              NN
                            near neighbors. The score P(
                              s
                           ) denotes the probability that syllable 
                              s
                            is acceptable.

The vital issue in k-NN-based classification is to retrieve the T
                           
                              NN
                            near neighbors. In our system, the basic assumption is that the syllable neighbors share similar acoustic features with the target syllable. Then, two groups of acoustic features are designed and they constitute the acoustic knowledge that will be delivered from the acoustic model to the syllable evaluator:
                              
                                 •
                                 
                                    Syllable-level F0 features. These features describe the shape of the F0 at the syllable level (Wang et al., 2008; Yin et al., 2014). Given a training set of natural recordings, the length-normalized F0 trajectories of all the syllables are clustered context-dependently based on the decision tree and the Maximum-Likelihood decision criteria (Odell, 1995). This training method has been used on syllable F0 features for unit-selection speech synthesis, although the syllable F0 features are different (Ling et al., 2010). At the evaluation stage, each synthetic syllable is first assigned to one of the clusters by the decision tree. Next, the score S
                                    
                                       Ft
                                    
                                    =1−
                                    Rank
                                    
                                       Ft
                                    /N
                                    
                                       inst
                                     is calculated to be the syllable-level F0 features, where N
                                    
                                       inst
                                     is the number of training samples belonging to this cluster and Rank
                                    
                                       Ft
                                     is the rank of the synthetic syllable in that cluster in terms of the distance between the F0 trajectory of the synthetic syllable and the cluster center. A similar procedure is used to calculate S
                                    
                                       Fd
                                     for the trajectory of the F0 derivatives.


                                    Transition features. For each synthetic syllable, the F0 and spectral trajectories around the boundary with the previous syllable and between the phones in the current syllable are extracted. The features S
                                    
                                       Cf
                                     and S
                                    
                                       Cs
                                     can then be obtained by a similar method as the syllable-level F0 features. In contrast with unit-selection speech synthesis, syllable evaluation only utilizes the transition features to identify the neighboring similar syllables. The acceptability of the target syllable depends on the acceptability of similar syllables. We assume that the similar syllables have similar acoustic parameters, both with respect to the spectrum and F0. The acoustic parameters of the synthetic syllables are influenced not only by the current acoustic model but also by the surrounding models (Tokuda et al., 1995). The influence of the surrounding models may not be completely captured by the context of the current acoustic model. Thus, we utilize transition features to discriminate the synthetic syllables that are generated from the same acoustic model but influenced by different surrounding acoustic models.

The similarity between the acoustic features of two syllables is calculated as the Euclidean distance between the two acoustic feature vectors. However, rather than searching the neighbors among the entire syllable corpus, we utilized the contextual features of each syllable and designed three search spaces to narrow down the search areas:
                              
                                 •
                                 
                                    
                                       S1
                                    : all the samples with the same syllable symbol as the target 
                                       s
                                    ;


                                    
                                       S2
                                    : the sample syllables using the identical acoustic model clusters as the target 
                                       s
                                     for deriving the Syllable-level F0 and Transition features;


                                    
                                       S3
                                    : all the samples with identical contextual features as the target 
                                       s
                                    .

Because the syllable samples in S3 are generated by the same acoustic model as the target syllable, they resembles the target syllable most. However, the number of samples in S3 may be less than T
                           
                              NN
                           . If so, the secondary search space S2 is used. If the sample syllables remain inadequate, the tertiary search space S1 is used. In the utilized search space, the T
                           
                              NN
                            neighbors are retrieved based on the Euclidean distance between the acoustic features of 
                              s
                            and the sample syllables in the search space. If a synthetic syllable is unable to find T
                           
                              NN
                            neighbors in all the search spaces, it is labeled an Unclassified Syllable to indicate that the calculated score of P(
                              s
                           ) may be unreliable. A synthetic syllable is labeled a Perfect Syllable if its P(
                              s
                           )=1.

To collect the syllable corpus for our k-NN evaluator, native Mandarin speakers were asked to evaluate a set of synthetic utterances and assign binary tags to each syllable in the utterances based on whether it is was an acceptable syllable in that utterance. Additional details regarding the syllable corpus are given in Section 4.4.

All the synthetic utterances 
                              u
                           
                           
                              i
                            from the same input concept form a set 
                              G
                           
                           ={
                              u
                           
                           1, …, 
                              u
                           
                           
                              m
                           }. Because each 
                              u
                           
                           
                              i
                            in 
                              G
                            contains words that differ from the other candidates, finding the best 
                              u
                           
                           
                              i
                            may be impossible. Thus, we proposes a heuristic algorithm to remove the unacceptable candidates. After calculating P(
                              s
                           ) for all the syllables in 
                              u
                           
                           
                              i
                           , the following features can be derived to describe the quality of 
                              u
                           
                           
                              i
                           :
                              
                                 •
                                 
                                    
                                       
                                          L
                                          Lsco
                                          i
                                       
                                    : the lowest syllable score P(
                                       s
                                    ) in 
                                       u
                                    
                                    
                                       i
                                    ;


                                    
                                       
                                          L
                                          Aver
                                          i
                                       
                                    : the average syllable score P(
                                       s
                                    ) in 
                                       u
                                    
                                    
                                       i
                                    ;


                                    
                                       
                                          L
                                          Rper
                                          i
                                       
                                    : the ratio of Perfect Syllables in 
                                       u
                                    
                                    
                                       i
                                    .


                                    
                                       
                                          L
                                          Runc
                                          i
                                       
                                    : the ratio of Unclassified Syllables in 
                                       u
                                    
                                    
                                       i
                                    .


                                    G
                                    
                                       Lsco
                                    : the lowest L
                                    
                                       Lsco
                                     of all the 
                                       u
                                    
                                    
                                       i
                                     in 
                                       G
                                    ;


                                    G
                                    
                                       Have
                                    : the highest L
                                    
                                       Aver
                                     of all the 
                                       u
                                    
                                    
                                       i
                                     in 
                                       G
                                    .

After conducting a series of informal subjective evaluation tests, we found that 
                              
                                 L
                                 Lsco
                                 i
                              
                            is the most informative quantity in this task, followed by 
                              
                                 L
                                 Aver
                                 i
                              
                           , and so on. Based on these findings, Algorithm 1 is used to retrieve the utterance(s) with acceptable synthetic quality from the set 
                              G
                           . The pre-defined value T
                           
                              A
                            describes the threshold of how distant an acceptable utterance's average score 
                              
                                 L
                                 Aver
                                 i
                              
                            can be from the G
                           
                              Have
                            of the current set 
                              G
                           . T
                           
                              A
                            is heuristically set to 0.05 in our implementation.

In the last steps of Algorithm 1, although 
                              
                                 L
                                 Rper
                                 i
                              
                            and 
                              
                                 L
                                 Runc
                                 i
                              
                            are utilized to filter out the unacceptable candidates as many as possible, 
                              G
                           
                           
                              acc
                            may contain more than one utterance. In this case, the final output of the CTS system is randomly chosen from the utterances in 
                              G
                           
                           
                              acc
                           .


                           
                              Algorithm 1
                              Retrieve acceptable utterance set 
                                    G
                                 
                                 
                                    
                                       acc
                                    
                                 
                              


                                 
                                    
                                       
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              

@&#EXPERIMENTS AND RESULTS@&#

This section details the experiments for the proposed methods. Section 4.2 examines the effectiveness of the syntactic features in a constrained environment for acoustic model clustering. Section 4.3 compares the proposed method for acoustic modelling with the conventional method in a realistic environment wherein the hand-annotated prosodic features are unavailable. Note that, in order to acquire the objective evaluation results, the synthetic samples are predicted given the natural duration of the test data. For subjective evaluation, the duration information of each sample is predicted by each system. Section 4.4 will detail the experiments for utterance filtering, including Section 4.4.2 for syllable evaluation and Section 4.4.3 for utterance evaluation.

A training corpus with consistent matching between the acoustic data and linguistic features is a precondition for building a CTS system. Thus, we followed the four steps in Pan (2002) to prepare our CTS corpus. First, an SPL set for 378 sentences was manually crafted after analyzing a telephone dialogue corpus for bank customer service. Then, given the SPL input set, the KPML-based NLG generated the texts of the 378 sentences. In the third stage, a female speaker recorded all the 378 utterances in a simulated conversational environment. Finally, the prosodic features were manually annotated for all the recorded utterances. Although the length of the corpus is 35min, it is sufficient for training a Mandarin HMM-based parametric speech synthesizer in a limited domain.

For the acoustic modeling of speech synthesis, the spectral and F0 parameters of the training corpus were extracted by STRAIGHT (Kawahara et al., 1999). Line spectral pairs (LSPs) in 40 dimensions and the power of the current frame formed the static spectral parameters. Finally, the static parameters along with their first-order and second-order dynamic components constituted the complete acoustic features. Due to the small size of the corpus, a cross-validation approach was adopted for the objective evaluation of the experiments. Thus, the results in this paper are more conclusive than those in Wang et al. (2014). The 340 sentences in the training set were split into 9 subsets. Each split contained nearly 300 sentences for training and 40 sentences for validation. The objective measures from each split were averaged to yield the overall objective measurement. For the subjective evaluation, a fixed test set of 38 sentences was randomly selected from the CTS corpus and the other sentences were used as a training set.

A pilot experiment was conducted to test the effectiveness of the syntactic features in model clustering. As shown in Table 3
                        , 4 systems were constructed. All the systems used the syntactic features in Table 1 and contextual features in Section 2.3 as the model context both for the training and test data. The syntactic features were automatically generated from the NLG component and the prosodic features were hand-annotated. Thus, the system Pr denoted the baseline system in an ideal environment. Because the systems Sy and nSyPr utilized prosodic features in the model context, the alignment between the silent pause and the acoustic model for silence was explicitly given. Thus, Sy and nSyPr avoided the problem mentioned in Section 3.1.3 and implicitly used the prosodic features. This experiment only inspected the usefulness of the syntactic features in model clustering by altering the question set of each system while fixing other factors.

The model training recipe, including the MDL factor for the decision-tree-based model clustering (Shinoda and Watanabe, 1997), was identical for all the systems. The aforementioned cross-validation approach was adopted to objectively evaluate the performance of the 4 systems. In total, 4×9=36 systems were trained, thus, 4 sets of average objective results were collected.

As Fig. 6
                        (a) shows, the 4 systems all achieved a similar performance on spectral prediction. This finding is reasonable because syntactic and prosodic features have a weak influence on the segmental aspect of speech signals. Regarding F0 prediction, Sy yields a lower RMSE than nSyPr after utilizing syntactic features, as shown in Fig. 6(b). The number of clustered acoustic models for the F0 stream of Sy is 1102 while that of nSyPr is 961. Because the MDL factor was identical across all the systems, the increased number of model clusters indicates that integrating syntactic features can discriminate more patterns of F0 features when building the decision trees for model clustering. However, the comparison between SyPr and Pr shows that their RMSEs on F0 prediction differ by less than 0.2Hz. This result means that the effectiveness of the syntactic features is diminished when prosodic features are present. A potential explanation for this finding is the correlation between the syntactic and prosodic features. If such a correlation exists, further improvements in performance due to syntactic features are predicted to be minor when prosodic features are utilized.

Finally, several subjective evaluations were conducted. All 340 sentences were utilized to train the acoustic model for each system for the subjective evaluations. For each experimental group, 20 randomly selected synthetic sentences from the test set were evaluated by 10 native speakers. The results are shown in Table 4
                        . The first two rows in Table 4 show the performance of the systems wherein either prosodic or syntactic features were added to nSyPr. Both the syntactic and prosodic features can improve synthesized speech, although the improvement due to syntactic features is not significant (p
                        >0.05). However, the difference between Pr and Sy is insignificant, which means that syntactic features can partly compensate for the loss incurred during acoustic modelling with missing prosodic features. Based on the comparison between Pr and SyPr, we can see that syntactic features cannot introduce further improvements of performance when the prosodic features are ‘switched on’.

Overall, based on hand-annotated prosodic features and explicit alignment of the silent pause, both the objective and subjective tests suggest that the syntactic features are beneficial to model clustering. However, their effectiveness is repressed by manually annotated prosodic phrasing features.

The previous experiments were conducted in a constrained environment. Particularly, we argued in Section 3 that the performance of a baseline system will degrade in a realistic environment wherein the automatic prosodic phrasing module may deliver inaccurate output to the speech synthesizer, or the prosodic features in the training data may contain noise if they are not manually annotated. Furthermore, because systems in the previous experiment used both the syntactic and prosodic features in the model context, the system Sy was not equivalent to the proposed system. Experiment in this section tests the performance of a baseline system in simulated realistic environments. Then, the proposed CTS system without using prosodic phrasing features is tested and compared with the baseline system.

To simulate the realistic environments, manually annotated prosodic features in the training or test data should be replaced by those provided by an automatic prosodic phrasing module. We utilized two automatic prosodic phrasing modules, PM
                           1 and PM
                           2. Module PM
                           2 was a maximum-entropy-based classifier provided by iFlyTek (Li et al., 2005). It was trained on a news corpus consisting of greater than ten thousand sentences with manually annotated prosodic phrasing structure. The feature templates for PM
                           2 covered the surface words in the neighborhood, the POS and the syllable numbers of the words; however, syntactic features were not utilized. The performance of PM
                           2 on predicting the PP boundaries on the news corpus was 0.697 (Li et al., 2005). The performance of PM
                           2 on the CTS corpus is shown in Table 5
                           . Evidently, the performance is much lower for the CTS corpus versus the news corpus. Besides PM
                           2, a decision-tree-based classifier, PM
                           1, was trained using the entire CTS corpus. All the syntactic features were utilized as the input to PM
                           1. Because PM
                           1 was trained and tested on the entire corpus, the performance of PM
                           1 was much higher than PM
                           2 on the CTS corpus. We assume that PM
                           1 simulates the most amenable realistic environment wherein the baseline CTS system can be applied. The baseline CTS system's performances in the simulated realistic environments based on PM
                           1 and PM
                           2 may approximate its performance in a ‘real’ realistic environment.

Given PM
                           1 and PM
                           2, the baseline CTS system was tested in 4 simulated realistic environments, as shown in Table 6
                           . Pr
                           01 and Pr
                           02 indicate the performance of the baseline CTS system in realistic environments where a test sentence can not acquire manual prosodic annotation. Pr
                           11 and Pr
                           22 represent a baseline system wherein manual prosodic annotation of the training set is also missing. The objective results are shown in Fig. 7
                           .

Compared with the baseline system Pr, which is under ideal and unrealistic conditions, Pr
                           01 to Pr
                           22 are inferior in both spectral and F0 parameter prediction. Furthermore, as the trend from Pr
                           01 to Pr
                           02 or Pr
                           11 to Pr
                           22 show, the performance of the baseline CTS system decays with the declining performance of automatic prosodic phrasing. This trend is reasonable because the noise in the prosodic phrasing features affects the model context of the surrounding acoustic models and the locations of the breaks in the utterances. The comparisons between Pr
                           01-Pr
                           11 and between Pr
                           02-Pr
                           22 further indicate that noise in the prosodic features of the training set degrades the acoustic model. Essentially, the performance of the baseline CTS system in a realistic environment deteriorates compared with an ideal environment due to noise in the prosodic phrasing features in the test and, possibly, the training data.

The pilot experiment indicates that syntactic features can be beneficial if they are added into the context of an acoustic model. It would be interesting to know whether the degradation of the performance of the baseline CTS system in a realistic environment could be mitigated by using syntactic features. Thus, we constructed the 4 groups SyPr
                           **, which are shown in Table 6. Compared with Pr
                           **, SyPr
                           ** use syntactic features generated by KPML in the model context. The performance of these 4 systems is shown in Fig. 7(b). Comparing SyPr
                           ** with their counterparts in Pr
                           ** shows that ‘error-free’ syntactic features in the acoustic model context can reduce the degradation of performance caused by noise in the prosodic phrasing features. However, a certain loss caused by noise in the automatically generated prosodic phrasing features is still evident.

Based on the method described in Section 3.1, the CTS system SYN was constructed without any prosodic phrasing features. The performance of SYN is shown in Fig. 7. Note that SYN differs from Sy in Table 3 because Sy utilizes prosodic features to locate silent breaks. The comparison between SYN and Pr in Fig. 6(a) indicates that the performance of the proposed CTS method is inferior to the baseline system under ideal conditions. However, the comparisons between SYN and Pr
                           ** in Fig. 7 show that SYN achieves a comparable performance to the baseline systems in a realistic environment.

Based on the results of the objective evaluation, readers may argue that combing syntactic and prosodic features is the best choice because SyPr
                           ** in Fig. 7 achieve better performance than SYN. However, the objective results represent only part of the whole picture. For the objective evaluation, the test synthetic utterances were force-aligned to natural recordings to calculate the RMSEs. Thus, the rhythmic structure of the test synthetic utterance is actually copied from the natural recording. To further test all the systems, a subjective evaluation was conducted in which the duration of each linguistic unit was predicted based on the duration model in each system. Several groups of subjective preference tests were conducted, and the training set with 340 utterances in the pilot study was utilized to train the acoustic model. Among the 38 test sentences, 20 were randomly chosen and synthesized by each system, and 10 native Mandarin speakers participated in subjective tests on the 20 synthetic utterances.

The number of syllables in the 20 utterances ranges from 9 to 30. On average, the natural waveform of each utterance contains 16.8 syllables, 7.10 manually annotated PW boundaries and 1.70 PP boundaries. When PM
                           1 was used on the 20 utterances, 7 utterances acquired 1 PP boundary error (9 in total) and 5 utterances contained 1 PW boundary error. For PM2, 13 utterances acquired at least 1 PP boundary error (14 in total) and 8 utterances contained at least 1 PW boundary error (10 in total). Note that the ‘test set’ in Table 5 denotes the 38 test sentences that contain the 20 sentences used in the subjective evaluation.

The results of the subjective evaluation are shown in Table 7
                           . As the first row of Table 7 shows, SYN is inferior to Pr which is the baseline system under ideal conditions; however, the comparison between SYN and the Pr
                           ** shows a trend of degradation for the baseline system. According to the native speakers’ feedback, some of the synthetic utterances from Pr
                           ** were unacceptable due to their unnatural rhythmic structure. Furthermore, the synthetic utterances from Pr
                           11 and Pr
                           22 occasionally contained unclear pronunciation, which may have been due to noise of the prosodic phrasing features in the training set and the mismatch between acoustic model and data caused by this noise. The comparison between SYN and SyPr
                           ** shows this same trend. Overall, the results of the SYN and SyPr
                           ** comparisons are similar to those of the SYN and Pr
                           ** comparisons.

The above results show that the performance of the baseline system cannot be salvaged by simply adding syntactic features to the acoustic model context. On one hand, as discussed in Section 3, an unnatural rhythmic structure is principally caused by erroneous prosodic phrasing features in the test data. Furthermore, the effectiveness of the syntactic features in modelling the pitch variation is somewhat reduced by the prosodic phrasing features, as indicated by the results of the pilot test. Although SYN may not attain the pitch variation as accurately as the baseline CTS system with prosodic phrasing features, SYN avoids the severe degradation caused by incorrectly predicted prosodic boundary types. Furthermore, SYN does not require prosodic annotation of the training data and is therefore a competitive method for building Mandarin CTS systems.

In the method proposed in Section 3.2, the k-NN-based syllable quality evaluator requires a training corpus of synthetic syllables with manually annotated tags of ‘acceptable’ or ‘unacceptable’. In our experiments, 1130 sentences were derived from the 38 SPL structures of the test set following the sentence variation methods introduced in Section 3.2.1. Then, all the sentences were synthesized by SYN, which lead to 17,379 synthetic syllables in total. The acoustic features of every syllable were extracted using the methods described in Section 3.2.2.

Five native speakers were invited to annotate the quality of each synthetic syllable. During annotation, 38 of the 1130 synthetic utterances were randomly selected as a 511-syllable validation set. The remainder of the synthetic utterances was randomly split into 10 groups. Then, the utterances in the validation set were added to each of the 10 groups. As a result, each group contained approximately 150 utterances. After listening to a synthetic utterance, the annotators assigned a binary score to every syllable in the synthetic utterance. Each annotator was required to annotate 2 groups with a several day break between annotations. Each annotator annotated the same validation set twice. Thus, the consistency of the syllable annotation across the annotators and time can be measured. Using the annotation results on the validation set from one of the 10 groups as a reference, the consistency of the remaining 9 sets can be measured by calculating F
                           1 scores. As shown in Table 8
                           , the average F
                           1 score for the acceptable syllable is greater than 0.95 because the number of acceptable syllables is much higher than the number of unacceptable ones. For the unacceptable syllable, the test on the two validation sets annotated by the same annotator (intra-annotator consistency) yields F
                           1
                           =0.669 after averaging the scores of the 5 annotators. However, the average F
                           1 across different annotators (inter-annotator consistency) is only 0.388. In other words, annotators did not share the same criteria for judging unacceptable syllables. However, this low consistency is acceptable for building the syllable quality evaluator because the evaluator is expected to cover as many of the acoustic patterns of the unacceptable syllables as possible. All the syllables, including the syllables from the 38 validation utterances, formed the corpus of synthetic syllables, resulting in 14,135 acceptable and 3244 unacceptable syllables. Because a syllable in the validation set may achieve different annotations by different annotators, a worst-win strategy was utilized before adding such syllables to the syllable corpus: if any of the annotators assigned ‘unacceptable’ to the syllable, that syllable was considered to be ‘unacceptable’.

Given a corpus of synthetic syllables with manual annotations, a k-NN based evaluator for rating the quality of each synthetic syllable can be constructed. This experiment examines the performance of the evaluator under different configurations of input features. Among the 17,379 syllables in the corpus, 12,167 samples were randomly chosen as the training set. The remaining samples were split into validation and test sets with 2606 and 2606 samples, respectively. The validation set was utilized to tune the value of T
                           
                              NN
                           . However, the only constraint was T
                           
                              NN
                           
                           ∈[5, 15]. To measure the accuracy and recall over the results of the syllable evaluation, the continuous p(s) was binarized based on a threshold of 0.5. The proportion of unclassified syllables was less than 1%, even when K
                           =15; thus, these unclassified syllables were ignored during the evaluation.

In total, 12 automatic syllable evaluators were constructed and the F1 scores on the test set vs. the human annotation are shown in Table 9
                           . In the SE
                           1 to SE
                           3 group, SE
                           3, which has the most delicate search space, yields the best performance. A similar result was obtained for SE
                           6 in the SE
                           4 to SE
                           6 group. This result directly testifies to the benefit of the defined search space. On the other hand, the performance of the systems in the SE
                           4 to SE
                           6 group is significantly lower than that of their counterparts in the SE
                           1 to SE
                           3 group. This result is explained by the fact that the systems in SE
                           4 to SE
                           6 randomly selected the nearest instances without using the acoustic features in the search space. Thus, acoustic features are necessary for syllable evaluation. However, the comparisons between SE
                           3 and any system from SE
                           7 to SE
                           12 indicate that the performance of the syllable evaluator may not drop severely if a portion of the acoustic features is removed. One reason may be that the combination of these acoustic features contains redundant information. However, the best strategy uses all the acoustic features.

To evaluate the performance of the utterance filtering, SPL structures for 19 new sets of input concepts were crafted after being randomly selected from the SPL in the CTS corpus and manually revised to represent clauses in the same domain with novel concepts. Next, 963 sentences were derived from the 19 SPL structures following the method introduced in Section 3.2.1 and synthesized using the SYN, which resulted in 19 groups of synthetic utterances. The syllables in all the synthetic utterances were automatically evaluated using the best syllable evaluator SE
                           3 in Section 4.4.2. Next, the utterances were evaluated using Algorithm 1, which was introduced in Section 3.2.1. After the utterance evaluation, only 35 of the 963 utterances were judged to be acceptable. For the 19 utterance groups corresponding to the 19 input SPL structures, 1 group contained 3 acceptable utterances and 14 groups contained 2 acceptable utterances, while the remaining 4 groups contained only 1 acceptable utterance each.

We compared the naturalness of the utterances that were automatically judged to be acceptable and unacceptable with a subjective evaluation test to test the effectiveness of the proposed method. Due to the small number of acceptable utterances, all the 35 acceptable utterances were involved in the subjective evaluation. Moreover, 45 utterances were randomly selected from the unacceptable utterances of the 19 groups. The naturalness of these 80 utterances was evaluated by 10 listeners. The mean opinion scores (MOSs) of the utterances judged to be acceptable and those judged to be unacceptable for each input concept were calculated and are shown in Fig. 8
                           (b). For the sake of comparison, these 80 utterances were also manually labeled as acceptable or unacceptable. The MOSs of these two categories based on the manual classification are shown in Fig. 8(a).

As shown in Fig. 8(a), the utterances that were judged to be acceptable by human annotators significantly outperformed the ones that were judged to be unacceptable, with an average MOS difference of approximately 0.343. These results illustrate the variation in the naturalness of synthetic utterances that express the same input concept. From Fig. 8(b), we can see that the automatic evaluation on the quality of the synthetic utterances can also help us find the candidate utterance with higher naturalness. The acceptable utterances for 14 of the 19 input concepts achieved higher MOSs than the unacceptable ones and the average MOS difference is approximately 0.304. Although the gap between the MOSs of the acceptable and unacceptable utterances in Fig. 8(b) is not as large as the one shown in Fig. 8(a), this result shows that the automatic evaluation method based on utterance filtering can infer the acceptability of the synthetic utterance.

To measure the final performance of the proposed CTS system, one synthetic utterance was randomly selected from the acceptable cluster for each input concept shown in Fig. 8(b). These utterances represented the final output to express the input concepts and their average MOS is indicated by the UF
                           3 (Utterance filtering with the syllable evaluator SE
                           3) in Table 10
                           . To examine the performance of utterance filtering without using acoustic knowledge in the syllable evaluator, the results based on SE
                           6 were calculated and are indicated by UF
                           6. To inspect the performance of utterance filtering that uses the least delicate search space for syllable evaluation, the results based on SE
                           1 were calculated and are shown by UF
                           1. As a reference, the average MOS of the 19 randomly selected acceptable utterances based on the manual evaluation in Fig. 8(a) was calculated and denoted by MS (manual selection). RS denotes the result when one utterance was randomly selected for each group whether it was acceptable or not. RS represents the performance of the CTS system without utterance filtering. The MOSs are shown in Table 10. The results of the Wilcoxon signed rank test of the different systems are shown in Table 11
                           .

As observed in Tables 10 and 11, the average MOS of RS is significantly lower than UF
                           1, UF
                           6 and UF
                           3. This indicates that the CTS system with utterance filtering can generate synthetic utterances of higher acoustic quality than conventional methods. However, the system with utterance filtering is significantly lower than MS. This result is reasonable because MS represents the top performance that utterance filtering can achieve. Another noteworthy comparison is between UF
                           1, UF
                           6 and UF
                           3. Although the results of the syllable evaluators in Table 9 show that SE
                           3 achieves better performance than SE
                           6 and SE
                           1, the performance of UF
                           3 based on SF
                           3 is not significantly different from that of UF
                           6 and UF
                           1. In fact, the differences between the groups in Table 10 is less than 0.5. Because the CTS system with utterance filtering only selects candidates without further improvements to the acoustic quality of the synthetic utterances, this small difference makes sense. Despite the small difference, we suggest that sharing acoustic knowledge to select a well synthesized utterance rather than directly synthesizing the single ‘best’ generated text can be a beneficial strategy.

@&#CONCLUSION@&#

This paper has proposed two types of knowledge sharing to improve the performance of CTS systems. To avoid the severe degradation caused by incorrectly predicted prosodic boundary types in conventional CTS systems, the first proposed method utilizes the syntactic features produced by the NLG module to replace the prosodic features for the acoustic modelling of speech generation. The experimental results showed that this method achieved a performance comparable to the conventional approach while avoiding the cost of constructing the automatic prosodic phrasing module. Using only syntactic information may constrain the performance of the proposed method. If other linguistic factors are also utilized, the proposed method may be further improved. Both the motivation for and the experiments on the proposed method concern Mandarin CTS systems. Whether the proposed solution can achieve similar performance in other languages is worthy of further investigation.

To achieve the second type of knowledge sharing, the acoustic cues of synthetic speech are utilized to evaluate multiple synthetic utterances that express the same input concept. We call this approach as utterance filtering. The experimental results show that this approach can help a CTS system discard synthetic utterances with inferior acoustic quality that express the input concept. It is noteworthy that the performance of this method depends on the grammatical and lexical variability of the multiple utterance candidates. Currently, we use simple rules to generate the candidate sentences. If more comprehensive rules can be derived, the diversity of the sentence candidates could be further improved and the increased performance due to the utterance filtering would be more significant.

The above two approaches optimize CTS systems by integrating the NLG and the speech synthesizer modules. On the other hand, the performance of CTS systems can also be improved by optimizing the modules separately, especially considering recent progresses on improving speech synthesizer using deep learning techniques (Ling et al., 2015). Thus, a comparison between such separate optimization and our proposed methods may further yield informative results. This comparison may be our future work.

@&#ACKNOWLEDGEMENT@&#

This work was partially funded by the National Nature Science Foundation of China (grant no. 61273032).

@&#REFERENCES@&#

