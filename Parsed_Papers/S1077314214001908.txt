@&#MAIN-TITLE@&#Connected image processing with multivariate attributes: An unsupervised Markovian classification approach

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We perform an unsupervised classification of the nodes of the Max-Tree.


                        
                        
                           
                           The Max-Tree is considered as a hidden Markov tree.


                        
                        
                           
                           Multivariate probability density functions enables to model multivariate attributes.


                        
                        
                           
                           Model parameters are estimated from the sole observations.


                        
                        
                           
                           We perform experiments on astronomical images and retinal images segmentation.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Image filtering

Image segmentation

Connected filter

Max-Tree

Connected component tree

Hierarchical modeling

Markov model

Mathematical morphology

@&#ABSTRACT@&#


               
               
                  This article presents a new approach for constructing connected operators for image processing and analysis. It relies on a hierarchical Markovian unsupervised algorithm in order to classify the nodes of the traditional Max-Tree. This approach enables to naturally handle multivariate attributes in a robust non-local way. The technique is demonstrated on several image analysis tasks: filtering, segmentation, and source detection, on astronomical and biomedical images. The obtained results show that the method is competitive despite its general formulation. This article provides also a new insight in the field of hierarchical Markovian image processing showing that morphological trees can advantageously replace traditional quadtrees.
               
            

@&#INTRODUCTION@&#

In image processing, connected operators are morphological operators that concentrate on the non deformation of edges. In binary images, as their name suggests, their very principle is to work on the connected components of an image, and the only allowed operation is the removal of such components. Connected operators take their roots in the notion of filters by reconstruction [1,2]. They have been studied in the context of binary image processing in [3,4] and their extension to gray-scale images appeared in [5–8]. In this case, we say that an operator is connected if it is connected at all thresholding levels of the image.

Gray-scale connected operators started to become popular when a powerful framework and an efficient algorithm were proposed in order to compute connected operators by Salembier et al. [9]. This framework relies on a hierarchical representation of the image called the Max-Tree (also commonly known as the (connected) component tree). This representation is a tree where each node corresponds to a connected component of a threshold of the image and the parent relation is given by the inclusion relation among the connected components. This representation can then be used in the following 4-steps process (see Fig. 1
                     ) to define connected operators: (1) compute the Max-Tree of the image, (2) equip each node of the tree with some relevant attributes (area, compactness, moments, entropy, …), (3) select nodes in the tree according to their attribute values, and finally (4) reconstruct an image from the filtered tree. The last reconstruction step consists in assigning a new value to each pixel of the image using the content of the nodes selected during the previous step. Connected operators have since been applied to various types of images: document images [10–12], biomedical imaging [13–16], remote sensing [17,18], or astronomical imaging [19,20]. Although the primary aim of connected operators was to perform image filtering [9], they have now been implied in various image analysis tasks such as segmentation [8,21,22], retrieval [23], classification [24] or registration [25].

It is also noteworthy that several connected operators (morphological reconstruction, flooding, region growing, …) can be formulated in the framework of the Image Foresting Transform (IFT) [26]. The IFT relies on an efficient and versatile formulation based on the classical shortest path problem and provides a unified framework for a wide variety of image operators. On the other hand it has also been observed that connected operators share deep links with the TV-L1 optimization scheme [27].

@&#RELATED WORKS@&#

Researches have been carried out in order to improve the capacity or the efficiency at each step of the method.

About the construction of the representation, a first category of works concentrates on the definition of the connected components: connections [28], second-generation connections [29,30,14,31], hyperconnections [12], or directed connections [32]. The relations among various definitions of connections have been studied in [33].

Other works proposed to replace the Max-Tree by other representations, generally designated by morphological trees: binary partition trees [34], hierarchies induced by constrained connectivities [35], hierarchies of minimum spanning forests [36], the tree of shapes [37,38], the connectivity tree [39], the alpha-tree [40], … One can note that the relations among some of these representations have been studied in [41,42].

The use and definition of relevant node attributes have also been studied in various works. Among the proposed attributes, we find: area [43,44], dynamic [45], simplicity and entropy [9], various geometrical moments and tensors [13,21], multispectral volume and color [20], contrast [10], Mumford-Shah energy [46] and so on.

While most connected operators designed so far rely on a simple thresholding on the attribute values – i.e., a node is selected if its attribute value is larger than a specified threshold – it has immediately been noted that such a purely local strategy tends to be sensitive to noise whenever the attributes is not increasing: i.e., when the attribute value does not vary monotonically along a branch of the tree. In the seminal work of Salembier et al. [9] the authors proposed several regularization strategies that all rely on the idea that the selection process should realize a pruning of the tree. The three proposed strategies were the followings:
                           
                              •
                              max: a branch is pruned at its highest selected node;

min: a branch is pruned at its lowest selected node;

Viterbi: an energy criterion is optimized in order to determine the best pruning level.

Recently, Xu et al. proposed in [16,47] to apply another connected filter on the tree representation seen as an image whose adjacency relation is given by the parent–children relation in the tree, and whose pixel values are the attribute values. This approach provides a non local strategy to detect relevant changes in the attribute values.

Another difficulty of the node selection process is the handling of multiple or multivariate attributes. In [48] Urbach et al. proposed two strategies to handle multivariate attributes: the direct extension of the thresholding approach where a node is selected if each of its attributes is greater than a particular threshold (equivalent to a marginal ordering of the attribute space), or using a distance to a reference attribute vector where a node is selected if the distance from its attribute vector to a reference vector is lower than a given threshold (equivalent to a reduced ordering of the attribute space). In both cases, there is a need to define a vector ordering which is a non trivial problem. In [49], the authors proposed to learn the distance function used to compare vector attributes from the distribution of the attributes of the nodes of interest. They modeled this distribution as a multivariate Gaussian, then learn the parameters of the distribution using a ground truth, and finally select the nodes with a threshold on the Mahalanobis distance defined by the model parameters. Although this approach was a first attempt to introduce some learning on multivariate data in the construction of connected operators, the node selection process still remained purely local.

Finally, the reconstruction step has been initially studied in [9] which defines a natural reconstruction: a pixel is valued with the level of the smallest selected node that contains it. Urbach et al. proposed in [24] to alleviate the problem of non increasing attributes by modifying the reconstruction process with the subtractive rule. In this strategy, whenever a node is removed, the level of its descendants are lowered by the height of the deleted node. This strategy has been used for the production of shape-size pattern spectra. Finally, in [11] the authors proposed a reconstruction process based on the notion of hyperconnection that enables the preservation of texture details during the reconstruction process.

In this article, we propose a novel approach in order to select nodes in the Max-Tree. Our idea is to perform an unsupervised Bayesian classification of the nodes using a hierarchical Markovian model. This approach provides several benefits:
                           
                              •
                              a natural handling of multiple or multivariate attributes through the use of multivariate probability density functions;

a robust global process for the classification of the nodes;

an unsupervised approach that learns the model from each image without any ground truth.

Hierarchical Markovian models exist for about two decades now [50]. The advantage of these models compared to traditional hidden Markov random fields [51,52] is the possibly of computing exact inference without iterative algorithms. They have since been extensively applied to quadtrees [53,54] or similar structures [55]. In the same way, Hidden Markov Chains (HMC) allow an exact inference based on Maximum of Posterior Marginal (MPM) criterion and both approaches on chains [56,57] and quadtrees [58] have been refined through pairwise or triplet models in the past decade. Indeed, Markovian chains share similar properties with Markovian quadtrees than can be seen as hierarchical Markov chains [59].

In the proposed scheme, we use the Max-Tree to define a hierarchical Markovian probabilistic model. The attribute values of the nodes serve as observations, and we aim at finding the most probable hidden label value of each node of the tree. The existing iterative unsupervised classification algorithms for quadtree-like structures remain adapted to this model.

We propose several experiments in order to demonstrate how this approach can be used in different image analysis tasks: shape separation in artificial images, source detections in astronomical images, and vessel network segmentation in eye fundus images are some examples given in this paper to show the generic ability of this model. In those experiments, we study the use and combination of various node attributes and how to exploit the classification results in order to obtain a filtering, a segmentation, or extract sources. The results obtained on source detection are compared to those obtained with existing methods and the vessel network segmentation method is evaluated on two standard image databases.

The contribution of this article can also be considered from the field of hierarchical Markovian image processing where the replacement of the usual quadtree model by a morphological tree provides several benefits:
                           
                              •
                              morphological trees are naturally adapted to each image, removing the presence of the block artifacts of the quadtree [54];

they are more compact than quadtrees (less nodes in the tree) and can easily be preprocessed in order to drastically decrease the number of nodes which leads to better computation time;

they offer the possibility to integrate various node attributes as multivariate observations. More specifically, all the shape descriptors that have no meaning in a quadtree are now usable; and

observations are naturally present at every level of the tree, whereas we usually only have data at the leave level in a quadtree in practice.

This manuscript is organized as follows. Section 2 presents the Max-Tree, gives the definition of several node attributes, and explains how to reconstruct an image from the tree. Section 3 describes the probabilistic model used to classify the nodes of the tree, and the unsupervised classification algorithm (given in Algorithm A). The necessary pre-processing of the data is explained in Section 4 whereas some experiments and results are detailed in Section 5. Finally, Section 6 concludes this work.

We first give some basic notations and definitions that will be used throughout the article.

We represent the image domain by a non empty finite subset E of 
                        
                           
                              
                                 Z
                              
                              
                                 2
                              
                           
                        
                     . An element of E is called a pixel, or a point. A gray-scale image f is a function from the domain E to the set of real numbers 
                        
                           R
                        
                     . The value of the gray-scale image f at pixel x, denoted by 
                        
                           f
                           (
                           x
                           )
                        
                     , is called the level of x in f.

The Max-Tree of an image is based on the decomposition of every possible threshold of the image into connected components. Connected components may be defined by any appropriate mean: e.g., by path connectivity in a graph or a general connection on the set of all subsets of E. In this article, all the examples are based on a classical 8 neighborhood on a regular square grid.

Let 
                           
                              X
                              ⊆
                              E
                           
                         be a set of points, we denote by 
                           
                              
                                 
                                    CC
                                 
                                 
                                    x
                                 
                              
                              (
                              X
                              )
                           
                         the connected component of X that contains the pixel x. We denote by 
                           
                              CC
                              (
                              X
                              )
                           
                         the set of connected components of X: 
                           
                              CC
                              (
                              X
                              )
                              =
                              
                                 
                                    
                                       
                                          
                                             CC
                                          
                                          
                                             x
                                          
                                       
                                       (
                                       X
                                       )
                                       ,
                                       
                                       x
                                       ∈
                                       X
                                    
                                 
                              
                           
                        . Moreover, we assume that E is connected: i.e., for any x in E, we have 
                           
                              
                                 
                                    CC
                                 
                                 
                                    x
                                 
                              
                              (
                              E
                              )
                              =
                              E
                           
                        .

Given an image f, the level set of f at level k in 
                           
                              R
                           
                        , denoted by 
                           
                              
                                 
                                    Level
                                 
                                 
                                    k
                                 
                              
                              (
                              f
                              )
                           
                        , is the set of pixels where the value of f is larger than or equal to k: i.e., 
                           
                              
                                 
                                    Level
                                 
                                 
                                    k
                                 
                              
                              (
                              f
                              )
                              =
                              
                                 
                                    
                                       x
                                       ∈
                                       E
                                       |
                                       f
                                       (
                                       x
                                       )
                                       ⩾
                                       k
                                    
                                 
                              
                           
                        . We denoted by 
                           
                              CC
                              (
                              f
                              )
                           
                         the set of all connected components of all level sets of f: i.e., 
                           
                              CC
                              (
                              f
                              )
                           
                         is equal to 
                           
                              
                                 
                                    ⋃
                                 
                                 
                                    k
                                    ∈
                                    R
                                 
                              
                              CC
                              (
                              
                                 
                                    Level
                                 
                                 
                                    k
                                 
                              
                              (
                              f
                              )
                              )
                           
                        . It is well known that any two elements of 
                           
                              CC
                              (
                              f
                              )
                           
                         are either disjoint or comparable: i.e., for any A and B in 
                           
                              CC
                              (
                              f
                              )
                              ,
                              A
                              ∩
                              B
                              
                              ≠
                              
                              ∅
                           
                         implies either 
                           
                              A
                              ⊆
                              B
                           
                         or 
                           
                              B
                              ⊆
                              A
                           
                        .

Then, we can define the parent mapping on 
                           
                              CC
                              (
                              f
                              )
                           
                        , denoted by 
                           
                              
                                 
                                    par
                                 
                                 
                                    CC
                                    (
                                    f
                                    )
                                 
                              
                           
                        , that associates to any element n of 
                           
                              CC
                              (
                              f
                              )
                           
                         its parent 
                        
                           
                              
                                 
                                    par
                                 
                                 
                                    CC
                                    (
                                    f
                                    )
                                 
                              
                              (
                              n
                              )
                           
                        , also denoted by 
                           
                              
                                 
                                    n
                                 
                                 
                                    -
                                 
                              
                           
                        , and defined as the smallest element of 
                           
                              CC
                              (
                              f
                              )
                           
                         that is strictly larger than n if it exists or undefined otherwise. For the sake of concision, we will simply write par instead of 
                           
                              
                                 
                                    par
                                 
                                 
                                    CC
                                    (
                                    f
                                    )
                                 
                              
                           
                         when no confusion is possible. The Max-Tree of f is then the tree defined by the set of nodes 
                           
                              CC
                              (
                              f
                              )
                           
                         and the parent mapping 
                           
                              
                                 
                                    par
                                 
                                 
                                    CC
                                    (
                                    f
                                    )
                                 
                              
                           
                        . In this context, the elements of 
                           
                              CC
                              (
                              f
                              )
                           
                         are called nodes. The single node n in 
                           
                              CC
                              (
                              f
                              )
                           
                         such that 
                           
                              
                                 
                                    n
                                 
                                 
                                    -
                                 
                              
                           
                         is equal to undefined is the root of the tree. This construction is illustrated in Fig. 2
                        .

Several efficient data-structures and algorithms (
                           
                              O
                              (
                              n
                              )
                           
                         or 
                           
                              O
                              (
                              n
                              log
                              (
                              n
                              )
                              )
                           
                         time complexity depending on the type of data processed) have been proposed to compute the Max-Tree of an image: see [9,60,61] for a survey of the different algorithms.

Let n and m be two nodes in 
                           
                              CC
                              (
                              f
                              )
                           
                        . If the node n is the parent of the node m then we say that m is a child of n, and we denote by 
                           
                              
                                 
                                    n
                                 
                                 
                                    +
                                 
                              
                           
                        , the set of all children of n. If there exists a positive integer k such that 
                           
                              n
                              =
                              
                                 
                                    par
                                 
                                 
                                    k
                                 
                              
                              (
                              m
                              )
                              =
                              
                                 
                                    
                                       
                                          par
                                          (
                                          …
                                          (
                                          par
                                       
                                       
                                          ︸
                                       
                                    
                                 
                                 
                                    k
                                    
                                    times
                                 
                              
                              (
                              m
                              )
                              …
                              )
                              )
                           
                        , i.e., if n is the parent of the parent of … of m, we say that n is an ancestor of m and that m is a descendant of n. Given a node n, the set of descendants of n is denoted 
                           
                              n
                              >
                           
                        .

One can note that any non empty subset of nodes of any Max-Tree still verifies the disjoint or comparable property and defines thus a sub-tree of (or a forest). In the following, given a subset of A of 
                           
                              CC
                              (
                              f
                              )
                           
                        , we define the tree induced by A by the tree whose set of nodes if A and whose parent relation is 
                           
                              
                                 
                                    par
                                 
                                 
                                    A
                                 
                              
                           
                        . Indeed, one can show that the tree induced by A is the Max-Tree of a filtered version of f: this tree can be processed as any other Max-Tree.

The nodes of the Max-Tree of an image are usually equipped with attributes, i.e., features that are relevant for a given application. For example, if one is looking for vessels in a medical image, a feature measuring the elongation of the node may be a good point to start with.

We define hereafter some node attributes that will be used in the following of this article. Let f be a gray-scale image, and let 
                           
                              n
                              ⊆
                              E
                           
                         be a node in 
                           
                              CC
                              (
                              f
                              )
                           
                        .
                           
                              
                                 
                                    
                                       Area
                                       (
                                       n
                                       )
                                    
                                 
                              
                              is the number of pixels in 
                                    
                                       n
                                       :
                                       Area
                                       (
                                       n
                                       )
                                       =
                                       card
                                       (
                                       n
                                       )
                                    
                                 ;

the level of n is the largest level k in 
                                    
                                       R
                                    
                                  such that n belongs to the level set of f at level 
                                    
                                       k
                                       :
                                       Level
                                       (
                                       n
                                       )
                                       =
                                       max
                                       
                                          
                                             
                                                ℓ
                                                ∈
                                                R
                                                |
                                                n
                                                ∈
                                                
                                                   
                                                      Level
                                                   
                                                   
                                                      ℓ
                                                   
                                                
                                                (
                                                f
                                                )
                                             
                                          
                                       
                                    
                                 . By extension, we set 
                                    
                                       Level
                                       (
                                       undefined
                                       )
                                    
                                  equals 
                                    
                                       min
                                       
                                          
                                             
                                                f
                                                (
                                                x
                                                )
                                                ,
                                                
                                                x
                                                ∈
                                                E
                                             
                                          
                                       
                                    
                                 .

is the level of the highest node in the branch rooted in n:
                                    
                                       
                                          Highest
                                          (
                                          n
                                          )
                                          =
                                          
                                             max
                                          
                                          {
                                          Level
                                          (
                                          m
                                          )
                                          ,
                                          m
                                          ∈
                                          CC
                                          (
                                          f
                                          )
                                          |
                                          n
                                          
                                          is
                                          
                                          an
                                          
                                          ancestor
                                          
                                          of
                                          
                                          m
                                          }
                                       
                                    
                                 
                              

is the difference between the level of the highest node in the branch rooted in n and the level of the parent of n: 
                                    
                                       Depth
                                       (
                                       n
                                       )
                                       =
                                       Highest
                                       (
                                       n
                                       )
                                       -
                                       Level
                                       (
                                       
                                          
                                             n
                                          
                                          
                                             -
                                          
                                       
                                       )
                                    
                                 ;

is the number of pixels of 
                                    
                                       E
                                       ⧹
                                       n
                                    
                                  that are adjacent to n;

is the ratio of the area of n and the square of its perimeter:
                                    
                                       
                                          Circularity
                                          (
                                          n
                                          )
                                          =
                                          Area
                                          (
                                          n
                                          )
                                          /
                                          
                                             
                                                (
                                                Perimeter
                                                (
                                                n
                                                )
                                                )
                                             
                                             
                                                2
                                             
                                          
                                       
                                    
                                 The circularity measure is translation, rotation, and scale invariant (up to the quantization effects). It is equal to 
                                    
                                       1
                                       /
                                       (
                                       4
                                       π
                                       )
                                    
                                  for a circle which is the most compact shape, and it decreases as the shape goes off the perfect circle;

are the x and y coordinate of the centroid of the shape 
                                    
                                       n
                                       :
                                       
                                          
                                             x
                                          
                                          
                                             ¯
                                          
                                       
                                       (
                                       n
                                       )
                                    
                                  is equal to 
                                    
                                       (
                                       
                                          
                                             ∑
                                          
                                          
                                             (
                                             x
                                             ,
                                             y
                                             )
                                             ∈
                                             n
                                          
                                       
                                       x
                                       )
                                       /
                                       card
                                       (
                                       n
                                       )
                                    
                                  and 
                                    
                                       
                                          
                                             y
                                          
                                          
                                             ¯
                                          
                                       
                                       (
                                       n
                                       )
                                    
                                  is equal to 
                                    
                                       (
                                       
                                          
                                             ∑
                                          
                                          
                                             (
                                             x
                                             ,
                                             y
                                             )
                                             ∈
                                             n
                                          
                                       
                                       y
                                       )
                                       /
                                       card
                                       (
                                       n
                                       )
                                    
                                 ;

the central moment of order pq: 
                                    
                                       
                                          
                                             μ
                                          
                                          
                                             pq
                                          
                                       
                                       (
                                       n
                                       )
                                    
                                  is equal to 
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             (
                                             x
                                             ,
                                             y
                                             )
                                             ∈
                                             n
                                          
                                       
                                       
                                          
                                             (
                                             x
                                             -
                                             
                                                
                                                   x
                                                
                                                
                                                   ¯
                                                
                                             
                                             (
                                             n
                                             )
                                             )
                                          
                                          
                                             p
                                          
                                       
                                       
                                          
                                             (
                                             y
                                             -
                                             
                                                
                                                   y
                                                
                                                
                                                   ¯
                                                
                                             
                                             (
                                             n
                                             )
                                             )
                                          
                                          
                                             q
                                          
                                       
                                    
                                 ;

the elongation of n obtained from the central moments: 
                                    
                                       Elongation
                                       (
                                       n
                                       )
                                    
                                  is equal to 
                                    
                                       
                                          
                                             
                                                
                                                   λ
                                                
                                                
                                                   2
                                                
                                             
                                             /
                                             
                                                
                                                   λ
                                                
                                                
                                                   1
                                                
                                             
                                          
                                       
                                    
                                  with
                                    
                                       
                                          
                                             
                                                λ
                                             
                                             
                                                i
                                             
                                          
                                          =
                                          
                                             
                                                μ
                                             
                                             
                                                20
                                             
                                          
                                          (
                                          n
                                          )
                                          +
                                          
                                             
                                                μ
                                             
                                             
                                                02
                                             
                                          
                                          (
                                          n
                                          )
                                          ±
                                          
                                             
                                                4
                                                
                                                   
                                                      μ
                                                   
                                                   
                                                      11
                                                   
                                                   
                                                      2
                                                   
                                                
                                                (
                                                n
                                                )
                                                +
                                                
                                                   
                                                      (
                                                      
                                                         
                                                            μ
                                                         
                                                         
                                                            20
                                                         
                                                      
                                                      (
                                                      n
                                                      )
                                                      -
                                                      
                                                         
                                                            μ
                                                         
                                                         
                                                            02
                                                         
                                                      
                                                      (
                                                      n
                                                      )
                                                      )
                                                   
                                                   
                                                      2
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              

the first moment of Hu [62] of the shape n equivalent to a moment of inertia, it is translation, rotation, and scale invariant: 
                                    
                                       
                                          
                                             Hu
                                          
                                          
                                             1
                                          
                                       
                                       (
                                       n
                                       )
                                    
                                  is equal to 
                                    
                                       (
                                       
                                          
                                             μ
                                          
                                          
                                             20
                                          
                                       
                                       (
                                       n
                                       )
                                       +
                                       
                                          
                                             μ
                                          
                                          
                                             02
                                          
                                       
                                       (
                                       n
                                       )
                                       )
                                       /
                                       
                                          
                                             card
                                          
                                          
                                             2
                                          
                                       
                                       (
                                       n
                                       )
                                    
                                 .

All these attributes can be computed efficiently either during the construction of the tree or as a linear post-processing.

In order to filter an image using the Max-Tree, one must be able to map a tree back into the image space: this step is called a reconstruction. The reconstruction procedure involves: (1) to define the nodes of the tree that will be reconstructed: this is given by a selection criterion, and (2) to define which value will be assigned to the reconstructed nodes: this is a valuation function.

Formally, a criterion σ on 
                           
                              CC
                              (
                              f
                              )
                           
                         is a function that associates the value true or false to each node in 
                           
                              CC
                              (
                              f
                              )
                           
                        . Given a node n, we say that n satisfies σ if 
                           
                              σ
                              (
                              n
                              )
                              =
                              true
                           
                        . For example, given a size threshold 
                           
                              t
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    +
                                 
                              
                           
                        , a simple criterion is 
                           
                              
                                 
                                    σ
                                 
                                 
                                    a
                                 
                              
                           
                         defined for any node 
                           
                              n
                              ∈
                              CC
                              (
                              f
                              )
                           
                         by 
                           
                              
                                 
                                    σ
                                 
                                 
                                    a
                                 
                              
                              (
                              n
                              )
                              =
                              true
                           
                         if 
                           
                              Area
                              (
                              n
                              )
                              >
                              t
                           
                         and 
                           
                              false
                           
                         otherwise. This criterion is satisfied by the nodes whose area is larger than t. By abuse of notation, we also denote by 
                           
                              true
                           
                         (resp. 
                           
                              false
                           
                        ), the criterion that associates the value 
                           
                              true
                           
                         (resp. 
                           
                              false
                           
                        ) to every node in 
                           
                              CC
                              (
                              f
                              )
                           
                        .

A valuation v on 
                           
                              CC
                              (
                              f
                              )
                           
                         is a function that associates a value to each node in 
                           
                              CC
                              (
                              f
                              )
                           
                        : i.e., v maps 
                           
                              CC
                              (
                              f
                              )
                           
                         to 
                           
                              
                                 
                                    R
                                 
                                 
                                    d
                                 
                              
                           
                         with d a strictly positive integer. The mapping Level that associates its level to any node is a natural valuation, but any attribute value can be used.

Finally, the reconstruction of the Max-Tree constrained by the criterion σ for the valuation v, is an image from E to 
                           
                              
                                 
                                    R
                                 
                                 
                                    d
                                 
                              
                           
                         denoted by 
                           
                              R
                              [
                              CC
                              (
                              f
                              )
                              ,
                              σ
                              ,
                              v
                              ]
                           
                        . Given a pixel x of E, the value of 
                           
                              R
                              [
                              CC
                              (
                              f
                              )
                              ,
                              σ
                              ,
                              v
                              ]
                           
                         at pixel x is the valuation of the smallest node in 
                           
                              CC
                              (
                              f
                              )
                           
                         that contains x and that satisfies σ:
                           
                              (1)
                              
                                 R
                                 [
                                 CC
                                 (
                                 f
                                 )
                                 ,
                                 σ
                                 ,
                                 v
                                 ]
                                 (
                                 x
                                 )
                                 =
                                 v
                                 (
                                 
                                    min
                                 
                                 {
                                 n
                                 ∈
                                 CC
                                 (
                                 f
                                 )
                                 |
                                 x
                                 ∈
                                 n
                                 
                                 and
                                 
                                 σ
                                 (
                                 n
                                 )
                                 =
                                 true
                                 }
                                 )
                              
                           
                        Two examples of reconstructions are given in Fig. 3
                        . It can be shown that the reconstruction of the whole (using the criterion 
                           
                              true
                           
                        ) Max-Tree of the function f with the valuation Level is equal to f itself: 
                           
                              R
                              [
                              CC
                              (
                              f
                              )
                              ,
                              true
                              ,
                              Level
                              ]
                              =
                              f
                           
                        . In other words, if we reconstruct the tree without any filtering using the initial level values of the components we get the original image back. Also note that the root node must satisfy σ in order to guaranty that Eq. (1) is always correctly defined.

In the following, given an image f from E to 
                        
                           R
                        
                     , we consider the problem of classifying the nodes of the Max-Tree of f into K classes using a Bayesian approach. In this context, we consider that each node owns two values: an observation and a label value. The label value is hidden and we want to determine its more probable value according to a probabilist model that defines how observations are linked to label values (likelihood) and the assumptions we have on the distribution of the labels (prior). Note that in the context of Bayesian classification the nodes of the tree are usually called sites.

The labels are represented by the mapping X from the set of nodes 
                        
                           CC
                           (
                           f
                           )
                        
                      to the set of labels 
                        
                           
                              
                                 Ω
                              
                              
                                 X
                              
                           
                           =
                           1
                           ,
                           …
                           ,
                           K
                        
                     . The observations are represented by the mapping 
                        
                           Y
                        
                      from the set of nodes 
                        
                           CC
                           (
                           f
                           )
                        
                      to 
                        
                           
                              
                                 R
                              
                              
                                 d
                              
                           
                        
                     . Our goal is then, for each node n in 
                        
                           CC
                           (
                           f
                           )
                        
                     , to find the label 
                        
                           
                              
                                 ℓ
                              
                              
                                 n
                              
                           
                        
                      that maximizes the marginal posterior probability of having 
                        
                           X
                           (
                           n
                           )
                        
                      equals 
                        
                           
                              
                                 ℓ
                              
                              
                                 n
                              
                           
                        
                      knowing 
                        
                           Y
                        
                     :
                        
                           (2)
                           
                              
                                 
                                    ℓ
                                 
                                 
                                    n
                                 
                              
                              =
                              arg
                              
                                 
                                    
                                       max
                                    
                                    
                                       k
                                       ∈
                                       
                                          
                                             Ω
                                          
                                          
                                             X
                                          
                                       
                                    
                                 
                              
                              P
                              (
                              X
                              (
                              n
                              )
                              =
                              k
                              |
                              Y
                              )
                           
                        
                     
                  

To simplify notation, we denote the discrete probability and the probability density function with the same symbol P. Whenever the situation is not ambiguous we write 
                        
                           P
                           (
                           X
                           )
                        
                      for 
                        
                           P
                           (
                           X
                           =
                           x
                           )
                        
                     .

The idea is to use the Max-Tree of f to define a hierarchical Markov model on the nodes 
                           
                              CC
                              (
                              f
                              )
                           
                        . This construction is illustrated in Fig. 4
                        . In such a model:
                           
                              •
                              the probability of observing the data 
                                    
                                       y
                                    
                                  on the node n only depends of the label of 
                                    
                                       n
                                       :
                                       P
                                       (
                                       Y
                                       (
                                       n
                                       )
                                       =
                                       y
                                       |
                                       X
                                       )
                                       =
                                       P
                                       (
                                       Y
                                       (
                                       n
                                       )
                                       =
                                       y
                                       |
                                       X
                                       (
                                       n
                                       )
                                       )
                                    
                                 ;

the probability of having the label k on the non root node n conditional on all forefathers of n only depends of the label of the father of 
                                    
                                       n
                                       :
                                       P
                                       (
                                       X
                                       (
                                       n
                                       )
                                       =
                                       k
                                       |
                                       X
                                       )
                                       =
                                       P
                                       (
                                       X
                                       (
                                       n
                                       )
                                       =
                                       k
                                       |
                                       X
                                       (
                                       
                                          
                                             n
                                          
                                          
                                             -
                                          
                                       
                                       )
                                       )
                                    
                                 ; and

for the root node r, we have 
                                    
                                       P
                                       (
                                       X
                                       (
                                       r
                                       )
                                       =
                                       k
                                       |
                                       X
                                       )
                                       =
                                       P
                                       (
                                       X
                                       (
                                       r
                                       )
                                       =
                                       k
                                       )
                                    
                                 .

This model provides a hierarchical Markovian regularization in the classification process as the label of a node only depends of the label of its parent.

Under these hypothesis, the joint probability of X and 
                           
                              Y
                           
                         can be factorized as:
                           
                              (3)
                              
                                 P
                                 (
                                 X
                                 ,
                                 Y
                                 )
                                 =
                                 P
                                 (
                                 X
                                 (
                                 r
                                 )
                                 )
                                 ×
                                 
                                    
                                       
                                          ∏
                                       
                                       
                                          
                                             
                                                
                                                   n
                                                   ∈
                                                   CC
                                                   (
                                                   f
                                                   )
                                                
                                                
                                                   n
                                                   ≠
                                                   r
                                                
                                             
                                          
                                       
                                    
                                 
                                 P
                                 (
                                 X
                                 (
                                 n
                                 )
                                 |
                                 X
                                 (
                                 
                                    
                                       n
                                    
                                    
                                       -
                                    
                                 
                                 )
                                 )
                                 ×
                                 
                                    
                                       
                                          ∏
                                       
                                       
                                          n
                                          ∈
                                          CC
                                          (
                                          f
                                          )
                                       
                                    
                                 
                                 P
                                 (
                                 Y
                                 (
                                 n
                                 )
                                 |
                                 X
                                 (
                                 n
                                 )
                                 )
                              
                           
                        with r the root node.

It is well known that in such an acyclic model, the posterior marginal probabilities (2), and thus the label of each node, can be computed with a deterministic belief message passing algorithm [63]. This algorithm requires two traversals of the tree:
                           
                              •
                              the top-down pass, where the tree is browsed from the leaves to the root, computes for each node n in 
                                    
                                       CC
                                       (
                                       f
                                       )
                                    
                                 , the posterior probability of the label 
                                    
                                       X
                                       (
                                       n
                                       )
                                    
                                  conditionally to the observations available in the descendants of 
                                    
                                       n
                                       :
                                       P
                                       (
                                       X
                                       (
                                       n
                                       )
                                       |
                                       Y
                                       (
                                       m
                                       )
                                       ,
                                       m
                                    
                                  is a descendant of n);

the bottom-up pass, where the tree is browsed from the root to the leaves, computes for each node n in 
                                    
                                       CC
                                       (
                                       f
                                       )
                                    
                                 , the posterior probability of the label 
                                    
                                       X
                                       (
                                       n
                                       )
                                    
                                  conditionally to the observations 
                                    
                                       Y
                                       :
                                       P
                                       (
                                       X
                                       (
                                       n
                                       )
                                       |
                                       Y
                                       )
                                    
                                 .

The detailed procedure is given in Algorithm 1 in appendix. One can note that in the context of quadtree processing, trees being represented with the root at the top, the “top-down pass” is called “bottom-up pass” (and conversely).

However, this in-scale procedure assumes that the model parameters (root probabilities, transition probabilities, and data-driven likelihoods) are known and a more complex iterative algorithm, presented in the next section, must be used in order to obtain an unsupervised classification strategy when those parameters also have to be estimated.

In this section, we present an unsupervised classification algorithm which is a direct adaptation of the algorithms described in [53,54] and the reader can refer to those articles for a detailed description.

We only consider the case of Gaussian likelihoods for the data-driven term. Thus, for any n in 
                           
                              CC
                              (
                              f
                              )
                           
                         and any class k, we have:
                           
                              (4)
                              
                                 P
                                 (
                                 Y
                                 (
                                 n
                                 )
                                 |
                                 X
                                 (
                                 n
                                 )
                                 =
                                 k
                                 )
                                 =
                                 N
                                 (
                                 Y
                                 (
                                 n
                                 )
                                 ;
                                 
                                    
                                       μ
                                    
                                    
                                       k
                                    
                                 
                                 ,
                                 
                                    
                                       Σ
                                    
                                    
                                       k
                                    
                                 
                                 )
                              
                           
                        with 
                           
                              N
                              (
                              ·
                              ;
                              
                                 
                                    μ
                                 
                                 
                                    k
                                 
                              
                              ,
                              
                                 
                                    Σ
                                 
                                 
                                    k
                                 
                              
                              )
                           
                         the multivariate Gaussian distribution of mean 
                           
                              
                                 
                                    μ
                                 
                                 
                                    k
                                 
                              
                           
                         in 
                           
                              
                                 
                                    R
                                 
                                 
                                    d
                                 
                              
                           
                         and of 
                           
                              d
                              ×
                              d
                           
                         covariance matrix 
                           
                              
                                 
                                    Σ
                                 
                                 
                                    k
                                 
                              
                           
                        .

Then, the transition probability from a label to another one, i.e., the prior probability of having the label k of 
                           
                              
                                 
                                    Ω
                                 
                                 
                                    X
                                 
                              
                           
                         on a non-root node in 
                           
                              CC
                              (
                              f
                              )
                           
                         knowing the label 
                           
                              
                                 
                                    k
                                 
                                 
                                    ′
                                 
                              
                           
                         of its father, is the same for all nodes:
                           
                              (5)
                              
                                 P
                                 (
                                 X
                                 (
                                 n
                                 )
                                 =
                                 k
                                 |
                                 X
                                 (
                                 
                                    
                                       n
                                    
                                    
                                       -
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       k
                                    
                                    
                                       ′
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       t
                                    
                                    
                                       
                                          
                                             k
                                          
                                          
                                             ′
                                          
                                       
                                       k
                                    
                                 
                              
                           
                        with 
                           
                              
                                 
                                    t
                                 
                                 
                                    
                                       
                                          k
                                       
                                       
                                          ′
                                       
                                    
                                    k
                                 
                              
                           
                         a real number in the interval 
                           
                              [
                              0
                              ,
                              1
                              ]
                           
                         such that 
                           
                              
                                 
                                    ∑
                                 
                                 
                                    k
                                    ∈
                                    
                                       
                                          Ω
                                       
                                       
                                          X
                                       
                                    
                                 
                              
                              
                                 
                                    t
                                 
                                 
                                    
                                       
                                          k
                                       
                                       
                                          ′
                                       
                                    
                                    k
                                 
                              
                              =
                              1
                           
                        . Usually, we have for any k in 
                           
                              
                                 
                                    Ω
                                 
                                 
                                    X
                                 
                              
                           
                        , that 
                           
                              
                                 
                                    t
                                 
                                 
                                    kk
                                 
                              
                           
                         is close to 1 which means that, the prior probability that a node has the same label that its parent is high.

Finally, for the root node r in 
                           
                              CC
                              (
                              f
                              )
                           
                         and any label k, we set
                           
                              (6)
                              
                                 P
                                 (
                                 X
                                 (
                                 r
                                 )
                                 =
                                 k
                                 )
                                 =
                                 
                                    
                                       π
                                    
                                    
                                       k
                                    
                                 
                              
                           
                        in the interval 
                           
                              [
                              0
                              ,
                              1
                              ]
                           
                         with the constraint 
                           
                              
                                 
                                    ∑
                                 
                                 
                                    k
                                    ∈
                                    
                                       
                                          Ω
                                       
                                       
                                          X
                                       
                                    
                                 
                              
                              
                                 
                                    π
                                 
                                 
                                    k
                                 
                              
                              =
                              1
                           
                        .

The parameters of the model are thus:
                           
                              •
                              
                                 K mean vectors 
                                    
                                       
                                          
                                             μ
                                          
                                          
                                             k
                                          
                                       
                                    
                                  of dimension d;


                                 K covariance matrices 
                                    
                                       
                                          
                                             Σ
                                          
                                          
                                             k
                                          
                                       
                                    
                                  of dimension 
                                    
                                       d
                                       ×
                                       d
                                    
                                 ;


                                 K prior probabilities 
                                    
                                       
                                          
                                             π
                                          
                                          
                                             k
                                          
                                       
                                    
                                  for the label of the root node;

one 
                                    
                                       K
                                       ×
                                       K
                                    
                                  transition probability matrix 
                                    
                                       (
                                       
                                          
                                             t
                                          
                                          
                                             
                                                
                                                   k
                                                
                                                
                                                   ′
                                                
                                             
                                             k
                                          
                                       
                                       )
                                    
                                 .

The number of classes K is assumed to be constant and will not be estimated by the algorithm.

The model parameters can be learned from the observations using a two steps iterative procedure:
                           
                              1.
                              Compute the label estimates with the message passing algorithm according to the current model parameters.

Update the model parameters with the Expectation–Maximization (EM) algorithm according to the labels estimated in the previous step.

This leads to a completely unsupervised classification procedure. One can note that this algorithm does not guaranty the optimality of the solution as the EM algorithm may get trapped in a local maximum.

The overall procedure is described in Algorithm 2 in appendix. The initialization of the model parameters with K classes is based on the estimation of a Gaussian mixture model with K components on the whole set of observations.

One iteration of the algorithm runs in 
                           
                              O
                              (
                              N
                              ×
                              
                                 
                                    K
                                 
                                 
                                    3
                                 
                              
                              )
                           
                         time complexity, with N the number of nodes in the tree and K the number of classes. In practice, the algorithm converges in a few iterations (less than 10) which is coherent with the convergence rate usually observed [53,54,59]. The Max-Tree has at most as many nodes as the number of pixels in the image, and experiments suggest that, for natural images, the number of nodes in the Max-Tree is of the same order than the number of pixels.

In order to validate our implementation, we have simulated random labels and observations on a Max-Tree of an arbitrary image f according to the hierarchical Markov process described in Section 3.1. The image f (Fig. 5
                        (a)) is a traditional image of blood. Its Max-Tree has 17883 nodes. We have simulated a random process with 3 classes and observations in 
                           
                              
                                 
                                    R
                                 
                                 
                                    3
                                 
                              
                           
                        . Fig. 5(b) and (c) are respectively the reconstructed images of the sampled observations (
                           
                              R
                              [
                              CC
                              (
                              f
                              )
                              ,
                              true
                              ,
                              Y
                              ]
                           
                        ) and labels (
                           
                              R
                              [
                              CC
                              (
                              f
                              )
                              ,
                              true
                              ,
                              X
                              ]
                           
                        ). Finally, Fig. 5(d) is the reconstruction of the estimated labels given by the unsupervised procedure described in Section 3.2. We see that the estimated labels are close to the original labels, there are indeed 799 misclassified nodes (error rate of 4.44%).

The algorithm has been implemented in Java and was executed on a 2GHz Core I7 processor. On the previous simulated example, the algorithm converges in 2 iterations with a total running time of 500ms. The rapid convergence here is not surprising as the data have been generated with the direct model and thus they exactly fit the hypothesis of the algorithm. On the more complex scenario of astronomical image processing (Section 5.2), the 180000 nodes of the tree are classified in 9 classes: in this case, 1 iteration of the algorithm takes about 6s and the algorithm converges in 3 iterations.

Processing the tree space is just like processing any signal and some pre-processing may be required. Nevertheless, in this case, pre-processing may be performed at two different places: on the original image f or on its Max-Tree (which can also be directly interpreted as a pre-processing on the original image). A pre-processing of the input image f generally aims at enhancing the presence of the object of interests in the Max-Tree and/or removing unwanted objects. Then, a pre-processing of the tree may be required in order to ensure that the tree fulfills the hypothesis used by the node classification algorithm.

The main hypothesis of the algorithm is the Gaussian likelihood which is flexible enough to handle most cases. Nevertheless, it is known that the Gaussian model is particularly sensitive to outliers and it is thus important to remove those outliers before running the classification algorithm.

In practice, outliers can be detected by observing the histograms of the observations 
                        
                           Y
                        
                      and searching for features that would hardly fit in a Gaussian mixture model. For example, Fig. 6
                     (a) represents the histogram of the attribute Circularity on a Max-Tree of an image. We see several peaks with large weights that do not fit the Gaussian hypothesis. A careful inspection of the observations through the reconstruction of the spurious nodes allows us to see that those peaks indeed correspond to small nodes where the quantification effects are important: there are many small nodes but only few possible shapes for them. One simple solution is then to remove the small nodes: this gives a much smoother distribution (see Fig. 6(b)), and the data can be safely injected into the classifier. One can note that the threshold value used to construct Fig. 6(b) is not very sensitive: values between 10 and 30 pixels give similar results. One can also note that the classification algorithm is able to accommodate deviations from the Gaussian model as the truncated distribution at 0 in Fig. 6(b).

Another important question is the selection of the number of classes K. When the dimension of the observation is small (1 or 2), that number can be visually estimated from the histograms by evaluating the number of components needed in a Gaussian mixture model to fit the distribution. In the ideal case, the number of classes K will directly match the number of objects of interest and the association between the twos will be trivial. For example, in the experiment on retinal image segmentation (see Section 5.3), we have 
                        
                           K
                           =
                           2
                        
                      with one class for the vessels and the other one for the background. Unfortunately, in some cases K will be larger than the number of objects to identify. For example in an experiment below where we want to separate squares from circles (see Section 5.1), we need to introduce a third class to capture the noise (i.e., everything that is neither a square nor a circle). Another possible case is that the distribution of the observations among the samples of a single object of interests is multimodal or strongly asymmetric. It is then possible that several classes will be necessary to correctly model this object. In the experiments on retinal image segmentation we have observed that a model with 3 classes (2 for the background and 1 for the vessels) give a better result on some images.

One can note that if K is too large, then the algorithm is subject to numerical instabilities which are easily identified by their two possible manifestations: overflows in numeric values or label switches which prevent the algorithm from converging. The same phenomenon appears in Gaussian mixture model estimation [64].

@&#EXPERIMENTS@&#

In order to evaluate the capacity of the proposed method, we have performed three experiments. The first experiment involves synthetic data and aims at showing the possibility of realizing a shape based classification of the nodes of the Max-Tree in order to separate simple shapes. In a second experiment on astronomical images, we show that our approach enables to reliably detect very faint sources in noisy images and we compare it to two other methods. Finally, in a third experiment on retinal image segmentation, we show that the method can obtain good results on standard benchmarks.

This experiment is a demonstration of the method capacity to separate different shapes appearing at various scales, luminosities, and occluding each others. The test image f is represented in Fig. 7
                        (a) and we aim at separating squares from circles.

In order to perform this separation we use two classical shapes attributes for the observations: the circularity and the first Hu moment. Formally, let n be a node of the Max-Tree of f, the observation 
                           
                              Y
                              (
                              n
                              )
                           
                         in n is equal to the pair 
                           
                              (
                              Circularity
                              (
                              n
                              )
                              ,
                              
                                 
                                    Hu
                                 
                                 
                                    1
                                 
                              
                              (
                              n
                              )
                              )
                           
                        . Fig. 7(b) and (c) are respectively the reconstruction of the tree with the 2 attribute values (
                           
                              R
                              [
                              CC
                              (
                              f
                              )
                              ,
                              true
                              ,
                              Circularity
                              ]
                           
                        , and 
                           
                              R
                              [
                              CC
                              (
                              f
                              )
                              ,
                              true
                              ,
                              
                                 
                                    Hu
                                 
                                 
                                    1
                                 
                              
                              ]
                           
                        ). The tree is preprocessed by removing the nodes n in 
                           
                              CC
                              (
                              f
                              )
                           
                         smaller than 5 pixels or having a depth smaller than 25: this gives a new set 
                           
                              
                                 
                                    CC
                                 
                                 
                                    ′
                                 
                              
                              (
                              f
                              )
                           
                         defined by 
                           
                              
                                 
                                    
                                       n
                                       ∈
                                       CC
                                       (
                                       f
                                       )
                                       |
                                       Area
                                       (
                                       n
                                       )
                                       ⩾
                                       5
                                       
                                       and
                                       
                                       Depth
                                       (
                                       n
                                       )
                                       ⩾
                                       25
                                    
                                 
                              
                           
                        .

The nodes of the induced by 
                           
                              
                                 
                                    CC
                                 
                                 
                                    ′
                                 
                              
                              (
                              f
                              )
                           
                         are classified in 3 classes in order to obtain a first class for circles 
                           
                              
                                 
                                    ω
                                 
                                 
                                    1
                                 
                              
                           
                        , a second class for the noise 
                           
                              
                                 
                                    ω
                                 
                                 
                                    2
                                 
                              
                           
                        , and a third class 
                           
                              
                                 
                                    ω
                                 
                                 
                                    3
                                 
                              
                           
                         for squares. The reconstruction 
                           
                              R
                              [
                              
                                 
                                    CC
                                 
                                 
                                    ′
                                 
                              
                              (
                              f
                              )
                              ,
                              true
                              ,
                              X
                              ]
                           
                         of this first classification corresponds to Fig. 7(d). The noise class is filtered out using a simple criterion 
                           
                              
                                 
                                    σ
                                 
                                 
                                    1
                                 
                              
                           
                         only satisfied if the estimated node label is different from the label of the noise class 
                           
                              
                                 
                                    ω
                                 
                                 
                                    2
                                 
                              
                           
                        : for each node n in 
                           
                              
                                 
                                    CC
                                 
                                 
                                    ′
                                 
                              
                              (
                              f
                              )
                           
                        , 
                           
                              
                                 
                                    σ
                                 
                                 
                                    1
                                 
                              
                              (
                              n
                              )
                              =
                              true
                           
                         if and only if 
                           
                              X
                              (
                              n
                              )
                              
                              ≠
                              
                              
                                 
                                    ω
                                 
                                 
                                    2
                                 
                              
                           
                        . The filtered reconstruction 
                           
                              R
                              [
                              
                                 
                                    CC
                                 
                                 
                                    ′
                                 
                              
                              (
                              f
                              )
                              ,
                              
                                 
                                    σ
                                 
                                 
                                    1
                                 
                              
                              ,
                              X
                              ]
                           
                         is shown in Fig. 7(e): circles (in black) and squares (in white) are well separated.


                        Fig. 8
                         represents the histograms of the 3 estimated classes for the two attributes – Circularity and 
                           
                              
                                 
                                    Hu
                                 
                                 
                                    1
                                 
                              
                           
                         – and their estimated probability density functions. It can be seen that:
                           
                              •
                              the attribute Circularity enables to separate the noise from the two types of shapes, but it has a poor power of separation between squares and circles; and

the attribute 
                                    
                                       
                                          
                                             Hu
                                          
                                          
                                             1
                                          
                                       
                                    
                                  enables to separate squares from circles, but it does not allow us to separate them from noise.

These observations explain the good behavior of the classification process.

In this experiment, we compare our method to a more classical quadtree Markovian segmentation on astronomical images. In such images, the goal is to detect all the sources that shine on a dark background (see Fig. 9
                        (a)). Despite the apparent simplicity of the problem, the difficulty here relies mainly on the large variations in scale (from few pixels to thousands of pixels) and in brightness (from less than 0dB to dozen of dB) of the various sources. This leads mainly to two difficult cases: the extraction of very faint sources drown in the background noise and the separation of close or superposed sources. One can also note the presence of artifacts that should not be detected as sources, like the diffraction spikes around the bright stars that are caused by the support of the secondary mirror of the telescope.

First, a pre-processing of the original image f is performed in order to compress its dynamic range (astronomical images are stored in linear pixel scale). This gives the image 
                           
                              
                                 
                                    f
                                 
                                 
                                    ′
                                 
                              
                           
                         defined by, for any pixel x in E:
                           
                              (7)
                              
                                 
                                    
                                       f
                                    
                                    
                                       ′
                                    
                                 
                                 (
                                 x
                                 )
                                 =
                                 
                                    
                                       f
                                       (
                                       x
                                       )
                                    
                                    
                                       |
                                       f
                                       (
                                       x
                                       )
                                       |
                                    
                                 
                                 log
                                 (
                                 1
                                 +
                                 |
                                 f
                                 (
                                 x
                                 )
                                 |
                                 )
                              
                           
                        Then, the Max-Tree of 
                           
                              
                                 
                                    f
                                 
                                 
                                    ′
                                 
                              
                           
                         is pre-processed by removing very small and very large nodes in order to obtain the set of nodes 
                           
                              
                                 
                                    CC
                                 
                                 
                                    ′
                                 
                              
                              (
                              
                                 
                                    f
                                 
                                 
                                    ′
                                 
                              
                              )
                              =
                              
                                 
                                    
                                       n
                                       ∈
                                       CC
                                       (
                                       
                                          
                                             f
                                          
                                          
                                             ′
                                          
                                       
                                       )
                                       |
                                       10
                                       ⩽
                                       Area
                                       (
                                       n
                                       )
                                       ⩽
                                       20
                                       ,
                                       000
                                    
                                 
                              
                           
                        .

In this application we use the level of the nodes for the observations: for each node n in 
                           
                              
                                 
                                    CC
                                 
                                 
                                    ′
                                 
                              
                              (
                              
                                 
                                    f
                                 
                                 
                                    ′
                                 
                              
                              )
                              ,
                              Y
                              (
                              n
                              )
                           
                         is equal to 
                           
                              Level
                              (
                              n
                              )
                           
                        . The nodes of the tree induced by 
                           
                              
                                 
                                    CC
                                 
                                 
                                    ′
                                 
                              
                              (
                              
                                 
                                    f
                                 
                                 
                                    ′
                                 
                              
                              )
                           
                         are classified in 9 classes. A segmentation map is obtained by reconstructing the estimated label values 
                           
                              R
                              [
                              
                                 
                                    CC
                                 
                                 
                                    ′
                                 
                              
                              (
                              
                                 
                                    f
                                 
                                 
                                    ′
                                 
                              
                              )
                              ,
                              true
                              ,
                              X
                              ]
                           
                         (see Fig. 9(b), note that labels have been sorted so that largest labels correspond to brightest objects). Finally, in order to obtain a list of sources, we consider that each regional maximum of the segmentation map is a source located on its centroid.

In order to compare our results, we have also run a quadtree Markovian segmentation method – Marsiaa [54]
                        
                           1
                           
                              http://lsiit-miv.u- strasbg.fr/paseo/LSBdetection.php#MARSIAA.
                        
                        
                           1
                         – and a reference source extractor software in astronomy – SExtractor
                           2
                           
                              http://www.astromatic.net/software/sextractor.
                        
                        
                           2
                         – on the same image. Marsiaa was configured to behave as close as possible to our algorithm (Gaussian likelihoods, EM optimization method, and 9 classes) such that the main difference relies in the tree structure used to represent the image. Default parameters were used for SExtractor. Our algorithm, Marsiaa and SExtractor extracted respectively 2157, 1742, and 1799 sources. For comparison, the ninth SDSS Data Release [65] (SDR9
                           3
                           
                              http://www.sdss3.org/dr9/.
                        
                        
                           3
                        ) which contains a catalog of sources compiled from several frames taken in several wavelengths and analyzed by several source detection and deblending algorithms contains roughly 3400 sources in the same area of the sky.


                        Fig. 10
                        (a) is a close up view of Fig. 9(a) with the position of the sources detected by the three algorithms. The segmentation map obtained with our algorithm and Marsiaa are depicted respectively in Fig. 10(b) and (c). It can be seen that our algorithm is able to extract very faint sources that were both missed by Marsiaa [54] and SExtractor. Nevertheless, configurations where a faint source is very close to a brighter one is difficult to disambiguate in our scheme as the faint object may not appear as a maximum in the segmentation map: such objects are then missed by the algorithm. A comparison of the two segmentation maps clearly shows the two different tree structures with noisy borders for the Max-Tree and a block effect for the quadtree.

In this last experiment, we propose a method to segment the vascular network in retinal images (see for example Fig. 11
                        (a)). The characteristics of this vascular network (length, width, organization, …) are markers of several pathologies and its automatic segmentation is an active research field (see [68,69,66] among others).

In the following, we consider only the green channel of the retinal images. This image is pre-processed using a morphological opening with a flat cross structuring element of 3 pixels diameter in order to remove holes and reconnect some faint vessels. Then, a black top-hat (difference between a closing and the original image) with a circular structuring element of 5 pixels diameter is performed in order to remove large scale variations from the image and to inverse the image: the vascular network now appears as a bright structure over a mostly flat dark background. We denote this image by 
                           
                              
                                 
                                    f
                                 
                                 
                                    ′
                                 
                              
                           
                        . The Max-Tree of 
                           
                              
                                 
                                    f
                                 
                                 
                                    ′
                                 
                              
                           
                         is pre-processed by removing very small nodes in order to obtain the new set of nodes 
                           
                              
                                 
                                    CC
                                 
                                 
                                    ′
                                 
                              
                              (
                              
                                 
                                    f
                                 
                                 
                                    ′
                                 
                              
                              )
                              =
                              
                                 
                                    
                                       n
                                       ∈
                                       CC
                                       (
                                       
                                          
                                             f
                                          
                                          
                                             ′
                                          
                                       
                                       )
                                       |
                                       Area
                                       (
                                       n
                                       )
                                       ⩾
                                       5
                                    
                                 
                              
                           
                        .

Then we equip each node nin 
                           
                              
                                 
                                    CC
                                 
                                 
                                    ′
                                 
                              
                              (
                              
                                 
                                    f
                                 
                                 
                                    ′
                                 
                              
                              )
                           
                         with a vectorial observation composed of the level of the node, the inverse of its first Hu moment and its elongation: i.e., 
                           
                              Y
                              (
                              n
                              )
                              =
                              (
                              Level
                              (
                              n
                              )
                              ,
                              1
                              /
                              
                                 
                                    Hu
                                 
                                 
                                    1
                                 
                              
                              ,
                              Elongation
                              (
                              n
                              )
                              )
                           
                        . The nodes in 
                           
                              
                                 
                                    CC
                                 
                                 
                                    ′
                                 
                              
                              (
                              
                                 
                                    f
                                 
                                 
                                    ′
                                 
                              
                              )
                           
                         are then classified in 2 classes. Finally, a segmentation map is obtained by reconstructing the estimated label values 
                           
                              R
                              [
                              
                                 
                                    CC
                                 
                                 
                                    ′
                                 
                              
                              (
                              
                                 
                                    f
                                 
                                 
                                    ′
                                 
                              
                              )
                              ,
                              true
                              ,
                              X
                              ]
                           
                         (see Fig. 11(c) and (d) and Fig. 12
                        (c)–(f)).

This segmentation procedure is evaluated on two standard retinal image databases: Digital Retinal Images for Vessel Extraction (DRIVE [66]) and Structured Analysis of the Retina (STARE [67]). DRIVE is made of 40 color images divided equally in a training set and a test set. As our method learns directly from each image, the training set is not used. Each of the 20 images of the test set measures 
                           
                              565
                              ×
                              584
                           
                         pixels. It comes with 2 manual segmentations and a mask of the eye fundus (central region of interest). The STARE image database contains 20 color images, all used for tests (no training set). Each image measures 
                           
                              700
                              ×
                              605
                           
                         pixels and comes with 2 manual segmentations. There is no official eye fundus mask for the STARE database but they can be easily derived from the results provided by the authors of the database [67].

Both databases use the same quality measures. For each image, we define:
                           
                              •
                              the total number of pixels inside the mask of the eye fundus denoted by N;

the number of True Positives (TP): number of pixels classified as vessel in the segmentation and in the ground truth;

the number of True Negatives (TN): number of pixels classified as background in the segmentation and in the ground truth (inside the eye fundus mask);

the True Positive Rate (TPR): the number of true positives (TP) divided by the number of pixels marked as vessel in the ground truth;

the True Negative Rate (TNR): the number of true negatives (TN) divided by the number of pixels marked as background in the ground truth;

the Accuracy: the number of true positives (TP) plus the number of true negatives (TN) divided by the total number of pixels N;

In the case of DRIVE, the ground truth corresponds to the first expert segmentation. For STARE, both experts ground truths are used and the results are averaged. Finally the score on each database is defined as the average score obtained on each image of the base. The standard deviation (σ) of the Accuracy is also computed in order to measure the stability of the algorithm.

The results are summarized in Table 1
                        . It can be seen that the method performs well on DRIVE were it places third for the 3 performance measures. The results are more contrasted on STARE where it can be seen that also the method has a quite low TPR it outperforms the other methods in terms of TNR and Accuracy. One can note that among the methods presented in Table 1 most are completely dedicated to the retinal vessel network extraction with complicated pre-processings to enhance the presence of faint vessels, while our method is just a simple specialization of the general classification procedure. On the other hand, the method of Xu et al. [16] relies on a morphological filtering of the Max-Tree of the image: the image representation is thus the same as ours but the way it is processed is different. The fact that we obtain quite similar results suggests that both methods seem able to extract the main structures of the Max-Tree with a slightly better sensitivity for our statistical approach.

@&#CONCLUSION@&#

In this article, we have proposed a new way to construct connected filters for image processing and analysis using multivariate attributes. The proposed strategy relies on a hierarchical Markovian classification process. Up-to-our knowledge, this is the first method that is able to handle multivariate attributes in a global and unsupervised way for the construction of connected filters. This approach allowed us to construct robust yet efficient unsupervised algorithms for several image analysis tasks: filtering, segmentation, and source detection. We have shown how to use the method on different experiments involving different types of images, various attributes and several uses of the classification results. Our results were compared to other methods and evaluated on standard image databases.

The benefits of this work can also be seen from the point of view of Markovian segmentation approaches. The use of the Max-Tree as an alternative for the quadtree offers several advantages: better adaptation to the image topology, possibility of using richer attributes as observations, and more compact model.

The possible developments of this works are numerous. Apart from the many possible applications in image analysis, it would be interesting to test the method using others morphological trees, constructed on the image or its gradient, like the binary partition tree [34] or the tree of shapes [37,38]. From the point of view of the classification method, several developments are desirable: the automatic optimization of the number of classes [74], the use of more general multivariate probability density functions for the modelization of the data driven term (like copula) [75], or the automatic selection of the most relevant attributes for a given problem.


                     
                        Algorithm 1
                        Bottom-up/top-down tree segmentation. 
                              
                                 
                              
                           
                        

Unsupervised EM/Markovian classification of the node of the tree. 
                              
                                 
                              
                           
                        

@&#REFERENCES@&#

