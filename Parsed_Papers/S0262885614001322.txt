@&#MAIN-TITLE@&#BIG-OH: BInarization of gradient orientation histograms

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           BIG-OH, binary quantization of gradient orientation based descriptors, is proposed.


                        
                        
                           
                           Quantized SIFT descriptors reduce memory by 88% compared to classical SIFT.


                        
                        
                           
                           BIG-OH has performance comparable to SIFT and GLOH.


                        
                        
                           
                           BIG-OH has better performance than BRISK, CARD, BRIEF, and other descriptors.


                        
                        
                           
                           BIG-OH is effective for large scale applications such as copy detection.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Gradient orientation histograms

SIFT

Gradient based keypoint descriptors

Keypoint descriptor quantization

@&#ABSTRACT@&#


               
               
                  Extracting local keypoints and keypoint descriptions from images is a primary step for many computer vision and image retrieval applications. In the literature, many researchers have proposed methods for representing local texture around keypoints with varying levels of robustness to photometric and geometric transformations. Gradient-based descriptors such as the Scale Invariant Feature Transform (SIFT) are among the most consistent and robust descriptors. The SIFT descriptor, a 128-element vector consisting of multiple gradient histograms computed from local image patches around a keypoint, is widely considered as the gold standard keypoint descriptor. However, SIFT descriptors require at least 128bytes of storage per descriptor. Since images are typically described by thousands of keypoints, it may require more space to store the SIFT descriptors for an image than the original image itself. This may be prohibitive in extremely large-scale applications and applications on memory-constrained devices such as tablets and smartphones. In this paper, with the goal of reducing the memory requirements of keypoint descriptors such as SIFT, without affecting their performance, we propose BIG-OH, a simple yet extremely effective method for binary quantization of any descriptor based on gradient orientation histograms. BIG-OH's memory requirements are very small—when it uses SIFT's default parameters for the construction of the gradient orientation histograms, it only requires 16bytes per descriptor. BIG-OH quantizes gradient orientation histograms by computing a bit vector representing the relative magnitudes of local gradients associated with neighboring orientation bins. In a series of experiments on keypoint matching with different types of keypoint detectors under various photometric and geometric transformations, we find that the quantized descriptor has performance comparable to or better than other descriptors, including BRISK, CARD, BRIEF, D-BRIEF, SQ, and PCA-SIFT. Our experiments also show that BIG-OH is extremely effective for image retrieval, with modestly better performance than SIFT. BIG-OH's drastic reduction in memory requirements, obtained while preserving or improving the image matching and image retrieval performance of SIFT, makes it an excellent descriptor for large image databases and applications running on memory-constrained devices.
               
            

@&#INTRODUCTION@&#

The problem of finding images that are partly or wholly similar to a query image in a large gallery has long been a central concern of image processing and computer vision researchers. Some of the important applications are image retrieval, video indexing, texture recognition, image classification, and object recognition. Many of the most successful approaches to these search problems first pinpoint repeatable keypoint locations, compute a descriptor vector measuring properties of the local image patch around each keypoint, then match keypoints using a similarity measure over pairs of keypoint descriptors. Some of the most successful algorithms for keypoint detection and description include SIFT [1], SURF [2], and ORB [3].

The SIFT descriptor is considered by many to be the gold standard for keypoint descriptors. It is quite resistant to image transformations [4,5] and has been used in many applications including image classification [6], image/video copy detection [7], object recognition [1], and image stitching [8].

There are two major problems with the use of keypoint descriptor matching for image search problems as we attempt to scale to today's extremely large problems such as video copy detection on the Internet. Both problems stem from the high dimensionality of typical keypoint descriptors such as SIFT. The first problem is storage. A single image might have thousands of keypoints, and storing the set of raw descriptors for a single image could require more storage than the image itself. This would make it difficult for, say, a movie studio to index the frames of its video collection for purposes of illegal copy detection. The second problem is the time required to evaluate the similarity of two descriptors. Similarity computation involving measures such as Euclidean distance is 
                        O
                        
                           n
                        
                     , where n is the dimensionality of the descriptor vector. Searching for an image similar to a query image among millions of images is extremely expensive in terms of time and/or money, so reducing the time required for the descriptor distance calculation could substantially decrease the cost of large-scale image search.

There are several possible ways to increase descriptor matching speed. The first is to reduce the dimensionality of the descriptor using dimensionality reduction techniques such as principal components analysis (PCA). Ke and Sukthankar [9] propose and evaluate a PCA-based SIFT descriptor (PCA-SIFT). The method is fast for matching, due to the reduced dimensionality, and it is also effective for image retrieval. However, PCA does not necessarily provide a discriminative representation for recognition, since it is unsupervised and the basis vectors it produces are global [10]. A second possibility is to use “smart” lower-dimensional descriptors that throw away information unlikely to be useful for robust matching and preserve information that is invariant to image transformations. One example is the 64-element version of the SURF descriptor [2]. A third possibility is to use binary descriptors such as BRIEF [11], BRISK [12], ORB [3], and CARD [13]. Binary descriptors require very little storage, reducing the time required to move large numbers of descriptors through the storage hierarchy. Better storage throughput leads to accelerated matching and search operations. The main limitation of binary descriptors, however, is that they tend not to perform well under image deformations such as changes in viewpoint and scale. A fourth group of approaches, used by many applications, aims to find only approximate nearest neighbors of a query descriptor based on hash codes. Every descriptor is coded and quantized by a hash function that might be randomized or learned from data [14–17]. These methods also generate binary descriptors, but since discriminative projections are learned from data, they can greatly decrease memory and computational costs. A fifth group of approaches, used by many large applications, is based on techniques for vector quantization. A large number of feature vectors are collected, and then a clustering algorithm is applied to map each vector in the feature space to a discrete cluster. The most common approach based on quantization is the bag-of-visual-words (BoVW) model, in which an image is represented by a sparse frequency vector over a set of feature descriptor cluster IDs. The BoVW model is used in many image retrieval, copy detection, and image classification applications [18]. However, both hashing and vector quantization have limitations. For example, they are not intrinsically adaptive, their performance depends strongly on hashcode or codebook size, and the quantization process may in some cases discard information in the raw descriptors that may be useful for discrimination [19]. Another limitation is that the hashcode or codebook learned from one image set may not generalize well to a new image set.

Recently, other approaches based on vector quantization have introduced simpler deterministic mappings not requiring training sets. One such alternative approach is scalar quantization [20], in which we specify a fixed equivalence relation over the feature space and code each equivalence class with a bit vector.

In this paper, we propose BIG-OH, a new method based on the scalar quantization approach. BIG-OH quantizes high dimensional gradient-based descriptors such as SIFT to a short binary vector. BIG-OH is similar to but more compact and computationally faster than previously proposed scalar quantization methods [20]. We reduce the memory required from SIFT's 128bytes (512bytes if 32-bit floating point numbers are used) to only 16bytes. Since the mapping is fixed, the quantizer does not require off-line visual word learning, which eliminates training time and the issue of adaptiveness to new features.

We find that the use of binary signatures accelerates the matching process, speeding up the naive implementation of the distance computation by a factor of 2.4. Since the descriptor space is so small, the matching process can be further accelerated by the use of a very small lookup table as described in Section 4.3.1, yielding a speedup of 10 over the naive Euclidean distance measure for the 128-byte SIFT descriptor.

BIG-OH itself is extremely simple yet very effective for practically encountered image transformations. Although one might expect that the large speedups just described would come at the cost of reduced matching accuracy or reduced image retrieval performance, to the contrary, despite the use of radical binary quantization, we find the method to be quite resistant to image transformations such as blurring, illumination, and rotation.

The proposed quantized descriptor, BIG-OH, is thus useful for reducing the storage and computational resources required for large scale image-to-image matching applications. We also anticipate its usefulness on mobile devices with limited storage and compute resources such as smartphones and tablets.

To demonstrate the performance of the proposed method, we perform an extensive series of experiments. We find that BIG-OH is equally discriminative and robust compared to the state-of-the-art descriptors for feature matching under different photometric and geometric transformations, and we also find, surprisingly, that it modestly outperforms SIFT for image retrieval.

In detail, we perform experiments to measure the following performance criteria:
                        
                           •
                           Speedup of descriptor match scoring.

Decrease in descriptor storage requirements.

Matching results consistent with SIFT.

Accurate matching in comparison to state-of-the art algorithms under image transformations such as rotation, viewpoint change, and scale change. This experiment is designed to evaluate the robustness of descriptors under different levels of challenging distortions.

Accurate image retrieval under severe image transformations. This experiment is designed to evaluate the distinctiveness of the descriptors.

The rest of the paper is organized as follows. In Section 2, we briefly survey related work and state-of-the-art descriptors. In Section 3, we explain the BIG-OH quantization method. In Section 4, we describe our experimental methods and results. Finally, we conclude in Section 5.

@&#RELATED WORK@&#

Most of the successful feature point descriptors described in the literature can be classified into two types: those based on gradient histograms [2,1,9,4,13] and those based on local pixel intensity differences without explicit gradient calculations [21,11,3,22]. We will call the latter group “intensity-based” descriptors.

Local binary patterns (LBP) [23] are among the most widely-used intensity-based features, with applications in face recognition [24], 3D textured surface recognition [25], human detection [26], and background subtraction [22]. The LBP operator is particularly attractive due to its computational simplicity. To compute LBP features for a given image patch, we compare the gray-level intensity of a pixel with that of k of its neighbors at a pixel distance of r. The recommended values for r and k are r
                     =1 and k
                     =8, respectively. From the comparisons, we obtain a binary vector expressing the relationship between the gray level intensity at the point of interest to each of its neighbors. We then obtain a histogram of these binary patterns over the keypoint's affine region. Since the local binary pattern operator has 2
                        k
                      possible values, the histogram has 2
                        k
                      entries. Indeed, the main limitation of LBP is the large number of bins. Due to this limitation, LBP is not very useful for local image patches, but it is very useful for whole-image representations for image classification.

There is a quantized version of the LBP operator known as CSLBP [21]. In CSLBP, rather than comparing each pixel's intensity to that of its k neighbors, only center-symmetric pairs are compared [21]. The resulting histogram size for CSLBP
                        r,k
                      is 2
                        k/2. This makes CSLBP more appropriate for local keypoint characterization than LBP itself. In practice, although CSLBP
                        r,k
                      is simple to compute and efficient, it has less tolerance to rotation and scale changes than SIFT, and the descriptor dimensionality is still typically more than that of SIFT.

Calonder et al. [11] propose a binary descriptor known as BRIEF, which, similarly to LBP and CSLBP, is computed from a set of binary intensity tests over a given patch. The main difference between BRIEF and LBP-based descriptors is that the pattern of pixel comparisons that is applied to each patch to obtain the descriptor bit vector is chosen in advance arbitrarily, rather than according to a specific pattern such as center-symmetric pairs. The authors find that selection of test pixels according to a Gaussian distribution works well [11,3]. BRIEF is fast due to the use of simple pairwise comparisons leading to a bit string. The robustness of BRIEF under some transformations such as blur and illumination is similar to that of SIFT, but it is quite sensitive to rotation and scale changes. This sensitivity makes BRIEF inappropriate for applications in which robustness to rotation and scale changes is necessary.

Rublee et al. [3] propose an extension of BRIEF known as ORB. The ORB descriptor is rotation invariant and noise resistant. Like BRIEF, ORB uses FAST features for keypoint detection [27]. FAST is neither rotation nor scale invariant, but it is very fast to compute. To make ORB rotation invariant, the method calculates the centroid and moments of the pixel values in the image patch and uses the moments to obtain the orientation for each feature.

Leutenegger et al. [12] propose the Binary Robust Invariant Scalable Keypoints (BRISK) descriptor, which aims for both scale and rotation invariance. The authors compute a binary string similarly to BRIEF and ORB. They take symmetric patterns for pixel intensity comparisons similar to the DAISY descriptor [28] (though note that DAISY is constructed for dense points, whereas BRISK is designed to work with keypoints). BRISK is computationally more expensive and also requires more space than ORB or BRIEF [5].

Although intensity-based keypoint descriptors are simple to compute and have proven immensely successful in image classification and object detection, gradient-based keypoint descriptors outperform them on object detection and image retrieval tasks. We now turn our attention to gradient-based keypoint descriptors.

The most widely-used method in the gradient-based family is SIFT [1], with applications in objection detection, image retrieval, copy detection, robotics, and image classification. To compute a SIFT descriptor, the patch around a keypoint is divided into a grid (4×4 in nearly all implementations). In each cell, the gradient magnitude g(x,
                     y) and orientation θ(x,
                     y) are computed for each pixel. Each gradient orientation is then quantized into one of 8 directions, and a weighted histogram of the quantized orientations is computed. The weight in the histogram of each pixel is the pixel's gradient magnitude modulated by a circular Gaussian function that is centered over the keypoint and scaled to 1.5 times the keypoint's scale. The Gaussian modulation means that pixels near the keypoint contribute more strongly than those further away. Finally, the gradient orientation histograms of all cells are concatenated into one vector.

Bay et al. [2] propose a variation on SIFT known as speeded-up robust features (SURF). SURF is much faster than SIFT, mostly due to the extensive use of Haar-like filter responses that can be calculated quickly using integral images. One version of SURF optimizes further by cutting the dimensionality of the SIFT descriptor by half. SURF has been successfully used in various applications such as shot boundary detection [29,30], video scene detection [30,31], image recognition, and feature tracking [32]. However, Rublee et al. [3] show that SIFT outperforms SURF in feature matching under rotation transformations.

Another well known gradient-based method is called gradient location-oriented histogram (GLOH) [4]. GLOH descriptors are computed over a log-polar grid. Three concentric circles are placed around the keypoint with radiuses 6, 11, and 15. The outer two circles are divided into 8 angular directions, and the center circle is not divided, giving 17 local regions. In each region, the gradient orientations are quantized and summarized by a 16-bin histogram. The resulting descriptor is a 272 element vector comprising the 17 gradient orientation histograms. Since the raw GLOH descriptor is quite long, the authors reduce the dimensionality of the descriptor to 128 using principal components analysis (PCA) calculated over 47,000 training image patches.

Another method based on gradient orientation histograms is Ambai and Yoshida compact and real-time descriptors (CARD) [13]. The authors use lookup tables to accelerate the computation of gradient orientation histograms, and they binarize the resulting descriptor using the signs of the descriptor vector's projection onto a lower dimensional subspace. The method is thus demonstrably faster than SIFT (16 times faster in the author's experiments) and uses less storage. However, the method's robustness to image transformations is lower than that of SIFT.

GLOH and CARD both use linear dimensionality reduction techniques as part of their approach. The general idea of dimensionality reduction for keypoint descriptors was introduced by Ke et al. [9]. Their algorithm, PCA-SIFT, reduces the traditional SIFT descriptor to 36 dimensions using PCA. Other authors have found that nonlinear dimensionality reduction methods work better than PCA-SIFT in some cases [10].

Recently, methods based on learning binary codes for existing descriptors by projecting them onto discriminative hyperplanes have emerged. Examples include LSH [14,15], Semi-supervised hashing [16], supervised hashing with kernels [33], Spectral Hashing (SH) [34], LDAHash [17], and D-BRIEF [35]. Weiss et al. [34] propose a method to encode descriptors using compact binary codes. They formulate the hashing problem as a graph partition problem. Strecha et al. [17] applies Linear Discriminant Analysis (LDA) to SIFT descriptors and obtains binary codes by discriminative projections, whereas Trzcinski and Lepetit [35] also applies discriminative projections on keypoint patches and threshold their coordinates to obtain the binary descriptors. They name their descriptor Discriminative BRIEF (D-BRIEF). D-BRIEF only requires four bytes. Despite the very small space (only 232 distinct features), D-BRIEF's performance is comparable to that of SIFT. In our experiments, we observe that D-BRIEF's performance gradually decreases as distortion becomes more severe.

In large-scale applications, SIFT descriptors are quantized before indexing and matching. Sivic and Zisserman [36] propose a bag of visual word model for SIFT descriptors and demonstrate the method's efficacy at video matching and indexing. They quantize descriptors to cluster centroids obtained by the k-means algorithm, and the image is then represented by a sparse frequency vector of visual words. As discussed earlier, the main limitation of this approach is scalability, as k-means is computationally very expensive for large values of k. Nister and Stewenius [37] propose an extension of the visual word model, introducing a vocabulary tree that hierarchically quantizes the descriptors. The authors show that with a large vocabulary, performance is increased compared to flat k-means. The bag of visual words model has become the standard state-of-the art framework for large scale image and object retrieval applications, and many extensions have been proposed [18,20,38,39]. One example is that of Jégou et al [38], who divide the SIFT descriptor into different parts, then quantize each part using separate k-means models, and finally concatenate the centroids. Zhou et al. [20] quantize a SIFT vector by thresholding each element of the descriptor vector by the median value in the vector. The resulting descriptor is 128bits. Zhou et al. name this general method scalar quantization (SQ). To decrease quantization error, they also produce a 256-bit vector using two thresholds.

In the next section, we introduce an extremely simple new method based on the scalar quantization approach.

The BIG-OH procedure follows the standard keypoint descriptor computation pipeline designed by Mikolajczyk and Schmid [4]. It can be broken down into the steps of (1) keypoint detection, (2) keypoint patch normalization, (3) orientation histogram computation, and (4) descriptor computation.

In step (1), SIFT filters the image with difference of Gaussian (DoG) filters at multiple scales. The DoG filters are set up to approximate Laplacian-of-Gausian (LoG) filters. To detect DoG keypoints, a Gaussian pyramid is constructed by progressively blurring and subsampling the image. At each level of the pyramid, the differences between blurred images are taken and local extrema (maxima/minima) are identified and treated as candidate keypoints. Next, keypoints with low contrast and keypoints on simple edges are eliminated [1]. We use Lowe's DoG keypoint detector, but we also experiment with Hessian-affine and Harris-affine keypoint detectors. Hessian-affine keypoints are points at which the determinants of the Hessian are local maxima over space and scale. Harris-affine keypoints are points at which a criterion related to the magnitude of the eigenvalues of the local second moment matrix are local maxima over space and scale. The Hessian criterion emphasizes blob-like regions, whereas the Harris criterion emphasizes corner-like structures. Both detectors return an elliptical region around the keypoint [40–42]. In practice, with default parameter settings, we get fewer keypoints from Harris-affine and Hessian-affine detectors than from DoG—on average, over the Mikolajczyk dataset (see Section 4.1.2), we get 1397 keypoints from the Harris-affine detector, 1636 keypoints from the Hessian-affine detector, and 2135 keypoints from the DoG detector.

In step (2), following the standard procedure [4,21,42], to obtain scale, rotation, and affine invariance, we correct the local image patch in the neighborhood around the keypoint for the orientation, scale, and (for Hessian-affine and Harris-affine keypoints) anisotropic distortion parameters provided by the keypoint detection algorithm. DoG regions are corrected for rotation according to the dominant gradient direction, and elliptical regions obtained by Harris-affine and Hessian-affine detectors are rotated and scaled to circular regions. In every case, the normalized image patch is extracted into a 41×41pixel grid.

In step (3), we compute gradient orientation histograms identically to SIFT, except that we do not perform Gaussian weighting of the gradient magnitudes, since such weighting will have little to no effect on the keypoint descriptors after eventual binarization.

Finally, in step (4), we compute the BIG-OH descriptor. Although the BIG-OH descriptor itself is the main contribution of this paper, we briefly explain step (3), gradient orientation histogram computation, so that the reader can in turn fully understand step (4), the computation of the BIG-OH descriptor.

A keypoint is normally represented as a tuple (x,
                        y,
                        θ,
                        σ,
                        
                           R
                        ), where x and y are the coordinates, θ is the dominant orientation, σ is the scale, and 
                           R
                         is the normalized image patch around the keypoint. For each pixel in 
                           R
                        , the gradient magnitude g(x,
                        y) and orientation ψ(x,
                        y) are computed as follows:
                           
                              (1)
                              
                                 
                                    
                                       
                                          g
                                          
                                             x
                                             y
                                          
                                          =
                                          
                                             
                                                
                                                   R
                                                   x
                                                
                                                
                                                   
                                                      x
                                                      y
                                                   
                                                   2
                                                
                                                +
                                                
                                                   R
                                                   y
                                                
                                                
                                                   
                                                      x
                                                      y
                                                   
                                                   2
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          ψ
                                          
                                             x
                                             y
                                          
                                          =
                                          arctan
                                          
                                             
                                                
                                                   
                                                      R
                                                      y
                                                   
                                                   
                                                      x
                                                      y
                                                   
                                                
                                                
                                                   
                                                      R
                                                      x
                                                   
                                                   
                                                      x
                                                      y
                                                   
                                                
                                             
                                          
                                          ,
                                       
                                    
                                 
                              
                           
                        where 
                           R
                        
                        
                           x
                        (x,
                        y)=
                        
                           R
                        (x
                        +1,
                        y)−
                        
                           R
                        (x
                        −1,
                        y) and 
                           R
                        
                        
                           y
                        (x,
                        y)=
                        
                           R
                        (x,
                        y
                        +1)−
                        
                           R
                        (x,
                        y
                        −1).

The orientation histogram is computed from g(⋅,⋅) and ψ(⋅,⋅). We remind the reader that since 
                           R
                         was already normalized by θ and σ, the resulting orientation histogram is necessarily rotation and scale invariant to the extent that the estimated θ and σ are accurate. Following SIFT, we quantize the orientations to eight bins. We simply sum the magnitudes of the gradient vectors falling into each of the eight bins to obtain a gradient orientation histogram 
                           R
                        
                        
                           H
                        .

Orientation histograms yield good performance when they are computed on small regions over a region of interest (ROI) and concatenated. As in SIFT, we divide the patch 
                           R
                         into a 4×4 grid and calculate the quantized orientation histogram separately in each cell, resulting in 16 orientation histograms from 
                           R
                        , which we write 
                           R
                        
                        
                           H
                        
                        ={h
                        1,
                        h
                        2,…,
                        h
                        16}. Each h
                        
                           i
                         is an eight-bin histogram, resulting in the final 128-element vector 
                           R
                        
                        
                           H
                        .

We now proceed to step (4), the computation of the BIG-OH descriptor. In practice, we find that SIFT gradient orientation histograms tend to change rapidly under certain types of distortion. However, we also find that the direction of the differences in magnitudes between bins in the same histogram tend to be preserved over those distortions. BIG-OH is based on this simple observation; it is a binary signature of the direction of the differences in magnitudes within each gradient orientation histogram.

In particular, we compute a binary signature B
                        
                           H
                         over the individual orientation histograms in 
                           
                              R
                              H
                           
                        . B
                        
                           H
                         has one bit for each bin in each histogram in 
                           
                              R
                              H
                           
                        . Each bit indicates whether the magnitude in the corresponding histogram bin is larger or smaller than the magnitude in another histogram bin at an offset distance ϕ. The parameter ϕ indicates the offset used in determining which pairs of bins should be compared in deriving the representative binary histogram. For each individual orientation histogram h
                        
                           i
                        
                        ∈
                        
                           
                              R
                              H
                           
                        , we construct the corresponding binary histogram b
                        
                           i
                        
                        =(b
                        
                           i
                        (0),…,
                        b
                        
                           i
                        (7)) as follows:
                           
                              (2)
                              
                                 
                                    b
                                    i
                                 
                                 
                                    j
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      1
                                                   
                                                   
                                                      if
                                                      
                                                      
                                                         h
                                                         i
                                                      
                                                      
                                                         j
                                                      
                                                      ≥
                                                      
                                                         h
                                                         i
                                                      
                                                      
                                                         
                                                            
                                                               j
                                                               +
                                                               ϕ
                                                            
                                                         
                                                      
                                                      
                                                      mod
                                                      
                                                      8
                                                      .
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      0
                                                   
                                                   
                                                      otherwise
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

Finally, we concatenate the individual binary histograms to obtain the complete 128-bit BIG-OH descriptor B
                        
                           H
                        
                        ={b
                        1,
                        b
                        2,…,
                        b
                        16}. An example binarization of a gradient orientation histogram is shown in Fig. 1
                        .


                        Fig. 2
                         compares the matching performance of BIG-OH under various photometric transformations, using various values for ϕ, to that of SIFT. Performance is measured by the F-score as explained in Section 4.4.3. It can be seen that SIFT's performance is mainly preserved after quantization for most values of ϕ. In most cases, we use ϕ
                        =1.

As we shall see, the BIG-OH descriptor exhibits equivalent and in some cases better tolerance to certain kinds of image degradation than does the SIFT descriptor.

To evaluate the performance of BIG-OH in comparison to SIFT and other state of the art algorithms, we performed three experiments. The first examines the computational speedup and reduction in storage requirements for BIG-OH in comparison to SIFT and other methods. The second experiment explores the matching performance of BIG-OH in comparison to SIFT and other algorithms. Finally, in the third experiment, we compare the utility of BIG-OH and SIFT descriptors for image retrieval under challenging transformations such as cropping, scale, and occlusion.

We use several different datasets in the experiments.

For the computational speedup experiment (experiment I), we use synthetic data consisting of uniform random bytes for all descriptors, since real data is not needed to evaluate raw match score calculation performance. This synthetic dataset is only used to determine the time required for raw descriptor distance calculations. For the rest of the experiments in this paper, we use the same standard data sets that state of the art algorithms have been tested on.

To evaluate the quality of descriptor matching under different types of image transformations in experiment II, we use data provided by the Visual Geometry Group (VGG), Katholieke Universiteit Leuven, Inria Rhone-Alpes, and the Center for Machine Perception. It can be accessed online
                              1
                           
                           
                              1
                              
                                 http://www.robots.ox.ac.uk/vgg/research/affine/.
                            along with software to execute matching protocols. The images in this dataset contain different types of geometric and photometric transformations on different types of scenes (scenes with repeated textures and more structured scenes containing specific objects). Examples are shown in Fig. 3
                           . For each transformation type T, an original image I
                           0 is gradually deformed in five steps, so each transformation T is represented by six images: T
                           ={I
                           0,
                           I
                           1,…,
                           I
                           5}. All of the images for a particular transformation of a particular source image are related by homographies. In our experiments, we follow the common practice of using I
                           0 as a query and subsets of the remaining images as gallery images.

The types of transformations in the dataset are as follows. Details are reported by Mikolajczyk and Schmid [4].
                              
                                 •
                                 Illumination transformation (Leuven): illumination change sequences are obtained by varying the camera aperture.

Scale change and blur transformation (Bikes and Trees): scale and blur change sequences are acquired by varying the camera zoom and focus.

JPEG compression transformation (UBC): JPEG-transformed sequences are generated with the standard xv image browser, with the image quality parameter set between 2% and 40%.

Viewpoint change transformation (Graffiti and Wall): viewpoint change sequences for planar surfaces are obtained by varying the camera position from a frontal view to one with significant foreshortening, up to 50–60°.

Zoom and rotate transformations: zoom and rotation transformations are applied to two types of images: a structured scene (Boat) and a textured scene (Bark). Image rotations are obtained by rotating the camera around its optical axis in the range of 30–45°.

This dataset has been used in many evaluations of keypoint matching methods [1,4,21].

To evaluate the utility of BIG-OH for image retrieval (IR) in comparison with SIFT under various challenging transformations such as rotation, occlusion, and scale change, we use three datasets. The first dataset, which we call IR-I, is provided by Ke and Sukthankar [9], available online.
                              2
                           
                           
                              2
                              
                                 http://www.cs.cmu.edu/yke/pcasift/.
                            The dataset comprises 30 images of 10 different scenes (three views per scene). The three different views of each scene are dramatically different from each other, incorporating changes in viewpoint, scale, out of plane rotation, and occlusion. The entire dataset is shown in Fig. 4(a). In contrast to the Mikolajczyk dataset, the different images of the same scene are not related by homographies.

The second dataset, which we call IR-II, is provided by Zhou et al. [39]. The dataset comprises 36 different scenes with a total of 1088 images. All the images are severely distorted by challenging transformations typically encountered in copy detection scenarios. The transformations include using a camcorder to record the video being played on a screen, placing a picture within another picture, inserting patterns, JPEG compression, change of illumination, cropping, blurring, image flipping, text insertion, zooming and rotation, viewpoint change, and decrease in image quality. All of these transformations may occur along with image shifting, contrast changing, and image warping. Fig. 5(a) shows one image randomly selected from each scene, and Fig. 5(b) shows 10 arbitrarily selected images of each of 10 different scenes.

The third dataset is the OXFORD 5K dataset [18], which we call IR-III. This is one of the most frequently used datasets for image retrieval evaluation experiments. It contains 55 query images of 11 landmarks included among a total of 5062 images collected from Flickr.
                              3
                           
                           
                              3
                              
                                 http://www.flickr.com/.
                           
                        

We also use the Paris 100K dataset, which is an unlabeled dataset. The images in Paris 100K are used as additional distractors in experiment III. More details about OXFORD 5K and Paris 100K can found in the original work [18].

The number of images along with number of descriptors for IR-II, IR-III and Paris 100K are shown in Table 1
                           .

Here we describe the method used to match keypoint descriptors from different images. Each image is represented by a set of local keypoint descriptors. Each descriptor is of the form (x,
                        y,
                        θ,
                        σ
                        1,
                        σ
                        2,
                        d
                        
                           c
                        ). We use σ
                        1 and σ
                        2, in the case of Harris and Hessian affine keypoints, to represent the separate scales of the keypoint location in the direction θ and orthogonal to θ. For DoG keypoints, it is always the case that σ
                        1
                        =
                        σ
                        2, due to the isotropic scale assumed by the method. For the descriptor itself, we have c
                        ∈{BIG−OH,SIFT,…}.

Two images are said to be similar, or matching, if they have many similar descriptors. Two descriptors d
                        1 and d
                        2 are said to be similar if they are close to each other based on some distance measure. More specifically, given two images Q and R with local keypoint sets E and F, respectively, we perform standard nearest neighbor (NN) matching subject to a “reliability” constraint. We consider the keypoint pair (e
                        
                           i
                        ,
                        f
                        
                           j
                        ), where e
                        
                           i
                        
                        ∈
                        E,
                        f
                        
                           j
                        
                        ∈
                        F to be similar if their descriptors d
                        
                           i
                         and d
                        
                           j
                         satisfy the following two conditions for distance measure α(⋅,⋅):
                           
                              •
                              Nearest neighbors
                                    
                                       (3)
                                       
                                          α
                                          
                                             
                                                d
                                                i
                                             
                                             
                                                d
                                                j
                                             
                                          
                                          =
                                          
                                             min
                                             
                                                
                                                   d
                                                   k
                                                
                                                ∈
                                                F
                                             
                                          
                                          α
                                          
                                             
                                                d
                                                i
                                             
                                             
                                                d
                                                k
                                             
                                          
                                          .
                                       
                                    
                                 
                              

Reliable match
                                    
                                       (4)
                                       
                                          
                                             T
                                             m
                                          
                                          ⋅
                                          α
                                          
                                             
                                                d
                                                i
                                             
                                             
                                                d
                                                j
                                             
                                          
                                          <
                                          
                                             min
                                             
                                                
                                                   d
                                                   k
                                                
                                                ∈
                                                F
                                                ,
                                                k
                                                ≠
                                                j
                                             
                                          
                                          α
                                          
                                             
                                                d
                                                i
                                             
                                             
                                                d
                                                k
                                             
                                          
                                          .
                                       
                                    
                                 
                              

Here T
                        
                           m
                        
                        >1 is a threshold ensuring a stable match under noise conditions.

This method is widely used in computer vision applications [1,4,21].

The distance measure α(⋅,⋅) is arbitrary; any distance measure could be used. The most widely used distance measure for keypoint descriptor matching in computer vision is the simple squared Euclidean distance
                           
                              (5)
                              
                                 α
                                 
                                    
                                       d
                                       1
                                    
                                    
                                       d
                                       2
                                    
                                 
                                 =
                                 
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       m
                                    
                                    
                                       
                                          
                                             
                                                
                                                   d
                                                   1
                                                
                                                
                                                   i
                                                
                                                −
                                                
                                                   d
                                                   2
                                                
                                                
                                                   i
                                                
                                             
                                          
                                          2
                                       
                                    
                                 
                                 ,
                              
                           
                        with m
                        =128 for SIFT. For binary vectors as in BIG-OH, one alternative is the Hamming distance. The Hamming distance between two binary vectors of equal length is the sum of positions in which the corresponding bits are different:
                           
                              (6)
                              
                                 H
                                 
                                    
                                       b
                                       1
                                    
                                    
                                       b
                                       2
                                    
                                 
                                 =
                                 
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       m
                                    
                                    
                                       
                                          
                                             
                                                b
                                                1
                                             
                                             
                                                i
                                             
                                             −
                                             
                                                b
                                                2
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Hamming distance is commonly used in information theory, coding theory, and cryptography. Note that Hamming distance is simply squared Euclidean distance specialized to binary vectors.

We use Euclidean distance in all of the experiments reported in this paper except Experiment I, in which we demonstrate the utility of Hamming distance for boosting the matching speed with the help of lookup tables.

As already explained, BIG-OH is a binary descriptor, necessarily reducing the memory required to store a set of descriptors. By reducing the storage requirements, we can also speed up computation by more effectively utilizing prefetching and caching in the storage hierarchy. Bit vector distance calculations can be further speed up by using lookup tables indexed by bytes comprising packed bit vectors. First we show how BIG-OH with Hamming distance and lookup tables can speed up descriptor match scoring, then we explore BIG-OH's reduced storage requirements in comparison to SIFT.

Since the binary XOR operator applied to two bits returns true when the bits are different, the bitwise XOR is very common in Hamming distance implementations. One alternative is to trade space for time and use a lookup table mapping XOR results to corresponding Hamming distances.
                              4
                           
                           
                              4
                              The Hamming distance calculation could be sped up even more by using POPCNT instructions available on some modern CPUs.
                            Since BIG-OH descriptors contain 16 8-bit histograms of gradient orientations, the result of the XOR operation is a 16-byte array in which each element is 8-bits (one byte). To calculate the Hamming distance between two 128-bit descriptors, then, we can build a 256-element lookup table and look up each of the 16 XOR result bytes in this table sequentially, summing the results:
                              
                                 (7)
                                 
                                    
                                       H
                                       L
                                       xor
                                    
                                    
                                       
                                          d
                                          1
                                       
                                       
                                          d
                                          2
                                       
                                    
                                    =
                                    
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          16
                                       
                                       
                                          L
                                          
                                             
                                                XOR
                                                
                                                   
                                                      
                                                         d
                                                         1
                                                      
                                                      
                                                         i
                                                      
                                                      ,
                                                      
                                                         d
                                                         2
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                
                                             
                                          
                                          .
                                       
                                    
                                 
                              
                           
                        

Here the notation d(i) refers to the ith byte of the packed bit vector d and L is the lookup table of length 256.

The computation time can be further reduced slightly by precomputing not only the bit count results but also the XOR results themselves, storing them in a table 
                              T
                              :
                              N
                              ×
                              N
                              ↦
                              N
                           , where 
                              N
                              =
                              
                                 0
                                 1
                                 2
                                 …
                                 255
                              
                           . 
                              T
                            requires only 64kB of memory regardless of the size of the descriptor database. Now, Hamming distance can be implemented as
                              
                                 (8)
                                 
                                    
                                       H
                                       L
                                       lookup
                                    
                                    
                                       
                                          d
                                          1
                                       
                                       
                                          d
                                          2
                                       
                                    
                                    =
                                    
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          16
                                       
                                       
                                          T
                                          
                                             
                                                
                                                   d
                                                   1
                                                
                                                
                                                   i
                                                
                                                ,
                                                
                                                   d
                                                   2
                                                
                                                
                                                   i
                                                
                                             
                                          
                                          ,
                                       
                                    
                                 
                              
                           where again d(i) means the ith byte of the packed bit vector d.


                           Fig. 6
                            shows the running time for large numbers of match score computations over our synthetic descriptor database. The experiments were performed on a desktop machine with 2GB RAM and a 2.5GHz quad-core processor, and each point is an average over five runs.

We do not show results for the lookup table method 
                              H
                           
                           
                              L
                           
                           
                              lookup
                           , because it cannot be distinguished from those for 
                              H
                           
                           
                              L
                           
                           
                              xor
                            at the graph's resolution, although numerically it is approximately 12% faster than 
                              H
                           
                           
                              L
                           
                           
                              xor
                            (it takes 0.0425s using Eq. (7) and brute force to find the nearest neighbor of a given descriptor in a database of 250K descriptors, whereas the same computation using Eq. (8) takes only 0.0374s).

To store a single SIFT descriptor, a minimum of 128 (unsigned) bytes is required, and some implementations, following Lowe's suggestion [1], store the descriptor as a unit-norm vector of floating point values, requiring 512bytes. In contrast, the BIG-OH binary descriptor only requires 16bytes. BIG-OH is thus extremely compact compared to SIFT and other binary descriptors such as BRIEF, ORB, and BRISK. To compare the memory requirements of different keypoint descriptors, we use the Object Classes Challenge 2010 dataset [43]. The data set has 11,321 images. We applied the VLFEAT implementation of the DoG keypoint detector with default parameters, and it extracted 6.5 million keypoints from the data set. In Fig. 7
                           , we thus show the raw amount of memory required to store 6.5 million (synthetic) keypoint descriptors under SIFT, BIG-OH, and other state of the art descriptors. The difference is large: with BIG-OH, the entire set of keypoint descriptors can be trivially stored in the main memory of a standard PC, whereas the other representations would require specialized data management techniques to process the entire data set.

The small amount of storage required for BIG-OH descriptors makes it especially attractive for the deployment of applications including machine vision algorithms such as augmented reality, image matching, object recognition, and artwork recognition on mobile devices such as smartphones and tablets.

Experiment II consists of three main parts. First, we evaluate the performance of BIG-OH with different values of ϕ. Second, we explore the consistency of BIG-OH and SIFT matches under illumination changes, blurring, JPEG compression, viewpoint changes, zooming, and rotation on the Mikolajczyk dataset. Finally, in the third part, we measure keypoint matching accuracy in comparison to other state-of-the art descriptors, namely SIFT [1], PCA-GLOH [4], PCA-SIFT [9], BRISK [12], CARD [13], SQ [20], D-BRIEF [35], and BRIEF [44], also on the Mikolajczk dataset.

In the matching accuracy evaluations, we use the Mikolajczyk dataset and the match score computation methodology explained in Section 4.2. We use the standard definitions of precision, recall, and F-score:
                              
                                 (9)
                                 
                                    recall
                                    =
                                    
                                    
                                       
                                          #
                                          
                                          of
                                          
                                          correct
                                          
                                          matches
                                       
                                       
                                          total
                                          
                                          correspondences
                                       
                                    
                                 
                              
                           
                           
                              
                                 (10)
                                 
                                    precision
                                    =
                                    
                                    
                                       
                                          #
                                          
                                          of
                                          
                                          correct
                                          
                                          matches
                                       
                                       
                                          total
                                          
                                          matches
                                       
                                    
                                 
                              
                           
                           
                              
                                 (11)
                                 
                                    F
                                    −
                                    score
                                    =
                                    
                                    2
                                    ×
                                    
                                       
                                          precision
                                          ×
                                          recall
                                       
                                       
                                          precision
                                          +
                                          recall
                                       
                                    
                                    .
                                 
                              
                           
                        

The number of correct matches is determined by the overlap error ϒ 
                           [4,42]. Overlap error measures how well two elliptical regions represented by conics A and B correspond under a known homography δ, defined by the ratio of their intersection and union:
                              
                                 (12)
                                 
                                    ϒ
                                    =
                                    1
                                    −
                                    
                                       
                                          
                                             A
                                             ∩
                                             
                                                δ
                                                T
                                             
                                             Bδ
                                          
                                       
                                       
                                          
                                             A
                                             ∪
                                             
                                                δ
                                                T
                                             
                                             Bδ
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        

A match is assumed to be correct if ϒ
                           <0.5. Details can be found in the original work [4,42].

To compare BIG-OH with SIFT in terms of F-score at different values of ϕ, for each transformation group in the Mikolajczyk dataset, we extracted keypoints from image I
                           0 (original image) and I
                           3 (moderately distorted image), performed nearest neighbor matching as previously explained, and then we counted as false any matches for which ϒ
                           <0.5. We repeated the precision and recall calculation with different reliable matching thresholds T
                           
                              m
                            (refer to Section 4.2) and recorded the highest F-score over all T
                           
                              m
                           .

The results are shown in Fig. 2. In most cases, ϕ
                           =1 gives the most consistent performance. We thus set ϕ
                           =1 in all remaining experiments.

When we perform BIG-OH binarization of the gradient histograms in the SIFT descriptor, we expect distances between corresponding BIG-OH descriptors to be correlated with the distances between SIFT descriptors extracted at the same points. If BIG-OH preserves the relative ordering of distances between pairs of SIFT descriptors, we would expect BIG-OH to return nearest neighbor matches similar to what SIFT would return under the same conditions. In this experiment, we validate this hypothesis by first extracting DoG keypoints from two images of the same scene from the Mikolajczyk dataset, getting SIFT and BIG-OH matches for the two images according to the algorithm of Section 4.2, and then comparing the correspondences according to the two algorithms. For each DoG keypoint in the first image of a pair, if the keypoint has a match in the second image according to both SIFT and BIG-OH, we plot on a scatterplot the point (x,
                           y) where x is the (arbitrary) index of the matching point according to SIFT and y is the index of the matching point according to BIG-OH. When the match is consistent, we have x
                           =
                           y, and a perfectly consistent descriptor would produce a diagonal scatterplot.


                           Fig. 8
                            shows the results of this process for image I
                           0 matched with I
                           1 (least distorted) and I
                           0 matched with I
                           5 (most distorted) over all 6 transformations. We see that BIG-OH is quite consistent with SIFT for all types of transformations across all levels of degradation.

Finally, we compared the F-score performance of BIG-OH with that of other state of the art descriptors. The results are shown in Figs. 9 and 10
                           
                           . As the Graffiti, Boat, and Bark sets contain rotation, BRIEF, which is not rotation invariant, is not included in Fig. 10.

BIG-OH's performance is very competitive. On the illumination, blur, and JPEG transformations, its performance is just behind BRIEF, SIFT, and PCA-GLOH; on the rotation-based transformations, it performs slightly worse than SIFT and PCA-GLOH. BIG-OH outperforms the other binary descriptors and quantizers (excepting BRIEF on transformations not including rotation) and the other SIFT approximation method (PCA-SIFT), whereas it is slightly outperformed by the compute- and storage-intensive descriptors SIFT and PCA-GLOH. BIG-OH performs this well using a much smaller memory footprint than the other methods; Fig. 11
                            plots the average F-score of each method over all levels of transformation against the descriptor's memory footprint, in bytes per descriptor.

In this experiment, we show that BIG-OH is also effective in image retrieval applications. We use the IR datasets [9,18,39] discussed in Section 4.1.3. IR-I is designed to evaluate keypoint matching accuracy over viewpoint changes, and IR-II and IR-III are designed to evaluate image copy detection methods. All three datasets contain severely deformed images.

On IR-I, following Ke and Sukthankar's [9] demonstration of PCA-SIFT's consistency with SIFT, we compare BIG-OH directly to SIFT. On IR-II and IR-III, the large-scale databases, we perform a full comparison of BIG-OH with SIFT, CARD, SQ, D-BRIEF, and SIFT with BoVW quantization.

To evaluate the retrieval performance of each method, we use mean average precision (mAP), which is obtained by, for each query image, obtaining a match score for every gallery image exhaustively, ranking by match score, finding the match score cutoff necessary to retrieve all copies of the query image, calculating the precision of retrieval at that cutoff (the number of copies divided by the number of gallery images with match score above the threshold), and averaging that precision value over every query image.

The match score for a single pair of images I
                        
                           p
                         and I
                        
                           q
                         is, given a value of the stable matching ratio threshold T
                        
                           m
                        ,
                           
                              
                                 M
                                 
                                    
                                       I
                                       p
                                    
                                    
                                       I
                                       q
                                    
                                    
                                       T
                                       m
                                    
                                 
                                 =
                                 
                                    
                                       ∥
                                       D
                                       
                                          
                                             I
                                             p
                                          
                                       
                                       
                                          ∩
                                          
                                             T
                                             m
                                          
                                       
                                       D
                                       
                                          
                                             I
                                             q
                                          
                                       
                                       ∥
                                    
                                    
                                       ∥
                                       D
                                       
                                          
                                             I
                                             p
                                          
                                       
                                       ∥
                                    
                                 
                                 ,
                              
                           
                        where D(I
                        
                           p
                        ) and D(I
                        
                           q
                        ) are the sets of descriptors in images I
                        
                           p
                         and I
                        
                           q
                        , respectively, 
                           D
                           
                              
                                 I
                                 p
                              
                           
                           
                              ∩
                              
                                 T
                                 m
                              
                           
                           D
                           
                              
                                 I
                                 q
                              
                           
                         is the set of paired descriptors in D
                        
                           p
                         and D
                        
                           q
                         found to be in correspondence using stable matching ratio threshold T
                        
                           m
                        , and ∥
                        D(I
                        
                           p
                        )∥ is the number of keypoints in image I
                        
                           p
                        . Ideally, for each image I
                        
                           p
                        , the first entries in the ranked list should be I
                        
                           p
                         itself followed by distorted versions of I
                        
                           p
                         in M.

First we report results of the comparison of BIG-OH and SIFT on IR-I. This dataset has three views of each of 10 scenes. None of the views are distinguished as canonical, so with IR-I we use every image as a query image. We report results under two conditions: when T
                        
                           m
                         is fixed at the standard value of 1.5 (the default value used by many SIFT implementations such as VLFEAT
                           5
                        
                        
                           5
                           
                              http://www.vlfeat.org/.
                        ), and when T
                        
                           m
                         is tuned to the dataset to obtain the best possible results. For each descriptor and for each keypoint, we varied T
                        
                           m
                         between 1.0 and 3.5, obtained the pairwise match scores, and sorted the results by match score for each query image.


                        Table 2
                         shows the mAP for IR-I dataset. BIG-OH has better mAP for DoG and Harris affine keypoints, and SIFT has better mAP for Hessian affine keypoints. Overall, SIFT and BIG-OH are quite consistent.

Now we turn our attention to larger datasets containing hundreds of thousands of images in turn containing millions of descriptors, as shown in Table 1. Efficient indexing of keypoint descriptors for large scale image search is an active and robust area of research [20,36,38,45,46]. However, since the scope of this paper does not include indexing, we show results for exhaustive search for every descriptor type. We compare every descriptor of each query image with the all of the images in the gallery.

In experiment II and the first part of experiment III, we have demonstrated that BIG-OH performs consistently with several different keypoint detectors. In the large scale comparison of experiment III, however, we only use the Hessian affine detector, as it is widely used in many image and video applications [7,21,36].

We implemented the SIFT BoVW scheme similarly to Sivic and Zisserman [36]. We obtained the visual vocabulary using a standard flat k-means clustering with k
                        =200,000 over all three datasets. Due to quantization, the match score is calculated slightly differently from the other descriptors. Each descriptor in the query image votes for each image in the gallery containing a descriptor with the same index. The gallery is simply ranked by the number of votes. This is the same approach used by Jegou et al. [47].

As mentioned in Section 4.1.3, when evaluating each descriptor type on IR-II and IR-III, we optionally include more than 227 million distractor descriptors from the Paris 100K dataset.

In experiment III, we included SIFT, binary variations of SIFT (CARD, SQ, and BIGOH), and other binary descriptors (BRIEF and D-BRIEF). We did not include BRISK, used in experiment II, because of its weak tolerance to image degradations.


                        Table 3
                         shows the mAP of each descriptor in the comparison set on the IR-II and IR-III datasets, with and without the Paris 100K distractor set. It can be seen that BIG-OH and SQ perform substantially better than SIFT and the binary descriptors. Fig. 12
                         plots the average performance of each descriptor against its memory footprint.

With the caveat that a real-world implementation of BIG-OH would require efficient indexing, which has not been considered in this paper, BIG-OH's performance is especially promising considering its simplicity. Compared to SQ, BIG-OH's quantization method is faster (there is no sorting step required) and more compact, requiring only 16bytes to store one descriptor compared to SQ's 32bytes. CARD is equally compact as BIG-OH but has lower retrieval performance. BRIEF performed very well in terms of matching accuracy when the transformations did not include rotation in experiment II, but it does not perform well in large-scale IR. D-BRIEF is the most compact descriptor (only 32bits) and was quite competitive in experiment II, but its performance clearly starts to deteriorate as the size of the database increases (as shown in Table 3).

@&#DISCUSSION AND CONCLUSION@&#

We have proposed a simple and effective quantization method for gradient orientation histograms called BIG-OH-quantized SIFT gradient orientation histograms have discriminative power comparable to SIFT, PCA-GLOH, and other descriptors.

The BIG-OH quantization procedure is simpler than that of any other existing binary gradient descriptor. It is also faster than other binary gradient histograms such as CARD [13]: CARD requires linear projection (via matrix multiplication) of descriptor vectors using large sparse projection matrices followed by binarizaton based on the sign of each element of the projection.

BIG-OH is effective for image copy detection, and the storage requirements for its features are small enough to enable efficient management for large-scale applications where millions of features must be stored and indexed.

In current work, we are developing a framework based on BIG-OH for image retrieval on smartphones and tablet devices.

@&#ACKNOWLEDGMENTS@&#

This research was supported by fellowships to Junaid Baber from the University of Balochistan, the National Institute of Informatics, and the Asian Institute of Technology. We are thankful to Wengang Zhou for providing the IR-II dataset.

@&#REFERENCES@&#

