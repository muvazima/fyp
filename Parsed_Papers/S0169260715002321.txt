@&#MAIN-TITLE@&#Computer-aided gastrointestinal hemorrhage detection in wireless capsule endoscopy videos

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           An automated GI hemorrhage detection scheme from WCE images is proposed..


                        
                        
                           
                           Features from the NGLCM of the spectrum of the frames and SVM are employed.


                        
                        
                           
                           We introduce difference average- a new feature that operates on NGLCM.


                        
                        
                           
                           Validity of the method is confirmed by statistical and graphical analyses.


                        
                        
                           
                           The performance of the proposed scheme, compared to the existing ones is promising.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Wireless capsule endoscopy (WCE)

Bleeding detection

Support vector machine

Normalized Gray Level Co-Occurrence Matrix

@&#ABSTRACT@&#


               
               
                  Background and objective
                  Wireless Capsule Endoscopy (WCE) can image the portions of the human gastrointestinal tract that were previously unreachable for conventional endoscopy examinations. A major drawback of this technology is that a large volume of data are to be analyzed in order to detect a disease which can be time-consuming and burdensome for the clinicians. Consequently, there is a dire need of computer-aided disease detection schemes to assist the clinicians. In this paper, we propose a real-time, computationally efficient and effective computerized bleeding detection technique applicable for WCE technology.
               
               
                  Methods
                  The development of our proposed technique is based on the observation that characteristic patterns appear in the frequency spectrum of the WCE frames due to the presence of bleeding region. Discovering these discriminating patterns, we develop a texture-feature-descriptor-based-algorithm that operates on the Normalized Gray Level Co-occurrence Matrix (NGLCM) of the magnitude spectrum of the images. A new local texture descriptor called difference average that operates on NGLCM is also proposed. We also perform statistical validation of the proposed scheme.
               
               
                  Results
                  The proposed algorithm was evaluated using a publicly available WCE database. The training set consisted of 600 bleeding and 600 non-bleeding frames. This set was used to train the SVM classifier. On the other hand, 860 bleeding and 860 non-bleeding images were selected from the rest of the extracted images to form the test set. The accuracy, sensitivity and specificity obtained from our method are 99.19%, 99.41% and 98.95% respectively which are significantly higher than state-of-the-art methods. In addition, the low computational cost of our method makes it suitable for real-time implementation.
               
               
                  Conclusion
                  This work proposes a bleeding detection algorithm that employs textural features from the magnitude spectrum of the WCE images. Experimental outcomes backed by statistical validations prove that the proposed algorithm is superior to the existing ones in terms of accuracy, sensitivity, specificity and computational cost.
               
            

@&#INTRODUCTION@&#

The classical endoscopic procedure has enabled clinicians to investigate the human gastro-intestinal (GI) tract. Despite being efficacious for the upper (duodenum, stomach and food pipe) and lower part (colon and terminal ileum) of the GI tract, the traditional endoscopy miserably fails to examine the small intestine. The human small intestine is about 8m long and conventional endoscopy such as Colonoscopy or Esophagogastroduodenoscopy cannot image it satisfactorily. To overcome the limitations of traditional endoscopy, Iddan et al. [1] pioneered the invention of wireless capsule endoscopy (WCE). The WCE system consists of a pill-shaped capsule. The capsule has a built-in video camera, video signal transmitter, light-emitting diode and a battery. It is swallowed by the patient and is propelled forward by peristalsis of human GI tract. It records images as it moves forward along the GI tract and transmits them at the same time using radio frequency. It transmits over the course of about 8h until its battery runs out. Due to its promising performance for the visualization of human GI tract, U.S. Food and Drug Administration (FDA) approved it in 2001 [2].

Manual classification of bleeding and non-bleeding endoscopic video frames has a number of limitations. The power supply of the capsule has limitations which result in low resolution (576×576) of endoscopic video frames. The video frame rate is also low (2 frames/second). Besides, about 60,000 images have to be inspected per examination. It takes an experienced clinician about 2h which may not be pragmatic in most clinical scenarios. Since the evaluation process is time-consuming and a large volume of images have to be inspected, bleeding detection becomes more subject to human error. So a computer-aided detection (CAD) of bleeding frames can make this monumental task easy for clinicians.

Given Imaging Ltd. [3] designed a software called Suspected Blood Indicator (SBI) for automatic detection of bleeding frames. But SBI demonstrates poor sensitivity and specificity and often fails to detect any kind of bleeding other than that of the small intestine [4]. The software designed by Given Imaging Ltd. allows the physician to view two consecutive frames at the same time. But due to low frame rate, two consecutive frames may not contain the area of interest. Consequently, the clinician has to toggle between images making the evaluation process even more onerous and time-consuming. All the aforementioned problems of manual screening can be eliminated by the use of CAD.

@&#RELATED WORK@&#

The previous works on GI hemorrhage detection can roughly be classified as: color based, texture based and color and texture based methods. Color based methods [5–8] basically exploit the ratios of the intensity values of the images in the RGB or HSI domain. Texture based approaches attempt to utilize the textural content of bleeding and non-bleeding images to perform classification [9–11]. It has been reported that the combination of color and texture descriptors exhibit good performance in terms of accuracy [12]. Again, depending on the region of operation, CAD bleeding and tumor detection literature can be categorized into three groups – whole image based [12–15], pixel based [5,6,16] and patch based methods [9,17] as in [7]. Whole image based methods are fast but often fail to detect small bleeding regions. Pixel based methods have to operate on each pixel of the image to generate the feature vectors. As a result, they are computationally very expensive. It can be expected that patch based methods will achieve good accuracy while keeping the computational cost low. However, patch based methods show high sensitivity but low specificity and accuracy. Besides, informative patches need to be identified manually by a clinician which hinders the idea of making the whole process automatic.

Li and Meng [9] put forward a chrominance moment and Uniform Local Binary Pattern (ULBP) based solution to bleeding detection. Yanan Fu et al. [7] came up with a super-pixel and red ratio based solution that was promising in terms of accuracy. But it was reported that this method has a high computational cost and fails to detect images with poor illumination and minor angiodysplasia regions whose hue is similar to normal tissue. Hwang et al. [16] utilized Expectation Maximization Clustering algorithm for CAD of bleeding frames. Some prior works [10,11] employed MPEG-7 based visual descriptors to identify medical events such as blood, ulcer and Crohn's disease lesions. Pan et al. [5] formed a 6-D feature vector using R,G,B,H,S,I values and used probabilistic neural network (PNN) as classifier. Liu et al. [6] proposed Raw, Ratio and Histogram feature vectors which are basically the intensity values of the image pixels and used support vector machine (SVM) to detect GI bleeding. Hegenbart et al. [18] utilized scale invariant wavelet based texture features to detect Celiac disease in endoscopic videos. Using MPEG-7 based visual descriptors, Bayesian and SVM, Cunha et al. [19] segmented the GI tract into four major topographic areas and performed image classification. For a more comprehensive review on computer aided decision support system for WCE videos, [20] can be consulted.

In this work, we aim to draw inferences (bleeding or non-bleeding) on the spatial domain of an image by extracting features in the frequency domain. Fig. 1
                         depicts the steps of the proposed scheme. At first, we compute discrete Fourier transform (DFT) of the endoscopic video frames. Afterwards we take the log transform of the magnitude spectrum of the frames. Normalized Gray Level Co-occurrence Matrix (NGLCM) matrix is then constructed to extract features from each log transformed magnitude spectrum. The selected features are computed from NGLCM. The features are then fed into support vector machine classifier to perform classification of the frames.

There are significant distinctions between the proposed approach and previous studies on bleeding detection in the literature. It has also some advantages. Both are described as follows.
                           
                              •
                              To the best of the authors’ knowledge, none of the existing works in the literature on GI bleeding detection attempt to solve the problem in the frequency domain. However, spectral texture descriptors have been used for other image classification problems such as texture classification [21,22], remote sensing image classification [23], tumor recognition in colonoscopy images [24] etc.

Most of the state-of-the-art works use either all [5,7,8] or any two [6] of the R, G and B channels. An advantage of this method lies in the fact that it uses only one channel. That is any one of the three channels can be used.

In this work we propose ‘difference average’, a new feature that can be implemented on the NGLCM. The experimental results of this feature are promising.

Our algorithm shows promising performance to correctly classify tiny bleeding regions.

The proposed scheme requires less number of features than many of the existing methods such as [15,17]. Lastly, we conduct our experiments on a large data-set. This ensures reliability and effectiveness of our algorithm in practical scenarios because an algorithm that only works well on a small data-set cannot ensure that it will work in real-world implementations.

One major concern of this approach may be the computational cost associated with the computation of DFT of the images. But with the development and implementation of fast Fourier transform (FFT) algorithms, one can expect that this approach will not be computationally costly. Besides, the method is non-iterative. These assumptions are later supported by experimental results that show the proposed method is indeed computationally inexpensive. The proposed algorithm outperforms the state-of-the-art methods implemented on the same data-set in accuracy, sensitivity and specificity.

Computer-aided GI bleeding detection is a machine learning problem and has three basic steps – feature generation or extraction, feature selection and classification. Hence, the rest of the article is organized as follows: Section 2 expounds the feature generation part of our algorithm, intuitively describes the reason behind the choice of the proposed feature extraction scheme, provides mathematical formulation of the selected features and introduces the new feature we propose in this work. In Section 3, we statistically prove that the differences of the chosen features are statistically significant. Section 4 describes the classifier we choose in the proposed method. We provide the details of the experiments conducted to demonstrate the efficacy and superiority of our algorithm, present the experimental results and explicate their significance in Section 5. Finally, Section 6 presents how this work can further be extended and Section 7 concludes the article.

We initiate this section with a brief review of spectral estimation using the DFT. The reason for choosing the DFT based texture descriptor is then expounded. We then discuss the construction of the NGLCM and provide the mathematical expressions of the features of our algorithm.

At first we compute the Fourier spectrum of each of the endoscopic video frames. The 2-D discrete Fourier transform of a WCE image f(x, y) of size M
                        ×
                        N can be expressed as:


                        
                           
                              (1)
                              
                                 F
                                 (
                                 p
                                 ,
                                 q
                                 )
                                 =
                                 
                                    ∑
                                    
                                       x
                                       =
                                       0
                                    
                                    
                                       M
                                       −
                                       1
                                    
                                 
                                 
                                    ∑
                                    
                                       y
                                       =
                                       0
                                    
                                    
                                       N
                                       −
                                       1
                                    
                                 
                                 f
                                 (
                                 x
                                 ,
                                 y
                                 )
                                 
                                    e
                                    
                                       −
                                       j
                                       2
                                       π
                                       (
                                       
                                          px
                                          M
                                       
                                       +
                                       
                                          qy
                                          N
                                       
                                       )
                                    
                                 
                              
                           
                        where p
                        =0, 1, 2, 3, …, M
                        −1 and q
                        =0, 1, 2, 3, …, N
                        −1. The frequency spectrum of an image can be obtained from the absolute value of its DFT. The frequency spectrum of an image is a measure of its frequency distribution which can generate patterns depending on the content of the image in the spatial domain. In general, high frequency components indicate sharp transition of intensities and low frequency components indicate intensities with a slow rate of change. In bleeding images, there will be sharp transitions of intensities from bleeding regions to their neighboring non-bleeding regions. These transitions will be absent in the non-bleeding frames. This is precisely why the magnitude spectrum can be useful for this particular application. Furthermore, log transformation is used with a view to reducing computational cost. An important trait of log transformation is that it compresses the dynamic range of images with large variations in the pixel intensities [25]. Fourier spectrum of an image contains a gargantuan dynamic range of values. Magnitude spectrum values may range from 0 to as large as 107 or even higher. This ensure that the maximum intensity values are not too high so that we can have a GLCM of manageable size. For instance, without applying log transformation if the highest value in the Fourier spectrum was 107, then the size of each GLCM would have been 107
                        ×107 making the algorithm computationally very expensive.

We know that in signal classification if the transform is suitably chosen, transform domain features may exhibit more useful classification-related information than the original signal. Keeping that in mind, this work explores to forge a connection between the spatial and the frequency domain of an image to generate meaningful feature descriptors and classify bleeding and non-bleeding frames. It is observed from numerous visual inspections that the frequency spectrum of the bleeding images tend to show straight lines near or along the diagonal directions. However, these lines are absent in the non-bleeding frequency spectrum. This observation is illustrated in Fig. 2
                        . The right two columns of Fig. 2 shows typical bleeding WCE frames, where spectral lines are generated in the ±45° directions of the magnitude spectrum. No such patterns appear in non-bleeding frames as we can see from the left two columns of Fig. 2. This phenomenon can be exploited to generate features for the machine learning algorithm.

To capture the above mentioned visual observations mathematically, the most appropriate strategy is to use texture feature descriptors. Texture is essential for both human visual perception and image analysis [26]. However, texture does not have any widely agreed definition. Portilla and Simon-celli [27] provide a general definition with which many researchers agree:
                           “Loosely speaking, texture images are spatially homogeneous and consist of repeated elements, often subject to some randomization in their location, size, color, orientation etc.”
                        
                     

Since, texture features are widely used to detect various texture patterns in images, they are the most appropriate descriptors in the proposed frequency domain based framework. Texture measures are used to quantify the observed visual differences of bleeding and non-bleeding spectra and these measures are utilized to train the classifier.

After computing out the magnitude spectrum of the endoscopic frames, we extract textural features from them. In our work, we employ both global and local feature descriptors. Global texture features are simple and widely used in the image classification literature to measure the overall textural content of the image. Global features are computed considering the image as a whole whereas local feature descriptors operate on a small region or a few pixels. Since in the GI hemorrhage detection problem the bleeding regions may be small and localized, local feature descriptors are very promising for achieving classification result with high accuracy. Global features are measures of distribution of intensities but they carry no information regarding the relative position of pixels with respect to each other [25]. Considering these pros and cons both global and local feature descriptors are utilized to devise the algorithm. Entropy of the magnitude spectrum is used as global feature descriptor in this study. Contrast, Sum Entropy, Sum Variance, Difference Variance and Difference Average that operate on the NGLCM are used to capture local textural information. In other words, our feature set consists of a global feature and five local features.

In order to capture local textural information from the spectrum of the images, we employ features from the NGLCM of the magnitude spectrum of the WCE frames. The Gray Level Co-occurrence Matrix (GLCM) is an L
                        ×
                        L matrix of the input image where L is the number of gray levels of the image. Fig. 3
                         illustrates the construction of GLCM from an image. If two consecutive pixels of the input image have pixel values i and j, then the (i, j)th element of the GLCM is incremented. This operation is done for every pair of pixels in the image. In this way, the GLCM is formed. The position operator P governs how this pixels are related to each other. The effect of P on the detection performance will be discussed later in this paper. In this work, the performance of Normalized Gray Level Co-occurrence Matrix on the frequency spectrum of WCE images is inspected. The NGLCM can be constructed from the GLCM using the following relation:
                           
                              (2)
                              
                                 N
                                 (
                                 i
                                 ,
                                 j
                                 )
                                 =
                                 
                                    
                                       G
                                       (
                                       i
                                       ,
                                       j
                                       )
                                    
                                    R
                                 
                              
                           
                        where R is the total number of pixel pairs in the GLCM. In essence, NGLCM maps the image to a matrix that indicates the probability of occurrence of two consecutive pixel values. This implies that NGLCM carries local textural information of the image to be extracted by the features. Besides, NGLCM based texture features have been widely used in various applications for texture analysis and image classification. These two factors motivated the use of NGLCM in the proposed method.

Various statistical measures such as mean, moment, entropy etc. are used as global texture descriptors [25] to measure the overall textural content of the image. These features operate on the entire image. After conducting repeated experiments, it has been found that entropy of the frequency spectrum demonstrates good performance as a global texture descriptor. Entropy of the frequency spectrum (En) is defined as:
                           
                              (3)
                              
                                 En
                                 =
                                 −
                                 
                                    ∑
                                    
                                       i
                                       =
                                       0
                                    
                                    
                                       L
                                       −
                                       1
                                    
                                 
                                 H
                                 (
                                 
                                    z
                                    i
                                 
                                 )
                                 
                                    log
                                    2
                                 
                                 [
                                 H
                                 (
                                 
                                    z
                                    i
                                 
                                 )
                                 ]
                              
                           
                        where H(z
                        
                           i
                        ) is the normalized histogram and L is the number of gray levels of the frequency spectrum. It computes the randomness of the pixels values of the magnitude spectrum.

Haralick et al. [29] first proposed 14 features to perform GLCM based texture analysis. Although all of these features are not widely used, they are examined and it has been found that four of them yield good algorithmic performance. These features are:
                           
                              1.
                              Contrast (Con):
                                    
                                       (4)
                                       
                                          Con
                                          =
                                          
                                             ∑
                                             i
                                          
                                          
                                             ∑
                                             j
                                          
                                          |
                                          i
                                          −
                                          j
                                          
                                             |
                                             2
                                          
                                          N
                                          (
                                          i
                                          ,
                                          j
                                          )
                                       
                                    
                                 
                                 Con measures the contrast in gray levels among neighboring pixels. Its value ranges from zero (for a constant GLCM) to (L
                                 −1)2.

Sum entropy (SE):
                                    
                                       (5)
                                       
                                          SE
                                          =
                                          
                                             ∑
                                             
                                                i
                                                =
                                                2
                                             
                                             
                                                2
                                                L
                                             
                                          
                                          
                                             P
                                             
                                                x
                                                +
                                                y
                                             
                                          
                                          (
                                          i
                                          )
                                          log
                                          [
                                          
                                             
                                                P
                                                
                                                   x
                                                   +
                                                   y
                                                
                                             
                                          
                                          (
                                          i
                                          )
                                          ]
                                       
                                    
                                 
                              

Sum variance (SV):
                                    
                                       (6)
                                       
                                          SV
                                          =
                                          
                                             ∑
                                             
                                                i
                                                =
                                                2
                                             
                                             
                                                2
                                                L
                                             
                                          
                                          
                                             
                                                (
                                                i
                                                −
                                                SE
                                                )
                                             
                                             2
                                          
                                          
                                             P
                                             
                                                x
                                                +
                                                y
                                             
                                          
                                          (
                                          i
                                          )
                                       
                                    
                                 
                                 SV is a measure of variability of the elements of NGLCM with respect to SE.

Difference variance (DV):
                                    
                                       (7)
                                       
                                          DV
                                          =
                                          Variance
                                          [
                                          
                                             P
                                             
                                                x
                                                −
                                                y
                                             
                                          
                                          ]
                                       
                                    
                                 where P
                                 
                                    x+y
                                  and P
                                 
                                    x−y
                                  are defined as follows:


                        
                           
                              (8)
                              
                                 
                                    P
                                    
                                       x
                                       +
                                       y
                                    
                                 
                                 (
                                 k
                                 )
                                 =
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    L
                                 
                                 
                                    ∑
                                    
                                       j
                                       =
                                       1
                                    
                                    L
                                 
                                 N
                                 (
                                 i
                                 ,
                                 j
                                 )
                                 
                                 i
                                 +
                                 j
                                 =
                                 k
                                 =
                                 2
                                 ,
                                 3
                                 ,
                                 4
                                 ,
                                 .
                                 .
                                 .
                                 .
                                 ,
                                 2
                                 L
                              
                           
                        
                        
                           
                              (9)
                              
                                 
                                    P
                                    
                                       x
                                       −
                                       y
                                    
                                 
                                 (
                                 k
                                 )
                                 =
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    L
                                 
                                 
                                    ∑
                                    
                                       j
                                       =
                                       1
                                    
                                    L
                                 
                                 N
                                 (
                                 i
                                 ,
                                 j
                                 )
                                 
                                 |
                                 i
                                 −
                                 j
                                 |
                                 =
                                 k
                                 =
                                 0
                                 ,
                                 1
                                 ,
                                 2
                                 ,
                                 .
                                 .
                                 .
                                 .
                                 ,
                                 L
                                 −
                                 1
                              
                           
                        
                     

Here, a new local textural feature called Difference Average (DA) which operates on the NGLCM is proposed. DA is expressed as


                        
                           
                              (10)
                              
                                 DA
                                 =
                                 
                                    ∑
                                    
                                       i
                                       =
                                       0
                                    
                                    
                                       L
                                       −
                                       1
                                    
                                 
                                 
                                    iP
                                    
                                       x
                                       −
                                       y
                                    
                                 
                                 (
                                 i
                                 )
                                 .
                              
                           
                        It is evident that DA is the mean of P
                        
                           x−y
                        (k). It expresses the mean value of the pixel differences throughout the entire NGLCM considering the pixel difference a random variable. It was envisioned that this feature would give an idea about the expected pixel difference value of the NGLCM. This information can be valuable in the context of texture classification and its applications such as computer-aided diagnosis. Experimental results show that the proposed feature exhibits discriminating values from bleeding frames to non-bleeding frames which perspicuously evinces that DA can be used in texture classification and other similar applications where the rest of the GLCM based texture features are used.

Here we explicate how we choose effective features to perform the classification in the proposed framework.

At this point we are faced with two questions. Firstly, how do we choose a set of features from the fourteen Haralick features that have the best discriminatory capability? Secondly, how do we make sure that the discriminatory capability of the selected set of features is statistically significant? Statistical hypothesis testing is the solution to both of the above problems.

Although most of the previous papers demonstrate good levels of accuracy, they omit the feature selection stage. In other words, they do not provide any statistical background of their feature generation stage. A method that involves a set of selected features without testing for statistical significance can have lethal repercussions. Firstly, it does not say whether the method is actually robust and invariant to data-set. Secondly, it remains unknown whether the discriminatory capability of the features are statistically significant or not. Therefore, statistical hypothesis testing in the context of any signal classification problem is of paramount importance to find out whether the extracted features are informative enough.

To assess whether the values of the features in the two classes differ significantly, we perform a one-way analysis of variance (ANOVA). The test is carried out in MATLAB's Statistics Toolbox at 95% confidence level. Hence, a difference is statistically significant if p
                        <
                        α(=.05). Any feature having p-value greater than α was discarded. In this way, four out of the fourteen Haralick features are chosen. Table 1
                         gives the mean, standard deviation (SD) and p-values of the features for bleeding and non-bleeding frames in R, G and B channels. The global feature En, the four Haralick features and our proposed feature pass the test as we can see from Table 1. Mean and standard deviation give a rough idea about the feature values of the population. But they do not tell us much about the separability of the descriptors between the two classes. The experimental validation of the efficacy of the selected features will be provided in the experimental results section.

Support vector machine (SVM) has gained popularity in the last decade due to its widespread application in handwritten digit recognition [30]. SVM [31] is a supervised machine learning algorithm that maps the data into a higher dimensional feature space by finding a hyperplane with a maximal margin. The reason for finding a maximum margin hyperplane is that for a binary classification problem, the data in both classes will have more room on each side of the hyperplane. Thus the chance of misclassification is minimized. For a set of N labeled training instances Tr
                     ={(x
                     
                        i
                     , y
                     
                        i
                     )|i
                     =1, 2, ...., N}, where 
                        
                           x
                           i
                        
                        ϵ
                        
                           
                              
                                 ℝ
                              
                           
                           n
                        
                      ad y
                     
                        i
                     
                     ϵ{−1, 1} an unknown test instance is classified by:
                        
                           (11)
                           
                              f
                              (
                              x
                              )
                              =
                              sign
                              
                              
                              
                                 
                                    
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          N
                                       
                                       
                                          α
                                          i
                                       
                                       
                                          y
                                          i
                                       
                                       K
                                       (
                                       
                                          x
                                          i
                                       
                                       ,
                                       x
                                       )
                                       +
                                       w
                                    
                                 
                              
                           
                        
                     where K(x
                     
                        i
                     , x) is the kernel function, 
                        w
                      is the bias and α
                     
                        i
                      is the Lagrange multiplier.

Some two-class classification problems do not have a simple hyperplane as a useful separating criterion. So besides using linear SVM, we experimented using polynomial and Radial Basis Function (RBF) kernels. The polynomial kernel function K(x, y) can be expressed as:
                        
                           (12)
                           
                              K
                              (
                              x
                              ,
                              y
                              )
                              =
                              
                                 
                                    (
                                    x
                                    ·
                                    y
                                    +
                                    1
                                    )
                                 
                                 d
                              
                              
                              d
                              >
                              0
                           
                        
                     and the radial basis kernel is:
                        
                           (13)
                           
                              K
                              (
                              x
                              ,
                              y
                              )
                              =
                              exp
                              (
                              −
                              γ
                              ∥
                              x
                              −
                              y
                              
                                 ∥
                                 2
                              
                              )
                              
                              γ
                              >
                              0
                           
                        
                     where ∥.∥ is the Euclidian L2-Norm and γ governs the spread of K.

Experimentations are carried out to measure the effectiveness of the proposed algorithm empirically. This section provides the details of our experiments. We have evaluated our algorithm against four published algorithms. The results along with rigorous analyses and discussions are presented in this section.

More than 3500 WCE images were extracted from 16 bleeding and 16 non-bleeding videos taken from 32 patients. The images were already labeled by experienced clinicians. Only frames that were identical were removed to avoid undesired repetition of images. Non-informative frames contaminated by residual food, turbid fluids, bubbles, specular reflections or fecal materials were not removed. It was done due to the fact that the experimental set-up must emulate real world settings. In real world applications, it is highly unlikely that a clean set of bleeding and non-bleeding images will be available to the clinicians. Therefore, a successful and pragmatic algorithm must be capable of dealing with these frames. So except for the obvious repetitive frames, all the extracted frames from the 32 videos were used to construct the data-set. The training set consisted of 600 bleeding and 600 non-bleeding frames taken from the 12 different patients (i.e., 6 bleeding and 6 non-bleeding patients). This set was used to train the SVM classifier. On the other hand, the test set consisted of 860 bleeding and 860 non-bleeding frames taken from rest of the patients (i.e., 10 bleeding and 10 non-bleeding patients). Therefore, our training and test data do not have images from the same patient. This set was used to evaluate SVM's classification performance. Since the size of our data-set was large, we used a publicly available SVM software package called LIBSVM [32]. To eliminate the effect of the peripheral dark regions as we can see in Fig. 2, the original PillCam SB2 images of 576×576 were resized to 426×426.

The objective measures used to evaluate the performance of the proposed method are accuracy, sensitivity and specificity. These measures are often used to determine the performance of algorithms in the literature [6,11,12]. They can be expressed by the following formulae:
                           
                              (14)
                              
                                 Accuracy
                                 =
                                 
                                    
                                       
                                          T
                                          P
                                       
                                       +
                                       
                                          T
                                          N
                                       
                                    
                                    
                                       
                                          T
                                          P
                                       
                                       +
                                       
                                          F
                                          P
                                       
                                       +
                                       
                                          T
                                          N
                                       
                                       +
                                       
                                          F
                                          N
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (15)
                              
                                 Sensitivity
                                 =
                                 
                                    
                                       
                                          T
                                          P
                                       
                                    
                                    
                                       
                                          T
                                          P
                                       
                                       +
                                       
                                          F
                                          N
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (16)
                              
                                 Specificity
                                 =
                                 
                                    
                                       
                                          T
                                          N
                                       
                                    
                                    
                                       
                                          T
                                          N
                                       
                                       +
                                       
                                          F
                                          P
                                       
                                    
                                 
                              
                           
                        where T
                        
                           P
                         is the number of bleeding frames identified correctly, T
                        
                           N
                         is the number of non-bleeding frames classified correctly, F
                        
                           P
                         is the number of non-bleeding images identified incorrectly as bleeding and F
                        
                           N
                         is the number of bleeding frames misclassified as non-bleeding.

Higher values of sensitivity indicate that the algorithm's capability of detecting the bleeding images is high. If the sensitivity is low, the algorithm is likely to miss many of the bleeding frames – the consequence of which may be colossal for the patient. On the other hand, high specificity means the algorithm is successfully detecting non-bleeding frames reducing the number of false alarms. So for GI hemorrhage detection, sensitivity is more significant than specificity. In general, we expect that a CAD algorithm will demonstrate high values of accuracy, sensitivity and specificity.

Urgent clinical cases of patients may demand quick detection. A computationally expensive algorithm may fail to meet up the demand of the situation. Therefore, besides having high accuracy, a practically feasible algorithm must be fast. So another measure of performance of any GI bleeding detection algorithm is its time cost. Since the training is done off-line and does not require the clinician, the classifier training time can be ignored. The time spent by the classifier to classify should only be taken into consideration.


                        Fig. 4
                         presents the scatter diagram of the selected features taking two at a time. The figure clearly shows that the proposed features are separable. So there is a prospect of good classification performance of a linear classifier which as we shall see later, turns out to be true. Besides Fig. 4 correlates with the p-values presented in Table 1 and gives an experimental validation of our feature selection stage.

We conducted more experiments to find out the effectiveness of the selected six features in terms of the standard measures – accuracy, sensitivity and specificity. Table 2
                         further elucidates that the proposed set of features is a discriminatory one showing high values of accuracy, sensitivity and specificity for features taken only one at a time. The high values of the three measures prove that our feature set indeed efficaciously captures the textural information we set out to exploit. The results of Table 2 also reflect the findings of the scatter plots in Fig. 4 and speak for the discriminatory capability of the six selected features.


                        Table 3
                         presents the performance of the proposed method for different kernel functions. In our experiments, the order of polynomial kernel d has been set to 3 and γ of RBF kernel has been set to 10. Linear SVM exhibits 99.19% accuracy. This result corroborates with our assumption mentioned before that a linear classifier will work better for this particular choice of feature descriptors. For further simulations, we use linear SVM classifier.

As it was stated earlier in Section 2, the choice of P affects the overall performance of the algorithm. Then what is the best choice of P to achieve the highest possible accuracy? Further researches were conducted to answer this question. Table 4
                         gives an indication to how performance varies with P. Here, 0° denotes one pixel to the right, 90° denotes one pixel above and 45° denotes the first pixel from the center pixel in the direction of the line bisecting 0° and 90°. 135° refers to the pixel that lies in an angle 45° with the 90° direction. So a pixel and the pixel immediately to its right is found to be the best choice of P that should be employed in the proposed GI hemorrhage detection algorithm.

We denote the superpixel based method [7] by SP, the Chrominance Moment and ULBP based method [9] by CM
                        _
                        LBP, Raw, Histogram and Ratio feature based method [6] by RHR and the Probabilistic Neural Network based method [5] by PNN. The method proposed in this article is denoted by DFT
                        _
                        NGLCM. Fig. 5
                         shows the accuracy, sensitivity and specificity of the proposed technique against SP, CM
                        _
                        LBP, PNN and RHR. Here, DFT
                        _
                        NGLCM
                        −
                        R means that only R-channel was used, DFT
                        _
                        NGLCM
                        −
                        G implies that only G-channel was used and DFT
                        _
                        NGLCM
                        −
                        B means that only B-channel was used to perform classification. While implementing the methods in performance comparison, the parameters were chosen such that a particular implementation produces the best accuracy. For CM
                        _
                        LBP the simulations were done using 5, 10, 15, 20, 25 and 30 neurons. We utilized a publicly available LBP package [33] to simulate the ULBP part of this work. The result that demonstrated the highest accuracy were picked. RHR was implemented by downsampling the images by a factor of k (k
                        =3, 9, 17, 21, 25 and 29) as was done in the original paper. Here the best accuracy, sensitivity and specificity of all the simulations are reported. All the algorithms were implemented using MATLAB 2013a on a computer with Intel(R) Core(TM) i5-3470, 3.2 GHz CPU, 4 GB of RAM. The aforementioned data-set was used for all the experiments for meaningful comparison. Fig. 5 presents the comparison results. Despite its simplicity, the DFT based texture descriptor based classification algorithm emerged as the most successful one in terms of all the three standard measures as we can see in Fig. 5.


                        Table 5
                         presents the comparison of speed of the proposed technique against SP, CM
                        _
                        LBP, PNN and RHR. Although RHR is slightly faster than the proposed method as we can see in Table 5, the former has only 73.72% accuracy, 75.35% sensitivity, 72.09% specificity. Due to lack of high values of both accuracy and time, RHR is unsuitable for practical application. In addition, the low computational cost of the proposed scheme is also promising for real-time hemorrhage detection from WCE videos.

@&#DISCUSSION@&#

In this section we provide analysis of the results of our experimentations. Although it was hinted in the Section 1.3of Section 1, here we attempt to add a few more details to the following question – what previously unresolved issues were the proposed algorithm able to overcome?

A few factors contributed to the high values of accuracy, sensitivity and specificity, lower value of execution time and made this work an important step toward fully automated GI hemorrhage detection. Firstly, for small bleeding regions the proposed method works better than whole image or patch based methods. Fig. 6
                        (a)–(c) shows three frames containing small bleeding regions. These tiny bleeding regions span a very small number of selected patches. Therefore, they do not contribute much to the feature values, risking the chance of misclassification in a patch based approach. Again, whole image based methods rely on various statistical features of the entire image. A few bleeding pixels hardly affect or alter values of whole image based statistical features resulting missing detection and hence low sensitivity. In the frequency domain, as seen from Fig. 6(d)–(f), these bleeding frames are marked by the appearance of non-horizontal and non-vertical lines, making it easier and more suitable to be captured by the textural features. As a result, our method was able to solve the small bleeding region problem by taking the feature extraction process altogether in the frequency domain and considering features from the whole magnitude spectrum. This also accounts for the high accuracy of the proposed scheme. Secondly, the proposed method was also able to overcome the limitation of [7] of detecting non-bleeding frames that contain fluid, intestinal villi and cavity making the hue of the image dark red. This is illustrated in Fig. 7
                        . The non-bleeding WCE frame in Fig. 7(a) often gets misclassified as bleeding due to its dark red region created by intestinal villi. However, as the intensity varies slowly from dark red to bright region, unlike Fig. 6 diagonal lines are not generated in the ±45° directions of the magnitude spectrum and the image is not misclassified as bleeding. Thirdly, the efficacy of our method lies partially in the choice of a discriminatory set of features too as we can see from the p-values in Table 1. The scatter diagrams of Fig. 4 and results of Table 2 also reveal that the performance of the individual features are quite well. In fact, this actually gives rise to the question – if the performance of the features considered individually is good, why are we using all the six features in conjunction, instead of using only one? The reason is that using only one feature will make the algorithm less robust. In other words, there will be greater variations of algorithmic performance among various data-sets, rendering the detection scheme less reliable. Furthermore, an algorithm reliant on a feature alone is likely to fail to capture the differences of bleeding and non-bleeding magnitude spectrums. Since we have employed six features to capture the differences between bleeding and non-bleeding magnitude spectrums, it is very likely that one or more feature will always be able to capture the difference mathematically irrespective of the data-set. Besides, a detection algorithm based on only one feature is rather ambitious and cannot ensure that it will work in real-world settings.

We now explain the benefit and justification of including our proposed feature, namely the difference average (DA). The DA is proposed with a view to capturing textural information from NGLCM. It signifies the mean value of the intensity differences of the NGLCM. It is also related to Difference Variance (DV). DV measures the dispersion of the elements of NGLCM with respect to DA. If the values of the elements of NGLCM exhibit greater variability with respect to one another, the value of DA will be higher. The opposite scenario will cause DA to become lower. Fig. 8
                         illustrates the box-whisker plot of DA on the test dataset. The non-overlapping notches (marked in red) of the two box-whisker plots manifest the difference of DA values between the two classes. We have also calculated the mean and SD values of DA for the test dataset as presented in Table 6
                        . It is seen that the mean and SD values of DA on the test dataset for the two classes are significantly different. Therefore, DA is an efficacious feature that extracts important classification related information from NGLCM. DA can also be implemented for other image analysis and classification problems where NGLCM is employed.

We now discuss some misclassification cases of the proposed algorithm. Fig. 9
                         shows two cases of false and missing detections of our algorithm. It is found that images containing bubbles and specular reflection exhibit similar patterns as bleeding in the frequency domain. As a result, these frames can also get classified as bleeding. Again, the proposed scheme fails to detect bleeding frames if the bleeding region is completely engulfed by intestinal villi or cavity. However, compared to the thousands of images that have to be screened in one examination, these frames are too small in number to drastically affect classification accuracy.

In this section, we identify some directions of future research and the possible areas that this work can be extended. Firstly, this algorithm's computational cost can further be reduced by implementing it on CUDA based Graphical Processing Unit (GPU). Secondly, the proposed approach can be implemented to solve other image classification problems such as medical image classification, image retrieval, remote sensing etc. Thirdly, the proposed method can be extended for other CAD problems of WCE videos such as Crohn's disease detection, ulcer detection, tumor detection etc. as well. Fourthly, future studies can also explore to combine multiple classifiers to enhance the algorithm's performance. Classifier boosting can be adopted for better classification performance as well. Fifthly, how the algorithm behaves for different choices of the position operator P may also be an interesting topic of further research.

@&#CONCLUSION@&#

In this work, the problem of computer-aided GI bleeding detection was addressed by selecting feature descriptors from the NGLCM of the images in the frequency domain. The accuracy of the scheme is promising. The proposed algorithm was evaluated against previously published works. The results of performance comparison were also significant. The superiority of the algorithm was also confirmed by statistical hypothesis testing and graphical analysis. We can expect that the proposed method will be ideal for practical implementation since it does not require any sort of human intervention like selecting informative patches and therefore making WCE technology less problematic and convenient for both patients and clinicians. We thus come into a conclusion, as the experimental results suggest, the devised scheme is simple, yet effective and efficient.

@&#ACKNOWLEDGMENT@&#

The authors would like to thank Given Imagine Ltd. for generously providing the WCE data (www.capsuleendoscopy.org).

@&#REFERENCES@&#

