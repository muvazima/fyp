@&#MAIN-TITLE@&#Local configuration pattern features for age-related macular degeneration characterization and classification

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           Automated detection of age-related macular degeneration (AMD) using fundus images.


                        
                        
                           
                           Features are extracted using local configuration pattern (LCP) method.


                        
                        
                           
                           Ranked features are subjected to various classifiers.


                        
                        
                           
                           Proposed method classifies two classes with 97.80% accuracy


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Retina

Age-related macular degeneration

Fundus imaging

Local configuration pattern

Support vector machine

@&#ABSTRACT@&#


               
               
                  Age-related Macular Degeneration (AMD) is an irreversible and chronic medical condition characterized by drusen, Choroidal Neovascularization (CNV) and Geographic Atrophy (GA). AMD is one of the major causes of visual loss among elderly people. It is caused by the degeneration of cells in the macula which is responsible for central vision. AMD can be dry or wet type, however dry AMD is most common. It is classified into early, intermediate and late AMD. The early detection and treatment may help one to stop the progression of the disease. Automated AMD diagnosis may reduce the screening time of the clinicians. In this work, we have introduced LCP to characterize normal and AMD classes using fundus images. Linear Configuration Coefficients (CC) and Pattern Occurrence (PO) features are extracted from fundus images. These extracted features are ranked using p-value of the t-test and fed to various supervised classifiers viz. Decision Tree (DT), Nearest Neighbour (k-NN), Naive Bayes (NB), Probabilistic Neural Network (PNN) and Support Vector Machine (SVM) to classify normal and AMD classes. The performance of the system is evaluated using both private (Kasturba Medical Hospital, Manipal, India) and public domain datasets viz. Automated Retinal Image Analysis (ARIA) and STructured Analysis of the Retina (STARE) using ten-fold cross validation. The proposed approach yielded best performance with a highest average accuracy of 97.78%, sensitivity of 98.00% and specificity of 97.50% for STARE dataset using 22 significant features. Hence, this system can be used as an aiding tool to the clinicians during mass eye screening programs to diagnose AMD.
               
            

@&#INTRODUCTION@&#

AMD is a multi-factorial ocular disease caused by deterioration of cells in the macula (See Fig. 1
                     b) [1]. It is one of the leading causes of central vision loss [2] in people aged over 50 years [3]. AMD is characterized by drusen, retinal pigmentation, CNV and atrophy of photoreceptors [4]. It has several risk factors viz. age, smoking, hypertension and family history [5–7]. Recent World Health Organization (WHO) report reveals that 8 million people are affected with severe blindness due to AMD [8]. Globally, United Nations (UN) estimates that 20–25 million people are having AMD [9] and this figure may increase to 196 million in 2020 and 288 million in 2040 [10]. According to the presence of clinical features, AMD is mainly classified into three stages viz. early, intermediate and late 
                     [1]. They are briefly explained below.
                        
                           (i)
                           
                              Early AMD is characterized by the presence of drusen with a size of 
                                 ≥
                                 15
                                 
                                 μ
                                 m
                                 
                                 and
                                 <
                                 63
                                 
                                 μ
                                 m
                               in diameter. Also, it has abnormal lesions viz. hyperpigmentations or hypopigmentations [1,11].


                              Intermediate AMD is graded due to the presence of medium sized i.e., 
                                 ≥
                                 63
                                 
                                 μ
                                 m
                                 
                                 and
                                 <
                                 125
                                 
                                 μ
                                 m
                               drusen and pigment abnormalities [1,4]. The lesion is present around the macula (not in the centre) [1,11].


                              Late AMD is characterized with GA and CNV. It is categorized into two types viz. dry and wet 
                              [1] which are described below:
                                 
                                    (a)
                                    
                                       Dry AMD or non-neovascular AMD is characterized by the presence of drusen and GA in the centre of the macula (See Fig. 1b) [1,11]. Almost 90% of the vision loss is caused due to dry AMD [2]. There is no specific treatment method that may reduce the progression of late AMD [12].


                                       Wet AMD is also known as neovascular AMD characterized by CNV [5]. Eyes affected with CNV may leak blood (See Fig. 1c) and the leakage is expressed as classic or occult [5]. The leakage can be diagnosed using fluorescein angiography and indocyanine green dyes [5]. Wet AMD is found in 10% of total macular degeneration cases [2]. Neovascular AMD can be treated using thermal laser photo-coagulation [5].

AMD can be diagnosed by identifying drusen from the retinal fundus images [13]. Automatic segmentation of drusen and its measurement is needed to automate the diagnostic process [14]. Hence, several authors have proposed AMD detection using automated drusen segmentation [15–27].

Brandon et al. [15] proposed multi-level analysis to isolate drusen from the retinal images. Their method obtained correct detection rate of 87.00%. But this method is susceptible to Optic Disc (OD) and blood vessel variations. Sbeh et al. [16] used mathematical morphology to segment bright spots. Shape, contrast and area criterion are used to segregate the drusen from these spots. However, performance measures of their method are not reported. Adaptive Local Thresholding (HALT) operator is used in [17] to segment drusen with vague boundary as well as small drusen and reported a sensitivity of 98.00%. Gaussian derivative filters and k-NN classifier is used in [18] to identify drusen and obtained a sensitivity and a specificity of 77.00% and 88.00% respectively. Their method is not effective to identify all drusen spots. Soliz et al. [19] used Independent Component Analysis (ICA) to detect drusen phenotypes and their method obtained an accuracy of 100% using only 12 fundus images. Amplitude Modulation (AM)–Frequency Modulation (FM) based multi-scale features is used in [20] to identify drusen and reported an Area Under receiver operator characteristics Curve (AUC) of 1. Their method is able to characterize slow varying intensities in the fundus images. Mexican hat wavelet and Support Vector Data Description (SVDD) are used to detect drusen from normal and AMD images. Their method is tested using seven images and reported 100% accuracy [21]. Liang et al. [22] proposed intensity based drusen segmentation approach and reported a sensitivity and specificity of 75.00%. Removal of blood vessel and location of macula are needed in their approach. Multi-resolution locally-adaptive segmentation method is proposed to segment drusen spots. Their method obtained a sensitivity of 95.00% and a specificity of 96.00% by selecting threshold manually [23]. Statistical modelling based drusen segmentation approach is used in [24] and reported an Area Under receiver operator characteristics Curve (AUC) of 0.99 and false positives are removed using post processing. Sobel operator and Gaussian function are used in [25] to segment drusen. Their method obtained a kappa agreement of 0.60. This thresholding method produced a large number of false positives. Optimal filter bank is developed in [26] to differentiate drusen and flecks. Their method obtained an AUC of 0.85. Cheng et al. [27] proposed BIF to detect drusen in the fundus images. Their method used Gabor functions to develop BIF and reported a sensitivity and a specificity of 86.30% and 91.90% respectively. The reported works in the literatures perform only drusen segmentation. They did not extend their method to detect AMD, except in [17,26]. Authors [17,26] segmented the drusen first and then detected the AMD class.

Further, inverse anomaly segmentation is proposed in [28] to identify AMD lesions and reported a segmentation accuracy of 90.00%. Case Based Reasoning (CBR) and Dynamic TimeWarping (DTW) methods are proposed in [13] to discriminate normal and AMD classes with a sensitivity of 86.00%. Inverse segmentation using statistical texture features is used in [29] to identify healthy and unhealthy areas from AMD images and reported an accuracy of 92.76% and 96–100% respectively. Spatial histogram and hierarchical image decomposition methods are developed to classify normal and AMD classes [11]. Their techniques obtained an accuracy of 74.00% and 100% using spatial histogram and hierarchical decomposition [11] respectively. Wavelet, GLCM, color, histogram based features and Weighted Frequent Sub-Graph Mining (WFSM) are used in [30] to classify normal and AMD classes. Their method achieved an accuracy of 99.90%. Zheng et al. [31] used tree based hierarchical decomposition and WFSM for automated detection of AMD and reported an accuracy of 99.60%. AM-FM based decomposition and statistical moments and histogram percentiles are used as features in [32] to discriminate Diabetic Retinopathy (DR) and AMD classes. Their method reported an AUC of 0.84 and 0.77 using RIST and UTHSCSA respectively. Texture and DWT based features are used in [14,33] to discriminate normal and AMD classes. Their method yielded an accuracy of 93.70% and 95.07% respectively.

Important limitations of the aforementioned methods except [14,33] requires removal of OD, blood vessel and macula detection, before feature extraction. The difference between the reported works in the literature and proposed work in this paper is (a) anomaly and normal anatomy of fundus images such as drusen, OD and blood vessel segmentation are not needed, and (b) this work uses a less number of features to obtain highest performance measures compare to all reported works [11,14,30,33].

The proposed work (See Fig. 2
                     ) starts with preprocessing using Contrast Limited Adaptive Histogram Equalization CLAHE [34] to enhance the image contrast. Then LCP [35] based features are extracted from normal and AMD classes. Further, the statistical significance of the extracted features are evaluated using t-test. Finally, the selected significant features are ranked and fed to the DT, k-NN, NB, PNN and SVM classifier with various kernel functions. The performance of the classifiers is evaluated using 10-fold cross validation strategy.

Dataset description, preprocessing, feature extraction, ranking and selection are explained in Section 2. Various supervised classifiers are discussed briefly in Section 3. Obtained results are reported and presented in Section 4. The advantages and limitations of the proposed method are discussed in Section 5. Finally, the paper concludes in Section 6.

Three datasets viz. (i) Private, (ii) ARIA and (iii) STARE are used to evaluate the proposed method. The details of these dataset are briefly described below.


                        
                           Private
                           
                           dataset
                        : The normal (n=270) and AMD (n=270) fundus images are collected from Ophthalmology Department, Kasturba Medical College, Manipal, India . The images are acquired by the clinicians using Zeiss FF450 plus mydriatic retinal camera with a 50
                           °
                         field of view. Acquired images are stored with 480×364 pixels resolution and a bit rate of 24. Patient׳s consent was taken to use the images for research purpose. Experienced clinicians reviewed and graded the images (See Fig. 3
                        ) into normal, Early AMD, Intermediate AMD and Late AMD as per AREDS [36] classification.


                        
                           ARIA
                           
                           dataset
                        : These images are acquired by the clinicians of St Pauls Eye Unit and University of Liverpool, UK using Zeiss FF450+ fundus camera with a 50
                           °
                         field of view. Acquired images are stored with 768×576 pixels resolution and a bit rate of 24. ARIA has 101 normal and 60 AMD images. These images are available in the following link: http://www.eyecharity.com/aria_online.


                        
                           STARE
                           
                           dataset
                        : This dataset has 36 normal and 47 AMD images which are collected from the Shiley Eye Centre at the University of California and the Veterans Administration Medical Centre, USA using TOPCON fundus camera with a 35
                           °
                         field of view. STARE images has a resolution of 700×605 pixels and a bit rate of 24. These images can be accessible in the following link: http://www.ces.clemson.edu/~ahoover/stare.

The collected fundus images from three datasets viz. private, ARIA and STARE are subjected to preprocessing to enhance the contrast using CLAHE [34]. It is applied on the green channel of the Red Green Blue (RGB) color fundus image. CLAHE splits the images into different blocks and applies adaptive histogram equalization. Hence, this method stretches color distribution of an image using the most frequent intensity values [34,37], and yields a contrast improved image. In CLAHE two key parameters viz. block size and clip limit are used to tune the contrast of an image [38,39]. However, larger block size and high clip limit increase the noise in the fundus image [38,39]. Hence, in this work we have used a block size of 8 
                           ×
                         8 and a clip limit of 0.01 to enhance the image contrast [38]. Thus, visible anomalies viz. drusen, CNV and normal structures viz. Optic Disc (OD), blood vessels and macula are enhanced [40].

Fundus image of normal and AMD exhibit texture properties due to the presence of drusen, CNV and normal structures. Several methods are proposed to extract texture features from fundus images to detect AMD [14,30], glaucoma [41], Diabetic Macular Edema (DME) [42] and DR [43]. Among various texture measures local texture descriptor viz. LBP is widely used to extract features from medical images, since it has low computational complexity, invariance to rotation and illumination changes [44]. It compares the pixel gray level with its neighbouring pixels and forms a binary pattern [45]. Its limitation is that estimates the intensity information by taking average and variance of neighbouring pixels [35]. Hence, Guo et al. [35] proposed LCP that combines local structural and microscopic configuration information. Local information is extracted using LBP, microscopic configuration provides image configuration and pixel-wise interaction which is named as Microscopic Configuration Modelling (MiC). Block diagram of the LCP feature extraction is shown in Fig. 4
                        .

LBP computed using circular symmetric operator (See Fig. 5
                           ) which compares centre pixels with its neighbouring pixels using the following equation:
                              
                                 (1)
                                 
                                    
                                       
                                          LBP
                                       
                                       
                                          P
                                          ,
                                          R
                                       
                                    
                                    =
                                    
                                       
                                          ∑
                                       
                                       
                                          p
                                          =
                                          0
                                       
                                       
                                          P
                                          −
                                          1
                                       
                                    
                                    s
                                    (
                                    
                                       
                                          
                                             g
                                          
                                          
                                             p
                                          
                                       
                                       −
                                       
                                          
                                             g
                                          
                                          
                                             c
                                          
                                       
                                    
                                    )
                                 
                              
                           where 
                              
                                 
                                    s
                                    (
                                    x
                                    )
                                    =
                                    {
                                    
                                       
                                       
                                          
                                             
                                                1
                                                ,
                                             
                                             
                                                x
                                                ≥
                                                0
                                             
                                          
                                          
                                             
                                                0
                                                ,
                                             
                                             
                                                x
                                                <
                                                0
                                             
                                          
                                       
                                    
                                 
                              
                           where P and R denote the number of neighbouring pixels and radius of the neighbourhood, respectively (See Fig. 5), p represents neighbouring pixel intensity values, g
                           
                              c
                            is the centre pixel intensity values and s(x) denotes a step function.

Rotation invariant LBP (
                              
                                 
                                    LBP
                                 
                                 
                                    P
                                    ,
                                    R
                                 
                                 
                                    rin
                                    2
                                 
                              
                           ) is obtained with uniformity measure U and it can be defined as
                              
                                 (2)
                                 
                                    
                                       
                                          LBP
                                       
                                       
                                          P
                                          ,
                                          R
                                       
                                       
                                          rin
                                          2
                                       
                                    
                                    =
                                    {
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      P
                                                      =
                                                      0
                                                   
                                                   
                                                      P
                                                      −
                                                      1
                                                   
                                                
                                                s
                                                (
                                                
                                                   
                                                      
                                                         g
                                                      
                                                      
                                                         p
                                                      
                                                   
                                                   −
                                                   
                                                      
                                                         g
                                                      
                                                      
                                                         c
                                                      
                                                   
                                                
                                                )
                                                ,
                                             
                                             
                                                U
                                                (
                                                
                                                   
                                                      
                                                         LBP
                                                      
                                                      
                                                         P
                                                         ,
                                                         R
                                                      
                                                   
                                                   ≤
                                                   2
                                                
                                                )
                                             
                                          
                                          
                                             
                                                P
                                                +
                                                1
                                             
                                             
                                                Otherwise
                                             
                                          
                                       
                                    
                                 
                              
                           In this work LBP is computed using various scales (R=2, 3 and 4) and pixel counts (P=8, 10 and 12) [35].

MiC is designed to extract microscopic features from fundus images which reflects the texture patterns. Image configuration is modelled by estimating the optimal weights of the neighbouring pixel intensities which helps one to reconstruct central pixel intensity linearly [35]. It is defined by
                              
                                 (3)
                                 
                                    E
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          0
                                       
                                    
                                    ,
                                    …
                                    ,
                                    
                                       
                                          w
                                       
                                       
                                          P
                                          −
                                          1
                                       
                                    
                                    )
                                    =
                                    |
                                    
                                       
                                          
                                             g
                                          
                                          
                                             c
                                          
                                       
                                       −
                                       
                                          
                                             ∑
                                          
                                          
                                             p
                                             =
                                             0
                                          
                                          
                                             P
                                             −
                                             1
                                          
                                       
                                       
                                          
                                             w
                                          
                                          
                                             p
                                          
                                       
                                       
                                          
                                             g
                                          
                                          
                                             p
                                          
                                       
                                    
                                    |
                                 
                              
                           where g
                           
                              p
                            and g
                           
                              c
                            denote the intensity values of neighbouring pixels and centre pixels, respectively, 
                              
                                 
                                    w
                                 
                                 
                                    p
                                 
                              
                              
                              (
                              p
                              =
                              0
                              ,
                              …
                              ,
                              P
                              −
                              1
                              )
                            are weighting parameters associated with neighbouring pixel g
                           
                              p
                            and 
                              E
                              
                              (
                              
                                 
                                    w
                                 
                                 
                                    0
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    w
                                 
                                 
                                    P
                                    −
                                    1
                                 
                              
                              )
                            denotes the reconstruction error [35]. Reduction of reconstruction error and optimal parameter selections are decided by least square estimation [46]. Optimal parameter W
                           
                              L
                            is computed using the following equation:
                              
                                 (4)
                                 
                                    
                                       
                                          W
                                       
                                       
                                          L
                                       
                                    
                                    =
                                    
                                       
                                          (
                                          
                                             
                                                V
                                             
                                             
                                                L
                                             
                                             
                                                T
                                             
                                          
                                          
                                             
                                                V
                                             
                                             
                                                L
                                             
                                          
                                          )
                                       
                                       
                                          −
                                          1
                                       
                                    
                                    
                                       
                                          V
                                       
                                       
                                          L
                                       
                                       
                                          T
                                       
                                    
                                    
                                       
                                          C
                                       
                                       
                                          L
                                       
                                    
                                 
                              
                           where C
                           
                              L
                            is the least square problem and V
                           
                              L
                            is the neighbouring pixel intensities [35].

Rotation invariant features are obtained by applying 1D Fourier transform to the optimal parameter W
                           
                              L
                            and can be written as [35]
                           
                              
                                 (5)
                                 
                                    
                                       
                                          H
                                       
                                       
                                          L
                                       
                                    
                                    (
                                    k
                                    )
                                    =
                                    
                                       
                                          ∑
                                       
                                       
                                          p
                                          =
                                          0
                                       
                                       
                                          P
                                          −
                                          1
                                       
                                    
                                    
                                       
                                          W
                                       
                                       
                                          L
                                       
                                    
                                    (
                                    i
                                    )
                                    ·
                                    
                                       
                                          e
                                       
                                       
                                          −
                                          j
                                          2
                                          π
                                          kp
                                          /
                                          P
                                       
                                    
                                 
                              
                           
                        

Magnitude of H
                           
                              L
                            is considered as MiC feature and is defined as [35]
                           
                              
                                 (6)
                                 
                                    |
                                    
                                       
                                          H
                                       
                                       
                                          L
                                       
                                    
                                    |
                                    =
                                    [
                                    |
                                    
                                       
                                          H
                                       
                                       
                                          L
                                       
                                    
                                    (
                                    0
                                    )
                                    |
                                    ;
                                    |
                                    
                                       
                                          H
                                       
                                       
                                          L
                                       
                                    
                                    (
                                    1
                                    )
                                    |
                                    ;
                                    …
                                    ;
                                    |
                                    
                                       
                                          H
                                       
                                       
                                          L
                                       
                                    
                                    (
                                    P
                                    −
                                    1
                                    )
                                    |
                                    ]
                                 
                              
                           
                        


                           
                              |
                              
                                 
                                    H
                                 
                                 
                                    L
                                 
                              
                              |
                            encodes the pixel-wise interaction relationships and local contrast of each pattern produces discriminative information jointly with pattern occurrences of LBP. Thus, LCP provides both microscopic configuration and local shape information [35]. It can be written as
                              
                                 (7)
                                 
                                    LCP
                                    =
                                    [
                                    [
                                    |
                                    
                                       
                                          H
                                       
                                       
                                          p
                                       
                                    
                                    |
                                    ;
                                    
                                       
                                          O
                                       
                                       
                                          p
                                       
                                    
                                    ]
                                    ;
                                    [
                                    |
                                    
                                       
                                          H
                                       
                                       
                                          1
                                       
                                    
                                    |
                                    ;
                                    
                                       
                                          O
                                       
                                       
                                          1
                                       
                                    
                                    ]
                                    ;
                                    …
                                    ;
                                    [
                                    |
                                    
                                       
                                          H
                                       
                                       
                                          l
                                          −
                                          1
                                       
                                    
                                    |
                                    ;
                                    
                                       
                                          O
                                       
                                       
                                          l
                                          −
                                          1
                                       
                                    
                                    ]
                                    ]
                                 
                              
                           where 
                              |
                              
                                 
                                    H
                                 
                                 
                                    p
                                 
                              
                              |
                            is computed using Eq. (6), O
                           
                              p
                            represents occurrence of the pth local pattern of interest obtained using LBP and l is the total number of patterns of interest [35].

LCP method generates 81, 121 and 169 features for different neighbours P=8, 10 and 12 and radius R=2, 3 and 4 respectively. During LCP computation, use of large neighbours may under-determine the LCP technique during the estimation of reconstruction coefficients [35]. Hence, in this work small neighbours viz. P=8, 10 and 12 and radius R=2, 3 and 4 are chosen to extract LCP features. LCP is implemented using the method available in the following link: http://www.cse.oulu.fi/CMV/Downloads/LBPMatlab.

Feature selection discard the non-significant features to improve the classifier performance [47,48]. Initially, the data is evaluated to test for normal distribution using chi-square goodness-of-fit test [49]. The test results reveal that the data is normally distributed. Hence, in this work we have chosen t-test to evaluate statistical significance [49]. It compares the population means between the groups, if the difference is large it rejects the following null hypothesis:


                        Null hypothesis H
                        0: The data between groups has normal distributions with equal means 
                        [49].


                        Alternative hypothesis H
                        1: The means of the data are not equal between groups [49].

The significant features are ranked using p-value and sequentially fed to the different supervised classifiers to obtain the highest classifier performance.

AMD classification is performed using DT, k-NN, NB, PNN and SVM with RBF, linear, quadratic and polynomial kernel functions.

DT is a predictive model which derives the decision from the training data [50]. It has three nodes viz. root node, internal nodes and leaf nodes. These nodes split the complex decision into several simpler decision to obtain the final result [50,51]. k-NN classifier is also known as memory-based classification, since the training sample are required during run-time [52]. The unknown samples are classified according to the class of their nearest neighbours by calculating the distance between the neighbours [52]. In this work, we have used k=1 to obtain highest performance. NB is a probabilistic classifier and works on the principle of Bayes theorem [53]. It makes the conditional independence assumption to reduce the learning complexity during classifier modelling. Maximum likelihood estimation is used to estimate the classifier parameters [53]. PNN is a feed forward neural network with four layers viz. input, pattern, summation and output [54]. It classifies patterns using Parzen window probability density function estimator. The compete transfer function of summation layer determines the output [54]. In this work, the sigma (σ) value of the summation layer varied from 0.01 to 1 using bootstrap [55] method to obtain highest classification performance [33]. SVM is a statistical learning approach developed by Vapnik [56]. It uses structural risk minimization principle to construct optimal hyperplane and support vectors for best possible discrimination [53,57] of normal and AMD classes. It can be applied for both linearly separable and non-separable data. However, kernel functions viz. quadratic, polynomial and RBF are used to convert non-separable data into separable one in the high dimensional feature space [53,58]. The RBF kernel width (σ) is varied from 0.01 to 5 using bootstrap method [55] to obtain highest classifier performance [33].

@&#RESULTS@&#

The proposed AMD detection system is validated using private, ARIA and STARE datasets. LCP features has two components viz. (i) LCC and (ii) PO which characterizes the presence of drusen, CNV, GA and local shape information [35]. In this work LCP is computed with three LBP neighbourhood configuration viz. (P,R)=
                        {
                        (
                        8
                        ,
                        2
                        )
                        ,
                        (
                        10
                        ,
                        3
                        )
                        ,
                        (
                        12
                        ,
                        4
                        )
                        }
                      
                     [35]. Each configuration viz. (P,R)=
                        {
                        (
                        8
                        ,
                        2
                        )
                        }
                     , (P,R)=
                        {
                        (
                        10
                        ,
                        3
                        )
                        }
                      and (P,R)=
                        {
                        (
                        12
                        ,
                        4
                        )
                        }
                      yielded a different number of LCC coefficients and PO. The configuration 
                        (
                        8
                        ,
                        1
                        )
                      provided 72 LCC and 9 PO, 
                        (
                        10
                        ,
                        3
                        )
                      generated 110 LCC and 11 PO, and finally 
                        (
                        12
                        ,
                        4
                        )
                      obtained 156 LCC and 13 PO. Using these LCP configuration features seven combinations viz. (P,R)=
                        {
                        (
                        8
                        ,
                        2
                        )
                        }
                     , (P,R)=
                        {
                        (
                        10
                        ,
                        3
                        )
                        }
                     , (P,R)=
                        {
                        (
                        12
                        ,
                        4
                        )
                        }
                     , (P,R)=
                        {
                        (
                        8
                        ,
                        2
                        )
                        ,
                        (
                        10
                        ,
                        3
                        )
                        }
                     , (P,R)=
                        {
                        (
                        8
                        ,
                        2
                        )
                        ,
                        (
                        12
                        ,
                        4
                        )
                        }
                     , (P,R)=
                        {
                        (
                        8
                        ,
                        3
                        )
                        ,
                        (
                        12
                        ,
                        4
                        )
                        }
                      and (P,R)=
                        {
                        (
                        8
                        ,
                        2
                        )
                        ,
                        (
                        10
                        ,
                        3
                        )
                        ,
                        (
                        12
                        ,
                        4
                        )
                        }
                      (see Tables 1–4
                     
                     
                     
                     )are formulated. Fig. 6
                      shows the LCC for normal and AMD fundus images of private, ARIA and STARE datasets. It can be seen from the figure (See Fig. 6) that LCC values are lower for normal class and higher for AMD class.

Further, these different combinations of LCP features are subjected to test of Gaussianity using Chi-square goodness-of-fit test [49]. The test result confirmed that features are normally distributed. Hence, in this work t-test is used to find the statistical significance of features [49]. Further, significant LCP features are ranked using 
                        p
                        -value
                        
                        (
                        p
                        ≤
                        0.05
                        )
                      of the t-test, features with lowest p-value obtained rank one and vice-versa. Further, feature with 
                        p
                        -value
                        >
                        0.05
                      is discarded from the list (See Fig. 7
                     ) and remaining features whose 
                        p
                        -value
                        ≤
                        0.05
                      are considered and those features are fed sequentially to various supervised classifiers to discriminate normal and AMD classes. Fig. 7 shows the plot of p-value vs. ranked features for different P and R configurations of private, ARIA and STARE datasets. From the plot we understand that private, ARIA and STARE datasets has 259, 58 and 62 significant ranked features for various (P,R) configurations respectively.

Further, these ranked significant features are sequentially fed to supervised classifiers viz. DT, k-NN, NB, PNN and SVM with different kernel functions for classifications. The performance measures viz. sensitivity, specificity and accuracy are computed using 10-fold cross validation [59] technique. Initially dataset is divided into 10 subsets, where nine subsets are used to form a training set and one subset is used as testing set. This procedure is repeated ten times and the average classification performance is tabulated. Tables 1–3 show the average accuracies over 10-fold for private, ARIA and STARE datasets with various P and R neighbourhood configuration respectively. Table 1 shows that among various classifiers, SVM with linear kernel obtained a highest average accuracy of 93.52% using 104 ranked significant LCP features for private dataset (See Fig. 7a) with (P,R)=
                        {
                        (
                        8
                        ,
                        2
                        )
                        ,
                        (
                        10
                        ,
                        3
                        )
                        ,
                        (
                        12
                        ,
                        4
                        )
                        }
                     . Similarly, Table 2 reveals that SVM with RBF kernel yielded a highest average accuracy of 91.36% with 35 significant features (See Fig. 7b) for ARIA dataset using (P,R)=
                        {
                        (
                        10
                        ,
                        3
                        )
                        }
                      configuration. Finally, STARE obtained a maximum average accuracy of 97.78% using 22 significant features (See Fig. 7c) with (P,R)=
                        {
                        (
                        12
                        ,
                        4
                        )
                        }
                     .

Moreover, SVM with quadratic kernel obtained an highest classification accuracy of 91.67% (See Table 1) using 23 significant features with (P,R)=
                        {
                        (
                        10
                        ,
                        3
                        )
                        ,
                        (
                        12
                        ,
                        4
                        )
                        }
                      neighbourhood for private data which is very close to SVM-Linear performance (See Table 1) using less number (n=23) of features. Similar results are obtained using SVM-Quadratic kernel for ARIA (accuracy=90.63%) and STARE (accuracy=95.28%) datasets with nine and 45 features using (P,R)=
                        {
                        (
                        8
                        ,
                        2
                        )
                        ,
                        (
                        10
                        ,
                        3
                        )
                        ,
                        (
                        12
                        ,
                        4
                        )
                        }
                      neighbourhood configuration respectively. Other than SVM, the classifiers viz. PNN and DT yielded a highest average accuracy of 88.70% (164 features) (See Table 1) for private dataset and 88.79% (27 features) (See Table 2) and 91.67% (2 features) (See Table 3) for ARIA and STARE datasets respectively.

The best performance of the AMD detection system (See Fig. 8
                     ) and time taken for training and testing (10-fold) are reported in Table 4. It shows that SVM classifier with linear and RBF kernels outperformed among other kernels (See Tables 1 and 3). In particular, SVM-Linear kernel yielded a highest average classification accuracy of 97.78%, sensitivity of 98.00% and specificity of 97.50% using 22 features for STARE data with (P,R)=
                        {
                        (
                        12
                        ,
                        4
                        )
                        }
                      (See Fig. 8c). However, private and ARIA dataset use 104 (See Fig. 8a) and 35 (See Fig. 8b) significant LCP features to obtain a maximum average accuracy of 93.52% and 91.36% using SVM-linear and RBF kernels respectively.

These results (see Table 4) demonstrate that SVM-linear obtained highest performance for private and STARE dataset and SVM-RBF yielded maximum performance for ARIA dataset. Hence, to propose a final model for AMD detection we performed majority voting [60] using SVM-linear, SVM-quadratic and SVM-RBF classifiers. The results of SVM-MV shown in Table 1 (private), Table 2 (ARIA) and Table 3 (STARE) revealed that SVM-MV and (P,R)=
                        {
                        (
                        8
                        ,
                        2
                        )
                        ,
                        (
                        10
                        ,
                        3
                        )
                        ,
                        (
                        12
                        ,
                        4
                        )
                        }
                      configuration obtained a highest accuracy of 93.15% and 91.93% using 81 and 34 LCP features for private and ARIA dataset, respectively. However, SVM-MV with (P,R)=
                        {
                        (
                        8
                        ,
                        2
                        )
                        ,
                        (
                        10
                        ,
                        3
                        )
                        }
                      configuration yielded an optimum accuracies of 90.68% and 97.59% (see Table 5
                     ) using a less number of LCP features (5 and 19) for both public datasets viz. ARIA and STARE respectively. The accuracy difference is +1.25% (ARIA) and −1.2% (STARE) using (P,R)=
                        {
                        (
                        8
                        ,
                        2
                        )
                        ,
                        (
                        10
                        ,
                        3
                        )
                        ,
                        (
                        12
                        ,
                        4
                        )
                        }
                      and (P,R)=
                        {
                        (
                        8
                        ,
                        2
                        )
                        ,
                        (
                        10
                        ,
                        3
                        )
                        }
                      configurations respectively (see Tables 1–3). The less number of features used for classification significantly reduces the computational time of the proposed model (See Table 4 and 5). Hence, SVM-MV and (P,R)=
                        {
                        (
                        8
                        ,
                        2
                        )
                        ,
                        (
                        10
                        ,
                        3
                        )
                        }
                      are the suitable pair to design an automated AMD detection system. The computational time (training and testing using 10-fold cross validation) required for majority vote (SVM-MV) comprising SVM-linear, SVM-quadratic and SVM-RBF classifiers for private, ARIA and STARE database are presented in Table 5.

@&#DISCUSSION@&#

Fundus imaging is much cheaper imaging modality than OCT [61]. Recently, fundus image analysis and data mining techniques are used often for early detection of DR [62], glaucoma [63], and AMD [7]. In this work, we have introduced LCP based feature extraction and data mining technique to develop automated AMD detection system using digital fundus images to detect normal and AMD classes. LCP captures both microscopic image information and local shape information of an image [35]. Hence, this approach explores various neighbourhood combinations and extracts the features which best represent the AMD abnormalities. LBP uses (P,R)=
                        {
                        (
                        8
                        ,
                        1
                        )
                        ,
                        (
                        16
                        ,
                        2
                        )
                        ,
                        (
                        24
                        ,
                        3
                        )
                        }
                      pixel neighbourhood and radius to extract local contrast features [45]. However LCP uses (P,R)=
                        {
                        (
                        8
                        ,
                        2
                        )
                        ,
                        (
                        10
                        ,
                        3
                        )
                        ,
                        (
                        12
                        ,
                        4
                        )
                        }
                      configuration which helps one to capture subtle changes and also to estimate significant reconstruction coefficient (http://www.cse.oulu.fi/CMV/Downloads/LBPMatlab) [35].

Our proposed method yielded good results for all the three dataset (See Table 4) for different P and R combinations. Tables 1 and 3 show that the results obtained for various classifiers using a number of ranked feature for private, ARIA and STARE datasets respectively. Our results show that kernel functions of SVM obtained better performance using less number of features compared to linear SVM. However, the best results are obtained using linear SVM for both private (accuracy=93.52%) and STARE (accuracy=97.78%) datasets. The best performances (See Table 4) viz. sensitivity 98.00%, specificity 97.50%, PPV 98.33% and accuracy 97.78% are obtained using the linear SVM kernel for STARE dataset. These results show that the proposed LCP features and linear SVM are a suitable combination for the classification of retinal images to identify normal and AMD classes.

Brief discussion of reported works [11,14,13,28–33] of AMD detection using fundus images is described in Section 1. The available literatures use private dataset for automated AMD detection except [11,14,13,30,31]. References [11,14,13,30,31] have used public domain datasets (ARIA and STARE) to evaluate their algorithms. In this work, we have validated our work with all the three databases. The summary of AMD detection methods is presented in Table 6
                     .

Salient features of this study are briefly described below:
                        
                           •
                           The anatomical structures of retinal images viz. OD and blood vessels are to be extracted in [11,13,30–32]. It is difficult to extract these morphological features as they are prone to noise and selecting proper structuring element is not easy. However, in the current study we do not perform any segmentation.

The current study uses 22 significant features to obtain an average accuracy of 97.78% where as literatures [11,14,30,33] used 1262, 54, 50–400 and 121 features and obtained an accuracy of 100%, 95.07%, 99.60% and 93.70%, respectively.

The linear configuration patterns capture both microscopic configuration and shape information of an image [35]. Hence, this method captures subtle changes in the pixels due to the presence of drusen and other anomalies. Hence, the proposed approach produced a highest sensitivity of 98.00% and specificity of 97.50%.

The proposed system is evaluated using both private and public datasets and obtained a highest average accuracy of 93.52% and 97.78% respectively.

The plot of number of features vs. performance measures (see Fig. 8) revealed that 22 significant LCP features are sufficient to obtain the highest performance (see Fig. 8c) for STARE dataset.

The proposed system (See Table 4) is carried out with Intel i7-4770 3.47GHz processor, 16 GB 1600MHz CL9 DDR3-RAM and MATLAB 2012b computational environment. The time taken for feature extraction is 273.621; 297.405; 109.332Sec and classification is (TrT: 0.418, TsT: 0.254)Sec; (TrT: 0.190, TsT: 0.099)Sec; (TrT: 0.170, TsT: 0.048)Sec for entire private, ARIA and STARE datasets respectively.

The proposed method obtained a highest classification accuracy of 97.78% for STARE dataset using SVM-Linear and (P,R)=
                        {
                        (
                        8
                        ,
                        2
                        )
                        ,
                        (
                        10
                        ,
                        3
                        )
                        ,
                        (
                        12
                        ,
                        4
                        )
                        }
                      configuration (see Table 4). Additionally, we proposed a generalized model for AMD screening using SVM-MV and (P,R)=
                        {
                        (
                        8
                        ,
                        2
                        )
                        ,
                        (
                        10
                        ,
                        3
                        )
                        }
                      (see Table 5) configuration which can be used for tele-monitoring of AMD patients. The number of features (38, 5 and 19) used for classification in the proposed generalized model varies for private, ARIA and STARE datasets, respectively. The resolution of images for private (480×364), ARIA (768×576) and STARE (700×605) datasets is different and also the presence of AMD lesion varies in these datasets. Lesions in the STARE dataset are large as compared to private and ARIA dataset. However with a large number of features in the classification phase may improve the classification accuracy.

@&#CONCLUSION@&#

We have proposed an automated eye screening system using linear configuration patterns which characterizes normal and AMD fundus images as LCC and PO features. The Gaussianity of the features are verified for two classes and significant features are selected and ranked using 
                        p
                        -value
                        
                        (
                        p
                        ≤
                        0.05
                        )
                     . Since, the feature distribution is Gaussian, a simple linear SVM with 22 optimal significant LCP features yielded a highest sensitivity of 98.00%, specificity of 97.70% and accuracy of 97.78% for STARE dataset. The result suggests that the proposed screening system is suitable for AMD screening.

None declared.

@&#ACKNOWLEDGEMENTS@&#

Authors thank Social Innovation Research Fund (SIRF/Project Code: T1202), Singapore for providing grant for this research. Also authors would like to acknowledge Head of Department and all staff members of Department of Ophthalmology, Kasturba Medical College, Manipal, India for sharing the images for this study.

@&#REFERENCES@&#

