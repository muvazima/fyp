@&#MAIN-TITLE@&#Performance comparison of multi-label learning algorithms on clinical data for chronic diseases

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           We evaluate multi-label learning algorithms for the analysis of clinical data.


                        
                        
                           
                           We focus on patients affected by multiple chronic diseases.


                        
                        
                           
                           We use a summary statistics approach to extract features on medical time series.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Multi-label learning

Complex patient

Chronic disease

Clinical data

Summary statistics

@&#ABSTRACT@&#


               
                  Graphical abstract
                  
                     
                        
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

Chronic diseases, also called noncommunicable diseases (NCDs) [1], are characterized by a long duration and generally a slow progression. Widespread chronic diseases include cardiovascular diseases, chronic respiratory diseases and diabetes. Chronic conditions are a major concern for public health programs of governments, particularly due to their negative effect in the continuous growth of medical care costs [2]. Chronic obstructive pulmonary disease (COPD) is an incurable illness, mainly due to tobacco smoking, where the treatment merely slows the progress of the condition. The World Health Organization (WHO) estimates that 64 million people have COPD worldwide in 2004 [3]. Concerning another major chronic disease, diabetes affects 347 million people worldwide in 2008 [4]. WHO projects that diabetes will be the 7th leading cause of death in 2030 [5]. Type 2 diabetes consists of 90% of people with diabetes, and is mostly the consequence of excess body weight and physical inactivity [6].

Despite the technical progress in the medical area which allows patients to be monitored in a more continuous way [7], the treatment of chronically ill patients, which can develop several comorbidities, remains complex for the physician. The continuous monitoring generates larger quantity of data. Often these measures are heterogeneous, such as laboratory tests, physiological values or electrocardiograms. On the one side, physicians willing to take optimal decisions will have to aggregate the information contained in these data. On the other side, such aggregation will become (or are already) unmanageable for humans. In addition, physicians are frequently in charge of hundreds of patients, as reported in [8]. Therefore, there is a need for state-of-the-art data-mining and machine learning tools to assist physicians by providing aggregated information about their patients. Indeed, as reported in [9], medical doctors would use tools that improve their understanding of an illness even if these involve more cognitive effort than in the standard practice. Several challenges appear during the design of such tools. Chronically ill patients, such as a diabetic patient, suffer frequently from several comorbidities in relation with the main disease. New approaches in the machine learning field, such as Multi-Label Learning (MLL), which have received, in the last few years, substantial contributions from the machine learning community [10–12], are then the good candidate for modeling the profile of a patient affected by several comorbidities. Another challenge concerns the characteristics of medical signals. Clinical data consist of multivariate time series that are often irregular by the fact that a patient may present various number of records with respect to another patient and the values can be nonuniformly sampled. The processing of data with these characteristics is challenging and techniques for the extraction of features are needed. One approach consists on relying on quantization methods, such as k-means clustering and Bag-of-Words (BoW), that have been proven successful in several medical data processing tasks [13]. Another approach would be to extract summary statistics for the different types of sequential clinical data [14].

MLL differs from classical machine learning by tackling the learning problem from a different perspective. In contrast to the classical classification tasks where each observation belongs to only one mutually exclusive class, in MLL decision areas of labels (i.e. classes) overlap. This aspect leads to the annotation (i.e. instead of classification) of observations with zero, one or several labels. In addition, instead of expressing the presence or the absence of a label as a binary variable, it is possible to express the confidence of the presence of a label through a score or a probability. This formulation looks natural for many problems in real life, such as the detection of emotions in music [15,16], the semantic scene classification [17] or the classification of text into topics [18].

Regarding the application of such approaches in the medical domains, we can mention several research works. In genomics field, Barutcuoglu et al. proposed a Bayesian framework for the prediction of gene function [19]. Independently for each gene function, a Support Vector Machine (SVM) is trained, then a Bayesian network is built for combining the multiple classifier results. The graph structure of the network is based on a hierarchical gene taxonomy. The aim of this network is to avoid inconsistent set of predictions, where for a given gene a specific label may be predicted relevant while its inclusive parent label is predicted irrelevant. In the biology field, Xiao et al. developed the iLoc-Virus predictor [20] for predicting the subcellular locations of proteins according to their sequence information. In their work, they focus on viral proteins, those generated by viruses. Being able to predict the locations of viral proteins in a viral infected cell is important for improving antiviral drugs. As a virus protein can have more than one location, MLL methods accommodate well, and thus the ML-kNN [21] algorithm was chosen for their predictor. The following work is focused on chronic diseases, although they are not based on MLL but on related techniques. Huang et al. proposed a system for the prognosis and the diagnosis of chronic diseases which is based on data mining and case-based reasoning [22]. Data mining techniques are used to discover patterns from health examination data. More precisely, a decision tree induction algorithm is applied to find rules which will serve to the chronic diseases classification of new cases. Afterwards, case-based reasoning, which consists on the analysis of old cases to provide solution for a new case, aims to support physicians for the diagnosis and the treatments of chronic diseases. Regarding the evaluation, the experiment data were collected from a professional health examination center, and a feasibility test was performed with 12 discharged real cases. Amaral et al. developed a clinical decision support system to assess patients affected by chronic obstructive pulmonary disease (COPD) based on the forced oscillation technique (FOT) [23]. FOT is a noninvasive method to assess the breathing mechanics, using small amplitude pressure oscillations to stimulate the respiratory system in order to evaluate the flow response. Several machine learning classifiers were attempted, such as naive Bayes (NB), k-nearest neighbors (KNN), decision trees (DT), artificial neural networks (ANN), or support vector machines (SVM). Based on a dataset of 50 volunteers (where 25 have COPD), non-linear classifiers such as ANN and SVM and the lazy learning KNN classifier were able to reach a proper accuracy for COPD clinical diagnosis (sensitivity 
                        >
                        87
                        %
                     , specificity 
                        >
                        94
                        %
                     ).

We are motivated by the problem of studying multi-label learning techniques for the analysis of clinical data in order to identify patients that may be affected by chronic diseases. We use the MIMIC-II clinical database [24] where 19,773 patients of various intensive care units (ICUs) are diagnosed with one or several chronic diseases according to the coding scheme of the International Classification of Disease revision 9 (ICD-9).
                        2
                     
                     
                        2
                        
                           http://www.who.int/classifications/icd
                        
                      Being able to characterize patients, based on their clinical data, open several applications, such as the identification of patient cohorts in the context of comparative effectiveness studies or in the case of clinical decision support systems [14]. In a previous paper [25], Bromuri et al. report on a new classifier which combines BoW and supervised dimensionality reduction algorithms to perform multi-label classification on health records of chronically ill patients. In the framework of this research, we discovered the following new challenges. Although the quantization method (BoW) used is convenient for the feature extraction when dealing with irregular time series, we think that a finer feature extraction approach based on summary statistics [14] will improve the results while making it easier to identify the influent characteristics.

In addition, the evaluation of a new MLL technique for the classification of chronic diseases based on the analysis of clinical data is made difficult by the fact that there are no studies which provide a large experimental comparison of state-of-the-art MLL algorithms on such data. The main contribution of this work is a large experimental review of multi-label learning approaches for the analysis of clinical data of chronically ill patients. We provide an extended description on properties of the dataset, on the way features are extracted using summary statistics and how the evaluation is conducted.

The rest of this document is organized as follows: Section 2 presents a background on evaluation metrics and methods for multi-label learning; Section 3 describes the MIMIC-II database and its properties; Section 4 defines the methodology for building models; Section 5 presents the results for the multi-label algorithms considered in this study; finally, Section 6 concludes this paper and draws the lines for future work.

@&#BACKGROUND@&#

This section begins with the formal definition of a MLL problem and their related evaluation metrics. Then, a state-of-the-art of the existing MLL techniques is described.

With L for the finite set of labels, and with X for the domain of observation, the training set T is defined as 
                        T
                        =
                        {
                        (
                        
                           
                              x
                           
                           
                              1
                           
                        
                        ,
                        
                           
                              Y
                           
                           
                              1
                           
                        
                        )
                        ,
                        (
                        
                           
                              x
                           
                           
                              2
                           
                        
                        ,
                        
                           
                              Y
                           
                           
                              2
                           
                        
                        )
                        ,
                        …
                        ,
                        (
                        
                           
                              x
                           
                           
                              n
                           
                        
                        ,
                        
                           
                              Y
                           
                           
                              n
                           
                        
                        )
                        }
                        (
                        
                           
                              x
                           
                           
                              i
                           
                        
                        ∈
                        X
                        ,
                        
                           
                              Y
                           
                           
                              i
                           
                        
                        ⊆
                        L
                        )
                     . Based on these definitions, a multi-label classifier h is defined as 
                        h
                        :
                        X
                        →
                        
                           
                              2
                           
                           
                              L
                           
                        
                     . In addition, some evaluation metrics are based on the output of a real-valued scoring function f defined as 
                        f
                        :
                        X
                        ×
                        L
                        →
                        R
                     . For an observation x
                     
                        i
                      and its attached label set Y
                     
                        i
                     , the scoring function f will output larger values for labels in Y
                     
                        i
                      than those not in Y
                     
                        i
                     , i.e. 
                        f
                        (
                        
                           
                              x
                           
                           
                              i
                           
                        
                        ,
                        
                           
                              y
                           
                           
                              1
                           
                        
                        )
                        >
                        f
                        (
                        
                           
                              x
                           
                           
                              i
                           
                        
                        ,
                        
                           
                              y
                           
                           
                              2
                           
                        
                        )
                      for any 
                        
                           
                              y
                           
                           
                              1
                           
                        
                        ∈
                        
                           
                              Y
                           
                           
                              i
                           
                        
                      and 
                        
                           
                              y
                           
                           
                              2
                           
                        
                        ∉
                        
                           
                              Y
                           
                           
                              i
                           
                        
                     . Finally, some evaluation metrics need also a ranking function 
                        
                           
                              rank
                           
                           
                              f
                           
                        
                        (
                        ·
                        ,
                        ·
                        )
                     , which maps the outputs of 
                        f
                        (
                        
                           
                              x
                           
                           
                              i
                           
                        
                        ,
                        y
                        )
                      for any 
                        y
                        ∈
                        L
                      to 
                        {
                        1
                        ,
                        2
                        ,
                        …
                        ,
                        |
                        L
                        |
                        }
                      such that if 
                        f
                        (
                        
                           
                              x
                           
                           
                              i
                           
                        
                        ,
                        
                           
                              y
                           
                           
                              1
                           
                        
                        )
                        >
                        f
                        (
                        
                           
                              x
                           
                           
                              i
                           
                        
                        ,
                        
                           
                              y
                           
                           
                              2
                           
                        
                        )
                      then 
                        
                           
                              rank
                           
                           
                              f
                           
                        
                        (
                        
                           
                              x
                           
                           
                              i
                           
                        
                        ,
                        
                           
                              y
                           
                           
                              1
                           
                        
                        )
                        <
                        
                           
                              rank
                           
                           
                              f
                           
                        
                        (
                        
                           
                              x
                           
                           
                              i
                           
                        
                        ,
                        
                           
                              y
                           
                           
                              2
                           
                        
                        )
                     .

In classic learning approach of multiclass problems, the evaluation is done through common metrics such as accuracy, precision, and recall. In multi-label problems, the evaluation is more complicated and need extended evaluation metrics. The following five evaluation metrics [21], that are described below, are commonly used with multi-label problems. Note that these evaluation metrics consider all the set of labels, this is not a per label evaluation, thus the results are sensitive to the distribution of labels in the dataset.

Let a testing set 
                           S
                           =
                           {
                           (
                           
                              
                                 x
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 Y
                              
                              
                                 1
                              
                           
                           )
                           ,
                           (
                           
                              
                                 x
                              
                              
                                 2
                              
                           
                           ,
                           
                              
                                 Y
                              
                              
                                 2
                              
                           
                           )
                           ,
                           …
                           ,
                           (
                           
                              
                                 x
                              
                              
                                 m
                              
                           
                           ,
                           
                              
                                 Y
                              
                              
                                 m
                              
                           
                           )
                           }
                        .

Hamming loss is defined as the fraction of the proper labels to the total number of labels. The score lies between 0 and 1, where 0 corresponds to the best result:
                              
                                 (1)
                                 
                                    
                                       
                                          hloss
                                       
                                       
                                          S
                                       
                                    
                                    (
                                    h
                                    )
                                    =
                                    
                                       
                                          1
                                       
                                       
                                          m
                                       
                                    
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          m
                                       
                                    
                                    
                                       
                                          |
                                          h
                                          (
                                          
                                             
                                                x
                                             
                                             
                                                i
                                             
                                          
                                          )
                                          Δ
                                          
                                             
                                                Y
                                             
                                             
                                                i
                                             
                                          
                                          |
                                       
                                       
                                          |
                                          L
                                          |
                                       
                                    
                                    ,
                                 
                              
                           where ▵ represents the symmetric difference.

One-error evaluates the fraction of top-ranked labels not part of the relevant label set. The score lies between 0 and 1, where 0 corresponds to the best result:
                              
                                 (2)
                                 
                                    one
                                    -
                                    
                                       
                                          error
                                       
                                       
                                          S
                                       
                                    
                                    (
                                    f
                                    )
                                    =
                                    
                                       
                                          1
                                       
                                       
                                          m
                                       
                                    
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          m
                                       
                                    
                                    γ
                                    (
                                    arg
                                    
                                    
                                       
                                          max
                                       
                                       
                                          y
                                          ∈
                                          L
                                       
                                    
                                    f
                                    (
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    y
                                    )
                                    )
                                    ,
                                 
                              
                           where
                              
                                 (3)
                                 
                                    γ
                                    (
                                    y
                                    )
                                    =
                                    {
                                    
                                       
                                          
                                             
                                                1
                                             
                                             
                                                if
                                                
                                                y
                                                ∉
                                                
                                                   
                                                      Y
                                                   
                                                   
                                                      i
                                                   
                                                
                                                ,
                                             
                                          
                                          
                                             
                                                0
                                             
                                             
                                                otherwise
                                                .
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

Coverage evaluates how many steps are needed, on the average, to go down the list of labels so as to cover all the relevant labels of the observation. A score as small as possible is better:
                              
                                 (4)
                                 
                                    
                                       
                                          coverage
                                       
                                       
                                          S
                                       
                                    
                                    (
                                    f
                                    )
                                    =
                                    
                                       
                                          1
                                       
                                       
                                          m
                                       
                                    
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          m
                                       
                                    
                                    
                                       
                                          max
                                       
                                       
                                          y
                                          ∈
                                          
                                             
                                                Y
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                    
                                    
                                       
                                          rank
                                       
                                       
                                          f
                                       
                                    
                                    (
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    y
                                    )
                                    −
                                    1
                                    .
                                 
                              
                           
                        

Ranking loss evaluates the average part of reversely ordered label pairs, for the observation. The score lies between 0 and 1, where 0 corresponds to the best result:
                              
                                 (5)
                                 
                                    
                                       
                                          rloss
                                       
                                       
                                          S
                                       
                                    
                                    (
                                    f
                                    )
                                    =
                                    
                                       
                                          1
                                       
                                       
                                          m
                                       
                                    
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          m
                                       
                                    
                                    
                                       
                                          1
                                       
                                       
                                          |
                                          
                                             
                                                Y
                                             
                                             
                                                i
                                             
                                          
                                          ∥
                                          (
                                          L
                                          ⧹
                                          
                                             
                                                Y
                                             
                                             
                                                i
                                             
                                          
                                          )
                                          |
                                       
                                    
                                    ×
                                    |
                                    {
                                    (
                                    
                                       
                                          y
                                       
                                       
                                          1
                                       
                                    
                                    ,
                                    
                                       
                                          y
                                       
                                       
                                          2
                                       
                                    
                                    )
                                    |
                                    f
                                    (
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    
                                       
                                          y
                                       
                                       
                                          1
                                       
                                    
                                    )
                                    ≤
                                    f
                                    (
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    
                                       
                                          y
                                       
                                       
                                          2
                                       
                                    
                                    )
                                    ,
                                    (
                                    
                                       
                                          y
                                       
                                       
                                          1
                                       
                                    
                                    ,
                                    
                                       
                                          y
                                       
                                       
                                          2
                                       
                                    
                                    )
                                    ∈
                                    
                                       
                                          Y
                                       
                                       
                                          i
                                       
                                    
                                    ×
                                    (
                                    L
                                    ⧹
                                    
                                       
                                          Y
                                       
                                       
                                          i
                                       
                                    
                                    )
                                    }
                                    |
                                    ,
                                 
                              
                           where 
                              ⧹
                            is the set-theoretic difference.

Average precision evaluates the average fraction of relevant labels ranked above a particular label 
                              y
                              ∈
                              
                                 
                                    Y
                                 
                                 
                                    i
                                 
                              
                           . The score lies between 0 and 1, where 1 corresponds to the best result:
                              
                                 (6)
                                 
                                    
                                       
                                          avgprec
                                       
                                       
                                          S
                                       
                                    
                                    (
                                    f
                                    )
                                    =
                                    
                                       
                                          1
                                       
                                       
                                          m
                                       
                                    
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          m
                                       
                                    
                                    
                                       
                                          1
                                       
                                       
                                          |
                                          
                                             
                                                Y
                                             
                                             
                                                i
                                             
                                          
                                          |
                                       
                                    
                                    ×
                                    
                                       
                                          ∑
                                       
                                       
                                          y
                                          ∈
                                          
                                             
                                                Y
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                    
                                       
                                          |
                                          {
                                          
                                             
                                                y
                                             
                                             
                                                ′
                                             
                                          
                                          |
                                          
                                             
                                                rank
                                             
                                             
                                                f
                                             
                                          
                                          (
                                          
                                             
                                                x
                                             
                                             
                                                i
                                             
                                          
                                          ,
                                          
                                             
                                                y
                                             
                                             
                                                ′
                                             
                                          
                                          )
                                          ≤
                                          
                                             
                                                rank
                                             
                                             
                                                f
                                             
                                          
                                          (
                                          
                                             
                                                x
                                             
                                             
                                                i
                                             
                                          
                                          ,
                                          y
                                          )
                                          ,
                                          
                                             
                                                y
                                             
                                             
                                                ′
                                             
                                          
                                          ∈
                                          
                                             
                                                Y
                                             
                                             
                                                i
                                             
                                          
                                          }
                                          |
                                       
                                       
                                          
                                             
                                                rank
                                             
                                             
                                                f
                                             
                                          
                                          (
                                          
                                             
                                                x
                                             
                                             
                                                i
                                             
                                          
                                          ,
                                          y
                                          )
                                       
                                    
                                    .
                                 
                              
                           
                        

Madjarov et al. did an extensive comparison of multi-label learning methods on various benchmark datasets [11]. They propose three categories of methods for multi-label learning: algorithm adaptation methods, problem transformation methods and ensemble methods.

Algorithm adaptation methods are modifications of existing machine learning algorithms for solving multi-label learning problems. The modification allows the new algorithm to handle directly multi-label data. Adaptations for multi-label learning have been proposed for AdaBoost [26], k-nearest neighbors [21], decision trees [27,28], neural networks [29] and support vector machines [30].

Problem transformation methods transform a multi-label learning problem into several single-label classification problems which allow the use of existing machine learning algorithms. Problem transformation methods can be divided into three groups: binary relevance methods, label power-set methods and pair-wise methods.

The binary relevance (BR) method [31] splits the multi-label learning problem into several binary classification problems using the one-against-all strategy. Since the BR method has a major drawback to not consider correlation between labels, the classifier chain (CC) approach [32] was introduced as an extension. In a similar way to the BR method, the CC approach involves a binary transformation for each label, and the extension concerns the addition in the feature space of binary variables for label relevances of all previous classifiers, thus resulting to a classifiers chain which considers a form of correlation between labels.

The label power-set (LP) method [31] transforms the multi-label learning problem into a single-class classification problem by taking into account the set of all possible combinations of labels, thus the LP method directly considers the labels correlation. As the space of possible classes can be very large, LP cannot be applied as it is for practical applications. To solve this issue, the Pruned Problem Transformation (PPT) method [33] has been proposed. PPT is an efficient method that keeps, based on the occurrence of observations in the training set, only combined labels that occur more than a predefined threshold, discarding other combinations. Hierarchy Of Multi-label Learners (HOMER) [34] is another approximation of LP which is based on the divide-and-conquer paradigm where the set of labels is organized following a tree structure, build using a clustering algorithm, where each node is a simpler multi-label classification problem dealing only with a subset of labels. This is an efficient algorithm that is adapted to problems containing large set of labels.

The pair-wise method handles the multi-label learning problem using binary classifiers in a round-robin approach. The idea is to have 
                              L
                              (
                              L
                              −
                              1
                              )
                              /
                              2
                            classifiers which consider all pairs of labels. Given an observation, each binary classifier predicts one of the two labels. When all classifiers have been evaluated, a majority voting algorithm is used and the labels are ranked according to their number of votes. Recent works in pair-wise method are [35,36].

Ensemble methods are extensions built on algorithm adaptation methods or problem transformation methods.

The ensemble of classifier chains (ECC) [32] is an extension that has classifier chains (CC) as a base method. The driving principle is to have for each independent CC, a random chain ordering and a training on a random selection of the training set. A label is determined relevant if it was predicted by a percentage of classifiers according to a threshold value.

Random forest predictive clustering tree (RF-PCT) [28,37] is an extension that has predictive clustering tree (PCT) as a base method. PCT is a decision tree organized as a hierarchy of clusters. In a random forest, each tree in the ensemble is learned using a bootstrap sample from the training set. Each PCT provides a multi-label classification and then the fusion of their result is done by using some voting scheme.

Random k-Labelsets (RAkEL) [38] is an extension that is based on LP. RAkEL tackles the computational efficiency issue of LP by breaking the original set of labels into k smaller random subsets where LP can be applied efficiently. A simple voting process is used to determine the final set of relevant labels.

In this section, we describe the characteristics of the MIMIC-II clinical database [24]. We also explain how we use these data for our study related to chronic diseases.

The data were gathered during a seven year period, beginning in 2001, from Intensive Care Unit (ICU) of Boston׳s Beth Israel Deaconess Medical Center (BIDMC). The MIMIC-II clinical database [24] is publicly and freely available after registration. The last release of the database contains around 33,000 patients. We choose to skip the neonates and the children in order to concentrate only on the adult population (
                        ≥
                        16
                      years old) which consists of around 24,000 patients, where we extracted a subset of 19,773 patients with chronic diseases. Regarding the restriction to the adult population, we motivate this decision by the divergence which exists between these two groups in terms of medical conditions and treatment plans. The average age of the patients in the database is 67 years old. A high proportion of elderly patients in ICU can be explained by the worsening of their comorbidities [39]. The distribution of the population is 56% of men and 44% of women.

The clinical data we consider are the laboratory tests and the items registered in the chart. By chart, we mean a logbook per patient which records the results of heterogeneous examinations, such as fluid assessment, physiological measure, or severity score which evaluates vital functions. Important information such as the age and the gender of the patient is part of the chart. According to the length of the stay, a patient will make several laboratory tests and various examinations. Thus, clinical data of patients are time series. In order to attenuate the amount of missing values, we take a subset of items, from the laboratory tests and from the chart, that are present at least for 80% of the patients. We end up with 76 items from the laboratory test and from the chart. A detailed table, with the descriptive statistics, is available in Appendix A. Note that we do not apply any feature selection algorithms, since these algorithms will select a subset of features that will optimize the accuracy for this particular database. Instead, we want to keep this work as general as possible to allow generalization to other clinical databases, in particular ICU databases.

Note that the data is not missing at random according to the documentation of the database.
                        3
                     
                     
                        3
                        
                           http://mimic.physionet.org/UserGuide/node16.html
                        
                      Thus techniques such as interpolation or imputation will not give appropriate results. Several approaches exist for handling the problem of missing values in medical datasets [40]. A trivial approach is to substitute the mean for the missing values [41], however this is rarely an acceptable solution [42]. A better approach is to look at medical knowledge to substitute with values within a plausible range [42]. Given these considerations, we consider a plausible range of physiological values in the case of missing data according to information we gathered in the medical literature. We either impute physiological values in ranges that are plausible given the patient disease or we impute physiological values of a healthy person if appropriate.

As labels we consider 10 families of chronic diseases where their distributions among the 19,773 extracted patients are presented in Table 1
                     . We use the coding scheme of the International Classification of Disease revision 9 (ICD-9)2 available in the MIMIC-II database for building the 10 families of chronic diseases as described in Table 2
                     . Providing a definitive diagnostic is not realistic in our settings. The ICD-9 codes system, which is especially used for morbidity statistics or health insurance systems, is not sophisticated enough to define a label for a precise diagnostic. In addition, even when a diagnostic is available, for a given disease the treatment is often specific for each patient, due to different symptoms, results of other examinations, interaction of medications, or allergies. Based on these considerations, we decided to form 10 families of chronic diseases by taking into account the medical relevance, the characteristics of the dataset and the hierarchical structure of the ICD-9 coding system. Providing information on belonging of a patient to one or several disease families will support the physician by suggesting the directions for further investigations.

Related to the definition of labels, it is important to compute the label cardinality and the label density. Label cardinality quantifies the number of alternative labels that characterize the observations in the dataset on average. With respect to label cardinality, label density considers also the number of labels. The two measures are useful because multi-label algorithms may present a different behavior in datasets with similar cardinality, but different density.


                     Label cardinality: is the average number of labels of the observations in a dataset D:
                        
                           (7)
                           
                              LC
                              (
                              D
                              )
                              =
                              
                                 
                                    1
                                 
                                 
                                    |
                                    D
                                    |
                                 
                              
                              
                                 
                                    ∑
                                 
                                 
                                    i
                                    =
                                    1
                                 
                                 
                                    |
                                    D
                                    |
                                 
                              
                              |
                              
                                 
                                    Y
                                 
                                 
                                    i
                                 
                              
                              |
                              .
                           
                        
                     
                  


                     Label density: is the average number of labels of the observations in a dataset D divided by 
                        |
                        L
                        |
                     
                     
                        
                           (8)
                           
                              LD
                              (
                              D
                              )
                              =
                              
                                 
                                    1
                                 
                                 
                                    |
                                    D
                                    |
                                 
                              
                              
                                 
                                    ∑
                                 
                                 
                                    i
                                    =
                                    1
                                 
                                 
                                    |
                                    D
                                    |
                                 
                              
                              
                                 
                                    |
                                    
                                       
                                          Y
                                       
                                       
                                          i
                                       
                                    
                                    |
                                 
                                 
                                    |
                                    L
                                    |
                                 
                              
                              .
                           
                        
                     
                  

The dataset presents a label cardinality of 2.37 and a label density of 0.237, with 1023 possible combinations, of which 522 are present in the dataset.

@&#METHODS@&#

In this section we describe the feature extraction and the standardization that we apply on the data, then we describe the multi-label learning algorithms considered in this study.

Laboratory events and chart events of each patient are summarized into one feature vector. Due to the heterogeneity and the different frequencies of the selected medical data, we propose the following approach for the feature extraction according to the type of the measured values:

Numerical variables consist of measured values such as blood pressure, creatinine and temperature. When they appear one time, such as the height at the patient admission, they are taken in the feature vector as they are. When they appear several times, the following summary features are computed: mean, median, standard deviation and range (max–min).

Categorical variables consist of observed values such as cardiovascular function assessment score and urine color. For a patient which did several times a particular examination where results are discrete values which can be divided into mutually exclusive classes, we can represent this information as a histogram. Then, the relative frequency of each category of the histogram is used as feature. There is also the case where only one observation exists for each patient, such as the gender at the patient admission, in that case, we encode as feature the value in a binary variable.

Distance based algorithms, such as ML-kNN, may be affected by distortions on distances due to the scale of features can be different. Then, we compute the z-score for each element of our feature vectors set, as defined below.

Let a data set 
                           X
                           =
                           {
                           
                              
                                 x
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 x
                              
                              
                                 2
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 x
                              
                              
                                 n
                              
                           
                           }
                         where 
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                         is a d-dimensional feature vector 
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                           =
                           [
                           
                              
                                 x
                              
                              
                                 i
                                 1
                              
                           
                           ,
                           
                              
                                 x
                              
                              
                                 i
                                 2
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 x
                              
                              
                                 id
                              
                           
                           ]
                        . The z-score value z
                        
                           ij
                         of a value x
                        
                           ij
                         is
                           
                              (9)
                              
                                 
                                    
                                       z
                                    
                                    
                                       ij
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             x
                                          
                                          
                                             ij
                                          
                                       
                                       −
                                       
                                          
                                             
                                                
                                                   X
                                                
                                                
                                                   ¯
                                                
                                             
                                          
                                          
                                             j
                                          
                                       
                                    
                                    
                                       
                                          
                                             S
                                          
                                          
                                             j
                                          
                                       
                                    
                                 
                                 ,
                              
                           
                        where 
                           
                              
                                 
                                    
                                       X
                                    
                                    
                                       ¯
                                    
                                 
                              
                              
                                 j
                              
                           
                         is the jth dimension sample mean and S
                        
                           j
                         is the jth dimension sample standard deviation.

According to the analysis performed in [11], we will consider the following multi-label learning algorithms for our evaluations on clinical data: Binary Relevance (BR) [31], Classifier Chain (CC) [32] and HOMER [34] for problem transformation methods; and Random k-labelsets (RAkEL) [38] for ensemble methods. Regarding problem adaptation methods, we will take into consideration the AdaBoostMH [26] which is the multi-label adaptation of AdaBoost. We will also consider the ML-kNN [21] which is the multi-label adaptation of kNN.

For transformation methods which need a base classifier, we decided to use Support Vector Machines (SVM) [43], Naive Bayes (NB) [44] and Decision Trees (J48) [45]. The choice of SVM with an RBF kernel is motivated by its ability to model non-linear problems where classes are not linearly separable. SVM needs two parameters, the first one is the coefficient of the RBF kernel (γ), the second one is the penalty of the error term (c). The choice of NB is justified by the fact the algorithm works well in many scenarios due to its simple structure and its surprising classification performance, even with the conditional independence assumption [44]. Similar to NB, decision trees (using J48 implementation) are simple algorithms that scale optimally to large dataset, and furthermore they provide an easy explanation of the rules used for the classification. J48 needs two parameters, the first one is the confidence threshold for pruning (p), and the second one is the minimum number of instances per leaf (l).

Concerning transformation methods, no additional parameters are needed for BR and CC, HOMER needs a parameter for defining the number of clusters (k) for the k-means algorithm [46] which is used during the initialization stage of HOMER. Regarding ensemble methods, RAkEL needs two parameters where the first one defines the number of models you want to consider in the ensemble, and the second one defines the size of the subset of labels considered inside each specific model of the ensemble. According to Tsoumakas et al. [38], a reasonable choice, which provides a trade-off between the computational complexity and the predictive performance, is to specify the number of models as
                           
                              (10)
                              
                                 min
                                 (
                                 2
                                 ⁎
                                 |
                                 L
                                 |
                                 ,
                                 100
                                 )
                                 ,
                              
                           
                        where L is the label set, and to specify the size of the subset of labels as
                           
                              (11)
                              
                                 |
                                 L
                                 |
                                 /
                                 2
                                 ,
                              
                           
                        where L is the label set. In our case, as 
                           |
                           L
                           |
                           =
                           10
                        , the number of models is 20 and the size of the subset of labels is 5.

Concerning adaptation methods, AdaBoostMH does not need an additional parameter as it optimizes the hamming loss of decision stumps (Weka implementation
                           4
                        
                        
                           4
                           
                              http://www.cs.waikato.ac.nz/ml/weka/
                           
                        ). ML-kNN needs the number of neighbors (N) and a smooth parameter (σ) which controls the strength of the uniform prior (Laplace approximation of the prior). ML-kNN is a multi-label algorithm which is widely used and will serve as baseline for our evaluation. Fig. 1
                         represents how these selected learning algorithms are divided into groups using the categorization presented in the background section.

@&#EXPERIMENTS@&#

In this section we describe how the experiments were conducted and we discuss about the results.

Regarding the software environment in use, all the multi-label learning algorithms and evaluation metrics have been implemented with the Java programming language. The following Java libraries have been used: Mulan
                        5
                     
                     
                        5
                        
                           http://mulan.sourceforge.net
                        
                      (version 1.4) and Weka
                        4
                      (version 3.7.6). The operating system is a Ubuntu Linux 12.04 LTS 64 bits. Regarding the hardware environment, we used a workstation equipped with an Intel Core i7 CPU 870 at 2.93GHz and 16GB of memory (RAM). Concerning the training time, as shown in Table 6, although the CPU has 4 cores (8 threads due to hyper-threading), only 1 core is used in practice by the fact that the Java implementation of the algorithms is single thread.

The large size of the MIMIC-II dataset allows us to divide the dataset randomly in three parts where each contains enough observations for fitting optimally a model. The first part, called training set, is used to build all models across all parameters. The model selection is done using a grid search over a defined parameter space, described in Table 3
                        . The second part, called validation set, is used to select the best parameters for each algorithm. Finally the third part, called test set, is used for computing the reported results according to the best parameters. Table 3 shows the parameters selected by the grid search for each of the algorithms. This process, which consists of the permutations of the three sets, is repeated 6 times. The reported results, in Tables 4 and 5
                        
                        , are the mean of the six iterations under a confidence interval of 95%. The process schema of the experiment is presented in Fig. 2
                        .

@&#RESULTS@&#

Among the evaluation metrics that we consider, the hamming loss is for us the most important one because it represents the ability of the algorithm to discriminate the symbols associated to the illnesses that the patient has. The second one is the ranking loss as it concerns the ranking of the labels according to the dominant disease of the patient.

By looking at the results, in Tables 4 and 5, we can say that decision trees perform optimally considering all the evaluation metrics, and they have the following advantages: they scale well to large datasets and they are easy to interpret. SVM-based approaches give the best accuracy for the hamming loss but they do not rank well the illnesses, and in addition, they take a very long time to train when the Gramian matrix is big, as in the case of the MIMIC-II dataset, as shown in Table 6
                        .

Concerning the ML-kNN, often used as the gold standard in multi-label classification tasks, it obtains a performance very close to decision trees, but it is not very scalable, as the other instance-based learning approaches. AdaBoostMH is not competitive in these settings, we think that this is caused by the fact that this algorithm is optimizing mainly the hamming loss rather than the ranking loss, as discussed in [26]. Regarding naive Bayes approaches, the non-competitive results may be explained by the fact that the NB assumes statistical independence between the features. However, in our dataset, we know that some features are highly correlated according to medical observations such as the very well known link between diabetes and hypertension, as described in the literature [47]. We also evaluate the system against a random multi-label classification in order to know the floor or the ceiling which must not be excessed.

Regarding the transformation methods, CC, which is an extension to consider the correlation between the labels, as described in Section 2, is not able to improve significantly the results compared to the BR method, which considers each label independently. As discussed in [48], an explanation could be a suboptimal ordering of the labels in the chain, another reason could be an accumulation of errors induced by the chain of CC which tends to increase when the number of labels is large. In addition, after extensive benchmarks on various datasets, the authors of [48] concluded that the performance of CC dramatically drops when the complexity of the dataset increases, such as a larger number of labels, a greater cardinality or a higher label dependency. The hierarchy of multi-label classifiers (HOMER) is not bringing a significant improvement with respect to other approaches, except when NB is used as the base classifier. We think that for the way HOMER works, the creation of artificial meta labels as parents of smaller groups of related labels attenuates the previously discussed issue of NB.

The RAkEL algorithm, being an ensemble method, suffers from scalability problems as we have a large dataset. RAkEL extends the LP method by breaking the original set of labels in several random subsets where LP is used internally. Although RAkEL improves substantially over LP, the drawbacks inherent of LP remain. Moreover RAkEL needs two additional parameters which have been defined as fixed values according to the recommendation of RAkEL׳s authors [38], since in the case of our study we have a large dataset. In particular, according to Table 6, a grid search over 4 parameters (instead of 2) with an SVM as the base classifier would have been impracticable under our available computing power. However, despite the scalability issue, RAkEL with an SVM as the base classifier obtains competitive results to the BR methods, in particular it improves the results for the average precision and the ranking loss, which is relevant for the ranking of the labels according to the dominant disease of the patient. In addition, RAkEL with an SVM as the base classifier achieves the best performance, over all other methods considered in our study, for the one-error evaluation metric which evaluates the reliability of the top-ranked label.

@&#DISCUSSION@&#

Our recommendation regarding the analysis of these results concerning medical data and chronic diseases is to choose decision tree based algorithms, because they performed well across all evaluation metrics and scale up to large dataset. However, if the scenario of the hamming loss is more important, when we want to achieve the best performance regarding the identification of all diseases a patient may be affected, regardless of their rankings or their significance levels, the best algorithms are those based on SVM with BR, but with the inconvenience to be slow at training time. Another consideration to make is that in the case of chronic diseases, like in MIMIC-II, pure multi-label algorithms such as ML-kNN or AdaBoostMH do not seem to have an advantage to the BR approaches. On the other hand, the bad performance of the NB algorithm gives us a direction: in multi-label medical domains, the correlation between the features is an important characteristic to take into consideration.

Quite surprisingly we discovered that the most used multi-label learning algorithms, with the exception of RAkEL, do not improve the results with the respect of a binary relevance approach that makes the independence assumption of the chronic illnesses. It is difficult to give a complete explanation about these results, however we can provide two opposite reasons for this behavior. The first one could be that the feature extraction process cannot model optimally the correlation between the features in order that multi-label learning approaches can exploit this information. The second one could be that the extracted features are sufficient for discriminating the various illnesses. To confirm this, we think that further studies are required with algorithms and processing methods that can deepen the analysis of dependencies of physiological measurements and chronic diseases. Furthermore, in view of the promising results of the RAkEL algorithm for the ranking of diseases, we think that further developments in the direction of algorithms exploiting ensemble methods and label power-set methods should be considered.

We experienced that, in the particular context of large medical datasets, it is more convenient to use algorithms with few parameters. By the use of sequential data, in order to maintain their interpretability, we decided to use a summary statistics approach for the feature extraction rather than an unsupervised approach such as vector quantization techniques. We think that for this dataset, the conception of a well trained binary relevance method is sufficient to obtain a decent model. More sophisticated multi-label learning methods, such as the promising RAkEL method, can bring an added value but at the cost of a greater complexity.

@&#CONCLUSION@&#

In this contribution we presented an evaluation of multi-label learning algorithms on patients affected by chronic diseases. The emphasis of the work is on trying to model the relationship between different chronic illnesses by means of the multi-label paradigm. In this study we have been faced with the MIMIC-II dataset which contains a large number of patient records. This aspect leads to scalability problems with classifiers where multiple parameters need to be optimized, such as the use of a grid search method, to obtain an optimal model.

Future work implies attempting algorithms in the direction of: combining the advantages of the BR and the LP methods, exploiting better the correlation between the features and restraining the number of parameters to optimize. From these considerations, we proposed in [49] a preliminary attempt of a probabilistic multi-label learning framework for the analysis of medical data. In addition to trying multi-label algorithms, another interesting direction is to infer a correlated illness to the known one without having access to the key feature that can discriminate it. A further work would be to consider the multi-label analysis of continuous non-invasive signals such as ECG, breathing and activity to see if multi-label methods can help to diagnose comorbid pathological conditions. More precisely, working with ECG signals from the MIMIC-II Waveform Database, will allow us to detect the presence (or co-presence) of cardiovascular diseases such as myocardial infarction, coronary artery disease, or arrhythmia.

This work was partially supported by the EU FP7 287841 COMMODITY12 project.

@&#ACKNOWLEDGMENT@&#

This work was partially supported by the EU FP7 
                  287841 COMMODITY12 project.

In this appendix, we present the list of variables from the MIMIC-II dataset that is considered in our study. The descriptive statistics presented below are computed across all measured values for all patients. In Table A.1
                     , the descriptive statistics (mean, median and standard deviation) for the quantitative values are given. In Table A.2
                     , the descriptive statistics (frequency) for the categorical values are given.

@&#REFERENCES@&#

