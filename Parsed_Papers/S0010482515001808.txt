@&#MAIN-TITLE@&#Comparative assessment of feature extraction methods for visual odometry in wireless capsule endoscopy

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Wireless capsule endoscopes can be localized based solely on visual features.


                        
                        
                           
                           State-of-the-art visual localization strategies are compared.


                        
                        
                           
                           Several visual feature extraction methods are applied.


                        
                        
                           
                           A significantly lower localization error than the state of the art is reported.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Wireless capsule endoscopy

Localization

Visual odometry

Feature extraction

Algorithm

Small bowel

Gastrointestinal tract

@&#ABSTRACT@&#


               
               
                  Wireless capsule endoscopy (WCE) enables the non-invasive examination of the gastrointestinal (GI) tract by a swallowable device equipped with a miniature camera. Accurate localization of the capsule in the GI tract enables accurate localization of abnormalities for medical interventions such as biopsy and polyp resection; therefore, the optimization of the localization outcome is important. Current approaches to endoscopic capsule localization are mainly based on external sensors and transit time estimations. Recently, we demonstrated the feasibility of capsule localization based—entirely—on visual features, without the use of external sensors. This technique relies on a motion estimation algorithm that enables measurements of the distance and the rotation of the capsule from the acquired video frames. Towards the determination of an optimal visual feature extraction technique for capsule motion estimation, an extensive comparative assessment of several state-of-the-art techniques, using a publicly available dataset, is presented. The results show that the minimization of the localization error is possible at the cost of computational efficiency. A localization error of approximately one order of magnitude higher than the minimal one can be considered as compromise for the use of current computationally efficient feature extraction techniques.
               
            

@&#INTRODUCTION@&#

Wireless capsule endoscopy (WCE) is a non-invasive procedure for the visual inspection of the gastrointestinal (GI) tract [1]. It is performed with a miniaturized, swallowable endoscope packed into a capsule of the size of a large vitamin pill. It is equipped with one or more color cameras and during its journey to the anus it wirelessly transmits color images to an external receiver using radiofrequency (RF) signals [2].

The knowledge of the exact location of the capsule endoscope within the GI tract is important for the localization of abnormalities. Since WCE is currently used only for screening purposes, any medical intervention for biopsy or treatment is performed at a secondary phase. For example, if a lesion, such as a polyp, is detected in the small bowel by WCE, a biopsy and/or lesion resection can be performed at a second phase with a more invasive technique, such as surgery or device-assisted enteroscopy [3]. Therefore, the accurate estimation of the location of the lesion by WCE increases the yield and outcome of subsequent interventions. In the case of an open surgical procedure this can result in minimal loss of intestine tissue during the operation, whereas in the case of double-balloon enteroscopy it can determine the insertion route (antegrade or retrograde) of the enteroscope as well as the insertion depth so as to minimize the patients׳ discomfort [4,5].

In clinical practice, the location of an abnormality can be approximated by the estimation of the expected transit time of the capsule with respect to anatomic landmarks, and decision-making is based on a devised time index [6]. However, this localization approach can be very inaccurate, especially if the capsule cannot visualize the cecum [7,8].

In addition to the transit time estimation, commercial WCE platforms provide the localization of the capsule in a 2-dimensional (2D) projection of the body with respect to the umbilicus, e.g., the abdominal quadrant where the capsule is located. This is accomplished by a wearable RF sensor array, which receives the RF signals transmitted by the capsule while it wirelessly transmits the images. The estimation of the position of the capsule is based on the triangulation principle, with average position errors ranging between 3.7 and 11.4cm [9,10]. A more recent approach enables localization of the capsule in the 3-dimensional (3D) human body space with an average localization error of 13.3cm3 
                     [11].

RF signals are influenced by tissue densities, juxtaposition of different organs, and other anatomic considerations which can affect capsule localization accuracy [6]. Considering that magnetic waves are less sensitive in the presence of human tissue, various magnetic localization techniques have been proposed [10]. The most promising ones involve magnetic sensor arrays capable of localizing capsules equipped with a permanent magnet in the 2D or 3D body space. The average position errors reported from ex-vivo experiments range between 1.8 and 10mm and the average orientation errors between 2° and 3° [9]. Other localization techniques of capsule endoscopes are also possible, e.g., using x-ray and magnetic resonance imaging; however, they require expensive equipment and could result in adverse health effects [10].

A drawback of both the RF and magnetic localization techniques is that they do not directly provide information related to the distance the capsule travels within the GI tract. Having the 2D or 3D body coordinates of the capsule endoscope it is difficult to determine its location with respect to an anatomic landmark, such as the pylorus, needed in the case of a surgical intervention after WCE. To cope with this issue a concept capsule endoscope design with three wheels has been proposed [12,13]. The wheels are mounted on expandable and retractable legs designed to keep contact with the mucosal surface. As the capsule moves within the GI tract the wheels spin and measure the distance traveled by conventional (wheel) odometry.

Recently we proposed a visual odometry approach to capsule endoscope localization that requires neither external sensors nor wheels to measure the distance traveled by the capsule from an anatomic landmark [14]. It is based on the estimation of the motion of the capsule by tracking visual features extracted from consecutive video frames. In this paper, we perform an extensive comparative study to determine the optimal feature extraction scheme that minimizes the displacement and orientation error for capsule endoscope localization. Implementation and time performance issues of the feature extraction methods are addressed and novel results indicating up to one order of magnitude lower error rates are obtained in the majority of the test cases.

The rest of this paper consists of four sections: Section 2 reviews the state-of-the-art capsule endoscope localization approaches that have been based solely on visual features. Section 3 describes the methods compared in this study, and Section 4 presents the results obtained. A summary of conclusions is provided in the last section.

@&#RELATED WORK@&#

The concept of visual localization of capsule endoscopes appeared in 2008, with techniques based on topographic video segmentation, i.e., the segmentation of the WCE video into a number of consecutive segments that correspond to different parts of the GI tract [15,16]. These techniques were based on the supervised classification of the video frames using color, texture and motion features. This way the different parts of the GI tract, or the transitions between them, such as the esopahgogastric junction, the pylorus and the ileocecal valve, can be automatically recognized. Since then, several variations of these techniques have been proposed [17,18]. Recently the feasibility of using an unsupervised learning technique to solve the localization problem has been demonstrated. This technique is based on Scale Invariant Feature Transform (SIFT) for the extraction of local image features, and on a probabilistic latent semantic analysis model for data clustering [19]. Such topographic video segmentation techniques can be used to infer in which part of the GI tract a capsule is located, and not the exact location of the capsule within each part. The latter can be estimated on the basis of the transit time [15] and/or by application of a more accurate localization technique within the part of the GI tract that is of interest.

Such a localization technique can be based on capsule motion estimation. A motion estimation algorithm relies on the analysis of consecutive video frames to infer how much the capsule has moved, in terms of relative displacement and orientation. This is usually performed in three steps [20]. The first step is feature extraction from each frame. Contemporary feature extraction approaches [21,22] involve both the automatic detection of salient key-points within frames and the estimation of numeric descriptors from the neighborhood of each key-point. In the second step, for each key-point of two consecutive frames its similarity with the key-points of the next frame within a fixed distance is estimated. In the third step the geometric transformation, e.g., scaling and rotation of each frame with respect to its previous one is estimated, and the displacement and orientation of the traveling capsule endoscope can be determined.

In the context of WCE three approaches of this kind have been proposed for the estimation only of the orientation of the capsule endoscope, as a means to assist sensor-based localization [23–25]. Two of them [23,24] were based on the Scale Invariant Feature Transform (SIFT) algorithm [21] and [25] was based on Speeded-Up Robust Features (SURF) [22]. Due to the lack of ground truth data, e.g., measurements of the actual rotation of the capsule within the GI tract, these algorithms were validated with simulation data, produced by manual rotation of real WCE images at various degrees. In [23] rotation estimation was based on the Lucas–Kanade–Tomasi (KLT) feature tracker [26], and in [24] it was based on Singular Value Decomposition (SVD) of a correlation matrix between point sets. Both of these approaches were able to estimate the orientation of the capsule with reasonable errors for rotations of up to 20–25°. In [25] the Random Sample Consensus (RANSAC) algorithm [27] was used for the estimation of the geometric transformation between consecutive frames, resulting in reasonable orientation errors for rotations of up to 35° [28].

Simulation data including not only rotation but also scaling (to simulate the forward or backward displacement of the capsule) were used in our recent works for capsule localization by visual odometry [14,28]. In these works we showed the feasibility of motion estimation to determine both the orientation and the displacement of a capsule endoscope. They were based on an extended SURF-RANSAC scheme capable of estimating not only the rotation but also the scaling of the video frames with a reasonable error for scaling factors between 0.2 and 5.

An emulation platform [29] was considered as an alternative to the simulation approach used for the validation of the localization of the capsule endoscope. This was created by bending, twisting and painting a 1.5m plastic tube with a diameter of 3cm, similar to the small intestine. A wired camera was used instead of a capsule. Motion estimation was based on an extension of the SIFT-SVD scheme [24] for the estimation of both the orientation and rotation of the capsule endoscope. The emulation platform enabled absolute distance measurements in cm, and showed that the Mean Square Error (MSE) reached up to 4cm for the total length of the tube.

A preliminary ex-vivo experiment with animal intestine (phantom model) was performed in [30], where we have compared the performance of the SURF and Maximally Stable Extremal Regions (MSER) algorithms for feature extraction with respect to forward or backward motion estimation. The results indicated an advantage of the SURF algorithm over MSER, with an average accuracy of 81.5%.

In another study [31] a motion estimation algorithm that follows a different strategy for visual localization of capsule endoscopes was proposed. It uses the Fibonacci searching technique to find the maximum mutual information between two frames and to estimate the corresponding transformation, from which the displacement and the rotation of the capsule can be derived. Its experimental evaluation was based on the publicly available simulation dataset produced in [14]. The results did not indicate any significant advantage over the previously-mentioned SURF-RANSAC scheme.

The current studies indicate the feasibility of visual capsule endoscope localization. Motivated by their encouraging results, in this paper we perform a thorough comparative study which aims to guide future research towards an optimal representation of the WCE video contents for accurate visual capsule endoscope localization.

@&#METHODS@&#

In this paper we focus on feature extraction, which is the first step in the motion estimation process, and we investigate the performance of various state-of-the-art feature extraction algorithms for capsule endoscope localization. In the following we provide a brief description of these algorithms. 
                     Fig. 1 illustrates two consecutive video frames in a WCE video sequence. These frames are used throughout this section, in order to show the key-points detected by the considered algorithms.

The Scale Invariant Feature Transform (SIFT) [21] has been designed to provide key-points that are robust in scale and rotation transformations, resilient to image noise and invariant to illumination and distortion. The SIFT key-point extraction algorithm adopts a cascade filtering approach. Images are convolved with various Gaussian filters at several scales. The differences of the resulting blurred images (DoGs) are then calculated at several octaves leading to the calculation of the local extrema of the DoG images over scale and space. Low-contrast or edge key-points are rejected. Finally, for each key-point, the scale (at which it was detected) and the dominant gradient orientation of its neighborhood are determined. For the computation of the SIFT descriptor, a local histogram from the neighborhood of each key-point is created based on the orientation values, and it is normalized to a unit vector to ensure robustness to illumination changes.


                        
                        Fig. 2 illustrates the SIFT key-points extracted from the typical WCE frames of Fig. 1. Scale is proportional to the size of the circle and dominant orientation is represented by the angle of the radius.

Speeded-Up Robust Features (SURF) [22] constitute a significantly fast point extraction and description scheme and have been proven to achieve high repeatability, distinctiveness and robustness to certain image transformations and varying illumination conditions. In order to detect local key-points, the SURF algorithm adopts a fast approximation of the Hessian matrix that exploits integral images [32], carried out on several octaves of a Gaussian scale-space. The SURF descriptor is based on first-order Haar wavelets and aims to capture the intensity content distribution around each point. For each point, a scale and a dominant orientation are determined. To make the descriptor robust to contrast changes, the computed descriptor vector is finally turned into a unit vector.

An example of the key-points from which the SURF descriptors are extracted is illustrated in 
                        Fig. 3, where the WCE video frames of Fig. 1 are used. In comparison to SIFT, SURF produces less key-points.

Local Intensity Order Pattern (LIOP) features [33] form a local image descriptor, based on the concept of local order pattern. In general, an order pattern occurs from sorting a set of pixels by increasing intensity. Considering a pixel and a set of its neighbors, a local order pattern occurs from sorting those neighbors by increasing intensity. LIOP exploits the basic principle that the relative order of pixel intensities remains unchanged when the intensity changes are monotonic. First, images are blurred and then, an affine covariant region detector is used to localize key-points and their neighborhoods, which are then normalized to circular, fixed-size regions. Orientations are discarded and noise is removed with Gaussian smoothing, resulting in the “local patch”. In order to compensate for rotation changes all the pixels in this patch are sorted by their intensity values and then the patch is equally quantized into B ordinal bins. The Local Intensity Order Patterns (LIOPs) are constructed using the intensity order of all sampled neighboring points, thus exploiting the local information while providing a rotation-invariant description. Finally, the descriptor is constructed by accumulating and concatenating the LIOPs of points in each ordinal bin.

An example of the interest points from which the LIOP features are extracted is illustrated in 
                        Fig. 4, where we use the WCE video frames of Fig. 1. Yellow circles denote the extracted local patches using the Harris-Affine detector. It may be observed that a significantly smaller set of keypoints is extracted, compared to SIFT and SURF.

The Maximally Stable Extremal Regions (MSER) features [34] are based in the notion of extremal regions. They are invariant to affine transformations, covariant to adjacency preserving transformations, show stability and scale invariance and have been very popular for fast and efficient blob detection. The MSER algorithm involves local image binarization using a predefined set of thresholding values. This way it constructs a set of local intensity minima, which grows continuously, until regions corresponding to two local minima become adjacent and subsequently merge. The set of maximal regions is then defined as the set of all connected components that result from the consecutive thresholdings. Using the inverse of images, the set of minimal regions is constructed. Finally, intensity levels that are local minima of the rate of change of the area function are selected as thresholds producing Maximally Stable Extremal Regions.

An example of the stable regions extracted using the MSER algorithm is illustrated in 
                        Fig. 5, where we illustrate elliptical regions that have been fit to the extremal regions. Since MSER is a region selection algorithm, and does not strictly impose the feature extraction algorithm, in order to capture the visual properties of the stable regions we choose to extract the SURF features from the aforementioned elliptical regions.

Good Features to Track (GFtT) [35] have been a modification of the Harris corner detector [36] in terms of its scoring function, which has been shown to significantly improve its results. GFtT define a function that expresses the notion that corners may be defined as image regions with large intensity variation across all directions and use it to decide whether an image window contains a corner, an edge or depicts a flat region. Since this algorithm only provides a method for interest point selection, we choose to use the FREAK descriptor (which is described in Section 3.7), in order to capture the low-level visual properties of the regions surrounding the extracted corner points. In 
                        Fig. 6 we illustrate the good features to track that have been extracted from the typical WCE frames of Fig. 1.

Features from Accelerated Segment Test (FAST) [37] provide a computationally efficient corner detection method. They are significantly faster than other methods, e.g. when compared to the aforementioned DoG method of SIFT. The algorithm of FAST corner detector is rather simple. It classifies a candidate point p as a corner if a set of contiguous pixels in an appropriate Bresenham circle of radius 3 consists solely of brighter/darker pixels than the intensity of p, plus/minus a given threshold t. Improvements include a high speed test and a machine learning approach, which aim to further improve extraction speed. Typically, a non-maximum suppression step is finally performed. For a given pixel p, the corner strength is defined as the maximum value of t that makes p a corner.

Since FAST only provides a method for interest point selection, we also choose herein to use the FREAK descriptor, so as to capture the low-level visual properties of the regions surrounding FAST points. In 
                        Fig. 7 we illustrate FAST features extracted from the typical WCE frames of Fig. 1.

FREAK (Fast REtinA Keypoint) [38] is a binary visual descriptor, i.e. it provides a description in a binary form. Several binary descriptors have been proposed during the past few years and focus on the description of the area surrounding a key-point. They combine fast extraction and matching times and are preferred in real-time applications. They typically follow the same approach: they use a predefined sampling pattern, a set of sampling pairs and an orientation compensation method, in order to provide invariance on rotation. Typically scaling invariance is handled by the corresponding key-point detection algorithm. The FREAK algorithm proposes a circular sampling grid inspired by the distribution of the receptive fields over the retina. Sampling points have higher density near the key-point, while their density drops exponentially. As for the sampling pairs, it adopts a learning strategy. More specifically, using a set of key-points, the authors selected the non-correlated sampling pairs among the set of all possible pairs of the sampling grid. A cascade approach is used for matching and orientation compensation is performed on a predefined set of 45 symmetric sampling pairs, by selecting the pair with the largest gradient. As it has already been mentioned, in this work we use FREAK to capture the visual properties of GFtT and FAST features.

@&#EXPERIMENTS AND RESULTS@&#

In this section we present the approach we followed for the evaluation of the visual features we have described in Section 3, the experimental setup, the data set we have used and the results of the descriptor comparative study.

If we carefully observe the extracted points between two consecutive video frames (i.e., those extracted by SURF and depicted in Fig. 3) and if we consider the case when the camera׳s viewpoint changes slowly, we shall be in general able to notice that a) they contain several similar visual features; and b) a subset of these features seems to follow the same geometric transformation. These features will be denoted as inliers, while the remaining as outliers. If we estimate this transformation, the calculation of the rotation and scaling is trivial.

For the estimation and maximization of the set of inliers, we choose to use the RANSAC algorithm [27]. This algorithm is able to select the best model (bounded by a user-defined probability) in the presence of outliers. In our case, the model is the geometric transformation; inliers are visually matching features between two frames that follow this transformation; outliers are visually matching features that do not follow this transformation. We only consider affine transformations that map any given point of interest x
                        
                           i
                         of a given video frame to a corresponding point 
                           
                              
                                 x
                              
                              
                                 i
                              
                              
                                 ′
                              
                           
                         of the consecutive frame. Given the set of correspondences of inliers between two consecutive frames, i.e. the pairs 
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                           ↔
                           
                              
                                 x
                              
                              
                                 i
                              
                              
                                 ′
                              
                           
                        , we may define a transformation T as 
                           
                              
                                 x
                              
                              
                                 i
                              
                              
                                 ′
                              
                           
                           =
                           T
                           ⋅
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                        .

The estimation of an image transformation using RANSAC originates from the task of stereoscopic camera calibration [39], where the images captured by two cameras differ only by means of a perspective transform. Since then, this simple idea has been extended and instead of two images taken by a stereoscopic camera system, it may also consider the case of a single camera system moving slowly. This way it captures approximately the same “scene” by a slightly different viewpoint. Ideally and since the variation of the viewpoint is not expected to be very high in typical WCE videos, a small number of false correspondences between consecutive frames are typically expected. However, in our case, many false correspondences are often introduced due to the equipment, e.g. noise, or the compression, e.g., MPEG artifacts. This justifies the choice of RANSAC, because of its ability to correctly estimate a model even in the presence of a large number of outliers.

In our work, and for a pair of video frames, RANSAC is applied as follows:
                           
                              a)
                              We select invariant keypoints/regions and extract appropriate visual descriptors from each frame.

We construct a set of visually matching points/regions (tentative matches) using an appropriate distance function for each descriptor and a predefined threshold T
                                 
                                    c
                                 . In order to improve the quality of matches, we adopt the nearest neighbor ratio strategy [39]. We should note herein that the location of these points is not taken into account within this matching strategy and also that a given point of a frame may match to more than one points of the other frame.

After a predefined number of trials, RANSAC selects the largest subset of the aforementioned tentative matches that conform to the same geometric transformation T. At each trial RANSAC randomly selects a triplet of points (since affine transformations may be described using exactly 3 points) and then identifies the tentative matches that support the corresponding transformation. The goal is to select the largest such subset, at the end of the trials. We should note that this subset is optimal with a (user-defined) probability P
                                 
                                    R
                                  and denotes the set of inliers. Thus, the remaining matches are considered outliers.

This way we exploit the main advantage of the RANSAC algorithm, which is its ability to select the optimal model in the presence of a significantly large number of outliers, without a brute-force (extensive) matching approach. We should emphasize that RANSAC relies a lot on the tentative correspondences that consist of its input and the accuracy of the selected model on the user-defined probability P
                        
                           R
                        . It is common to select a significantly large value of probability P
                        
                           R
                        , thus leading to results that are as accurate as possible.


                        
                        Fig. 8(a) illustrates an example of the tentative correspondences between the SURF descriptors of two given video frames. We should observe a) the large number of tentative correspondences, compared to the number of inliers, illustrated in Fig. 8(b); and b) that the selection of the inliers is not a trivial task for the human observer.

Once the transformation matrix T is extracted using the set of inliers, it is then trivial to extract rotation θ and scale s. T is formulated as
                           
                              
                                 T
                                 =
                                 
                                    (
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         s
                                                      
                                                      
                                                         c
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         s
                                                      
                                                      
                                                         s
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         t
                                                      
                                                      
                                                         x
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             
                                                
                                                   −
                                                   
                                                      
                                                         s
                                                      
                                                      
                                                         s
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         s
                                                      
                                                      
                                                         c
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         t
                                                      
                                                      
                                                         y
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             
                                                0
                                             
                                             
                                                0
                                             
                                             
                                                1
                                             
                                          
                                       
                                    
                                    )
                                 
                                 ,
                              
                           
                        where 
                           
                              
                                 s
                              
                              
                                 c
                              
                           
                           =
                           s
                           ⋅
                           
                           cos
                           
                           θ
                        , 
                           
                              
                                 s
                              
                              
                                 s
                              
                           
                           =
                           s
                           ⋅
                           
                           sin
                           
                           θ
                         and t
                        
                           x
                        , t
                        
                           y
                         denote the translations on x- and y-axis, respectively. Given T, we are able to extract the estimated scale 
                           
                              
                                 s
                              
                              
                                 e
                              
                           
                         as 
                           
                              
                                 s
                              
                              
                                 e
                              
                           
                           =
                           
                              
                                 
                                    
                                       s
                                    
                                    
                                       c
                                    
                                    
                                       2
                                    
                                 
                                 +
                                 
                                    
                                       s
                                    
                                    
                                       s
                                    
                                    
                                       2
                                    
                                 
                              
                           
                         and the estimated rotation 
                           
                              
                                 θ
                              
                              
                                 e
                              
                           
                         as 
                           
                              
                                 θ
                              
                              
                                 e
                              
                           
                           =
                           
                              
                                 tan
                              
                              
                                 −
                                 1
                              
                           
                           
                              (
                              
                                 
                                    
                                       s
                                    
                                    
                                       c
                                    
                                 
                                 /
                                 
                                    
                                       s
                                    
                                    
                                       s
                                    
                                 
                              
                              )
                           
                        . Then we compare these values to the known (artificial) ones r and θ and calculate the corresponding errors.

For the comparative study of the feature extraction approaches we used the publicly available dataset used in [14].
                           1
                        
                        
                           1
                           
                              http://innovation.teilam.gr/mst/dataset.htm.
                         This includes a set of 117 WCE video clips acquired using Given Imaging PillCam capsules. These video clips are publicly available through the Given Imaging Atlas database [40]. Each video clip has a resolution of 576×576 pixels, and an average duration of approx. 20s. Since our intention was to study small bowel videos, 2 video clips from the esophagus and the colon were excluded (videos no. 5 and 13, respectively). All video frames were extracted from the aforementioned video clips, at a frame rate of 5fps. Then the desired geometric transformations were applied on them (i.e., rotations and changes of scale). As we have described in Section 4.1, these transformations depend on the capsule׳s changes of orientation and displacements. The evaluation approach we used has also been applied in both [31,23], and in our previous work [14].

More specifically, for each extracted video frame we constructed a set of artificially rotated and/or scaled versions of it. We then extracted all the local descriptors as we have presented in Section 3. We then used an appropriate distance measure (sum of squared distances for all descriptors except from FREAK, where sum of absolute distances was used) and a manually defined threshold T
                        
                           c
                        , to create a set of tentative correspondences and applied RANSAC, in order to exclude outliers and select the set of inliers. Using the inliers, we then extracted the corresponding transformation matrix from which it was trivial to extract rotation and/or scaling. Using these values we estimated orientation (rotation) and/or displacement (scaling) errors, by comparing them to the corresponding known artificial values.

For the extraction of the SIFT and LIOP features we have used the Matlab [41] implementation of the VLFeat library [42], while for the extraction of the SURF, MSER, GFtT, FAST and FREAK we used the Matlab 2013b built-in functions. We extracted SIFT features having set the peak threshold and the non-edge threshold values equal to 10 and 5, respectively. SURF features were extracted using 4 scales and 4 octaves. The intensity threshold of the LIOP features was set equal to 10. In the case of the MSER features, the threshold and the maximum area variation were set equal to 2 and 0.25, respectively. GFtT and FAST were extracted using a minimum accepted quality of corners equal to 0.01. In the case of the GFtT, the Gaussian filter dimension was set equal to 5. The minimum intensity of FAST corners was set equal to 0.2.

We also used the built-in RANSAC implementation of MATLAB. In order to determine the set of the tentative correspondences, we set the threshold T
                        
                           c
                         equal to 10% for the FREAK descriptor and 1% for all other descriptors. The use of the nearest neighbor ratio matching strategy further strengthened matches and since we desired a significantly high level of accuracy, we set the probability P
                        
                           R
                         to find the maximum number of inliers equal to 99.5%. Since this leads to a larger number of random trials, we set a maximum of 5000 trials. This obviously increased the estimation time in approaches that produce a large number of keypoints and result to a large number of tentative correspondences. However, since the main goal of our work is to study the accuracy and because typically this process would be offline, we do not consider this as a drawback.

@&#EXPERIMENTAL RESULTS@&#

We tested all the aforementioned approaches using artificial frames resulting from all combinations between nine different rotation angles, varying from 0° to 45° with a rotation step of 5° and 14 different scales, varying from a factor of 0.3 to a factor of 5.0, as lower or higher scales are less likely to appear in frame sequences of WCE videos. The results obtained are summarized in 
                        
                        
                        
                        Tables 1–4 and in 
                        
                        
                        
                        Figs. 9–12.


                        Table 1 and the diagram of Fig. 9 compare the performance of the feature extraction methods investigated in terms of the mean absolute orientation error, averaged over all scales, while Table 3 and the diagram of Fig. 10 in terms of the mean absolute scale error averaged over all rotation angles. We also compare our results with those presented in [31,23] and also with the results of our previous work [14]. These are presented in Tables 2 and 4 and Figs. 11 and 12, where scale and rotation angle are set to 1.0° and 0°, respectively. In all Tables, a few cases are indicated as giving a “Large” error. In the results introduced by this work, or by our previous work [14], this may happen for two possible reasons: a) the algorithm fails to produce results for more than 5% of frames, due to a very small number of correct correspondences; or b) the value of the mean absolute orientation error is more than 60 or the value of the mean absolute scale error is more than 15. However, we should note that even smaller values of error cannot be used for practical applications, since they are very likely to produce a very large accumulated error, even in the case of a sequence of a relatively small number of frames.

SIFT features proved the best in all cases, outperforming all other approaches. When applied to all scales, they resulted in a very small rotation angle error, i.e. varying from 0.0037° to 0.131°, on average and for all tested scales and from 0.0016° to 0.0033°, at the original scale. It also resulted in a scaling error at most equal to 0.0021 on average and for all rotation angles and practically equal to 0, at the original rotation angle. This can be explained since SIFT is invariant to translations, rotations and scaling transformations [43].

SURF, LIOP and MSER features follow in terms of performance, and in many cases they are comparable to SIFT. More specifically, SURF features produced a rotation error between 0.3093° and 1.4068°, on average and for all tested scales, and from 0.0119° to 0.0215° at the original scale. Although these errors are relatively small, they are still above a few orders of magnitude than SIFT. The scaling error for SURF was between 0.0003 and 0.4673 for all tested rotation angles, and between 0.0002 and 0.0124 at the original angle. In this case it should be noted that for scales larger than 0.8 the errors were comparable to those obtained with SIFT.

LIOP features showed overall a better performance than SURF. Their resulting orientation error was between 0.0227 and 0.4985 for all tested scales. However, at the original scale they showed a comparable performance to SURF, with a slight advantage for rotation angles larger than 20°. The resulting errors in this case were between 0.0120 and 0.0238. Scaling errors varied between 0.0005 and 0.9010 for all rotation angles and between 0.0002 and 0.0021 at the original angle. LIOP features have also been designed to be robust among other transformations to scaling and/or rotation. It should be noted that we have observed that the SIFT algorithm, when compared to SURF and LIOP, produces more key-points which achieve better repeatability; thus feature matching becomes more efficient. This can explain the aforementioned results. However we should note that in a recent evaluation [44] but at a significantly different data set, LIOP outperformed SIFT.

The MSER features produced scaling errors which varied between 1.4304 and 1.9025 for all tested scales and between 0.0773 and 0.4949 at the original scale. As for orientation errors, they vary between 0.0104 and 1.6545 for scales up to 4.0 and for all angles, while they become unusable for scales larger than 4.0. This did not also surprise us, since in [34] they have been shown to be reliable for scales up to 3.5. At the original rotation angle, scale errors are between 0.0087 and 0.3191. It may be observed that they perform better in scales lower than 1, i.e. in backward motion between frames.

Finally the performance of GFtT and FAST may be considered rather poor, as they appear rather unstable, and in many cases they become unusable as they lead to a “Large” error. It should not surprise us that these two features showed poor performance for large angles and scales. Large orientation errors occur since these features have not been designed to be rotation invariant. Moreover, since they both are corner detectors their large displacement errors at large scales occur due to the fact that corners at detected with the use of a fixed-size window. But due to scaling, neighborhoods of pixels change drastically, while the size of the windows remains unchanged. Our method, using SIFT, SURF or LIOP outperformed both [31,23] in terms of orientation error, for all tested angles. It also outperformed [31] in terms of displacement error, for all tested scales when using SIFT and for the majority of scales when using SURF or LIOP. We should note that improvements over [14] occur due to: a) the use of a different SURF implementation; b) the use of the nearest neighbor matching strategy within RANSAC; and c) different selection of thresholds.

In terms of computational performance, the MSER features are the fastest, whereas the LIOP are the slowest. Indicative average times per frame for a rotation of 20° and a scaling factor equal to 2.0 are as follows: 138.0ms for MSER, 149.2ms for FAST, 186.0ms for SURF, 429.5ms for GFtT, 1.41s for SIFT and 2.35s for LIOP. All experiments were performed on a personal computer with Intel® i7 920, 8M Cache, 3.1GHz, processor, with 6GB of RAM. However we do not provide further details, as this evaluation falls out of the scope of this work.

@&#CONCLUSIONS@&#

In this paper we presented an extensive comparative assessment of several state-of-the-art techniques which aim to localize a capsule endoscope using solely visual features and a motion estimation model. In contrast to most approaches that rely on external, high-cost, intrusive equipment, the proposed approach does not require any external sensors, does not require any additional cost and does not discomfort patients by any means. Moreover, it is able to provide a more accurate estimation of the location of e.g. abnormalities, allowing specialists to treat patients more effectively. We feel that the use of commercial visualization products along with a tracking approach, as the one proposed herein, an endoscopist shall be able to combine quantitative with qualitative information and precisely locate abnormalities of the GI tract.

The method proposed in [14] has been proven to produce reliable estimations with significantly small errors in the measurement of both the orientation and displacement of the WCE capsule. In the case of the SIFT features, the retrieved values of these measures have been almost perfect. SIFT algorithm was constantly able to produce invariant key-points and a set of at least three correspondences (i.e. the minimum required number of correspondences) per pair of frames that are needed for the estimation of an affine geometric transformation. SURF and LIOP features also proved reliable. We should notice that to the best of our knowledge, the use of LIOP features had not been investigated prior to this study in the research area of WCE. The performance of MSER, although at a first glance appears promising, remains a few magnitudes of error above the one of SIFT. FAST and GFtT features in conjunction with a binary visual descriptor, i.e. FREAK, had also not been investigated prior to this study. However, they have proven to produce non-practical errors. In comparison to state-of-the-art techniques [31,23] and with our previous work [14], we achieved an improvement of about an order of magnitude of error. Although the use of SIFT features introduces delays in the estimation process, due to their higher extraction time and the larger number of features they produce, we feel that results compensate.

In this study we have made a significant step towards the determination of the most robust state of the art feature extraction algorithm for visual odometry in WCE. Future work includes the investigation of different strategies for wireless capsule endoscope localization, e.g., by relaxing the affinity assumption of the presented methods and by improving the deformable model concept introduced in [45] for capsule velocity estimation, further experimentation with phantom models of animal intestines where ground truth information can be established ex-vivo.

There is no conflict of interest in relation to this paper.

@&#ACKNOWLEDGMENTS@&#

We would like to thank the anonymous reviewers for their constructive comments that helped us to significantly improve the quality of this paper.

@&#REFERENCES@&#

