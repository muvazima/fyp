@&#MAIN-TITLE@&#Joint Sparsity and marginal classification for improving Sparse Imputation performance in speech recognition

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           The self-similarity nature of speech is used to improve the Sparse Imputation method.


                        
                        
                           
                           The similar frames of speech utterance are identified using marginal classification.


                        
                        
                           
                           The Joint Sparsity method is used to reconstruct noisy component of similar frames together.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Sparse Imputation

Robust automatic speech recognition

Compressive sensing

Joint Sparsity

Self-similarity

@&#ABSTRACT@&#


               
               
                  Sparse Imputation (SI) is a relatively new method that reconstructs missing spectral components of noisy speech signal with the help of the sparse-based representation approaches. In this method, the redundancy of signal in the frequency domain helps to rebuild noisy spectral components from the remained reliable ones. On the other hand different parts of speech signal, despite time intervals between them, can be inherently similar to each other. In this paper, a major modification over the SI method is proposed that in addition to data redundancy property of speech signal in small regions, takes the advantages of its self-similarity nature over long intervals. By identifying mostly similar frames, using a method based on the marginal classification, the Joint Sparsity method is applied and a method dubbed as the Joint Sparse Imputation is presented. The experiments conducted on AURORA 2 data set show that the proposed method significantly improves the recognition results in different noisy conditions, compared to the original SI method.
               
            

@&#INTRODUCTION@&#

The Sparse Imputation (SI) is a method in the field of noise robust speech recognition which follows reconstructing missing spectral components of speech signal using the sparse representation-based approaches. In fact, this approach is a subset of the missing feature/data approaches [1,2] which concentrates on compensation of the additive noise effects. In such methods, with the help of the mask estimation methods [3–6], unreliable spectral components (on which the destructive effect of noise is significant) are first identified and tagged. These unreliable components are then reconstructed using the remained reliable ones on which the effect of noise is negligible.

The Sparse Imputation method was first introduced by Gemmeke [7]. In this method, the emerging theory of compressive sensing (CS) [8] was successfully utilized to reconstruct the unreliable components of noise corrupted signals. In the Sparse Imputation, the CS is utilized to compensate noisy feature vectors for the task of automatic speech recognition; but, there are other approaches that use the CS to enhance speech signal quality [9].

In the common CS approach, signal y is sampled using a random measurement matrix 
                        
                           
                              
                                 Φ
                              
                              
                                 measurment
                              
                           
                        
                     ; the goal of the CS approach is to recover y from the sampled signal 
                        
                           b
                           =
                           
                              
                                 Φ
                              
                              
                                 measurment
                              
                           
                           y
                        
                      using the recovery algorithm of CS as given by
                        
                           (1)
                           
                              
                                 
                                    x
                                 
                                 
                                    ^
                                 
                              
                              =
                              arg
                              
                              
                                 
                                    min
                                 
                                 
                                    x
                                 
                              
                              {
                              ‖
                              
                                 
                                    Φ
                                 
                                 
                                    measurment
                                 
                              
                              y
                              -
                              
                                 
                                    Φ
                                 
                                 
                                    measurment
                                 
                              
                              A
                              x
                              
                                 
                                    ‖
                                 
                                 
                                    2
                                 
                              
                              +
                              λ
                              ‖
                              x
                              
                                 
                                    ‖
                                 
                                 
                                    1
                                 
                              
                              }
                              ,
                           
                        
                     where 
                        A
                      is a sparsifying basis for y and x is a sparse vector in the 
                        A
                      space. The scalar 
                        
                           λ
                        
                      is a constant weight used to balance between the fidelity (
                        
                           
                              
                                 ℓ
                              
                              
                                 2
                              
                           
                        
                      norm) and the sparseness (
                        
                           
                              
                                 ℓ
                              
                              
                                 1
                              
                           
                        
                      norm) terms. In contrast, in the missing feature [2] problem, we have a noisy signal 
                        
                           
                              
                                 y
                              
                              
                                 ̃
                              
                           
                           =
                           y
                           +
                           n
                        
                      that parts of it is destroyed by a random additive noise (
                        
                           n
                        
                     ). Discarding the destroyed parts and modeling this effect of noise with a measurement matrix 
                        
                           
                              
                                 Φ
                              
                              
                                 noise
                              
                           
                        
                      such as 
                        
                           
                              
                                 Φ
                              
                              
                                 measurment
                              
                           
                        
                     , we could recover y from 
                        
                           b
                           =
                           
                              
                                 Φ
                              
                              
                                 noise
                              
                           
                           
                              
                                 y
                              
                              
                                 ̃
                              
                           
                        
                      (reliable components of observed noisy signal), using an algorithm similar to the recovery algorithm of CS.

Here, 
                        A
                      could be a sparsifying basis for y 
                     [10] or a dictionary consisted of exemplars of y 
                     [11]. The clean estimate of 
                        
                           
                              
                                 y
                              
                              
                                 ̃
                              
                           
                        
                      would be 
                        A
                        
                           
                              x
                           
                           
                              ^
                           
                        
                     .

The main basis of the missing feature methods is established on the inherent information redundancy in speech signal. This means that the information in speech signal is not limited to a certain range of its time–frequency spectrum. For more explanation, it can be referred to the performed experiments in [12,13] showing that even with a small part of speech spectrum, still the remaining signal is understandable to the human listeners. On the other hand, in numerous references it has been pointed out that speech signal has considerable fractal properties in the time domain [14–16]. The fractal properties are closely related to self-similarity and in other words it means that different parts of speech, despite having time intervals from each other, could be potentially very similar and could have the same natures. This phenomenon motivated us to propose a method that leverages the self-similarity feature of speech signal to help better reconstruction of the missing spectral components of noisy speeches. The importance of this issue in the field of the missing feature is that if some of the spectral components of a frame are lost, one would hope that in another time position of the same signal, there are one or more less degraded frames with nearly similar components, which, along with reliable components of the given frame, can be used for better reconstruction of its unreliable components. In this manner, a higher efficiency of missing feature methods can be expected. In other words, the noisy frames that are similar in nature are reconstructed together, in a Joint manner. A similar approach, also known as the Multiple Measurement Vectors, is used in many applications such as the distributed compressive sensing [17,18] and the images noise removal [19].

In this paper, we aim to exploit the self-similarity property, available in speech signal using the Joint Sparsity approach to improve the performance of the Sparse Imputation method. For this purpose, we first offer a method based on the GMM clustering and marginalization process in order to be able to determine the similar speech frames, despite the destructive effect of noise on them. Then, the relations needed to use the Joint Sparsity approach inside the Sparse Imputation method are presented.

The paper is organized as follows, in Section 2.1 we introduce the Sparse Imputation method. In Section 2.2, details of the Joint Sparsity method and its mathematical relations are presented. In Section 2.3, the necessary relations for employing the Joint Sparsity are detailed. Moreover, the proposed method for determining the similar frames of speech is presented in this section. Section 3 specifies the exploited data set and the performed experiments. Implementation results and the discussion are presented in Section 4, and finally in Section 5 conclusions are presented.

@&#METHODS@&#

The Sparse Imputation method is mainly introduced to compensate the effect of noise over the speech features. For this purpose, Gemmeke [20] adapted the compressive sensing approach to estimate the unreliable features of the noisy speech. In this way, he created a dictionary 
                           A
                         from clean speech exemplars, in which, each exemplar is the Logarithm of the (Mel) Filter Bank Energies (LFBE) of a windowed speech frame. Assume that the clean feature vector y is affected by the additive noise 
                           
                              n
                           
                         and yielded the observed noisy feature vector 
                           
                              
                                 y
                              
                              
                                 ~
                              
                           
                           =
                           y
                           +
                           n
                        ; so, the effect of the noise may be presented by a binary vector, as given by
                           
                              (2)
                              
                                 m
                                 (
                                 i
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   1
                                                   
                                                      
                                                         =
                                                      
                                                      
                                                         def
                                                      
                                                   
                                                   reliable
                                                
                                                
                                                   if
                                                   
                                                   y
                                                   (
                                                   i
                                                   )
                                                   -
                                                   n
                                                   (
                                                   i
                                                   )
                                                   >
                                                   θ
                                                
                                             
                                             
                                                
                                                   0
                                                   
                                                      
                                                         =
                                                      
                                                      
                                                         def
                                                      
                                                   
                                                   unreliable
                                                
                                                
                                                   otherwise
                                                
                                             
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        where “1” means that the i-th frequency components of 
                           
                              
                                 y
                              
                              
                                 ~
                              
                           
                         is less affected by noise and is so reliable to represent a clean feature. On the other hand, “0” means that the information of corresponding frequency element is buried in the noise level and its clean counterpart should be estimated. The threshold value 
                           
                              θ
                           
                         may be obtained empirically. This process is called “mask estimation”; there are many different approaches [3–6] to estimate 
                           
                              m
                           
                        , which are not in the scope of this paper.

The vector 
                           
                              m
                           
                         is then converted to a measurement like matrix 
                           
                              
                                 
                                    Φ
                                 
                                 
                                    noise
                                 
                              
                           
                         and is put in (1). To do this, diagonal matrix 
                           
                              
                                 
                                    Φ
                                 
                                 
                                    noise
                                 
                              
                           
                         is constructed such as 
                           
                              Diag
                              (
                              
                                 
                                    Φ
                                 
                                 
                                    noise
                                 
                              
                              )
                              =
                              m
                           
                        . The estimated clean vector y is obtained by first solving
                           
                              (3)
                              
                                 
                                    
                                       x
                                    
                                    
                                       ^
                                    
                                 
                                 =
                                 arg
                                 
                                 
                                    
                                       min
                                    
                                    
                                       x
                                    
                                 
                                 {
                                 ‖
                                 
                                    
                                       Φ
                                    
                                    
                                       noise
                                    
                                 
                                 y
                                 -
                                 
                                    
                                       Φ
                                    
                                    
                                       noise
                                    
                                 
                                 A
                                 x
                                 
                                    
                                       ‖
                                    
                                    
                                       2
                                    
                                 
                                 +
                                 λ
                                 ‖
                                 x
                                 
                                    
                                       ‖
                                    
                                    
                                       1
                                    
                                 
                                 }
                                 ,
                              
                           
                        and then, evaluating 
                           
                              
                                 y
                              
                              
                                 ~
                              
                           
                           =
                           A
                           
                              
                                 x
                              
                              
                                 ^
                              
                           
                        . Assuming that noise is additive, the estimated components are bounded by the observed noisy components, as an upper bound for them.

The matrix 
                           
                              
                                 
                                    Φ
                                 
                                 
                                    noise
                                 
                              
                           
                         in (3) is responsible for picking the reliable elements of 
                           
                              
                                 y
                              
                              
                                 ~
                              
                           
                        , and also keeping the corresponding row vectors of 
                           A
                        . In this method, dictionary 
                           A
                         should contain enough number of exemplars of clean speech feature vectors. In [20], it is proposed to integrate the context by adding adjacent frames (between 5 and 30 adjacent frames) to exemplars of the dictionary.

The block diagram of this method is illustrated in Fig. 1
                        . In this figure, the training and test phases are divided by a straight dashed line. The first block from left in test phase represents the mask estimation process and is corresponding to (2). In second block, the estimated 
                           
                              
                                 
                                    Φ
                                 
                                 
                                    noise
                                 
                              
                           
                         is applied over the prepared dictionary from train phase and finally, the reconstruction process is conducted in the third block according to (3). In this manner, each frame is reconstructed separately without any knowledge of other similar frames.

If we know that the sparse solutions of some of the measurements are defined in approximately the same range of the dictionary members, the Joint Sparsity approach can then be adapted to solve the problem [21]:
                           
                              (4)
                              
                                 
                                    
                                       min
                                    
                                    
                                       X
                                    
                                 
                                 
                                 ‖
                                 X
                                 
                                    
                                       ‖
                                    
                                    
                                       w
                                       ,
                                       2
                                       ,
                                       1
                                    
                                 
                                 
                                 s
                                 .
                                 t
                                 .
                                 
                                 A
                                 X
                                 =
                                 B
                                 .
                              
                           
                        Here, the sparse solution of the given measurements constitutes the columns of matrix X. Columns of matrix B are also the same corresponding measurements. Vector x
                        
                           i
                         is the ith row of matrix X. Operator 
                           
                              
                                 
                                    
                                       
                                          •
                                       
                                    
                                 
                                 
                                    w
                                    ,
                                    2
                                    ,
                                    1
                                 
                              
                           
                         is called weighted 
                           
                              
                                 
                                    ℓ
                                 
                                 
                                    2
                                    ,
                                    1
                                 
                              
                           
                         norm and is defined as follows:
                           
                              (5)
                              
                                 ‖
                                 X
                                 
                                    
                                       ‖
                                    
                                    
                                       w
                                       ,
                                       2
                                       ,
                                       1
                                    
                                 
                                 
                                 ≔
                                 
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       N
                                    
                                 
                                 
                                    
                                       w
                                    
                                    
                                       i
                                    
                                 
                                 ‖
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 
                                    
                                       ‖
                                    
                                    
                                       2
                                    
                                 
                                 
                              
                           
                        where N is the number of samples presented in dictionary 
                           A
                        .

By changing the arrangement of vectors and matrices as follows, the problem will be converted into a group sparsity problem [21]:
                           
                              (6)
                              
                                 
                                    
                                       A
                                    
                                    
                                       ′
                                    
                                 
                                 ≔
                                 
                                    
                                       I
                                    
                                    
                                       1
                                    
                                 
                                 ⊗
                                 A
                                 =
                                 
                                    
                                       
                                          
                                             
                                                A
                                             
                                             
                                             
                                             
                                                ∅
                                             
                                          
                                          
                                             
                                             
                                                A
                                             
                                             
                                             
                                          
                                          
                                             
                                             
                                             
                                                ⋱
                                             
                                             
                                          
                                          
                                             
                                                ∅
                                             
                                             
                                             
                                             
                                                A
                                             
                                          
                                       
                                    
                                 
                                 ,
                                 
                                 
                                    
                                       x
                                    
                                    
                                       ′
                                    
                                 
                                 ≔
                                 vec
                                 (
                                 X
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      1
                                                   
                                                
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      2
                                                   
                                                
                                             
                                          
                                          
                                             
                                                ⋮
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      L
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                                 ,
                                 
                                 
                                    
                                       b
                                    
                                    
                                       ′
                                    
                                 
                                 ≔
                                 vec
                                 (
                                 B
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      b
                                                   
                                                   
                                                      1
                                                   
                                                
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      b
                                                   
                                                   
                                                      2
                                                   
                                                
                                             
                                          
                                          
                                             
                                                ⋮
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      b
                                                   
                                                   
                                                      L
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                                 ,
                              
                           
                        where the sparse vectors x
                        
                           i
                         are converted into a large sparse vector by setting them on each other. The same arrangement is done on the vectors b
                        
                           i
                        . Matrix 
                           A
                         is extended in an appropriate manner as shown in (6). By grouping the components related to each row of matrix X in vector 
                           
                              
                                 x
                              
                              
                                 ′
                              
                           
                        , this problem is then rewritten in the form of a group sparsity problem as follows:
                           
                              (7)
                              
                                 
                                    
                                       min
                                    
                                    
                                       
                                          
                                             x
                                          
                                          
                                             ′
                                          
                                       
                                    
                                 
                                 
                                 ‖
                                 
                                    
                                       x
                                    
                                    
                                       ′
                                    
                                 
                                 
                                    
                                       ‖
                                    
                                    
                                       w
                                       ,
                                       2
                                       ,
                                       1
                                    
                                 
                                 
                                 s
                                 .
                                 t
                                 .
                                 
                                 
                                    
                                       A
                                    
                                    
                                       ′
                                    
                                 
                                 
                                    
                                       x
                                    
                                    
                                       ′
                                    
                                 
                                 =
                                 
                                    
                                       b
                                    
                                    
                                       ′
                                    
                                 
                                 .
                              
                           
                        
                     

Using this scheme, not only the Eq. (4) can be solved for each b
                        
                           i
                        , but also a different matrix 
                           A
                         can be used for each b
                        
                           i
                        , which its benefits are introduced in the next section.

As discussed in the introduction, one of the proven properties of speech signal is its self-similarity. This means that for each frame in an utterance (long enough), there are a number of nearly similar frames in the same utterance. So, by finding the similar frames and put them together, Joint Sparsity can be used to reconstruct unreliable components of them in a united process, considering the reliable remained components of them together. In this section we present the proposed method dubbed as “Joint sparse Imputation”. In this method, the power of the Joint Sparsity approach is incorporated inside the original Sparse Imputation method. In this manner, the nearly acoustically similar frames of noisy speech are reconstructed jointly. To realize this approach, a method to find similar frames is also proposed in which, only the remained reliable components of the frames are used. Moreover, a complementary approach to reduce the computational cost of the entire process is proposed, which also increases the imputation performance of the proposed method.

Assume that the feature vectors derived from the frames of a speech utterance are divided into K clusters. So the vectors belonged to the kth cluster are supposed to be mostly similar. Since, the feature vectors of the frames included in a cluster are degraded differently by noise, each one will have its own 
                           
                              
                                 
                                    Φ
                                 
                                 
                                    noise
                                 
                              
                           
                         matrix. This matrix must be correspondingly applied to the dictionary samples. So, we define a dictionary matrix and an observed vector for kth cluster as given by
                           
                              (8)
                              
                                 
                                    
                                       B
                                    
                                    
                                       
                                          
                                             Φ
                                          
                                          
                                             k
                                          
                                       
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      Φ
                                                   
                                                   
                                                      1
                                                      ,
                                                      k
                                                   
                                                
                                                
                                                   
                                                      b
                                                   
                                                   
                                                      1
                                                      ,
                                                      k
                                                   
                                                
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      Φ
                                                   
                                                   
                                                      2
                                                      ,
                                                      k
                                                   
                                                
                                                
                                                   
                                                      b
                                                   
                                                   
                                                      2
                                                      ,
                                                      k
                                                   
                                                
                                             
                                          
                                          
                                             
                                                ⋮
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      Φ
                                                   
                                                   
                                                      
                                                         
                                                            L
                                                         
                                                         
                                                            k
                                                         
                                                      
                                                      ,
                                                      k
                                                   
                                                
                                                
                                                   
                                                      b
                                                   
                                                   
                                                      
                                                         
                                                            L
                                                         
                                                         
                                                            k
                                                         
                                                      
                                                      ,
                                                      k
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                                 ,
                                 
                                 
                                    
                                       A
                                    
                                    
                                       
                                          
                                             Φ
                                          
                                          
                                             k
                                          
                                       
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      Φ
                                                   
                                                   
                                                      1
                                                      ,
                                                      k
                                                   
                                                
                                                A
                                             
                                             
                                             
                                             
                                          
                                          
                                             
                                             
                                                
                                                   
                                                      Φ
                                                   
                                                   
                                                      2
                                                      ,
                                                      k
                                                   
                                                
                                                A
                                             
                                             
                                             
                                          
                                          
                                             
                                             
                                             
                                                ⋱
                                             
                                             
                                          
                                          
                                             
                                             
                                             
                                             
                                                
                                                   
                                                      Φ
                                                   
                                                   
                                                      
                                                         
                                                            L
                                                         
                                                         
                                                            k
                                                         
                                                      
                                                      ,
                                                      k
                                                   
                                                
                                                A
                                             
                                          
                                       
                                    
                                 
                                 ,
                              
                           
                        where 
                           
                              
                                 b
                              
                              
                                 l
                                 ,
                                 k
                              
                           
                         represents lth observed noisy vector of kth cluster, and 
                           
                              
                                 
                                    Φ
                                 
                                 
                                    1
                                    ,
                                    k
                                 
                              
                           
                         is its related mask matrix. The number of vectors included in cluster k is shown by 
                           
                              
                                 
                                    L
                                 
                                 
                                    k
                                 
                              
                           
                        .

Now, the following equation can be used to reconstruct the members of the kth cluster:
                           
                              (9)
                              
                                 
                                    
                                       min
                                    
                                    
                                       
                                          
                                             x
                                          
                                          
                                             ′
                                          
                                       
                                    
                                 
                                 
                                 ‖
                                 
                                    
                                       x
                                    
                                    
                                       ′
                                    
                                 
                                 
                                    
                                       ‖
                                    
                                    
                                       w
                                       ,
                                       2
                                       ,
                                       1
                                    
                                 
                                 
                                 s
                                 .
                                 t
                                 .
                                 
                                 
                                    
                                       A
                                    
                                    
                                       
                                          
                                             Φ
                                          
                                          
                                             k
                                          
                                       
                                    
                                 
                                 
                                    
                                       x
                                    
                                    
                                       ′
                                    
                                 
                                 =
                                 
                                    
                                       B
                                    
                                    
                                       
                                          
                                             Φ
                                          
                                          
                                             k
                                          
                                       
                                    
                                 
                              
                           
                        
                     

By evaluation of 
                           
                              
                                 x
                              
                              
                                 ′
                              
                           
                        , the reconstructed vectors could be obtained as follows:
                           
                              (10)
                              
                                 
                                    
                                       y
                                    
                                    
                                       l
                                       ,
                                       k
                                    
                                 
                                 =
                                 A
                                 
                                    
                                       
                                          
                                             x
                                          
                                          
                                             ~
                                          
                                       
                                    
                                    
                                       
                                          
                                             (
                                             (
                                             l
                                             -
                                             1
                                             )
                                          
                                          
                                             ∗
                                          
                                       
                                       N
                                       +
                                       1
                                       :
                                       
                                          
                                             l
                                          
                                          
                                             ∗
                                          
                                       
                                       N
                                       )
                                    
                                 
                              
                           
                        where 
                           
                              
                                 y
                              
                              
                                 l
                                 ,
                                 k
                              
                           
                         is the lth estimated clean vectors of group k.

Prior to applying the above equations, the similar spectral frames should be grouped despite the destructive effects of noise on them. For this purpose, some modified clustering methods should be applied to speech frames. In this case, there are two major problems. First, since the observed frames are noisy, some of the spectral components are unreliable; therefore, the desired clustering method should be changed so that it becomes usable for missing data condition. Second, the clustering process should be distinctively run for the frames of an observed utterance that certainly would not be efficient in terms of the computational cost.

In order to solve these two problems, a method based on a simple classification approach is proposed. First, the exemplars of dictionary, which were previously extracted from a clean database and nearly cover its variations, are grouped in different clusters by applying a clustering method. At the stage of identifying the cluster of a speech frame, a marginalized classification method is then used for classifying the noisy vectors which has relatively a low computational cost.

This is done first by applying the Gaussian Mixture Model (GMM) clustering approach over the dictionary samples. Next, the posterior probabilities of 
                           
                              
                                 
                                    y
                                 
                                 
                                    ̃
                                 
                              
                           
                         against different components of the evaluated GMM are used as the implemented classification criteria. These are exactly the responsibilities of the GMM clusters for 
                           
                              
                                 
                                    y
                                 
                                 
                                    ̃
                                 
                              
                           
                        , as given by
                           
                              (11)
                              
                                 P
                                 (
                                 k
                                 |
                                 
                                    
                                       y
                                    
                                    
                                       ̃
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       p
                                       (
                                       
                                          
                                             y
                                          
                                          
                                             ̃
                                          
                                       
                                       |
                                       k
                                       )
                                       ·
                                       p
                                       (
                                       k
                                       )
                                    
                                    
                                       p
                                       (
                                       
                                          
                                             y
                                          
                                          
                                             ̃
                                          
                                       
                                       )
                                    
                                 
                                 .
                              
                           
                        
                     

Since p(
                           
                              
                                 
                                    y
                                 
                                 
                                    ̃
                                 
                              
                           
                        ) is the same for different GMM components, it could be ignored while comparing the responsibilities of the components. p(k) is the prior probability of the k-th cluster evaluated in the training phase of the GMM. Direct evaluation of p(
                           
                              
                                 
                                    y
                                 
                                 
                                    ̃
                                 
                              
                           
                        |k) is not valid because 
                           
                              
                                 
                                    y
                                 
                                 
                                    ̃
                                 
                              
                           
                         may have some unreliable parts. So, a marginalization process similar to [22] is exploited to evaluate p(
                           
                              
                                 
                                    y
                                 
                                 
                                    ̃
                                 
                              
                           
                        |k) as given by
                           
                              (12)
                              
                                 p
                                 (
                                 
                                    
                                       y
                                    
                                    
                                       ~
                                    
                                 
                                 |
                                 k
                                 )
                                 =
                                 
                                    
                                       ∏
                                    
                                    
                                       i
                                       |
                                       m
                                       (
                                       i
                                       )
                                       =
                                       1
                                    
                                 
                                 N
                                 (
                                 
                                    
                                       y
                                    
                                    
                                       ̃
                                    
                                 
                                 (
                                 i
                                 )
                                 |
                                 
                                    
                                       μ
                                    
                                    
                                       k
                                    
                                 
                                 (
                                 i
                                 )
                                 ,
                                 
                                    
                                       σ
                                    
                                    
                                       k
                                    
                                 
                                 (
                                 i
                                 )
                                 )
                                 ×
                                 
                                    
                                       ∏
                                    
                                    
                                       i
                                       |
                                       m
                                       (
                                       i
                                       )
                                       =
                                       0
                                    
                                 
                                 
                                    
                                       ∫
                                    
                                    
                                       -
                                       ∞
                                    
                                    
                                       
                                          
                                             y
                                          
                                          
                                             ~
                                          
                                       
                                       (
                                       i
                                       )
                                    
                                 
                                 N
                                 (
                                 x
                                 |
                                 
                                    
                                       μ
                                    
                                    
                                       k
                                    
                                 
                                 (
                                 i
                                 )
                                 ,
                                 
                                    
                                       σ
                                    
                                    
                                       k
                                    
                                 
                                 (
                                 i
                                 )
                                 )
                                 dx
                                 ,
                              
                           
                        where 
                           
                              
                                 y
                              
                              
                                 ̃
                              
                           
                        (i) and 
                           
                              
                                 μ
                              
                              
                                 k
                              
                           
                           (
                           i
                           )
                         respectively represent the i-th element of the observed vector and the mean vector of cluster k, and 
                           
                              
                                 σ
                              
                              
                                 k
                              
                           
                           (
                           i
                           )
                         is the i-th element of the main diagonal of the covariance matrix of cluster k. Through this approach, all the observed vectors of an utterance are assigned to one of the K clusters of the dictionary.

Using the 
                           
                              
                                 
                                    ℓ
                                 
                                 
                                    2
                                    ,
                                    1
                                 
                              
                           
                         norm instead of the 
                           
                              
                                 
                                    ℓ
                                 
                                 
                                    1
                                 
                              
                           
                         norm in the proposed method causes a considerable increase in the final computational cost of the Algorithm. Moreover, comparing 
                           
                              
                                 A
                              
                              
                                 
                                    
                                       Φ
                                    
                                    
                                       k
                                    
                                 
                              
                           
                         and 
                           
                              
                                 B
                              
                              
                                 
                                    
                                       Φ
                                    
                                    
                                       k
                                    
                                 
                              
                           
                         with their counterpart in the Sparse Imputation (
                           
                              
                                 Φ
                              
                              
                                 noise
                              
                           
                           A
                         and 
                           
                              
                                 
                                    Φ
                                 
                                 
                                    noise
                                 
                              
                              y
                           
                        ) shows that their dimensions have grown with factor of 
                           
                              
                                 
                                    L
                                 
                                 
                                    k
                                 
                              
                           
                        . These factors cause a huge increase in the complexity of the algorithm. So, to face this inefficiency, it is proposed that during solving (9) for an observed vector assigned to cluster k, only the exemplars of the dictionary that belongs to cluster k, denoted by 
                           A
                           k
                        , are involved. So, 
                           
                              
                                 A
                              
                              
                                 
                                    
                                       Φ
                                    
                                    
                                       k
                                    
                                 
                              
                           
                         reduces to:
                           
                              (13)
                              
                                 
                                    
                                       A
                                    
                                    
                                       
                                          
                                             Φ
                                          
                                          
                                             k
                                          
                                       
                                       ,
                                       Reduced
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      Φ
                                                   
                                                   
                                                      1
                                                      ,
                                                      k
                                                   
                                                
                                                
                                                   
                                                      A
                                                   
                                                   
                                                      k
                                                   
                                                
                                             
                                             
                                             
                                             
                                          
                                          
                                             
                                             
                                                
                                                   
                                                      Φ
                                                   
                                                   
                                                      2
                                                      ,
                                                      k
                                                   
                                                
                                                
                                                   
                                                      A
                                                   
                                                   
                                                      k
                                                   
                                                
                                             
                                             
                                             
                                          
                                          
                                             
                                             
                                             
                                                ⋱
                                             
                                             
                                          
                                          
                                             
                                             
                                             
                                             
                                                
                                                   
                                                      Φ
                                                   
                                                   
                                                      
                                                         
                                                            L
                                                         
                                                         
                                                            k
                                                         
                                                      
                                                      ,
                                                      k
                                                   
                                                
                                                
                                                   
                                                      A
                                                   
                                                   
                                                      k
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

By solving (9), while using 
                           
                              
                                 A
                              
                              
                                 
                                    
                                       Φ
                                    
                                    
                                       k
                                    
                                 
                                 ,
                                 Reduced
                              
                           
                         instead of 
                           
                              
                                 A
                              
                              
                                 
                                    
                                       Φ
                                    
                                    
                                       k
                                    
                                 
                              
                           
                        , the reconstructed vector is given by
                           
                              (14)
                              
                                 
                                    
                                       y
                                    
                                    
                                       l
                                       ,
                                       k
                                    
                                 
                                 =
                                 
                                    
                                       A
                                    
                                    
                                       k
                                    
                                 
                                 
                                    
                                       
                                          
                                             x
                                          
                                          
                                             ~
                                          
                                       
                                    
                                    
                                       (
                                       (
                                       l
                                       -
                                       1
                                       )
                                       ∗
                                       N
                                       +
                                       1
                                       :
                                       l
                                       ∗
                                       N
                                       )
                                    
                                 
                              
                           
                        
                        Fig. 2
                         represents the block diagram of the Joint Sparse Imputation method. In this figure, the train and test phases are divided by a straight dashed line. In the train phase, after generating the dictionary and completing the training phase of a GMM over it, in the last block, the dictionary exemplars are classified to the predefined number of clusters, using the trained GMM model. In the first block of the test phase, the marginalized classification is performed according to (12). In the middle block, equation (13) is implemented; finally, in the third block, the reconstruction process is simultaneously performed over all frames of each cluster, using (9).

In order to evaluate the benefits of the proposed method, an ASR task in a wide variety of noise conditions is implemented. The experiments are conducted on the common connected digits telephone database, known as AURORA 2 [23]. This database is intended for evaluation of noise robustness techniques, and is constructed from filtered connected digit utterances sampled at 8kHz to emulate the telephone line condition. Different kinds of noise signals with 6 levels of SNR (−5, 0, 5, 10, 15, 20dB) are artificially added to the clean signals to form noisy utterances. This database consists of two training sets (clean and multi-condition) and three test sets (A, B and C) where noise types used in test set A and the multi-condition training set are the same (subway, babble, exhibition and car noise). Each training set contains 8440 utterances with different levels of SNR; there are 1001 utterances for each level of SNR. In this paper, the test set A is employed to conduct the experiments.

The clean training set is employed to train the acoustic models and to construct the needed dictionary for the Sparse Imputation method. The noisy utterances corresponding to babble, exhibition and car noise from test set A, in four SNR levels of −5, 0, 5, 10dB are employed as the selected test set. The remained noisy utterances of the test set A, which are polluted by the subway noise, are used as the development set in the experiments. The details of AURORA 2 corpus is summarized in Table 1
                        .

To accomplish the experiments, two types of features are extracted from 25ms hamming windowed speech frames with time shift of 10ms: First, 13 MFCC coefficients, plus their 13 delta and 13 delta-delta derivatives. The Cepstral Mean Normalization (CMN) is also applied to the features. This feature set is used to train the ASR acoustic models. The second type of features is defined as a 23-dimensional LFBE vector obtained by applying 23 filters of the Mel filter bank to the speech frame spectrum and then applying the logarithm function to the outputs of the filters. In addition, as suggested in the original Sparse Imputation method [7], a sliding window with the length of 11 frames with window shift of 1 is slipped over the frames to incorporate a longer temporal context to them. So, the final LFBE vector would have a dimension of 251=23×11. The imputation process is performed on this feature set and then it will be de-windowed and converted to the first type of features (MFCC) by applying the discrete cosine transform (DCT) to be ready to apply to the ASR system.

The ASR system employed for performing the experiments is based on the Hidden Markov Models (HMM) and is developed using HTK 3.4 software package [24]. Moreover, the HTK configuration system provided in AURORA 2 package is chosen as the HMMs’ setting. By applying this configuration, each whole uttered digit is distinctively modeled with a 16 states, left to right HMM. Each state of the HMMs is comprised of 3–6 diagonal Gaussian components.

2000 clean speech segments (of length T
                        =11 frames) randomly picked from the training set of the speech corpus form the dictionary needed for the imputation process. The LFBE features are evaluated for the selected frames and are put in the dictionary columns. The exemplars of the dictionary are normalized to have unit Euclidean length. The pilot tests showed that the order by which the dictionary samples are picked does not considerably affect the final results. So, the used dictionary is kept fixed through all the experiments.

The GMM used in the marginal classification process has 12 components and, as mentioned in Section 2.3, is trained over the exemplars of the dictionary. The number of components is chosen empirically over a part of the development set by conducting some experiments, in which different magnitudes of K were examined and evaluated over the defined task. Although, one could train this GMM with a data set larger than the constructed dictionary, just only the dictionary samples are employed in this step to ensure that the proposed method have used the same amount of data used in the SI method to have a fair comparison between them in the following conducted experiments.

The YALL 1 toolbox [21]
                        
                           1
                           
                              http://yall1.blogs.rice.edu/.
                        
                        
                           1
                         is used to solve (9) in the proposed algorithm. This MATLAB toolbox is designed especially to solve the group and Joint Sparsity problems and allows users to define different dictionaries for different groups.

The implemented methods converged in less than 1000 iterations; so, the limit of the iteration number is set to 1000 in all the conducted experiments.

The experiments of this work are mainly designed in order to concentrate on the reconstruction performance of the implemented methods and omit other factors that can affect the recognition performance. One of these factors is the mask estimation process, formulated in (2). Since, development of this process is out of scope of this paper, we construct 
                           
                              
                                 
                                    Φ
                                 
                                 
                                    noise
                                 
                              
                           
                         for noisy feature vectors, using the oracle knowledge of clean counterpart of noisy utterance [1]. In the current research, accepting this simple method as the employed masking process lets us to only concentrate over the following reconstruction methods. The value of the m must be evaluated by the element wise comparison detailed in (2). The optimal values for the threshold 
                           
                              θ
                           
                         for all the implemented methods are distinctively obtained over the development set. Moreover, as recommended in [20], small isolated regions of the reliable features are discarded.


                     Fig. 3
                      shows the average percentage of word recognition accuracy obtained over the speech signals polluted by three different noises of babble, car and exhibition, versus different noise levels. In this figure, the result of recognition of the test set without applying any reconstruction method is specified by the “baseline” method. The proposed method is shown briefly by “JSI” (Joint Sparsity Imputation). SI_YALL represents the original SI method where the YALL toolkit is used to solve the involved minimization problem. The solving algorithm of L1 norm minimization should be the same for a fair comparison between the proposed and the original SI methods; but, regarding SI_YALL poor results, the experiment is repeated another time by employing the FISTA algorithm [25]. This approach is briefly specified as SI_FISTA. The preliminary experiments show that using FISTA algorithm leads to better results than the use of sparse lab
                        2
                        
                           http://www.sparselab.stanford.edu.
                     
                     
                        2
                      toolbox suggested in [20]. Also, “Multi” denotes the experiments in which the ASR models are trained using the multi-condition data set described in Section 3.1.

The computational costs of the implemented methods are given in Table 2
                     . The values shown in this table are the average time needed to reconstruct one frame of a speech signal in 5dB babble noise conditions, via three different approaches. To conduct the experiments a machine with core i7-2600k processor with 8GB of RAM is employed, and the involved codes are developed and implemented inside the MATLAB software.

In Table 2, the second and third rows are both related to the proposed method implemented in two different ways: in the first one, related to the second row of the table, the dictionary matrix presented in (8) is used inside the relation (9); but instead, the second one uses the reduced dictionary presented in (13). As shown in the table, using the dictionary presented in (13) considerably reduces the complexity of the proposed method (more than 6 times). It is worth mentioning that this scheme not only decreases the complexity of the proposed method, but also improves the performance of the reconstruction process. Fig. 4
                      shows the recognition accuracies obtained via the proposed method (JSI) in 2 cases of using 
                        
                           
                              A
                           
                           
                              
                                 
                                    Φ
                                 
                                 
                                    k
                                 
                              
                           
                        
                      and 
                        
                           
                              A
                           
                           
                              
                                 
                                    Φ
                                 
                                 
                                    k
                                 
                              
                              ,
                              Reduced
                           
                        
                      dictionaries, for the babble noise condition. Moreover, to compare the proposed method with the original SI method, the recognition results obtained via applying the SI_FISTA method are also showed in Fig. 4.

As shown in this figure, using 
                        
                           
                              A
                           
                           
                              
                                 
                                    Φ
                                 
                                 
                                    k
                                 
                              
                              ,
                              Reduced
                           
                        
                      despite increasing the running speed of the SI process, significantly improves the recognition performance. This is because that in the case of using 
                        
                           
                              A
                           
                           
                              
                                 
                                    Φ
                                 
                                 
                                    k
                                 
                              
                              ,
                              Reduced
                           
                        
                     , to reconstruct the missed part of data, the involved dictionary exemplars are selected corresponding to the measured vector (
                        
                           
                              b
                           
                           
                              l
                              ,
                              k
                           
                        
                     ), and the atoms unrelated to 
                        
                           
                              b
                           
                           
                              l
                              ,
                              k
                           
                        
                      are preliminary excluded from the following decomposition search. In this manner, the production rate of the artifacts that may be produced inside the reconstruction process considerably decreases.

As shown in Fig. 3, in terms of the final recognition performance, the proposed JSI method towers above the rest methods in all SNRs, except the −5dB case. This is because that in presence of such a high level of noise, the performance of the speech frames classification process, needed inside the overall process, dramatically decreases. Fig. 5
                      shows the frame classification accuracy versus the speech signal SNR, in five different levels of noise. In this figure, the reference tags needed to evaluate the frame level classification accuracy in the noisy conditions are obtained by applying the marginal classification process, included in the proposed JSI method, to the clean speech data.

According to Fig. 5, as the SNR decreases, the frame classification error dramatically increases. As a result, while implementing the JSI process, a number of unrelated speech frames might be placed in the same groups. So, their joint measurements not only do not help the imputation process, but also may cause problems in the reconstruction process and introduce serious artifacts in many places of the reconstructed signal.

@&#CONCLUSION@&#

In this paper, an efficient modification over the original Sparse Imputation method, employing a complementary idea, was proposed. Regarding the inherent redundancy in speech signal and its self-similarity properties, the joint information over the non-adjacent frames were considered. According to the similarity of these frames, it is expected that the evaluated weights for the dictionary exemplars must be mostly close for them. So, to realize this approach, the SI relations were rewritten in the form of a Joint Sparsity problem where similar noisy frames are reconstructed together. Moreover, a low complexity GMM-based classification method, supported by a marginalization process, was exploited to find (not necessarily adjacent) similar noisy frames in a processed utterance. To further reduce the complexity of the proposed approach a solution based on clustering dictionary was also proposed. Implementation results, averaged over three kinds of noise at 5 different noise level of AURORA 2 showed that this method is well able to reconstruct the missing components and outperforms the original SI method for about 2.4% absolute improvement in the average recognition accuracy.

@&#REFERENCES@&#

