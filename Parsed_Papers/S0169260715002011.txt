@&#MAIN-TITLE@&#Multi-modality sparse representation-based classification for Alzheimer's disease and mild cognitive impairment

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Multi-modality classification on 113 AD, 110 MCI patients and 117 normal controls.


                        
                        
                           
                           Originally single-modality SRC was extended as a multi-modality framework (wmSRC).


                        
                        
                           
                           The wmSRC performed better than each single-modality based SRC method.


                        
                        
                           
                           The wmSRC performed better or equally well compared to MKL, RF and JRC.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Alzheimer's disease (AD)

Mild cognitive impairment (MCI)

Multi-modality

Neuroimaging data

Sparse representation-based classification (SRC)

@&#ABSTRACT@&#


               
               
                  Background and objective
                  The discrimination of Alzheimer's disease (AD) and its prodromal stage known as mild cognitive impairment (MCI) from normal control (NC) is important for patients’ timely treatment. The simultaneous use of multi-modality data has been demonstrated to be helpful for more accurate identification. The current study focused on extending a multi-modality algorithm and evaluating the method by identifying AD/MCI.
               
               
                  Methods
                  In this study, sparse representation-based classification (SRC), a well-developed method in pattern recognition and machine learning, was extended to a multi-modality classification framework named as weighted multi-modality SRC (wmSRC). Data including three modalities of volumetric magnetic resonance imaging (MRI), fluorodeoxyglucose (FDG) positron emission tomography (PET) and florbetapir PET from the Alzheimer's disease Neuroimaging Initiative database were adopted for AD/MCI classification (113 AD patients, 110 MCI patients and 117 NC subjects).
               
               
                  Results
                  Adopting wmSRC, the classification accuracy achieved 94.8% for AD vs. NC, 74.5% for MCI vs. NC, and 77.8% for progressive MCI vs. stable MCI, superior to or comparable with the results of some other state-of-the-art models in recent multi-modality researches.
               
               
                  Conclusions
                  The wmSRC method is a promising tool for classification with multi-modality data. It could be effective for identifying diseases from NC with neuroimaging data, which could be helpful for the timely diagnosis and treatment of diseases.
               
            

@&#INTRODUCTION@&#

Alzheimer's disease (AD) is the most common form of dementia [1]. The identification of AD and its prodromal stage named as mild cognitive impairment (MCI) from normal control (NC) has attracted much attention in recent decades. Biomarkers based on various neuroimaging modalities such as volumetric magnetic resonance imaging (MRI) and positron emission tomography (PET) measuring either metabolic or pathological burden with different radioactive tracers have been considered to discriminate AD or MCI with promising results [2–4].

MRI, which measures the structure of the cerebrum, has turned out to be an efficient tool for detecting the structural changes caused by AD or MCI. For example, the brain atrophy of spatial patterns or the atrophy of brain regions such as hippocampal and parahippocampal have been characterized as efficient biomarkers for the prediction of conversion from MCI to the subsequent AD [5–7]. Furthermore, fluorodeoxyglucose PET (FDG-PET), a technique for measuring glucose metabolism, is also a sensitive biomarker for the detection of AD or MCI. Plenty of FDG-PET studies have identified distinct abnormalities patterns of brain metabolic in individuals that diagnosed with AD or MCI, such as the reduction of glucose metabolism in parietal, frontal and posterior cingulate cortices [6,8–10]. Most recently, the Pittsburgh compound B, florbetapir or flutematmol PET, a means to measure the accumulation of amyloid in the brain non-invasively, has been introduced and demonstrated to be promising for differentiating AD or MCI from NC, such as the higher uptake of florbetapir in the anterior and posterior cingulate cortex, frontal medial cortex for AD or MCI patients [11,12].

Each neuroimaging modality could offer valuable information for AD or MCI, and studies reported that biomarkers from different modalities could offer complementary information for different aspects of a given disease process [4,12–15]. Combining these potentially complementary pieces of information from various modalities have been suggested to produce more powerful classifiers [4,16–18]. As a matter of fact, several groups have reported that exploiting the combination of multi-modality data to identify AD or MCI outperforms that based on each single-modality data alone [2,19,20]. Though all these multi-modality explorations reported positive results, the performances of different algorithms (for multi-modality data integration) can vary, and should be interesting to compare.

Indeed, there have been numerous reports on various ways of combining multi-modality data for efficient classification. For example, a weighted multiple kernel learning (MKL) model has been widely applied to combine different modalities for AD or MCI classification [2,19,21]; a linear weighted random forest (RF) model can also efficiently discriminate AD or MCI from NC by combining different modalities [4]. Those studies demonstrate that the weighted combination approach is a simple-while-effective way for multi-modality analysis. Furthermore, methods that include joint feature selection for classification, such as the joint regression and classification (JRC) algorithm that was recently proposed by Zhu et al. [22,23] have been indicated to be effective in AD/MCI diagnosis with directly concatenating features from multi-modalities.

Sparse representation-based classification (SRC), a relative recently introduced method in pattern recognition and machine learning, has been put forward by Wright et al. and has been indicated to be an efficient tool in face recognition [24,25]. SRC assumes that, if there are sufficient training samples from each class, then each test sample can be expressed as a sparse linear combination of the training samples, and its class label can be assigned with minimum representation residual. Recently, Liu et al. [26] introduced SRC into neuroimaging communities and demonstrated its feasibility and effectiveness for discriminating AD or MCI from NC with MRI data. However, the use of SRC for multi-modality data and its performance for the classification of AD or MCI from NC have not been concerned a lot.

In this paper, data from three modalities i.e. MRI, FDG-PET and florbetapir PET were combined, and a weighted multi-modality SRC (wmSRC) method was extended and carried out to examine its robustness and the classification accuracy for AD/MCI. Our experimental results indicated that the wmSRC method could achieve better or comparable classification performance for both AD and MCI classification, compared with some other state-of-the-art multi-modality classification algorithms.

@&#MATERIAL AND METHODS@&#

The sparse representation-based classification (SRC) is first presented in this section; then the extended multi-modality framework based on SRC, named as weighted multi-modality SRC (wmSRC) is described to combine multi-modality data for classification.

SRC is put forward by Wright et al. [24], its classification model can be simply described as follows:

SRC uses sparse coding to denote a test sample by a linear combination of some atoms from a given dictionary, where the dictionary atoms are actually the training samples. Suppose D-dimensional N training samples from K classes A
                           =[A
                           1, …, A
                           
                              l
                           
                           …, A
                           
                              K
                           ]∈ℜ
                           
                              D×N
                           , where D is the number of features, N
                           =
                           N
                           1
                           +…+
                           N
                           
                              l
                           
                           +…+
                           N
                           
                              K
                           , and 
                              
                                 
                                    A
                                    l
                                 
                                 =
                                 [
                                 
                                    a
                                    1
                                    l
                                 
                                 ,
                                 …
                                 ,
                                 
                                    a
                                    i
                                    l
                                 
                                 ,
                                 …
                                 
                                    a
                                    
                                       
                                          N
                                          l
                                       
                                    
                                    l
                                 
                                 ]
                                 ∈
                                 
                                    ℜ
                                    
                                       D
                                       ×
                                       
                                          N
                                          l
                                       
                                    
                                 
                              
                            consists of N
                           
                              l
                            training samples from the l-th class, then for a test sample y
                           ∈
                           ℜ
                           
                              D
                           , the sparse coding can be written as:
                              
                                 (1)
                                 
                                    
                                       x
                                       =
                                       arg
                                        
                                       min
                                        
                                       
                                          
                                             
                                                x
                                             
                                          
                                          1
                                       
                                       ,
                                          
                                       subject
                                        
                                       to
                                       
                                          
                                             
                                                
                                                   A
                                                   x
                                                   −
                                                   y
                                                
                                             
                                          
                                          2
                                       
                                       ≤
                                       ε
                                       ,
                                    
                                 
                              
                           where 
                              
                                 
                                    
                                       
                                          .
                                       
                                    
                                    1
                                 
                              
                            represents the standard L1 norm, 
                              
                                 
                                    
                                       
                                          .
                                       
                                    
                                    2
                                 
                              
                            represents the standard Euclidean norm, ɛ
                           >0 is error tolerance, 
                              
                                 x
                                 =
                                 
                                    
                                       
                                          x
                                          1
                                       
                                       ,
                                       …
                                       ,
                                       
                                          x
                                          l
                                       
                                       ,
                                       …
                                       ,
                                       
                                          x
                                          K
                                       
                                    
                                 
                                 ∈
                                 
                                    ℜ
                                    N
                                 
                              
                            is the sparse coefficient, where x
                           
                              l
                            consists of N
                           
                              l
                            representation coefficients that corresponding to the l-th class.

After the sparse coding, the reconstruction residual from the l-th class can be denoted as
                              
                                 (2)
                                 
                                    
                                       
                                          r
                                          l
                                       
                                       (
                                       y
                                       )
                                       =
                                       
                                          
                                             
                                                
                                                   A
                                                   
                                                      x
                                                      l
                                                   
                                                   −
                                                   y
                                                
                                             
                                          
                                          2
                                       
                                       ,
                                          
                                       l
                                       =
                                       1
                                       ,
                                       …
                                       ,
                                       K
                                    
                                 
                              
                           
                        

Finally, the class label of the test sample 
                              y
                            can be assigned as the class with the minimum reconstruction residuals:
                              
                                 (3)
                                 
                                    
                                       label
                                       (
                                       y
                                       )
                                       =
                                       arg
                                        
                                       
                                          
                                             min
                                          
                                          l
                                       
                                       
                                          r
                                          l
                                       
                                       (
                                       y
                                       )
                                    
                                 
                              
                           
                        

The convex problems in Eq. (1) can be efficiently solved by plenty of tools, for example, the L1-magic software package [27], the GPSR package [28] and the L1-homotopy package [29]. Here in this study, the GPSR package was adopted to solve the optimization problem.

The above SRC model is defined for the classification of single-modality data. Several previous researches have shown that combining multi-modality features is helpful for classification and the weighted combination manner is efficient for taking full advantage of complementary information from different modalities [2,4,19].

Here, the weighted multi-modality SRC (wmSRC) is defined for classification. Suppose we have M modalities for each sample, the sparse coding of the test sample 
                              y
                            can be calculated for each modality as:
                              
                                 (4)
                                 
                                    
                                       
                                          x
                                          m
                                       
                                       =
                                       
                                          arg
                                       
                                        
                                       min
                                       
                                          
                                             
                                                
                                                   
                                                      x
                                                      m
                                                   
                                                
                                             
                                          
                                          1
                                       
                                       ,
                                          
                                       subject
                                        
                                       to
                                        
                                       
                                          
                                             
                                                
                                                   
                                                      A
                                                      m
                                                   
                                                   
                                                      x
                                                      m
                                                   
                                                   −
                                                   
                                                      y
                                                      m
                                                   
                                                
                                             
                                          
                                          2
                                       
                                       ≤
                                       ε
                                       ,
                                          
                                       m
                                       =
                                       1
                                       ,
                                       …
                                       ,
                                       M
                                    
                                 
                              
                           
                        

The residual from the l-th class of the m-th modality can be denoted as:
                              
                                 (5)
                                 
                                    
                                       
                                          r
                                          l
                                          m
                                       
                                       (
                                       y
                                       )
                                       =
                                       
                                          
                                             
                                                
                                                   
                                                      A
                                                      m
                                                   
                                                   
                                                      x
                                                      l
                                                      m
                                                   
                                                   −
                                                   y
                                                
                                             
                                          
                                          2
                                       
                                       ,
                                          
                                       m
                                       =
                                       1
                                       ,
                                       …
                                       ,
                                       M
                                       ;
                                        
                                       l
                                       =
                                       1
                                       ,
                                       …
                                       ,
                                       K
                                    
                                 
                              
                           
                        

Then the class label for the test sample 
                              y
                            will be:
                              
                                 (6)
                                 
                                    
                                       label
                                       (
                                       y
                                       )
                                       =
                                       arg
                                        
                                       
                                          
                                             min
                                          
                                          l
                                       
                                       
                                          ∑
                                          
                                             m
                                             =
                                             1
                                          
                                          M
                                       
                                       
                                          
                                             α
                                             m
                                          
                                          
                                             r
                                             l
                                             m
                                          
                                          (
                                          y
                                          )
                                       
                                    
                                 
                              
                           where α
                           
                              m
                           
                           ≥0 is the combining weight for the m-th modality, and α
                           
                              m
                            is constrained by 
                              
                                 
                                    ∑
                                    
                                       m
                                       =
                                       1
                                    
                                    M
                                 
                                 
                                    
                                       α
                                       m
                                    
                                    =
                                    1
                                 
                              
                           .

The combining weight parameters α can be optimized using a grid search approach through 10-fold cross-validation on the training samples as follows. First, the whole samples are randomly equally partitioned into 10 subsets, and each time 1 subset is taken as the test set while the remaining 9 subsets are denoted as the training set. Then, 10 times weights selection is performed on the training set. In particular, at each time, the weights for multiple modalities are determined through a grid search approach with the range of [0,1] at a step size of 0.1. Finally, the optimal weights will be determined as the average of the weights that obtained by the 10 times weights selection.

After determining the optimal value of weights α, the class labels for the test samples can be assigned according to Eqs. (4)–(6). Then the performance of wmSRC can be evaluated by calculating the accuracy, sensitivity and specificity on the test samples. Accuracy is the ratio of samples correctly classified among the whole test samples; sensitivity is the ratio of positive class that are correctly identified; while specificity is the ratio of negative class that are accurately classified.

Data used in this paper were downloaded from the Alzheimer's disease Neuroimaging Initiative (ADNI) database (http://www.adni-info.org/). ADNI studies include brain-imaging techniques such as MRI, PET (including FDG-PET and florbetapir PET). For more information, please see http://adni.loni.usc.edu/about/.

The participated subjects were between 55 and 90 years old, had a study partner able to provide an independent evaluation of functioning. The severity of cognitive impairment was assessed using Mini-Mental State Examination (MMSE) [30] and Clinical Dementia Rating (CDR) scores [31] according to ADNI protocols. Individuals met the National Institute of Neurological and Communicative Disorders and Stroke/Alzheimer's Disease and Related Disorders Association (NINCDS/ADRDA) criteria were assigned to the AD or MCI group [32].

In this paper, data from three modalities including MRI, FDG-PET and florbetapir PET were used. Thus, only subjects with all the three corresponding modalities cross-sectional data were included (acquired within 4 months). As a result, a total of 340 subjects were included, with 113 AD patients, 110 MCI patients (27 progressive MCI (pMCI), which converted to AD within 36 months; and the remaining 83 stable MCI (sMCI), which remained stable during the same time interval or drop-out of the study), and 117 NC subjects. The clinical and demographic information of all the subjects are listed in Table 1
                           , and their IDs in ADNI dataset are reported in Table S1.

The preprocessing of the MRI data was performed via Voxel-Based Morphometry 8 (VBM8) Toolbox (http://dbm.neuro.uni-jena.de/vbm8), which involved two main segmentation and normalization steps. First, each MRI image was segmented into a rigid-body aligned gray matter, white matter and cerebrospinal fluid using adaptive maximum posterior and partial volume estimation [33,34]. Then, the gray matter images obtained from the segmentation step were used for the following analysis: a diffeomorphic anatomical registration using exponential Lie algebra (DARTEL) protocol was applied to normalize the gray matter images [35] with template creation and image registration iteratively, and the gray matter maps were normalized to the Montreal Neurological Institute (MNI) space. Thereafter, the registered gray matter maps were multiplied by Jacobian determinants with only non-linear warping to exclude individual differences in total intracranial volume.

All FDG-PET images were first co-registered to each individual's MRI images using a rigid body transformation and subsequently warped to the cohort specific DARTEL template. Then the standard uptake value ratio (SUVr) image, defined in relation to the whole brain as the reference mask, was calculated for each FDG-PET image.

All florbetapir PET images were first co-registered to each individual's MRI images using a rigid body transformation and subsequently warped to the cohort specific DARTEL template. Then the standard uptake value ratio (SUVr) image, defined in relation to the cerebellum as the reference mask, was calculated for each florbetapir PET image.

90 regions of interest (ROIs) (45 for each hemisphere, see Table S2) were first defined according to a prior anatomical automatic labeling (AAL) atlas [36]. Then, for each subject, the mean volume of gray matter, SUVr value of FDG-PET and SUVr value of florbetapir PET from each ROI were computed by averaging the corresponding value of all the voxels within that ROI as the feature of MRI, FDG-PET and florbetapir PET respectively.

All the features extracted from three modalities data (MRI, FDG-PET and florbetapir PET) were adopted for the classification of AD from NC, MCI from NC, and pMCI from sMCI. To verify the effectiveness of wmSRC, the single-modality results based on SRC, and the multi-modality results from several other state-of-the-art multi-modality methods were compared with those from wmSRC.

The single-modality SRC was performed respectively on MRI, FDG-PET and florbetapir PET (denoted as SRC-MRI, SRC-FDG-PET and SRC-florbetapir PET), for the classification of AD from NC, MCI from NC, and pMCI from sMCI. Then the classification accuracy and the area under the ROC curve (AUC) were compared with those from wmSRC, which combined multi-modality data of MRI, FDG-PET and florbetapir PET.

The wmSRC method was compared with several other state-of-the-art multi-modality methods in this study. We were apparently not able to include all of numerous multi-modality methods and considered four of them.

The first we compared the wmSRC was the SRC-concatenation method. The most intuitive and simple way of combining multi-modality data, in the frame work of SRC, was direct feature concatenation into a huge vector. The second method was MKL that based on support vector machine (SVM). It combined kernels from different modalities with corresponding kernel weights [2,4,21]. The third method we examined was the RF method in [4]. Pairwise similarity matrices from each modality were combined in this algorithm. The fourth method was JRC that was recently proposed by Zhu et al. [22,23]. In JRC, features from multiple modalities were first concatenated into one vector, then high-level information inherent like sample–sample relations and variable–variable relations (variables indicated clinical scores and class label, we used the clinical score of MMSE and the class label as variables in this study) were adopted for joint feature selection. SVM was finally performed on the selected features for classification. All these multi-modality methods were applied on the MRI, FDG-PET and florbetapir PET data for the discrimination of AD from NC, MCI from NC and pMCI from sMCI.

The classification performance from wmSRC was first compared with those from SRC-concatenation, MKL, RF and JRC methods, based on all the 90 features of each modality (without feature selection for wmSRC, SRC-concatenation, MKL and RF; note that JRC already included a feature selection procedure). Then, the performance under feature selection was further examined for wmSRC, SRC-concatenation, MKL and RF, respectively as follows: a two-sample t-test was performed on each feature of MRI, FDG-PET and florbetapir PET, respectively. The 90 features of each modality were then ranked according to the significance of the two-sample t-test; then, classification performances of the four different methods, including wmSRC, SRC-concatenation, MKL and RF were investigated along with different number (from 1 to 90) of the ranked 90 features. Note that since the main aim of performing feature selection here was to examine the effectiveness of the wmSRC algorithm with various numbers of features compared to other multi-modality methods, so we decided to use the simple two-sample t-test for the feature selection.

@&#RESULTS@&#

The performance of using single-modality SRC (SRC-MRI, SRC-FDG-PET and SRC-florbetapir PET, respectively) and multi-modality wmSRC (combining MRI, FDG-PET and florbetapir PET) were examined. As it is shown in Table 2
                         and Fig. 1
                        , the multi-modality method of wmSRC can achieve higher accuracy for classifying AD or MCI from NC and pMCI from sMCI than all the single-modality methods.

For discriminating AD from NC, the wmSRC can achieve an accuracy of 94.8% (with 95.6% of the sensitivity, and 94.0% of the specificity), which is much better than the best accuracy of 90.9% with single-modality method (using FDG-PET). Furthermore, the area under the ROC curve (AUC) is 0.986 with wmSRC, which is better than the single-modality method (AUC=0.951, p
                        =0.004 for SRC-MRI; AUC=0.969, p
                        =0.051 for SRC-FDG-PET; and AUC=0.914, p
                        <0.001 for SRC-florbetapir PET).

For classifying MCI from NC, the multi-modality method can achieve an accuracy of 74.5% (with sensitivity of 66.4%, and specificity of 82.1%), which is also better than the single-modality method (the best classification accuracy is 71.8% when using FDG-PET). Furthermore, the AUC is 0.770 for the multi-modality method, which is numerically greater than the single-modality methods (AUC=0.728, p
                        =0.004 for SRC-MRI; AUC=0.745, p
                        =0.398 for SRC-FDG-PET; and AUC=0.752, p
                        =0.493 for SRC-florbetapir PET).

For the discrimination of pMCI from sMCI, the wmSRC can achieve an accuracy of 77.8% (with 74.1% of the sensitivity, and 81.5% of the specificity), which is much better than the best accuracy of 67.0% with single-modality method (using FDG-PET). Furthermore, the AUC is 0.807 with wmSRC, which is numerically greater than the single-modality methods (AUC=0.536, p
                        =0.001 for SRC-MRI; AUC=0.741, p
                        =0.401 for SRC-FDG-PET; and AUC=0.704, p
                        =0.176 for SRC-florbetapir PET).

Particularly, the combining weight parameters α in wmSRC optimized by the grid search approach that corresponding to MRI, FDG-PET and florbetapir PET are 0.5, 0.3 and 0.2, respectively for discriminating AD from NC; 0.4, 0.3 and 0.3 for classifying MCI from NC; and 0.1, 0.3 and 0.6 for the discrimination of pMCI from sMCI.

The wmSRC method was further compared with several other multi-modality methods, including SRC-concatenation, MKL, RF and JRC methods, for AD (or MCI) classification. As it is shown in Table 3
                           , the wmSRC can achieve an accuracy of 94.8% for discriminating AD from NC, which is much better than those of SRC-concatenation (89.6%) and RF (90.3%), and comparable with those of MKL (93.5%) and JRC (93.5%). For the classification of MCI from NC, wmSRC can achieve an accuracy of 74.5%, SRC-concatenation of 73.6%, MKL of 73.1%, RF of 73.0%, and JRC of 75.8%. As to the discrimination of pMCI from sMCI, the wmSRC can achieve an accuracy of 77.8%, which is superior to other three methods of SRC-concatenation (72.0%), MKL (72.2%) and RF (70.7%), and comparable to JRC (75.9%), indicating that wmSRC is an efficient method for AD and MCI discrimination.

In addition to characterizing the classification performance of wmSRC based on all the 90 features (without feature selection) for AD (or MCI) classification, the performance under feature selection (with 1, 2, 3, …, or 90 features from the ranked 90 features for each modality) were also investigated for wmSRC, as well as for SRC-concatenation, MKL and RF.

The results of the four different methods with different number of the ranked features for AD (or MCI) classification are displayed in Fig. 2
                           . The figure shows that the wmSRC method performs superb to the other three multi-modality methods along with all the number of ranked features for AD classification and pMCI classification. Moreover, the wmSRC method is stable and robust to the number of features included. For example, as it is shown in Fig. 2a and c for AD classification and pMCI classification, even one feature from MRI, FDG-PET, and florbetapir PET can make the wmSRC method achieve a reasonable accuracy for the discrimination, but the SRC-concatenation and RF methods cannot. For the MCI classification in Fig. 2b, the wmSRC method shows comparable performance with other methods; in addition, it shows more stable to the number of features than other methods.

The wmSRC method can reach an encouraging classification accuracy even with less than 5 features (the top 5% ranked features) for AD/MCI classification (specifically, higher than 90% for classifying AD from NC, higher than 73% for the discrimination of MCI from NC, and higher than 80% for discriminating pMCI from sMCI).

@&#DISCUSSION@&#

In this paper, the multi-modality classification method, wmSRC, was proposed and compared with some other state-of-the-art multi-modality methods on 340 ADNI subjects to identify AD and MCI, with three modalities of volumetric MRI, FDG-PET and florbetapir PET. The results demonstrated the high accuracy and the effectiveness of wmSRC (94.8% for AD classification, 74.5% for MCI classification, and 77.8% for the discrimination of pMCI from sMCI).

The wmSRC method had several advantages in discriminating AD and MCI. First, the classification results showed that the wmSRC method could achieve better classification accuracy than the methods based on single-modality SRC (SRC-MRI, SRC-FDG-PET and SRC-florbetapir PET), and the AUC from wmSRC was either statistically significantly or at least numerically greater than those from single-modality methods. These results were consistent with the findings of previously published reports [2,4,20]. Second, the classification results proved that the weighted combination approach was a simple-while-effective multi-modality analysis method. Notably, our use of the wmSRC method combined multi-modality information at the decision phase or at the classification stage, which was different from some multi-modality methods like MKL and SRC-concatenation that combined multi-modality features at the feature selection phase [2]. Should the feature selection be used to characterize the single or multi-modality nature, then our method belonged to the single-modality category. Third, Table 3 indicated that wmSRC was better than several other multi-modality classification methods such as RF and SRC-concatenation, and comparable to those of MKL and JRC [2,4,19,22,23], note that JRC also adopted an additional clinical score information of MMSE besides the class label information. These results further suggested that wmSRC was a promising tool for AD and MCI classification.

The weights used to combine the three imaging modalities were determined differently for classifying AD from NC, MCI from NC and pMCI from sMCI. We employed the grid search approach for the weight determination. The combining weight parameters in wmSRC that corresponding to different modalities of MRI, FDG-PET and florbetapir PET were 0.5, 0.3 and 0.2, respectively for discriminating AD from NC; 0.4, 0.3 and 0.3 for classifying MCI from NC; and 0.1, 0.3 and 0.6 for the diagnosis of pMCI from sMCI. It is clear that these three modalities contributed differently depending on classification questions, indicating the complementary relations among these modalities. These interesting results were consistent with previous studies [17,37], and could provide information relevant for modalities selection in the future studies.

The effect of different number of discriminant features on the multi-modality methods (wmSRC, SRC-concatenation, MKL and RF) was also investigated. The simple two-sample t-test was used as the feature selection method to rank the 90 features. From Fig. 2, we could see that for AD classification, the wmSRC method achieved obvious improvement over the SRC-concatenation and RF methods along with all the number of ranked features, and slightly better or comparable with the MKL method. For the discrimination of pMCI from sMCI, the wmSRC method also achieved better results than other methods, including SRC-concatenation, MKL and RF, across all the number of ranked features. While for MCI classification, none of the four multi-modality methods showed obvious superiority for the classification accuracy. These results indicated that the wmSRC may be more applicable and efficient for the classification of AD from NC, and pMCI from sMCI. Moreover, the wmSRC method was stable (with less ups and downs) along with different number of ranked features (Fig. 2), which revealed that redundant features may introduce little interference for the classification. This property is valuable especially when there are many features and we don’t know how many discriminant features are enough for the effective classification, then the wmSRC can help, as wmSRC can give stable results even with some redundant features (see the solid lines in Fig. 2a–c).

As it is shown in Fig. 2, the wmSRC could achieve a promising accuracy even with less than 5 features (the top 5% ranked features) for AD/MCI classification, so to conveniently apply a small set of features to effectively discriminate AD/MCI, the top 5–10% ranked features (4–9 features) from each modality can be chosen as biomarkers for the final classification.

Several limitations existed in this study. First, there are many other data sources that are useful for AD or MCI distinction, such as CSF, APOE. As not every subject had all of the modalities, the current study was limited to three modalities of MRI, FDG-PET and florbetapir PET. Second, the wmSRC was only compared with four other multi-modality methods, including SRC-concatenation, MKL, RF and JRC, additional studies are needed to compare wmSRC to more methods. Third, the feature selection method used in this study for the validation of the effectiveness of wmSRC under different number of features was limited to the simple two sample t-test method, other feature selection methods are needed to be further investigated. In any way, the multi-modality classification methods deserve to be explored in the future studies.

@&#CONCLUSIONS@&#

This paper proposed a weighted multi-modality sparse representation classifier named as wmSRC to combine the multi-modality features for classification of AD and MCI. The main contribution of this paper was to use the weights (optimized by a grid search) to combine the classification results of multiple SRCs from multi-modality features. Experiments on ADNI database had validated the effectiveness of wmSRC. The experimental results suggested that wmSRC was a promising tool for neuroimaging data classification especially diseases diagnosis (classification). Using wmSRC, encouraging identification results can be achieved, which is valuable for the timely diagnosis and treatment of diseases.

There are no known conflicts of interest associated with this publication and there has been no significant financial support for this work that could have influenced its outcome.

@&#ACKNOWLEDGEMENTS@&#

The data used in this study (downloaded from ADNI database) was approved by Institutional Review Board (IRB) of each participating site including Banner Alzheimer's Institute, and was conducted in accordance with Federal Regulations, Internal Conference on Harmonization (ICH) and Good Clinical Practices (GCP).

This work was supported by the Funds for International Cooperation and Exchange of the National Natural Science Foundation of China (61210001), the General Program of National Natural Science Foundation of China (61222113), and Program for New Century Excellent Talents in University (NCET-12-0056).

Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.cmpb.2015.08.004.

The following are the supplementary data to this article:
                        
                           
                        
                     
                     
                        
                           
                        
                     
                  

@&#REFERENCES@&#

