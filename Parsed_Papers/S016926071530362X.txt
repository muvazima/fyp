@&#MAIN-TITLE@&#Automated anterior segment OCT image analysis for Angle Closure Glaucoma mechanisms classification

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A new method for automated classification of various mechanisms of angle closure disease.


                        
                        
                           
                           Compound image transform features are directly extracted from the raw AS-OCT images without any segmentation.


                        
                        
                           
                           Anterior chamber area measurement is not required in this method.


                        
                        
                           
                           Selected morphological features are used to achieve better classification result.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Angle closure glaucoma

Compound image transforms

Feature selection

Segmentation-free method

Machine learning classifier

@&#ABSTRACT@&#


               
               
                  Background and objectives
                  Angle closure glaucoma (ACG) is an eye disease prevalent throughout the world. ACG is caused by four major mechanisms: exaggerated lens vault, pupil block, thick peripheral iris roll, and plateau iris. Identifying the specific mechanism in a given patient is important because each mechanism requires a specific medication and treatment regimen. Traditional methods of classifying these four mechanisms are based on clinically important parameters measured from anterior segment optical coherence tomography (AS-OCT) images, which rely on accurate segmentation of the AS-OCT image and identification of the scleral spur in the segmented AS-OCT images by clinicians.
               
               
                  Methods
                  In this work, a fully automated method of classifying different ACG mechanisms based on AS-OCT images is proposed. Since the manual diagnosis mainly based on the morphology of each mechanism, in this study, a complete set of morphological features is extracted directly from raw AS-OCT images using compound image transforms, from which a small set of informative features with minimum redundancy are selected and fed into a Naïve Bayes Classifier (NBC).
               
               
                  Results
                  We achieved an overall accuracy of 89.2% and 85.12% with a leave-one-out cross-validation and 10-fold cross-validation method, respectively. This study proposes a fully automated way for the classification of different ACG mechanisms, which is without intervention of doctors and less subjective when compared to the existing methods.
               
               
                  Conclusions
                  We directly extracted the compound image transformed features from the raw AS-OCT images without any segmentation and parameter measurement. Our method provides a completely automated and efficient way for the classification of different ACG mechanisms.
               
            

@&#INTRODUCTION@&#

Glaucoma is a kind of optic neuropathy disorder which makes a gradual loss of eye vision due to the damages of retinal cells and retinal nerve fiber layer in the eye [1]. It is mainly associated with increased intraocular pressure (IOP) of fluid inside the eye [2]. If it is left untreated, the damaged optic nerves will not be recovered, leading to eventual blindness; however, reducing IOP may slow disease progression. It is the second major cause of visual impairment and blindness worldwide. Globally, the number of glaucoma patients is estimated as 60.5 millions in 2010, which may increase to almost 80 millions by 2020 [3]. The timing of interference is a key element in glaucoma treatment and it is usually asymptomatic in its early stage; so early diagnosis is required to slow down the disease progression toward complete vision loss.

There are mainly three types of glaucoma, namely, angle closure glaucoma (ACG), open angle glaucoma and developmental glaucoma. ACG is a specific type of glaucoma in which a sudden rise in IOP is experienced, usually as a result of poor drainage due to the eye, producing more aqueous humor than it can remove, hence causing a build-up of fluids [4]. ACG is more prevalent than the other two types. An eye that is susceptible to ACG usually has noticeable features which can be identified upon visual examination [5]. Anterior chamber angle (ACA) assessment is mainly used for the detection of ACG, and it can be visualized and quantified by anterior segment optical coherence tomography (AS-OCT) imaging technique [6,7].

It has been observed that ACG could be the result of one or more mechanisms in the anterior segment of the eye, such as exaggerated lens vault (L), pupil block (PB), thick peripheral iris roll (PIR), and plateau iris (PL) [6]. Laser peripheral iridotomy (LPI) is performed at an early and appropriate time for the eyes with anatomically narrow angles. Ophthalmologists and clinicians have found that the LPI is not always suitable for treating ACG patients with different mechanisms, and many patients continue to have appositional angle closure after treatment using LPI [7]. The optimal treatments for ACG patients with different mechanisms should be different. Therefore, it is important to classify these four mechanisms of ACG effectively, in order to not only provide different treatments for patients with different mechanisms, but also help the doctors tailor the best treatment for each mechanism accordingly.

It has been demonstrated that these four mechanisms of ACG have different patterns of angle configurations with different anterior chamber (AC) parameters [7]. AS-OCT provides excellent repeatability and reproducibility for imaging the anterior segment of eyes. AS-OCT was used to measure the AC parameters in the above mentioned four different mechanisms of ACG [6]. Wirawan et al. used some key parameters which were provided by customized software measured from the segmented AS-OCT images such as scleral spur-to-scleral spur distance, anterior chamber depth, anterior chamber angle and area [8]. Some reliable selected features were fed into Adaboost classifier and it was found to be clinically important to distinguish the four mechanisms of ACG [8]. In another study [9], the selected features are cross examined with four types of ACG mechanisms. In [10], various supervised and unsupervised features selection methods are used to find the reliable features and achieve the classification of different ACG mechanisms. However, the measurement of AC parameters relies on the accurate identification of the scleral spur in the segmented AS-OCT images by the clinicians, which is subjective and may not be reliable, introducing additional noise to the features. In [11], a new ensemble learning method based on error-correcting output code (ECOC) is proposed with application to classification of four ACG mechanisms and it is shown as an effective approach for multiclass classification. The existing methods for the classification of different ACG mechanisms are still not fully automated, requiring the help of the clinicians in feature extraction.

In this paper, our study is motivated by providing a fully automated expert system for the classification of different ACG mechanisms. We attempt to directly extract the features from the raw AS-OCT images without segmentation and parameter measurement. It is less subjective without the intervention of doctors. Specifically, in the proposed method, compound transform [12] is applied to the raw AS-OCT images to extract around three thousand different morphological features, from which a small set of discriminative features is selected for classification.

In the rest of this paper, Section 2 reviews related literature on general glaucoma diagnosis methods. The proposed method is presented in Section 3, followed by experimental results in Section 4. The conclusion is made with future works highlighted in the last section.

During the past one decade, several studies have investigated the usefulness of the computer-based expert support systems for the early detection of glaucoma using different imaging modalities as listed in the literature [13–18]. From the fundus images, using image processing techniques, the optic disk and blood vessels were extracted which could provide useful information to diagnose glaucoma. Higher order spectra features and textural features of fundus images combined with a random-forest classifier resulted with an accuracy of more than 91% for correctly identifying the glaucoma images [13]. An algorithm based on neural network classifier using the internal morphological features from the fundus images [14] used for glaucoma diagnosis. Another study [15] explored the ability of the combination of the imaging methods for distinguishing the normal eyes from the glaucoma defect using optic nerve head stereo photographs, scanning laser polarimetry, confocal scanning laser ophthalmoscopy and optical coherence tomography (OCT) images.

The optical nerve head (ONH) topography images were explored by orthogonal decomposition method for the detection of glaucoma in [16]. The super pixel information from three-dimensional OCT images was used in the boosting algorithm based machine classifier to improve early detection of glaucoma [17]. An expert support system for glaucoma using three-dimensional ONH features, stereometric ONH parameters, disc and cup margin features were measured and used in [18]. Aforementioned works have attempted to distinguish the glaucoma and non-glaucoma cases using various image sources, rather than classifying any subtypes of glaucoma. AS-OCT images are most convenient in diagnosis for ACG, since they provide useful information for clinicians to observe a cross sectional view of the anterior chamber and the inside angle structures of the eye. Classifying various mechanisms of ACG using reliable features from the measured parameters of AS-OCT images was studied through machine learning classifiers with improved accuracy in [8–11].

There were a number of tools available for glaucoma with different diagnostic approaches. However, the sensitivity and specificity of any single diagnostic test are not adequate to be considered as the golden standard. Also, to the best of our knowledge, no works have been done previously for ACG mechanism classification using textural features without prior segmentation. Hence, in this research work, we make an attempt to classify the different ACG mechanisms just based on AS-OCT images in a fully automated process. Features based on the compound hierarchy of algorithms representing morphology (CHARM) algorithm are extracted, followed by a multivariate classifier for classification.

@&#PROPOSED METHOD@&#

The proposed method for the classification of different ACG mechanisms is shown in Fig. 1
                     , including the acquisition of raw AS-OCT images followed by the extraction of a large number of morphological features using compound image transforms and image statistics [19]. A small set of discriminative and inter-dependent features selected by using maximum relevance-minimum redundancy (mRMR) algorithm [20] is fed into a Naïve Bayes Classifier for classification.

The data used in this work consist of AS-OCT images of 74 ACG patients provided by the Department of Ophthalmology in the National University Hospital, Singapore (NUHS). Ethics approval was obtained from the review board of NUHS and the written consent was obtained from all subjects prior to AS-OCT imaging. The numbers of patients with different mechanisms are listed in Table 1
                        . A skilled technician obtained the clinical images through a horizontal scan, including sections of the nasal and temporal quadrants, of all subjects using AS-OCT (Visante, software version 2.01.88; Carl Zeiss Meditec, Dublin, CA) in a dark room (0lux), with the images centered on the pupil. 1300nm infrared light was used to acquire cross-sectional view of the anterior segment image with high resolution. The standard AS single-scan protocol, producing 256 scans in 0.125s, was used to obtain the scans. The image saturation and noise were adjusted, and the polarization for each scan was optimized by the examiner to obtain the best quality images. Because several AS-OCT images were acquired for each patient, the image with the smallest amount of artifacts was selected for analysis. AS-OCT images with artifacts resulting from movement and the eyelids, and poor quality images secondary to tear film abnormalities and corneal scars were excluded. Each eye image was captured several times with undilated state of the pupil and only images with clearly visible scleral spurs were analyzed qualitatively by three glaucoma specialists (P.T.K. Chew, M.C. Aquino and C.C. Sng). They were categorized into four groups of images based on ACG mechanism. Each mechanism has its own distinctive morphological characteristics [5] as shown in Fig. 2
                         which is illustrated in the dotted lines.

The LV mechanism (Fig. 2a) can be spotted by the lens pushing the iris upward, which caused to reduce the angle between the cornea and the iris. The PB mechanism (Fig. 2b) can be seen by a convex forward iris profile, causing a shallow peripheral anterior chamber. The PIR mechanism (Fig. 2c) can be detected by a thick iris, which narrows the angle between the iris and cornea due to the circumferential folds along the periphery of the iris. The PL mechanism (Fig. 2d) can be identified by a sharp rise of the iris at the periphery, closer to the angle wall, before sharply turning away from the angle wall toward the visual axis.

The feature set used in the analysis is based on the compound image transforms and image statistics. The principle is to extract a very large number of image generic image descriptors (up to ∼3000 features) from the raw AS-OCT image. Compound transform has been validated to be useful in classification of cell images [12,19]. The features are listed into three major groups (A, B and C) as shown in Fig. 3
                         and the graphical representation of the extracted features is illustrated in Fig. 4
                        . The types of features described in each group as follows; Group A: high contrast features includes edge statistics, object statistics and Gabor statistics features; Group B: polynomial decompositions features includes Chebyshev statistics, Chebyshev-Fourier statistics, Zernike polynomials features; Group C: pixel statistics and textures features includes Haralick and Tamura features, multi-scale histograms, first four moments, pixel intensity statistics features, radon features, fractal features and Gini coefficient.

These features are calculated from the raw pixels, and image transforms as well as multi-order transforms, including the Fourier transform, wavelet transform (Symlet 5, Level 1), Chebyshev transform and tandem combinations of these transforms. Altogether, the feature vector consists of 2919 values, each of which provides the various information of an image (Fig. 4). Features generated by using compound transform have been widely used for analysis of complex visual arts and paintings [21], and it is very effective in classification of galaxy images [22–24], knee radiography images [25,26]. As for face recognition, both coarse and fine features generated by compound transform are required [27]. The abundant features generated by compound transform can fall into the following groups [12].

High contrast features include edges and objects, statistics about object number, spatial distribution, size and shape information.
                              
                                 •
                                 Edge statistics features were calculated using Prewitt gradient [28], which comprises the mean, median, variance, and 8-bin histogram of both the magnitude and the direction components. Some of the additional edge features consist of the total number of edge pixels, the direction homogeneity [29], and the difference between the direction histogram bins are sampled into a four-bin histogram.

Object statistics such as Otsu and Inverse Otsu were calculated for all 8-connected objects found in the Otsu and Inverse Otsu binary mask of the object [30] respectively. The Euler Number, and the minimum, maximum, mean, median, variance, and a 10-bin histogram of both the objects areas and distances from the objects to the image centroid are also included [31].

Gabor statistics features were calculated through the convolution with a kernel in the form of a Gaussian harmonic function [32,33] since the Gabor filters is suitable for filtering in the spatial and frequency domain. Seven distinct frequencies were used for providing seven image descriptor values.

In polynomial decomposition, a polynomial is produced which can approximate the image to some extent of fidelity. The generated polynomial coefficients are used as descriptors of the image.
                              
                                 •
                                 Chebyshev statistics [34] were generated using a 32-bin histogram of a 1×400 vector calculated by Chebyshev transformation of the image with an order of N
                                    =20.

Chebyshev-Fourier features [35] included a 32-bin histogram of the polynomial coefficients of a Chebyshev-Fourier transformation with a polynomial order of up to N
                                    =23.

Zernike features [36] are the absolute values of Zernike polynomial approximation coefficients of the image [29] which generates 72 image descriptors. These features include rotation invariance, expression efficiency, robustness to noise and multilevel representation for describing the shapes of the patterns.

Pixel statistics are mainly based on the distribution of pixel intensities of the image, including histograms and moments. Texture features are the inter-pixel variations in intensity for several directions and resolutions.
                              
                                 •
                                 First mean, standard deviation, skewness, and kurtosis were computed on each image stripes in four different directions (0°, 45°, 90° and 135°). Each set of stripes is finally sampled into a 3-bin histogram, which provides 48 image descriptors.

Haralick features were computed on the image's co-occurrence matrix based on standard equations as described in [37], and contribute 28 image descriptors. These were obtained by a tabulation of how often many combinations of pixel brightness values occur in the image.

Multi-scale histograms were calculated using various numbers of bins (3, 5, 7, and 9), as proposed by [38], providing 24 image descriptors.

Tamura texture features [39] of contrast, directionality, and coarseness and its 3-bin histogram were utilized to provide 6 image descriptors.

Radon transform features [40] were calculated for four different directions (0°, 45°, 90°, and 135°), as well as each of the resulting series were convolved into a 3-bin histogram, which provide a total of 12 image features.

Fractal features were computed as proposed by [41,42], providing a total of 20 image features. Since, the concept of fractal dimension is an indicator of the surface roughness, fractal-based texture analysis has a correlation between texture coarseness.

Pixel intensity features such as mean, median, standard deviation, minimum and maximum pixel intensity values were calculated using standard formulas.

Gini coefficient is used to compute the quantitative measure of the inequality with which an object's brightness is distributed amongst its constituent pixels. One Gini coefficient is calculated using the method described in [43].

Since, the larger set of image features can be more informative; nevertheless, all the 2919 image features are not equally informative since some of the features are expected to be noisy and more redundant. In the proposed method, mRMR feature selection algorithm is used, which is shown to be superior for reliable and useful feature selection in previous works on ACG mechanisms classification [8–10]. The mRMR feature selection algorithm is used as described in the following subsection, for selecting the most informative image features as well as rejecting the redundant and noisy features for classification.

Minimum Redundancy Maximum Relevance (mRMR) is a feature selection algorithm that aims to find features that are most relevant to the target classes, while reducing the redundancy between the selected features simultaneously [20]. To find relevant features, the mutual information between a feature and the target class should be maximized. The mutual information between two variables m and n is defined as,
                           
                              (1)
                              
                                 
                                    I
                                    (
                                    m
                                    ,
                                    n
                                    )
                                    =
                                    ∫
                                    
                                       ∫
                                       
                                          p
                                          (
                                          m
                                          ,
                                          n
                                          )
                                          ⋅
                                          log
                                          
                                             
                                                p
                                                (
                                                m
                                                ,
                                                n
                                                )
                                             
                                             
                                                p
                                                (
                                                m
                                                )
                                                ⋅
                                                p
                                                (
                                                n
                                                )
                                             
                                          
                                          d
                                          m
                                           
                                          d
                                          n
                                       
                                    
                                 
                              
                           
                        
                     

The mutual information between the selected features set F and class n can be defined as,
                           
                              (2)
                              
                                 
                                    D
                                    (
                                    F
                                    ,
                                    n
                                    )
                                    =
                                    
                                       1
                                       
                                          |
                                          F
                                          |
                                       
                                    
                                    
                                       ∑
                                       
                                          
                                             m
                                             i
                                          
                                          ∈
                                          F
                                       
                                    
                                    
                                       I
                                       (
                                       
                                          m
                                          i
                                       
                                       ,
                                       n
                                       )
                                    
                                 
                              
                           
                        with F containing x features {m
                        1, m
                        2, …, m
                        
                           x
                        } while that between each two features m
                        
                           i
                         and m
                        
                           j
                         (i, j
                        =1, 2, …, x) in F is defined as,
                           
                              (3)
                              
                                 
                                    R
                                    (
                                    F
                                    )
                                    =
                                    
                                       1
                                       
                                          |
                                          F
                                          
                                             |
                                             2
                                          
                                       
                                    
                                    
                                       ∑
                                       
                                          
                                             m
                                             i
                                          
                                          ,
                                          
                                             m
                                             j
                                          
                                          ∈
                                          F
                                       
                                    
                                    
                                       I
                                       (
                                       
                                          m
                                          i
                                       
                                       ,
                                       
                                          m
                                          j
                                       
                                       )
                                    
                                 
                              
                           
                        
                     

The objective of feature selection is to select a compact feature set with high discrimination and no redundancy by maximizing D(F,n) and minimizing R(F)simultaneously. Hence the final feature set is determined by maximizing the objective function Φ(D, R) which is defined as D(F, n)−
                        R(F) and D(F, n)/R(F) by using mutual information difference (MID) and mutual information quotient (MIQ) criteria, respectively. In practice, the best feature set is found by using incremental search methods such as sequential forward selection method by maximizing Φ(D, R) in each searching step.

Naive Bayes Classifier (NBC) demonstrates its effectiveness in binary and multiclass classification when the features are independent of each other [44]. It generates a single probabilistic model for each class, assuming conditional independence of the features given the class label. NBC can often outpace more sophisticated classification methods for the medical diagnosis model [45]. Since the mutual information among the features selected by using mRMR is minimized, these features are predominantly independent of each other and suitable for NBC. Besides, classification based on Bayes theorem is useful for medical model diagnosis since it combines training samples with a priori knowledge to get the posterior probability of a hypothesis. It considers the contribution of all the features independently to the probability of the target class and hence, it provides more useful information for decision support other than a class label or class probability [46].

The performance of the classifier can be computed in terms of accuracy, F-measure, sensitivity and specificity and which in turn can be derived from the rates of true positives (TP), true negatives (TN), false positives (FP) and false negatives (FN) using standard formulas [9].
                           
                              (4)
                              
                                 
                                    Accuracy
                                    =
                                    
                                       
                                          TP
                                          +
                                          TN
                                       
                                       
                                          TP
                                          +
                                          TN
                                          +
                                          FP
                                          +
                                          FN
                                       
                                    
                                 
                              
                           
                        Concerning the statistical significance, the F-measure is measured to calculate the test's accuracy using precision and recall rates;
                           
                              (5)
                              
                                 
                                    F
                                    -measure
                                    =
                                    2
                                    ⋅
                                    
                                       
                                          
                                             
                                                Precision
                                                *
                                                Recall
                                             
                                             
                                                Precision
                                                +
                                                Recall
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where
                           
                              (6)
                              
                                 
                                    Precision
                                    =
                                    
                                       
                                          TP
                                       
                                       
                                          (
                                          TP
                                          +
                                          FP
                                          )
                                       
                                    
                                     
                                    and
                                     
                                    Recall
                                    =
                                    
                                       
                                          TP
                                       
                                       
                                          (
                                          TP
                                          +
                                          FN
                                          )
                                       
                                    
                                 
                              
                           
                        
                     

Our experiment was conducted using Matlab Toolbox from the original authors for feature selections with mRMR method [19]. Each feature in the original feature set was normalized to have zero mean and a standard deviation of unity and discretized in order to compute the mutual information for feature selection. Prior to classification using NBC, ranked features are selected amongst extracted 2919 features using mRMR algorithm with mutual information difference. Leave-one-out cross-validation (LOOCV) is used due to the small sample size.


                     Fig. 5
                      shows the graph of the accuracy when training the classifier using NBC based on the mRMR ranked features and it is found that the accuracy of the mRMR feature selection algorithm grow quickly to a peak of 89.20%, at its top fifteen features. Beyond this peak, the accuracy of the algorithm was found to dip slightly and eventually stabilized. The accuracy is gradually reached to 84.90% when using all 2919 extracted features as shown in Fig. 5.

The mRMR feature selection algorithm with NBC was able to perform well on a small feature set that is 15 out of 2919 of the entire feature set, due to the selection of features that had high relevance to the target class while reducing features that may have been correlated to the features already selected. The fifteen selected features are shown in Table 2
                     , which would hence be significant in the detection of the ACG mechanism. All these fifteen features are revealing the morphology and shape information, high-contrast features, textural statistical distribution of the pixel values as explained in the Section 3.2, hence it is useful for the ACG mechanism classification using AS-OCT images.

The accuracy with different numbers of features (from 1 to 2919) is compared in Fig. 5 when the classifier is trained using NBC based on the mRMR ranked features. It is found that the accuracy of the mRMR feature selection algorithm improves quickly to a peak of 89.20%, at its top fifteen features. Beyond this peak, the accuracy of the algorithm dips slightly, and eventually stabilizes. The accuracy reaches 84.90% when using all 2919 ranked features as shown in Fig. 5. This shows robustness of feature selection in the proposed method. We also compared the accuracy with different combinations of the top 15 features. The variation of accuracy with different combinations of top 15 features is given in Table 3
                     . It is observed that the accuracy is 58.35% when only the top feature is used and it progressively improves once more features are added.

The accuracy, F-measure, specificity, sensitivity and receiver operating characteristic (ROC) area of each class using Naïve Bayes Classifier based on the selected 15 features are calculated and is shown in Table 4
                     . Our method achieves 89.20% of accuracy, 89.30% of F-measure with a sensitivity of 88.90% and a specificity of 89.19%. Table 5
                      shows the confusion matrix of 4-way classifier using NBC through LOOCV method. From the experimental results as shown in Tables 4 and 5, we can see that the classification accuracy of PIR and PL mechanisms is relatively higher than that of L and PB. There are two reasons for this: (1) from a medical point of view, both PB and L have very evident visual characteristics, such as convex forward iris profile in PB and greatly reduced AC volume in L, which are highly discriminative; (2) the sample size of the experimental dataset is imbalanced, with more samples in PB and L and less samples in the PIR and PL.

Since the number of samples collected for PIR and PL is small when compared to other two mechanisms which will reflect the imbalanced dataset for machine learning. When we provide more samples with equal number, the overall accuracy and F-measure can be improved. In machine learning application, feature selection (e.g., mRMR) is an important task to obtain high classification performance. However, most feature selection methods are based on statistics of the data. More data will make the selected features more confident. That is to say, more samples will help to improve the performance of feature selection, and also the classification accuracy, and F-measure.

The performance of the NBC classifier of the proposed method has been compared (see Table 6
                     ) with the other most used contemporary classifiers for medical diagnosis such as ANN viz. multilayer perceptron model using a back-propagation algorithm, k-Nearest Neighborhood (k-NN), Adaboost classifier and random forest (RF) classifier. For each classifier, we used leave-one-out cross-validation (LOOCV) and k-fold cross-validation (here k
                     =10) for a fair comparison. In 10-fold cross validation, the training set is divided into 10 sets of equal number of samples. Each time, 9 of these sets are used for training (k-1) and the remaining one for testing. This procedure is repeated 9 times (folds) using a different part for testing in each case. Finally the average value of all ten results is estimated to analyze the classifier performance. LOOCV method will work in the same above mentioned concept when k
                     =1 in k-fold cross validation method. Due to small number of samples, the statistics of the data is hard to be fully modeled, and there is some difference between LOOCV and 10-fold results for this present work. Since the selected features using mRMR are independent with each other; the combination of mRMR with NBC provides better result than the other multivariate classifiers.

To the best of our knowledge, there is no such an automated method for classifying different ACG mechanisms. The other methods are mainly based on the manual observation of clinicians, which is not an automated way for classification of different ACG mechanisms. Future work includes using a large dataset to learn better discriminative features and improve the classification accuracy. Furthermore, some samples may have multiple mechanisms simultaneously, which should be considered by proposing new machine learning methods. This study proposes a fully automated way for the classification of different ACG mechanisms, which is without intervention of doctors and less subjective when compared to the existing methods.

@&#CONCLUSIONS@&#

Angle-closure glaucoma, which is a specific type of glaucoma, has been observed to be the result of one or more mechanisms in the anterior segment of the eye. Traditional methods for classification of the different mechanisms are based on calculation of the key parameters, such as the angle opening distance (AOD), trabecular-iris space area (TISA) and angle recess area (ARA) from the segmented AS-OCT images with the help of doctors, which are not fully automated. Since the manual diagnosis mainly based on the morphology of each mechanism, in this study, we directly extract multiple morphological features using compound transform and image statistics, from which a small set of features is selected by using mRMR, and then used for classification. Our method provides a completely automated and efficient way for the classification of different ACG mechanisms. Segmentation of anterior chamber area and its measurement is not required in this method. Experimental results show that our method achieves 89.20% classification accuracy.

The limitation of this method is that the all features extracted from the raw AS-OCT images are based on compound image transformed features and its supervised learned feature selection method. Practically, manual labeling of various mechanism in huge ACG dataset are time consuming tasks for medical practitioners. In future work, unsupervised learning method and transfer learning method will be explored to conquer this problem. The possibility that a mechanism occurs in conjunction with other mechanisms could also be studied e.g. an iris roll mechanism occurring with a lens mechanism. Classifying all samples under only one class label, when some of these samples may have the features of two or more mechanisms, may adversely affect the performance of feature selection and classification; this can be investigated as well.

None declared.

@&#ACKNOWLEDGEMENT@&#

This work was supported by Ministry of Education (MoE) AcRF Tire 1 Funding, Singapore, under Grant M4010981.020 RG36/11.

@&#REFERENCES@&#

