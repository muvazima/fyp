@&#MAIN-TITLE@&#Drug/nondrug classification using Support Vector Machines with various feature selection strategies

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Our aim was to classify small molecule compounds as drugs and nondrugs using Support Vector Machines (SVM).


                        
                        
                           
                           We used three feature selection methods to improve the performance of the SVM classifier.


                        
                        
                           
                           Our findings revealed that data pre-processing and feature selection enhance the performance of the classifiers.


                        
                        
                           
                           Our study and findings will contribute to improvement of our understanding of the early-phase drug design.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Support Vector Machines

Molecular descriptors

Feature selection

Drug discovery

Machine learning

@&#ABSTRACT@&#


               
               
                  In conjunction with the advance in computer technology, virtual screening of small molecules has been started to use in drug discovery. Since there are thousands of compounds in early-phase of drug discovery, a fast classification method, which can distinguish between active and inactive molecules, can be used for screening large compound collections. In this study, we used Support Vector Machines (SVM) for this type of classification task. SVM is a powerful classification tool that is becoming increasingly popular in various machine-learning applications. The data sets consist of 631 compounds for training set and 216 compounds for a separate test set. In data pre-processing step, the Pearson's correlation coefficient used as a filter to eliminate redundant features. After application of the correlation filter, a single SVM has been applied to this reduced data set. Moreover, we have investigated the performance of SVM with different feature selection strategies, including SVM–Recursive Feature Elimination, Wrapper Method and Subset Selection. All feature selection methods generally represent better performance than a single SVM while Subset Selection outperforms other feature selection methods. We have tested SVM as a classification tool in a real-life drug discovery problem and our results revealed that it could be a useful method for classification task in early-phase of drug discovery.
               
            

@&#INTRODUCTION@&#

Traditionally, drug discovery process starts with the identification of the disease-associated protein [1] and then this process is followed by testing of the disease protein against thousands of known and new compounds to find lead compounds that can interact with the target protein and show the potential effectiveness against disease. These lead compounds can serve as candidates for the drug to be further analyzed in pre-clinical studies. Since thousands of compounds screened from compound libraries in this step, virtual screening (VS) can be used to search libraries in order to identify structures, which are most likely to bind to a target protein [2,3]. Since VS is a computational filter, which reduces the size of a chemical library to be screened experimentally, it can reduce time and effort in finding lead compounds and thereby saves money. Early-phase VS often employs to eliminate potentially unwanted molecules (i.e. inactive or toxic) from a compound library [4]. Therefore, machine-learning (ML) methods can be used for VS by analyzing the structural features of molecules of known activity or inactivity [5].

The main issue in early-phase of drug discovery process is the evaluation of drug compounds. Hereby, drug compounds have been studied from different perspectives [6] including prediction of oral bioavailability [7–10], drug-like [11–18] and lead-like [19] compounds, number [20] and topology of rings [21], molecular frameworks [22,23] and fragments [14,24–28]. Over a decade, various ML methods have been applied to biology, chemistry and drug discovery [29]. Supervised ML methods such as linear discriminant analysis [30] and decision trees [31] were used to predict structural properties of compounds. Furthermore, logistic regression [6], Bayesian networks [11] or artificial neural networks [12] have been used to distinguish between drugs and nondrugs. In addition to the activity studies, principal component analysis, Bayesian networks, neural networks and Support Vector Machines (SVM) were used in various chemogenomic studies [6].

SVM is one of the most widely used ML methods. Recently, it is used in a variety of drug discovery applications. There are some studies tried to design new kernel functions for SVM to combine compound structures with other data [32–38] and transfer similarity calculations into a combined feature space [39–43]. In addition to kernel design studies, SVM was used to predict compounds with single-target activity [4,40,44–58] and multi-target activity [59–63], and different compounds properties [64–76]. Furthermore, SVM methodology was used to predict drug-likeness score for targets [77], target–ligand interactions [78] and protein–ligand binding affinities [79–81]. More detailed information can be found in Heikamp and Bajorath [29].

In the present study, we applied SVM to a real-life drug discovery problem, particularly, the comparison of active against inactive molecules for screening large compound libraries. We built our SVM models with incorporation of various feature selection methods, including SVM/Recursive Feature Elimination (SVM/RFE), Wrapper Method (WM) and Subset Selection (SS). Additionally, to check for external validity of the study, we compared our results with literature in the field.

@&#METHODOLOGY@&#

The training and test sets of compounds have been taken from a different publication [6]. Based on that study, the training set contained 311 drugs and 320 nondrugs and the test set, which was a independent set of compounds, contained 98 drugs and 118 nondrugs. The data matrix consisted of 34 molecular descriptors as follows: log
                        P (the logarithm of the octanol/water partition coefficient), NHA (number of heavy atoms), MW (molecular weight), NoC (number of carbons), AC (atom count), HC (hydrogen count), HBDC (hydrogen bond donor count), HBAC (hydrogen bond acceptor count), RBC (rotatable bond count), AlRC (aliphatic ring count), ArRC (aromatic ring count), AAC (aromatic atom count), BC (bond count), RC (ring count), MSA (molecular surface area), PSA (polar surface area), APSA (apolar surface area), MP (molecular polarizability), WI (Wiener index), BI (Balaban index), HI (Harary index), hWI (hyper-Wiener index), PI (Platt index), RI (Randic index), SI (Szeged index), WPI (Wiener polarity index) and 8 more ligand efficiency indices; ΔG
                        
                           Bind
                        /NHA, ΔG
                        
                           Bind
                        /MW, ΔG
                        
                           Bind
                        /NoC, ΔG
                        
                           Bind
                        
                        /PSA, ΔG
                        
                           Bind
                        /MSA, ΔG
                        
                           Bind
                        /APSA, ΔG
                        
                           Bind
                        /WI, ΔG
                        
                           Bind
                        /P, where ΔG
                        
                           Bind
                         is the binding energy and P is the octanol/water partition coefficient. More detailed information about the molecular descriptors and the data sets can be found in Garcia-Sosa et al. [6].

SVM is a popular classification tool, which originally presented by Vapnik and his co-workers and has taken great interest from science community because of its strong mathematical background and excellent empirical successes. SVM is also capable of nonlinear classification and handling high-dimensional data, thus applied in many fields such as computational biology, text classification, image segmentation and cancer classification [82,83].

In binary classification problems, let {x
                        1, …, x
                        
                           n
                        } is a given training data that are n-dimensional vectors in some space 
                           
                              (
                              
                                 x
                                 i
                              
                              ∈
                              
                                 ℝ
                                 n
                              
                              )
                           
                         and {y
                        1, …, y
                        
                           n
                        } are their class labels where y
                        
                           i
                        
                        ∈{−1, +1}. The aim here is to find a hyperplane and obtain an equation, which separates the training data into two parts that all data points with same class labels exist on the same side of the hyperplane. Here, the data points that are closest to the hyperplane in both side is called support vectors and the objective of SVM is to maximize the margin 
                           
                              1
                              /
                              w
                           
                        , which is the distance between two support vectors or minimize 
                           
                              
                                 w
                                 2
                              
                              /
                              2
                           
                         equivalently. SVM takes advantage of Lagrange multipliers and Karuck Kuhn Tucker conditions to overcome this optimization problem.

When the data is linearly non-separable, slack variables {ξ
                        1, …, ξ
                        
                           n
                        }, which is a penalty introduced by Cortes and Vapnik [84], can be used to allow misclassified data points, where ξ
                        
                           i
                        
                        >0. Moreover, in many classification problems, the separation surface is nonlinear. SVM deals with this problem by mapping the input vectors to a high-dimensional space by using kernel functions (e.g. polynomial, radial-based, sigmoidal). A detailed description of SVM algorithm can be found in [82].

Since data pre-process and feature selection (FS) improve the prediction performance of predictors, provide faster and more cost-effective predictors and offer a better understanding of the underlying process that generated the data, they are the most crucial steps in ML methods. In pre-processing step, we applied the following two steps to our training set: (i) we have split the 34 molecular descriptors into 6 categories (8 topological indices, 10 atom and bond counts, 3 size and shape descriptors, 2 pharmacophore descriptors, 3 physical descriptors and 8 ligand efficiency descriptors) based on their properties [85] (see Fig. 1
                        ), (ii) we have computed the Pearson's correlation coefficient for the entire pairs of descriptors in each category. For all cross terms that were either strongly positive or negative (r
                        >0.90 or r
                        <−0.90) correlated with each other, we have discarded one descriptor based on t-test. Thus, we have selected the descriptor that provides more information for classification. In addition to the correlation filter, we used three FS methods in this study, including SVM–Recursive Feature Elimination (SVM–RFE), Wrapper Method (WM) and Subset Selection (SS).

This method proposed by Guyon et al. [86] and originally used for gene selection for cancer classification in microarray data analysis. It uses weight value of the decision hyperplane given by the SVM as ranking criterion and contains following four steps: (i) evaluate the worth of an feature by using an SVM classifier, (ii) features are ranked by the square of the weight assigned by the SVM, (iii) eliminate features with the smallest weight, (iv) repeat the process with the training set restricted to the remaining features. Its primary objective is to eliminate redundant features and give better subsets to improve the classification performance [87].

In this approach, feature selection is performed using an ML algorithm as a black box. This method carries out a search for a good subset of the features using the ML algorithm itself as a part of the evaluation function. An estimation method is used to predict the accuracy of the ML algorithm. Finally, a search method is performed to search possible parameters in the space. In this study, SVM algorithm was used as an ML classifier, and a 10-fold cross validation was performed to estimate the accuracy of the classifier for a set of features. Genetic algorithm was used as a search method. More detailed information can be found in Kohavi and John [88].

Since this method is also a derivation of Wrapper Method, it uses an ML algorithm to estimate the worth of a set of features. This feature selection method evaluates feature subsets on training set or a separate test set rather than using cross-validation and performs a search in the parameter space. For our study, we applied SVM classifier to predict the subset of the features, and we used a different test set to evaluate the feature subsets. Finally, we used genetic algorithm as a search method [88,89].

In order to compare the performance of the methods, we calculated several measures, including; accuracy rate (AR), sensitivity (SE), specificity (SP), positive predictive value (PPV), F1-score (F1S), area under the curve (AUC) and Matthews correlation coefficient (MCC) as follows:
                           
                              
                                 
                                    A
                                    R
                                    =
                                    
                                       
                                          T
                                          P
                                          +
                                          T
                                          N
                                       
                                       
                                          T
                                          P
                                          +
                                          T
                                          N
                                          +
                                          F
                                          P
                                          +
                                          F
                                          N
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              
                                 
                                    S
                                    E
                                    =
                                    
                                       
                                          T
                                          P
                                       
                                       
                                          T
                                          P
                                          +
                                          F
                                          N
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              
                                 
                                    S
                                    P
                                    =
                                    
                                       
                                          T
                                          N
                                       
                                       
                                          T
                                          N
                                          +
                                          F
                                          P
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              
                                 
                                    P
                                    P
                                    V
                                    =
                                    
                                       
                                          T
                                          P
                                       
                                       
                                          T
                                          P
                                          +
                                          F
                                          P
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              
                                 
                                    F
                                    1
                                    S
                                    =
                                    
                                       
                                          2
                                          (
                                          P
                                          P
                                          V
                                          *
                                          S
                                          E
                                          )
                                       
                                       
                                          P
                                          P
                                          V
                                          +
                                          S
                                          E
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              
                                 
                                    M
                                    C
                                    C
                                    =
                                    
                                       
                                          T
                                          P
                                          *
                                          T
                                          N
                                          −
                                          F
                                          P
                                          *
                                          F
                                          N
                                       
                                       
                                          
                                             
                                                (
                                                T
                                                P
                                                +
                                                F
                                                P
                                                )
                                                (
                                                T
                                                P
                                                +
                                                F
                                                N
                                                )
                                                (
                                                T
                                                N
                                                +
                                                F
                                                P
                                                )
                                                (
                                                T
                                                N
                                                +
                                                F
                                                N
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where TP
                        =true positives, TN
                        =true negatives, FP
                        =false positives and FN
                        =false negatives.

For all SVM classifiers, radial basis function used as kernel to classify both training and test sets. A grid search method is used to optimize kernel parameter gamma and complexity parameter of the models. To optimize model parameters, 10 fold cross-validation is used. In this study, we used WEKA (Waikato Environment for Knowledge Analysis) machine learning software [90] for both feature selection methods and SVM construction.

@&#RESULTS@&#

Our main aim in this study was to distinguish between drug and nondrug compounds by using SVM with several FS methods. We summarized our data pre-process and feature selection steps in Fig. 2
                     . After application of the Pearson's correlation filter, number of features in the training set reduced from 34 to 11. Then we applied a single SVM to this reduced training set. Furthermore, we applied our three FS methods to this reduced training set. The results as follows: (i) 7 out of 11 features have been selected by SVM/RFE, (ii) 7 out of 11 features have been selected by WM, and (iii) 6 out of 11 features have been selected by SS. Fig. 3
                      displays the selected features by the FS methods and the Pearson's correlation filter. Finally, we trained four different classifier models with the selected features using SVM. After a grid search for the optimization of the model parameters, the gamma value was selected as 0.01 for all four models and the complexity parameter was chosen as 100 for single SVM, SVM/RFE and WM, and as 1000 for SS.

We used the training set to establish four different SVM models. Then, a separate test set is used to optimize the parameter of the SVM models and to test their classification abilities. Calculated performance measures for training and test sets are shown in Tables 1 and 2
                     
                     . Training set results showed that FS improves the performance measures of the SVM classifier. According to test set results, single SVM and SVM/RFE had similar results based on AR (76% and 77%, respectively), while WM and SS had also similar but better AR results (80% and 81%, respectively). All methods had comparable results in terms of SE and F1S (ranging from 87% to 89% and 77% to 80%, respectively). According to SP measure, single SVM had the lowest rate, while SS had the highest (64% and 75%, respectively). Considering PPV criteria, WM and SS represent similar and best performance (73% and 74%, respectively), while single SVM represent the worst performance (67%). SS method outperforms other three methods in terms of AUC measure (SS=88% and other methods ranging from 85% to 86%). Regarding MCC measure, SS and WM (0.64 and 0.62 respectively) outperforms SVM-RFE and single SVM (0.58 and 0.55).

Further, a series of cut-off values are applied for classification and SE/SP measures are calculated for each cut-off value. Then, a receiver operating characteristic (ROC) curve is constructed and area under this curve is calculated using the trapezoidal rule. The ROC curves from Fig. 4
                      and high AUC values from Table 2 revealed that single SVM has powerful discrimination ability between drugs and nondrugs by its own. Moreover, FS methods, especially SS, increase the discrimination ability of the SVM classifier.

Furthermore, we compared our results with a previous study, where the authors used logistic regression to classify compounds as drugs and nondrugs [6]. In their study, the authors used various logistic regression models for classifying compounds. We compared our best SVM model, which was SS, with one of their regression models, which had the highest AR, based on AUC. At the end, SS outperformed logistic regression model with 88% to 82% respectively (see Fig. 5
                     ).

Even the performances of each SVM method differs, they gave consistent results in compound classifications. Fig. 6
                      displays the true classification agreements between methods. All methods classified 157 compounds correctly (list of compounds can be found in Supplementary Material 1). When we look to three method agreements, single SVM missed eight compounds and all of them were nondrugs (2qbq, 2i2c, 3f1a, 2gyi, 3eko, 2pu1, 3f19 and 2qnq), SVM/RFE missed three compounds (nondrug: 3f80, drug: acebutolol and albuterol salbutamol), SS missed only one compound (drug: sulfasalazine) and WM did not miss any compounds. In two method agreements, two nondrug compounds (2rke and 2hw2) are truly classified both in SS and WM. When we look at disagreements, SS correctly classified four compounds (drug: labetalol, nondrugs: 2iuz, 3cm2, 2jh5) by its own, single SVM classified two compounds (drug: norepinephrine, nondrug: 3djk), SVM/RFE and WM classified only one compound (drug: montelukast and drug: fenoldopam, respectively) successfully.

Moreover, we calculated descriptor importance for each model. For this purpose, we calculated chi-square statistics for each predictor and ranked them from most important to less based on these statistics. Top five descriptors for each model is demonstrated in Fig. 7
                     . All four methods found HBDC as the best descriptor and PSA as the second best descriptor. Single SVM and SVM/RFE methods defined RBC as third best and WI as forth best descriptors, while SS and WM defined log
                     P as third best and BI as forth best descriptors. The fifth best descriptor differed in each model (ΔG
                     
                        Bind
                     /MW for single SVM, ARC for SS, NHA for SVM/RFE and RC for WM).

We obtained well results for each SVM method. AR were obtained between 76% and 81%, SE and SP values were between 87–89%, 64–75%, respectively. PPV values were between 67% and 74%, F1S were 77–80% and AUC were 85–88%.

In three method agreements, single SVM missed 8 compounds, while other methods missed lower. Furthermore, the FS methods mostly gave better results (based on AR, SP, PPV, F1S) than a single SVM. Thus, feature selection using any method was more useful than using a single SVM model in our dataset.

Looking on single successes, SS correctly classified four compounds more than other single method successes and misclassified just one compound lower than single SVM and SVM/RFE models. This made an increase in the performance of SS model. Performance measures were consistent with these findings and SS model gave the highest AR with 81%, highest SP with 75%, highest PPV with 74%, highest AUC with 88% and highest F1S (also WM had the same performance) with 80%. SP value of SS was higher (88%) than SVM/FRE, but lower than other two models.

Molecular descriptors play a key role especially in drug activity studies and selecting important descriptors may also be fundamental. Since there are too many descriptors both experimental and theoretical, it can be complex to understand the descriptor and drug activity relationship pattern. However, with a few number of descriptors, the underlying mechanism of the drug activity can be revealed easily. Furthermore, selecting important descriptors is useful for improving both speed and accuracy of statistical learning methods for the prediction of pharmacokinetic and toxicological properties of drug molecules [91,92]. Veber et al. [93] discussed that lower RBC, lower HBDC and PSA might be associated with good oral bioavailability. Several authors stated that HBDC, PSA, RBC and logP are important descriptors for drug-likeness and ADME (absorption, distribution, metabolism, excretion) predictions [16,94,95]. Our results were consistent with the previous studies and showed that HBDC is the most significant predictor and PSA is the second best predictor in drug/non-drug classification based on the descriptor importance results. Moreover, RBC, logP, WI and BI are the other important predictors. Finally, our results were found better than the study of Garcia-Sosa et al. [6] in which they found AUC, AR and SE as 82%, 78% and 88%, while we found as 88%, 81% and 88%. Thus, SVM should be preferable as compared to logistic regression model.

@&#CONCLUSIONS@&#

As stated in previous studies [4,58], in early-phase of drug discovery, it is very important to eliminate potentially undesirable compounds, such as inactive or toxic molecules. Since there are thousands of compounds in small-molecule libraries, the filtering method should be both fast and effective. After a fast filtering method, virtual screening methods, such as docking, can be used as a next step in drug discovery process. In this study, we used four SVM models for a real life drug–nondrug classification problem. As our study revealed that data pre-process and feature selection are important steps in machine learning process since they increase the performance of the classifiers. Moreover, the results appear to show a better AUC value than one case of logistic regression [6]. Based on our findings, we can clearly state that machine learning classification methods, such as SVM, can be used as a filter in early-phase of drug discovery. The main limitation of this study was the relatively small sample size. The second limitation of the study may be related to the training set which was quite balanced. However, in practice, the number of nondrug-like molecules is significantly larger than that of drug-like molecules. This fact should be considered in the future works and further studies with larger data set are needed to validate our findings.

@&#ACKNOWLEDGMENTS@&#

The authors are grateful to Prof. Dr. Sevim Dalkara of Hacettepe University Faculty of Pharmacy Department of Pharmaceutical Chemistry for useful advice and suggestions.

Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.cmpb.2014.08.009.

The following are the supplementary data to this article:
                        
                           
                        
                     
                  

@&#REFERENCES@&#

