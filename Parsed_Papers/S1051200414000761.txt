@&#MAIN-TITLE@&#Robust feature extraction based on an asymmetric level-dependent auditory filterbank and a subband spectrum enhancement technique

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We propose cGCFB-based robust cepstral feature (RCGCC) for speech recognition tasks.


                        
                        
                           
                           A sigmoid-shaped suppression rule is introduced for auditory spectrum enhancement.


                        
                        
                           
                           Short-term cepstral mean and scale normalization is proposed to reduce mismatch.


                        
                        
                           
                           Performance evaluation is carried out on the AURORA-2, -4 and -5 corpora.


                        
                        
                           
                           Proposed RCGCC outperformed other front-ends in real-time reverberant environment.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Speech recognition

Compressive gammachirp

Auditory spectrum enhancement

Feature normalization

@&#ABSTRACT@&#


               
               
                  In this paper we introduce a robust feature extractor, dubbed as robust compressive gammachirp filterbank cepstral coefficients (RCGCC), based on an asymmetric and level-dependent compressive gammachirp filterbank and a sigmoid shape weighting rule for the enhancement of speech spectra in the auditory domain. The goal of this work is to improve the robustness of speech recognition systems in additive noise and real-time reverberant environments. As a post processing scheme we employ a short-time feature normalization technique called short-time cepstral mean and scale normalization (STCMSN), which, by adjusting the scale and mean of cepstral features, reduces the difference of cepstra between the training and test environments. For performance evaluation, in the context of speech recognition, of the proposed feature extractor we use the standard noisy AURORA-2 connected digit corpus, the meeting recorder digits (MRDs) subset of the AURORA-5 corpus, and the AURORA-4 LVCSR corpus, which represent additive noise, reverberant acoustic conditions and additive noise as well as different microphone channel conditions, respectively. The ETSI advanced front-end (ETSI-AFE), the recently proposed power normalized cepstral coefficients (PNCC), conventional MFCC and PLP features are used for comparison purposes. Experimental speech recognition results demonstrate that the proposed method is robust against both additive and reverberant environments. The proposed method provides comparable results to that of the ETSI-AFE and PNCC on the AURORA-2 as well as AURORA-4 corpora and provides considerable improvements with respect to the other feature extractors on the AURORA-5 corpus.
               
            

@&#INTRODUCTION@&#

Speech intelligibility as well as the performance of speech recognition systems degrades in practical environments due to a variety of signal variabilities. Additive noise and reverberation are the important causes of signal variabilities. Additive noise from interfering noise sources and convolution noise arising from acoustic environments mainly cause a reduction of speech recognition performance.

The acoustic features most commonly used in speech recognition systems are Mel Frequency Cepstral Coefficients (MFCC) [1] and Perceptual Linear Prediction (PLP) [2] features. Both MFCC and PLP front-ends perform well in matched environments, where speech data are collected from reasonably clean environments. However, their performance degrades severely when the testing environment is different from the training environment. Degradation of performance due to mismatched environments has been a barrier for deployment of speech recognition technologies. Various sources give rise to this mismatch, such as background noise, channel/handset distortion, room reverberation. Most of the sources of speech variability produce additive distortion (e.g., background noise) and/or convolutional distortion (e.g., channel/handset mismatch) in the speech signal. Among all different additive noises, a multi-speaker or babble noise environment, where the interference is speech from speakers in the vicinity, is one of the most challenging noise conditions. Therefore, it is necessary to address this problem, i.e., performance degradation due to mismatch environments, to enable the deployment of recognition systems in real world conditions.

Various research has been reported in the literature to improve the robustness of speech recognition systems under additive noise and reverberation. The methods to compensate for the effects of environmental mismatch can be implemented at the front-end (feature domain techniques) or at the back-end (model domain techniques) or both. The model domain methods adapt each acoustic model to make it fit better to the mismatched acoustic environment so that the adapted models will be able to classify the mismatched speech features collected in the testing environment. The typical examples of this category include the well-known noise masking [20], speech and noise decomposition (SND) [21], vector tailor series (VTS) [22], maximum a posteriori (MAP) [23,24], maximum likelihood linear regression (MLLR) [25], model-based stochastic matching [26], statistical reestimation (STAR) [27–29], parallel model combination (PMC) [30], optimal likelihood weighting based on the criteria of minimum classification error (MCE) and maximum mutual information (MMI) [31], etc.

The objective of feature domain techniques is to make features more consistent in diverse environmental conditions. Feature domain methods can be classified into two subgroups. One subgroup of feature domain techniques aims at modifying the test features and making those features match the acoustic conditions better for the trained models: speech enhancement methods [32–37], codeword dependent cepstral normalization (CDCN) [38], feature-based stochastic mapping [26], multivariate Gaussian based cepstral normalization (RATZ) [39], feature normalization methods, such as cepstral mean normalization (CMN) [7,11,40], the stereo-based piecewise linear compensation for environment (SPLICE) [41,42]. The other subgroup of feature domain techniques, on the other hand, aims at making a special robust speech feature representation, which is used for both training and testing, to reduce the sensitivity to the various acoustic conditions. This paper deals with the latter subgroup. Robust feature extractors are usually obtained either by appending a pre-processing step, like speech enhancement [8,9,14,15], or by incorporating algorithms in an MFCC or PLP computation framework such as PNCC [3], amplitude modulation-based cepstral features [43,44], frequency masking [9], or by adding a post-processing step, like feature normalization techniques [7,11] (e.g., cepstral mean normalization (CMN)) or by combining any two or all of the above mentioned steps [3,4]. Most of the front-ends use, in addition to other techniques for environmental mismatch compensation, a feature normalization technique, at the least CMN, as a post-processing scheme.

Additive noise reduction approaches usually have a tradeoff between the amount of noise reduction and speech distortion induced due to processing of a speech signal. At very low SNR the intensity of this induced distortion is high, thereby deteriorating the performance of the speech recognition systems. Compensation of reverberant noise is usually done by dereverberation, which can be obtained by inverse filtering the impulse response of the room [12,13]. However, room impulse response is dependent on the distance between the speaker and the microphone and on the conditions of the room. Therefore, extracting a common set of robust features, which can perform well at low SNRs and also can handle various room impulse responses, is a difficult and challenging task.

To deal with additive noise distortion various speech enhancement methods have been proposed in [8,9,14,15]. In [10,12,13] several approaches have been proposed for handling convolutional noise distortions. The ETSI-AFE, described in [4], uses a two-stage Wiener filter and a blind equalization technique, which is based on the comparison to a flat spectrum and the application of the LMS algorithm, for improving robustness of ASR systems against additive noise distortions and channel effects. The PNCC technique, proposed in [3], includes the use of a gammatone filter-bank (GTFB) and a power law nonlinearity in place of the Mel filter-bank and log nonlinearity, used in conventional MFCCs framework, a medium duration power bias subtraction technique, for noise reduction, based on the arithmetic mean (AM)–geometric mean (GM) ratio and cepstral mean normalization as a post-processing scheme for DC offset removal, for robust feature extraction.

In this work, for robust features extraction, we propose to enhance the speech auditory spectrum using a weighting rule based on the subband a posteriori signal-to-noise ratio (SNR). In order to allow a realistic and controllable frequency-domain asymmetry and to model most of the level dependency observed in basilar membrane (BM) filtering, the proposed method includes the use of a compressive gammachirp filter-bank (cGCFB) [16] for auditory spectral analysis. We use a power function nonlinearity as it has been found in [3] that it is more robust than the logarithmic nonlinearity used in a conventional MFCC framework. As a post-processing scheme for the normalization of the features, we use the short-time cepstral mean and scale normalization (STCMSN) technique, proposed in [7]. Feature normalization is normally performed over the whole utterance with the assumption that the channel effect is constant over the entire utterance, such as CMN (or CMVN). Also, normalizing a feature vector over the entire utterance is not a feasible solution in real-time applications as it causes an unnecessarily long processing delay. To relax this assumption and to reduce the processing delay, cepstral features in the proposed method are normalized over a sliding window of 1.5 s duration. In this paper, we denote this proposed feature extractor as the Robust Compressive Gammachirp filterbank Cepstral Coefficients (RCGCC). Replacing cGCFB with the GTFB in the proposed RCGCC feature extraction framework, we also present Robust Gammatone Filterbank Cepstral Coefficient (RGFCC) features to show the effectiveness of using cGCFB for the auditory spectral analysis in the proposed front-end. Experimental recognition results on the AURORA-2, AURORA-4, and AURORA-5 corpora demonstrate that the proposed RCGCC feature extractor outperforms the MFCC and PLP front-ends and provides comparable (and sometimes better) results to most state-of-the-art front-ends used in this work.

The complete block diagram of the proposed robust compressive gammachirp filterbank cepstral coefficient (RCGCC) feature extractor for robust speech recognition is shown in Fig. 1
                     . In the RCGCC feature extractor, processing of a speech signal begins with pre-processing (including DC removal and pre-emphasis, typically using a first-order high-pass filter). Short-time Fourier Transform (STFT) analysis is performed using a finite duration (25 ms) Hamming window with a frame shift of 10 ms to estimate the power spectrum of the signal. Compressive gammachirp filter-bank (cGCFB) integration is performed on both speech and noise power spectra for auditory spectral analysis. A sigmoid-shaped weighting rule is applied to enhance the auditory spectrum. The 13-dimensional static features, obtained after applying a power function nonlinearity with a coefficient of 1/15 and the discrete cosine transform (DCT), are normalized using the short-time cepstral mean and scale normalization (STCMSN) technique. A detailed description of various stages of the proposed method is given below.

A Hamming-windowed direct spectrum estimator (i.e., the squared magnitude of the Fourier transform of the signal) is the most often used power spectrum estimation method for speech processing applications. For the m-th frame (frame length is 25 ms with a frame shift of 10 ms) and k-th frequency bin an estimate of the windowed periodogram can be expressed as:
                           
                              (1)
                              
                                 
                                    
                                       X
                                    
                                    
                                       ˆ
                                    
                                 
                                 (
                                 m
                                 ,
                                 
                                    
                                       k
                                    
                                    
                                       1
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       |
                                       
                                          ∑
                                          
                                             j
                                             =
                                             0
                                          
                                          
                                             N
                                             −
                                             1
                                          
                                       
                                       w
                                       (
                                       j
                                       )
                                       x
                                       (
                                       m
                                       ,
                                       j
                                       )
                                       
                                          
                                             e
                                          
                                          
                                             −
                                             
                                                
                                                   i
                                                   2
                                                   π
                                                   j
                                                   
                                                      
                                                         k
                                                      
                                                      
                                                         1
                                                      
                                                   
                                                
                                                N
                                             
                                          
                                       
                                       |
                                    
                                    
                                       2
                                    
                                 
                                 ,
                              
                           
                         where 
                           
                              
                                 k
                              
                              
                                 1
                              
                           
                           ∈
                           {
                           0
                           ,
                           1
                           ,
                           …
                           ,
                           K
                           −
                           1
                           }
                         denotes the frequency bin index, i is the imaginary unit, N is the frame length, 
                           x
                           (
                           m
                           ,
                           j
                           )
                         is the time domain speech signal and 
                           w
                           (
                           j
                           )
                         denotes the time domain window function called a taper, which usually is symmetric and decreases towards the frame boundaries (e.g., Hamming). Direct spectrum estimators, such as a Hamming-windowed periodogram, are attractive as these estimators are completely independent of data and therefore do not suffer from problems arising due to modeling deficiencies. However, these methods are not robust to noise environments.

The estimation of a noise power spectrum from the noisy speech signal plays a very important rule in noise reduction/speech enhancement algorithms. For relatively stationary noises, an accurate estimation of the noise spectrum can be done using minimum statistics (MS)-based approaches [49,50]. These algorithms lead to less satisfying results for rapidly changing noises. In this paper, for accurate estimation of noise power spectra, we employ a minimum mean square error (MMSE) – soft speech presence probability (SPP) (MMSE-SPP)-based noise estimation approach, proposed in [17]. In this method, the initial estimate of the noise power spectrum is computed by averaging the first ten frames of the speech spectrum. The advantage of this method is that it does not require a bias correction term as required by an MMSE-based noise spectrum estimation method; it also results in less overestimation of noise power and is computationally less expensive [51]. The MMSE-SPP based noise estimation procedure is summarized in Algorithm 1
                        . The reason behind choosing MMSE-SPP-based noise spectrum estimation method is that it is computationally simple and requires only one parameter to be tuned.

Dominant speech analysis techniques for speech recognition, such as MFCC [1] and PLP [2], try to emulate human auditory perception. The widely used MFCC front-end [1] was proposed more than two decades ago as a representation based on a crude approximation of the auditory response, with little rigorous justification for its implementation [45]. A more general approach for auditory spectral analysis is based on gammatone filterbanks, which involve dedicated design of mathematical forms of frequency response that match physiological experimental results. The gammatone filterbank (GTFB) consists of a series of non-uniform overlap bandpass filters, which model the frequency selectivity property of human hearing. Its name is due to the nature of its impulse response, which is a gamma envelope modulated by a tone carrier centered at 
                           
                              
                                 f
                              
                              
                                 c
                              
                           
                            Hz
                        
                        
                           
                              (2)
                              
                                 
                                    
                                       g
                                    
                                    
                                       t
                                    
                                 
                                 (
                                 t
                                 )
                                 =
                                 a
                                 
                                    
                                       t
                                    
                                    
                                       n
                                       −
                                       1
                                    
                                 
                                 
                                    
                                       e
                                    
                                    
                                       −
                                       2
                                       π
                                       b
                                       t
                                    
                                 
                                 cos
                                 (
                                 2
                                 π
                                 
                                    
                                       f
                                    
                                    
                                       c
                                    
                                 
                                 t
                                 +
                                 φ
                                 )
                                 
                                 for 
                                 t
                                 >
                                 0
                                 ,
                              
                           
                         where a, b, n, t, φ, and 
                           
                              
                                 f
                              
                              
                                 c
                              
                           
                         are the amplitude normalization constant, bandwidth of the filter in Hz, order of the filter, time, phase shift and center frequency, respectively.

Some attempts have been made to utilize GTFB in ASR for auditory spectral analysis of the speech signal, for instance [3,43,44,46–48], which show significant performance gains in various noise types and signal-to-noise ratio (SNR) levels. The compressive gammachirp filter bank (cGCFB), introduced by Irino et al. in [16], is a generalization of the GTFB. The frequency response of a gammatone filter is very nearly symmetric, which is not a good match to the auditory data, and it cannot model the level-dependent properties observed in basilar membrane (BM) filtering. The cGCFB allows a realistic and controllable frequency-domain asymmetry and has also been shown to model most of the level dependency observed in the BM filtering [16].

In this work, we have adopted cGCFB for the auditory spectral analysis of the speech signal. The frequency response of the cGCFB is given by:
                           
                              (3)
                              
                                 
                                    
                                       H
                                    
                                    
                                       c
                                       g
                                       c
                                    
                                 
                                 =
                                 
                                    
                                       a
                                    
                                    
                                       Γ
                                    
                                 
                                 ⋅
                                 
                                    |
                                    
                                       
                                          H
                                       
                                       
                                          g
                                          t
                                       
                                    
                                    (
                                    f
                                    )
                                    |
                                 
                                 ⋅
                                 
                                    
                                       e
                                    
                                    
                                       
                                          
                                             c
                                          
                                          
                                             1
                                          
                                       
                                       
                                          
                                             θ
                                          
                                          
                                             1
                                          
                                       
                                    
                                 
                                 ⋅
                                 
                                    
                                       e
                                    
                                    
                                       
                                          
                                             c
                                          
                                          
                                             2
                                          
                                       
                                       
                                          
                                             θ
                                          
                                          
                                             2
                                          
                                       
                                    
                                 
                                 ,
                              
                           
                         where 
                           
                              
                                 H
                              
                              
                                 g
                                 t
                              
                           
                           (
                           f
                           )
                         is the frequency response of the GTFB, 
                           
                              
                                 θ
                              
                              
                                 1
                              
                           
                           =
                           
                              
                                 tan
                              
                              
                                 −
                                 1
                              
                           
                           (
                           
                              
                                 f
                                 −
                                 
                                    
                                       f
                                    
                                    
                                       r
                                       1
                                    
                                 
                              
                              
                                 
                                    
                                       b
                                    
                                    
                                       1
                                    
                                 
                                 B
                                 (
                                 
                                    
                                       f
                                    
                                    
                                       r
                                       1
                                    
                                 
                                 )
                              
                           
                           )
                        , 
                           
                              
                                 θ
                              
                              
                                 2
                              
                           
                           =
                           
                              
                                 tan
                              
                              
                                 −
                                 1
                              
                           
                           (
                           
                              
                                 f
                                 −
                                 
                                    
                                       f
                                    
                                    
                                       r
                                       2
                                    
                                 
                              
                              
                                 
                                    
                                       b
                                    
                                    
                                       2
                                    
                                 
                                 B
                                 (
                                 
                                    
                                       f
                                    
                                    
                                       r
                                       2
                                    
                                 
                                 )
                              
                           
                           )
                        , 
                           
                              
                                 f
                              
                              
                                 r
                                 1
                              
                           
                         and 
                           
                              
                                 f
                              
                              
                                 r
                                 2
                              
                           
                         are the center frequencies of the low-pass asymmetric function (LP-AF) 
                           
                              
                                 e
                              
                              
                                 
                                    
                                       c
                                    
                                    
                                       1
                                    
                                 
                                 
                                    
                                       θ
                                    
                                    
                                       1
                                    
                                 
                              
                           
                         and high-pass asymmetric function (HP-AF) 
                           
                              
                                 e
                              
                              
                                 
                                    
                                       c
                                    
                                    
                                       2
                                    
                                 
                                 
                                    
                                       θ
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        , respectively. 
                           
                              
                                 a
                              
                              
                                 Γ
                              
                           
                         is an amplitude normalization factor, 
                           
                              
                                 f
                              
                              
                                 r
                                 2
                              
                           
                         is defined in terms of a frequency ratio 
                           
                              
                                 f
                              
                              
                                 
                                    ratio
                                 
                              
                           
                         as 
                           
                              
                                 f
                              
                              
                                 r
                                 2
                              
                           
                           =
                           
                              
                                 f
                              
                              
                                 
                                    ratio
                                 
                              
                           
                           ⋅
                           
                              
                                 f
                              
                              
                                 p
                                 1
                              
                           
                        , with 
                           
                              
                                 f
                              
                              
                                 p
                                 1
                              
                           
                           =
                           
                              
                                 f
                              
                              
                                 r
                                 1
                              
                           
                           +
                           
                              
                                 
                                    
                                       c
                                    
                                    
                                       1
                                    
                                 
                                 
                                    
                                       b
                                    
                                    
                                       1
                                    
                                 
                                 B
                                 (
                                 
                                    
                                       f
                                    
                                    
                                       r
                                       1
                                    
                                 
                                 )
                              
                              n
                           
                        , the peak frequency of the passive gammachirp filter [16]. So, the frequency response of the cGCFB can be obtained from the GTFB by multiplying the frequency response 
                           |
                           
                              
                                 H
                              
                              
                                 g
                                 t
                              
                           
                           (
                           f
                           )
                           |
                         of the GT filter with HP- and LP-AFs. The high-pass asymmetric function makes the pass-band of the cGCFB more symmetric at lower levels. Fig. 2
                         shows how cGCFB gains (or weights) are computed from the GTFB weights. The number of filters used in this work is 64 and 
                           
                              
                                 b
                              
                              
                                 2
                              
                           
                           =
                           2.17
                        , 
                           
                              
                                 c
                              
                              
                                 2
                              
                           
                           =
                           2.20
                        , 
                           
                              
                                 b
                              
                              
                                 1
                              
                           
                           =
                           1.81
                        , 
                           
                              
                                 c
                              
                              
                                 1
                              
                           
                           =
                           −
                           2.96
                        . Fig. 3
                         presents a comparison of shapes and weights of the Mel filterbank (MelFB), GTFB and compressive GTFB (cGCFB).

In this work, we propose a sigmoid-shaped weighting rule 
                           H
                           (
                           k
                           ,
                           m
                           )
                         to enhance the auditory spectrum 
                           
                              
                                 S
                              
                              
                                 a
                                 s
                              
                           
                           (
                           k
                           ,
                           m
                           )
                        . Proposed weighting rule is based on the subband a posteriori SNR (in dB) 
                           
                              
                                 γ
                              
                              
                                 s
                                 b
                              
                           
                           (
                           k
                           ,
                           m
                           )
                         and is formulated as [18]:
                           
                              (4)
                              
                                 H
                                 (
                                 k
                                 ,
                                 m
                                 )
                                 =
                                 
                                    1
                                    
                                       1
                                       +
                                       
                                          
                                             e
                                          
                                          
                                             −
                                             
                                                
                                                   ϑ
                                                   (
                                                   k
                                                   ,
                                                   m
                                                   )
                                                
                                                τ
                                             
                                          
                                       
                                    
                                 
                                 ,
                              
                           
                         where k is the subband index, m is the frame index, τ is a parameter that controls the lower limit of the weighting function and 
                           ϑ
                           (
                           k
                           ,
                           m
                           )
                         is the subband instantaneous SNR (in dB) defined as:
                           
                              (5)
                              
                                 ϑ
                                 (
                                 k
                                 ,
                                 m
                                 )
                                 =
                                 
                                    
                                       γ
                                    
                                    
                                       s
                                       b
                                    
                                 
                                 (
                                 k
                                 ,
                                 m
                                 )
                                 −
                                 4.5
                                 ,
                              
                           
                         where 
                           
                              
                                 γ
                              
                              
                                 s
                                 b
                              
                           
                           (
                           k
                           ,
                           m
                           )
                           =
                           max
                           (
                           10
                           
                              
                                 log
                              
                              
                                 10
                              
                           
                           (
                           
                              
                                 
                                    
                                       S
                                    
                                    
                                       a
                                       s
                                    
                                 
                                 (
                                 k
                                 ,
                                 m
                                 )
                              
                              
                                 
                                    
                                       N
                                    
                                    
                                       a
                                       s
                                    
                                 
                                 (
                                 k
                                 ,
                                 m
                                 )
                              
                           
                           )
                           ,
                           −
                           4.0
                           )
                        , 
                           
                              
                                 N
                              
                              
                                 a
                                 s
                              
                           
                           (
                           k
                           ,
                           m
                           )
                         is the noise power spectrum mapped onto the auditory frequency axis. Here, 
                           τ
                           =
                           4.5
                         is chosen experimentally.

In order to remove the outliers from the weighting function, as given by Eq. (4), due to noise variability, we use a two-dimensional median filter. For smoothing the decision regions, a two-dimensional moving average filter is also applied [18]. In order to achieve optimal performance it is desirable to have a feature extractor that is well suited both for clean and adverse acoustic conditions.

While the auditory filtering (e.g., Mel-scale filterbank) approximates the nonlinear characteristics of the human auditory system in frequency, the natural logarithmic nonlinearity or power function nonlinearity deals with the loudness nonlinearity. It approximates the relationship between a human's perception of loudness and the sound intensity [62]. Power function nonlinearity or root compression (which can be expressed as 
                           
                              
                                 y
                              
                              
                                 1
                              
                           
                           =
                           
                              
                                 y
                              
                              
                                 a
                              
                           
                        , 
                           0
                           <
                           a
                           <
                           1
                        ) followed by DCT leads to better compaction of energy and hence more robust to mismatch condition [3,63,64]. In this paper, similar to [3], we use a power function nonlinearity with a coefficient of 
                           a
                           =
                           1
                           /
                           15
                         to approximate the loudness nonlinearity of human perception. The static features, obtained after applying the discrete cosine transform (DCT), are normalized using the short-time cepstral mean and scale (STCMSN) approach [7].

In order to compensate for the effects of environmental mismatch feature normalization strategies are employed in speech (and speaker) recognition systems. These techniques are preferred because a priori knowledge and adaptation are not required under any environment. Most of the normalization techniques are applied as a post-processing scheme on the cepstral coefficient features. Normalization techniques can be classified as model-based or data distribution-based techniques. In model-based normalization techniques, certain statistical properties of speech such as mean, variance, and moments, are normalized to reduce the residual mismatch in feature vectors, e.g., short-time mean and variance normalization (STMVN). Data distribution-based techniques aim at normalizing the feature distribution to the reference, such as short-time Gaussianization (STG) [54,55]. Some feature normalization techniques have been proposed in the past for speech and speaker recognition systems, including feature warping [54], STG [55], cepstral mean normalization (CMN) [56,57], cepstral variance normalization (CVN) [58], histogram equalization [59] and RASTA filtering [60,61]. Almost all the feature extractors include feature normalization technique as a post-processing scheme. Feature normalization is normally performed over the whole utterance with the assumption that the channel effect is constant over the entire utterance, such as CMN, mean and variance normalization (MVN). Also, normalizing a feature vector over the entire utterance is not a feasible solution in real-time applications as it causes unnecessarily long processing delay. To relax this assumption and to reduce the processing delay MFCC features are normalized over a sliding window of more than 1 s duration. The feature vector to be normalized is located at the center of the sliding window.

In the proposed feature extractor the static features, are normalized using the short-time cepstral mean and scale normalization (STCMSN) approach [7] over a sliding window of 1.5 s duration (i.e., 150 frames for a frame shift of 10 ms). In STCMSN approach the n-th feature space and m-th frame 
                           c
                           (
                           n
                           ,
                           m
                           )
                         are normalized as
                           
                              (6)
                              
                                 
                                    
                                       c
                                    
                                    
                                       s
                                       t
                                       m
                                       s
                                       n
                                    
                                 
                                 (
                                 n
                                 ,
                                 m
                                 )
                                 =
                                 
                                    
                                       c
                                       (
                                       n
                                       ,
                                       m
                                       )
                                       −
                                       
                                          
                                             μ
                                          
                                          
                                             s
                                             t
                                          
                                       
                                       (
                                       n
                                       ,
                                       m
                                       )
                                    
                                    
                                       
                                          
                                             d
                                          
                                          
                                             s
                                             t
                                          
                                       
                                       (
                                       n
                                       ,
                                       m
                                       )
                                    
                                 
                                 ,
                              
                           
                         where 
                           
                              
                                 μ
                              
                              
                                 s
                                 t
                              
                           
                           (
                           n
                           ,
                           m
                           )
                         and 
                           
                              
                                 d
                              
                              
                                 s
                                 t
                              
                           
                           (
                           n
                           ,
                           m
                           )
                         are the short-time mean and short-time difference between the upper and lower bound, respectively. Short-time mean 
                           
                              
                                 μ
                              
                              
                                 s
                                 t
                              
                           
                           (
                           n
                           ,
                           m
                           )
                         of the n-th feature space is defined for a short-time window of 
                           L
                           =
                           150
                         frames as:
                           
                              (7)
                              
                                 
                                    
                                       μ
                                    
                                    
                                       s
                                       t
                                    
                                 
                                 (
                                 n
                                 ,
                                 m
                                 )
                                 =
                                 
                                    1
                                    L
                                 
                                 
                                    ∑
                                    
                                       j
                                       =
                                       m
                                       −
                                       L
                                       /
                                       2
                                    
                                    
                                       m
                                       +
                                       L
                                       /
                                       2
                                    
                                 
                                 c
                                 (
                                 n
                                 ,
                                 j
                                 )
                                 .
                              
                           
                         Short-time difference between the upper and lower bound 
                           
                              
                                 d
                              
                              
                                 s
                                 t
                              
                           
                           (
                           n
                           ,
                           m
                           )
                         of the n-th feature space is defined for a short-time window of 
                           L
                           =
                           150
                         frames as:
                           
                              (8)
                              
                                 
                                    
                                       d
                                    
                                    
                                       s
                                       t
                                    
                                 
                                 (
                                 n
                                 ,
                                 m
                                 )
                                 =
                                 
                                    max
                                    
                                       (
                                       m
                                       −
                                       L
                                       /
                                       2
                                       )
                                       ⩽
                                       j
                                       ⩽
                                       (
                                       m
                                       +
                                       L
                                       /
                                       2
                                       )
                                    
                                 
                                 
                                 
                                    (
                                    c
                                    (
                                    n
                                    ,
                                    j
                                    )
                                    )
                                 
                                 −
                                 
                                    min
                                    
                                       (
                                       m
                                       −
                                       L
                                       /
                                       2
                                       )
                                       ⩽
                                       j
                                       ⩽
                                       (
                                       m
                                       +
                                       L
                                       /
                                       2
                                       )
                                    
                                 
                                 
                                 
                                    (
                                    c
                                    (
                                    n
                                    ,
                                    j
                                    )
                                    )
                                 
                                 .
                              
                           
                         The main idea behind STCMSN technique is that under mismatched conditions a difference of log spectrum between the training and test environments is removed by adjusting the short-time mean and short-time scale. The advantage of short-time feature normalization technique is that it does not use constant channel assumption as used in the full utterance-based feature normalization method, and reduces the unnecessary long processing delay [7]. Fig. 4
                         presents the zero-th cepstral coefficient feature (c0) of a clean speech signal and that of a noisy speech signal, having a signal-to-noise ratio (SNR) of 5 dB, (a) before applying feature normalization and (b) after normalizing by the STCMSN method. It is observed from Fig. 4(a) that the effects of additive noise on a clean speech signal are:
                           
                              •
                              The minimum values of the cepstral features are elevated.

The valleys of the cepstra are affected by the additive noise energy while the peaks remain almost unaffected.

@&#PERFORMANCE EVALUATION@&#

The AURORA-2 [5], AURORA-4 [52] and AURORA-5 [6] corpora are used for the performance evaluation of the proposed RCGCC (robust compressive gammachirp filterbank cepstral coefficients) feature extractor and for comparing its performances to the conventional MFCC, PLP (HTK version of PLP), ETSI-AFE [4], and PNCC [3] features, in the context of speech recognition. The robust gammatone filterbank cepstral coefficient (RGFCC) feature extractor is also presented and compared with the proposed RCGCC front-end. For performance evaluation we have considered both matched (training and test environments are the same) and mismatched (test environment is different from the training environment) environments. A brief description of the corpora, mentioned above, used for the performance evaluation are given in the following section.


                        
                           AURORA-2
                        : The AURORA-2 database, widely used for noise robust front-end evaluation, consists of connected digit (English) task spoken by American English talkers [5]. There are two training sets (clean training set and multi-condition training set) and three test sets (test sets A, B and C). The clean training set, constitutes mismatched training/testing conditions [7], used in this work consists of 8440 clean speech recordings only from 55 male and 55 female adult speakers. The noisy training and test utterances are obtained by artificially corrupting the clean digit utterances with 8 different additive noises (subway, babble, car and exhibition hall, restaurant, street, airport and train-station) and 2 convolutional noises (MIRS (modified intermediate reference system) filtered subway and street noise) at SNRs of 20, 15, 10, 5, 0, and −5 dB. The number of test utterances for each noise type and SNR level (clean, 20, 15, 10, 5, 0, and −5 dB) is 1001. Test sets A, B and C are composed of 28 028, 28 028, and 14 014, speech recordings, respectively. The speech recordings in the three test sets are uttered by 52 female and 52 male speakers.


                        
                           AURORA-5
                        : The AURORA-5 corpus comprises real time recording in a meeting room, recorded in a hands-free mode at the ICSI (International Computer Science Institute) in Berkeley [6]. The database consists of 2388 utterances from 24 speakers with 7800 digits in total. The speech was captured with four different microphones, labeled as mic 6, mic 7, mic E and mic F, placed at the middle of the table in the meeting room. The recordings contain only a small amount of additive noise, providing the typical effect of hands-free recording in a reverberant room.


                        
                           AURORA-4
                        : The AURORA-4 continuous speech recognition corpus, derived from the Wall Street Journal (WSJ0) corpus, is divided into 3 sets, namely, training, development (dev test) and evaluation (eval or test) sets. This task is often referred to as the 5k closed vocabulary task, i.e., there are no out-of-vocabulary words (OOVs) in the evaluation set. The training set contains 7138 utterances from 83 speakers, totaling 14 h of speech data. 14 evaluation sets were defined in order to study the degradations in speech recognition performance due to microphone conditions, filtering and noisy environments. Each of the filtered versions of the evaluation set recorded with a Sennheiser microphone and secondary microphone was selected to form the two evaluation sets. The remaining 12 subsets were defined by randomly adding each of the 6 noise types (car, babble, restaurant, street traffic, airport, and train-station noises) at a randomly chosen SNR between 5 and 15 dB for each of the microphone types as mentioned above. The goal was to have an equal distribution of each of the 6 noise types and the SNR with an average SNR of 10 dB [52]. Each of the test sets contains 166 utterances from 8 speakers, totaling 20.69 min of speech data. The 14 test sets are grouped into the following 4 families A, B, C, and D [52,53] as described in Table 1
                        . The number inside the brackets represents the test set number defined in the AURORA-4 corpus [52]. More details about the AURORA-4 database (AURORA-2 as well as AURORA-5) can also be found in [19].

For the performance evaluation of our proposed feature extractor, we have chosen matched and mismatched conditions. Features extracted from the clean training data are used for training the recognizer and decoding is performed on both matched and mismatched (due to additive noise, channel/handset distortion and reverberation) environments. For our experiments, we use 13 static features (including the 0-th cepstral coefficient) augmented with their delta and double delta coefficients, making 39-dimensional feature vectors. The analysis frame length is 25 ms with a frame shift of 10 ms. The Delta and double features were calculated using a 7-frame (lag size 3-frame) and 5-frame (lag size 2-frame) window, respectively. For all the feature extractors, the features, before appending delta and double delta features, are normalized using the feature normalization technique: MVN is used for MFCC, and PLP features, CMN is used for PNCC, ETSI-AFE uses a blind equalization technique (for PNCC and ETSI-AFE exactly the same normalization methods are used as mentioned in the respective references, i.e., in [3,4]) and in the proposed RCGCC method STCMSN is used to normalize the feature vectors. In order to provide a comparison of performances, in terms of speech recognition accuracy, of using logarithmic nonlinearity versus power-law nonlinearity and MVN versus STCMSN, we considered the following versions of MFCC front-ends:


                        
                           MFCC
                        : Conventional Mel-frequency cepstral coefficient (MFCC) features that utilize logarithmic nonlinearity for compression and the MVN method for feature normalization.


                        
                           MFCC_I
                        : Mel-frequency cepstral coefficient (MFCC) features that utilize a power-law nonlinearity for compression and the MVN method for feature normalization.


                        
                           MFCC_II
                        : Mel-frequency cepstral coefficients (MFCC) features that utilize a power-law nonlinearity for compression for compression and the STCMSN method for feature normalization.

To show the reason behind choosing a compressive gammachirp filterbank (cGCFB) instead of a gammatone filterbank, we also compared the performance of the RCGCC feature extractor to that of the RGFCC (robust gammatone filterbank cepstral coefficients). The only difference between the RCGCC and RGFCC front-ends is in the auditory filterbank integration. In the RGFCC, a gammatone filterbank is used instead of cGCFB.

For the recognition task we use the HTK speech recognizer. For the connected digit recognition task on the AURORA2 and AURORA-5 corpora the back-end configuration in the standard AURORA evaluation framework [5] was used: 16-state whole-word HHM models for the digits, 3-state and 1-state HMM models for silence (sil) and short pause (sp), respectively. Each whole-word HMM model was associated with a 3-component GMM (Gaussian mixture model) for the state emitting probability density. 6-component GMM is used for the sil/sp states.

For the continuous speech recognition task on the AURORA-4 corpus, all experiments employed state-tied crossword speaker-independent triphone acoustic models with four Gaussian mixtures per state. A single-pass Viterbi beam search-based decoder was used along with a standard 5K lexicon and bigram language model with a prune width of 250 [52,53].

For the performance evaluation of the proposed feature extractor method and comparing the recognition performances of the feature extractors considered in this paper, the percentage word accuracy (%WAcc) is used. WAcc is a more representative figure of the recognizer performance. WAcc is defined as
                           
                              (9)
                              
                                 
                                    WAcc
                                 
                                 =
                                 
                                    
                                       N
                                       −
                                       D
                                       −
                                       S
                                       −
                                       I
                                    
                                    N
                                 
                                 ×
                                 100
                                 %
                                 ,
                              
                           
                         where N is total number of words in the test database, D is the number of deletions (i.e., words of the original sentence missing in the recognized sentence), S is the number of substitutions (i.e., words of the original sentence each appearing substituted by a different word in the recognized sentence), and I represents the number of insertions (i.e., the recognized sentence includes a new word between two words of the original sentence). The standard way to evaluate ASR performance of a new system (
                           
                              
                                 WAcc
                              
                              
                                 new
                              
                           
                        ) is by measuring relative improvement (RI) with the baseline system (
                           
                              
                                 WAcc
                              
                              
                                 
                                    baseline
                                 
                              
                           
                        ). RI can be defined
                           
                              (10)
                              
                                 
                                    RI
                                 
                                 =
                                 
                                    
                                       
                                          
                                             WAcc
                                          
                                          
                                             
                                                new
                                             
                                          
                                       
                                       −
                                       
                                          
                                             WAcc
                                          
                                          
                                             
                                                baseline
                                             
                                          
                                       
                                    
                                    
                                       100
                                       −
                                       
                                          
                                             WAcc
                                          
                                          
                                             
                                                baseline
                                             
                                          
                                       
                                    
                                 
                                 ×
                                 100
                                 .
                              
                           
                         A positive RI indicates how much performance is improved by the new system than the baseline and a negative value indicates how much degradation of performance is provided by the new system over the baseline.

In order to achieve optimal performance it is desirable to have a feature extractor that is well suited both for clean and adverse acoustic conditions. Figs. 5, 6, and 7
                        
                        
                         present the auditory spectrum of the clean speech signal (Fig. 4) and noisy speech signals (Figs. 5–7) obtained (a) without and (b) with applying the sigmoid-shaped weighting rule of the proposed method. It is observed from Figs. 5–7 that the auditory spectrum enhancement (ASE) method of the proposed feature extractor did not introduce any distortion to the clean auditory spectrum and enhanced the noisy auditory spectrum. The effectiveness of using a power-law nonlinearity and short-time cepstral mean and scale normalization (STCMSN) for noise robust performance is demonstrated in Table 2
                        , in the context of speech recognition experiments on the AURORA-2 & -5 corpora, by incorporating them in the MFCC feature extraction framework. It is observed from Table 2 that use of the power-law nonlinearity (MFCC_I) provided an average gain of 0.77% and 4.5% in word recognition accuracy on the AURORA-2 and AURORA-5 corpora, respectively, over a logarithmic compression (e.g., MFCC). Normalizing the cepstral features by the STCMSN method (e.g., MFCC_II) instead of the MVN method (e.g., MFCC_I) yielded an additional average gain of 3.1% and 2.8% in recognition accuracy on the AURORA-2 and AURORA-5 corpora, respectively.

The most important modules of the proposed RCGCC feature extractor are: compressive gammachirp filterbank, noise spectrum estimation – subband a posteriori SNR-based auditory spectrum enhancement, power-function nonlinearity and STCMSN. In order to show how much each of these modules contributes to the noise robust performance, we compared the performance of the RCGCC and RGFCC and RCGCC_I (here, for feature normalization CMN is utilized instead of the STCMSN) front-ends on the AURORA-4 LVCSR task. Comparing the performances of the RCGCC and RCGCC_I, from the presented results in Table 3
                        , it is evident that the contribution (in terms of recognition accuracy) of STCMSN as the feature normalization module in the proposed feature extractor is about 3.8% on the average. In the LVCSR task on the AURORA-4 corpus (Tables 3 and 6), there is not much difference in performance between the RCGCC and RGFCC, i.e., the contribution of the compressive gammachirp filterbank is almost the same as that of the gammatone filterbank. In the case of a small-vocabulary connected-digits task on the AURORA-2 and -5 corpora (see Tables 4 and 5
                        
                        ), the proposed RCGCC method performed better than the RGFCC in terms of word recognition accuracy. Since our previous work [18] was based on the small-vocabulary connected-digits task and we found that the compressive gammachirp filterbank performed better than the gammatone filterbank, we chose the compressive gammachirp filterbank for auditory spectral analysis in the proposed front-end.


                        Table 4 presents the average word accuracy (in %), averaged over the 10 noise scenarios, mentioned in Section 3.1, of all the three test sets A, B, and C of the AURORA-2 connected digit corpus. In additive noise conditions on the TIdigits task, the proposed RCGCC feature provided comparable results to that of the PNCC and ETSI-AFE features and outperformed the MFCC and PLP features. Average relative improvements (RI in %) obtained by the proposed feature are presented in the last column of Table 4.

In order to demonstrate the performance of the proposed feature extractor in real-time reverberant environments (i.e., not simulated reverberant environments), experiments were conducted on the meeting recorder digits (MRDs) subset of the AURORA-5 corpus. Table 5 presents a comparison of the performances in terms of word accuracy (in %) of the proposed feature extractor with the other considered feature extractors, including the RGFCC, in a real-time reverberant environment on the AURORA-5 corpus. In convolutive noise conditions, the proposed method provided consistently better word accuracy on all microphones of the AURORA-5 corpus than all other methods. The average relative improvements obtained by the proposed method over the MFCC, PLP, PNCC, and ETSI-AFE front-ends are 35.6%, 41.0%, 11.3%, and 66.1%, respectively. From Table 5 it is evident that the ETSI-AFE front-end has the lowest word accuracy compared to all other feature extractors. In a reverberant environment the ETSI-AFE front-end is not as effective as its performance in additive background noise distortions, which is consistent with the studies of [10]. The proposed RCGCC method performs well both in additive noise distortions and reverberation on the TIdigits task. The recognition accuracies (%WAcc) obtained by all the feature extractors used in our experiments on the AURORA-4 LVCSR task are presented in Table 6
                        . The average relative improvements achieved by the proposed method are 33.8% and 30.6% over the MFCC and PLP features, respectively. The proposed feature extractor provided comparable recognition accuracy to the PNCC and ETSI-AFE front-ends on the LVCSR task. In matched conditions (training the recognizer using clean training features and decoding on the clean test features) of all the three corpora, the proposed method performed well compared to the other two robust front-ends.

Under mismatched conditions RCGCC provided comparable results to the PNCC and ETSI-AFE front-ends. Performances of all the front-ends, including the RCGCC and RGFCC front-ends, are also presented in Tables 7 and 8
                        
                         under various noise scenarios (car, babble, restaurant, street, airport, and train-station) on the test sets B and D, respectively, of the AURORA-4 corpus. Computationally, measured in terms of the time taken to extract features from all AURORA-4 data, the proposed RCGCC method was found less expensive compared to the PNCC and ETSI-AFE. In order to extract features from 11 758 speech recordings (AURORA-4 corpus, average duration of utterances = 7.0 s) on an AMD machine (clock speed 2300 MHz, 12 core, 128 GB RAM) MFCC, RCGCC, PNCC, and ETSI-AFE (C implementation) front-ends took 867, 1694, 24 842 and 2950 s, respectively. All the feature extractors except ETSI-AFE were implemented in MATLAB. The execution time mentioned above for each method is the average of 4 execution times.

@&#CONCLUSIONS@&#

In this paper we presented a robust feature extractor, denoted in this paper as robust compressive gammachirp filterbank cepstral coefficients (RCGCC), that includes the use of an asymmetric level-dependent compressive gammachirp filterbank (cGCFB) for mapping the speech spectrum onto the auditory frequency axis, a simple auditory domain spectrum enhancement technique based a sigmoid type suppression rule, an MMSE – soft speech presence probability (SPP)-based noise spectrum estimator method [17], and a short-time cepstral mean and scale normalization (STCMSN) method [7], for speech recognition tasks. Speech recognition experiments are performed on the AURORA-2 and AURORA-5 TIdigits corpora and the AURORA-4 large vocabulary continuous speech recognition (LVCSR) corpus to cover matched as well as various mismatch conditions due to additive background noise, channel distortion and real time reverberation. It is evident from the reported results that the proposed method provided comparable results to that of the ETSI-AFE and PNCC front-ends under additive noise distortions (AURORA-2 and AURORA-4 tasks) and performed better under reverberant environments (on the AURORA-5 task). The proposed robust feature extractor performed well both in matched and mismatched training/test environments and was also found computationally, measured in terms of execution time, less expensive.

@&#ACKNOWLEDGEMENTS@&#

The authors would like to thank the reviewers for their relevant and constructive remarks which have enabled us to significantly improve the quality of the paper.

@&#REFERENCES@&#

