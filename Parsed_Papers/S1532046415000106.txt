@&#MAIN-TITLE@&#HBLAST: Parallelised sequence similarity – A Hadoop MapReducable basic local alignment search tool

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           HBLAST partitions both database and input query sequences using Hadoop MapReduce.


                        
                        
                           
                           The dynamic virtual partitioning method optimises use of underlying hardware.


                        
                        
                           
                           It minimises data segmentation and recompilation on memory constrained hardware.


                        
                        
                           
                           HBlast is compared against CloudBlast, a Hadoop based BLAST alternative.


                        
                        
                           
                           It demonstrates well balanced computational load and improved scalability.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Cloud computing

Sequence alignment

Bioinformatics

Big data

Genomics

Hadoop

@&#ABSTRACT@&#


               
               
                  The recent exponential growth of genomic databases has resulted in the common task of sequence alignment becoming one of the major bottlenecks in the field of computational biology. It is typical for these large datasets and complex computations to require cost prohibitive High Performance Computing (HPC) to function. As such, parallelised solutions have been proposed but many exhibit scalability limitations and are incapable of effectively processing “Big Data” – the name attributed to datasets that are extremely large, complex and require rapid processing. The Hadoop framework, comprised of distributed storage and a parallelised programming framework known as MapReduce, is specifically designed to work with such datasets but it is not trivial to efficiently redesign and implement bioinformatics algorithms according to this paradigm. The parallelisation strategy of “divide and conquer” for alignment algorithms can be applied to both data sets and input query sequences. However, scalability is still an issue due to memory constraints or large databases, with very large database segmentation leading to additional performance decline. Herein, we present Hadoop Blast (HBlast), a parallelised BLAST algorithm that proposes a flexible method to partition both databases and input query sequences using “virtual partitioning”. HBlast presents improved scalability over existing solutions and well balanced computational work load while keeping database segmentation and recompilation to a minimum. Enhanced BLAST search performance on cheap memory constrained hardware has significant implications for in field clinical diagnostic testing; enabling faster and more accurate identification of pathogenic DNA in human blood or tissue samples.
               
            

@&#INTRODUCTION@&#

Advances in next generation sequencing technologies coupled with decreasing wet lab costs [1] has resulted in unprecedented acquisition of vast genomic data sets. To translate the promise of these data into new biological discoveries, innovative computational approaches are required for timely and efficient processing and analysis.

Aligning sequences to determine similarity is an essential and widely used computational procedure for biological sequence analysis in computational biology and bioinformatics. [2,3]. A wide range of computational algorithms have been applied to the sequence alignment challenge, including slow, yet accurate, methods like dynamic programming and faster but less accurate heuristic or probabilistic methods. The Basic Local Alignment Search Tool (BLAST) [4], a heuristic version of the pairwise local alignment Smith Waterman algorithm [5], remains the most widely used computational procedure for alignment interrogating biological databases based on a heuristic version of the pairwise local alignment Smith Waterman algorithm. It compares the similarity of a reference protein or DNA sequence against a database of sequences, above a specified threshold, and returns similar, statistically significant, matches.

In spite of its heuristic approach, it still faces significant scalability challenges associated primarily with the necessity to search new and ever expanding databases; such as UniMES for metagenomic data sets [6] which continue to expand exponentially as Next Generation Sequencing (NGS) costs continue to decline. BLAST, along with most other bioinformatics algorithms, is designed to execute locally i.e. sequentially. However, the increased throughput of genome sequencing has led to massive data generation requiring a significant increase in the speed of execution of these algorithms. The advent of cloud computing and big data “scale out” technologies such as Hadoop provide cost effective processing of terabyte sized data sets so it is now possible to analyse these vast datasets rapidly; an important requirement in the rapidly expanding field of molecular diagnostics [7].

Thus, as the size of genomic data sets increase more rapidly than local processing power and disk read speed, it is intuitive to port these naturally parallel bioinformatics tasks to utilise the Hadoop MapReduce framework. Popular approaches to parallelising BLAST using Hadoop are three fold: The first and most common approach distributes the input query sequences amongst a cluster of nodes, the second approach partitions the database amongst nodes and finally a hybrid approach partitions both the input sequences and the database. The drawback of the first approach is that it exhibits limited scalability and load balancing does not occur with a small number of input sequences. The second approach requires a sophisticated algorithm to partition the database in order to ensure scalability and optimal performance. Furthermore, it results in high disk I/O. The final hybrid approach is preferable as it handles large databases as well as a large number of input query sequences, however it is the most complicated to implement and deploy while minimising inter-node communications and optimising the partitioning methods.

This paper presents HBlast, a hybrid “virtual partitioning” approach that automatically adjusts the database partition size depending on the Hadoop cluster size as well as the number of input query sequences.

A traditional parallelised approach includes the Message Passing Interface (MPI) [8] a parallel programming paradigm where a root device spawns programs on all machines in its “MPI World”. More recently, CUDA, NVIDIA’s parallel programming model for modern graphic processors (GPUs) addresses highly parallel computations on a single node.

Darling et al. proposed mpiBLAST [9], partitioning a sequence database based on a modified version of NCBI BLAST with Sul et al. proposing MR-MPI-BLAST which utilises the NCBI BLAST program with an MPI wrapper [10]. The drawback of MPI based systems is that they offer limited scalability, particularly when working with large data sets. Data locality i.e. processing the data where it is stored is not considered, with data instead moved over the network to be computed on a different physical node. In contrast, modern paradigms such as Hadoop use local storage and processing to avoid network bottlenecks. Furthermore, Hadoop explicitly considers fault tolerance which is not supported by default in MPI distributions.

GPU based approaches such as GPU-BLAST [11], SWCUDA [12], CUDASW++ [13] and GPU Smith-Waterman [14] have also been proposed. However, such approaches exhibit inherent drawbacks such as high power consumption and lower performance over multicore servers. Furthermore, the use of GPUs does not lift memory constraints like MPI or Hadoop, exhibits limited scalability and programming complexity.

More recently, the Hadoop framework [15], an Apache project developed from an architecture proposed by Google, has demonstrated significant ability to process terabyte and petabyte sized data sets in a timely and cost effective manner using a scale out approach. It is an open-source software framework for distributed storage and large scale parallel processing of data-sets on commodity hardware clusters. The architecture is comprised of two components: the Hadoop Distributed File System (HDFS) and MapReduce, the JAVA programming Application Program Interface (API).

Both HDFS and MapReduce are based on a master–slave architecture. In HDFS, the master, referred to as the Name Node is responsible for maintaining metadata relating to the distribution of files (partitioned into blocks) across the cluster. These blocks are then distributed to slaves, referred to as Data Nodes. Each block is replicated across three data nodes by default, to ensure fault tolerance. Block replication and robustness of the cluster in the case of Data Node failure is an inherent characteristic of HDFS. In MapReduce, the master known as the JobTracker is responsible for allocating MapReduce tasks to nodes within the cluster, ideally those nodes that actually store the data. These slave nodes are referred to as Task Trackers which execute the map tasks i.e. java code in parallel. Results are then sent to nodes that run reduce tasks (another piece of java code). Reduce tasks typically aggregate the results output from the distributed mappers and write the final results to HDFS. While previously we reviewed the use of Hadoop in genomics [16], herein we focus on Hadoop based BLAST specific schemes.

Solutions such as CloudBurst [17] and MapReduce Blast [18] propose custom BLAST implementations, restructured in the format of a MapReduce algorithm, rather than using the well validated NCBI BLAST program. The drawback of this and a subtle prohibiting factor associated with MapReduce is that the code needs to be designed and implemented to fit the map and reduce paradigm, a task which is not trivial, and which unintentionally introduce algorithmic errors. Furthermore, CloudBurst is designed for the alignment of small sequences.

CloudBlast [19] proposes a solution where query sequences are distributed and processed in parallel. Such an approach works well and is fault tolerant for small databases with many queries. However, if the database is large and exceeds the local node’s RAM the performance drops dramatically. Furthermore, additional effort is required to ensure load balancing and optimal use of the cluster. With a small number of input query sequences, few data nodes will execute, leaving remaining nodes idle. K-mulus [16] also approaches the parallelization of BLAST with the distribution of input query sequences. The pre-processed database partitions clusters of similar sequences which are distributed to all nodes in the cluster via the Hadoop Distributed Cache mechanism. The speed up in processing is achieved via light weight indexing of the clusters so that only database partitions similar to the input sequences are searched. K-mulus has scalability problems similar to CloudBlast in that it does not work well with large clusters or with large databases.

bCloudBLAST [16] developers designed a hierarchical solution where both input sequences and database partitions are distributed among several Hadoop clusters. A manager tier controls the job trackers in each cluster with the job trackers distributing to the task tracker nodes. Two approaches were taken; firstly, to distribute precompiled database partitions over all clusters’ nodes or secondly to have a specific cluster responsible for one particular partition. Input query sequences are split and sent to the appropriate clusters. However, the scalability and complexity of the solution still exhibits some drawbacks e.g. the database partitioning needs to be done manually depending on cluster configuration i.e. number of nodes, number of clusters, RAM of individual nodes, etc.

The premise of the proposed HBlast solution, similar to bCloudBLAST, is also the parallel distribution of both the database and the input files containing the query sequences to form inputs to each map task on the Hadoop data nodes. However, unlike bCloudBlast, HBlast utilizes a single Hadoop cluster which is more cost effective and has the flexibility to modify its strategy from segmenting only the database to segmenting only input query sequences in a more flexible way, without database recompilation. Furthermore, in contrast to all other Hadoop based outlined approaches, HBlast automatically adjusts the database partition size depending on the number of nodes in the Hadoop cluster and the number of input query sequences. This is accomplished with runtime virtual partitioning of the database.

HBlast achieves virtual partitioning of the database at runtime by firstly dividing the database into a number of smaller partitions and uploading these to HDFS. Secondly, these partitions are combined to form larger “virtual partitions”. Each virtual partition aims to fit completely in a node’s RAM while still providing full utilisation of each of the cluster nodes.

Thus the primary advantages of HBlast over current approaches are:
                        
                           •
                           Automated database partitioning before job submission, with no recompilation necessary if the cluster scales.

Ability to run on memory constrained nodes when the database does not fit RAM.

Improved load-balancing via virtual partitioning of the database and splitting of input files with query sequences.

Improved scalability over existing parallel BLAST approaches.

Little inter-node communication and mapper launch overhead.

Simplicity of deployment.

The experimentation described in this paper makes use of the NCBI database. HBlast uses the standard NCBI BLAST tool ‘makeblastdb’ to convert and partition the database in FASTA format into a set of binary files which can be used by native BLAST applications. The compiled version of the database is partitioned into multiple parts with a maximum size of 1.5GB each. Each partition can be considered as a stand-alone database. An arbitrary set of partitions can be combined to form a “virtual partition” which fits in local RAM and is input to the map tasks running on each Hadoop data node, as illustrated in Fig. 1
                        .

It is difficult to predict the necessary partition size until the Hadoop job is submitted, because the number of nodes can shrink or grow. Furthermore it is preferable in terms of performance to split the query files in order to more efficiently load balance the job and as the number of sequences in the input query files can vary. When choosing the smallest partition size HBlast takes into account two important aspects: Firstly, a database should ideally fit into the local RAM of a Hadoop node as otherwise heavy disk I/O will rapidly slow down the database search process. Secondly, it is not possible to make all N nodes work without partitioning the database into at least N pieces. Virtual partition size is deducted from the maximum number of partitions required to cover both use cases. Figs. 2a and 2b
                        
                         show the two extreme use cases.


                        Fig. 2a shows a cluster of 99 nodes – to run a database search with a single query, the database can be divided into 99 parts. These database fragments are distributed among all nodes. Due to the large number of pieces the size of one database piece is relatively small and can fit into the RAM of any node. This will result in a very fast search but will take considerable effort to aggregate the results.


                        Fig. 2b presents a different challenge when choosing the most appropriate strategy for searching. In this scenario, there are many input queries which can be split into 99 sets to be further distributed to the nodes. However, the database is so large that only a part of it can fit into RAM a time. The database has to be divided into several partitions and processing must occur one partition a time on a single node to minimize disk I/O.

It is important to note that the number of virtual partitions should be minimised as the more partitions that exist, the slower native BLAST applications will work due to the internal operation of the algorithm. This was highlighted through HBlast experimentation where if 1 BLAST process was executed on 1 full nt database and 100 BLAST searches were executed on 100 partitions of it, the accumulated result of the latter will be at least 3 times slower regardless of the input query sizes utilised.

When preliminary database partitioning and compilation are finished, each database partition will be distributed into HDFS and will stay there to form virtual partitions for all subsequent HBlast runs. HBlast then receives the database and an input query sequence file and calculates the number of map tasks necessary to keep the cluster workload balanced while utilising local node memory within its limits. The input query sequence file will also be split to facilitate this. The input file i.e. input record for the map task of the Hadoop job consists of the filename for the input query sequences and the virtual partition i.e. smaller physical database partitions collectively forming the virtual partition. A map task record is the set of key and values that is input into a Hadoop mapper i.e. java code to allow the code to fulfil its job. The format of the value of the input record for the map task is illustrated in Fig. 3
                        . This is described further in the next section.

The optimal splitting of the input query sequence file and assembly of the virtual partitions is achieved as follows, with pseudo code displayed in Fig. 4
                        :
                           
                              (1)
                              To keep the cluster balanced HBlast tries to divide the job into tasks 3 times larger than the number of nodes.

HBlast checks the minimum and maximum number of database partitions to fulfill memory and work distribution requirements. It then counts the number of query sequences in the input file and splits the file so that their combination with virtual partitions will result in the number of map tasks as specified in step 1.

When the map preparation step is finished, all files are distributed into HDFS. A Python script in the form of a Hadoop driver file is then used to load the NCBI BLAST executables and run the MapReduce job executing BLAST in parallel.

The HBlast MapReduce algorithm is outlined and illustrated in Fig. 5
                        , along with input and output keys and values.

Each mapper running on a data node receives as an input record a line from the file prepared during the query file/database virtual partitioning step described in the previous section. Each record is comprised of a key and a value. The key is the byte offset in the file which is ignored by the mapper task. The value is the file name of the query sequence data and a set of database partition files that will constitute a virtual partition for this particular map task. This virtual partition must fit into the node’s RAM. Since Hadoop nodes can run on multicore processors, several map processes can run simultaneously. Instead of running multiple maps on the same node, HBlast utilizes the multithreading mode of NCBI BLAST – this allows for easier memory management and less database partitions. A large number of partitions slow down the BLAST search process as observed by Darling et al. for mpiBLAST [7] and proven via experimental evaluation with HBlast.

Once the input record is received, each HBlast mapper executes the NCBI blastn program using Python streaming. On first execution, each mapper retrieves the necessary query sequence file and database partitions from HDFS to their local file system and subsequent runs use the local system as a caching mechanism. The output of this process is a key which is the query sequence ID and a list of values containing the database sequence IDs and their corresponding identity scores. In accordance with default MapReduce shuffle and sort behaviour, all of the same keys go to the same reducer.

The reducer further sorts this information according to identity score or any configured parameter of interest. It outputs the queried sequence IDs sorted by identity score pairs.

HBlast and comparative schemes are evaluated using a Hadoop cluster on a private cloud infrastructure. Physical servers are comprised of two 8 core Intel Xeon 2.1GHz CPUs with hyper threading (32 vCPUs). Each physical node contains 128GB RAM, two 300GB SCSI disks with the hosts connected via a 1GbE switch. Each node runs XenServer 6.2 hypervisor with Virtual Machines (VMs) configured for Hadoop and assigned 4GB of RAM. The Hadoop cluster consists of 1 name node/job tracker and 15 data nodes/task trackers. Each Hadoop node runs the Cloudera Distribution of Hadoop (CDH4), with the data nodes running NCBI BLAST+2.2.28.

Experiments were conducted on the 52GB NCBI nt (nucleotide sequence) database comprised of data sets from Genbank and EMBL amongst others [20] as well as input query sequence data. Processing this database requires a high number of read requests to disk. In contrast, a subset of smaller nt databases were also compiled, with some of the smaller databases fitting entirely into local RAM on the virtual servers. Input query sequence data is comprised of 100 sequences derived from contigs. These were assembled with a 5-miseq assembly pipeline [21] from staphylococcus reads [22] with the distribution of query sequence lengths displayed in Fig. 6
                     . Input query lengths consumed 2.72MB in storage. The average sequence length is 28,000 bases with 75% of sequences comprised of over 1000 bases and 50% with a length greater than 10,000. Eight of the longest sequences range between 101,000 and 152,000 bases.

The performance of HBlast is evaluated with respect to database and query sequence scalability, cluster size and cloud provider performance. It is compared against CloudBlast, representative of one of the most cited parallelised BLAST solutions. CloudBlast parallelization is based on the distribution of input query sequences distribution among Hadoop nodes. HBlast enhances this by allowing for parallelization of database access via technique called virtual partitioning. Experiments were conducted to determine whether virtual partitioning of the database along with the input query distribution is optimal for memory constrained systems.

HBlast is firstly compared with respect to the timeliness to run the BLAST algorithm against the full nt database, as well as two smaller database subsets, as a function of increasing number of query sequences. The performance is illustrated in Fig. 7
                        .

HBlast and CloudBlast have comparable performance on small subsets of the nt database (up to 2 million sequences; Fig. 7). Such databases fit in the node’s local RAM and thus in the case of HBlast a virtual partition will include the whole database and will not be different from a compiled database used by CloudBlast.

Both HBlast and CloudBlast need to fetch the database from HDFS to the local file systems and incur a small amount of processing overhead related to the distribution of database partitions to the processing nodes i.e. Hadoop data nodes. However, it should be noted that in case of HBlast this overhead is minimised further by caching the database in the local file systems.

As the database size is increased to 5 million sequences, a noticeable degradation in the performance of CloudBlast is evident. This is incurred as the database size is larger than the RAM available in the nodes. It can be observed that for an nt database subset of 5 million sequences CloudBlast is over 2.5 times slower than HBlast.

Finally, as shown in Fig. 7, CloudBlast completely fails to function when evaluated with the full nt of 23 million sequences. This is as a result of the CloudBlast requirement to cache the entire database on each data node in order for the algorithm to function. Given the size of the database, it is not feasible to cache large databases in local memory, highlighting the lack of scalability of this solution. In contrast, HBlast demonstrates linear scalability with the increase of database size and number of query sequences.

Both schemes demonstrate identical quality with respect to the BLAST output.

It should be noted that the performance of the BLAST parallelisation solutions discussed in Section 2 that use database partitioning, degrades when there are too many partitions. This was a major drawback of mpiBLAST [7]. HBlast minimises this effect by including input query sequence distribution and was evaluated across a variable number of nodes i.e. cluster size, to test this strategy, with the performance shown in Fig. 8
                        .

The performance gain of HBlast is close to linear when adding more processing nodes to the Hadoop cluster.

Finally, the performance of HBlast is evaluated with respect to public vs private cloud performance. This identified a further drawback of CloudBlast and other solutions discussed in Section 2. These solutions are not compatible with public cloud infrastructure solutions such as the AWS Elastic Map Reduce (EMR) service which relies on the Simple Storage Service (S3). Fig. 9
                         compares the performance of HBlast using the described private cloud environment vs a 16 node public AWS cloud cluster using a ‘c1.xlarge’ instance comprised of 4 virtual CPUs, 7.5GB RAM and 2×40GB disks. An input query set of 10,000 sequences and the full nt database were utilised.

As seen in Fig. 9, the use of public cloud infrastructure leads to further delays. This can be attributed to the use of S3 storage and multi-tenancy. S3 storage is a cloud based solution accessed via the HTTP protocol which is used to store any input data necessary for transfer into the Hadoop cluster. It is thus impossible to put any data into the cluster before the AWS EMR job is submitted. On job submission, the input data is copied from S3 to the newly created cluster, incurring additional delay. Multi-tenancy is a key trait of cloud computing where physical resources are shared among many users. It is therefore common that programs running on behalf of a particular user compete with other user’s programs for CPU, network bandwidth or disk I/O. Thus, multi-tenancy does not guarantee a user the exclusive use of the same underlying physical resources which further contributes to delay.

To deal with the continuous increase in sequence alignment data, a distributed database and input query sequence partitioning strategy is proposed. Furthermore, BLAST is executed in parallel based on an algorithm derived using the MapReduce programming framework. These components, collectively referred to as HBlast, are advantageous over existing approaches due to faster execution, ease of deployment and improved scalability. Experimental results show that HBlast demonstrates improved performance on both large and small databases with linear scalability. It is memory, disk and network friendly during all phases of processing with fast database distribution and little or no disk and network activity with data residing completely in RAM. Future work will address the introduction of a complex key to avoid further sorting by identity score in the reducer. Further refinements may involve expanding the algorithm to include global alignments; enabling the identification of weak, yet significant hits which would otherwise be missed using the local, heuristic approach of BLAST. Such improved global alignment strategies will likely facilitate enhanced phylogenetic analysis [23] – enabling improved molecular probe identification and design with obvious benefits to our future health and wellbeing.

@&#ACKNOWLEDGMENTS@&#

This work is supported through the EC FP7 Marie Curie Industry Academia Partnership Pathways Programme (IAPP) as part of the ClouDx-I FP7-PEOPLE-2012-IAPP Project.

@&#REFERENCES@&#

