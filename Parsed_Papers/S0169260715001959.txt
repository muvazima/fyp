@&#MAIN-TITLE@&#Reliable emotion recognition system based on dynamic adaptive fusion of forehead biopotentials and physiological signals

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A new dynamic fusion method for designing an emotion recognition system is proposed.


                        
                        
                           
                           A weight is assigned to each classifier based on its performance.


                        
                        
                           
                           The performance of the classifiers during the training and testing phases is considered.


                        
                        
                           
                           Static weights in varying contexts such as emotions do not produce acceptable results.


                        
                        
                           
                           Dynamic weighting strategy improves the performance of the system considerably.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Emotion recognition system

Dynamic adaptive fusion of classification units

Forehead bioelectric signals

Physiological signals

Human computer interactions

@&#ABSTRACT@&#


               
               
                  In this study, we proposed a new adaptive method for fusing multiple emotional modalities to improve the performance of the emotion recognition system. Three-channel forehead biosignals along with peripheral physiological measurements (blood volume pressure, skin conductance, and interbeat intervals) were utilized as emotional modalities. Six basic emotions, i.e., anger, sadness, fear, disgust, happiness, and surprise were elicited by displaying preselected video clips for each of the 25 participants in the experiment; the physiological signals were collected simultaneously. In our multimodal emotion recognition system, recorded signals with the formation of several classification units identified the emotions independently. Then the results were fused using the adaptive weighted linear model to produce the final result. Each classification unit is assigned a weight that is determined dynamically by considering the performance of the units during the testing phase and the training phase results. This dynamic weighting scheme enables the emotion recognition system to adapt itself to each new user. The results showed that the suggested method outperformed conventional fusion of the features and classification units using the majority voting method. In addition, a considerable improvement, compared to the systems that used the static weighting schemes for fusing classification units, was also shown. Using support vector machine (SVM) and k-nearest neighbors (KNN) classifiers, the overall classification accuracies of 84.7% and 80% were obtained in identifying the emotions, respectively. In addition, applying the forehead or physiological signals in the proposed scheme indicates that designing a reliable emotion recognition system is feasible without the need for additional emotional modalities.
               
            

@&#INTRODUCTION@&#

Emotion/affect is an internally mental perception of an object or event that can be associated with an expressive overt behavior. Incorporation of emotional intelligence in computers can improve their interactions with humans. Since human computer interactions (HCIs) have become unavoidable, more accurate HCIs that are comparable with human–human interactions are more desired and beneficial for users. Emotional skills power computers to understand and recognize human emotional states and give an appropriate response [1,2]. For example, a person who is working with a computer in a state of impatience or boredom expects the computer's response to be faster and more accurate [1]. In addition, for a student in an online learning environment, learning efficiency would increase if the computer were able to detect his or her emotions and create an appropriate situation according to this state [1]. Various applications for computers and systems identify different emotions, such as human-like communications, robotics, learning environment, entertainment, and games, providing assistance or feedback to a person, human–computer mediated systems, medicine, mental health, etc. [1–3].

Given the importance of emotions in human life, a new research field related to emotional phenomena was introduced by Rosalind Picard: affective computing [1]. The goal of affective computing is to design systems that are emotionally intelligent. To design an emotional system that detects different emotions, methods utilized by humans in their everyday communications such as facial expressions, speech and sounds, gestures, and body movements [4–11] as well as physiological patterns can be used [12–23]. For example, Motta and Picard [9] and D’Mello and Graesser [10] analyzed children's physical movements with the use of a body pressure measuring system (BPMS) to evaluate their interest in a learning environment and working with computers.

It has been shown that specific physiological patterns in each emotion are created. Ekman and Levenson presented the first findings that considerable changes in the autonomic nervous system (ANS) are created in accordance with emotional scenarios [24]. Since physiological signals are the result of ANS activity, they cannot be easily imitated. These signals are the same among people with different languages, cultures, and even gender and age [12,13]. In one of the first studies that used physiological signals, Picard and her colleagues recognized eight emotional states using blood volume pressure (BVP), skin conductance (SC), respiration rate (RR), and facial muscle activity [14]. They used personalized imagery to elicit the desired emotions from an actor and achieved overall recognition accuracy of 81% using hybrid linear discriminant classification. Nasoz et al. applied galvanic skin response (GSR) and heart rate variability (HRV) to recognize six types of emotions induced by selected movie clips [17]. The researchers achieved the best recognition accuracy rate of 83% with the Marquardt backpropagation algorithm. Yuan Lin and his colleagues identified four music induced emotional states (joy, anger, sadness and pleasure) from electroencephalogram (EEG) signals. They attained an average recognition accuracy of 82% using the SVM classifier [25]. In another study, Frantizidis et al. proposed a two-step classification procedure for discriminating emotional states from EEG signals [26]. The first classification level involved arousal discrimination, and then valence discrimination was performed. Using Mahalanobis distance (MD) and SVM classifiers, overall classification rates of 79.5% and 81.3%, respectively, were obtained [26]. AlZoubi et al. designed an emotion recognition system for detecting eight emotional states (e.g., boredom, confusion, curiosity, delight, flow/engagement, surprise, and neutral) during interactions between students and a tutoring system [20]. The researchers used three physiological signals (electrocardiogram (ECG), electromyogram (EMG), and GSR) and combinations. In that study, single-channel and three-channel multimodal models were generally more diagnostic than two-channel models. Recently, Khosrowabadi et al. applied a six-layer biologically inspired feedforward neural network to discriminate human emotions from EEG [27]. In that study, EEG data were collected from participants while they were subjected to audio-visual stimuli. Overall classification accuracy of 70.8% and 71.4% was obtained for arousal and valence discrimination, respectively.

Since the human body system may utilize a combination of emotional methods to represent a specific emotion, multimodal emotional systems have been proposed by researchers investigating affect [11–14,17,20,22,23,28–32]. An important consideration when designing a multimodal system is combining or creating fusion between the signals information. Fusion methods are usually implemented at the level of the extracted features or the results from individual classification units [33–37]. Fusing input signals due to a lack of the same time resolution usually is not considered.

In the design of a multimodal emotion recognition system, as we described some of them, the conventional method of feature fusion as well as simple fusion of classification units has been applied more frequently [11–23,28–32]. For example, Kim et al. applied the simple feature fusion scheme by concatenating the features extracted from the physiological signals to identify four types of emotional states [13]. Koelstra et al. implemented a statistical weighting scheme at the decision-level fusion of the classification units to determine the contribution of each emotional modality. They applied EEG and peripheral physiological signals as emotional measures with multimedia content analysis (MCA) [32]. Their system using fusion of all the modalities recognized arousal, valence, and liking/disliking ratings of music video-induced emotions, with F1-scores of 0.616, 0.647, and 0.618, respectively. In addition, Wagner et al. evaluated different statistical decision-level fusion such as weighted majority voting, weighted average, maximum, minimum and median rules, and more as well as cascading specialist, for designing a multimodal emotion recognition system using facial, gestural, and speech signals [11]. They obtained the best average accuracy of 55% using an emotion-adapted decision-level fusion scheme for identifying four categories of emotions: positive-high, positive-low, negative-high, and negative-low.

Although in feature-level fusion complete information on the signals is available, the system's response rate is decreased due to the high-dimensional features set [11]. In addition, the created feature set may contain redundant information that is not necessary for the system. In classifier or decision-level fusion, the signals create several independent classification units, and then the individual subsystem results are combined in a predetermined manner. In this type of fusion, if some of the classifiers produce incorrect results, other subsystems compensate and ultimately create the desired output. In addition, each unit applies only the features of some of the signals; therefore, the response rate of the system can be higher than the feature-level fusion scheme [11]. Despite all these advantages, in the classifier fusion schemes some of the signals may not be sufficiently informative, and therefore, their corresponding classification units do not act properly. This can undesirably influence the final accuracy of the ensemble system. Therefore, because of the varied performances of the modalities, their acceptability in the multimodal system is not the same [11,32]. The fusion process can be improved by using an effective weighting method, such that classification units with better performance obtain greater weights and therefore contribute more to the system's results [38].

In this study, a new adaptive fusion method is proposed that combines the results of individual classification units in a multimodal emotion recognition system. The work is based on the idea that more effective units should have greater impact on the system result. In this regard, a dynamic weight is assigned to each classification unit that determines its acceptability in the multimodal emotional system. The weights of the classification units are determined by considering their performance in both training and testing phases. Bioelectric signals recorded from the forehead and physiological measurements make up emotional information in our multimodal emotion recognition system. The forehead electrodes are placed in such a way that although the minimum number of electrodes is used, emotional information of the brain, facial muscle activity, and eye movements is provided. In addition, we use three types of peripheral physiological measurements common in emotion-related studies to provide sufficient emotional information for a reliable emotion recognition system.

In both types of signals, the minimum number of recording electrodes and sensors is utilized. However, the possibility of further simplifications of the system is examined. The emotion recognition system should be simplified as much as possible when it is used for practical applications. We explore whether utilizing the proposed fusion method with a smaller number of electrodes and sensors in an appropriate design of the system is feasible or not. This makes the recording process less obtrusive to users.

The remaining of this paper is structured as follows. In Section 2, we first briefly describe the structure of the emotion recognition system. In this section, details about the experimental procedures, the physiological signals, methodology, and the proposed adaptive fusion scheme are provided. In Section 3, we present and discuss the results. More discussions and conclusions are presented in Sections 4 and 5, respectively.

Designing a reliable emotion recognition system is a challenging task. The intensity of the elicited emotion, as well as its duration, is not the same for everyone. Regarding the action of different organs in an emotional state, emotions are beyond physical reactions but include the inner sense, thoughts, and other changes of which an individual may not be aware. Emotions are associated with personality, mood, and temperaments as well as an individual's experiences [32]. Thus, accurate identification of different emotional states is a complex issue.

The basic structure of the emotion recognition system used in this work is depicted in Fig. 1
                        . In each part, different methods have been examined by affective researchers to design an acceptable emotional system [4–23]. Eliciting an accurate emotional state comparable with the factual emotional situation, performing effective preprocessing, and using appropriate methods in feature extraction and classification units can improve the performance of the system. In addition, to design an emotion recognition system, several issues must be considered, including emotion stimuli and assessment method, the model of emotions, and whether the system is user dependent or independent.


                        Emotion stimuli and assessment method: To elicit a specific emotion, appropriate stimuli should be presented. The emotion should be elicited as accurately as possible for the person comparable with real-world emotional situation.

Picard et al. introduced five factors that can influence emotion elicitation results: subject elicited versus event elicited, laboratory setting versus real world, expression versus feeling, open recording versus hidden recording, and emotion purpose versus other purpose [14].

In emotion-related studies, stimuli with controlled conditions are usually presented in the laboratory environment. In this work, we used selected emotional videos to elicit the desired emotional states and a self-assessment method to verify the authenticity of the elicited emotions [39,40]. Gross and Levenson showed that categorized emotional films performed better due to their dynamic nature [39]. The international affective picture system (IAPS) [41], music [13,18,23,25], thoughts and personal imagery, remembering emotional situations and events [14,42] in addition to multimodal methods [12] are other possible approaches for eliciting different emotional states.


                        Model of emotions: Six basic emotions (happiness, sadness, fear, disgust, anger, and surprise) proposed by Ekman's model (1987), were considered in our study [43]. In the discrete model of emotion, emotional states are considered separately since they do not have common attributes. In the dimensional model, different emotions are categorized based on their comparable characteristics in two or more dimensions. An accepted dimensional model considers emotions in two dimensions, arousal and valence [44]. Arousal ranges from the low to high level of excitation while valence presents the pleasant and unpleasant nature of emotions. Fig. 2
                         shows the basic emotions applied in this study in a two-dimensional model of arousal and valence.


                        User dependent or independent: The emotion recognition system in this work is user independent [11,13,23,25,26,32,45]. However, the first studies in this field involved user-dependent methods [11,14]. Since the incidence of emotions as a result of differences in individual, ethnic, and cultural characteristics is not exactly the same for all people, emotion recognition systems can be considered dependent on users. But user-dependent systems require complex and time-consuming training processes for each person. In addition, user-dependent systems are not expected to generalize for novel individuals [20]. Such a system loses comprehensiveness and generality for the desired application.

Two groups of subjects participated in our experiment: one for labeling emotional stimuli and the other for recording physiological signals. The first group was composed of 25 healthy subjects with an age range of 24–28 years (mean=26.2 and SD=1.95), and the second group included 25 healthy subjects with an age range of 23–32 years (mean=25.52 and SD=2.77). All subjects were male students. The subjects in the first group were not allowed to participate in the second phase. Before the study began, all subjects filled out and signed a questionnaire and an informed consent form. In the questionnaire, we inquired about the subjects’ health status and possible unwanted effects of the emotional videos on the subjects’ mental and psychological states. Subjects with these problems are excluded from continuing the experiment. All methods in this study were approved by the University of Tarbiat Modares's Medical Ethics Committee before we collected data.

According to Ekman's discrete model of emotions, six basic emotions (anger, sadness, fear, disgust, surprise, and happiness) were considered in this work [43]. To elicit a specific emotion for the individual participating in the second phase of the experiment, a video that had been previously labeled (by the subjects in the first group) for that emotion was displayed.

The first group of subjects participated in multiday sessions and labeled the video clips. After the participants viewed each video, they were presented a second questionnaire in which they rated the amount of emotion induced by using a self-assessment method on a scale from 0 to 8 indicating the low to high intensity levels of the evoked emotional state (0 means not at all, 4 somewhat, and 8 extremely or a great deal) [46]. The videos were ranked for each emotional state according to the scores, and those with highest scores were applied in the second phase of the experiment for recording the physiological signals.

For the second group of the subjects, to evoke the desired emotion independent of each previous one, avoid habituation effects, and reduce the effects of emotional interferences, the selected emotional videos were displayed randomly, and the desired signals were recorded at the same time. Before the experiment was started for each subject, the emotional videos were randomly numbered from 1 to 6. Then the participant randomly chose the numbers, and the corresponding videos were displayed. The experimental protocol for eliciting each emotional state is depicted in Fig. 3
                           .

Before each recording, the subjects were directed to relax with their eyes closed for 2min. In addition to creating a fixed baseline mode, before an emotional video was displayed, a video with a neutral emotional state was presented, and the corresponding physiological signals were collected. In all cases, the laboratory environment conditions and the recording process such as temperature (21–25°C), light, and humidity of the room are kept the same for all subjects as much as possible. The intensity level of the sound and brightness of the monitor were adjusted so that the subject felt comfortable. After we showed each video, we asked the subjects whether they were comfortable in the experiment environment and whether they were tired. If they were tired, the experiment was continued at another session. Table 1
                            briefly describes the specifications of the emotional videos, the recording process, and the signals.

To ensure the authenticity of the elicited emotion, at the end of each recording, a second questionnaire similar to the first group was presented to the subjects [46]. Each subject rated the amount of emotion induced by using the described self-assessment method.

Recorded signals from those subjects who gave the highest score were utilized in the subsequent steps to design the emotion recognition system. If the desired emotion was not elicited for a person (with a score lower than 4), another video was displayed. Fig. 4
                            shows the appraisal rate of each emotion during the recording process.

In this study, bioelectric signals recorded from the forehead (EEG, EMG and Electrooculogram (EOG)) as well as BVP, SC, and interbeat intervals (IBI) were utilized as emotional modalities. We used the FlexComp Infiniti
                              1
                           
                           
                              1
                              
                                 http://www.thoughttechnology.com.
                            recording device and a laptop (Core i3 CPU with speed of 2.53GHz and 4GHz of RAM) to collect the signals. In all recordings, the sampling frequency was adjusted to 2048Hz.

To make a recording that contained suitable emotional information with the minimum number of electrodes, configuration of the forehead electrodes was based on studies conducted by Firoozabadi (2008) and Rezazadeh et al. (2010) [18,47,48]. This configuration collected three types of biosignals, forehead electroencephalogram (fEEG), forehead electromyogram (fEMG), and forehead electrooculogram (fEOG), which are significant modalities for emotion-related studies. The electrodes were placed bipolar with 2cm spacing. Bioelectric signals were recorded from the Frontalis and Temporalis muscles, which have important roles in lifting the eyebrows and moving the mouth and cheeks. On the Frontalis muscle, electrodes were placed at the top of the eyebrows and at the beginning of the nasal muscle. On the Temporalis muscles, the electrodes were positioned on the top and bottom of the eye line. In our configuration, the electrodes were close to the Fp1, Fp2, F7, and F8 sites of the 10–20 system (international standard for EEG electrode placement). Reference electrodes were placed on the left and right ear lobes as well. Therefore, we used three pairs of forehead electrodes to collect bioelectric signals.

To record the peripheral physiological signals, related sensors were attached by straps on the subjects’ fingers. The BVP sensor was placed on the fleshy part of the first joint of the middle finger, and two SC sensors were placed on the middle joint of the index and ring fingers. The IBI signal is derived from the BVP signal automatically by the FlexComp recording device. Fig. 5
                            shows the placement of the forehead biosignal electrodes and the peripheral physiological sensors. In the following, we present more information on the recorded physiological signals.


                           Electroencephalogram signal provides information about the electrical activity of the human brain. According to the 10–20 system, the temporal (T3–T6), frontal (F3, F4), and prefrontal (Fp1, Fp2) lobes play important roles in emotional activities [19,49]. Previous studies suggested that the left and right hemispheres of the brain are devoted to specific types of emotions [49]. The left anterior hemisphere is associated with approach or positive emotions while the right anterior hemisphere is responsible for the withdrawal response or negative emotions [49]. Since the alpha (8–12Hz) and beta (13–30Hz) subband frequencies [26,32] are dominant at rest and in a state of excitement and alertness, respectively, the amount of brain activity in these modes could be related to the dimensional model of emotion. The ratio of the beta band power to the alpha band power is an indicator of emotional arousal [50]. The alpha and the beta band power in the left and right hemispheres can be compared to determine emotional valence [45].


                           Electromyogram signal, which indicates muscles activity, can be used to provide emotional information (facial EMG has been more considered in this area) [20,32]. Facial expressions are created as a result of the activities of each of the 44 symmetrical facial muscles. Determination of the most effective muscles and appropriate placement of the electrodes are important in obtaining desirable emotional information. In previous emotion-related studies, several facial muscles such as the Corrugator (moves the eyebrows), the Zygomaticus major and the Levator (lifts the corners of the lips), and the Masseter (clench the jaw) have been investigated more. Each muscle creates specific patterns of arousal and valence in different emotional states; for example, the Corrugator is associated with a high level of emotional arousal. In addition, the Zygomaticus major is used more in high-valence emotions and the corrugator in low-valence emotions [20,51].


                           Electrooculogram is a biopotential that represents the quality of eye movements. The EOG shows the movements of a single electrical dipole oriented from the retina to the cornea [52]. The EOG, pupil size, and gaze direction signals has been used in affective and HCI-related researches [32,53]. Since the new placement method uses three pairs of forehead electrodes, emotional information can be retrieved from all three bioelectric signals [47,48].


                           Blood volume pressure is the amount of blood flow in the vessels and is usually measured with plethysmography [32]. BVP specifies the amount of infrared light reflected by the skin. Other common signals used in emotion-related studies such as heart rate (HR), HRV, and IBI (which is approximately the reverse of the heart rate) can be determined from BVP. Previous studies showed that specific BVP patterns are created in different emotional states; for example, stress can increase blood pressure [32]. Studies have also shown that anger, fear, and sadness create greater HR than disgust [54]. Furthermore, low arousal emotional states are associated with decreased BVP and HR.


                           Skin conductance is a measurement of the electrical resistance of the skin and changes due to the activity of the perspiratory glands. SC (GSR and electrodermal activity (EDA) can be applied interchangeably) increases with an arousing emotion [20,13]. Kim et al. showed a linear correlation between skin conductance and arousal changes in emotional states [13]. The results of previous studies suggest that using skin conductance and its characteristics can help distinguish among different emotions.

Extracted physiological signals should be conditioned and preprocessed to obtain acceptable and trusted results. Different noises and artifacts may affect the quality of the signals. Quality of signals is dependent on numerous factors, the most important of which are recording equipment, environment (electromagnetic interferences), placement of the measurement sensors, and the preparation and collaboration of the subjects (movements, mental concentration, etc.). However, to reduce these undesirable effects, many activities can be performed. Due to the subtle nature of the physiological measurements, noise from movement of the electrodes and leads is unavoidable.

For the recorded signals, we deleted the first and last 10s of the signals that were more exposed to artifacts [13]. It is performed to remove transient noise created as a result of the subjects’ movements during the recording process, mostly at the beginning and at the end of each recording [13]. In addition, a 50Hz notch filter was applied through the recording device to avoid power line interference. Using the electrode configuration for recording prefrontal signals, by using proper filtering, the fEEG, fEMG, and fEOG parts can be extracted. A Chebyshev type II filter with −60dB attenuation at the stop band was applied. Pass bands of the filters for separating fEOG, fEEG, and fEMG signals were adjusted in 1–4Hz, 5–50Hz, and 60–500Hz intervals, respectively.

For designing a reliable emotion recognition system, choosing appropriate and efficient features of the signals is of special importance. The required emotional information is generated by the selected features. If the data specifying the desired emotion are more accurate, identifying the state will be easier. In this study, we consider the combination of the time and frequency domain features for the physiological signals. The features are listed in Table 2.

One of the most important considerations for the applied features in designing an emotion recognition system is their simplicity and acceptable computational speed that make them suitable for real-time applications [13]. Thus, we used simple time and frequency domain features that did not require complex transformations and heavy computations. The 192 features described in Table 2
                         were calculated for the signals. We normalized the features extracted for each emotion in order to determine the amount of changes that occurred from relaxation (or without emotion) to that state of specific emotion [17,23]. It is also performed to minimize individual differences between the subjects in experiencing a particular emotion.

The features in Table 2 were also computed for the neutral emotional states and the values for the normalized features, F
                        normalized, were determined as follows:
                           
                              (1)
                              
                                 
                                    
                                       F
                                       
                                          normalized
                                       
                                    
                                    =
                                    
                                       
                                          
                                             F
                                             
                                                emotional
                                             
                                          
                                          −
                                          
                                             F
                                             
                                                neutral
                                             
                                          
                                       
                                       
                                          
                                             F
                                             
                                                neutral
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where F
                        emotional and F
                        neutral are the feature values extracted from an emotional state and a neutral emotion, respectively.

To use the most salient features and eliminate non-relevant ones and therefore speed up the training phase of the system, the best subset of the features for each signal was determined. Sequential forward floating selection (SFFS) was utilized for this goal [56]. SFFS is an improved version of the sequential forward selection (SFS) method. SFFS is similar to SFS in nature but does not calculate all combinations of the features.

As a result of using floating search, the SFFS method avoids the nesting problem of the selected features [56]. Initially, a subset of the features is selected based on the accuracy of the system similar to other forms of wrapper methods. Then other features are added to or removed from the subset to create the best significant selected one. This process can be terminated before the features end [56]. Fig. 6
                         briefly represents the SFFS algorithm.

Following the preparation of the feature set, it is employed for training a classifier in order to identify the desired emotional states. Two types of classification methods namely Support Vector Machine and k-nearest neighbors were evaluated in this work.

The SVM aims to generate a separating hyper-plane in such a manner that the margin between data of different classes is maximized. For a given training set of input–output pairs (x
                           
                              i
                           , y
                           
                              i
                           ), where 
                              
                                 
                                    x
                                    i
                                 
                                 ∈
                                 
                                    ℝ
                                    n
                                 
                              
                           , i
                           =1, 2, …, l and y
                           ∈{1, −1}
                           
                              l
                           , the following optimization problem must be solved [26]:
                              
                                 (2)
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         min
                                                      
                                                      
                                                         ω
                                                         ,
                                                         b
                                                         ,
                                                         ξ
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      1
                                                      2
                                                   
                                                   
                                                      ω
                                                      T
                                                   
                                                   ω
                                                   +
                                                   C
                                                   
                                                      ∑
                                                      
                                                         i
                                                         =
                                                         1
                                                      
                                                      l
                                                   
                                                   
                                                      
                                                         ξ
                                                         i
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             
                                                
                                                   subject
                                                    
                                                   to
                                                
                                             
                                             
                                                
                                                   
                                                      y
                                                      i
                                                   
                                                   (
                                                   
                                                      ω
                                                      T
                                                   
                                                   ϕ
                                                   (
                                                   
                                                      x
                                                      i
                                                   
                                                   )
                                                   +
                                                   b
                                                   )
                                                   ≥
                                                   1
                                                   −
                                                   
                                                      ξ
                                                      i
                                                   
                                                   ,
                                                    
                                                   
                                                      ξ
                                                      i
                                                   
                                                   ≥
                                                   0
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        


                           ξ
                           
                              i
                            Measures the misclassification of x
                           
                              i
                           , C
                           >0 determines the penalty parameter of the error term and ω is the normal vector of the separating hyper-plane. In addition, function ϕ maps the training vector x
                           
                              i
                            to a higher dimensional space. The SVM finds a linear separating hyper-plane with the maximum margin. Different kernel functions, K(x
                           
                              i
                           , x
                           
                              j
                           )≡
                           ϕ(x
                           
                              i
                           )
                              T
                           
                           ϕ(x
                           
                              j
                           ), can be used to support vector classification, such as linear, polynomial, radial basis function (RBF), and sigmoid. In the present study, we used RBF kernel function 
                              
                                 K
                                 (
                                 
                                    x
                                    i
                                 
                                 ,
                                 
                                    x
                                    j
                                 
                                 )
                                 =
                                 exp
                                 
                                    
                                       −
                                       γ
                                       
                                          
                                             
                                                
                                                   
                                                      x
                                                      i
                                                   
                                                   −
                                                   
                                                      x
                                                      j
                                                   
                                                
                                             
                                          
                                          2
                                       
                                    
                                 
                              
                            
                           [26]. The grid search method was applied to optimize the parameters γ and C 
                           [58]. In addition, we implemented a pair-wise SVM, based on the one-against-one approach, for our multiclass emotion recognition problem [59].

An instance is classified by the majority vote of its k-nearest neighbors, measured by a distance function [45]. A Euclidean distance measure, defined by the following equation, was used in our system. 
                              
                                 
                                    x
                                    i
                                    1
                                 
                              
                            and 
                              
                                 
                                    x
                                    i
                                    2
                                 
                              
                            are the ith features of the first and second class respectively.
                              
                                 (3)
                                 
                                    
                                       d
                                       =
                                       
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                n
                                             
                                             
                                                
                                                   
                                                      (
                                                      
                                                         x
                                                         i
                                                         1
                                                      
                                                      −
                                                      
                                                         x
                                                         i
                                                         2
                                                      
                                                      )
                                                   
                                                   2
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

In multimodal systems, fusion can be applied at three levels: the input data, extracted features, or classification units. Fusion at the decision level, or classification unit, is of greater interest, as it has more effective advantages, which include the following:
                           
                              (1)
                              Increasing the response rate of the system, as a result of breaking the features set down into several smaller sets.

The possibility of using more effective features of the signals independently.

Improving the accuracy of the system, as a result of the capacity of the correct units to compensate for the error of those that are incorrect.

In adaptive classifier fusion, more efficient units can make a greater contribution to the results. A weight is assigned to each classifier according to specific criteria that determine its reliability and importance in the multimodal system. The proposed method in this study is based on the dynamic fusion of the classification units [60]. In the following, we define several criteria that specify the weight of each unit. The testing phase results and the performance of the classifiers in the training condition are utilized to produce the desired weight of each classifier. The first criterion is defined regarding the classification units’ performance in identifying the emotional state; while the second knowledge component takes into account the information content of the signals, regardless of the classifiers’ performance. In addition, the third criterion creates the dynamic nature of the weight of each classification unit which is determined according to the performance of the classifier during the testing phase to identify the arousal level of the intended emotion.


                        a) Classifier commitment coefficient: This coefficient reflects the commitment level of each classifier decision in the multimodal system. Let i
                        =1, 2,…, P be the number of classifiers; j
                        =1, 2,…, M be the number of classes or emotional states in our study; and α
                        
                           ij
                         be the activation of the ith classifier for the jth class. The commitment coefficient that is defined as follows compares the performance of the classifiers in the identification of the desired classes [60].
                           
                              (4)
                              
                                 
                                    
                                       α
                                       i
                                    
                                    =
                                    
                                       α
                                       
                                          i
                                          l
                                       
                                    
                                    −
                                    
                                       1
                                       
                                          M
                                          −
                                          1
                                       
                                    
                                    
                                       ∑
                                       
                                          
                                             
                                                
                                                   j
                                                   =
                                                   1
                                                
                                             
                                          
                                          
                                             
                                                
                                                   j
                                                   ≠
                                                   l
                                                
                                             
                                          
                                       
                                       M
                                    
                                    
                                       
                                          α
                                          
                                             i
                                             j
                                          
                                       
                                    
                                 
                              
                           
                        where α
                        
                           il
                         is the activation of the most activated class for the ith classifier.

b) Cross-classifier correlation coefficient: This measure represents the amount of agreement between different classifiers in generating the system results. In this study, the mutual information coefficient as a correlation measure is determined for the calculated features of the signals. Mutual information indicates the amount of information that one classification unit conveys about other ones [38]. The mutual information 
                           
                              
                                 β
                                 
                                    m
                                    ,
                                    n
                                 
                                 j
                              
                           
                         between the classification units, m and n for class j, is defined as [38]:
                           
                              (5)
                              
                                 
                                    
                                       β
                                       
                                          m
                                          ,
                                          n
                                       
                                       j
                                    
                                    =
                                    
                                       1
                                       2
                                    
                                    log
                                    
                                       
                                          
                                             
                                                
                                                   Σ
                                                   m
                                                
                                             
                                          
                                          ⋅
                                          
                                             
                                                
                                                   Σ
                                                   n
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   Σ
                                                   
                                                      m
                                                      ,
                                                      n
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

where Σ
                        
                           m
                         and Σ
                        
                           n
                         are the variances of the features utilized by the classifiers m and n, and Σ
                        
                           m,n
                         is the covariance between these features. Cross-classifier correlation coefficient for the classifier i, is defined as:
                           
                              (6)
                              
                                 
                                    
                                       β
                                       i
                                    
                                    =
                                    
                                       1
                                       M
                                    
                                    ⋅
                                    
                                       1
                                       
                                          P
                                          −
                                          1
                                       
                                    
                                    
                                       ∑
                                       
                                          j
                                          =
                                          1
                                       
                                       M
                                    
                                    
                                       
                                          ∑
                                          
                                             
                                                
                                                   
                                                      
                                                         i
                                                         1
                                                      
                                                      =
                                                      1
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         i
                                                         1
                                                      
                                                      ≠
                                                      i
                                                   
                                                
                                             
                                          
                                          P
                                       
                                       
                                          
                                             β
                                             
                                                i
                                                ,
                                                
                                                   i
                                                   1
                                                
                                             
                                             j
                                          
                                       
                                    
                                 
                              
                           
                        
                     

c) Classifier confidence level: This measure is the amount of confidence of each unit in the multimodal system. For example, if the goal is to identify a person, in the appropriate lighting conditions, video data are better than audio. In the present study, this level of confidence for each unit is determined according to how to detect emotional arousal. Different emotional states can be identified in terms of the arousal and pleasure levels. As a measure of the subsystem abilities, if the amount of emotional arousal identified by a classifier is the same as the results for a higher number of the classifiers, it will receive a greater weight. In the testing phase, if the number of the classifiers that identically recognize arousal level (high or low) of the desired emotion is n
                        max, the confidence level coefficient of each classifier is defined as follows:
                           
                              (7)
                              
                                 
                                    
                                       γ
                                       i
                                    
                                    =
                                    
                                       n
                                       
                                          max
                                       
                                    
                                 
                              
                           
                        
                     

According to these criteria, we define the weight of each classification unit as:
                           
                              (8)
                              
                                 
                                    
                                       μ
                                       i
                                    
                                    =
                                    
                                       
                                          
                                             α
                                             i
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                k
                                                =
                                                1
                                             
                                             P
                                          
                                          
                                             
                                                α
                                                k
                                             
                                          
                                       
                                    
                                    ⋅
                                    
                                       
                                          
                                             β
                                             i
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                k
                                                =
                                                1
                                             
                                             P
                                          
                                          
                                             
                                                β
                                                k
                                             
                                          
                                       
                                    
                                    ⋅
                                    
                                       
                                          
                                             γ
                                             i
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                k
                                                =
                                                1
                                             
                                             P
                                          
                                          
                                             
                                                γ
                                                k
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (9)
                              
                                 
                                    
                                       ω
                                       i
                                    
                                    =
                                    
                                       
                                          
                                             μ
                                             i
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                k
                                                =
                                                1
                                             
                                             P
                                          
                                          
                                             
                                                μ
                                                k
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Such that ω
                        
                           i
                        
                        ≥0 for i
                        =1, 2 and 
                           
                              
                                 ∑
                                 
                                    i
                                    =
                                    1
                                 
                                 P
                              
                              
                                 
                                    ω
                                    i
                                 
                                 =
                                 1
                              
                           
                        . Fig. 7
                         shows the weights calculation procedures [60].

In this work, a new adaptive weighting method for fusing the results of the classification units was evaluated to design a reliable emotion recognition system. In the structure of the system, we considered three subsystems with the idea that emotional information to be utilized for each part almost equally. In the first part of the system, only the features of the fEEG signals were applied. The second part was designed based on the combination of the fEMG and fEOG features. Finally, emotional information extracted from the physiological signals was used to create the third part of the system. Fig. 8
                         shows the system structure.

A total of 55, 45, 45, 19, 16, and 12 features were determined for the fEEG, fEMG, fEOG, BVP, SC, and IBI signals, respectively, which are shown in Table 2. The features of the forehead biosignals were calculated for each of the three channels. Then the best features of the signals were specified using the SFFS feature selection method.

Features selected by establishing the described subsystems were applied to identify the emotional states. The SVM and KNN classification algorithms were evaluated to identify the emotions.

To evaluate the proposed dynamically weighted classifier fusion-based emotional system (DWCFES), four other emotional systems were also designed to provide a basis for comparison. These systems include the feature fusion-based emotional system (FFES), the classifier fusion-based emotional system (CFES) with the majority voting method, a statically weighted classifier fusion-based emotional system (SWCFES), and a system that applies only the α and β parameters without the dynamic component in the proposed fusion scheme (DWCFES-Gamma). To design the FFES, selected features of the signals were merged into a feature set and applied for a single classifier. In the fusion of classification units using the majority voting method, an odd number of subsystems must be used to obtain acceptable results. To avoid complexity, in this study three subsystems were considered in the structure of the emotion recognition system. For the SWCFES scheme, another adaptive weighting method that statically determines the weight of each classification unit was considered. Static weighting schemes apply only the previous performance of classification units in such a way that units with better results during the training phase contribute more to the testing condition to produce the final result of the system. In the fusion method utilized, each classification unit is assigned a weight that is inversely proportional to its error produced, given the training conditions, meaning that the units with a smaller error in the identification of the desired emotion will have a greater weight. With nonnegative and normalization constraints, ω
                        
                           i
                        
                        ≥0 and 
                           
                              
                                 ∑
                                 
                                    i
                                    =
                                    1
                                 
                                 N
                              
                              
                                 
                                    ω
                                    i
                                 
                                 =
                                 1
                              
                           
                        , the weight of ith classification unit is defined as:
                           
                              (10)
                              
                                 
                                    
                                       ω
                                       i
                                    
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   e
                                                   i
                                                
                                             
                                             
                                                −
                                                1
                                             
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             N
                                          
                                          
                                             
                                                
                                                   
                                                      e
                                                      i
                                                   
                                                
                                                
                                                   −
                                                   1
                                                
                                             
                                          
                                       
                                    
                                    ,
                                     
                                    i
                                    =
                                    1,2,...
                                    ,
                                    N
                                 
                              
                           
                        where N is the number of utilized classification units and e
                        
                           i
                         is the error of the ith classification unit estimated in the training conditions. ω
                        
                           i
                         in Eq. (10) is an optimal weight that minimizes the squared error of the ensemble system according to the proof presented in [61]. In the DWCFES, SWCFES, and DWCFES-Gamma schemes, the subsystem results were fused using the weighted linear combination method and the system final results were obtained.

We considered the emotion recognition system user independent; thus, the features of all the subjects were merged and applied to design the systems. We used the leave on out (LOO) method for cross validation because of the small sample sizes and the high-dimensional feature set. In each case, the data point to be classified was excluded from the feature set, and the remaining data were used to train the classifiers [14].

@&#EXPERIMENTAL RESULTS@&#

To evaluate the performance of the proposed fusion method in detecting the desired emotional states, four other schemes described in the previous section were considered as well. Fig. 9 shows the F1-scores of the systems in identifying each emotion. The F1-score is a statistical measure for determining the performance of a pattern recognition problem. It can be interpreted as a weighted average of the sensitivity and precision measures as follows [62]:
                        
                           (11)
                           
                              
                                 F
                                 1
                                 −
                                 score
                                 =
                                 2
                                 ×
                                 
                                    
                                       Sensitivity
                                       ⋅
                                       Precision
                                    
                                    
                                       Sensitivity
                                       +
                                       Precision
                                    
                                 
                              
                           
                        
                     
                  

Where sensitivity or true positive rate (TPR) is the proportion of positive cases that were correctly identified (=TP/(TP+FN)); precision or positive predictive value (PPV) gives the rate for correct predictions of positive cases (=TP/(TP+FP)) [62]; and TP, FN, FP, and TN are the number of true positive, false negative, false positive, and true negative assessments, respectively.

As shown, the DWCFES performed better in terms of F1-scores compared to other structures. In this scheme, both classification methods had better performance in recognizing the desired emotional state. For example, sadness was not identified appropriately by the FFES, CFES and SWFES structures; with the DWCFES, the F1-score of this emotion improved. Also the proposed fusion method performed more successfully in identifying the other emotional states. Comparing the DWCFES with a system that was designed without the dynamic component γ (see Fig. 9
                     d and e), shows that the dynamic weighting approach for fusing the classification units improved the F1-score of the system. As shown in Fig. 9, the SWCFES as a result of using an adaptive (though static) weighting approach provided better results compared to the FFES and CFES schemes. However, the proposed dynamic fusion method outperformed the SWCFES.

To perform a more detailed comparison of the designed emotional systems, the recognition accuracy (number of correct assessments/number of all assessments=(TP+TN)/(TP+TN+FP+FN)) and the response latency are presented in Fig. 10. The systems were evaluated based on their runtime by MATLAB software for the purpose of response time comparisons.

Because latency time depends on the hardware configuration and CPU speed, we present the system's response time normalized by the maximum value obtained.

As shown in Fig. 10
                     b, the dynamic fusion scheme obtained acceptable latency compared to the other structures. Although the response time of the SWCFES and DWFES increased slightly compared to the CFES, the adaptive fusion of the classifiers improved the recognition accuracy instead. As shown in Fig. 10a, the DWCFES achieved the best recognition accuracy, which was 9% and 3% higher compared to the FFES using the SVM and KNN classifiers, respectively. In the FFES scheme, fusion of all features increased the dimension of the feature set and consequently the system's response time. However, due to the information from multiple signals in the feature set, this system is more accurate than the CFES structure. The results of the SWCFES and DWCFES-Gamma show that the overall recognition accuracies and response times achieved by the systems in identifying the desired emotions are close. However, according to Fig. 9, when the SWCFES and DWCFES-Gamma were compared each performed better for some of the emotions. In the DWCFES-Gamma, since the alpha and beta components are determined on the basis of the past performance of the classification units and the information content of the signals, the results were acceptable. In addition, the response latency times indicate that the SVM method generally behaves slower than the KNN. The computational complexity of the SVM classifier, considering both training and testing conditions, due to the need to solve a nonlinear optimization problem is higher than the KNN classification method. This makes the SVM classifier slower despite the significant results. The results also showed that compared to the CFES, the adaptive fusion method does not make the SWCFES and DWCFES too slow.

As previously stated, in the design of emotion recognition systems based on the fusion of the classifiers, three subsystems were considered. The subsystems were designed using the fEEG, fEMG+fEOG and BVP+SC+IBI features. With the SVM, overall recognition accuracies of 60%, 54.5% and 62.3%, respectively, were obtained by the respective subsystems. Using the KNN classifier, the desired emotions were recognized by the subsystems with the overall recognition accuracies of 62.7%, 64.7% and 59.3%, respectively. The results obtained by the DWCFES show that the performance of the proposed method is promising; compared to the best subsystems, the percentage improvement in recognition accuracy was 22% and 15% with the SVM and KNN classifiers, respectively.

To further evaluate the performance of the proposed method, confusion matrixes of the DWFCES (along with the best systems in other structures) are presented in Table 3. A confusion matrix shows that how the system has recognized the desired emotions and whether in identifying some states has been confused or not.

The DWCFES scheme that uses a SVM classifier to identify the intended emotions except fear and surprise had an acceptable performance. In some cases, the system wrongly identified these two emotions instead of each other. However, as shown in Figure 9, the SVM classifier with the other structures did not provide favorable performance in identifying these two emotions, either. In Table 3
                     , the specificity, sensitivity and F1-score measures have been presented as well. Sensitivity and specificity (=TN/(TN+FP)) measures specify the ability of a system to identify the desired emotions and reject undesired ones. According to the measures, the improved performance of the DWCFES scheme is confirmed. Although the results of the FFES are acceptable, its high response time makes it unsuitable for real-time applications.

We calculated the specificity and sensitivity measures for each class and then considered the average value as the specificity and sensitivity of the system. The overall F1-scores in Table 3 were determined based on the average value of the sensitivity and precision measures. The specificity, sensitivity, and F1-score values of the selected emotional systems demonstrate the remarkable performance of the DWCFES in identifying the emotions of interest and rejecting the non-interest ones.

To investigate the possibility of simplifying the emotional system, we designed a system that used only forehead or physiological signals. Using fewer signals facilitates the recording process by decreasing the number of electrodes and sensors. In addition, less inconvenience is created for users. By reducing the size of the input data, the complexity of the system and its response time are reduced. We examined three subsystems that used any of the signals that independently identify the desired emotion scenarios. The results obtained for the units were then fused with the proposed method. The results are presented in Table 4.

In each subsystem, the best features of the signals were determined as before with the SFFS feature selection method. The KNN and SVM classification techniques were applied to identify the emotional states. The results showed the ability of the proposed method in the design of a reliable emotion recognition system, without requiring additional emotion measures. In the event each subsystem fails in identifying the emotion, the final result shown in Table 4
                      is notable. The proposed strategy improved the recognition accuracy at least 4% (third row) for the best single modality. By using the proposed fusion method and applying more favorable features and classification methods, we can improve the accuracy of the emotion recognition system.

@&#DISCUSSION@&#

Affective computing is trying to give computer systems the ability to understand and interpret humans’ various emotional states. With this skill, HCI will be more similar to human–human interactions (HHIs) and thus more beneficial for human users. Until now, various emotion recognition systems have been presented [11–14,17,20,22,23,28–32]. These systems face several challenges including eliciting the desired emotions similar to real-world emotional situations and collecting trustworthy emotional signals. These problems will be more apparent when we consider that people in different emotional situations may represent dissimilar behavior reactions. In the most studies, to produce sufficient information to accurately identify the desired emotions, multiple emotional measures have been applied [11–14,17,20,22,23,28–32]. In these systems, an important issue is how to exploit or optimally combine the signals information. For this reason, along with all the considerations, the structure of the emotion recognition system is of special importance.

In this study, a new dynamic fusion method for combining the results of emotional signals was proposed. Fusion of the recognition units was performed using a weighted linear combination method. In this strategy, the weight of each unit is determined according to its functionality and performance in the training and testing phases. In most weighted fusion methods, the weights of different classifiers are usually determined based on their previous accuracy [11,32]. This requires a significant amount of training and testing data. In addition, the calculated weights provide acceptable results only for the applied data. Static weights in varying contexts such as emotional states will not produce acceptable results. Thus, calculating the weights on the fly based on the classifiers’ current performance enables the real-time system to determine their impact on the results [38].

In our system which was designed in user-independent mode, dynamic fusion of the classifiers significantly improved performance compared to the conventional fusion of the features and classifiers using the majority voting method, and compared to the static fusions of the classification units. The dynamic emotional system can adapt to each new user. The results confirm the method's desirable performance. Compared with previous emotion recognition systems that applied simple fusion of the features and classification units [11–23,28–32], the results in this work have improved. Using the SVM and KNN classification methods, the six emotional states were identified with recognition accuracy of 84.7% and 80%, respectively. By using SVM, all emotions except fear and surprise were identified correctly. As shown in Table 3, the system identified fear incorrectly as surprise and vice versa in a number of cases.

The method applied to a smaller number of modalities also had favorable results. Table 4 presents the results for when only forehead biosignals or peripheral physiological measures were used. As shown, in all cases the results improved considerably. The system that used forehead signals achieved better performance. Thus, using the SVM and KNN classifiers, the recognition accuracy of 82.7% and 74.7% was obtained, respectively. This suggests that a system with the proposed method can be implemented without multiple emotional measures.

The features extracted from the signals as well as the feature selection and classification techniques applied in this work are those that in previous emotion-related studies had the most use and best performance. Since the SFFS feature selection method works based on accuracy, the selected features will produce the minimum possible error. The SVM classifier performed better than the KNN in our investigation. The best KNN performance was obtained with k
                     =7.

The results for the proposed method compared with previous studies are considerable when we consider the number of target emotions and the number of subjects [11,13,25,26]. In most studies, emotions were recognized based on the arousal and valence scales [16,21,23,32]. Such systems have many problems detecting different emotional states with the same level of arousal and valence. In addition, since the system in this study is independent of the user, the results are notable. Due to the differences in the methods utilized in the stimuli of emotional states, feature extraction, feature selection, and classification algorithms as well as the number of desired emotions, comparing previous emotion-related studies and the results in this work is not possible. However, in contrast to user-independent emotion recognition systems [11,13,23,25,26,32,45], the best results for six emotional states were obtained in this study. In addition to recognition accuracy, response rate is an important requirement. The proposed scheme in terms of the response latency was acceptable.

Several limitations of the present study should be addressed in future research to improve the performance of the emotion recognition system. One limitation is the small sample sizes used for each emotion. To more generalize the performance of the system to real-world conditions, using more samples with different subjects is important. In this work, almost all the subjects (except those who gave a score lower than 4) participated only once for each emotion in the experiment; therefore, the emotional system might not be generalizable to additional sessions [14]. The issue of day differences in emotion recognition systems should be considered because variations in the recording equipment (e.g., sensor placement, sensors impedance), emotion dependent factors (psychological and baseline mood), and none emotional issues (e.g., sugar, sleep, hormones) may lead to large changes in physiological signals and therefore undermine the recognition accuracy of the system [14]. We also suggest that using emotion-relevant features of physiological signals can significantly improve system performance. The concerns could be addressed in future studies to provide the best possible performance of an emotion recognition system using the proposed dynamic fusion strategy.

@&#CONCLUSION@&#

In this study, we presented an adaptive fusion method for a reliable multimodal emotion recognition system. The proposed method is based on the weighted linear combination of the classifiers’ results. Dynamic weights assigned to the classification units determine their impact on the system's output. The results represent the capability of the emotional system using the proposed fusion scheme in identifying the desired emotional states. In terms of recognition accuracy and response latency, the performance of our proposed method was effective compared to the conventional fusion of features and classifiers that used the majority voting method. In addition, the proposed dynamic fusion method outperformed two static weighting schemes; the overall performance was improved in the dynamic fusion method, because the weight of each classification unit is determined based on both the current performance and the results previously acquired in the training conditions. This finding was confirmed when we compared the performance of the proposed method with the weights that were statically determined without the dynamic component. We also implemented an emotion recognition system with a smaller number of emotional modalities. Applying the proposed fusion method only to forehead or physiological signals indicates that the system can be designed without additional emotional modalities. In this way, an emotion recognition system with minimum complexity and less obtrusive to users is desirable for real-time human–computer interaction applications.

@&#ACKNOWLEDGMENT@&#

We thank all participants in the experiment for their patience and honest cooperation.

@&#REFERENCES@&#

