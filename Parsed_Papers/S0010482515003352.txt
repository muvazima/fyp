@&#MAIN-TITLE@&#Manifold ranking based scoring system with its application to cardiac arrest prediction: A retrospective study in emergency department patients

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We develop a novel manifold ranking based risk scoring system.


                        
                        
                           
                           We apply the novel scoring system for the prediction of cardiac arrest in emergency department patients.


                        
                        
                           
                           The risk scoring system is designed to handle both balanced and imbalanced datasets.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Cardiac arrest

Machine learning

Scoring system

Manifold ranking

Emergency medicine

@&#ABSTRACT@&#


               
               
                  Background
                  The recently developed geometric distance scoring system has shown the effectiveness of scoring systems in predicting cardiac arrest within 72h and the potential to predict other clinical outcomes. However, the geometric distance scoring system predicts scores based on only local structure embedded by the data, thus leaving much room for improvement in terms of prediction accuracy.
               
               
                  Methods
                  We developed a novel scoring system for predicting cardiac arrest within 72h. The scoring system was developed based on a semi-supervised learning algorithm, manifold ranking, which explores both the local and global consistency of the data. System evaluation was conducted on emergency department patients׳ data, including both vital signs and heart rate variability (HRV) parameters. Comparison of the proposed scoring system with previous work was given in terms of sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV).
               
               
                  Results
                  Out of 1025 patients, 52 (5.1%) met the primary outcome. Experimental results show that the proposed scoring system was able to achieve higher area under the curve (AUC) on both the balanced dataset (0.907 vs. 0.824) and the imbalanced dataset (0.774 vs. 0.734) compared to the geometric distance scoring system.
               
               
                  Conclusions
                  The proposed scoring system improved the prediction accuracy by utilizing the global consistency of the training data. We foresee the potential of extending this scoring system, as well as manifold ranking algorithm, to other medical decision making problems. Furthermore, we will investigate the parameter selection process and other techniques to improve performance on the imbalanced dataset.
               
            

@&#BACKGROUND@&#

Prediction of clinical outcomes of patients in intensive care units (ICUs) is of high importance. For example, identifying patients who are at low risk of developing adverse outcomes without admission to ICU can help ration precious and expensive critical care resources. Recently, prediction of cardiac arrest within 72h has drawn researchers׳ attention because it allows clinicians to make timely and safe clinical decisions based on evidence-based prognostication to ensure the correct patients receive the appropriate monitoring and treatment.

Traditionally, outcome prediction is made by intensive care clinicians based on explicit judgment. However in the past decades, several scoring systems have been developed and widely used in ICUs [1], for instance, the Acute Physiology and Chronic Health Evaluation (APACHE) [2], Simplified Acute Physiology Score (SAPS) [3], and Mortality Probability Model (MPM) [4]. Each of these scoring systems typically predicts one specific kind of clinical outcome. For example, both APACHE and SAPS predict mortality. These traditional scoring systems lack specificity in the prediction of cardiac arrest. Therefore there is a need to develop a new and efficient scoring system specific for cardiac arrest prediction.

Another challenge for cardiac arrest prediction is the selection of patients׳ feature variables, which highly impacts the accuracy of prediction. Much research has been done to identify efficient indicators for prediction. For instance, Krizmaric et al. [5] have identified and correlated several variables (arrival time, initial EtCO2, final EtCO2, etc.) with out-of-hospital cardiac arrest, while patients’ vital signs such as body temperature and blood pressure are correlated with in-hospital cardiac arrest. Churpek et al. [6] have successfully derived a model using vital signs to predict in-hospital cardiac arrest. However, researches have recently shown that not all vital signs are useful in the prediction of clinical outcomes [7]. Liu et al. [8] identified heart rate variability (HRV) as a new effective variable for prediction of mortality when used in combination with some other vital signs. It is not known what the best combination of feature variables for accurate cardiac arrest prediction is.

One study aiming to identify efficient predictors for cardiac arrest and to develop a scoring system for prediction is the geometric distance scoring system by Liu et al. [9]. The results suggested that the combination of HRV parameters and vital signs is a strong and efficient model for cardiac arrest prediction. This scoring system computes a risk score for a point (in feature space representing a patient) based on both its geometric distances from the point neighbors and the predicted binary outcome generated by support vector machine (SVM), which is a generalized classifier with learning algorithm widely used for data mining. With the advancement of computation technology, it is now possible to use machine learning techniques to solve various bio-medical problems, for example, improving the accuracy and functionality of prediction models [10], identifying relevant factors [5] and discovering new ways to utilize medical signals [11]. Nonetheless, each machine learning technique has its own drawbacks and limitations; the performance of the geometric distance scoring system still has room for improvement.

In this study, we aim to develop a novel manifold ranking based scoring system, and test it on an application of cardiac arrest prediction. The geometric distance scoring system makes use of only local structure embedded by the data, whereas the proposed scoring system explores both the local and global consistency of the data and thus has potential to further improve the prediction accuracy. Manifold ranking, a semi-supervised graph-based learning algorithm used for information retrieval, plays the key role in the development of the proposed scoring system. This study shows that manifold ranking has potential to be used in other bio-medical applications.

@&#METHODS@&#

The dataset used in this paper was original collected by the Department of Emergency Medicine, Singapore General Hospital, Singapore for an observational clinical study between November 2006 and December 2007. Ethics approval from the Institutional Review Board (IRB) with a waiver of patient consent for the study was obtained.

All patients were initially triaged by a nurse, and those with Airway, Breathing, Circulation problems, or thought to be possibly unstable and needing close monitoring are routinely put on ECG monitoring using the LIFEPAK 12 defibrillator/monitor (Physio-Control, Redmond, WA). Lead II ECGs sampled at 125Hz were extracted as text files for HRV analysis using CODE-STAT Suite data review software (version 5.0, Physio-Control) and proprietary ECG extraction software. Charts were included for review if they had an ECG recording showing sinus rhythm and were excluded if they were in nonsinus rhythm (ventricular or supraventricular arrhythmias). A list of HRV parameters used in this study is shown in 
                        Table 1.

Demographic data and vital signs (systolic blood pressure, diastolic blood pressure, respiratory rate, heart rate, Glasgow coma scale, temperature, pain score, and oxygen saturation) were obtained from hospital records; clinical outcomes, e.g., cardiac arrest within 72h, were recorded for patients of interests. The 72h outcome was heuristically defined as it is commonly used in the emergency department as a cutoff time point. The core idea of the proposed algorithm is to compute a point׳s risk score based on its position both with respect to its neighbors and in the global manifold. For an unbiased score prediction process, a balanced training dataset, which consists of equal number of samples with positive and negative outcome, is required.

However, most of the medical data is highly imbalanced. For example, the collected dataset used in this study consists of a majority group of 973 negative samples (94.9% patients without cardiac arrest within 72) and a minority group of 52 positive samples (5.1% patients with cardiac arrest within 72h), giving an imbalance ratio of about 19. In this study, experiment on balanced data was first conducted followed by experiment on imbalanced data. The balanced dataset is made up of all 52 positive cases and another 52 randomly selected negative cases.

To derive a balanced training dataset out of an imbalanced one, the ensemble-based system [12,13] is adopted, where two commonly used methods are under-sampling and over-sampling. In this paper we implemented the under-sampling strategy. In brief, ensemble-based system is used to partition the majority training class (negative samples in our study) into 
                           
                              M
                           
                         non-overlapping groups randomly. Each group has almost the same number of samples as the minority class. By combining the 
                           
                              M
                           
                         majority class with the minority class, 
                           
                              M
                           
                         balanced training ensembles are created. Risk score prediction of every testing data is done for 
                           
                              M
                           
                         times based on the 
                           
                              M
                           
                         balanced training ensembles. 
                           
                              M
                           
                         risk scores are generated and the mean is assigned to the testing sample as the final risk score. Note that 
                           
                              M
                           
                         is the integer nearest to the imbalance ratio, which is defined as the ratio of the number of majority class samples to that of the minority class samples.

In our experiment, 
                           
                              M
                           
                         is chosen as 19, approximately equal to 973/52. Therefore 19 ensembles were constructed. All the positive samples are included in all the ensembles, but 973 negative samples are partitioned in 19 parts with approximately 52 samples in each ensemble. The imbalanced data is a superset of balanced data. Hence, we used the same set of parameters as calibrated during balanced data experiment.

Manifold ranking [14,15] has been found to have excellent performance especially in the information retrieval applications, such as image [16,17] and media retrievals [18]. However, this algorithm has only been applied in very few biomedical problems [19] In this section, we briefly review the manifold ranking algorithm before we proceed to extend it to a score prediction algorithm in the next section.

In information retrieval, we are given a database and some queries with the aim to investigate the relevance of unlabeled samples with respect to the query data. Given a set of 
                           
                              n
                           
                         data samples 
                           X
                           =
                           
                              
                                 
                                    x
                                    1
                                 
                                 ,
                                 …
                                 ,
                                 
                                    x
                                    l
                                 
                                 ,
                                 
                                    x
                                    
                                       l
                                       +
                                       1
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    x
                                    n
                                 
                              
                           
                           ∈
                           
                              R
                              
                                 m
                                 ×
                                 n
                              
                           
                        , the first 
                           l
                         points are the queries and the rest are unlabeled points. We define an initial score vector 
                           
                              y
                           
                           
                              =
                           
                           
                              
                                 
                                    [
                                 
                                 
                                    
                                       y
                                    
                                    
                                       1
                                    
                                 
                                 
                                    ,
                                 
                                 
                                    
                                       y
                                    
                                    
                                       2
                                    
                                 
                                 
                                    ,
                                    …
                                    ,
                                 
                                 
                                    
                                       y
                                    
                                    
                                       n
                                    
                                 
                                 
                                    ]
                                 
                              
                              
                                 T
                              
                           
                        , in which 
                           
                              
                                 
                                    y
                                 
                                 
                                    i
                                 
                              
                              
                                 =
                                 1
                              
                           
                         if 
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                         is a query and 
                           
                              
                                 
                                    y
                                 
                                 
                                    i
                                 
                              
                              
                                 =
                                 0
                              
                           
                         otherwise. We build a graph (e.g., 
                           
                              k
                           
                        -nearest neighbors graph) on all data samples 
                           
                              X
                           
                        . Let 
                           W
                           =
                           
                              
                                 
                                    w
                                    ij
                                 
                              
                              
                                 i
                                 ,
                                 j
                                 =
                                 1
                              
                              n
                           
                           ∈
                           
                              R
                              
                                 n
                                 ×
                                 n
                              
                           
                         denotes an affinity matrix, where 
                           
                              w
                              ij
                           
                         denotes an weight for an edge between points 
                           
                              i
                           
                         and 
                           
                              j
                           
                        . One commonly used method to define the weight is the heat kernel 
                           
                              w
                              ij
                           
                           =
                           exp
                           
                              
                                 −
                                 
                                    
                                       
                                          d
                                          2
                                       
                                       
                                          
                                             
                                                x
                                                i
                                             
                                             −
                                             
                                                x
                                                j
                                             
                                          
                                       
                                    
                                    
                                       2
                                       
                                          σ
                                          2
                                       
                                    
                                 
                              
                           
                        , if there is an edge linking 
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                         and 
                           
                              
                                 x
                              
                              
                                 j
                              
                           
                        . Otherwise 
                           
                              w
                              ij
                           
                           =
                           0
                        . 
                           
                              d
                              (
                           
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                           
                              −
                           
                           
                              
                                 x
                              
                              
                                 j
                              
                           
                           
                              )
                           
                         indicates the distance between 
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                         and 
                           
                              
                                 x
                              
                              
                                 j
                              
                           
                         defined on 
                           
                              X
                           
                        , such as Euclidean distance. Let 
                           f
                           :
                           x
                           →
                           
                              R
                              n
                           
                         be a ranking function which assigns a ranking scoring 
                           
                              f
                              i
                           
                         to each point 
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                        , and 
                           f
                           =
                           
                              
                                 
                                    
                                       
                                          f
                                          1
                                       
                                       ,
                                       
                                          f
                                          2
                                       
                                       ,
                                       …
                                       ,
                                       
                                          f
                                          n
                                       
                                    
                                 
                              
                              T
                           
                         can be viewed as a vector.

The ranking function 
                           f
                         can be found by minimizing the following cost function Q:
                           
                              (1)
                              
                                 Q
                                 
                                    f
                                 
                                 =
                                 m
                                 i
                                 n
                                 
                                 
                                    
                                       a
                                       r
                                       g
                                    
                                    f
                                 
                                 
                                 
                                    1
                                    2
                                 
                                 (
                                 
                                    
                                       ∑
                                       
                                          i
                                          ,
                                          j
                                          =
                                          1
                                       
                                       n
                                    
                                    
                                       w
                                       ij
                                    
                                    
                                       
                                          ∥
                                          
                                             
                                                
                                                   
                                                      
                                                         f
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         d
                                                         ii
                                                      
                                                   
                                                
                                             
                                             −
                                             
                                                
                                                   
                                                      f
                                                      j
                                                   
                                                
                                                
                                                   
                                                      
                                                         d
                                                         jj
                                                      
                                                   
                                                
                                             
                                          
                                          ∥
                                       
                                       
                                          2
                                       
                                    
                                    +
                                    μ
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       n
                                    
                                    
                                       
                                          ∥
                                          
                                             f
                                             i
                                          
                                          −
                                          
                                             y
                                             i
                                          
                                          ∥
                                       
                                       2
                                    
                                 
                                 )
                                 ,
                              
                           
                        where 
                           
                              μ
                              >
                              0
                           
                         is a regularization parameter and 
                           D
                           =
                           d
                           i
                           a
                           g
                           
                              
                                 d
                                 
                                    i
                                    i
                                 
                              
                           
                         is a diagonal matrix with 
                           
                              d
                              
                                 i
                                 i
                              
                           
                           =
                           
                              ∑
                              
                                 j
                                 =
                                 1
                              
                              n
                           
                           
                              w
                              ij
                           
                        .

The first term in the cost function is a smoothness constraint, which ensures that the classifying function assigns close ranking scores to nearby points in the space. The second term is a fitting constraint which ensures that the final ranking scores do not differ too much from the initial scores. Minimizing the cost function 
                           Q
                           
                              f
                           
                        , we get the optimal 
                           
                              f
                              ⁎
                           
                         in the following closed form:
                           
                              (2)
                              
                                 
                                    f
                                    ⁎
                                 
                                 =
                                 
                                    
                                       
                                          I
                                          −
                                          α
                                          S
                                       
                                    
                                    
                                       −
                                       1
                                    
                                 
                                 y
                              
                           
                        where 
                           
                              α
                              =
                              1
                              /
                              (
                              1
                              +
                              μ
                              )
                           
                        , 
                           
                              I
                           
                         is an identity matrix of size 
                           
                              n
                              ×
                              n
                           
                        , and 
                           
                              S
                           
                           
                              =
                           
                           
                              
                                 D
                              
                              
                                 −
                                 1
                                 /
                                 2
                              
                           
                           
                              W
                           
                           
                              
                                 D
                              
                              
                                 −
                                 1
                                 /
                                 2
                              
                           
                         is the symmetrical normalization of 
                           
                              W
                           
                        .

Alternatively, we can obtain the ranking function 
                           f
                         in the iteration form 
                           f
                           
                              
                                 t
                                 +
                                 1
                              
                           
                           =
                           α
                           S
                           f
                           
                              t
                           
                           +
                           
                              
                                 1
                                 −
                                 α
                              
                           
                           y
                        . In each iteration, each point receives information from its neighbors as shown in the first term and retains its initial information as shown in the second term. The iteration process is repeated until convergence. The solution at convergence is the same as Eq. (2) and the proof can be found in [15].

In information retrieval, a user first specifies a query and assigns initial score of 1 to the query and 0 to the rest. The closed form or iteration form can be used to calculate the final ranking scores for unlabeled samples, which reveal their semantic relevance to the query. Nonetheless, manifold ranking is not limited to the applications relating to ranking, searching and information retrieval. With the flexibility of initial score assignment scheme and final scores interpretation, we see a huge potential of manifold ranking in other areas, e.g., constructing scoring system.

We proposed a score prediction algorithm based on manifold ranking with some modifications. Firstly, the number of unlabeled testing samples is set at 1 in MRSPA, in contrast to a much larger number in the original manifold ranking algorithm. In real application, we can always predict one patient at each time and run the algorithm several times when more patients need to be classified. This modification makes the manifold ranking fit for the supervised learning nature of cardiac arrest prediction problem.

Secondly, initial score assignment scheme is adjusted as the following. In ranking problem, there is only one type of labeled samples, i.e., the query. Therefore 1 and 0 as initial scores are sufficient to distinguish the labeled and unlabeled samples. In contrast, the labeled samples in prediction problem consist of patients with both positive and negative outcomes. Therefore, the patient to be predicted is assigned a neutral initial score, i.e., zero. Training samples with cardiac arrest are assigned a positive initial score. Healthy training samples are assigned a negative initial score with the same magnitude.

Moreover, other methods to construct the affinity matrix are reviewed and investigated. Two distance measures can be used, i.e., cosine similarity and Euclidean distance. Three weighting methods can be used: Heat kernel weighting is used only with Euclidean distance. Cosine similarity weighting is used only with cosine similarity distance. Binary weighting can be used with both distance measures. Details of the combinations can be found in literature [20,21].

The proposed MRSPA is briefly explained. Each data is a point in the sample space and together they form the point set 
                           
                              X
                           
                           
                              =
                           
                           
                              [
                              
                                 
                                    
                                       x
                                    
                                    
                                       1
                                    
                                 
                                 
                                    ,
                                 
                                 
                                    
                                       x
                                    
                                    
                                       2
                                    
                                 
                                 
                                    ,
                                    …
                                    ,
                                 
                                 
                                    
                                       x
                                    
                                    
                                       n
                                    
                                 
                              
                              ]
                           
                           
                              ∈
                           
                           
                              
                                 R
                              
                              
                                 m
                                 ×
                                 n
                              
                           
                        . The first 
                           
                              n
                              −
                              1
                           
                         points are labeled sample and the last is the testing sample whose risk score is to be predicted. Assign initial scores to all samples and form an initial score vector 
                           
                              y
                           
                        : positive labeled samples are assigned a score of 100, negative labeled sample −100 and new testing sample 0.
                           
                              •
                              Construct the affinity matrix 
                                    W
                                    =
                                    
                                       
                                          
                                             w
                                             
                                                i
                                                j
                                             
                                          
                                       
                                       
                                          i
                                          ,
                                          j
                                          =
                                          1
                                       
                                       n
                                    
                                 : The cosine distance between any two pints can be calculated by using 
                                    
                                       cos
                                    
                                    
                                       
                                          (
                                          
                                             
                                                
                                                   x
                                                
                                                
                                                   i
                                                
                                             
                                             
                                                ,
                                             
                                             
                                                
                                                   x
                                                
                                                
                                                   j
                                                
                                             
                                          
                                          )
                                       
                                       
                                          =
                                          (
                                       
                                       
                                          
                                             x
                                          
                                          
                                             i
                                          
                                       
                                       
                                          ⋅
                                       
                                       
                                          
                                             x
                                          
                                          
                                             j
                                          
                                       
                                       
                                          )
                                          /
                                          (
                                       
                                       
                                          |
                                          
                                             
                                                
                                                   x
                                                
                                                
                                                   i
                                                
                                             
                                          
                                          |
                                       
                                       
                                          ⋅
                                       
                                       
                                          |
                                          
                                             
                                                
                                                   x
                                                
                                                
                                                   j
                                                
                                             
                                          
                                          |
                                       
                                       
                                          )
                                       
                                    
                                 . Form a 
                                    
                                       k
                                    
                                 -nearest neighbors (
                                    
                                       k
                                    
                                 -NN) graph using the cosine similarity: Nodes 
                                    
                                       i
                                    
                                  and 
                                    
                                       j
                                    
                                  are connected by an edge if 
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                  is among the 
                                    
                                       k
                                    
                                  nearest neighbors of 
                                    
                                       
                                          x
                                       
                                       
                                          j
                                       
                                    
                                  and vice versa. Add binary weighting to each edge: If there is an edge between 
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                  and 
                                    
                                       
                                          x
                                       
                                       
                                          j
                                       
                                    
                                 , the weight 
                                    
                                       w
                                       ij
                                    
                                  is 1, otherwise 0. 
                                    
                                       w
                                       ii
                                    
                                  is always 0 because there is no self-loop in the graph. The cosine distance and binary weighting can be replaced with other methods of constructing the affinity matrix, for instance, Euclidean distance with heat kernel weighting.

Symmetrically normalize 
                                    
                                       W
                                    
                                  by 
                                    
                                       S
                                    
                                    
                                       =
                                    
                                    
                                       
                                          D
                                       
                                       
                                          −
                                          1
                                          /
                                          2
                                       
                                    
                                    
                                       W
                                    
                                    
                                       
                                          D
                                       
                                       
                                          −
                                          1
                                          /
                                          2
                                       
                                    
                                 , where 
                                    D
                                    =
                                    
                                       
                                          
                                             d
                                             
                                                i
                                                i
                                             
                                          
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       n
                                    
                                  is the diagonal matrix with 
                                    
                                       d
                                       ii
                                    
                                  element equal to the sum of the 
                                    
                                       i
                                    
                                 -th row of 
                                    
                                       W
                                    
                                 . This is to make sure the weighting is spread symmetrically.

Update each point׳s score based on its 
                                    
                                       k
                                    
                                  neighbors: Iterate 
                                    f
                                    
                                       
                                          t
                                          +
                                          1
                                       
                                    
                                    =
                                    α
                                    S
                                    f
                                    
                                       t
                                    
                                    +
                                    
                                       
                                          1
                                          −
                                          α
                                       
                                    
                                    y
                                  until convergence, where 
                                    
                                       α
                                    
                                  is the parameter in (0,1) and 
                                    f
                                    
                                       t
                                    
                                  is the score vector at iteration 
                                    
                                       t
                                    
                                 . The final result 
                                    
                                       f
                                       ⁎
                                    
                                  after convergence can be calculated directly using 
                                    
                                       f
                                       ⁎
                                    
                                    =
                                    
                                       
                                          
                                             I
                                             −
                                             α
                                             S
                                          
                                       
                                       
                                          −
                                          1
                                       
                                    
                                    y
                                 .

The first 
                           
                              n
                              −
                              1
                           
                         elements in the score vector 
                           
                              f
                              ⁎
                           
                         are scores for training data, which have no future use. The last element in the score matrix 
                           
                              f
                              ⁎
                           
                         is output as the raw risk score for the testing data, which is denoted as 
                           
                              f
                              n
                              ⁎
                           
                        .

Based on the proposed MRSPA, a novel scoring system is proposed to compute a risk score reflecting a patient׳s clinical outcome. A brief summary of the proposed system is in Algorithm 1.
                           Algorithm 1
                           
                              Proposed manifold ranking-based score prediction
                           


                        Input
                        
                           
                              •
                              Processed HRV parameters and vital signs of 
                                    
                                       n
                                    
                                  training patients and one testing patient, where the training set includes the same number of healthy training samples as that of the patients with cardiac arrest.

Patients׳ hospital records and characteristics.


                        Step 1. Parameter selection
                        
                           
                              •
                              Run MRSPA on training samples under leave-one-out cross-validation (LOOCV) framework. Obtain risk score for every training samples. Compute the AUC value.

Repeat the last step under different parameter setting. Find out the combination of parameters that give the highest AUC value.


                        Step 2. Normalization parameter generation
                        
                           
                              •
                              Run MRSPA on training samples under the optimal parameter setting obtained from Step 1 under LOOCV framework. Obtain risk score for every training sample.

Record the maximum and minimum risk score of the training samples, i.e. 
                                    
                                       ma
                                    
                                    
                                       
                                          x
                                       
                                       
                                          D
                                       
                                    
                                  and 
                                    
                                       mi
                                    
                                    
                                       
                                          n
                                       
                                       
                                          D
                                       
                                    
                                 .


                        Step 3. Score calculation
                        
                           
                              •
                              Run MRSPA on both training and testing samples under the optimal parameter setting obtained from Step 1. Obtain the raw risk score 
                                    
                                       f
                                       n
                                       ⁎
                                    
                                  for the testing data.


                                 Step 4. Score normalization
                              

Perform min-max normalization on the risk score of testing data with the 
                                    
                                       ma
                                    
                                    
                                       
                                          x
                                       
                                       
                                          D
                                       
                                    
                                  and 
                                    
                                       mi
                                    
                                    
                                       
                                          n
                                       
                                       
                                          D
                                       
                                    
                                  to obtain the normalized risk score 
                                    
                                       
                                          f
                                          ˜
                                       
                                       n
                                       ⁎
                                    
                                 .


                                 Output
                              

Predictive risk score on the clinical outcome ranging from 0 to 100.

We select the optimal 
                              
                                 k
                              
                            and 
                              
                                 α
                              
                            as follows. First identify a set of 
                              
                                 k
                              
                            and 
                              
                                 α
                              
                            values that may give satisfactory results by simple trial and error. Under each combination of 
                              
                                 k
                              
                            and 
                              
                                 α
                              
                           , run MRSPA to generate a raw risk score for every sample in the training dataset using leave-one-out cross-validation (LOOCV) framework where all training samples other than the testing sample are used for risk prediction. Compute and record Area Under the Curve (AUC) of Receiver Operating Characteristics (ROC) curve generated by the training samples using each set of 
                              
                                 k
                              
                            and 
                              
                                 α
                              
                           . The combination that gives the highest AUC is selected as the optimal parameters and is used for the following steps.

Min-max normalization [22] is used in the proposed scoring system.
                              
                                 (3)
                                 
                                    
                                       
                                          
                                             v
                                          
                                          
                                             new
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                v
                                             
                                             
                                                −
                                             
                                             
                                                
                                                   min
                                                
                                                
                                                   D
                                                
                                             
                                          
                                          
                                             
                                                
                                                   max
                                                
                                                
                                                   D
                                                
                                             
                                             
                                                −
                                             
                                             
                                                
                                                   min
                                                
                                                
                                                   D
                                                
                                             
                                          
                                       
                                       
                                          ×
                                          100
                                       
                                    
                                 
                              
                           where 
                              
                                 
                                    v
                                 
                                 
                                    new
                                 
                              
                            is the risk score of a testing sample after normalization and 
                              
                                 v
                              
                            is the raw risk score computed by MRSPA. 
                              
                                 ma
                              
                              
                                 
                                    x
                                 
                                 
                                    D
                                 
                              
                            and 
                              
                                 mi
                              
                              
                                 
                                    n
                                 
                                 
                                    D
                                 
                              
                            are the maximum and minimum raw risk scores of all the testing samples, which are usually unknown during the prediction process. Therefore, the minimum and maximum scores of training samples are used to model those of the testing samples. This step is to compute the maximum and minimum scores of the training data for the use of normalization of testing samples. This is done under LOOCV framework: with each sample 
                              
                                 X
                              
                            in the training dataset as unlabeled sample, run MRSPA to generate a raw risk score for this sample 
                              
                                 X
                              
                            based on the rest of the training. After this process, each training sample should have a raw score. The highest score is saved as 
                              
                                 ma
                              
                              
                                 
                                    x
                                 
                                 
                                    D
                                 
                              
                            and the lowest as 
                              
                                 mi
                              
                              
                                 
                                    n
                                 
                                 
                                    D
                                 
                              
                           .

This is the core step in the scoring system, where raw risk score for the testing sample is generated. Run MRSPA on the training samples and the testing sample as follows. First, construct the initial score vector 
                              
                                 y
                              
                           . Second, construct the affinity matrix 
                              
                                 W
                              
                            revealing the geometric relationship among points. Third, keep updating risk scores based on a point׳s 
                              
                                 k
                              
                            neighbors according to the affinity matrix. Compute the raw risk scores vector 
                              
                                 f
                                 ⁎
                              
                            at convergence by using Eq. (2). Finally, record the raw risk score 
                              
                                 f
                                 n
                                 ⁎
                              
                            for the testing data.

For every new testing data and its training database, a normalization process needs to be done to make its score 
                              
                                 f
                                 n
                                 ⁎
                              
                            comparable to other testing data and other training database. For each testing data with a score 
                              
                                 f
                                 n
                                 ⁎
                              
                           :
                              
                                 (4)
                                 
                                    
                                       
                                          f
                                          ~
                                       
                                       n
                                       ⁎
                                    
                                    =
                                    {
                                    
                                       
                                          
                                             0
                                             ,
                                             
                                             
                                                f
                                                n
                                                ⁎
                                             
                                             ≤
                                             
                                                min
                                                D
                                             
                                          
                                       
                                       
                                          
                                             100
                                             ,
                                             
                                             
                                                f
                                                n
                                                ⁎
                                             
                                             ≥
                                             
                                                max
                                                D
                                             
                                          
                                       
                                       
                                          
                                             100
                                             ⋅
                                             
                                                
                                                   
                                                      f
                                                      n
                                                      ⁎
                                                   
                                                   −
                                                   
                                                      min
                                                      D
                                                   
                                                
                                                
                                                   
                                                      f
                                                      n
                                                      ⁎
                                                   
                                                   −
                                                   
                                                      max
                                                      D
                                                   
                                                
                                             
                                             ,
                                             
                                             
                                                min
                                                D
                                             
                                             <
                                             
                                                f
                                                n
                                                ⁎
                                             
                                             <
                                             
                                                max
                                                D
                                             
                                          
                                       
                                    
                                 
                              
                           where 
                              
                                 ma
                              
                              
                                 
                                    x
                                 
                                 
                                    D
                                 
                              
                            and 
                              
                                 mi
                              
                              
                                 
                                    n
                                 
                                 
                                    D
                                 
                              
                            are the maximum and minimum raw scores of training samples obtained from Step 2. As a result, 
                              
                                 
                                    f
                                    ˜
                                 
                                 n
                                 ⁎
                              
                            ranges from 0 to 100 and is recorded as the final risk score for the testing sample.

@&#PERFORMANCE EVALUATION@&#

Cross-validation framework is a well-known model validation technique to assess the generalization capability of an algorithm, usually a prediction algorithm. In each round of cross validation, the sample of data is partitioned into complementary subsets. Training is done on one subset and validation is done on the other subset. LOOCV simply means there is only one sample in the validation subset. Under LOOCV framework, 
                           
                              n
                           
                         round of prediction is performed for a dataset with 
                           
                              n
                           
                         samples.

The predicted outcome for each sample is reflected as a risk score from 0 to 100. By setting a proper threshold, each data can be classified as predicted positive, i.e. cardiac arrest within 72h, or predicted negative, i.e. healthy. As a result, the scoring system can be evaluated using traditional binary classifier evaluation methods. Sensitivity, specificity, the positive predictive (PPV) and the negative predictive value (NPV) can be formulated based on the predicted labels as follows:
                           
                              
                                 
                                    Sensitivity
                                 
                                 
                                    =
                                 
                                 
                                    TP
                                 
                                 
                                    /
                                    (
                                 
                                 
                                    TP
                                 
                                 
                                    +
                                 
                                 
                                    FN
                                 
                                 
                                    )
                                 
                              
                           
                        
                        
                           
                              
                                 
                                    Specificity
                                 
                                 
                                    =
                                 
                                 
                                    TN
                                 
                                 
                                    /
                                    (
                                 
                                 
                                    TN
                                 
                                 
                                    +
                                 
                                 
                                    FP
                                 
                                 
                                    )
                                 
                              
                           
                        
                     


                        
                           
                              
                                 
                                    P
                                    P
                                    V
                                    =
                                    
                                       T
                                       P
                                       /
                                       (
                                       T
                                       P
                                       +
                                       F
                                       P
                                       )
                                    
                                 
                              
                           
                        
                        
                           
                              
                                 
                                    N
                                    P
                                    V
                                    =
                                    T
                                    N
                                    /
                                    
                                       
                                          T
                                          N
                                          +
                                          F
                                          N
                                       
                                    
                                 
                              
                           
                        where true positive (TP) is the number of patients with cardiac arrest within 72h that are correctly predicted as positive; false positive (FP) is the number of healthy patients misclassified as positive; true negative (TN) is the number of healthy patients correctly predicted as negative; and false negative (FN) is the number of patients with cardiac arrest within 72h misclassified as negative. A good scoring system is associated with high sensitivity, specificity, PPV and NPV.

ROC curve is a graphical plot which illustrates the performance of a binary classifier as its threshold varies [23]. False positive rate, which equals to 1 – Specificity, is plotted on the 
                           
                              x
                           
                         axis. True positive rate, which equals to Sensitivity, is plotted on 
                           
                              y
                           
                         axis. For an ideal binary classifier, the true positive rate is 100% at every value of false positive rate. In general, the AUC is used as a numerical measure of the effectiveness of the binary classifier. Ideally AUC equals to 1.

@&#RESULTS@&#

In this retrospective observational study, a total number of 1386 critically ill patients were monitored and their ECG tracings were collected. However, only 1025 were used for analysis in this paper as the records of 361 patients had a high percentage of artifacts, non-sinus beats and ectopics combined together. Cardiac arrest within 72h was observed from 52 cases. It is worth mentioning that the dataset used in this study is identical to that used in Liu et al.’s study [9], thus ensuring a fair comparison between these two algorithms. Patient baseline characteristics used in this study can be found in 
                     Table 2.

Many existing manifold-ranking applications construct the affinity matrix by using the Euclidean distance with heat kernel weighting. Since the nature and distribution of medical data is different from the data in those applications, it׳s worth investigating the impact of different ways of constructing the affinity matrix on prediction performance. Four ways to construct the 
                           
                              k
                           
                        -NN graph were investigated here, i.e., cosine similarity with cosine weighting, or cosine similarity with binary weighting, or Euclidean distance with binary weighting or Euclidean distance with heat kernel weighting. 
                        Fig. 1 presents the performance of different method in terms of AUC. Constructing the graph using cosine similarity gives significant larger optimal AUC values. But the result given by binary weighting and that given by cosine weighting are of little distinction. For this experiment, cosine similarity with binary weighting method was used as the default affinity matrix setting.

Besides methods of constructing the affinity matrix, two parameters have effects on the performance result, i.e., 
                           
                              k
                           
                         and 
                           
                              α
                           
                        . We continued investigating the impacts of various 
                           
                              α
                           
                         and K. 
                        
                        Figs. 2 and 3 show the AUC of the resulting ROC curve at different values of 
                           
                              α
                           
                         with 
                           
                              k
                           
                         set to 13. As depicted in Fig. 2 with 
                           
                              α
                           
                         smaller than 0.03, AUC achieves its largest value 0.907 and keeps as constant. AUC drops significantly with the increase of 
                           
                              α
                           
                         when 
                           
                              α
                           
                         is greater than 0.1, as shown in Fig. 3. Therefore, the optimal 
                           
                              α
                           
                         value is any value smaller than 0.03. Parameter 
                           
                              α
                           
                         indicates a point׳s ability to receive score updates from the network as oppose to the ability to retain its own score. A small 
                           
                              α
                           
                         means a strong ability to retain its initial score value. This is expected because the scores of training data should not change too much throughout the iterations to make sure the information received by the testing data has correct meaning as we initially assigned.


                        
                        Fig. 4 shows the AUC under different values of 
                           
                              k
                           
                        , when 
                           
                              α
                           
                         is set to the optimal value 0.01. The scoring system gets its best performance (AUC=0.907), when 
                           
                              k
                           
                         is equal to 13. The AUC value is relatively sensitive to 
                           
                              k
                           
                         value. Therefore, we suggest a validation process to be conducted for each training database in order to obtain the optimal combination of 
                           
                              k
                           
                         and 
                           
                              α
                           
                        . This validation process should be done once before real testing data come in. For this study a combination is 
                           
                              k
                              =
                              13
                           
                         and 
                           
                              α
                              =
                              0.01
                           
                         was used.


                        
                        Table 3 presents the performance of the proposed scoring system on prediction cardiac arrest with 72h with cosine-binary method, 
                           
                              k
                              =
                              13
                              ,
                           
                         and 
                           
                              α
                              =
                              0.01
                           
                        . Cutoff points indicate the threshold. Samples with scores beyond cutoff are viewed as positive or with cardiac arrest; samples with scores below cutoff is considered negative or healthy. The performance measures (e.g. sensitivity and specificity) vary with the cutoff. By varying cutoff value, the sensitivity and specificity changes in opposite way. The optimal cutoff point is the one achieving maximum sum of sensitivity and specificity. In this experiment, the optimal point is observed at 47.4. With values of all the four measures greater than 70%, it is fair to say that the performance of this prediction is satisfactory.

Furthermore, we compared the performance of the proposed scoring system with the geometric distance scoring system. As shown in 
                        Fig. 5, the ROC curve of the proposed scoring system is closer to the left and top boundary for most of the sensitivity values, except for a small section where sensitivity falls between 65% and 82%. A quantitative comparison is shown in 
                        Table 4. At their corresponding optimal cutoff points, the proposed scoring system has a better sensitivity of 82.69% and NPV of 78.85%, whereas the geometric distance scoring system has a better specificity of 80.77% and PPV of 80.39%. It is worth noting that the sum of sensitivity and specificity for the two scoring system are very close (159.61% and 159.62% respectively). In cardiac arrest prediction, a high sensitivity is more favored than a high specificity because misclassifying an unhealthy patient is more risky than misclassifying a healthy patient. Moreover, the proposed scoring system has a higher AUC of 0.907, compared to the AUC value 0.824 of the geometric distance scoring system. Overall, the proposed scoring system is more favored for the balanced dataset.

The experiment on the imbalanced dataset was conducted under the same parameter settings, i.e. constructing affinity matrix using cosine-binary method, 
                           
                              k
                              =
                              13
                           
                        , and 
                           
                              α
                              =
                              0.01
                           
                        . Ensemble learning method was used to ensure a balanced training dataset. 
                        Fig. 6 shows the comparison of ROC between the proposed scoring system and the geometric distance scoring system on the imbalanced dataset. Both of the two scoring systems showed less satisfactory performance on imbalanced dataset compared to that on balanced dataset. The PPV and NPV shown in 
                        Table 5 are extremely low and high respectively. This is because the absolute number of positive samples is much smaller than that of negative samples. PPV and NPV are calculated using both positive samples׳ and negative samples׳ predicted results, and therefore the much larger absolute number of negative samples may distort these two values.

Based on the ROC curve in Fig. 6, the proposed scoring system appears to have better performance, as almost all parts of its ROC curve are either closer to the left or top boundary. According to the quantitative results of the comparison shown in Table 3, the proposed scoring system has better performance in terms of all the four measures at the optimal cutoff. Moreover the AUC of proposed scoring system is higher by 0.04. The analysis shows the proposed scoring system is able to handle imbalanced data and the performance is slightly better than the geometric distance scoring system.

@&#DISCUSSION@&#

We developed a novel scoring system and tested it on the application of cardiac arrest prediction. The proposed scoring system has four steps, namely parameter selection, normalization parameter generalization, score calculation and score normalization. It is constructed based on MRSPA. LOOCV was used in parameter selection and normalization parameter generalization. Final risk score is normalized using min-max method.

The capability of handling imbalanced dataset is of high importance in order for a method to be adopted in medical application, where positive cases are scarce and most of the database show biased distribution. Because of the imbalanced information obtained from the learning process, the prediction result may be biased toward the majority class. Consequently, the general performance on imbalanced dataset is expected to be less satisfactory than that on balanced dataset. In this study, we have shown that this proposed scoring system can be successfully implemented on both balanced and imbalanced datasets, even though MRSPA requires the training dataset to be a balanced set. Ensemble learning techniques was used for the implementation on imbalanced dataset. Researchers have been working on other techniques to diminish the effects of imbalanced data [13]. Future work includes exploring other learning strategies on highly imbalanced data.

The effects of three parameter settings were investigated in the study, namely the methods to construct affinity matrix, 
                        
                           k
                        
                      and 
                        
                           α
                        
                     . During the score spreading process, not only did the testing data keep updating its risk score, so did the training data. Training data should keep risk score as consistent to their original label as possible, in order to ensure that the training data always passes the correct information to the testing data. This can be guaranteed only by assigning 
                        
                           α
                        
                     , with a small value because 
                        
                           α
                        
                      indicates the ability of a point to change score away from its original score. This was verified by results shown in Figs. 2 and 3. Performance is stable and achieves its optimum when 
                        
                           α
                        
                      is small. Therefore we suggest a small 
                        
                           α
                        
                      value to be used in the proposed scoring system.


                     Fig. 1 indicates that using cosine distance with binary weighting to construct affinity matrix always achieves better performance compared to the other three combinations. Fig. 4 shows that performance is relatively sensitive to 
                        
                           k
                        
                      value settings. When 
                        
                           k
                        
                      value is too small, the testing data is unable to obtain enough information to make an accurate prediction. When 
                        
                           k
                        
                      value is too large, the testing data receives redundant and wrong information from irrelevant training data. Therefore, the parameter selection process using training data is critical for an accurate prediction. Parameters need to be adjusted before new testing data comes in by using LOOCV framework.

To evaluate the performance of the scoring system, experiments were conducted on dataset consisting of 15 HRV parameters and eight vital signs. The dataset is the same as that used in Liu et al.׳s study [9] for the purpose of fair comparison with the distance scoring system. This study did not compare the proposed method with traditional classification methods, e.g. SVM with linear kernel and SVM with radial basis function (RBF) kernel, because they were proved to be inferior to the geometric distance scoring system in many aspects [9]. The comparison results shown in Tables 2 and 3 demonstrate that the proposed method generally outperforms the distance scoring system on both balanced and imbalanced datasets. Two exceptions were observed from the experimental results on balanced dataset: the distance scoring system has a better specificity and PPV on balanced dataset. It is also worth mentioning that the proposed scoring system has a better sensitivity and NPV. Sensitivity indicates the capability of a classifier to correctly classify a positive case, while the specificity is related to the capability to correctly classify a negative case. To misclassify a high-risk patient as a healthy patient can result in adverse outcomes, such as death, for the patient. With all other measures in the acceptable range, the proposed method with a higher sensitivity value is preferred.

Machine learning techniques have been used in many methods in cardiac arrest prediction, for example, the recently developed distance scoring system [9] and ML-based prediction model [24]. These studies focus on data collection and feature selection and only use traditional distance-based ML techniques as a tool. Our study was among the first to explore the possibility of using advanced and emerging ML technique, i.e. manifold ranking, in the prediction of cardiac arrest especially as a scoring system. The improved prediction accuracy has further proved the strong correlation of the combination of HRV and vital signs with cardiac arrest within 72h. Moreover, the successful implementation shows that graph-based machine learning methods might hold promise for handling medical data, which cannot be easily classified using traditional methods.

Furthermore, the proposed scoring system has potential to be used as a general scoring system for other applications, as it does not impose any requirements on the types of feature variables and clinical outcome. Future studies can investigate the effectiveness of the proposed scoring systems on different biomedical data to predict other clinical outcomes or diseases. It is also worth noting that this study aims to explore the parameters and performance of the manifold ranking technique in medical decision making, rather than to make specific rules for cardiac arrest prediction. In order to derive an actual expert system, we will need to recruit more patients to further optimize the decision rules.

There are limitations in this study. Firstly, the proposed scoring system only achieves its best performance when the parameters were fine-tuned based on the training data and the characteristics of new samples matched those of the training samples. Additional parameter adjustment process is necessary when samples with new distribution are added. Future studies can also investigate the parameter selection process and aim to develop an automatic parameter selection process. Secondly, the built model is geographically dependent and is not readily applicable to all regions in the world. This is a common problem by most predictive models for decision making because the process of model derivation is data driven and external validation of the built model is necessary before implementation. The use of transfer learning method [25] may be a potential solution in handling the change of patient population. In future works on developing a risk stratification tool, we will also limit our patient population to the patients with acute coronary syndromes as they are at high risk of developing adverse cardiac events. Lastly, the dataset is comparably small with low event rate (5%). Since the collection of more samples is time consuming, we chose LOOCV as the validation framework, which may generate a model over-fitting the data. The ideal way to validate the model is using totally separate training and testing sets, and this will be our future work when more data are collected.

@&#CONCLUSION@&#

In this paper, we have proposed a novel scoring system based on manifold ranking algorithm to handle classification problem, specifically the prediction of cardiac arrest within 72h. The proposed scoring system improved the prediction accuracy by utilizing the global consistency of the training data. The proposed scoring system outperformed the geometric distance scoring system in terms of AUC and sensitivity on both balanced and imbalanced datasets. We foresee the potential of extending this scoring system, as well as manifold ranking algorithm, to other medical problems. In our future work, we will investigate the parameter selection process and other techniques to improve performance on imbalanced dataset.

Marcus Eng Hock Ong and Nan Liu have a patent filing related to this study (System and method of determining a risk score for triage, Application number: US 13/791,764). Marcus Eng Hock Ong and Zhiping Lin have a patent filing related to this study (Method of predicting acute cardiopulmonary events and survivability of a patient, Application number: US 13/047,348). Marcus Eng Hock Ong and Zhiping Lin also have a licensing agreement with ZOLL Medical Corporation for the patented technology. There are no further patents, products in development or marketed products to declare. All the other authors do not have either commercial or personal associations or any sources of support that might pose a conflict of interest in the subject matter or materials discussed in this manuscript.

TL, ZL, and NL planned and established the project, performed data analysis, and drafted the manuscript. ZXK, PPP, and AFWH performed data collection and drafted the manuscript. MEHO, YKY, and BSO performed data analysis and reviewed critical revisions. All authors took part in manuscript writing and approved the final manuscript.

@&#ACKNOWLEDGMENTS@&#

We would like to thank the contributions from doctors and nurses from the Department of Emergency Medicine, Singapore General Hospital for their assistance in this study.

@&#REFERENCES@&#

