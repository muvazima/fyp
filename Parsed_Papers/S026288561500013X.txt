@&#MAIN-TITLE@&#Precise localization of eye centers in low resolution color images

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Use of color information for accurately localizing eye centers.


                        
                        
                           
                           Statistical analysis of the color distribution of the skin, eye areas and irises.


                        
                        
                           
                           Radial symmetry computation both in grayscale images and in chrominance eye maps.


                        
                        
                           
                           We report high accuracy rates for low resolution color images.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Eye localization

Eye detection

Facial feature detection

Radial symmetry

@&#ABSTRACT@&#


               
               
                  The localization of eye centers and tracking of gaze constitutes an integral component of many human–computer interaction applications. A number of constraints including intrusiveness, mobility, robustness and high-price of eye tracking systems have hindered the way of the integration of eye trackers in everyday applications. Several ‘passive’ systems based on a single camera have been lately proposed in the literature, exhibiting though subordinate precision compared to the commercial, hardware-based eye tracking devices. In this paper we introduce an automatic, non-intrusive method for precise eye center localization in low resolution images, acquired from single low-cost cameras. To this end, the proposed system uses color information to derive a novel eye map that emphasizes the iris area and a radial symmetry transform which operates both on the original eye images and the eye map. The performance of the proposed method is extensively evaluated on four publicly available databases containing low resolution images and videos. Experimental results demonstrate great accuracy in challenging cases and resilience to pose and illumination variations, achieving significant improvement over existing methods.
               
            

@&#INTRODUCTION@&#

Eyes represent the most distinctive features of the human face, while their position and movements are a significant source of information about the cognitive and affective state of human beings; eyes hold a key role in expressing interest, intention and attention. Precise eye center localization constitutes the cornerstone for gaze monitoring and can be applied in an ever increasing range of applications [1]. These include face alignment and normalization, liveness detection (e.g. for security applications), non-glasses type 3D technologies, monitoring of drivers' attention and vigilance, visual attention analysis (e.g. for marketing purposes), attentive human–computer interaction (HCI) interfaces and interactive gaze-based interfaces for disabled people.

Although many commercial, off-the-shelf products for eye detection and tracking are available in the market, they all require dedicated, high-priced hardware. The most common approaches in research and commercial systems use active infrared (IR) illumination, to obtain accurate eye location through corneal reflection [2,3]. Although these approaches yield high-precision localization, their intrusiveness is controversial and they cannot be used outdoors, in daytime applications, due to ambient infrared illumination. Other hardware approaches require the use of special equipment such as contact lenses, special helmets and electrodes [4,5], causing discomfort to the users and introducing limitations, thus rendering them cumbersome for everyday applications. Algorithmic approaches constitute non-intrusive techniques which can be incorporated in many applications where the use of extra dedicated hardware is impracticable.

Despite active research in the field, eye center localization with high precision from completely unobtrusive and remotely located (i.e. not requiring special helmets, glasses or chin rests) image-based systems, remains a very challenging task. Eyes present great variability in shape and color depending on eye state (open/closed or anything in between), iris direction, facial expression, head pose and ethnicity. Occlusions caused by hair, glasses, reflections, shadows or pose (self-occlusions) make the localization process notably difficult. Furthermore, imaging conditions such as lighting, contrast, camera characteristics and further processing (e.g. compression) have a strong influence on how eyes appear in the image. The localization process becomes even more challenging when dealing with low resolution images derived from inexpensive imaging devices such as webcams, mobile devices or pinhole cameras.

In this paper, a fast, fully automatic method for accurate eye center localization in low resolution images and videos is presented. The goal of the proposed system is to locate eye centers accurately and robustly for HCI applications, reporting accuracies comparable to the commercial hardware-based eye trackers, with the use of a single low-cost camera.

The layout of the paper is organized as follows. In Section 2, a literature review in the area of eye localization is presented. In Section 3 a detailed description of the proposed algorithm is given. Section 4 presents the experimental setup and Section 5 the results obtained using the proposed algorithm, compared with the state-of-the-art systems for eye localization. The concluding Sections 6 and 7 report implications and inferences of the work presented in this paper.

@&#RELATED WORK@&#

Over the last decades, a great number of methods have been employed for the task of eye detection and tracking [6,7]. Eye localization methods, working under natural illumination and using a single camera, can be coarsely divided into two broad categories: appearance-based and feature-based.


                     Appearance-based methods, also known as holistic or image-based methods, incorporate eye knowledge implicitly by using the intensity distribution or filter responses of the eye area and its surroundings to train a system, using example datasets. They generally require a large amount of training data and powerful non-linear algorithms in order to learn the high variability of eyes. To this end, many machine learning algorithms have been employed, including neural networks [8,9], Bayesian models [10], hidden Markov models (HMMs) [11], support vector machines [12] and Adaptive Boosting (adaboost) [13,14]. Template matching has been used by Grauman et al. [15] to detect and track eyes by searching the image for the highest correlation with a moving template image. Though simple and straightforward, this method is prone to erroneous detections due to pose, facial expressions and other changes in the face appearance. Pentland et al. [16] were the first to extend the eigenface technique to describe facial features (i.e. eigeneyes, eigennoses, eigenmouths). Subspace methods report better results when compared to template matching; nevertheless their performance is largely dependent on the training set and fail to localize eyes precisely. A prevalent method for eye localization is to employ a cascade of boosted classifiers working with Haar features [14]. The main advantage of this approach is its high efficiency and computational speed. It suffers however from high false positive rates, and its discriminative capability may be limited in cases of challenging illumination conditions or pose variations. In a nutshell, although appearance-based methods can achieve remarkably high accuracy in detecting the eye area, they often fail to provide an accurate detection of the eye centers, a critical feature for gaze monitoring.


                     Feature-based methods make explicit use of a priori eye knowledge in order to derive features such as shape, geometry, color and symmetry. Geometrical information of edges has been widely used for eye detection [17,18], also combined with several other cues [19]. For detailed modeling of the eye shape, parametric models and complex shape-based methods have been employed. They achieve localization by constructing a generic eye model in which the eye is fitted through energy minimization; deformable-template models were proposed by Yuille et al. in [20] and widely used later on [21]. Despite the accuracy of these methods, they are computationally demanding, require high-resolution images, and a close to the eye initialization. A number of methods have been employed in order to model the circular shape of the iris using the Hough transform [22,23], however the circularity shape constraints render the method applicable only to frontal or near frontal faces in high resolution images. In order to overcome these issues, ellipse fitting algorithms have been also proposed [24,25]. Other popular techniques employed to localize the eye center by modeling the iris shape are the Starburst algorithm [26] and the Integro Differential Operator 
                     [27]. Hansen and Pece [28] also model the iris as an ellipse, fitting locally the ellipse to the image through an EM and RANSAC optimization scheme. The iris region is located in [29] using intensity differences between the center region and its neighboring regions using a contrast operator, and the detection accuracy is further enhanced using a Kalman tracker. Valenti et al. [30–33] propose a technique based on isophote curvatures and a voting process for real-time eye center detection with high accuracy, while a modified Mean Shift procedure is used for tracking. The desired features of the eyes can be enhanced using several filter responses [34]. To this end, Gabor filters have attracted much popularity [35,36]. However, filtering methods yield coarse estimates of the eye and usually additional techniques for finer localization are used. The idea of projection functions has been studied by Zhou and Geng [37] in order to locate exact iris centers. Experiments show that this method is sensitive to face orientation and lighting conditions. Symmetry operators have also been investigated, usually in combination with other techniques, for the purpose of automated eye detection [38–41]. Finally, color models of the eye have received very little attention [6] and color information has been solely used in order to distinguish the eye regions from the rest of the skin area [42–44], or for tracking the eye region [45].

The proposed method is based on a synergy of color and radial symmetry to precisely and robustly localize eye centers. The use of color is based on the finding that the color distribution of the eye and particularly the iris is consistently different to its surroundings. Symmetry uses as a basis the inherent radially symmetric brightness patterns of the iris and the pupil. The main contribution of the proposed method is the use of color information for accurately localizing eye centers rather than for defining the rough areas of the eye regions, which constitutes what was studied so far in the literature. The proposed method substantially differs to existing approaches as it uses chrominance information to build eye maps which distinguish and enhance the circular shape of the iris, leading to a highly radially symmetric pattern (Section 3.2). The effectiveness of using color information is supported by a statistical analysis of the color distribution of the skin, the eye areas and the irises. The novel eye map built contributes to a cumulative radial symmetry transform with the original eye region, demonstrating high accuracy even in lower resolution images.

The proposed eye center localization system is summarized as follows: once a face is detected in a given image, regions containing the eyes are defined. Color information is used to build an eye map which emphasizes the iris area. Subsequently, a radial symmetry transform is applied both to the eye map and the original eye image. The cumulative result of the transforms indicates the precise positions of the eye centers. The procedure is illustrated in Fig. 1
                     ; a detailed description of the different stages of the algorithm is given below.

The detection of faces in a given image is carried out using the real-time face detector proposed by Viola and Jones [46]. Based upon an ensemble of boosted cascade detectors working with Haar features, it represents the state-of-the-art method in face detection. Within each detected face a Region of Interest (ROI) containing each of the eyes is defined based on face geometry (Fig. 1). The dimensions of the ROIs are amply specified so as to contain the whole eye regions even when reaching the detection limits of the face detector, regarding the in-plane and out-of-plane rotations [46]. Therefore, the width and height of the eye regions are determined as EyeRegionWidth
                        =
                        FaceWidth/3 and EyeRegionWidth
                        =
                        FaceHeight/4. Subsequently, the proposed procedure is applied to each of the cropped eye ROIs in order to localize the exact positions of each eye center.

The modeling of the human skin color requires an appropriate color space in which skin can be discriminated from any other area in the image, following a certain distribution in the color space. The YCbCr color space has been extensively used in the literature for modeling the skin regions [42,47], presenting significant advantages compared to other color spaces [42,48]. In YCbCr the luminance information (Y) is decoupled from the chrominance information (Cb, Cr), while the skin color distribution constitutes a compact, well defined cluster in the color space.

The luminance information is not useful for discriminating skin regions since it is highly dependent on the lighting conditions and thus it is most often discarded. In contrast, chrominance information can efficiently model the skin ‘tone’ and separate skin from non-skin regions. The chrominance of different skin types (e.g. black, white, yellow) shares very similar Cb and Cr values, rendering the model suitable for covering all human races. The perceived difference of the skin tone across different races is mainly due to the luminance, which is also evident from Fig. 2
                           , where people with different skin tones share very similar chrominance distributions. The decoupling of the luminance information also makes the skin modeling more resilient to different lighting conditions and uneven illumination. An additional reason for selecting the YCbCr color space is that existing image and video compression standards are based on it, thus rendering the proposed method faster when applied on such kind of compressed images.

With the goal of distinguishing the eye regions (i.e. sclera, iris and pupil) from the surrounding skin area and to accentuate the appearance of the iris, we build eye maps using the YCbCr color space components. Although the contrast in the luminance component between the eye regions and the surrounding skin may be poor in many cases, their color distribution in the chrominance components is notably different.

The EyeMapC is derived from the chrominance components of the YCbCr color space, building on the fact that the color distribution of the eye regions is significantly different from the surrounding skin area, exhibiting high Cb values combined with low Cr values (Fig. 3(a)). The information in the two chrominance channels is combined to form the EyeMapC as follows:
                              
                                 (1)
                                 
                                    EyeMapC
                                    =
                                    
                                       1
                                       3
                                    
                                    
                                       
                                          
                                             
                                                
                                                   C
                                                   b
                                                
                                             
                                             2
                                          
                                          +
                                          
                                             
                                                
                                                   
                                                      C
                                                      r
                                                   
                                                   ¯
                                                
                                             
                                             2
                                          
                                          +
                                          
                                             
                                                C
                                                b
                                                /
                                                C
                                                r
                                             
                                          
                                       
                                    
                                 
                              
                           where Cb and Cr are normalized to the range [0, 1] and 
                              
                                 
                                    C
                                    r
                                 
                                 ¯
                              
                            denotes the complement of Cr (i.e. 1−
                           Cr). Large values on the eye map are observed at the positions of the eye regions and eyebrows where the color difference from the skin pixels is maximized.

In Fig. 3 the color distribution of the skin and the eye regions in the YCbCr color space is depicted. The experimental setup for obtaining the color distributions is presented in Appendix A. Although the samples used to build the color distributions originate from subjects with different skin tones and under different lighting conditions, the skin cluster as well as the eye cluster demonstrate great compactness inside the color space. By illustrating the normal probability density functions (pdf) of the skin and the eyes in the EyeMapC component (Fig. 3(e)) we obtain distributions presenting a limited overlap. This form of the distributions indicates that the non-linear combination of the chrominance information in Eq. (1) constitutes a methodical way to enhance the separability between the skin and the eye regions.

The irises constitute the most distinct regions in both the luminance component and chrominance derived (EyeMapC) component. Regarding their appearance in the luminance component, they present significantly lower brightness values than the sclera and the skin areas, as illustrated in Fig. 4(a). Correspondingly, in the EyeMapC component, the irises are among the brightest pixels (Fig. 4(b)). The division of the EyeMapC with the Y component makes the irises more prominent and distinguishable from the other surrounding areas, further narrowing down the overlap between the distributions, as shown in Fig. 4(c). The use of morphological operations (erosion/dilation) further accentuates the irises' darker appearance in the Y component and the brighter appearance in the EyeMapC component (Fig. 5
                           ). Moreover, the round shape of the structuring elements enhances the circular structure of the irises, facilitating the forthcoming radial symmetry transform. The new EyeMapI is derived as:
                              
                                 (2)
                                 
                                    EyeMapI
                                    =
                                    
                                       
                                          EyeMapC
                                          ⊕
                                          B
                                          1
                                       
                                       
                                          
                                             
                                                Y
                                                ⊖
                                                B
                                                2
                                             
                                          
                                          +
                                          δ
                                       
                                    
                                 
                              
                           where ⊕ and ⊝ denote gray-scale dilation and erosion, respectively. B1 and B2 are flat, circular structuring elements with radii being proportional to the size of the iris:
                              
                                 
                                    
                                       
                                          
                                             B
                                             1
                                             Rad
                                             =
                                             IrisRad
                                             /
                                             2
                                          
                                       
                                       
                                          
                                             B
                                             2
                                             Rad
                                             =
                                             B
                                             1
                                             Rad
                                             /
                                             2
                                          
                                       
                                    
                                 
                              
                           where
                              
                                 
                                    IrisRad
                                    =
                                    EyeRegionWidth
                                    /
                                    10
                                    .
                                 
                              
                           
                        

The size of the iris has little variation across humans [49] and its approximate size can be inferred from the size of the detected face (and therefore the size of the eye regions). In the aforementioned formulations, coarse estimations of the iris radius and thus B1 and B2 radii are adequate, as small deviations in the estimations do not have a noticeable influence in the outcome. A correction factor δ is adopted to suppress very large values caused by null or very small denominator values. A small static number would suffice, yet, experimental tests exhibited improved results when a dynamic, data-driven value of δ is used:
                              
                                 
                                    δ
                                    =
                                    mean
                                    
                                       
                                          Y
                                          ⊖
                                          B
                                          2
                                       
                                    
                                 
                              
                           
                           Fig. 5 depicts the stages for the construction of the eye maps. The resulting EyeMapI illustrates that the iris area is the most prominent region in the image. The use of the eye maps also gives emphasis to the circular pattern of the iris regions and serves as a great assist in cases when the iris reaches its shape limits due to occlusions by the eyelids or in the presence of other visual artifacts.

Symmetry constitutes one of the primary properties of the eyes and can be exploited both in the original eye image and in the eye map constructed. In our approach, a fast and highly efficient radial symmetry transform is used, first introduced by Loy and Zelinsky in [50]. The radial symmetry transform relies on a gradient-based interest operator that works by considering the contribution of each pixel to the symmetry of pixels around it. This approach reports improved performance combined with low time complexity when compared to other symmetry operators [50]. The radial symmetry transform was appropriately tailored in order to meet the specificities of the current application while the selection of parameters was rendered fully automatic. The outline of the radial symmetry transform is presented in Fig. 6
                        .

First, the gradient is calculated using a 3×3 Sobel operator. From each point p an affected pixel paf
                         is determined. This affected pixel is defined as the pixel that the gradient vector g (p) is pointing to, of distance n away from p. As the change of the direction of the gradient is from brighter to darker areas, the transform detects only dark regions of radial symmetry. The coordinates of the affected pixels are calculated by
                           
                              (3)
                              
                                 
                                    p
                                    
                                       a
                                       f
                                    
                                 
                                 =
                                 p
                                 −
                                 round
                                 
                                    
                                       
                                          
                                             g
                                             
                                                p
                                             
                                          
                                          
                                             
                                                g
                                                
                                                   p
                                                
                                             
                                          
                                       
                                       n
                                    
                                 
                              
                           
                        where round denotes the rounding of each vector element to the nearest integer.

For each radius n, an orientation projection image On
                         and a magnitude projection image Mn
                         are created. These images, initially zero, increase their values for each affected pixel as follows:
                           
                              (4)
                              
                                 
                                    O
                                    n
                                 
                                 
                                    
                                       p
                                       
                                          a
                                          f
                                       
                                    
                                 
                                 =
                                 
                                    O
                                    n
                                 
                                 
                                    
                                       p
                                       
                                          a
                                          f
                                       
                                    
                                 
                                 +
                                 1
                              
                           
                        
                        
                           
                              (5)
                              
                                 
                                    M
                                    n
                                 
                                 
                                    
                                       p
                                       
                                          a
                                          f
                                       
                                    
                                 
                                 =
                                 
                                    M
                                    n
                                 
                                 
                                    
                                       p
                                       
                                          a
                                          f
                                       
                                    
                                 
                                 +
                                 
                                    
                                       g
                                       
                                          p
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

The radial symmetry contribution at a radius n is derived by combining the magnitude projection image Mn
                         with the orientation projection image On
                         and convolving them with a two-dimensional Gaussian kernel An
                        
                        
                           
                              (6)
                              
                                 
                                    S
                                    n
                                 
                                 =
                                 
                                    
                                       
                                          M
                                          n
                                       
                                       ⋅
                                       
                                          O
                                          n
                                          α
                                       
                                    
                                 
                                 ∗
                                 
                                    A
                                    n
                                 
                              
                           
                        where α denotes the radial strictness parameter and An
                         is a two-dimensional Gaussian kernel with μ
                        =2n and σ
                        =
                        n/2. The Gaussian kernel is used to spread the influence of the affected pixels.

Finally, the contributions for every n in a set of radii N are averaged in order to form the final result:
                           
                              (7)
                              
                                 S
                                 =
                                 
                                    1
                                    
                                       N
                                    
                                 
                                 
                                    
                                       ∑
                                       
                                          n
                                          ∈
                                          N
                                       
                                    
                                    
                                 
                                 
                                    S
                                    n
                                 
                                 .
                              
                           
                        
                     

The parameters that have to be tuned for this process are the radial strictness parameter α and the set of radii N which define the range of radially symmetric features to be detected. A low radial strictness parameter (α
                        =1) proves to be the most judicious choice as it also gives emphasis to less radially symmetric features. The choice of the parameter is justified in view of the fact that the iris always preserves some level of symmetry, regardless of the eye state. The set of radii N
                        =[nmin
                        ,nmax
                        ] was calculated based on the size of the estimated iris size. The minimum radius is defined as nmin
                        
                        =
                        IrisRad/2 and the maximum as nmax
                        
                        =5·
                        IrisRad. In order to speed up calculations, it is possible to use a sparser set of non-continuous integer values instead. The result constitutes a very good approximation to the output obtained in the case where all the continuous ranges were considered.

The final transformed image is calculated by adding the individual results of applying the radial transform to the luminance component of the eye image and to the calculated eye map (as shown in Fig. 1). The position of the maximum value indicates the localized eye center
                           
                              (8)
                              
                                 
                                    
                                       x
                                       C
                                    
                                    
                                       y
                                       C
                                    
                                 
                                 =
                                 argmax
                                 
                                    
                                       
                                          S
                                          Luminance
                                       
                                       +
                                       
                                          S
                                          EyeMapI
                                       
                                    
                                 
                              
                           
                        where (x
                        
                           C
                        ,
                        y
                        
                           C
                        ) denotes the estimated eye center coordinates and S
                        (.) the radial symmetry transform outputs of the luminance component and the EyeMapI, respectively.

In order to systematically evaluate the performance of the proposed eye localization algorithm, extensive experiments were performed on frontal and near-frontal face images in four publicly available databases, namely, the GTAV face database [51], the MUCT face database [52], the color FERET face database [53] and the BUHP database [54]. The GTAV and the BUHP databases were specifically selected to evaluate the proposed algorithm's performance in low resolution images, being among the most characteristic low resolution face databases.

The GTAV face database [51] contains low resolution color images (320×240pixels) with strong pose and illumination variations. It includes a total of 44 people with 27 pictures per person, which correspond to different pose views (0°, ±30°, ±45°, ±60° and ±90° yaw angles) under three different illuminations (environment or natural light, strong light source from an angle of 45°, and an almost frontal mid-strong light source). Furthermore, additional frontal view pictures with different occlusions and facial expression variations are included. Under these circumstances, the GTAV dataset is regarded as one of the most challenging databases. Of all the images in the database, 713 images (of all 44 persons) were considered for the evaluation of our algorithm; images where the face detector failed to detect the face due to extreme poses, were excluded (i.e. all images with yaw angles of ±60°, ±90°, as well as some images with yaw angle of ±45° where one of the eyes was completely occluded). Images containing detected faces in which eyes were completely hidden (by sunglasses or hands), were also manually removed.

The MUCT face database [52] consists of 3755 color images (640×480pixels) of faces from people belonging to a wide variety of age and ethnicities, under varying lighting conditions. The images were acquired using five webcams, arranged at different positions relatively to the subject; eyes are always visible from all positions, yet there is a considerable variation in face poses. An analysis of the database ranks it among the most “difficult” publicly available, landmarked databases of faces [52], containing images with severe occlusions from glasses and reflections, various eye states and head poses. Images where the Viola–Jones face detector failed to locate the face correctly (54 images, corresponding to a 1.4% fail percentage), were excluded from the experiments.

The color FERET database of facial imagery [53] contains 11,338 images of 994 subjects. The images' resolution is 512×768pixels. For the purposes of our experiments and in order to conduct comparisons with other methods, only the frontal face (fa) and alternate frontal face (fb) partitions of the database were considered, resulting in a total of 2636 color images from all subjects. The subjects belong to a wide range of ages and races while most of them are photographed wearing glasses.

The BUHP database [54] consists of 45 video sequences of 200 frames each (approximately seven seconds), where 5 subjects perform free head motion including translations and both in-plane and out-of-plane rotations. The videos are acquired under uniform illumination, in a standard office environment and the eyes are always visible, except for some minor self-occlusions. The images in the video sequences have a resolution of 320×240pixels and the head occupies between 20 and 50% of the total frame area. The faces were not detected in 692 frames, corresponding to 7.73% of the database, mainly caused by extreme head poses surpassing the working range of the face detector.

The measure employed in order to evaluate the accuracy of the proposed eye center localization method is the normalized error, first introduced by Jesorsky et al. [55]. This normalized error ϵ, also denoted as worst eye error, considers from both eyes the detection with the greatest deviation from the true eye center and is formulated as:
                           
                              (9)
                              
                                 ϵ
                                 =
                                 
                                    
                                       max
                                       
                                          
                                             
                                                
                                                   
                                                      C
                                                      ˜
                                                   
                                                   l
                                                
                                                −
                                                
                                                   C
                                                   l
                                                
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      C
                                                      ˜
                                                   
                                                   r
                                                
                                                −
                                                
                                                   C
                                                   r
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             C
                                             l
                                          
                                          −
                                          
                                             C
                                             r
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 C
                                 ˜
                              
                              
                                 l
                                 \
                                 r
                              
                           
                         are the localized by the proposed method left and right eye centers and C
                        
                           l\r
                         stand for the manually labeled left and right eye centers respectively. The ‖C
                        
                           l
                        
                        −
                        C
                        
                           r
                        ‖ term represents the interoccular distance (distance between eye centers), and is used as a reference value for normalizing the localization error.

Upon calculation of the normalized error ϵ for each image, the accuracy of the algorithm was expressed as the number of the normalized errors that fell below an assigned threshold, divided by the total number of images in the database. The worst eye error indicates that both eyes are detected below the assigned error threshold. The thresholds used for our tests were ϵ≤0.25 which roughly corresponds to the distance between the eye center and the eye corners, ϵ≤0.1 which corresponds to the diameter of the iris and ϵ≤0.05 which corresponds to the pupil area (precise localization).

@&#EXPERIMENTAL RESULTS@&#

The evaluation of the proposed eye localization system yielded precise detections of eye centers in all the databases tested. Our method performs remarkably well in challenging illumination conditions, out-of-plane rotations, in the presence of glasses and partial occlusions by hair, reflections or shadows (Fig. 7
                     ). It is also resilient to the eye state, as long as the eyes are not completely occluded by the eyelids. The proposed system failed to extract accurate results only in cases when the eyes were entirely closed or in extreme cases of uneven illumination and occlusions (Fig. 8
                     ).

The results of the proposed system were compared against state-of-the-art methods for accurate center detection in the literature. Radial symmetry transform is used in [40] for fine localization of the eye centers, in coarse eye regions defined in advance, using Gabor filter responses. Although the parameters of the radial symmetry transform are not explicitly defined, a simulation using a wide range of values yielded an upper detection boundary for this approach. Timm et al. [18] propose a gradient-based eye localization approach utilizing a simple objective function, consisting of dot products. Although their method does not yield superior accuracy for precise detection (ϵ≤0.05), it is reported to have the best average performance over all values of ϵ. The method of Valenti et al. [30,31,32,33] which relies on isophote curvatures, reports a considerable improvement in accurately locating eye centers when compared to other state-of-the-art-methods. For databases where the accuracy results were not explicitly reported in their work (i.e. for GTAV and MUCT databases), a publicly available implementation
                           1
                        
                        
                           1
                           
                              https://staff.fnwi.uva.nl/r.valenti/index.php?content=EyeAPI.
                         of their basic algorithm using the optimal parameters (obtained through grid search), was used for comparison.

Except from the proposed method which considers the cumulative radial transform result in both the original and the EyeMapI image, we also examined the scenario where the radial symmetry transform was performed only on the EyeMapI image (i.e. considering only the SEyeMapI
                         component in Eq. (8)), denoted hereafter as Color Radial Symmetry (ColorRS). ColorRS is examined as an alternative solution, permitting a different tradeoff between performance and computational complexity. In particular, it generally achieves slightly lower accuracy rates, yet the processing time is reduced. A choice of the most appropriate method can be application specific.


                        Table 1
                         presents the performance of the proposed method against state-of-the-art methods in the GTAV face database. We observe that in the low-resolution and ‘difficult’ images of the specific database, the proposed method achieved significant improvement in performance over the rest of the approaches examined for high precision localization (ϵ≤0.05), as well as for detection within the iris area (ϵ≤0.1) and for coarser detection of the eye area (ϵ≤0.25). The large discrepancies between different methods for accurate localization (ϵ≤0.05) can be attributed to the low resolution of images; small deviations of a few pixels from the actual center can result in significant change of the overall performance.

In Table 2
                         the superior accuracy of the proposed method for high precision localization in MUCT face database is shown. Regarding coarser detections of the iris, the proposed method also achieved superior rates than the rest of the methods examined. Likewise, while ColorRS achieved superior results than most methods for precise detection, its performance was somewhat reduced compared to the proposed method. The main strength of the proposed method and the ColorRS method is their performance for very precise localization. In particular, for (ϵ≤0.05) the proposed method excelled by at least 10.3% and 11.3% in the respective databases examined, while the ColorRS method outperformed the rest of the approaches by at least 5.6% and 8.1%, respectively.

The performance of the proposed algorithm in the color FERET database, also compared with state-of-the-art methods in the literature is displayed in Table 3
                        . Three additional methods, mentioned in Section 2, were also considered for comparing the performance. The proposed method demonstrates higher localization rates than the rest of the methods, in all the examined cases. Although the color FERET database contains higher resolution image, without pose variations and with good illumination conditions, the rates for precise localization are reduced compared to the previously examined, more “challenging” databases. This phenomenon is attributed to the ground truth being sometimes unreliable, as also reported in [32]; in most cases the eyes are annotated anywhere within the iris area and not in their true center. As a result, in most of the cases where the algorithm fails to accurately detect the eye center, the localization is more accurate than the manual annotation. This also gives reasoning for the discrepancy between localizations for ϵ≤0.05 and ϵ≤0.1 (≈20%), which is observed for all the different methods. In the current dataset, the good performance of the proposed algorithm can be mostly witnessed for localizations within the iris area, with the failure cases occurring when the eyes are obscured to a great degree due to reflections from the glasses or are totally closed.

Finally, the performance of the proposed method in the low resolution color images of the BUHP database is shown in Table 4
                        . The proposed method presents superior accuracies compared to the ones obtained using the methods of Timm and Barth [18] and Yang [40] as well as to the accuracies reported by Valenti et al. in [33] using both their original method and their method robustified by using head pose cues. The ColorRS method performs slightly worse for very accurate localizations, presenting comparable performance to the second best method. For detections within the iris area the proposed methods share the best performance among the rest of the systems tested, while for coarser detection within the eye area the method of Valenti [33] incorporating pose information has a slight edge.

To quantitatively evaluate the robustness of the proposed method to pose and lighting variations we employed the GTAV and MUCT databases, appropriately segregated into subsets of different poses and illuminations. When referring to pose, solely out of plane rotations were considered (head's yaw and pitch angles), on the grounds that in-plane rotations of the face (roll angles) have absolutely no effect on the performance of the proposed method (on condition that the eye ROIs contain the eyes).

In GTAV database, three lighting subsets were created (natural light, strong light source from an angle of 45° and an almost frontal mid-strong light source) which, in turn, were each split into five subsets of different poses (0°, ±30°, ±45° yaw angles). Each different subset contains images from all 44 subjects of the database, including also the cases where the face detector failed (reaching its detection limits regarding out of plane rotation), to which the faces were manually annotated. The results for each illumination and pose variation are reported in Table 5
                        . To gain a better understanding of how the pose influences localization performance, instead of considering the worst eye, we obtained the accuracy for each eye separately (left/right).

The best accuracy is reported for the frontal mid-strong illumination with neutral pose (see Table 5), which usually resembles the case of photo shooting in a controlled environment. The average performance was almost equal for the natural (uniform) lighting and frontal mid-strong lighting, where phenomena like uneven or low contrast illumination and shadows are usually less observed. With regard to the pose variations, the proposed method retained a very competent performance, regardless of the pose. The accuracy of the proposed method inevitably decreased for greater poses, but mainly for the eyes at the opposite side of the face as only a part of the eye and its surrounding area remained visible. The noticeably lower accuracies of the eyes in the opposite side, at the extreme cases of ±45° are attributed to the great degree of self-occlusion from the face (in most cases that eye is not visible at all), hence eye localization is rendered obsolete.

Regarding the MUCT database, given that each subject was photographed with two or three random lighting setups from a total of 10 different lighting setups, only subsets with regard to pose were created. The arrangement of 5 cameras, as described in [45], allowed the categorization of all subjects in 5 pose categories, each containing 751 images: neutral pose (0°), +20° and +38° yaw angles, +21° and −22° pitch angles.

Drawing on the results of Table 6
                        , the difference in the proposed method's performance was marginal for the +20° and somewhat reduced for +38° yaw angles. Pitch angles did not affect the performance significantly; when the head was slightly tilted backwards (−22° pitch angle) we obtained the best performance – even over the neutral pose – while in the case where it was tilted forward (+21° pitch angle) a slightly lower performance was reported. A closer examination of this decrease in performance for bigger yaw and pitch angles yielded the following: in the neutral case, the irises are positioned directly towards the camera, getting the best visibility possible. As the angle was increasing, a smaller segment of the eyes was visible; eyelashes distorted or obscured the shape and appearance of the iris, whereas the nose (for yaw angles) and eyebrow structure (for pitch angles), which protrudes with regard to the eye socket, often caused shading effects.

@&#DISCUSSION@&#

The primary asset of the proposed system is its performance in low resolution images. The main reasoning behind this is that radially symmetric patterns are always preserved to a certain degree, regardless of the image resolution. Color information is also a trait that is preserved in images of lower resolution, greatly enhancing the performance of the proposed system. The GTAV and the BUHP are representative examples of databases containing low resolution images. Indicatively, in GTAV the iris diameters correspond approximately to 8pixels and for precise localization (ϵ≤0.05) to be successful, the detected center should be located within a radius of 2pixels from the manually annotated eye center. Experiments have shown that a total shift of a few pixels (1–2pixels) for all the eye center positions was able to reduce the system's accuracy for ϵ≤0.05 almost to half. Respectively, in the BUHP database the iris diameters roughly correspond to 6pixels which allows a detection distance of 1–2pixels from the true eye center for precise localization. In databases comprising of higher resolution images where no severe occlusions are present, the proposed method is capable of achieving almost perfect localization.

One of the most important observations derived from the current research is that, although rarely considered in literature, color comprises a very useful cue for accurate localization. The use of color information allows the system to cope well with the presence of illumination artifacts or shadows and cases where the iris is not the only dominant element in the eye image. These artifacts cannot be easily distinguished in a grayscale image as they can present the same intensity distribution as the iris or change the appearance of the iris (e.g. in the case of shadow), reducing the localization performance. Moreover, color information has proven to be very helpful in cases where the iris is merely visible and the circular shape reaches its limit, such as eye closure in a big degree or extreme positions of the eye in the eye socket. The color distribution generally constitutes a feature resilient to illumination conditions, pose and most appearance changes. It is worth mentioning that the method presented by Yang et al. [40] makes use of a quite similar radial symmetry transform but without incorporating any color information, achieving significantly lower accuracies (Tables 1–4). On the contrary, when utilizing the same radial symmetry transform solely on the eye map derived from color (ColorRS), instead of the grayscale image, the results were remarkably improved. This observation provides supporting evidence on the importance of the contribution of color information in future research.

One additional feature of the proposed algorithm is its low computational complexity. The most computationally demanding part is the radial symmetry transform, which however presents relatively low complexity, depending linearly on the size of the image i.e. O(KN), where K is the number of pixels and N is the radii (range) of the local neighborhood [50]. Using a Matlab implementation on a 2.53GHz Intel i5 (single core implementation), the system was able to process a 240×320pixel image in 0.12s (without considering face detection time). Similarly, ColorRS algorithm processed the same image in 0.07s, thus, the computational complexity was significantly reduced. With a hardware implementation or lower level C/C++ coding of the proposed methods, requirements of real time even for higher resolution images can be met.

@&#CONCLUSIONS@&#

In this paper we proposed a new, fully automated method for precise eye center localization, exploiting radial symmetry and color information. Our system constitutes a non-intrusive method based solely on a single low-cost camera. It combines simplicity with high precision, providing accurate localization, robustly tackling challenging scenarios of partial occlusions, illumination and pose variations.

An extensive evaluation of the proposed approach was performed on low resolution images and videos, containing many different cases of challenging conditions. A comparison with existing systems demonstrated a significant improvement in performance, especially for very accurate eye center localization. Given the high accuracy rates achieved by the proposed method, we believe that our system can represent a very promising low-cost alternative for everyday HCI applications.

In view of building statistical color models for the eye regions and the irises, experiments were performed using the MUCT database [52]. The MUCT database was selected as it provides great diversity in illumination conditions and subjects' ethnicity. Moreover, it is meticulously landmarked and thus different facial regions can be accurately pinpointed.

To build the color model of the eyes we first transformed the image in the YCbCr color space and extracted the points located inside the polygons that enclose both eye regions. From these points the mean value for each of the YCbCr channels was calculated. In a corresponding manner, in order to build the skin model, the skin patch under and between the eyes was considered (surrounding skin area) and the mean value was calculated. As a result, 3755 values, one for each image in the database, were collected to build each of the models.

In order to model the tone distribution of the irises, only the iris region out of the delineated eye region was retained, discarding the sclera regions (the white parts of the eye). The resulting color models for the eye areas and the irises are presented in Figs. 3 and 4, respectively.

@&#REFERENCES@&#

