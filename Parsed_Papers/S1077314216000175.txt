@&#MAIN-TITLE@&#Scene parsing using graph matching on street-view data

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           An effective scene parsing framework via graph matching guidance on street-level data is proposed.


                        
                        
                           
                           Graph matching is introduced to partially match image components taking into account the regional similarity of scenes.


                        
                        
                           
                           The proposed algorithm can be applied to small training and testing sets, and achieves competitive parsing performance.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Scene parsing

Graph matching

Markov random field

Street view

@&#ABSTRACT@&#


               
               
                  Scene parsing, using both images and range data, is one of the key problems in computer vision and robotics. In this paper, a street scene parsing scheme that takes advantages of images from perspective cameras and range data from LiDAR is presented. First, pre-processing on the image set is performed and the corresponding point cloud is segmented according to semantics and transformed into an image pose. A graph matching approach is introduced into our parsing framework, in order to identify similar sub-regions from training and test images in terms of both local appearance and spatial structure. By using the sub-graphs inherited from training images, as well as the cues obtained from point clouds, this approach can effectively interpret the street scene via a guided MRF inference. Experimental results show a promising performance of our approach.
               
            

@&#INTRODUCTION@&#

Scene parsing, which is the segmentation and classification of regions in an image with different semantics, is of great importance in the computer vision community. For decades, various approaches have been developed to parse scenes on image sets, so that algorithms can learn and infer results from the significant amount of information provided by images. However, there are difficulties when only images are used. First, it is difficult to train a sufficiently effective classifier to label the regions due to the diversity of the categories. Second, shadows heavily influence the labeling in most of the images, such that regions on two sides of the shadow edge are labeled with different categories although they represent the same object. In contrast, point clouds can provide extra cues that images cannot convey. For example, given depth and height measurement, the 3D shape of an object can be estimated precisely; shadows will never affect the segmentation of the point clouds. Methods integrating both images and LiDAR data have been explored over the past few years. In this paper, we focus our attention on the urban street view with data from both images and point clouds collected at street level; the data were provided by Google.

Many efforts have been made to accurately parse images into a variety of categories. These methods are mostly based on the 2D global or local features. Liu et al. [1] proposed a scene parsing framework based on dense image alignment over dense scale-invariant feature transform (SIFT) images, which has been a successful technique and performs very well. However, this method works on a pixel-wise level and the belief propagation optimization over the graph, with respect to pixels, has a high computational complexity. In addition, a large image database is needed [2], which makes this method difficult to use in practice. Farabet et al. [3,4] employed a multi-scale convolutional network to compute dense feature vectors centered on each pixel. After a max-pooling stage, a classifier is trained to estimate the histograms of all object categories. Another method is a scene segmentation approach that matches object boundaries or edges across scenes [5], which does not need to extract features from images explicitly. Similarly, Russell et al. [6] also proposed a scene segmentation method by matching image composites across scenes. Thus different visual clues can be collected to help enhance the parsing accuracy. Compared to the algorithms that work on pixel level, on the other hand, some attempts over superpixels have been made [7–9]. These methods are based on the fact that representation over superpixel can reduce the computational complexity remarkably, and also can naturally form an aggregate of pixels with a similar local appearance. Since the performance of the superpixel segmentation significantly influences the parsing results, state-of-the-art segmentation approaches [10,11] are always employed. The approach proposed by Tighe and Lazebnik [7] is a typical non-parametric parsing scheme over superpixels using the segmentation method in [11], in which a training stage is not necessary.

More accurate parsing performance can be achieved by integrating images and 3D information than when images alone are used. Some approaches [12–14] use structure-from-motion method [15], which allows 3D information in different scenes to be effectively estimated from image sequences (e.g., stereo, video). Especially, Xiao and Quan [14] developed a method in which a refined dense depth map was computed, providing extra information such as surface normal, planarity, height and distance to camera path. However, methods in this category heavily rely on the estimation accuracy of the 3D information and have a high computational complexity. An alternative to estimate the 3D information is to gather data using LiDAR sensors [16–19]. Zhao et al. [16,17] used Velodyne LiDAR sensors and cameras mounted on vehicles to capture images and LiDAR data. They detected a large range of objects as “obstacle” (e.g., car, tree, pedestrian and any other objects limited in a bounding box); Fuzzy logic inference was employed to classify them in various categories. Instead of using 3D information, Ardeshir et al. [20] proposed a method to conduct scene understanding with the help of location and address. Wang et al. [21] introduced a holistic scene understanding framework by integrating object detection, pose estimation, depth reconstruction and semantic segmentation. Since the main objects in street scenes are buildings, it is anticipated that the parsing scheme can also work on building facades. Unlike regular image parsing tasks with several labels, facade parsing aims at identification of targets with common shape and symmetry. Some researchers propose to implement facade parsing by using only images [22,23]. Other methods [24,25] try to interpret building facades using range data.

Almost all the aforementioned methods use the information in pixel or superpixel, either to train a classifier or to minimize the energy function. The information in pixel or superpixel can be considered as lower level vision features, while these features include SIFT, HOG or Color Histogram, which tend to be isolated without spatial relationship. However, in pictures of real world, pixels or superpixels appear in a more organized structure which can be referred to as higher level features, namely “objects”. One object always includes a collection of typical features and their spatial relationship, and this observation inspires many computer vision applications by representing an object using a graph (i.e., undirected graph with attributes on both nodes and edges). In street view images, similar objects appear repeatedly across scenes, such as buildings, cars and pedestrians. One can anticipate that matching similar objects in terms of structured graphs regardless of locations where they appear in the pictures, will make the parsing more effective and reliable. Compared with the methods that employ information at pixel or superpixel level, matching at object level can be considered as employing “higher” level visual cues from images. Moreover, identifying similar objects is a more natural way as how human beings recognize a scene. To the best of our knowledge, extensive research has not been conducted on scene parsing based on graph matching, though this has been shown to be successful in object matching, recognition and retrieval [26–29]. A similar method was proposed by Gould and Zhang [30], in which a correspondence graph is established via patch similarity to achieve the annotation transferring. Our work is inspired by [6] which collects low-level visual features across scenes.

A parsing algorithm via Markov Random Field (MRF) optimization on a guided graph derived from a graph matching of superpixels is proposed. In this scheme, the structure of the guided graph has advantages of both data fidelity and inherited sub-graphs from training images. The following contributions are made in this paper:

                        
                           1.
                           A graph matching scheme [31] is employed to interpret the semantic structure of the query scene with respect to the guidance scene. Taking into account the fact that similar images share a similar semantic structure, this step involves the calculation of sub-graphs in the query scene, which correspond to matched structures in the training scenes, providing more plausible evidence for the labeling stage.

Apart from normal MRF optimization, this approach takes both the query scene itself and the structures inherited from training scenes as input; consequently, the optimization is implemented on an enhanced graph structure where the label distribution is constrained and the inherited adjacency relationship is considered.

The rest of the paper is organized as follows. In Section 2, we introduce and elaborate on the proposed approach. Specifically, details how to find correspondence via graph matching and how to use the matching results as guidance to infer the labels of the query images are presented in in Sections 2.3 and 2.4, respectively. In Section 3, we present the experiments that was conducted on datasets from several cities with different street view styles. Experimental results show that approach achieves the promising performance. The conclusion and future research are presented in Section 4.

@&#METHODOLOGY@&#

In this section the workflow of the approach is detailed. First, preliminaries and pre-processing of the image data are introduced. A segmentation scheme is applied to the point clouds for partitioning structures with different semantics and constraining the probability distributions of the labels. By finding correspondences using graph matching between query images and training images, the regions that share similar structures and local appearances can be found. Finally, label inference is implemented using MRF on a guided graph, which is the combination of image fidelity and inherited structures. The potentials of the nodes in the graph follow distribution constraints derived from the point clouds.

The images are collected using perspective shuttering cameras and the range data is scanned using SICK LiDAR sensors. Examples of the data are shown in Fig. 1
                         (a) and (c). According to the figure, the camera intrinsics distort the image to some degree.

The images in the training set and the test set are labelled manually using the tool “labelMe” [2]. The images are segmented into the label set sky, building, window, door, vehicle, pedestrian, road, tree, and sign. Different categories can be specified using our method on other problems, depending on what is required to address. The regions that cannot be categorized into the above labels, are classified as “undefined”. One may observe that “window” and “door” appear on the building facades, therefore they also belong to the “building” category. This setting is used in order to examine the parsing ability of our algorithm on building facades.

All the training and test images are then over-segmented into superpixels using the approach in [11]. This method is capable of giving appropriate and sufficient information on image regions, and reaching a good balance between computational complexity and informational fidelity, as compared to a pixel-level approach. An example of superpixel segmentation is shown in Fig. 1 (b). The approach is effective under the assumption that images and range data are well aligned; otherwise an alignment procedure for the two types of data would need to be implemented first.

In Fig. 1 (c), the corresponding point cloud is shown from the same viewpoint as Fig. 1 (a). The point cloud is transformed into the camera pose using the corresponding camera intrinsics and projective transformation. To segment the objects in the point cloud, a framework is employed as follows. First, the ground points are separated and labeled using piece-wise random sample consensus (RANSAC) [32] plane fitting near the ground height with respect to the vehicle, assuming that the ground is locally planar. All points corresponding to the ground are then removed and the above-ground structures are retained. Since all other objects are originally attached to the ground, it is simple to partition and separate each single object using Euclidean clustering [33]. Thus the ground points and all the above-ground points without labels are obtained.

Since the captured point cloud is not sufficiently dense and information is missing, as shown in Fig. 1 (c), it is difficult to classify each structure perfectly. Rather than identifying the exact label of each isolated structure, our method recognizes the subset of labels to which the structure does not belong. For some specific structures where the features are apparent, such as “sky” and “facade”, the exact classification can be obtained. However, ambiguity occurs on small structures; for example, it can be unclear as to whether an object is a “pedestrian” or small “tree” due to the lack of contextual information from the sparse point cloud. Thus, only the feasible label set pedestrian, tree can be applied, and the exact classification is left until the following stages using image information. For now a uniform probability distribution summing to 1 on each feasible label set is used.

To calculate the feasible label set, the building facade is first identified by taking into account the size of structure and the normal on each point, since most of the normal vectors are parallel to the ground. Due to the fact that there are small irregularities on the plane of the facade, the difference of normal [34] is used to estimate the normal vector. This method not only identifies the planes of facades, but also calculates the edges of the windows and doors to some extent if they are made of glass (i.e., no reflection of laser from the area). The lack of precision is due to the fact that normals on a small scale, and near the edges of windows and doors, are not stable. Therefore, by finding “holes” and the “edges”, the possible positions of windows and doors can be located. However, given sparsity of the point cloud, the locations of windows and doors are still imprecise. This will be discussed further in Section 3.2.

The rest of the objects are interpreted using their sizes and 3D shapes. First, we construct a bounding box for each object. The objects can be assigned feasible label sets by taking into account the size of bounding boxes and the ratio of height, width and length. For the objects with low height, we exclude “sign” and “tree” from the feasible label set, since signs on road are mostly attached to a high pole (trees are also high). We calculate the ratio of the length to the height of the bounding box, and use this ratio to identify if an object does not belong to “vehicle”. This is because the shape of the bounding box of an vehicle is always a cuboid, thus the ratio tends to be above a threshold. For a high object, we also try to identify if it belongs to “tree ” or “sign” by using the shape of the object. Since the points of a tree diverse more than a sign or pole in the horizontal direction, we introduce the PCA to calculate the principle direction of the point cluster, and use overall variance along the principle direction to identify to which category an object belongs. As discussed above, sometimes it is difficult to identify the object into a single label, because of occlusion, partial scan or movement of the objects. In this case, an initial uniform distribution over undecided labels is assigned to the object. The regions with no points that are not labeled into aforementioned labels are considered to be in the category of “sky”. It should be noted again that the segmentation procedure does not provide a deterministic classification for each single object, rather it calculates the feasible set that an object may belong to with a relatively high belief. Besides, by narrowing the feasible label set for each object, the corresponding searching space for each node (superpixel) in MRF can further be reduced.

After segmenting the point clouds, the feasible label sets of the objects are obtained; and each single object in point clouds is assigned a uniform probability distribution. This information is used to assist the MRF inference.

Feature or region correspondence via graph matching is one of the fundamental problems in computer vision, which consists of forming undirected graphs on features for both reference and query images, and finding similarities between both node pairs and edge pairs using graph theory. Since the relative positions of the camera and the LiDAR sensor are fixed, it can be observed that, although there are many different images, their structures can be similar. It does not mean that the structures of the images are the same, instead, it’s more likely to find out some training samples that share a similar image structure as the query image. For example, many images taken by the front camera show a structure in which buildings are on both sides of the road and a portion of sky appears between them, as well as vehicles and pedestrians appearing on the road. This observation leads to the representation of image structures using graphs. In our work, the approach [31] to match sub-graphs between training images and query images is utilized.

The structures of the images are represented using adjacency graphs with respect to superpixels. An adjacency graph on an image is denoted as 
                           
                              G
                              =
                              
                                 〈
                                 N
                                 ,
                                 E
                                 ,
                                 A
                                 〉
                              
                           
                         where N is the node set of the graph while each element in N corresponds to a superpixel, an edge Eij
                         belongs to the edge set E if and only if the corresponding two superpixels i and j are adjacent. A is the attribute assigned to the node or the edge. The attribute Ai
                         assigned to the node i is a vectorized feature on the corresponding superpixel; and Aij
                         is the attribute assigned to the edge Eij
                        . Several features are adopted at this stage, including color histogram, normalized visual words in terms of SIFT features and texton histogram. The selected features are listed in Table 1
                        .

Specifically, the normalized visual words are computed by first finding clustering centers over dense SIFT descriptors on all the training images, and, a histogram is then formed by calculating to which cluster each pixel on a superpixel belongs. Determination of the features is presented as follows. Assuming that the centroids Ck
                         of the SIFT clusters have already been calculated using the Bag of Words (BOW) model, and the number of the centroids is K, then for a given superpixel Pi
                         the corresponding normalized feature Hi
                         is calculated as follows:

                           
                              (1)
                              
                                 
                                    
                                       
                                          
                                             B
                                             
                                                k
                                                ,
                                                i
                                             
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                ∑
                                                
                                                   h
                                                   ∈
                                                   
                                                      S
                                                      i
                                                   
                                                
                                             
                                             N
                                             e
                                             a
                                             r
                                             e
                                             s
                                             t
                                             
                                                (
                                                S
                                                I
                                                F
                                                T
                                                
                                                   (
                                                   h
                                                   )
                                                
                                                ,
                                                
                                                   C
                                                   k
                                                
                                                )
                                             
                                             
                                             d
                                             =
                                             1
                                             ,
                                             …
                                             ,
                                             K
                                          
                                       
                                    
                                    
                                       
                                          
                                             H
                                             i
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                1
                                                Z
                                             
                                             
                                                [
                                                
                                                   B
                                                   
                                                      1
                                                      ,
                                                      i
                                                   
                                                
                                                ,
                                                .
                                                .
                                                .
                                                ,
                                                
                                                   B
                                                   
                                                      K
                                                      ,
                                                      i
                                                   
                                                
                                                ]
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

where Si
                         is the ith superpixel and h represents a pixel in Pi, SIFT(h) calculates the SIFT feature of pixel h, Ck
                         is the kth centroid of BOW and Nearest( · ) is the nearest classifier. Z is a parameter to guarantee Hi
                         is normalized.

The original adjacency graph G represents a pairwise MRF which is easy for labeling inference. However, adjacency relationship alone cannot provide sufficient information for graph matching since affinity should also be considered in finding feature correspondence. Additionally, matching over all superpixels is a waste of computational resources because correspondence can be found only on a sub-set of the superpixels. Therefore, an “active graph” [31] should be constructed in order to reduce the candidate node and edge set and preserve the affinity between node pairs. To achieve this, we propose to use lλ
                        -distance as the metric to represent the similarity between superpixels:

                           
                              (2)
                              
                                 
                                    S
                                    
                                       (
                                       i
                                       ,
                                       j
                                       )
                                    
                                    =
                                    
                                       
                                          
                                             ∥
                                          
                                          
                                             
                                                F
                                                i
                                             
                                             −
                                             
                                                F
                                                j
                                             
                                          
                                          
                                             ∥
                                          
                                       
                                       λ
                                    
                                 
                              
                           
                        
                     

where i and j represent two superpixels from the query image and training image, respectively. Fi
                         and Fj
                         represent the corresponding feature vectors, respectively. For superpixel i, all the values of j that satisfy S(i, j) < σ are regarded as candidate nodes, and vice versa. σ is a threshold. To measure the similarity between edge pair, we follow the setting from the approach [31] by using Symmetric Transfer Error (STE). The difference in this research is that we normalized and stacked the three descriptors on superpixels (color histogram, normalized visual words and texton histogram) into a single feature vector. Further, the position of a superpixel is defined by the center of mass. We define the obtained “active graphs” as Gp
                         and Gq
                         for the query and training images, respectively.

For a query image, the global features are first computed and a subset is found from the training images using a K-nearest approach in terms of a l
                        2-distance. This subset leads to fewer images to compare, saving a significant amount of computational time. Two types of global features are adopted: GIST [35] and Spatiogram [36], as shown in Table 1 (b).

A graph matching approach proposed by Cho and Lee [31] is utilized as the basic scheme to find region correspondence in our implementation. In this approach, correspondences between node pairs are found by maximizing an energy function defined on a reweighted affinity-preserving graph. Let us regard both the query image and one training image as active graphs with attributes with respect to the superpixels, hence 
                           
                              
                                 G
                                 p
                              
                              =
                              
                                 (
                                 
                                    V
                                    p
                                 
                                 ,
                                 
                                    E
                                    p
                                 
                                 ,
                                 
                                    A
                                    p
                                 
                                 )
                              
                           
                         for query image and 
                           
                              
                                 G
                                 q
                              
                              =
                              
                                 (
                                 
                                    V
                                    q
                                 
                                 ,
                                 
                                    E
                                    q
                                 
                                 ,
                                 
                                    A
                                    q
                                 
                                 )
                              
                           
                         for a training image. A compatibility matrix 
                           
                              
                                 W
                                 
                                    i
                                    a
                                    :
                                    j
                                    b
                                 
                              
                              =
                              f
                              
                                 (
                                 
                                    A
                                    i
                                    p
                                 
                                 ,
                                 
                                    A
                                    j
                                    p
                                 
                                 ,
                                 
                                    A
                                    
                                       i
                                       j
                                    
                                    p
                                 
                                 ,
                                 
                                    A
                                    a
                                    q
                                 
                                 ,
                                 
                                    A
                                    b
                                    q
                                 
                                 ,
                                 
                                    A
                                    
                                       a
                                       b
                                    
                                    q
                                 
                                 )
                              
                           
                         measures the mutual consistency of the attributes between pairs of correspondences 
                           
                              (
                              
                                 V
                                 i
                                 p
                              
                              ,
                              
                                 V
                                 a
                                 q
                              
                              )
                           
                         and 
                           
                              (
                              
                                 V
                                 j
                                 p
                              
                              ,
                              
                                 V
                                 b
                                 q
                              
                              )
                              ,
                           
                         where A is the attribute assigned to both nodes and edges. i and j are nodes from graph Gp
                        , and a and b are nodes from graph Gq
                        . The elements of W are defined by using the concatenated feature descriptor on the superpixels and the STE measurement. Concretely, for a pairwise edge similarity W
                        
                           ia: jb
                        , an affine region feature i on the superpixel 
                           
                              x
                              i
                              p
                           
                         (concatenated feature), centered at the center of mass of the superpixel, is represented by an elliptic region, and the orientation is estimated using gradient histogram [37]. With this notation, the affine homography transformation 
                           
                              
                                 T
                                 
                                    i
                                    a
                                 
                              
                              
                                 (
                              
                              
                                 )
                                 ˙
                              
                           
                         is defined from a feature i in p to another feature a in q, so that the neighborhood points xp
                         and xq
                         of 
                           
                              x
                              i
                              p
                           
                         and 
                           
                              x
                              a
                              q
                           
                         are related by 
                           
                              
                                 x
                                 q
                              
                              =
                              
                                 T
                                 
                                    i
                                    a
                                 
                              
                              
                                 (
                                 
                                    x
                                    p
                                 
                                 )
                              
                           
                         
                        [31,38]. Then the transfer error of (j, b) with respect to (i, a) is denoted as 
                           
                              
                                 d
                                 
                                    j
                                    b
                                    |
                                    i
                                    a
                                 
                              
                              =
                              
                                 ∥
                                 
                                    x
                                    b
                                    q
                                 
                                 −
                                 
                                    T
                                    
                                       i
                                       a
                                    
                                 
                                 
                                    (
                                    
                                       x
                                       j
                                       p
                                    
                                    )
                                 
                                 ∥
                              
                           
                        . Thus the STE measurement W can further be defined as 
                           
                              
                                 W
                                 
                                    i
                                    a
                                    :
                                    j
                                    b
                                 
                              
                              =
                              m
                              a
                              x
                              
                                 (
                                 0
                                 ,
                                 α
                                 −
                                 
                                    (
                                    
                                       d
                                       
                                          j
                                          b
                                          |
                                          i
                                          a
                                       
                                    
                                    +
                                    
                                       d
                                       
                                          b
                                          j
                                          |
                                          a
                                          i
                                       
                                    
                                    +
                                    
                                       d
                                       
                                          i
                                          a
                                          |
                                          j
                                          b
                                       
                                    
                                    +
                                    
                                       d
                                       
                                          a
                                          i
                                          |
                                          b
                                          j
                                       
                                    
                                    )
                                 
                                 /
                                 4
                                 )
                              
                           
                        .

We denote 
                           
                              x
                              ∈
                              
                                 
                                    {
                                    0
                                    ,
                                    1
                                    }
                                 
                                 
                                    
                                       n
                                       p
                                    
                                    
                                       n
                                       q
                                    
                                 
                              
                           
                         representing correspondence between the two graphs which is a vectorized replica of 
                           
                              X
                              ∈
                              
                                 
                                    {
                                    0
                                    ,
                                    1
                                    }
                                 
                                 
                                    
                                       n
                                       p
                                    
                                    ×
                                    
                                       n
                                       q
                                    
                                 
                              
                              ,
                           
                         where np
                         and nq
                         are the numbers of superpixels in the query and training images respectively. Using the pre-defined W, the graph matching problem can then be formulated as finding vector x
                        * that maximizes the energy function:

                           
                              (3)
                              
                                 
                                    
                                       
                                          
                                             x
                                             *
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             arg
                                             max
                                             (
                                             
                                                x
                                                T
                                             
                                             W
                                             x
                                             )
                                             
                                             s
                                             .
                                             t
                                             .
                                          
                                       
                                    
                                    
                                       
                                          x
                                       
                                       
                                          ∈
                                       
                                       
                                          
                                             
                                                {
                                                0
                                                ,
                                                1
                                                }
                                             
                                             
                                                
                                                   n
                                                   p
                                                
                                                ,
                                                
                                                   n
                                                   q
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             X
                                             
                                                1
                                                
                                                   
                                                      n
                                                      q
                                                   
                                                   ×
                                                   1
                                                
                                             
                                          
                                       
                                       
                                          ≤
                                       
                                       
                                          
                                             
                                                1
                                                
                                                   
                                                      n
                                                      p
                                                   
                                                   ×
                                                   1
                                                
                                             
                                             ,
                                             
                                             
                                                X
                                                T
                                             
                                             
                                                1
                                                
                                                   
                                                      n
                                                      p
                                                   
                                                   ×
                                                   1
                                                
                                             
                                             ≤
                                             
                                                1
                                                
                                                   
                                                      n
                                                      q
                                                   
                                                   ×
                                                   1
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

where the constraints represent the one-to-one matching from Gp
                         to Gq
                        , which makes X an assignment matrix. 1
                           n × 1 denotes an all-ones vector with size n.

For more information on the optimization approach, refer to the work [31]. This approach is significantly faster than traditional methods in which graph nodes are image pixels, because superpixel segmentation guarantees the searching space to be small.

We define the matched sub-graph from the query image and the training image as 
                           
                              
                                 G
                                 M
                                 p
                              
                              =
                              
                                 (
                                 
                                    V
                                    M
                                    p
                                 
                                 ,
                                 
                                    E
                                    M
                                    p
                                 
                                 )
                              
                           
                         and 
                           
                              
                                 G
                                 M
                                 q
                              
                              =
                              
                                 (
                                 
                                    V
                                    M
                                    q
                                 
                                 ,
                                 
                                    E
                                    M
                                    q
                                 
                                 )
                              
                              ,
                           
                         respectively. Moreover, a node correspondence set is also established as 
                           
                              {
                              
                                 (
                                 
                                    V
                                    i
                                    p
                                 
                                 ,
                                 
                                    V
                                    a
                                    q
                                 
                                 )
                              
                              }
                           
                        . Once the correspondence between the query image and the training image is computed, it can be supposed that the adjacency or consistency relationship among superpixels is inherited from the graph of the training image. This inheritance can be partial, which means that only a part or a region of the two images holds the similarity. In other words, we find a common subgraph from the training image and the query image, and this subgraph represents the similar image structure from the two scenes. This relationship will guide the label inference in the next step.

Before the label inference, the data and smoothness terms in the MRF formula should be defined. Similar to the process outlined in the paper [16], given all labeled training sample superpixels, a classifier is trained by using a randomized decision forest. The input of the training stage includes the stacked descriptors and the corresponding labels. Unlike the method outlined in the paper [16], the stacked features are used without normalization, because each single feature in the stacked feature is a type of normalized histogram, that has already been uniformly scaled. After training, the classifier takes features of the test superpixel as input, and outputs a probability distribution vector Pi
                         which represents the probabilities of the test sample i belonging to each category. This distribution is considered the data term. The same definition as in [16] is used to determine the smoothness term. Potentials of energy function are thus obtained.

To utilize the obtained regions that match the training images, the structures of the adjacency graphs of the test images have to be modified. For a set of node pairs with correspondence 
                           
                              {
                              
                                 (
                                 
                                    V
                                    i
                                    p
                                 
                                 ,
                                 
                                    V
                                    a
                                    q
                                 
                                 )
                              
                              }
                              ,
                           
                         where ni
                         ∈ Vp
                         and na
                         ∈ Vq
                        , we adjust the graphs as follows. If 
                           
                              
                                 E
                                 
                                    a
                                    b
                                 
                                 q
                              
                              ∈
                              
                                 E
                                 M
                                 q
                              
                           
                         is an edge in the matched sub-graph corresponding to the training image, we add a new edge 
                           
                              E
                              
                                 i
                                 j
                              
                              p
                           
                         into the graph Gp
                         where (Vj, Vb
                        ) is in the correspondence set and 
                           
                              
                                 V
                                 
                                    a
                                    b
                                 
                                 q
                              
                              ∈
                              
                                 E
                                 q
                              
                           
                        . This means that the adjacency relationship between superpixels on the query image is enhance or established by identifying the matched superpixels and adjacency in the test image. This procedure is implemented over all candidate training images, and the enhanced graphs are defined as 
                           
                              
                                 G
                                 
                                    g
                                    u
                                    i
                                    d
                                    e
                                    d
                                 
                                 p
                              
                              =
                              <
                              
                                 N
                                 
                                    g
                                    u
                                    i
                                    d
                                    e
                                    d
                                 
                                 p
                              
                              ,
                              
                                 E
                                 
                                    g
                                    u
                                    i
                                    d
                                    e
                                    d
                                 
                                 p
                              
                              >
                           
                        .

A guided MRF is introduced to handle the refined labeling problem. In this scheme, both the label space and the adjacency relationship are constrained using the derived adjacency relationship and the distribution from the point clouds.

In general, the aim is the minimization of the following energy function:

                           
                              (4)
                              
                                 
                                    
                                       
                                          
                                             E
                                             (
                                             L
                                             )
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   ∈
                                                   
                                                      N
                                                      p
                                                   
                                                   ,
                                                   
                                                      L
                                                      i
                                                   
                                                   ∈
                                                   
                                                      D
                                                      (
                                                      i
                                                      )
                                                   
                                                
                                             
                                             
                                                ψ
                                                i
                                             
                                             
                                                (
                                                
                                                   L
                                                   i
                                                
                                                )
                                             
                                             +
                                             
                                                ρ
                                                1
                                             
                                             
                                                ∑
                                                
                                                   
                                                      (
                                                      i
                                                      ,
                                                      j
                                                      )
                                                   
                                                   ∈
                                                   
                                                      E
                                                      p
                                                   
                                                
                                             
                                             
                                                ψ
                                                
                                                   i
                                                   ,
                                                   j
                                                
                                             
                                             
                                                (
                                                
                                                   L
                                                   i
                                                
                                                ,
                                                
                                                   L
                                                   j
                                                
                                                )
                                             
                                             +
                                             
                                                ρ
                                                2
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                       
                                          
                                             ×
                                             
                                                ∑
                                                
                                                   
                                                      (
                                                      f
                                                      ,
                                                      g
                                                      )
                                                   
                                                   ∈
                                                   
                                                      E
                                                      H
                                                   
                                                
                                             
                                             
                                                ψ
                                                
                                                   f
                                                   ,
                                                   g
                                                
                                             
                                             
                                                (
                                                
                                                   L
                                                   f
                                                
                                                ,
                                                
                                                   L
                                                   g
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

where D(i) represents the category to which superpixel i belongs. Ep
                         is the edge collection in original graph and EH
                         is the edge collection in the inherited graph. ψi
                         is calculated by taking the minus log form of the output of the random decision forest. In other words, 
                           
                              
                                 ψ
                                 i
                              
                              
                                 (
                                 
                                    L
                                    i
                                 
                                 )
                              
                              =
                              −
                              l
                              o
                              g
                              
                                 P
                                 i
                              
                              
                                 (
                                 
                                    L
                                    i
                                 
                                 )
                              
                              ,
                           
                         and Pi
                         is the output of random forest on superpixel i. We employ same way to construct ψ
                        
                           i, j
                         as [16]. The first term in this formula represents the data fidelity according to the superpixel appearance, while the possible distribution of the labels on a specified superpixel is constrained under the cues from the point clouds and the subgraph inherited from training images. In Eq. (4), the first two terms are traditional in MRF inference, which measure data fidelity and neighboring smoothness, respectively. The third term, which is derived from the graph matching, is added based on the following fact. A superpixel does not appear in an image as a single element, instead a group of superpixels formulate an “object” in a way of cooccurrence. The objects then form an natural image in a semantic manner. In street view, although there exist deformation or displacement, similar objects frequently appear across scenes. Graph matching is a natural choice to find the object correspondence against deformation or displacement. In general, graph matching is utilized to find partial correspondence from query images and training images. Then, the matching from a region in the query image to a region in the training image gives more belief to which category the region belongs, since the cooccurrence is a reliable evidence. In this case, we should enhance the interaction between the nodes that belong to the same category, which is reflected by adding the third term.

An obstacle that lies apart from the building facade can be easily found from the point clouds, therefore the corresponding possible distribution can only be tree, car, pedestrian, vehicle, sign. In the MRF inference we assign the probabilities of these label states on a specific node summing to 1 subject to the output of randomized decision forest, and the rest probabilities of other states to be 0. Normally, if the number of the category is n, and a feasible distribution from point cloud is defined as 
                           
                              
                                 D
                                 i
                              
                              =
                              
                                 
                                    {
                                    0
                                    ,
                                    1
                                    }
                                 
                                 n
                              
                              ,
                           
                         then the probability distribution can be adjusted using:

                           
                              (5)
                              
                                 
                                    
                                       P
                                       i
                                       *
                                    
                                    =
                                    
                                       1
                                       Z
                                    
                                    
                                       P
                                       i
                                    
                                    •
                                    
                                       D
                                       i
                                    
                                 
                              
                           
                        
                     

where Pi
                         is the vector output using the trained classifier, • refers to the dot product of two vectors and Z is a parameter that guarantees 
                           
                              P
                              i
                              *
                           
                         to be a probability distribution. After adjusting the distribution, 
                           
                              P
                              i
                              *
                           
                         is employed as the new data term. A simple example is shown in Fig. 2
                         (a), where it is assumed that there are only three categories.

The second term is the smoothness term, which is defined on the edges of the graph. The third term, which is newly employed, measures the smoothness of the relationship that the query image inherits from the training image. This term adds a strong constraint, yielding an extra relationship on the nodes that ensures a similar inference on a similar pair of scenes. Alternatively, it can be said that a new weighted edge is added to the graph if the two end nodes inherit a relationship from the training images. Normally, an new edge is added if and only if:

                           
                              1.
                              This edge does not exist in the current graph;

After graph matching, a corresponding edge exists in the reference image;

The nodes of the reference edge belong to the same category. A simple example is depicted in Fig. 2 (b).

Formula (4) can be integrated as one in the pairwise MRF framework. To realize this integration, we simply add some inherited edges onto the graph and provide every node with a different distribution. The energy function is then transformed into:

                           
                              (6)
                              
                                 
                                    E
                                    
                                       (
                                       L
                                       )
                                    
                                    =
                                    
                                       ∑
                                       
                                          i
                                          ∈
                                          
                                             N
                                             
                                                g
                                                u
                                                i
                                                d
                                                e
                                                d
                                             
                                             p
                                          
                                          ,
                                          
                                             L
                                             i
                                          
                                          ∈
                                          
                                             D
                                             (
                                             i
                                             )
                                          
                                       
                                    
                                    
                                       ψ
                                       i
                                    
                                    
                                       (
                                       
                                          L
                                          i
                                       
                                       )
                                    
                                    +
                                    
                                       ∑
                                       
                                          
                                             (
                                             i
                                             ,
                                             j
                                             )
                                          
                                          ∈
                                          
                                             E
                                             
                                                g
                                                u
                                                i
                                                d
                                                e
                                                d
                                             
                                             p
                                          
                                       
                                    
                                    
                                       ψ
                                       
                                          i
                                          ,
                                          j
                                       
                                    
                                    
                                       (
                                       
                                          L
                                          i
                                       
                                       ,
                                       
                                          L
                                          j
                                       
                                       )
                                    
                                 
                              
                           
                        
                     

The optimization of the energy on this newly constructed graph can be achieved using Max-product Belief Propagation [33]. The message at time t from node i to node j is defined as:

                           
                              (7)
                              
                                 
                                    
                                       m
                                       
                                          i
                                          →
                                          j
                                       
                                       t
                                    
                                    =
                                    
                                       min
                                       
                                          L
                                          i
                                       
                                    
                                    
                                       (
                                       
                                          ψ
                                          i
                                          *
                                       
                                       
                                          (
                                          
                                             L
                                             i
                                          
                                          )
                                       
                                       +
                                       
                                          φ
                                          
                                             i
                                             ,
                                             j
                                          
                                       
                                       
                                          (
                                          
                                             L
                                             i
                                          
                                          ,
                                          
                                             L
                                             j
                                          
                                          )
                                       
                                       +
                                       
                                          ∑
                                          
                                             
                                                (
                                                k
                                                ,
                                                i
                                                )
                                             
                                             ∈
                                             
                                                
                                                   N
                                                   
                                                      g
                                                      u
                                                      i
                                                      d
                                                      e
                                                      d
                                                   
                                                
                                                
                                                   (
                                                   i
                                                   )
                                                
                                             
                                             j
                                          
                                       
                                       
                                          m
                                          
                                             k
                                             →
                                             i
                                          
                                          
                                             t
                                             −
                                             1
                                          
                                       
                                       )
                                    
                                 
                              
                           
                        
                     

Under the constraints from the point clouds, the value of 
                           
                              ψ
                              i
                              *
                           
                         is obtained by adjusting ψi
                        . After iterations, the image can be classified by taking the label with the max belief value. The street view, combined with range data, can be parsed effectively under this process.

@&#EXPERIMENTS@&#

To test the performance of the proposed parsing method, we conducted experiments on the datasets from New York, Paris, Rome and San Francisco. All the data is in the format as in the example shown in Fig. 1. In the experiments, the inference results, using graph matching with both images and point clouds, were compared with the results of using only images.

There is no public database containing well-aligned images and point clouds. In our comparison experiment, we test two sets of data.

The first database, including images and well-aligned point clouds of the street views of the aforementioned four cities (New York, Paris, Rome and San Francisco), was collected by Google. In this database, the images were taken with eight vehicle-mounted perspective rolling shutter cameras, each of which had its own camera intrinsics and distortion parameters. There are also three LiDAR scanners mounted on the front, left and right sides of the vehicle. The images from the same camera were taken at specific time interval and range data were captured. The coordinate system was UTM (Universal Transverse Mercator). The number of images in the datasets for the four cities were in tens of thousands. However, there were many overlaps since the time intervals were short. Parameters ρ
                        1 and ρ
                        2 in formula (4) and λ in formula (2) were not defined. They were set to 0.9, 0.3 and 1.3, respectively, as determined from the experiments, in order to achieve the best performance. The candidate size is set to 30.

The second database is LM+SUN[15], which is adopted to test the parsing performance when only images are available, though our algorithm is designed to deal with images and point clouds. This database contains 45,676 images, while 24,494 images are outdoor and the rest images are indoor. We only test our algorithm using the outdoor images, which are split into 23,994 training images and 500 testing images. ρ
                        1, ρ
                        2 and λ are set to 0.9, 0.2 and 1.6, respectively. Since the database is large, the size of the candidate set is chosen to be 400.

To present a quantitative evaluation of our method, we employ three criteria from [16].

                           
                              (8)
                              
                                 
                                    
                                       
                                          Precision
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                |
                                                
                                                   G
                                                   T
                                                   ∩
                                                   
                                                      D
                                                      R
                                                   
                                                
                                                |
                                             
                                             
                                                |
                                                
                                                   D
                                                   R
                                                
                                                |
                                             
                                          
                                       
                                    
                                    
                                       
                                          Recall
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                |
                                                
                                                   G
                                                   T
                                                   ∩
                                                   
                                                      D
                                                      R
                                                   
                                                
                                                |
                                             
                                             
                                                |
                                                
                                                   G
                                                   T
                                                
                                                |
                                             
                                          
                                       
                                    
                                    
                                       
                                          F-measure
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                2
                                                ×
                                                Recall
                                                ×
                                                Precision
                                             
                                             
                                                Recall
                                                +
                                                Precision
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

where 
                        GT represents the set of pixels that are classified to a specific category by the proposed approach, and DR represents the set of pixels that is a manual label to a specific category (i.e., ground truth). F-measure is the weighted harmonic mean of GT and DR, which is used to quantify the overall performance of the parsing scheme.

The performance on both images and range data was evaluated using the proposed method. Fig. 3 shows several parsing examples, where the first row contains the sample images from the dataset, the second and third rows correspond to the ground truth labeling and the parsing result, respectively. In this test, four sub-tests were conducted on each city. There are many images and overlaps in each sub-dataset, therefore for each city, we randomly choose 300 scenes as the training data and another 200 scenes as test samples, in order to avoid the overlaps and bias introduced by manual selection. The experiments for each city were conducted separately rather than combining all scenes together since the building and street styles can differ between cities. However, the building and street styles are similar in most cities, resulting in ease of identification in a specific city for both natives or travelers. This interesting phenomenon was discussed in [39].

A series of tests were performed with different sizes of the candidate image set. Experimental results are demonstrated in upper left of Fig. 4
                        . As shown, if the size of the candidate set was properly chosen, scenes can be effectively parsed using the proposed approach under complex environmental distortions. We found that 30 was a good choice in most cases. We also present the parsing performance with a candidate set size 30 on each category in Table 2
                        . The proposed algorithm worked well on structures with large areas such as sky, buildings and roads, where the superpixel feature is easier to discriminate and the corresponding portion of the point cloud is easier to classify. However, errors occurred when parsing windows and doors, as shown in the first and third columns of Fig. 3. Several factors led to these errors. First, when manually labeling the images, we did not label each window and door on the building facades since there were many items in these two categories; instead, we just label several apparent ones. This way of labeling made it difficult to train a good classifier for windows and doors. Second, because the point clouds collected are sparse, it’s relatively difficult to locate the windows and doors in the point clouds. Therefore, the corresponding probability distribution of the categories could not be effectively constrained. Third, it was also difficult to identify doors in point clouds, because they can be made of different materials, such as glass, metal, wood or plastic. Moreover, obstacles behind the glass windows or doors can affect the parsing results. Fortunately, most of the regions with other labels can be parsed with a high precision.

In addition to the difficulty in recognizing windows and doors, the proposed method also had trouble in finding small or tiny objects. This is because some tiny structures are smaller than the sizes of superpixels. In this case, the tiny objects either are amplified in the parsing results, or simply disappeared. This problem can be decreased by choosing smaller size of superpixels, but leading to lower computational efficiency.

A comparison test was conducted for the performance of the proposed approach and the traditional MRF inference without graph matching guidance, which uses images alone (image only). This is to test the tradition Classification+MRF framework on our dataset. Further, we test the performance obtained from point clouds and images, but without graph matching (without GM). We anticipate that this test can evaluate how much improvement the graph matching guidance procedure can provide.

We also compared the performance with the approaches [1], [7] and [16] using our datasets. Liu et al., [1] is chosen because it is a typical method using information from pixels, while [7] uses the clues from superpixels. Zhang et al. [16] computes the depth map first, then uses the 3D information to help the inference. The dataset set-up was the same as that in the Section 3.2. The upper right, lower left and lower right of Fig. 4 show the comparative results of the overall performance. Some typical results are shown in Fig. 5
                        .

Compared to the traditional MRF inference, the parsing performance was remarkably enhanced when information from both images and range data were used. The existence of the range data significantly reduced the ambiguity in inference using image alone. Besides, according to our experiment, the graph matching procedure can indeed improve the parsing performance by introducing more stringent constraints on MRF edges. In most cases, the method integrated with range data increased the accuracy in terms of F-measure, Precision and Recall by 20–30% over the basic MRF inference, and surpassing the method without GM by 15–20%. The approach proposed in [1] did not perform well on our datasets, reaching accuracies of only around 70–80% in the four datasets, due to the lack of information from the point clouds and the fact that the method in [1] required a very large set of training samples for parsing. In the experiments conducted in [1], thousands of images were selected and labeled as the training set to ensure that similar scene as query image exists. Although the scenes in our database are similar, the displacement of similar objects across scenes is very large, this is the case that [1] cannot handle. However, by handling the distortions via graph matching, our method required only hundreds of images for training, yet achieved better performance than approaches using images alone. As shown in Fig. 5, the method in [1] is able to find large structures such as sky, buildings and roads. However, small ones such as trees and pedestrians cannot be effectively interpreted since the dataset is small and sufficiently similar scenes cannot be found. Specifically, windows and doors can hardly be identified using the method from [1].

The approach in [7] introduced superpixels and thus performs better than that of [1] with our datasets. The results show that accuracies of over 80% on New York and Paris and approximately 85% on the Rome and San Francisco were obtained. However, our method still outperform the approaches in [7] by 5–10%. While [7] employed many local and global features (over 1000 dimensions on local feature and over 5000 dimensions of global features), the proposed method needed only 233 and 2472 dimensions, respectively. However, Fig. 5 demonstrates that the method from [7] still cannot handle the areas of windows and doors since a perfect classifier cannot be trained using our datasets.

Since the method proposed in [16] tried to construct the depth maps for each scene, we follow this framework by providing each scene with several images taken sequentially using the same camera. However, because the time interval between neighboring time stamps was not sufficiently dense, it was difficult to obtain the precise 3D information. As a result, the estimated depth maps merely provided limited information, or even wrong 3D information. Once the precise 3D information is unavailable, the method proposed in [16] will degenerate into a normal MRF inference scheme. In general, [16] reached accuracy between 75–83%. In Fig. 5, the parsing performance of the method from [16] heavily relies on the estimation of the depth maps. As in the third row and the fifth column of Fig. 5, the method from [16] has a poor performance when objects are too far from the view point. In this case, the estimated depth maps are almost impossible to provide reliable 3D information. This result demonstrate that using range data directly is more reliable than using 3D information estimated from structure from motion algorithms. It is supported by the experiments that by introducing graph matching on both images and range data, our method has more reliable parsing performance using less features from smaller training sets.

In this part, we follow the selection of the algorithms as in previous section. The scenes in this database is much more complex than in the Google database, since it not only contains the street view images, but also a large variety of outdoor scenes. The number of categories is much larger and compositions of the image components are various.

The parsing performance in terms of Precision, Recall and F-measure is shown in Fig. 7
                        
                        . Some typical parsing results are shown in Fig. 6. The algorithm in [7] holds the best overall performance on Precision, reaching over 81.6%. The overall Precision of the proposed framework is lower than [7] with 76.6%. However, our method has a better Recall performance than [7]. This is due to the selection of different superpixel segmentation strategies. Since [7] adopts a coarser superpixel segmentation method, it cannot effectively separate small objects or connected regions between two objects from two different categories. This leads to some misclassification on such regions. Instead, by utilizing a finer superpixel segmentation method, our algorithm can distinguish the boundary regions to some extent, though sacrificing some Precision performance. Liu et al. [1] has a similar Precision as the proposed method, since it parses the scenes by matching images at pixel level. Zhang et al. [16] degenerates to traditional Classification+Smoothing framework when depth information is unavailable. Note that in the second row of Fig. 6, no method achieves a good parsing performance. For [1], it’s difficult to find a similar scene as the query image, so the SIFT-flow procedure will try to collect the label information from some unreliable training sample. Since all the other methods are based on training and classification, the performance can heavily rely on the precision of the classifier. Unfortunately, this query image does not follow the category distribution in the training sample space, as one can observe that the overall color of the image tends to be red. In this case, a graph matching procedure can correct the classification to some extent by matching images partially.

@&#CONCLUSION@&#

In this paper, we present a novel approach for street scene parsing based on fusion of images and range data and graph matching. To the best of our knowledge, this researc\ h is the first study that has employed graph matching to generate a guidance in scene parsing problem, which helps find similar higher level visual features across scenes. Our method does not need a large training set and can significantly enhance the parsing performance by integrating 3D information from the point cloud data.

The next step in our research will be the realization of the recognition of windows and doors on the building facade by integrating a building structure interpretation scheme into the framework. Another area for future research will be the collection of cues from different scenes and development of a learning method to combine them together, in order to overcome the challenge that only part of two scenes can be accomplished with graph matching. More weight information from several training scenes will be utilized through a learning strategy.

@&#ACKNOWLEDGMENTS@&#

This research is supported by Google Faculty Research Award program 2014–2015.

@&#REFERENCES@&#

