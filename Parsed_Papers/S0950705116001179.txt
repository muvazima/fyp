@&#MAIN-TITLE@&#Hierarchical anonymization algorithms against background knowledge attack in data releasing

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We define a privacy model based on k-anonymity and one of its strong refinements to prevent the background knowledge attack.


                        
                        
                           
                           We propose two hierarchical anonymization algorithm to satisfy our privacy model.


                        
                        
                           
                           Our algorithms outperform the state-of the art anonymization algorithm in terms of utility and privacy.


                        
                        
                           
                           We extend an information loss measure to capture data inaccuracies caused by not-fitted records in any equivalence class.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Privacy preservation

Tabular data

Hierarchical anonymization algorithm

Background knowledge

Information loss metric

@&#ABSTRACT@&#


               
               
                  Preserving privacy in the presence of adversary’s background knowledge is very important in data publishing. The k-anonymity model, while protecting identity, does not protect against attribute disclosure. One of strong refinements of k-anonymity, β-likeness, does not protect against identity disclosure. Neither model protects against attacks featured by background knowledge. This research proposes two approaches for generating k-anonymous β-likeness datasets that protect against identity and attribute disclosures and prevent attacks featured by any data correlations between QIs and sensitive attribute values as the adversary’s background knowledge. In particular, two hierarchical anonymization algorithms are proposed. Both algorithms apply agglomerative clustering techniques in their first stage in order to generate clusters of records whose probability distributions extracted by background knowledge are similar. In the next phase, k-anonymity and β-likeness are enforced in order to prevent identity and attribute disclosures. Our extensive experiments demonstrate that the proposed algorithms outperform other state-of-the-art anonymization algorithms in terms of privacy and data utility where the number of unpublished records in our algorithms is less than that of the others. As well-known information loss metrics fail to measure precisely the imposed data inaccuracies stemmed from the removal of records that cannot be published in any equivalence class. This research also introduces an extension into the Global Certainty Penalty metric that considers unpublished records.
               
            

@&#INTRODUCTION@&#

Advances in the Internet and data processing technologies have accelerated data collection and dissemination. As collected data may contain private information, a breach of privacy is possible if it is disclosed-together with identifiers-to unauthorized parties. Removal of attributes that are identifiers, such as name and social security number, is not sufficient to protect privacy, when quasi identifiers
                        1
                     
                     
                        1
                        Quasi identifiers are those attributes that individually are not an identifier, but when combined together, they become identifiers.
                      (QI) exist. Hence, proposing promising approaches for privacy preservation has gained significant attention in the context of data collection and dissemination.

Anonymization is an approach to preserve individuals’ privacy by removing their identifiers from the data that is going to be published, while maintaining as much of original information as possible. Each anonymization framework includes a privacy model and an anonymization algorithm. Privacy models can be divided into syntactic and semantic models. Syntactic privacy models partition data into a set of groups (called equivalence classes) such that all records within each equivalence class are indistinguishable from one another from QI point of view. In the k-anonymity model, as the first syntactic privacy model, each equivalence class contains at least k records [1,2]. This model prevents identity disclosure
                        2
                     
                     
                        2
                        The individual, to whom a record is associated, cannot successfully be re-identified with probability more than 
                              
                                 1
                                 k
                              
                           .
                     , but it does not preserve the privacy against attribute disclosure
                        3
                     
                     
                        3
                        Attribute disclosure occurs when sensitive information about an individual is revealed.
                     . To address this issue, other variants of k-anonymity have been proposed [3–5]. Semantic privacy models add some noise to data in order to preserve privacy. The differential privacy model is a semantic privacy model in which it is guaranteed that deletion and addition of any individual’s record does not significantly affect the result of data analysis [6].

Each privacy model provides a defense against a particular adversary model. A common assumption is that the adversary has two pieces of information: (I) whether or not his/her targets exist in the microdata table and (II) the QI values of his/her targets [1–4]. None of the models mentioned above- including the differential privacy model- can preserve privacy if the adversary has additional information (called background knowledge) [7]. Hence, researchers have proposed enhanced models that assume the adversary has some background knowledge [8–13]. A background knowledge is any known fact that by itself is not a privacy disclosure, but the adversary combines it with other information to make more precise inference on target’s sensitive information. This is called a background knowledge attack. Examples of background knowledge in a particular medical dataset context are “a male breast cancer is rare”, “the prevalence of chronic bronchitis is higher among the 65+ age group compared to other groups; and, across all age groups, females have higher rates than males for both black and white races”, etc. [14].

In this work, we develop a syntactic-based anonymization framework in which we assume the adversary has background knowledge about the correlations among dataset attributes. In a syntactic privacy model, when an equivalence class is published, the adversary can estimate the probabilities of possible associations of sensitive values to his/her target (i.e. record respondent) without exploiting any background knowledge
                        4
                     
                     
                        4
                        Recall that adversaries know the QI values of their targets.
                     . When different sensitive attribute values exist in an equivalence class, the probability of associating a record respondent to sensitive values in the equivalence class is the same. By exploiting the background knowledge, the adversary may be able to discriminate one association from the others, resulting in privacy breaches. Modeling the background knowledge is an open problem in data anonymization [8]. We model the adversary’s background knowledge as a probability distribution associating the sensitive values to a record respondent based on QI values, called background knowledge distribution. The goal in our privacy model is to maximize uncertainties in identifying record respondents and their respective values for sensitive attributes in a given equivalence class. In the presence of adversary’s background knowledge, we attempt to create equivalence classes such that record respondents have similar background knowledge distributions in each class in order to achieve our goal. Therefore, when adversaries examine different associations of sensitive values (within each equivalence class) to their targets, they will not be able to discriminate any association with a high degree of certainty. The constraint of similar background knowledge distributions cannot prevent identity disclosure or attribute disclosure. Therefore, we also apply k-anonymity [1] and β-likeness [5]. To remain the anonymized data useful, a high similarity among QI values in each equivalence class is also required.

Hence, we propose to create equivalence classes with the following privacy requirements: (1) The background knowledge distributions within any equivalence class should be similar in order to prevent the so-called background knowledge attack, (2) k-anonymity: the size of each class is at least k, (3) β-likeness: the maximum relative difference in the frequency of sensitive values within any equivalence class and that of the overall microdata table does not exceed a given threshold β.

We present two syntactic anonymization algorithms based on the value generalization approach. We suggest a hierarchical procedure to satisfy our privacy requirements. First, we apply agglomerative clustering to prevent the background knowledge attack. We apply the clustering algorithm to generate clusters in which the difference of background knowledge distributions between each pair of records in the cluster is below a certain threshold. Then, each cluster is partitioned into a number of equivalence classes. We propose two algorithms to produce the equivalence classes: k-anonymity-primacy and β-likeness-primacy. The former prioritizes the QI attributes and generates equivalence classes in a β-likeness aware manner. For this purpose, we propose a clustering-based algorithm to select homogeneous records in terms of QI values, and then check whether β-likeness is satisfied. The latter focuses on the sensitive attribute values. It generates large equivalence classes in which β-likeness is satisfied. Then, large equivalence classes are split in order to satisfy k-anonymity.

A work close to ours is found in Riboni et al. [8]. They have proposed a privacy model based on adversary’s background knowledge and t-closeness [4]. Their anonymization algorithm applies Hilbert index transformation to create an ordered list of record respondents based on similarity of QI-values. We enforce a stronger model than t-closeness and propose two new anonymization algorithms to satisfy our privacy requirements. Sori-Comas et al. [15] also proposed two clustering-based anonymization algorithms attaining k-anonymity and t-closeness which is suitable to anonymize numerical values. They do not consider the adversary’s background knowledge.

We verify the effectiveness of our anonymization algorithms by running extensive experiments on two datasets: Adult dataset [16] and BKseq dataset [8]. We study the performance of our anonymization algorithms based on different parameters of our privacy model. The experimental results show that k-anonymity-primacy generates the anonymized microdata with low information loss while β-likeness-primacy incurs low privacy loss. The k-anonymity-primacy algorithm also generates more balanced equivalence classes compared to β-likeness-primacy. We further compare the performance of our proposed algorithms with the state of the art anonymization algorithms like Hilbert index-based algorithm [8]. The performance of our algorithms are better than Hilbert index-based algorithm in terms of both data utility and privacy.

Furthermore, we extend an information loss measure to capture data inaccuracies caused by generalization. In any anonymization algorithm, it is possible not to fit some records in any equivalence class. To protect the privacy of other record respondents, a simple solution is to remove them from the published data. We introduce an extension to the Global Certainty Penalty (GCP) metric [17] to consider this kind of information loss too, and we name it Removed Global Certainty Penalty (RGCP). The RGCP metric charges a penalty for each not-fit record. The penalty of each record is proportional to the range of QI values in the nearest equivalence class. We evaluate our algorithms using both GCP and RGCP. When a large number of records are removed by the algorithm, the advantage of RGCP over GCP is better seen.


                     Contributions. In summary, our contributions are as follows:

                        
                           - We propose two syntactic anonymization algorithms which simultaneously satisfy two privacy models (k-anonymity and β-likeness) against adversaries who have background knowledge on correlations of attributes. We conduct extensive experiments on different aspects of the algorithms, namely data utility, privacy, size of equivalence classes, and the run time. Then we compare our algorithms with microaggregation approaches. We also perform an experimental comparison between the closest work, Hilbert index-based algorithm [8], and the proposed algorithms. We demonstrate that the proposed algorithms outperform Hilbert index-based algorithm in terms of data utility and privacy.

- We extend GCP to measure information loss of equivalence classes when the generalization operation is performed. Our metric, called RGCP, considers unpublished records too.


                     Paper organization. In Section 2, we review research related to anonymization algorithms and information loss metrics. In Section 3, we define the problem and propose two solutions. In Section 4, we introduce a new information loss measure. Experiments and results are presented in Section 5. In Section 6, we discuss more the results and contributions. Finally, we conclude the paper in Section 7 followed by the future work discussion.

@&#RELATED WORK@&#

This research is built up on the concepts of syntactic-based anonymization algorithms, which consider adversaries with background knowledge and information loss measures. In this section, we study the literature of these domains from various aspects.

The k-anonymity model [1,2] and its variants such as t-closeness [4,18] are state of the art syntactic privacy models for tabular microdata. k-anonymity guarantees to prevent identity disclosure but not attribute disclosure. To address this problem, t-closeness guarantees that the difference between frequency of sensitive values within any equivalence class as well as in the overall microdata table does not exceed a given threshold t. The Earth Mover’s distance is a way to measure the cumulative difference between overall sensitive value distribution and that in an equivalence class [4,18]. Since this model does not really impose a size constraint on an equivalence class, it may fail to protect against an identity disclosure. For an extreme case, where all sensitive values in a microdata table are the same, it seems that an equivalence class will have size 1 or a very small size that results in an identity disclosure.

Cao and Karras [5] have identified one drawback of t-closeness. They have illustrated that the cumulative distance measure applied to t-closeness is not sufficient to prevent the sensitive attribute disclosure. In fact, this model does not take into account the less-frequent sensitive values, which are more vulnerable to privacy breach. They proposed the β-likeness model that uses a relative difference measure. Assume 
                           
                              p
                              
                                 s
                                 i
                              
                           
                         is the frequency of the sensitive value si
                         in the overall microdata. Also, assume 
                           
                              q
                              
                                 s
                                 i
                              
                           
                         is the frequency of si
                         within any equivalence class. The β-likeness model requires that 
                           
                              
                                 
                                    
                                       q
                                       
                                          s
                                          i
                                       
                                    
                                    −
                                    
                                       p
                                       
                                          s
                                          i
                                       
                                    
                                 
                                 
                                    p
                                    
                                       s
                                       i
                                    
                                 
                              
                              ≤
                              min
                              
                                 {
                                 β
                                 ,
                                 −
                                 ln
                                 
                                    p
                                    
                                       s
                                       i
                                    
                                 
                                 }
                              
                           
                        . The inequality implies that 
                           
                              
                                 q
                                 
                                    s
                                    i
                                 
                              
                              ≤
                              f
                              
                                 (
                                 
                                    p
                                    
                                       s
                                       i
                                    
                                 
                                 )
                              
                              =
                              
                                 p
                                 
                                    s
                                    i
                                 
                              
                              ×
                              
                                 (
                                 1
                                 +
                                 m
                                 i
                                 n
                                 
                                    {
                                    β
                                    ,
                                    −
                                    ln
                                    
                                       p
                                       
                                          s
                                          i
                                       
                                    
                                    }
                                 
                                 )
                              
                           
                        . This model, similar to t-closeness, does not prevent identity disclosure. Few works proposed a combination of privacy models to prevent identity and attribute disclosures [8,15].

Several anonymization algorithms, such as Mondrian [19], top-down specialization [20], bottom-up generalization [21], Hilbert index-based [8,22], and clustering-based algorithms [15,23,24] have been proposed. In order to preserve privacy, the algorithms use approaches such as generalization [19–22,25], anatomy [26], and microaggregation [15,23,27]. Generalization replaces the QI values with less specific values. Anatomy partitions the records and releases the QI and sensitive attribute values in two different tables. Microaggregation partitions the records into small clusters and then replaces each record with the centroid of its corresponding cluster. Generalization is suitable for categorical data while microaggregation is appropriate for dealing with numerical data [27]. Due to publishing real values of attributes, anatomy may have higher probability of privacy breach compared to generalization. One limitation of microaggregation, compared to generalization; is that the published records are synthetic and they do not correspond to real-world values existing in the original microdata. Therefore, its published data are meaningless whereas the published data in generalization is semantically consistent with the raw data. Our work focuses on the generalization approach due to its advantages over anatomy and microaggregation.

It is unknown how to optimally combine generalization and suppression to achieve lowest information loss in algorithms, such as Mondarian [19] and bottom-up generalization [21]. Clustering-based generalization algorithms partition records into some equivalence classes, and generalize all records in each equivalence class [24,28,29]. It should be mentioned that microaggregation algorithms could also be considered clustering-based anonymization algorithms that publish the aggregated values of each equivalence class.

Riboni et al. [8] proposed a privacy model based on k-anonymity and t-closeness and used a Hilbert index transformation to sort the records based on similar QI-values, and greedily chose each k records which satisfy t-closeness. Sori-Comas et al. [15] proposed two microaggregation-based anonymization algorithms attaining k-anonymous t-closeness. They used a clustering algorithm to generate k-record clusters. Thy supposed that sensitive attribute values can be ranked. One of our algorithms uses a similar principle; yet, it considers the adversary’s background knowledge too.

Most privacy models mentioned above fail against an adversary with background knowledge, a requirement that is proposed in more recent studies [8–13]. Different types of adversary knowledge have been considered in the literature. Data correlation is an important type of adversary’s knowledge that makes privacy preservation difficult [8,9,30–32]. Data correlation can be classified into two groups: (1) attribute correlation between two or more attributes, e.g. the ones in [8,9,30,32]; and, (2) row correlation that exists among rows of microdata, e.g. the one in [31]. Our work focuses on reducing the threat exposed by data correlation between QI and sensitive attributes.

Approaches for integrating adversary’s background knowledge are classified into two main categories: (1) approaches based on logic and rules [10–12,32]; and, (2) approaches based on probability tools [8,9,13]. The former proposes a language for expressing a certain amount of knowledge and a methodology for limiting the privacy risk. The latter models the adversary’s background knowledge as the probability distribution of associating sensitive values to an individual given the QI-values.

Li et al. [9] presented a general framework to extract background knowledge using kernel regression techniques and proposed (B, t)-privacy model based on the difference between adversary’s prior and posterior beliefs. They adopted a general anonymization algorithm, Mondrian, to satisfy the privacy model.

There are two categories of information loss measure in the literature: workload-dependent [20,21,33,34] and general purpose [8,9,17,22,35–38]. In the former, the data usage is known at the time of publishing and it can be taken into account during the anonymization to better maintain information. As an example, Iynegar [32] proposed the Classification Metric that measures the classification error between the original data and the anonymized one. In the latter, the data publisher does not know how the anonymized data will be analyzed by the recipient. Hence, the data publisher measures the similarity between the anonymized data and the original one. There are several information loss measures that fall into the latter category, including:

                           
                              •
                              
                                 Average equivalence class size measure (AEC) [19]. This measure computes the average size of equivalence classes.


                                 Discernibility metric (DM) [35]. This approach charges a penalty for every record. The penalty is proportional to size of corresponding equivalence class for each published record. It also charges a penalty, equal to size of whole microdata, for every removed record.


                                 Generalization height (GH) or minimal distortion 
                                 [36]. This approach charges a penalty to each instance of a value that is generalized.


                                 Precision measure (PM) [37]. This measures summarizes the degrees of generalization applied to all attribute values.


                                 Aggregate query answering (AQA) [8,9]. This approach counts the number of records in the anonymized microdata whose QI values belong to certain ranges.


                                 Sum of within-group squared error (SSE) [15]. This is sum of the distance of each record to the centroid of corresponding equivalence class.


                                 Non-uniform entropy measure (NUE) [38]. This computes a distance between the distributions of the original and anonymized attribute values.


                                 Normalized certainty penalty (NCP) [17] and Global certainty penalty (GCP) [17,22]. These approaches define a penalty by the weighted sum of ranges of all QI-values in each equivalence class. Assume that the microdata table T consists of QIs 
                                    
                                       
                                          A
                                          1
                                       
                                       ,
                                       …
                                       ,
                                       
                                          A
                                          d
                                       
                                    
                                  where all attributes are numeric. Record 
                                    
                                       r
                                       =
                                       (
                                       
                                          x
                                          1
                                       
                                       ,
                                       …
                                       ,
                                       
                                          x
                                          d
                                       
                                       )
                                    
                                  is generalized to record 
                                    
                                       
                                          r
                                          *
                                       
                                       =
                                       
                                          (
                                          
                                             [
                                             
                                                y
                                                1
                                             
                                             −
                                             
                                                z
                                                1
                                             
                                             ]
                                          
                                          ,
                                          .
                                          .
                                          .
                                          ,
                                          
                                             [
                                             
                                                y
                                                d
                                             
                                             −
                                             
                                                z
                                                d
                                             
                                             ]
                                          
                                          )
                                       
                                       ,
                                    
                                  thus NCP on Ai
                                  is given by 
                                    
                                       N
                                       C
                                       
                                          P
                                          
                                             A
                                             i
                                          
                                       
                                       
                                          (
                                          t
                                          )
                                       
                                       =
                                       
                                          
                                             
                                                z
                                                i
                                             
                                             −
                                             
                                                y
                                                i
                                             
                                          
                                          
                                             
                                                |
                                             
                                             
                                                A
                                                i
                                             
                                             
                                                |
                                             
                                          
                                       
                                    
                                  where |Ai
                                 | is the range of all records on Ai
                                  in the microdata table T. NCP of record r is defined as the weighted sum of NCP on all QI attributes or 
                                    
                                       N
                                       C
                                       P
                                       
                                          (
                                          r
                                          )
                                       
                                       =
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          n
                                       
                                       
                                          (
                                          
                                             w
                                             i
                                          
                                          ×
                                          N
                                          C
                                          
                                             P
                                             
                                                A
                                                i
                                             
                                          
                                          
                                             (
                                             r
                                             )
                                          
                                          )
                                       
                                    
                                 . GCP is the sum of NCP on all records in the microdata table T such that GCP ≥ 0 where the original data produces 0 indicating no information loss. This measure is maximized where one equivalence class contains all records. Ginita et al. [22] proposed a normalized formulation for the GCP measure.

It should be noted that different information loss measures are required for anonymization algorithms. For instance, SSE is adopted in microaggregation algorithms while other measures are often applied to generalization algorithms.

In this research, we focus on GCP as a commonly used measure that does not have limitations that other measures do. GCP takes into consideration the level of generalization of QI attributes as well as the size of each equivalence class. Yet, it does not consider all data inaccuracies. It is possible that some records will not fit in any equivalence classes. Some generalization algorithms would remove size 1 classes or counterfeit the sensitive value of not-fitting records in order to be inserted into the existing equivalence classes. Since the latter approach produces untrusted data, we focus on the former as a simple and trustworthy approach. Suppose that an anonymization algorithm would decide not to publish any records. The GCP value for this case would be zero while the algorithm has the greatest information loss. In Section 4, we propose an extension into the GCP measure to solve this flaw.

Assume that a data publisher is going to publish the original microdata table 
                           
                              T
                              =
                              {
                              
                                 r
                                 1
                              
                              ,
                              
                                 r
                                 2
                              
                              ,
                              …
                              ,
                              
                                 r
                                 n
                              
                              }
                           
                         where each record ri
                         corresponds to an individual vi
                        , so called record respondent. Each record ri
                         of T contains d QI attributes 
                           
                              
                                 A
                                 1
                              
                              ,
                              
                                 A
                                 2
                              
                              ,
                              …
                              ,
                              
                                 A
                                 d
                              
                           
                         and a single sensitive attribute S. D[Ai
                        ], 1 ≤ i ≤ d denotes the attribute domain of Ai
                         and 
                           
                              D
                              
                                 [
                                 S
                                 ]
                              
                              =
                              
                                 {
                                 
                                    s
                                    1
                                 
                                 ,
                                 
                                    s
                                    2
                                 
                                 ,
                                 …
                                 ,
                                 
                                    s
                                    m
                                 
                                 }
                              
                           
                         denotes the attribute domain of S. Let’s assume that ri
                        [Aj
                        ] denotes Aj
                         value of record ri
                         in T and ri
                        [QI] denotes QI values of record ri
                        .

An anonymization algorithm removes the explicit identifiers from the original records and then groups them to satisfy privacy requirements. Finally QI values of each group are generalized and equivalence classes are produced. The aim of adversary is to reconstruct the link between each record respondent and his/her sensitive attribute value. The adversary has the following knowledge:

                           
                              •
                              The anonymization algorithm is publicly known.

The adversary may know the set of record respondents.

The adversary may have background knowledge on correlations between QI and sensitive values. This is formally defined in Section 3.2.

In our framework, the background knowledge represents the prior probabilities of an individual’s association to sensitive attribute values based on QI values. Assume that 
                           
                              D
                              
                                 [
                                 Q
                                 I
                                 ]
                              
                              =
                              D
                              
                                 [
                                 
                                    A
                                    1
                                 
                                 ]
                              
                              ×
                              D
                              
                                 [
                                 
                                    A
                                    2
                                 
                                 ]
                              
                              ×
                              ⋯
                              ×
                              D
                              
                                 [
                                 
                                    A
                                    d
                                 
                                 ]
                              
                           
                         is the set of all possible QI values and 
                           
                              Σ
                              =
                              {
                              
                                 (
                                 
                                    p
                                    1
                                 
                                 ,
                                 
                                    p
                                    2
                                 
                                 ,
                                 …
                                 ,
                                 
                                    p
                                    m
                                 
                                 )
                              
                              |
                              
                                 ∑
                                 
                                    1
                                    ≤
                                    i
                                    ≤
                                    m
                                 
                              
                              
                                 p
                                 i
                              
                              =
                              1
                              ,
                              0
                              ≤
                              
                                 p
                                 i
                              
                              ≤
                              1
                              }
                           
                         is the set of all possible probability distributions where m is the number of distinct sensitive attribute values. The adversary’s background knowledge is a function 
                           
                              ω
                              :
                              D
                              [
                              Q
                              I
                              ]
                              ⟶
                              Σ
                           
                        . Thus, for an individual v with QI values q ∈ D[QI], the adversary’s background knowledge, ω(q), is modeled as a probability distribution 
                           
                              (
                              
                                 p
                                 1
                              
                              ,
                              
                                 p
                                 2
                              
                              ,
                              …
                              ,
                              
                                 p
                                 m
                              
                              )
                           
                         over D[S] such that pi
                         is the probability of associating si
                         with the individual v given q. According to this definition, a background knowledge distribution
                        
                           
                              (
                              
                                 p
                                 1
                              
                              ,
                              
                                 p
                                 2
                              
                              ,
                              …
                              ,
                              
                                 p
                                 m
                              
                              )
                           
                         is assigned to each record respondent existing in microdata table T. This knowledge is different from the distribution of sensitive values, which shows the frequency of each sensitive value in microdata table.

Precise estimation of the adversary’s background knowledge distribution is a challenging problem. Several authors have studied this problem and proposed effective algorithms for extracting this knowledge based on available data, e.g. the ones in [8,9,12]. We use the same method proposed in [8] to extract background knowledge in our work. Table 1
                         summarizes the notations we use.

In a syntactic privacy model, the goal is to maximize uncertainty of identifying the actual respondent of each record in a given equivalence class. Based on the adversary’s background knowledge defined in Section 3.2, we try to produce equivalence classes in which record respondents have similar background knowledge distributions.

                           Lemma 1
                           Uncertainty in equivalence class


                           Assume that 
                                 
                                    Σ
                                    
                                       E
                                       i
                                    
                                 
                               is the set of background knowledge distributions of record respondents existing in equivalence class Ei
                              . If the probability distributions in 
                                 
                                    Σ
                                    
                                       E
                                       i
                                    
                                 
                               are the same, the uncertainty of identifying the actual respondent of each record in Ei
                               will be maximized.

In a syntactic model, QI values of each equivalence class are the same. Thus, when data is published, the adversary tries to associate a record respondent to an actual sensitive value. The adversary uses background knowledge distributions to find the actual association. Hence, when these background knowledge distributions are the same, the adversary cannot discriminate any of the associations with high degree of certainty.□

Since it is difficult to cluster the records in such a way that all the records in a clusters have the same background knowledge distributions, we apply the restriction that the difference of background knowledge distributions of records in a group does not exceed a given threshold J. As this constraint is not sufficient to prevent identity and attribute disclosure risks, we apply k-anonymity [1] and β-likeness [5]. On the other hand, to maintain utility, we also want QI values in each equivalence class to be similar. Thus, our privacy model is defines as the following constraints.

                           Definition 1
                           Privacy constraints


                           Our privacy constraints include:

                                 
                                    1.
                                    Prevention of so-called background knowledge attack by restricting the difference of background knowledge distributions within each equivalence class up to the threshold J,

Prevention of identity disclosure by satisfying k-anonymity, and

Prevention of attribute disclosure by satisfying β-likeness.

We propose two hierarchical anonymization algorithms to fulfill privacy constraints. Both algorithms meet privacy constraints in two phases:

                           
                              1.
                              The background knowledge-based clustering phase (Lines 1 and 2 in Algorithm 1
                                 ): The original microdata is partitioned into some clusters such that the difference of background knowledge distributions of each pair of records within a cluster is up to J.

The microdata-based clustering phase (Lines 4 and 5 in Algorithm 1): Each cluster produced by background knowledge-based clustering is partitioned into a number of equivalence classes in which k-anonymity and β-likeness are satisfied.

It should be noted that each of the proposed anonymization algorithms runs the same background knowledge-based clustering but different microdata-based clusterings.


                        Background knowledge-based clustering prioritizes the prevention of background knowledge attack. This phase partitions records into groups in which the difference between background knowledge distributions does not exceed a given threshold J. In this phase, we apply hierarchical agglomerative clustering and create a dendrogram of records. Jensen–Shannon divergence [39] of background knowledge distributions is applied as the distance metric. Then, we cut the tree where the node and all of its descendants have distance of less than or equal to J. We use complete linkage method to ensure that the distance of each pair of records within a cluster is less than or equal to J. The background knowledge-based clustering returns a set of clusters 
                           
                              N
                              =
                              {
                              
                                 N
                                 1
                              
                              ,
                              
                                 N
                                 2
                              
                              ,
                              …
                              ,
                              
                                 N
                                 
                                    |
                                    N
                                    |
                                 
                              
                              }
                              ,
                           
                         
                        Ni
                        ⊆T. For each cluster Ni
                         satisfying |Ni
                        | ≥ k, we run the microdata-based clustering. The clusters violating |Ni
                        | ≥ k, are not published.

In the microdata-based clustering phase, each cluster Ni
                         is partitioned to ensure k-anonymity and β-likeness. It should be noted that if the maximum relative difference between the frequency of sensitive attribute values within any equivalence class from that in the microdata T is less than β, β-likeness is satisfied. We propose two different algorithms to achieve k-anonymity and β-likeness: k-anonymity-primacy and β-likeness-primacy. The order of satisfying k-anonymity and β-likeness results in different algorithms.


                        k-anonymity-primacy concentrates on k-anonymity and makes the equivalence classes in a β-likeness aware manner. This algorithm partitions homogeneous records
                           5
                        
                        
                           5
                           In terms of QI values.
                         into the clusters with the minimum size k and then check whether each cluster satisfies β-likeness. If not, the clusters in which β-likeness is violated, are merged as much as needed to satisfy this constraint.

The other algorithm, β-likeness-primacy, takes into account β-likeness constraint. The algorithm partitions the records into clusters in which β-likeness is satisfied. Since it creates large clusters, each cluster must be split as much as possible while enforcing k-anonymity.

The following lemma defines a condition which ensures that the microdata-based clustering always returns a k-anonymous β-likeness dataset.

                           Lemma 2
                           Let Ni
                               be a set of records (Ni
                               ⊆ T and |Ni
                              | ≥ k) and D[S] denote the domain of sensitive attribute values in T such that |D[S]| = m. PT
                               = 
                                 
                                    {
                                    
                                       p
                                       1
                                    
                                    ,
                                    …
                                    ,
                                    
                                       p
                                       m
                                    
                                    }
                                 
                               and 
                                 
                                    
                                       P
                                       
                                          N
                                          i
                                       
                                    
                                    =
                                    
                                       {
                                       
                                          p
                                          
                                             1
                                          
                                          
                                             N
                                             i
                                          
                                       
                                       ,
                                       …
                                       ,
                                       
                                          p
                                          
                                             m
                                          
                                          
                                             N
                                             i
                                          
                                       
                                       }
                                    
                                 
                               are sensitive value distributions in T and Ni
                              , respectively. The microdata-based clustering always returns a k-anonymous β-likeness dataset if each sensitive value sj
                               satisfies 
                                 
                                    
                                       p
                                       
                                          j
                                       
                                       
                                          N
                                          i
                                       
                                    
                                    ≤
                                    f
                                    
                                       (
                                       
                                          p
                                          j
                                       
                                       )
                                    
                                 
                               where f(pj
                              )= 
                                 
                                    (
                                    (
                                    1
                                    +
                                 
                              min{β, -lnpj
                              }) × pj
                              ).

In the worst case, a potential equivalence class generated by the microdata-based clustering consists of all records in Ni
                              . We create a bucket partition 
                                 
                                    B
                                    =
                                    {
                                    
                                       b
                                       1
                                    
                                    ,
                                    …
                                    ,
                                    
                                       b
                                       
                                          |
                                          B
                                          |
                                       
                                    
                                    }
                                 
                               as follows: For each sensitive value sj, bj
                               is a bucket that contains records with sj
                               as their sensitive values. If ∀bj
                               ∈ B, 
                                 
                                    
                                       
                                          
                                             |
                                          
                                          
                                             b
                                             j
                                          
                                          
                                             |
                                          
                                       
                                       
                                          
                                             |
                                          
                                          
                                             N
                                             i
                                          
                                          
                                             |
                                          
                                       
                                    
                                    =
                                    
                                       p
                                       
                                          j
                                       
                                       
                                          N
                                          i
                                       
                                    
                                    ≤
                                    f
                                    
                                       (
                                       
                                          p
                                          j
                                       
                                       )
                                    
                                    ,
                                 
                               the algorithm returns B as output. Hence, the microdata-based clustering returns a k-anonymous β-likeness dataset.□

The proposed algorithm in this section prioritizes QI values. It first selects k homogeneous records as a cluster. The cluster is then iteratively refined until β-likeness is satisfied.

As inputs, this algorithm takes the cluster Ni
                            generated by background knowledge-based phase, the distribution of sensitive attribute values within original microdata PT
                            and two parameters k and β. N′ is initialized to all input records (Line 3 in Algorithm 2
                           .). The algorithm first finds k-record clusters satisfying β-likeness (Lines 4–15 in Algorithm 2.). For this purpose, in each iteration, the algorithm finds the most distant record r
                           0 from the centroid of N′, ra
                           , and generates the cluster Ck
                            around r
                           0 by the GenerateCluster(r
                           0, N′, Ni, k, β, PT
                           ) function. It then checks whether Ck
                            satisfies β-likeness. If not, Ck
                            is stored in set Free.

The GenerateCluster function (Lines 22–38 in Algorithm 2) produces the cluster Ck
                           . If Ck
                            does not satisfy β-likeness, it iteratively refines the cluster until either β-likeness is satisfied or all records in N′ are examined. The RD(Ck, PT
                           ) function used by the GenerateCluster function, computes the maximum relative difference of frequency of sensitive attribute values in cluster Ck
                            from PT
                           .

It may happen that some k-record clusters will not satisfy β-likeness. Therefore, we try to generate equivalence classes with size larger than k to attain β-likeness (Line 16 in Algorithm 2). We propose two approaches: agglomerative and incremental approaches.

In the agglomerative approach, we iteratively merge k-record clusters (existing in Free) until β-likeness is satisfied. Given C, a set of k-record clusters, and PT
                           , this method includes the following steps:

                              
                                 1.
                                 Select the cluster Cv
                                     with the highest RD(Cv, PT
                                    ) (Line 3 in Algorithm 3
                                    ).

Merge Cv
                                     with the cluster Cw
                                     such that the obtained cluster incurs the lowest GCP (Line 4 in Algorithm 3).

If the merged cluster Cv
                                    ∪Cw
                                     satisfies β-likeness, it is considered as an equivalence class (Line 6 in Algorithm 3). Otherwise, the merged cluster is inserted into C (Line 8 in Algorithm 3).


                           β-likeness is monotonic. Hence, merging the clusters violating β-likeness decreases the relative difference of frequencies and improves the level of β-likeness. This approach may lead to large equivalence classes. See Algorithm 3 for details.

In the incremental approach, we iteratively insert a record into the cluster until the resulting cluster satisfies β-likeness. Given C, a set of k-record clusters, and PT
                           , this approach initially selects the cluster Cv
                            with the highest RD(Cv, PT
                           ) (Line 1 in Algorithm 4
                           ). The remaining k-record clusters are merged to create the set F. This approach selects the closest record in terms of QI values to Cv
                            and insert it into Cv
                           . It proceeds to insert a record to Cv
                            until β-likeness is satisfied. Then, the incremental approach finds the most distant record in F from Cv
                           , called r
                           1 and proceeds to generate next cluster around r
                           1 (Line 8 in Algorithm 4). See Algorithm 4 for detailed description.

The approach proposed in this section prioritizes the sensitive attribute values in order to satisfy β-likeness. The microdata records are first partitioned into a set of buckets based on sensitive attribute values. Then, the final equivalence classes are generated by proportionally selecting records from each bucket.

Let Ni
                            be a set of input records (Ni
                            ⊆ T), 
                              
                                 
                                    P
                                    T
                                 
                                 =
                                 
                                    (
                                    
                                       p
                                       1
                                    
                                    ,
                                    
                                       p
                                       2
                                    
                                    ,
                                    …
                                    ,
                                    
                                       p
                                       m
                                    
                                    )
                                 
                              
                            be the distribution of sensitive attribute values in the original microdata T and 
                              
                                 
                                    D
                                    
                                       N
                                       i
                                    
                                 
                                 
                                    [
                                    S
                                    ]
                                 
                                 =
                                 
                                    {
                                    
                                       s
                                       1
                                    
                                    ,
                                    
                                       s
                                       2
                                    
                                    ,
                                    …
                                    ,
                                    
                                       s
                                       
                                          m
                                          
                                             N
                                             i
                                          
                                       
                                    
                                    }
                                 
                              
                            denote the domain of sensitive attribute in Ni
                           . The β-likeness-primacy algorithm inputs Ni, PT
                           , 
                              
                                 
                                    D
                                    
                                       N
                                       i
                                    
                                 
                                 
                                    [
                                    S
                                    ]
                                 
                              
                            as well as k and β and outputs a set of equivalence classes (Algorithm 5
                           ). The algorithm is divided into three steps:

                              
                                 1.
                                 
                                    β_Partition: 
                                       
                                          
                                             D
                                             
                                                N
                                                i
                                             
                                          
                                          
                                             [
                                             S
                                             ]
                                          
                                       
                                     is partitioned into a number of subsets. All records in Ni
                                     for which the sensitive values are placed in the same subset, generate the bucket bj
                                    . Each bucket bj
                                     satisfies β-likeness.


                                    β_Split: To decrease the size of final equivalence classes and maintain utility, each bucket bj
                                     is split until β-likeness and k-anonymity are satisfied.


                                    Record selection: The records are selected from each bucket in order to decrease information loss of final equivalence classes.

In the first step (Line 1 in Algorithm 5), we partition records into a number of buckets. The records that their sensitive values obey 
                              
                                 
                                    ∑
                                    
                                       
                                          s
                                          j
                                       
                                       ∈
                                       
                                          b
                                          j
                                       
                                    
                                 
                                 
                                    p
                                    
                                       j
                                    
                                    
                                       N
                                       i
                                    
                                 
                                 ≤
                                 f
                                 
                                    (
                                    
                                       p
                                       
                                          l
                                          j
                                       
                                    
                                    )
                                 
                              
                            create the bucket bj
                            where 
                              
                                 p
                                 
                                    j
                                 
                                 
                                    N
                                    i
                                 
                              
                            is the frequency of sensitive attribute value sj
                            in Ni
                            and 
                              
                                 
                                    p
                                    
                                       l
                                       j
                                    
                                 
                                 =
                                 m
                                 i
                                 
                                    n
                                    
                                       
                                          s
                                          j
                                       
                                       ∈
                                       
                                          b
                                          j
                                       
                                    
                                 
                                 
                                    {
                                    
                                       p
                                       j
                                    
                                    }
                                 
                              
                           . As Cao and Karras [5] have shown, this condition ensures that each bucket satisfies β-likeness. It is easy to deduce that if Ni
                            satisfies Lemma 2, 
                              
                                 β
                                 _
                                 P
                                 a
                                 r
                                 t
                                 i
                                 t
                                 i
                                 o
                                 n
                              
                            always find a bucket partition achieving β-likeness. However, to maintain data utility, a bucket partition with the minimum number of buckets is better. The 
                              
                                 β
                                 _
                                 P
                                 a
                                 r
                                 t
                                 i
                                 t
                                 i
                                 o
                                 n
                              
                            function calculates the minimum number of buckets by the dynamic programming method proposed in [5]. To this end, the domain of sensitive attribute values in Ni
                            are sorted in ascending order in terms of PT
                           . Let NB[e] denote the minimum number of buckets to which we can partition sensitive attribute values 
                              
                                 {
                                 
                                    s
                                    1
                                 
                                 ,
                                 …
                                 
                                    s
                                    e
                                 
                                 }
                              
                           . The value of NB[e] is computed by

                              
                                 (1)
                                 
                                    
                                       N
                                       B
                                       
                                          [
                                          e
                                          ]
                                       
                                       =
                                       m
                                       i
                                       
                                          n
                                          
                                             {
                                             b
                                             |
                                             1
                                             ≤
                                             b
                                             ≤
                                             e
                                             −
                                             1
                                             ⋀
                                             i
                                             s
                                             B
                                             u
                                             c
                                             k
                                             e
                                             t
                                             (
                                             b
                                             ,
                                             e
                                             )
                                             =
                                             t
                                             r
                                             u
                                             e
                                             }
                                          
                                       
                                       
                                          {
                                          N
                                          B
                                          
                                             [
                                             b
                                             −
                                             1
                                             ]
                                          
                                          +
                                          1
                                          }
                                       
                                    
                                 
                              
                           
                           
                              
                                 (2)
                                 
                                    
                                       N
                                       B
                                       [
                                       0
                                       ]
                                       =
                                       0
                                       ,
                                       1
                                       ≤
                                       e
                                       ≤
                                       m
                                    
                                 
                              
                           where m is the number of distinct sensitive attribute values in microdata T and the isBucket(b, e) function checks whether sensitive attribute values 
                              
                                 {
                                 
                                    s
                                    b
                                 
                                 ,
                                 
                                    s
                                    
                                       b
                                       +
                                       1
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    s
                                    e
                                 
                                 }
                              
                            satisfy 
                              
                                 
                                    ∑
                                    
                                       j
                                       =
                                       b
                                    
                                    e
                                 
                                 
                                    p
                                    
                                       j
                                    
                                    
                                       N
                                       i
                                    
                                 
                                 ≤
                                 f
                                 
                                    (
                                    
                                       p
                                       
                                          l
                                          j
                                       
                                    
                                    )
                                 
                              
                           . This is called the eligibility constraint. 
                              
                                 β
                                 _
                                 P
                                 a
                                 r
                                 t
                                 i
                                 t
                                 i
                                 o
                                 n
                              
                            splits 
                              
                                 {
                                 
                                    s
                                    1
                                 
                                 ,
                                 …
                                 
                                    s
                                    e
                                 
                                 }
                              
                            into subsets based on Eq. 1
and then outputs the bucket partition B generated by these subsets.

In step 2, 
                              
                                 β
                                 _
                                 S
                                 p
                                 l
                                 i
                                 t
                              
                            reduces the size of the equivalence classes (Line 2 in Algorithm 5). It takes xi
                            records of each bucket, where xi
                            is approximately proportional to the size of the bucket. In particular, we determine the size of equivalence classes using a binary tree and split each node of tree in a top-down manner. We first consider the bucket partition B (generated by 
                              
                                 β
                                 _
                                 P
                                 a
                                 r
                                 t
                                 i
                                 t
                                 i
                                 o
                                 n
                              
                           ) as the root of tree. We represent the root as a potential equivalence class r = 
                              
                                 [
                                 |
                                 
                                    b
                                    1
                                 
                                 |
                                 ,
                                 |
                                 
                                    b
                                    2
                                 
                                 |
                                 ,
                                 …
                                 ,
                                 |
                                 
                                    b
                                    
                                       |
                                       B
                                       |
                                    
                                 
                                 |
                                 ]
                              
                            which |bi
                           | is the number of records in bi
                           . Then we try to partition the root into two children, dividing each bi
                            into 
                              
                                 b
                                 
                                    i
                                 
                                 1
                              
                            and 
                              
                                 b
                                 
                                    i
                                 
                                 2
                              
                           . The left child has 
                              
                                 
                                    |
                                 
                                 
                                    b
                                    
                                       i
                                    
                                    1
                                 
                                 
                                    |
                                 
                              
                            records of bucket bi
                            and the right child has 
                              
                                 
                                    |
                                 
                                 
                                    b
                                    
                                       i
                                    
                                    2
                                 
                                 
                                    |
                                 
                              
                            records. We set 
                              
                                 
                                    |
                                 
                                 
                                    b
                                    
                                       i
                                    
                                    1
                                 
                                 
                                    |
                                    =
                                 
                                 
                                    ⌊
                                    
                                       
                                          
                                             |
                                          
                                          
                                             b
                                             i
                                          
                                          
                                             |
                                          
                                       
                                       2
                                    
                                    ⌋
                                 
                              
                            and 
                              
                                 
                                    |
                                 
                                 
                                    b
                                    
                                       i
                                    
                                    2
                                 
                                 
                                    |
                                    =
                                    |
                                 
                                 
                                    b
                                    i
                                 
                                 
                                    |
                                    −
                                    |
                                 
                                 
                                    b
                                    
                                       i
                                    
                                    1
                                 
                                 
                                    |
                                 
                              
                           . The split is allowed only if both children satisfy eligibility and the number of records placed in each child is at least k (due to k-anonymity). If the root is split, we proceed to check if we can split each of children. When no node is split, we have a tree which its leaves determine the number of records of each equivalence class. The 
                              
                                 β
                                 _
                                 S
                                 p
                                 l
                                 i
                                 t
                              
                            function returns a list of leaves. The following example explains this function.

                              Example 1
                              Let table T include 26 records with D[S]={Flu, HIV, Hypertension, Brain tumors}, which contains 6 records with Flu, 4 with HIV, 8 with Hypertension and 8 with Brain tumors. Suppose that 
                                    
                                       β
                                       =
                                       2
                                       ,
                                    
                                 
                                 
                                    
                                       k
                                       =
                                       2
                                       ,
                                    
                                 
                                 
                                    
                                       
                                          P
                                          T
                                       
                                       =
                                       
                                          (
                                          
                                             6
                                             26
                                          
                                          ,
                                          
                                             4
                                             26
                                          
                                          ,
                                          
                                             8
                                             26
                                          
                                          ,
                                          
                                             8
                                             26
                                          
                                          )
                                       
                                       ,
                                    
                                 
                                 
                                    
                                       f
                                       (
                                       
                                          p
                                          1
                                       
                                       )
                                       =
                                       0.56
                                       ,
                                    
                                 
                                 
                                    
                                       f
                                       (
                                       
                                          p
                                          2
                                       
                                       )
                                       =
                                       0.44
                                       ,
                                    
                                  and 
                                    
                                       f
                                       
                                          (
                                          
                                             p
                                             3
                                          
                                          )
                                       
                                       =
                                       f
                                       
                                          (
                                          
                                             p
                                             4
                                          
                                          )
                                       
                                       =
                                       0.67
                                    
                                 . If the input records Ni
                                  contains 3 records with Flu, 2 with HIV, 4 with Hypertension, and 4 with Brain tumors, 
                                    
                                       β
                                       _
                                       P
                                       a
                                       r
                                       t
                                       i
                                       t
                                       i
                                       o
                                       n
                                    
                                  creates a bucket partition 
                                    
                                       B
                                       =
                                       {
                                       
                                          b
                                          1
                                       
                                       ,
                                       
                                          b
                                          2
                                       
                                       }
                                    
                                  where b
                                 1 contains Flu and HIV and b
                                 2 contains the remaining two. 
                                    
                                       β
                                       _
                                       S
                                       p
                                       l
                                       i
                                       t
                                    
                                  acts as follows: the root 
                                    
                                       r
                                       =
                                       [
                                       5
                                       ,
                                       8
                                       ]
                                    
                                  in Fig. 1
                                  shows a potential equivalence class with 5 records from b
                                 1 and 8 records from b
                                 2. We split r into 
                                    
                                       
                                          c
                                          1
                                       
                                       =
                                       
                                          [
                                          2
                                          ,
                                          4
                                          ]
                                       
                                    
                                  and 
                                    
                                       
                                          c
                                          2
                                       
                                       =
                                       
                                          [
                                          3
                                          ,
                                          4
                                          ]
                                       
                                    
                                 . Both nodes c
                                 1 and c
                                 2 satisfy eligibility and k-anonymity. We proceed to split c
                                 1 into 
                                    
                                       
                                          c
                                          3
                                       
                                       =
                                       
                                          [
                                          1
                                          ,
                                          2
                                          ]
                                       
                                    
                                  and 
                                    
                                       
                                          c
                                          4
                                       
                                       =
                                       
                                          [
                                          1
                                          ,
                                          2
                                          ]
                                       
                                    
                                  and c
                                 2 into 
                                    
                                       
                                          c
                                          5
                                       
                                       =
                                       
                                          [
                                          1
                                          ,
                                          2
                                          ]
                                       
                                    
                                  and 
                                    
                                       
                                          c
                                          6
                                       
                                       =
                                       
                                          [
                                          2
                                          ,
                                          2
                                          ]
                                       
                                    
                                 . c
                                 3, c
                                 4, and c
                                 5 satisfy the eligibility and k-anonymity, but c
                                 6 does not satisfy the eligibility constraint. Thus, c
                                 2 is not split. 
                                    
                                       β
                                       _
                                       S
                                       p
                                       l
                                       i
                                       t
                                    
                                  returns c
                                 2, c
                                 3 and c
                                 4 such that each node has 
                                    
                                       |
                                       B
                                       |
                                       =
                                       2
                                    
                                  buckets.

In the final step of β-likeness-primacy (Lines 3–5 in Algorithm 5), we create the equivalence classes based on the leaves made by 
                              
                                 β
                                 _
                                 S
                                 p
                                 l
                                 i
                                 t
                              
                           . We should also maintain minimum information loss. Hence, we select the records by Hilbert index method [5,38]. Assume that 
                              
                                 B
                                 =
                                 {
                                 
                                    b
                                    1
                                 
                                 ,
                                 …
                                 ,
                                 
                                    b
                                    
                                       |
                                       B
                                       |
                                    
                                 
                                 }
                              
                            is a bucket partition generated by 
                              
                                 β
                                 _
                                 P
                                 a
                                 r
                                 t
                                 i
                                 t
                                 i
                                 o
                                 n
                              
                           . Records of each bucket bi
                            are sorted in terms of Hilbert index transformation. Then for each leaf, generated by 
                              
                                 β
                                 _
                                 S
                                 p
                                 l
                                 i
                                 t
                              
                            and represented as an array a = 
                              
                                 [
                                 
                                    a
                                    1
                                 
                                 ,
                                 
                                    a
                                    2
                                 
                                 ,
                                 …
                                 ,
                                 
                                    a
                                    
                                       |
                                       B
                                       |
                                    
                                 
                                 ]
                                 ,
                              
                            we do as follows: a random record x is selected from b
                           1. Then a
                           1-1 records are selected by binary search within b
                           1 such that they are close to x. We add these records to an empty equivalence class Ei
                            and choose a
                           2 records from b
                           2 such that they are close to records in Ei
                           . We proceed this selection for all buckets bi
                            satisfying ai
                            > 0, 1 ≤ i ≤ |B|.

We propose two hierarchical anonymization algorithms to create equivalence classes satisfying k-anonymity and β-likeness. To protect against background knowledge attack, both algorithms use agglomerative clustering. Then, each cluster is partitioned into equivalence classes to ensure k-anonymity and β-likeness. We proposed two different algorithms: k-anonymity-primacy and β-likeness-primacy. Since the former keeps focus on homogeneity of record in terms of QI values, it may return a k-anonymous β-likeness dataset even if the condition of Lemma 2 is violated. It should be noted that β-likeness-primacy does not generate any equivalence classes where Lemma 2 is violated.

Anonymization algorithms strive to minimize the information loss of their anonymized data. The effort in deterministic anonymization algorithms allows the minimality attacks [40], as the adversary can simulate the whole process of deterministic anonymization algorithms and capture some undesired information. As discussed in [41], the use of randomization is a countermeasure against such attacks. Although background knowledge-based runs a deterministic algorithm, β-likeness-primacy uses the randomization in the record selection step (Step 3). Thus, the anonymization algorithm that applies β-likeness-primacy as the microdata-based clustering, prevents the minimality attack but the one applying k-anonymity-primacy does not.

The time complexity analysis shows that agglomerative clustering is one of the most time-consuming components in the proposed algorithms. The worst case time complexity of this component is O(n
                           2 
                           log n). In the next level of our anonymization algorithm, we should apply either k-anonymity-primacy or β-likeness-primacy. Due to intensive searches done in k-anonymity-primacy, this algorithm is rather slow, with O(n
                           3). Cost of agglomerative and incremental approaches applied in k-anonymity-primacy are 
                              
                                 O
                                 (
                                 
                                    
                                       n
                                       2
                                    
                                    
                                       k
                                       2
                                    
                                 
                                 )
                              
                            and 
                              
                                 O
                                 (
                                 
                                    
                                       
                                          n
                                          2
                                       
                                       +
                                       
                                          k
                                          2
                                       
                                    
                                    2
                                 
                                 )
                                 ,
                              
                            respectively where n is the number of records in the original microdata and k is the level of k-anonymity. In terms of computational cost, β-likeness-primacy has a great advantage over k-anonymity-primacy. In the worst case, β-likeness-primacy’s time complexity is in O(n log n).

Information loss measures are used to compare data quality in the entire anonymized data versus the quality in the original data. In this work, we consider the information loss measures applied in generalization algorithms. DM [35] and AEC [19] consider penalties per equivalence class rather than per record and are not able to consider the data distribution of attribute values. GH [36] and PM [37] take into account the height of generalization hierarchy. These measures are suitable for hierarchical data and treat each generalization as equally damaging, while the levels of different hierarchies are not equally-spaced in terms of information loss. Analogous to GH and PM, NUE does not consider the size of the equivalence classes. Moreover, NUE is an information-theoretic measure, which may overestimate the actual amount of information loss incurred by an anonymization algorithm [38]. On the other hand, GCP [17] considers both cardinality of each class and the domain of attribute values. It differentiates itself by assigning a weight to each attribute. GCP does not consider the effect of not-fitting records on measuring information loss while DM, AEC, GH, PM, and NUE introduce a penalty for not-fitting(removed) records. Some records may not be arranged in any equivalence classes to achieve the privacy requirements. This is shown in the following example.

As stated in section 2.3, GCP is the sum of NCP [17] on all records. NCP is defined by a weighted sum over ranges of all QI values in each equivalence class. A lower value of GCP means a better utility.

                        Example 2
                        Suppose the original microdata T contains five records represented by the points (square or triangle) in a two-dimensional space. Suppose that Figs. 2a, 2b, 3
                           a, and 3b are the outputs of different anonymization algorithms on T. Each black circle shows an equivalence class. The triangles are not fitted in any equivalence classes, so they are not published. Suppose that NCPa
                            and NCPb
                            are NCP of each record in an equivalence class in Fig. 2a and b, respectively. Based on the definition of NCP, NCPa
                            < NCPb
                           . GCP value is 4 × NCPa
                            in Fig. 2a and 5 × NCPb
                            in Fig. 2b. Therefore, the output of the anonymization algorithm on the left side has lower information loss in comparison with the output of the algorithm on the right. Analogous to Fig. 2a, the anonymization algorithm creates just one equivalence class in Fig. 3a while the other algorithm could not find any equivalence classes (Fig. 3b). The GCP values that correspond to the algorithms are 4 × NCPa
                            and zero in Fig. 3a and b, respectively, where zero means no information loss.


                     Figs. 2 and 3 show that if an anonymization algorithm removes more records, the less information loss may be generated in terms of GCP. Therefore, GCP favors removal of records over the generalization. To enable the useful analysis of anonymized data, we would better arrange the records into equivalence classes as much as possible instead of removing them.

To address these issues, we propose an extension of GCP called RGCP, which considers penalties for removed records. This penalty is proportional to NCP of the nearest equivalence class. Let 
                        
                           E
                           =
                           {
                           
                              E
                              1
                           
                           ,
                           
                              E
                              2
                           
                           ,
                           …
                           ,
                           
                              E
                              
                                 |
                                 E
                                 |
                              
                           
                           }
                        
                      be the set of all existing equivalence classes produced by an anonymization algorithm and 
                        
                           F
                           =
                           {
                           
                              f
                              1
                           
                           ,
                           
                              f
                              2
                           
                           ,
                           …
                           ,
                           
                              f
                              
                                 |
                                 F
                                 |
                              
                           
                           }
                           ,
                           |
                           F
                           |
                           ≤
                           n
                        
                      be the set of all unpublished records where n is the number of records in microdata T. We propose the following function for RGCP:

                        Definition 2
                        RGCP measure


                        
                           
                              
                                 
                                    
                                       
                                          
                                          
                                          
                                             
                                                R
                                                G
                                                C
                                                P
                                                =
                                                
                                                   {
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     ∑
                                                                     
                                                                        
                                                                           E
                                                                           i
                                                                        
                                                                        ∈
                                                                        
                                                                           E
                                                                           R
                                                                        
                                                                     
                                                                  
                                                                  
                                                                     |
                                                                  
                                                                  
                                                                     E
                                                                     i
                                                                  
                                                                  
                                                                     |
                                                                     ×
                                                                     N
                                                                     C
                                                                     P
                                                                  
                                                                  
                                                                     (
                                                                     
                                                                        E
                                                                        i
                                                                     
                                                                     )
                                                                  
                                                                  +
                                                                  
                                                                     ∑
                                                                     
                                                                        g
                                                                        ∈
                                                                        
                                                                           G
                                                                           R
                                                                        
                                                                     
                                                                  
                                                                  
                                                                     k
                                                                     R
                                                                  
                                                                  ×
                                                                  
                                                                     |
                                                                     g
                                                                     |
                                                                  
                                                                  ×
                                                                  N
                                                                  C
                                                                  P
                                                                  
                                                                     (
                                                                     g
                                                                     )
                                                                  
                                                               
                                                            
                                                         
                                                         
                                                            
                                                               :
                                                               |
                                                               E
                                                               |
                                                               >
                                                               0
                                                            
                                                         
                                                      
                                                      
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     ∑
                                                                     
                                                                        
                                                                           F
                                                                           i
                                                                        
                                                                        ∈
                                                                        F
                                                                     
                                                                  
                                                                  
                                                                     |
                                                                     
                                                                        g
                                                                        F
                                                                     
                                                                     |
                                                                  
                                                                  ×
                                                                  
                                                                     n
                                                                     R
                                                                  
                                                                  ×
                                                                  N
                                                                  C
                                                                  P
                                                                  
                                                                     (
                                                                     
                                                                        g
                                                                        F
                                                                     
                                                                     )
                                                                  
                                                               
                                                            
                                                         
                                                         
                                                            
                                                               :
                                                               |
                                                               E
                                                               |
                                                               =
                                                               0
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where ER
                            is the set of the existing equivalence classes that are not close to any of the unpublished records such that ER
                            ⊆ E. GR
                            is the set of all virtual equivalence classes and gF
                            is a virtual equivalence class containing all records. kR
                            > 1 and nR
                            > 1 are constant parameters.

In the case of existence of at least one equivalence class, |E| > 0, we find the nearest equivalence class Ej
                      for each not-published record fi
                      such that Ej
                      ∈ E. Suppose Ej
                      is the nearest equivalence class to the set of not-published records Q ⊆ F. The set 
                        
                           g
                           =
                           {
                           
                              E
                              j
                           
                           ∪
                           Q
                           }
                        
                      generates a virtual equivalence class. NCP(Ei
                     ) is NCP of each record in Ei
                      and NCP(g) is NCP of each record in g such that |g| is the size of g. Note that ∀Ei
                      ∈ ER, g ∈ GR
                     , 
                        
                           
                              E
                              i
                           
                           ∩
                           g
                           =
                        
                      ø. When 
                        
                           |
                           E
                           |
                           =
                           0
                           ,
                        
                      a virtual equivalence class gF
                      containing all records is generated. Then, for each record fi
                      ∈ F, NCP is computed regarding gF
                     , represented by NCP(gF
                     ).

One simple solution to find the nearest equivalence class is to measure the distance between the representative of the equivalence class and each removed record fi
                     . As attributes in a microdata table can be divided into numerical and categorical attributes, the representative consists of generalized categorical values and mean of generalized numerical values. The euclidean distance between the representative and removed record is adopted for numerical attributes. For categorical attribute, the length of the shortest path between the representative and fi
                      is computed and then normalized. The sum of the distances of numerical and categorical attribute values can be applied for the mix microdata [27].

According to Definition 2, RGCP is greater than or equal to 0. RGCP=0 corresponds to no information loss i.e., all records in microdata are left unchanged under the generalization. Two constant parameters 
                           
                              
                                 k
                                 R
                              
                              ∈
                              R
                           
                         and 
                           
                              
                                 n
                                 R
                              
                              ∈
                              R
                           
                         are introduced to distinguish different outputs of anonymization algorithms. They are set so that nR
                         > kR
                         > 1. We use the following example (Example 3) to compare RGCP with GCP.

                           Example 3
                           Consider Fig. 2 in Example 2. Suppose that 
                                 
                                    
                                       N
                                       C
                                       
                                          P
                                          l
                                       
                                    
                                    ¯
                                 
                               is NCP of each record in a virtual equivalence class in Fig. 2.a. 
                                 
                                    
                                       
                                          N
                                          C
                                          
                                             P
                                             l
                                          
                                       
                                       ¯
                                    
                                    =
                                    N
                                    C
                                    
                                       P
                                       r
                                    
                                 
                              . The RGCP values are 
                                 
                                    5
                                    ×
                                    
                                       k
                                       R
                                    
                                    ×
                                    
                                       
                                          N
                                          C
                                          
                                             P
                                             l
                                          
                                       
                                       ¯
                                    
                                 
                               in Fig. 2a and 5 × NCPr
                               in Fig. 2b. When kR
                               > 1, the RGCP value in Fig. 2a is higher than that in Fig. 2b which is desired in order to enable useful analyzes on anonymized microdata.

The RGCP values are 
                                 
                                    5
                                    ×
                                    
                                       k
                                       R
                                    
                                    ×
                                    
                                       
                                          N
                                          C
                                          
                                             P
                                             l
                                          
                                       
                                       ¯
                                    
                                 
                               and 
                                 
                                    5
                                    ×
                                    
                                       n
                                       R
                                    
                                    ×
                                    
                                       
                                          N
                                          C
                                          
                                             P
                                             r
                                          
                                       
                                       ¯
                                    
                                 
                               in Fig. 3a and b. Thus, The RGCP value in Fig. 3a is less than that in Fig. 3b.

If constant parameters nR
                         and kR
                         are set correctly, RGCP is desired over GCP, since RGCP considers more data inaccuracies. While RGCP is more precise than other mentioned information loss measures, it is still not entirely accurate. Analogously to GCP, this measure assumes that the attributes are independent and so are the individuals (rows of microdata table). In other words, it does not consider the correlation among values.

Moreover, RGCP is not monotone
                           6
                        
                        
                           6
                           The monotone property means that the information loss increases when the microdata is generalized. This property is important in scalability of anonymization algorithms [42].
                         when anonymization algorithm allows removing some records to satisfy the privacy requirements. The reason is that an increase in generalization may reduce the required amount of removal, effectively decreasing RGCP and the information loss. It should be noted that well-known information loss measures such as AEC, DM, GCP, PM, and NUE measures are not monotone [42].

With a close look at RGCP, we notice that the defined penalty depends on the size of the virtual equivalence class and consequently, the small-size equivalence class results in low penalty. To sum up, The RGCP measure favors the generalization of a record over total removing.

@&#EXPERIMENTS AND RESULTS@&#

In this section, we empirically evaluate our anonymization algorithms using two data sets and according to a number of measures.

Several experiments are conducted to compare the performance of our algorithms. Experiments are performed on a core i5, 2.5 GHZ workstation with 8GB RAM. We implement our algorithms in MATLAB. In our implementation, we use the linkage function of MATLAB as agglomerative clustering algorithm. The complete linkage method is selected as the way of measuring the distance between two clusters. For each algorithm, experiments are done with different values of corresponding privacy parameters. In order to directly compare other works with ours, the values are taken the same as those in [5,8]. The value of k, β and J are taken in the ranges of [3–20], [1–6], and [0.1–0.8], respectively. The values of privacy parameters are shown in Table 2
                        .

Experiments are conducted on two datasets consisting of numerical and categorical attributes as follows:

                           
                              - Adult dataset is a well-known data set from the UCI Machine Learning Repository [16]. This dataset consists of 45,222 records. Each record contains seven attributes, as shown in Table 3
                                 . We use the attributes age, gender, and education as the QI attributes and occupation as the sensitive attribute (Table 3). Different microdata tables are generated by randomly chosen 4000 records from the dataset.

- Bkseq dataset [8] is a synthetic dataset based on domain knowledge extracted from medical literatures. BKseq is the only dataset for which the background knowledge distributions are available. This dataset contains 23 microdata tables of patient records. Each microdata contains approximately 4000 records and each record contains 3 QI attributes: sex, age, weight and a sensitive attribute: the result of medical exams. The domain of each attribute is shown in Table 4
                                 .

We evaluate the proposed algorithms in terms of GCP, which measures information loss and Record Linkage (RL) [15], which measures privacy loss
                           7
                        
                        
                           7
                           RL is a disclosure risk measure.
                        . RL is defined as the amount of correct linkages that one can find between the anonymized and original microdata table.

                           
                              
                                 
                                    R
                                    L
                                    =
                                    
                                       ∑
                                       
                                          
                                             r
                                             j
                                          
                                          ∈
                                          T
                                       
                                    
                                    
                                       P
                                       
                                          R
                                          L
                                       
                                    
                                    
                                       (
                                       
                                          r
                                          
                                             j
                                          
                                          *
                                       
                                       )
                                    
                                 
                              
                           
                        where T is the original microdata. For each record rj
                         in T, 
                           
                              
                                 P
                                 
                                    R
                                    L
                                 
                              
                              
                                 (
                                 
                                    r
                                    
                                       j
                                    
                                    *
                                 
                                 )
                              
                           
                         is the record linkage probability of the anonymized version of rj
                        , namely 
                           
                              r
                              
                                 j
                              
                              *
                           
                        . The record linkage probability of 
                           
                              r
                              
                                 j
                              
                              *
                           
                         is calculated as follows:

                           
                              
                                 
                                    
                                       P
                                       
                                          R
                                          L
                                       
                                    
                                    
                                       (
                                       
                                          r
                                          
                                             j
                                          
                                          *
                                       
                                       )
                                    
                                    =
                                    
                                       {
                                       
                                          
                                             
                                                
                                                   
                                                      1
                                                      
                                                         
                                                            |
                                                         
                                                         
                                                            E
                                                            i
                                                         
                                                         
                                                            |
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   :
                                                   
                                                      r
                                                      j
                                                   
                                                   ∈
                                                   
                                                      E
                                                      i
                                                   
                                                
                                             
                                          
                                          
                                             
                                          
                                          
                                             
                                                0
                                             
                                             
                                                
                                                   :
                                                   o
                                                   t
                                                   h
                                                   e
                                                   r
                                                   w
                                                   i
                                                   s
                                                   e
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        Where Ei
                         is the equivalence class that is at the minimum distance from rj
                        . When rj
                         ∈ Ei
                        , 
                           
                              
                                 P
                                 
                                    R
                                    L
                                 
                              
                              
                                 (
                                 
                                    r
                                    
                                       j
                                    
                                    *
                                 
                                 )
                              
                           
                         is calculated as 
                           
                              1
                              
                                 
                                    |
                                 
                                 
                                    E
                                    i
                                 
                                 
                                    |
                                 
                              
                           
                        . The lower RL leads to the higher privacy of the record respondents.

In this section, we compare the results of the proposed algorithms in three aspects: R-U confidentiality map [43], actual size of equivalence classes, and the run time. R-U confidentiality map is a graphical representation of pairs of RL and GCP. We also compare our algorithms with microaggregation approaches. Since incremental and agglomerative approaches lead to different outputs of k-anonymity-primacy algorithm, we compare results of three anonymization algorithms which run the same background knowledge-based clustering: k-anonymity-primacy which runs incremental approach as postprocessing, k-anonymity-primacy which runs agglomerative approach as postprocessing and β-likeness-primacy. These algorithms are denoted as kInc_GE, kAgg_GE and β_GE, respectively. They publish generalized QI values as well as the sensitive attribute values. We study the impact of three privacy parameters: J, k and β.

In this section, we study the impact of the privacy parameters on GCP and RL measured by our algorithms. We analyze the behavior of our algorithms when J and β are set to 0.8 and 3, respectively and the k values are taken in the range 3–20. Results of evaluation on Adult and Bkseq datasets are shown in Fig. 4
                           . It is observed that β_GE leads to highest information loss and lowest privacy loss compared to kInc_GE and kAgg_GE. This is because that β_GE keeps focus on sensitive attribute values and puts all records into the equivalence classes. Therefore β-likeness may be satisfied with heterogeneous QI values which results in high loss of information. Moreover, the experimental results show kInc_GE outperform kAgg_GE due to lower information. This result is expected, since kInc_GE inserts iteratively one record into the existing clusters until β-likeness is satisfied while kAgg_GE merges the k-record clusters and generates larger equivalence classes.

We also see that the range of information loss caused by β_GE is smaller than those in kInc_GE and kAgg_GE. The reason is that the structure of the tree produced by β_GE is the same when J and β are constant and the k values are changed from 3 to 20. Therefore the k-anonymity constraint just leads to removing the leaves of size less than k. Since the number of these small size leaves attaining β-likeness is small, the information losses caused by β_GE on different values of k are close to each other. kInc_GE and kAgg_GE on the other hand may generate completely different equivalence classes over different values of k. Hence, the range of information loss may increase.

Next, we study the performance of the algorithms over different values of J. β and k are set to 3 and 5, respectively. The results, shown in Fig. 5
                            reaffirm our
                           
                           
                           
                           
                            previous findings. In both datasets, β_GE generates the equivalence classes with low RL and high GCP while kAgg_GE yields high RL. Unlike the results shown in Fig. 4, the range of information loss generated by β_GE is wide. This is because that when J values are changed, the background knowledge-based clustering may generate different clusters which causes different information losses.

We also evaluate the proposed algorithms as J and k are constant. In Fig. 6, J and k are set to 0.8 and 5, respectively and the β values are taken in the range 0.1–0.8. The experimental results, shown in Fig. 6, confirm our previous findings again.

From experiments of this section, we conclude that the earlier β-likeness is considered in anonymization algorithms, the more privacy is preserved in output. Moreover, merging the k-record clusters in kAgg_GE produces the larger equivalence classes and hence, causes lower utility.

In this section we want to show how close to k are the size of equivalence classes generated by our algorithms on Adult dataset when J and β are constant. To minimize information loss, the closer size to k is desired. Actual size of equivalence classes are shown in Table 5
                           . Each row represents the minimum size (the size of smallest equivalence class which determines the actual level of k-anonymity) and the average size of equivalence classes over different values of k. It can be seen that the average size of equivalence classes tends to be increased when the level of k-anonymity increases. Also, the average size of equivalence classes generated by kInc_GE and kAgg_GE is smaller compared to β_GE. Furthermore, the difference between the minimum and the average size of equivalence classes in β_GE, is more significant than kInc_GE and kAgg_GE. We conclude that kInc_GE and kAgg_GE create smaller equivalence classes compared to β_GE. The reason is that kInc_GE and kAgg_GE finds the k-record clusters satisfying β-likeness.

Then, the size of equivalence classes generated by our algorithms are compared in Table 6
                            while β and k are constant. In this table, each row represents the minimum and the average size of the equivalence classes over different values of J. The results reaffirm our previous findings. We see that the average size of the equivalence classes increases as J increases. Additionally, the average size of equivalence classes generated by β_GE is greater than those in kInc_GE and kAgg_GE. This is because that for large values of J, the background knowledge-based clustering generates a few clusters of large size. β_GE generates large buckets and large equivalence classes proportional to the buckets. Consequently, the size of equivalence classes may be increased.

In this section, we compare the information loss resulting from our proposed anonymization algorithms with microaggregation method. Two microaggregation methods are implemented by modifying k-anonymity-primacy which runs agglomerative or incremental approaches as postprocessing. They are denoted as kAgg_Mic and kInc_Mic algorithms, respectively. For this purpose, the microaggregation algorithms use SSE [16] instead of GCP, and release the mean of numerical QI values within each equivalence class. Categorical QI values are replaced with mode in their corresponding equivalence class. Since β_GE does not consider the homogeneity of QI values, the microaggregation method corresponding to β_GE is not taken into account in this section.

Neither GCP nor SSE is suitable for comparing these methods. GCP is not suitable for aggregated attribute values produced by microaggregation methods. SSE as a common information loss metric to evaluate microaggregation algorithms is not suitable for generalized attribute values. SSE uses the mean of each numerical range (generalized value) and consequently, a generalization algorithm can lead to higher information loss compared to microaggregation method in terms of SSE. Therefore, we compare the distributions of the attribute values produced by our generalization algorithms and the corresponding microaggregation algorithms in this section.

We compare the evaluation results in two aspects: the confidence interval of relative differences and the distance between the distributions of the anonymized and original QI attribute values. The relative differences of QI values
                              8
                           
                           
                              8
                              Assume that the QI value ai
                                  has frequency pi
                                  in the original microdata and frequency qi
                                  in the anonymized microdata. The relative difference is 
                                    
                                       D
                                       
                                          (
                                          
                                             p
                                             i
                                          
                                          ,
                                          
                                             q
                                             i
                                          
                                          )
                                       
                                       =
                                       
                                          
                                             
                                                q
                                                i
                                             
                                             −
                                             
                                                p
                                                i
                                             
                                          
                                          
                                             p
                                             i
                                          
                                       
                                    
                                 
                              
                            with 95% confidence interval are shown in Table 7
                           . Threshold J is set to 0.3 and β to 4. Each row represents parameter k, the anonymization algorithm and the relative difference of QI values within the Adult dataset. The experimental results show that the generalization algorithms (kAgg_GE and kInc_GE) incur small relative differences (less than 0.1) and are close to the corresponding microaggregation methods. As evident from Table 7, the relative difference of QI values published by β_GE is the highest which is expected, since this algorithm prioritizes the sensitive attribute values to generate equivalence classes.

We adopt Earth Mover’s distance (EMD) to measure the distance between distributions of original and anonymized QI values. The results are shown in Table 8
                           . Studying EMD of anonymized and original QI values shows when the range of an attribute is small, EMD will be less. Thus, EMD in the Sex values is less than the Education values and EMD in the Education value is less than the Age values. Moreover, EMD values caused by β_GE are the highest.

It can be seen that the EMD values resulted by kAgg_GE and Agg_MI are less than those by kInc_GE and Inc_MI. Furthermore, in categorical attributes such as Sex and Education, the generalization algorithms (kAgg_GE and kInc_GE) outperform the corresponding microaggregation algorithms (Agg_MI and Inc_MI). In numerical attributes such as Age, the microaggregation algorithms (Agg_MI and Inc_MI) result in less EMD than the corresponding generalization algorithms (kAgg_GE and kInc_GE). Next, we repeat these experiments on Adult and Bkseq datasets with different values of privacy parameters. The evaluation results reaffirm our previous findings.

In the last part of evaluation, we focus on measuring the run time of our algorithms on Bkseq dataset. To that end, we run two different experiments. At the first experiment, we set k = 5 and β = 3. The J values are taken in the range 0.1–0.7. The run time of our algorithms are shown in Fig. 7 as a function of J. It should be noted that Y-axis is a logarithmic scale. Since the background knowledge distributions of records in Bkseq dataset are close to each other, the background knowledge-based clustering generates one cluster for J ≥ 0.3, thus the run time of our algorithms is the same for J ≥ 0.3 in Fig. 7. The results show that the run time of kInc_GE and kAgg_GE is low for J < 0.3. The reason is that for J < 0.3, the background knowledge-based clustering generates more clusters of small size and consequently, kInc_GE and kAgg_GE require less rearrangement of records to satisfy β-likeness. Moreover, it can be seen that β_GE outperforms kInc_GE and kAgg_GE in terms of the run time and it is in agreement with the theoretical analysis of computational costs.

Finally, we choose values of privacy parameters such that a worst case in the run time can be seen. To that end, we set J = 0.6, thus the background knowledge-based clustering generates groups with maximum sizes and the microdata-based clustering requires more iterations to refine the k-record clusters. The run time of the algorithms for Bkseq dataset is shown in Fig. 8 as a function of k. It should be noted that the three algorithms have a cubic cost to generate the clusters in background knowledge-based clustering. Fig. 8 shows that β_GE is more efficient than kInc_GE and kAgg_GE. Taking a close look, kAgg_GE has the less run time compared to kInc_GE. The reason is that, although the worst case time complexity of both algorithms is O(n
                           2), kAgg_GE merges the k-records clusters. Thus, the number of iterations of kAgg_GE is less than kInc_GE which inserts one record to each equivalence class in each iteration. Figs. 7 and 8 show that kInc_GE and kAgg_GE may not scale well for large input clusters whereas β-likeness does.

We also investigate the number of records removed by the algorithms and their impact on information loss. It is assumed that records outside the equivalence class must be removed. We run different experiments to evaluate our proposed algorithms. Fig. 9 shows the number of records removed by the algorithms. Fig. 9a plots the percentage of removed records of Adult dataset in terms of different values of k when J and β are set to 0.8 and 3, respectively. Fig. 9b shows the percentage of removed records in terms of different values of J, when k and β are set to 5 and 3, respectively.

A few number of records, at most 11(<0.27%), are suppressed by the proposed algorithms to achieve the privacy model, as shown in Fig. 9. Fig. 9a shows that β_GE is more efficient than kInc_GE and kAgg_GE in terms of the number of removed records. Although the background knowledge-based clustering may remove a number of records in the first phase, β_GE puts the rest into the equivalence classes and consequently the least number of records is removed by β_GE.

Moreover, kInc_GE outperform kAgg_GE in terms of the number of removed records. This is because that kAgg_GE merges the k-record clusters. It is likely that some k-record clusters does not satisfy privacy model, and consequently the more records may be removed. Fig. 9b represents that small values of J lead to the removal of more records. These results are expected when a strict privacy (small values of the privacy parameters e.g 
                           
                              J
                              =
                              0.2
                           
                        ) is satisfied.

Then, we evaluate the proposed algorithms in terms of RGCP. We take k = 5 and β = 3. The J values are taken in the range 0.2–0.6. Fig. 10 plots the performance curves of the information loss caused by the algorithms. Each figure represents the information loss measured by GCP and RGCP. In each figure, GCP is less than RGCP, the reason is that GCP does not assign any penalty to removed records. The highest difference between RGCP and GCP is shown in J = 0.2 in which the algorithms remove more records to achieve the privacy model.

The next task is to compare the algorithms to the predecessor and closest work, Hilbert index-based algorithm [8]. It should be noted that the other close work [15] does not consider the adversary’s background knowledge, thus we cannot do any fair and clear comparison.

The Hilbert index-based algorithm sorts records based on the Hilbert index and takes each k records and check whether k-anonymity and t-closeness are satisfied. In order to create a playing field to compare Hilbert index-based to our algorithms, we conduct two experiments on Bkseq dataset provided in [8]. To do so, we change privacy constraints in our algorithms to be the same as those in Hilbert index-based algorithm. It means that both algorithms try to prevent the background knowledge attack as well as to satisfy k-anonymity and t-closeness models. Since β_GE cannot be modified to enforce t-closeness, we plot the evaluation results of kInc_GE and kAgg_GE in the following figures. We take 
                           
                              J
                              =
                              0.6
                           
                         and 
                           
                              t
                              =
                              0.5
                           
                         as selected in [8].

We first compare Hilbert index-based algorithm, kInc_GE and kAgg_GE in terms of information loss and privacy loss. Obviously, these values should be as low as possible. Fig. 11 presents RL and GCP caused by the algorithms over different values of k. Our algorithms can achieve the best performance. In each level of RL, our algorithms result to lower information loss. However, kInc_GE and kAgg_GE are performed slower compared to Hilbert index-based algorithm.

Next, we examine the utility loss from standpoint of aggregate query answering accuracy [9]. We compare the algorithms using the average relative query error in the returned number of individuals. A query is characterized by two parameters: the number of QI attributes and the expected selectivity
                           9
                        
                        
                           9
                           Expected selectivity is the expected ratio of records returned by the query.
                        . For each value of expected selectivity, we run 100 random queries and compute the average of relative query error. It should be noted that each query considers a single QI attribute. Fig. 12 plots the average relative error versus different expected selectivities. As evident from Fig. 12, the relative error of kInc_GE and kAgg_GE are very close to each other. Moreover, in most cases, the query error of our algorithms is better than Hilbert index-based.

It should be noted that our algorithms remove a small number (<27%) of records, while Hilbert index-based algorithm removes at most 30% of records [8].

@&#DISCUSSION@&#

We have analyzed our algorithms in terms of GCP as the information loss metric, and RL as the privacy loss metric over two datasets, regarding different values of privacy parameters.

The proposed privacy model has different goals including prevention of background knowledge attack and identity and attribute disclosures. The anonymization algorithms satisfy these goals in a hierarchical manner. The algorithms prioritize the prevention of background knowledge attack, which deals with exposed correlations between QI and sensitive attributes. Therefore, in the first phase of the algorithms, agglomerative clustering guarantees that the distributions extracted in the presence of background knowledge are similar in each equivalence class.

In the second phase, we enforce k-anonymity and β-likeness in order to prevent the identity and attribute disclosures. Since these two models pursue different goals, (the former keeps focus on homogeneity of records while the latter prioritizes the sensitive attribute values), we propose two different algorithms: k-anonymity-primacy and β-likeness-primacy (β_GE). k-anonymity-primacy can apply two different post-processings: incremental (kInc_GE) and agglomerative (kAgg_GE) approaches.

Partitioning based on the similarity of QI values results in the reduction of information loss and an increase in privacy loss (as can be seen in k-anonymity-primacy). On the other hand, partitioning according to the sensitive attribute values does not guarantee the similarity of QI values and as a result, information loss may be increased as can be seen in β-likeness-primacy.

Moreover, when the anonymization algorithm generates equivalence classes of small size, information loss will decrease, since with small-size clusters less records need to be identical in each equivalence class. Therefore, kInc_GE improves on kAgg_GE and, in turn, kAgg_GE improves on β_GE.

Despite the inherent benefits of microaggregation algorithm with respect to generalization (values does not lose granularity and the continuous nature of numbers is preserved [15]), an important concept that should be considered is trustfulness such that anonymized data by microaggregation are not as accurate as they are in the original microdata. This may happen because some important values are removed and data distributions of anonymized attribute values are not consistent with those in the original data. Trustfulness of generalization may dominate benefits of microaggregation.

However, generalization algorithms cannot process numerical values accurately, since generalization discretizes numbers to numerical ranges and it does not consider the distribution of numerical values in each numerical range. On the other hand, microaggregation has some deficiency in preserving categorical semantics, since microaggregation uses statistics such as mode as centroid of equivalence classes. Therefore, for mixed microdata (including numerical and categorical attributes), combination of generalization and microaggregation may be effective [27]. This is due to the fact that generalization preserves more statistical characteristic of categorical data than microaggregation algorithm while microaggregation performs better on numerical data.

@&#CONCLUSION@&#

In this paper, we propose anonymization algorithms which consider the adversary’s background knowledge in privacy model. We define a strong privacy model based on k-anonymity and β-likeness to protect against background knowledge attack.

Two anonymization algorithms are proposed to achieve our privacy model: k-anonymity-primacy and β-likeness-primacy. In the first phase of these algorithms, we use the agglomerative clustering to prevent background knowledge attack. In the next phase, our algorithms satisfy k-anonymity and β-likeness. k-anonymity-primacy keeps focus on homogeneity of QI values. β-likeness-primacy prioritizes the prevention of attribute disclosure through the β-likeness constraint. Thus β-likeness-primacy leads to lower privacy loss and higher information loss compared to k-anonymity-primacy. Two versions of k-anonymity-primacy are proposed with different post-processings: incremental and agglomerative approaches. Experimental results show that our anonymization algorithms cause a lower information loss and privacy loss compared to the well-known anonymization algorithms proposed previously.

We also propose an extension to the Global Certainty Penalty measure, which considers unpublished records. This measure charges a penalty for each removed record. The penalty of each removed record is proportional to the Normalized Certainty Penalty of the nearest equivalence class.

This work can open new directions for future works. β-likeness-primacy is a fast algorithm which does not return any k-anonymous β-likeness dataset when Lemma 2 is not satisfied. One direction is to refine the original microdata in order to satisfy Lemma 2 and generate a k-anonymous β-likeness microdata. In addition, we can propose an information loss measure to consider the correlation of attributes and their importance. Such measures help to improve the quality of analysis afterwards. We can also apply our algorithms to other anonymization operations like anatomy. We will try to evaluate the well-known algorithms in terms of RGCP.

@&#ACKNOWLEDGMENTS@&#

The authors would like to thank the reviewers for their comments, which helped improve the paper significantly.

@&#REFERENCES@&#

