@&#MAIN-TITLE@&#Interactive visualizations for decision support: Application of Rasmussen's abstraction-aggregation hierarchy

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Rasmussen's abstraction-aggregation hierarchy was employed to develop two decision support systems maintenance and diagnostics.


                        
                        
                           
                           Experimental evaluations of both systems indicted that higher abstraction displays enhanced the performance of experienced personnel.


                        
                        
                           
                           Less experienced personnel found the higher abstraction displays less useful, in some cases even confusing.


                        
                        
                           
                           Training and/or aiding should focus on enhancing abilities of less experienced personnel to use higher abstraction displays when appropriate.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Interactive visualizations

Abstraction-aggregation hierarchy

Maintenance

Enterprise diagnostics

@&#ABSTRACT@&#


               
               
                  Data visualization has of late received an enormous amount of attention from both researchers and practitioners. Even the popular press often includes impressive visualizations of various data sets. Interactive visualizations frequently include data visualizations, but they differ in that users employ the visualizations to make inferences, reach conclusions, and make decisions that result in changed and/or new visualizations. Data visualizations emphasize “what is,” but interactive visualizations address “what if.” In this way, interactive visualizations are often intended for decision support. This article addresses the design of interactive visualizations for decision support. An overall methodology is presented; central to this methodology is Jens Rasmussen's abstraction-aggregation hierarchy. The results of two applications and evaluations of the outcomes of using this methodology are discussed. The first application focused on interactive visualizations for helicopter maintenance. The second application addressed “enterprise diagnostics” in the automobile industry where subjects were asked to diagnose the cause of failed automobile brands. The results of these two applications are used to assess the efficacy of the proposed methodology.
               
            

@&#INTRODUCTION@&#

Big data and visualization of data are “hot” topics in many areas currently. Large data sets and decreasingly expensive computation and display technologies have enabled many “wow” experiences. Yet the question remains of how best to employ these assets to improve human decision making and problem solving. This article addresses this question.

Jens Rasmussen (1983, 1986, 1994) developed a principled approach to this question three decades ago – long before big data and pervasive portable devices. He argued that information at different levels of abstraction and aggregation would best support different modes of human decision making and problem solving. This article explores this possibility.

We proceed as follows. First, we review background on the design of interactive visualizations. Next we consider purposes of visualizations and present a methodology for design of visualizations. We then consider two applications that experimentally evaluated the use of Rasmussen's abstraction-aggregation hierarchy. The results of both experiments support the value of the approach but also illustrate its subtleties. We conclude with an assessment of what we have learned and likely next steps.

@&#BACKGROUND@&#

Edward Tufte is a recognized guru of graphical excellence. In his 1983 classic, The Visual Display of Quantitative Information (Tufte, 1983), he argues that graphical displays should:
                        
                           •
                           “Show the data

Induce the viewer to think about the substance rather than about the methodology, graphic design, the technology of graphic production or something else

Avoid distorting what the data have to say

Present many numbers in a small space

Make large data sets coherent

Encourage the eye to compare different pieces of data

Reveal the data at several levels, from a broad overview to the fine structure

Serve a reasonably clear purpose: description, exploration, tabulation, or decoration

Be closely integrated with the statistical and verbal descriptions of a data set.”

In a subsequent book (Tufte, 1997) he provides a powerful illustration of the consequences of ignoring these principles. Tufte reviews the decision to launch the Space Shuttle Challenger on January 28, 1986, resulting in explosion of the Challenger 73 s after the rockets were ignited and death of the seven astronauts aboard. This was due to failure of the rockets’ O-rings. NASA managers knew about the possible failure of the O-rings. The Morton Thiokol engineers who designed the rocket opposed launching, supporting this recommendation with a 13-chart presentation. However, the data that were presented in the charts were not fully representative of the phenomena of interest, i.e., the impact of temperature on the performance of the O-rings. The charts obscured the phenomena and provided ample room for NASA to argue that the evidence did not support a decision to delay the launch. Tufte concludes, “There was a clear proximate cause (of the accident): an inability to assess the link between cool temperature and O-ring damage on earlier flights. Such a pre-launch analysis would have revealed that this flight was at considerable risk.” He also notes that, beyond the inadequately represented data, “The accident serves as a case study of groupthink, technical decision making in the face of political pressure, and bureaucratic failures to communicate.”

Tufte's guidance is often applied within the broader context of humans interacting with systems, ranging from everyday things (Norman, 1988), to computers and portable devices (Moggridge, 2007), to complex systems and organizations (Rouse, 2007). Moggridge argues that the designing interactions within such contexts should be considered in terms of a hierarchy of complexity regarding humans and their interactions with the world:
                           
                              •
                              “Ecology: The interdependence of living things, for sustainable design

Anthropology: The human condition, for global design

Sociology: The way people relate to one another, for the design of connected systems

Psychology: The way the mind works, for the design of human–computer interactions

Physiology: The way the body works, for the design of physical (hu)man-machine systems

Anthropometrics: The sizes of people, for the design of physical objects”

This characterization of physical, human, and social phenomena has parallels in the discussions throughout this article. The key point here is that visualizations are used by humans in the context of interactions with technological systems and other people, and that these interactions occur in the context of economic and social systems that have historical roots and contemporary manifestations. Context, at multiple levels, really matters.

Data visualization has received an enormous amount of attention in recent years. Nevertheless, there are a variety of unsolved visualization problems (Chen, 2004, 2005). Chen argues for increased focus on usability, noting that the growth of usability studies and empirical evaluations has been relatively slow. It is much easier to conclude that a particular visualization is “cool,” then to determine that it is both usable and useful.

Chen also asserts that we need increased understanding of elementary perceptual-cognitive tasks, including the notion of visual inference and the impact of prior knowledge on task performance. This, he asserts, is particularly important for visualizations that are less about structure and more about dynamics and thus require visualizing changes over time.

The interpretation of visualizations often requires domain knowledge. For example, the symbology displayed and relationships among symbols often depend on prior understanding of aircraft or power plants, for example. Chen asserts that there have been limited compilations of the types of visualizations relevant to different domains, including how to visualize domain knowledge in itself. This really is not the case if one looks at the broader literature.

Visualizations that leverage abstractions derived from domain knowledge can be more effective than those that are based on the physical configuration of the system alone. Examples of this idea can be found in multiple domains including process control, aviation, military command and control, as well as medicine.

In process control, Beltracchi (1987) employed a visualization based on the Rankine cycle to reduce the cognitive load on operators of a nuclear power plant while Vicente et al. (1995) found that a physical view plus a view based on the functional purpose of the system aided in problem solving tasks for a thermal hydraulic process. Similarly, in aviation, Amelink et al. (2005) developed a visualization to aid pilots in their task of managing energy as opposed to just the positions of controls. Ellerbroek et al. (2013) developed an airborne separation assistance system display that reduced the projected forbidden zones presented to the pilot by incorporating the 3-dimensional interaction of the two conflicting aircraft as opposed to simply projecting the 3-D flight envelope of the conflicting aircraft onto a 2-D plane.

In the domain of military command and control, the RAPTOR system was developed to incorporate abstractions relevant to combat decision making (e.g., force ratio), into command and control displays that normally just provide information such as unit location and message traffic (Bennett et al., 2008; Hall et al., 2012). An experiment involving US Army officers found that those that used RAPTOR performed significantly better than those using an interface based on an actual military command and control system.

Finally, in the domain of medicine, McEwen et al. (2014) developed a visualization to display information on a patient's cardiovascular health. In additional to providing typical test results, the display also incorporates graphical presentations of relevant abstractions based on the diagnostic functions that need to be performed. (e.g., using the Framingham score to assess the overall risk of cardio vascular disease).


                        Chen (2004, 2005) also argues for practical needs such as education and training, intrinsic quality measures, and understanding the scalability of visualizations. For example, what works for 100 nodes may be less useful for 100,000 nodes. Lastly, Chen includes the challenge of understanding the aesthetics of visualizations. What looks good and what does not and why? The answers to these questions are unlikely to be universal across domains and cultures.

Beyond being aesthetically appealing, visualizations are usually intended to serve some useful purpose. Indeed, without purposes, visualizations are all just colors and curves. Not surprisingly, it is important to define the purpose of the visualization before, or at least during, its creation.


                     Ware (2012) suggests the following purposes:
                        
                           •
                           “Visualization provides an ability to comprehend huge amounts of data.

Visualization allows the perception of emergent properties that were not anticipated.

Visualization often enables problems with the data to become immediately apparent.

Visualization facilitates understanding of both large-scale and small-scale features of the data.

Visualization facilitates hypothesis formation.”


                     Rasmussen (1983, 1986, 1994) addressed the role of visualizations in operating and maintaining complex engineered systems such as nuclear power plants. He argued for the merits of thinking about information displays in terms of abstraction and aggregation. He proposed the following abstraction hierarchy:
                        
                           •
                           “Functional Purpose – production flow models, system objectives

Abstract Function – causal structure; mass, energy, and information flow topology, etc.

Generalized Functions – standard functions and processes, control loops, heat transfer, etc.

Physical Functions – electrical, mechanical, chemical processes of components and equipment

Physical Form – physical appearance and anatomy, material and form, locations, etc.”

Thus, the purpose of a system is a more abstract concept than how it functions. In turn, the systems’ abstract and generalized functions are more abstract than its physical functions and form. Later discussion of two applications of this idea illustrates the power of thinking in these terms.

Levels of aggregation are best illustrated in terms of the decomposition of a system, for example, an automobile. At the abstraction level of physical form, the vehicle can be decomposed into power train, suspension, frame, etc. The power train can be decomposed into engine, transmission, drive shaft, differentials, and wheels. The engine can be decomposed into block and cylinders, pistons, camshaft, valves, etc. As noted, these levels of aggregation are all represented within the same level of abstraction – physical form.

Rasmussen's hierarchies of abstraction and aggregation play a central role in the visualization design methodology presented later in this article. These constructs are central to usefully portraying complex systems for management, design, operations and maintenance. By the way, Rasmussen formalized these ideas, based on many years of research in Denmark, while on sabbatical with the first author's research group at Georgia Tech in the early 1980s.

As is elaborated below, one can view a use case of a set of visualizations as a trajectory in an abstraction-aggregation space. Each point in this space will have one or more associated visualizations. Each of these visualizations may portray, individually or in combination, data (e.g., performance history of subsystems), structure (e.g., what connects to what), dynamics (e.g., response over time), or other pertinent representations.

Rasmussen and many colleagues employed this construct and several others to develop a methodology termed Ecological Interface Design (EID). In his classic (Rasmussen, 1986), he focused on developing guidelines for control systems, with emphasis on higher-level supervisory tasks and longer-term maintenance and planning tasks. These guidelines were organized in terms of control requirements identification, decision task analysis, cognitive task analysis and design, cognitive task allocation, demand/resources matching, and interface design. Key concepts included the decision ladder, the means-ends abstraction hierarchy, and skill, rule, and knowledge-based behaviors. Applications discussed emphasized engineered systems with human operators and maintainers.


                        Rasmussen and Vicente (1989) focused on the phenomenon of human error and implications for EID. They considered four types of human errors: errors related to learning and adaptation, interference from competing cognitive control structures, lack of resources and intrinsic human variability. They introduced three principles.
                           
                              •
                              Integrate the observation and action surfaces, so the time-space loop is maintained.

Create a one-to-one mapping between the machine's internal processes and interface to make this visible to operators.

Display the process relational structure to support knowledge-based processing.


                        Vicente and Rasmussen (1992) extended these ideas by mapping EID principles to cognitive levels.
                           
                              •
                              “To support interaction via time-space signals, the operator should be able to act directly on the display, and the structure of the displayed information should be isomorphic to the part-whole structure of movements.”

“Provide a consistent one-to-one mapping between the work domain constraints and the cues or signs provided by the interface.”

“Present the work in the form of an abstraction hierarchy to serve as an externalized mental model that will support knowledge-based problem solving.”


                        Rasmussen et al. (1994) present an ambitious application of EID to design and evaluation of a library information retrieval system, BookHouse. The objective of BookHouse system was to “create an organization that reflects the users' needs and task perspective.” They develop classification and indexing principles with five dimensions (bibliographical data, subject matter, frame, author's intention, and accessibility). They show how to build a retrieval system with a search dialog that supports users to choose different databases and strategies. Each retrieval strategy requires its own retrieval functionality, each of which is detailed for user and computer separately.


                        Vicente (1999) extends the EID thinking to cognitive work analysis. He elaborates work domain analysis, control task analysis, strategies analysis, social organization and cooperation analysis, and worker competencies analysis. He argues that one should understand the work domain first and then design the work of operators. Work demands relate to cognitive constraints plus environment constraints. He concludes that operators should be able to understand the work domain via the designed interface and adjust their work practices to meet unpredictable work demands.


                        Bennett and Flach (2011) extend and illustrate EID in the context of two well-defined work domains -- the Game of Fifteen and a simple manual control task. They elaborate a framework that involves the three levels of abstraction proposed by Marr (1982) in his theory of vision as aggregates of Rasmussen's five levels of abstraction:
                           
                              •
                              Computational theory (functional purpose and abstract function)

Representation/algorithmic (general functions/activities)

Hardware implementation (physical function and physical form)

From this abstraction-decomposition framework, they conclude:
                           
                              •
                              “Productive thinking about situations involves a kind of progressive deepening in which concrete details are illuminated in the context of more abstract organizational, functional, and purposeful properties of the larger situation.”

“A prerequisite for the design of good representations is a work analysis process that provides insights into the couplings across levels of abstraction and decomposition.”

“Good representations leverage the loose coupling between levels of abstraction and decomposition to organize detailed information in ways that reflect constraints at higher levels of abstraction.”

We have built on this impressive body of work, quite literally in our first case study (helicopter maintenance), but more figuratively in our second case study (enterprise diagnostics). This difference is due to the second case study involving a system that is more economic and social than engineered. The implications of these differences are later elaborated.

Computer technology, including graphics hardware and software, have made it possible to create impressive and pleasing visualizations of a wide range of phenomena. If one's sole purpose is to impress people and collect “wows,” then the tools are available to achieve these ends. In this article, however, the goal is to create interactive visualizations that serve particular purposes.

Purposes of visualization were discussed earlier. Now, we need to consider users' purposes which visualizations are intended to support. Users’ purposes seldom include using visualizations; these are simply the means to other ends such as:
                        
                           •
                           Problem Solving, perhaps using topographic rules (Rouse, 1983) to explore structural relationships underlying the phenomena of interest

Pattern Recognition, perhaps using symptomatic rules (Rouse, 1983), or recognition-primed decision making (Klein, 2003), to identify regularities and anomalies

Procedure Execution, as illustrated later in this article for helicopter maintenance (Frey et al., 1992, 1993) which involves understanding how to execute procedural steps

Navigation, involving maps and signs (Rasmussen, 1983) needed to move from one location to another, ranging from geographic locations to finding the subsystems of a power plant, for example

The methodology outlined below is intended to help one to design visualizations that will support users in their pursuits of these types of purposes (Rouse, 2015).

Step 1: Identify information use cases, including knowledge levels of users.

Use cases provide descriptions of alternative ways in which users will employ the visualizations to achieve their purposes – before the visualizations have been created. These high-level descriptions can be characterized in terms of six general tasks:
                        
                           •
                           Retrieve data and visualizations relevant to questions of interest

Recognize characteristics of interest across chosen attributes

Construct or select and parameterize representations provided

Compute outputs of constructed representations over time

Compare outputs to objectives or across output variations

Refine constructed representations and return to Compute

Numerous examples of the use of such descriptions are provided by Rouse (2015).

Step 2: Define trajectories in abstraction-aggregation space.

Use cases define what information is needed and what actions are taken at every step of the task of interest. This includes the levels of abstraction and aggregation for requisite information elements and controls for each task.

Step 3: Design visualizations and controls for each point in space.

Transform the outputs of Step 2 into what specifically users can see and do. There is a wealth of possibilities, which need to be compiled into a manageable set of choices. Note that many of the possibilities will be domain dependent.

Step 4: Integrate across visualizations and controls to dovetail representations.

Visualizations and controls should not completely change for each step in a task. Integrated visualizations may be able to support more than one step. Individual controls may affect more than one view.

Step 5: Integrate across use cases to minimize total number of displays and controls.

The set of visualizations may serve more than one purpose. For example, novices may use it to learn about a domain while experts use it to address real problems of interest. Experts may, for example, see the same central visualizations as novices, but have access to additional information and controls to enable manipulations of phenomena that novices would not understand.

@&#SUMMARY@&#

The initial result of Step 1 is often a rather high-level description, perhaps just a few phrases. Step 2 may initially involve a single point in the abstraction-aggregation space, and hence Step 3 may initially involve a single visualization. Thus, the initial “spiral” through the prototyping and evaluation cycle will likely not involve Steps 4 and 5 at all.

We believe that this methodology is compatible with Ecological Interface Design. It begins with users’ intentions, abilities, and limitations, with particular emphasis on use cases. The complex task domains addressed in the following two case studies do not allow the assumption that users will inevitably comply with any particular uses cases. For these two domains, we found that level of expertise highly influenced choices of how to use the system. We also think that level of engagement is a significant factor, which we return to in later discussion.

By the early 1990s, two decades before the iPad and other tablet devices, computer-generated displays were getting small and flat enough, and sufficiently lightweight, to entertain using them as portable documentation systems. Under contract to the U.S. Navy, Frey et al. (1992, 1993) researched this possibility for maintenance of the blade fold system of the SH-3 helicopter.

The standard documentation to support blade fold maintenance included large (11 × 17 inch) location and schematic diagrams. The initial idea was to put these on small screens with capabilities to pan and zoom. We felt, however, this would not take advantage of the display technology. It was easy to imagine users getting lost due to having to zoom in to make the displays readable and getting lost panning over the large, flat geography of the schematics.

There were three tasks that these displays were intended to support. Procedure following involved using fully proceduralized job performance aids that specified each step in the process in terms of what to look at and what to do. A second task was circuit tracing, which mainly involved tracing electrical circuits and locating test points. Most challenging was problem solving. This involved determining the operational symptoms of failures of certain devices, identifying “half-split” test points given failure symptoms, and troubleshooting failures for which there were no procedures or job aids.

As shown in Table 1
                     , these tasks were classified as thinking or doing tasks. Thinking tasks involved inference, deduction, interpretation, and decision making. Doing tasks required navigation, locating devices, components or test points, observation, and manipulation.

As an example, consider driving a car versus maintaining a car. Driving is usually a “doing” task involving following a map, identifying waypoints, and manipulating the gas pedal, brakes, and steering wheel. This task requires a visualization that represents the physical form of the environment, usually enabled by having windows in the car. In contrast, troubleshooting a failure of the car, e.g., will not start, is typically a “thinking” task. This involves inferring what influences starting, deducing the consequences of the lack of any of these influences, interpreting the observed symptoms in the context of these influences and deductions, and deciding on a course of action, e.g., use the lights to check battery strength or open hood to check connections.

A set of 45–60 computer-generated displays was designed to replace the traditional hard copy schematics. The number of displays made available depended on the experimental conditions, which varied across the five experiments discussed below. These displays can be classified in terms of the three-by-three abstraction-aggregation space shown in Table 2
                     . These displays were designed with the hypotheses in Table 1 in mind.

Five experiments were conducted involving 55 SH-3 maintenance personnel, specifically Aviation Electricians at the Jacksonville Naval Air Station. These electricians were classified in terms of experience level, either high or low. The characteristics of the five experiments and their overall results are summarized in Table 3
                     .

Key results can be summarized as follows.
                        1
                     
                     
                        1
                        These conclusions are based on a large set of statistical tests of a range of metrics. Many conclusions are based on multiple results, all satisfying the p < 0.05 criterion. Presentation of the results for all these metrics and their many interactions cannot be done within the scope of this paper.
                      In the first experiment, people used fewer computer-generated displays than traditional hardcopy schematics. Experiments two and three supported the hypothesis that thinking tasks benefit more from higher abstraction displays and that level of aggregation decreases as people home in on the failure (thinking task) or switch sought (doing task).

Experiment four addressed the poorer performance of inexperienced subjects in the earlier experiments. “How to use” training improved the performance of inexperienced subjects, but they still had difficulties with higher abstraction displays. The last experiment evaluated display enhancements identified during the previous four experiments. These enhancements were found to improve performance.

In general, thinking tasks benefitted more from the high abstraction displays, i.e., high abstraction displays were used much more for problem solving tasks versus procedure following and circuit tracing tasks. High abstraction displays provided greater benefits to the more experienced maintainers.

High aggregation displays were used most for procedure following tasks. Aggregation levels employed shifted to lower levels as task performance progressed. Lower levels of abstraction were inherently more useful for procedure following and circuits tracing as less abstract reasoning was required.

Some of the maintainers, particularly those with less experience, had some difficulty getting lost in the large numbers of displays in the hierarchy. Inexperienced maintainers also had more difficulty understanding the high abstraction displays. “How to use” training decreased these difficulties, especially for the less experienced personnel.

We have often found that complex interactive visualizations, as well as decision support in general, require training in their use. Not all interactive visualizations are inherently intuitively easy to understand and use. In one application, we had to provide aiding in the use of the aiding. This resulted in support akin to Obi-Wan Kenobi in Star Wars that monitored use of the aiding and suggested different behaviors when users were not taking advantage of the full capabilities of the aiding.

The display set designed for the SH-3 maintainers demonstrably improved their performance. Beyond this impact, the maintainers, their instructors, and their managers asked to keep the experimental displays once the experiments were completed. This required approval, which had its own complications. Nevertheless, we viewed this as a definite endorsement of using the abstraction-aggregation space as a framework for designing interactive visualizations.

Rasmussen's early research focused on troubleshooting of electronics (Rasmussen and Jensen, 1974). His research on this topic closely aligned with our research, prompting in-depth discussions around the swimming pool in Mati, Greece at the NATO Conference on Mental Workload in 1977. The result was a plan for a NATO Conference on Human Detection and Diagnosis of System Failures held in Roskilde, Denmark in 1980, as well as a subsequent book (Rasmussen and Rouse, 1981).

By that time, Jens and the first author had moved on to decision support for detection, diagnosis, and compensation of systems failures in power and process plants, as well as complex vehicle systems like airplanes, supertankers, and space shuttles. Subsequently, we focused on enterprises as systems (Rouse, 2005a, 2005b, 2006). This has led recently to the concept of enterprise diagnostics. The idea is simple – create an interactive visualization that enables executives and senior managers to assess whether or not their enterprise is functioning correctly. If it is not functioning correctly, provide functionality that enables users to determine why their enterprise is malfunctioning.

A recently published study (Liu et al., 2015) addressed the withdrawal of 12 car brands from the market during the 1930s, 1960s, and 2000s including the following cars:
                           
                              •
                              1930s: Cord, Duesenberg, LaSalle, Pierce Arrow

1960s: DeSoto, Packard, Rambler, Studebaker

2000s: Mercury, Oldsmobile, Plymouth, Pontiac

The study focused on why these cars were removed. Explanations were derived at four levels: automobile, company, industry, and economy. Interestingly, only one of the twelve decisions was driven primarily by the nature of the car. Other forces usually dominated.

Data sources included quantitative data such as production levels for each car, market segment, and industry wide. Quantitative data also included financial information, e.g., revenues and profits, for companies and the industry as a whole. Data included text sources such as the 
                           New York Times
                         archive, which contributed almost 100 articles published over the past 100 years on these vehicles. A variety of online sources were also accessed. There were also rich graphical components including, of course, picture of vehicles, but also pictures of executives, and graphical timelines.

We created an interactive visualization for enterprise diagnostics in the automobile industry context. The user's task is to determine why Brand X failed. To support users in performing this task, we designed an interactive visualization based on Rasmussen's abstraction-aggregation hierarchy.


                        Table 4
                         provides an abstraction-aggregation hierarchy for supporting enterprise diagnostics. These levels are rather different than Rasmussen's physical form, physical functions, generalized functions, abstract function, and functional purpose. This is due to the fact that the failures of these vehicles in the marketplace cannot be explained, with one exception, by the nature of the engineered system, i.e., the vehicle. The overall nature of the enterprise, defined broadly, is much more organizational and social than engineered. Nevertheless, we feel that Rasmussen's notion of an abstraction hierarchy is completely aligned with supporting enterprise diagnostics.


                        Table 5
                         provides mnemonics for each point in this hierarchy. The use case elaborated below is expressed using these mnemonics.

Each point in the abstraction-aggregation hierarchy may include:
                           
                              •
                              Visualizations of quantitative data, e.g., production, economic projections

Visualizations of geographic data, e.g., geographic markets

Visualizations of temporal data, e.g., company timelines

Visualizations of structural relationships, e.g., product assembly

Newspaper and magazine articles, e.g., interviews, products announcements

Financial statements, e.g., Q-1s and K-1s

Company product and process descriptions

Company strategic, tactical and operational plans

Photographs of management, factories, dealers, etc.

Problem statement: Brand X was removed from the market. Why did the company make this decision? Provide evidence to support your answer.

There are many ways to approach this question. One could search for articles at the company level, i.e., C-M and C-L, looking for direct answers to the question from company press releases, industry publications or the 
                           Wall Street Journal
                        . Such articles might, for example, attribute the withdrawal of brand X to decreasing sales. An article in 
                           Fortune
                         on General Motors stated the company's problem was loss of market share. Not much of an insight!

Liu (Liu et al., 2015) has shown that the causal chain can be traced back from the symptoms (withdrawal) to earlier decisions (investments, acquisitions, etc.). Thus, deeper answers are needed than “Brand X was withdrawn because it was not selling.” The goal is to support users to identify the reasons it was not selling. They also should be able to determine the source(s) of the reasons. How did the company get into this situation?

To find deeper answers, one might start at the product level, i.e., P-H, P-M, and P-L. How was brand X doing relative to competing brands? Were sales in this market increasing, flat, or decreasing across all companies? Causes of brand X sales decreasing in a decreasing market are likely very different from causes of decreasing sales in an increasing market. Is brand X losing and others winning, or is everybody losing?

If everybody is losing, one might then move from product to company to industry to economy, i.e., P-H to C-H, I-H and E-H, to determine why. If only brand X is losing, one would likely explore the company level, moving from C-L to C-M to C-H to see whether brand X is really the problem rather than the rest of the company. To determine if brand X is the problem in itself, one might move from C-L to I-L to see how it competes with other brands in the market.

If brand X is not the source of its own problems, one would dig more deeply, likely into the company level, i.e., C-M and C-H. One would look into company leadership and financial situations as potential reasons that brand X was sacrificed. It could be that product lines had to be trimmed and brand X was selected as the least painful alternative.

Looking at C-H could lead to I-H to determine if the company is having difficulties competing in general. Perhaps the bigger players with more resources are quickly absorbing every good idea, cloning them, and rolling them out in a big way. Thus, brand X may have been highly competitive until the bigger competitors swamped it.

Another path would arise from discovering that everyone is losing, but other companies are not withdrawing brands. They may be better managed and have deeper pockets, or they may have a strategy that requires sustaining all of its brands. Comparing industry to company, i.e., I-H to C-H, as well as I-M to C-M, would enable identification of such differences.

In summary, this example use case is as follows:
                           
                              •
                              User would start at P-L, then move to P-M and P-H

If everybody is losing, user would move to C-H, then I-H and E-H

o If others are not withdrawing brands, user would compare C-H to I-H, and then C-M to I-M to determine why and report that conclusion

If only brand X is losing, user would move to C-L, then C-M and C-H

o If comparison of C-L to I-L leads the user to conclude that brand X is the problem, user would report that conclusion

If brand X is not the problem, user would move to C-M and C-H

o If the company management or financial situation is the problem, user would report that conclusion

o If the company is not the problem, user would compare C-H to I-H, and then C-M to I-M to determine whether the competition is the problem and report that conclusion

In order to evaluate whether or not decision makers would actually leverage available data in this manner, we developed an interactive visualization that allowed users to explore a subset of the data types discussed in the idealized use case. Again the objective is to diagnose why a given car was withdrawn from the marketplace. Quantitative and textual data were organized by the four levels of abstraction described above. The visualization allows the user to explore the data by these levels of abstraction if so desired.

There are three phases of problem solving in the experiment discussed below. First, subjects receive their assignments as shown in Fig. 1
                        . One of the twelve cars is highlighted and an introduction to the case is provided. The subjects’ only task is to read the introduction and click the “Analyze” button when they are ready to begin. During the course of the experiment each subject addressed all twelve cars in a randomized order.

In the second phase, subjects see an interface designed to facilitate the analysis of the available data based on the abstraction hierarchy. As shown in Fig. 2
                        , subjects can select information at four levels of abstraction – automobile, company, industry, and economy. The information sources include production data (for the car of interest and the industry as whole), US economic data such as GDP and CPI, and published news articles. Information sources were identified as belonging to one or more layers of abstraction. A filter allowed the subject to reduce the data available in the display to one layer of abstraction at a time if so desired.

Subjects move information sources from the left of Fig. 2 to the right and bottom, where they accumulate evidence to subsequently review. They can remove evidence that does not prove useful. This process of selection, review, and retention or discarding of information provides data on where subjects move in the abstraction-aggregation space as illustrated in the above use case.

The interface design was strongly influenced by the work of Pirolli and Card (2005). They developed an analysis model that is intended to support an analyst as he or she forages for information and then makes sense of it. In the spirit of their model, the interface consists of three key components:
                           
                              •
                              
                                 Available Data Window – implements Pirolli and Card's “Shoebox” of potentially relevant data


                                 Article Evidence and Chart Evidence Windows – implements Pirolli and Card's “Evidence File” that allows users to separate out the evidence that supports reaching a conclusion


                                 Data Filter – allows the user to filter the available data by the level of abstraction

At some point, subjects will feel that they have enough evidence to make a decision, i.e., diagnose why the car was withdrawn from the market. This leads to the third phase where they provide their diagnoses on the screen shown in Fig. 3
                        . They provide a yes/no response to each of four factors within each of four levels of abstraction -- automobile, company, industry, and economy. Once this response is made, the subject is taken back to the screen in Fig. 1 for the next assignment.

Leveraging the interactive visualization, a user should be able to identify which of a set of potential factors determined by Liu et al. (2015) contributed to the outcome for a given car. We were interested in how subjects’ use of displays at different levels of abstraction and aggregation affect the speed and accuracy of their decisions. However, we did not expect subjects to necessarily think in terms of abstraction and aggregation. Instead, we expected them to simply use the information sources they find most useful for informing their decisions. Their focus would be on what went wrong rather than the fundamental nature of the information sources.

This experiment involved 10 faculty members and graduate students in science and engineering. Five of these subjects had a high level of expertise, for some of them due to having participated in the study of these cars. Five of the subjects had a low level of expertise in the topic.

Each of them solved the twelve problems in a semi-random order – 4 subjects started with the 1930s, 4 in the 1960s, and 2 in the 2000s. They each addressed the four cars in the assigned period before moving on to the next period. Thus they learned much about each era as they addressed the four cars from that era. The cars were addressed in alphabetical order, with two subjects starting with, for example, Cord, two with Duesenberg, two with LaSalle, and two with Pierce Arrow. After they completed their initial car they moved to the next one in the alphabetical order. Subjects that started with Pierce Arrow, for example, next addressed Cord. Subjects who first addressed the 1930s, then moved to the 1960s. In contrast, those that started with the 2000s, then moved to the 1930s, and so on.

Each subject's choices of information sources were captured, as were their final decisions. Speed was simply the time from selecting the assigned car until the decision was entered, although time was also partitioned into two segments as discussed below. Accuracy was measured by comparing subjects' decisions to “ground truth” from Liu et al. (2015). The measure of accuracy was fraction correct.

We also measured the use of the pre-defined levels of abstraction and aggregation levels of the information sources chosen. We hypothesized that these average levels would correlate with speed and accuracy, but also vary by car. More specifically, we expected that diagnosis of the reasons for some cars failing would be more difficult than others. Consequently, these more difficult diagnoses would require accessing more information sources and/or result in slower, less accurate diagnoses.

@&#RESULTS AND DISCUSSION@&#

In this section, we first quickly review overall performance results and then focus more deeply on the usage trajectories employed by subjects. Of particular interest is how usage varied for expert and non-expert subjects.

MANOVA results showed that accuracy varied by car (p < 0.01) and era (p < 0.01). There was a significant interaction of car or era with expertise (p < 0.01). In other words, the superior accuracy of subjects with high expertise was only the case for some cars or eras. The impact of expertise was more pronounced for the more recent cars, particularly those from the 2000s. Subjects with high expertise read more articles (p < 0.01).

The time required for each subject to complete each car was tracked. This time was decomposed into two components: time spent reading the introduction and time spent evaluating and analyzing the evidence. None of the time differences were statistically significant.


                     Fig. 4
                      shows the usage trajectories for each of the five expert subjects and twelve cars; Fig. 5
                      shows the trajectories for the five non-expert subjects. It is clear that expert subjects made fuller use of the information resources. Both sets of subjects moved similar numbers of articles into the Shoe Box (12% higher for experts), but expert subjects opened and read many more articles (226% higher for experts). Experts read 211% more articles at the Company level and 183% more at the Industry level. Thus, experts focused much more on reading articles in general, and reading more abstract articles in particular.

In summary, subjects with high expertise were more accurate and sought more information to support their decisions, but were not faster. They inherently employed information from higher levels of abstraction, in part because they sought much more information in general. This result is aligned with those for big graphics and little screens.

Subjects with less expertise had particularly low accuracy for the 2000s era. This may be due to the complexity of the economic situation during this period. The Great Recession dominated this period. However, the relentless globalization of the automobile industry was also pervasive. Consequently, the four cars removed from the market in this era were not just the victims of the Great Recession.

This may have confused the low expertise subjects. They had all personally experienced the 2000s era and knew, obviously idiosyncratically, much more about the era than was presented in the experiment. For example, they knew the four cars removed from the market, may have owned one or more of these cars, and may have read various articles concerning their demise.

In general, the better performers had quite different information seeking and utilization behaviors. This raises an interesting question that will be addressed in our next experiment. Can we provide training and/or aiding that will enable poor information seekers and utilizers to match the results of the top performers?

What would this training/aiding have to do? First, it would have to foster understanding of the meaning and use of information at all levels of abstraction. For example, an automobile can be popular, e.g., LaSalle, but unexpectedly disrupt the company's portfolio. An automobile can be a classic, e.g., Duesenberg, but far too expensive in a highly depressed economy.

More generally, failures at the lowest levels of abstraction – products/services – can be due to failures at higher levels of abstraction, e.g., management decision or industry norms. This has been found to be true in Rasmussen's classic domain of nuclear power as well as the domain studied here.

Beyond training and aiding, we think that user engagement plays a significant role. It can be difficult to get novices fascinated by the domain of study. For example, we studied a large class of pre-med students that played an online game that involved managing many patients. The majority of students became very engaged with the patient interactions and information resources available. These students were much better at choosing good courses of actions for their patients. Those who seemed to just want to get the game over made much poorer decisions (Basole et al., 2013).

To minimize lack of engagement, our next experiment will employ auto industry personnel as subjects. We still expect to find varying engagement, but this population will have more inherent interest in the topic. While our overall goal is not to design enterprise diagnostics support systems for novices, we do not want to design systems only usable by experts.

@&#CONCLUSIONS@&#

This paper first briefly reviewed the basics of visualization to provide grounding for the subsequent sections. The next section addressed the purposes of visualization, the object of design being the fulfillment of purposes. A visualization design methodology was then presented and illustrated with two examples -- one for helicopter maintenance and the other for enterprise diagnostics.

Rasmussen's abstraction-aggregation hierarchy provides an important framework for addressing such problems. Both experiments support the conclusion that levels of abstraction may be more useful and usable for users with higher levels of expertise. Nevertheless, some problems are not solvable at the lowest levels of abstraction. For example, simply focusing on the attributes of the twelve cars in the enterprise diagnostics experiment would only allow correctly diagnosing one case.

We need to better understand how to train and/or aid users to take full advantage of interactive visualizations based on Rasmussen's abstraction-aggregation hierarchy. This is not too difficult for high expertise users, but low expertise users need help to be able to think about their systems at multiple levels. Of course, the ability to do this may be one of the hallmarks of expertise.

@&#ACKNOWLEDGMENT@&#

The authors are pleased to acknowledge Dakota Wixom who performed the software development work for the enterprise diagnostics experiment.

@&#REFERENCES@&#

