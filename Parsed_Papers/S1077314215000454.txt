@&#MAIN-TITLE@&#Selection of optimized features and weights on face-iris fusion using distance images

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A novel system for face-iris fusion on distance images is proposed.


                        
                        
                           
                           Optimal features, feature extractors and weights are selected.


                        
                        
                           
                           Backtracking Search Algorithm and Particle Swarm Optimization are used.


                        
                        
                           
                           The proposed system outperforms state-of-the-art face-iris recognition systems.


                        
                        
                           
                           The proposed system is robust against spoofing attacks.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Multimodal biometrics

Particle Swarm Optimization

Backtracking Search Algorithm

Information fusion

Spoof attacks

@&#ABSTRACT@&#


               
               
                  The focus of this paper is on proposing new schemes based on score level and feature level fusion to fuse face and iris modalities by employing several global and local feature extraction methods in order to effectively code face and iris modalities. The proposed schemes are examined using different techniques at matching score level and feature level fusion on CASIA Iris Distance database, Print Attack face database, Replay Attack face database and IIIT-Delhi Contact Lens iris database. The proposed schemes involve the consideration of Particle Swarm Optimization (PSO) and Backtracking Search Algorithm (BSA) in order to select optimized features and weights to achieve robust recognition system by reducing the number of features in feature level fusion of the multimodal biometric system and optimizing the weights assigned to the face-iris multimodal biometric system scores in score level fusion step. Additionally, in order to improve face and iris recognition systems and subsequently the recognition of multimodal face-iris biometric system, the proposed methods attempt to correct and align the location of both eyes by measuring the iris rotation angle. Demonstration of the results based on both identification and verification rates clarifies that the proposed fusion schemes obtain a significant improvement over unimodal and other multimodal methods implemented in this study. Furthermore, the robustness of the proposed multimodal schemes is demonstrated against spoof attacks on several face and iris spoofing datasets.
               
            

@&#INTRODUCTION@&#

Currently, the identification and verification of human beings based on physical or behavioral characteristics is a trend in places with high security needs. In general, most biometric systems in the real time applications use a single biometric characteristic; unimodal biometric is suffered due to different factors such as lack of uniqueness, non-universality and noisy data [1]. For instance, variations in terms of illumination, pose and expression lead to degradation of face recognition performance [1]. Performance of iris recognition can be degraded in non-cooperative situations [2]. In order to solve the problem raised by the single trait, multimodality that is extracting information from multiple biometric traits can be applied as a remedy and ultimately causes to improve the performance of biometric systems.

In this study, face and iris biometrics are used to fuse the information because of many similar characteristics of these two modalities. Information fusion for multimodal biometric systems can be performed at four different levels: sensor level, feature level, matching score level and decision level fusion [1]. Due to the ease in accessing and combining the scores, matching score fusion level is more popular among all fusion levels and involves three different categories. The first category is Transformation-based score fusion where normalization of matching scores into a common domain is needed prior to combining due to incompatibility of different modalities feature set. Classifier-based score fusion is the second category that concatenates the scores from different systems. In fact, the scores from different classifiers are treated as a feature vector where each matching score is considered as an element of feature vector. Finally, the third category is Density-based score fusion that requires an explicit estimation of genuine and impostor matching score densities leading to an increase in implementation complexity [3]. Current researchers’ studies [3,4] can be used as an evidence to state that employing score fusion techniques such as Sum Rule and Weighted-Sum Rule with a proper score normalization method leads to an unprecedented improvement on unimodal biometric systems performance. On the other hand, feature level fusion [43,44] considers the original feature sets of different modalities and therefore contains richer information about the raw biometric data compared to matching score level fusion and may lead to performance improvement. However, concatenating the feature sets may cause high dimensionality problem and produces noisy or redundant data; consequently affecting the performance [5]. In this respect, feature selection can be considered as a solution to enhance the performance of biometric systems by selecting an optimized subset of features from the original feature set based on a certain objective function.

In this study, effect of several techniques in different fusion levels is investigated on the proposed schemes using face and iris modalities. Local and global feature extraction methods namely subpattern-based PCA (spPCA) [6], modular PCA (mPCA) [7], Local Binary Patterns (LBP) [8], Principal Component Analysis (PCA) [9] and subspace Linear Discriminant Analysis (LDA) [10] are employed on face images in this study. For iris recognition, a publicly available library implemented by Masek and Kovesi [11] is applied to extract iris features. In order to evaluate the proposed schemes, CASIA Iris Distance 
                     [12] database, Print Attack face database [38], Replay Attack face database [39] and IIIT-Delhi Contact Lens iris database [40] are used. CASIA-Iris-Distance images were captured by a high resolution camera, so both dual-eye iris and face patterns are included in the image region with detailed facial features for multimodal biometric information fusion [12]. The Print-Attack Replay Database consists of 200 video clips of printed-photo attack attempts to 50 clients, under different lighting conditions and 200 real-access attempt videos from the same clients [38]. On the other hand, Replay-Attack Database consists of 1300 video clips of photo and video attack attempts to 50 clients, under different lighting conditions [39]. IIIT-Delhi Contact Lens iris database consists of 6570 iris images pertaining to 101 subjects. Both left and right iris images of each subject are available in this database. Iris images are captured using two iris sensors namely; Cogent dual iris sensor and VistaFA2E single iris sensor [40]. In this study, as a unimodal system, all five local and global methods are applied on face images of the database and both left and right irises of the corresponding face image are considered to extract iris features by using Masek & Kovesi iris recognition system. As a multimodal biometric system, the proposed scheme involves consideration of all face and both left and right iris scores along with Particle Swarm Optimization (PSO) [13] and Backtracking Search Algorithm [41] to select the optimized subset of features and weights. In order to enhance the accuracy of face and iris unimodal and multimodal systems, face images are detected and aligned based on the center position of both left and right eyes. Indeed, by using the center positions, angle of head roll and iris rotation can be measured to align the face images and rotate back the iris patterns. Prior to fusion, tanh normalization [14,15] is applied on the face and iris scores to transfer the scores into a common domain and range. The fusion of the two modalities, face and iris, is then tested with a well known combination method namely Weighted Sum Rule [5]. The proposed schemes are also tested on several spoofing attacks to show the robustness of the multimodal fusion schemes. In general, spoofing attacks [45–47] include cheating on biometric traits in order to have unauthorized access to the biometric system. Since it is not needed to have any specific knowledge on the system for spoofing, such as the feature extraction or matching algorithm used, the chance of having a spoofing attack on the biometric system is high. Therefore, constructing a robust multimodal system against spoof attacks is very important for the security of biometric systems. There are several ways to spoof face images such as: (i) face spoofing through photograph, (ii) face spoofing through video or (iii) 3D face model or mask of a genuine user. The most common, the cheapest and the easiest way to spoof face images is face spoofing through photograph or video. Spoofing attacks through photograph, known as ‘photo-attacks’ consist of submitting a photograph of a legitimate user to the face recognition system displayed in hard copy or on the screen of a portable computer or smart phone [45–47]. On the other hand, there are different methods to spoof iris images also. Some of the ways for iris spoofing can be stated as (i) iris spoofing through pupil dilation, (ii) iris spoofing through textured contact lenses and (iii) iris spoofing through print attack [48].

The proposed face-iris multimodal scheme is presented and compared with the existing unimodal and multimodal biometric systems in this study using Receiver Operator Characteristics (ROC) curves and Genuine Acceptance Rate (GAR). GAR at false acceptance rate (FAR 0.01%) is used to demonstrate the verification performance and recognition rate is also used to show the identification performance.

The contribution of our work is to use left and right iris patterns with optimized features of local and global based facial feature extraction methods using PSO and BSA to remove redundant data for the fusion of face-iris multimodal system with tanh score normalization and Weighted Sum Rule fusion method where the weights are also optimized using PSO and BSA. The proposed scheme can be used practically in person identification and verification systems using facial images. The iris information from left and right eye can be extracted from the face image of the same individual and the fusion of face-iris multimodal system can be performed to improve the performance of the individual face and iris recognition systems. In fact, recognition is performed on fusion of face and both of the eyes’ iris patterns and therefore the verification becomes undisputable.

The organization of the paper is as follows. Section 2 describes unimodal biometric systems. Section 3 has an overview on the fusion of face and iris biometrics at feature level and score level fusion. The proposed schemes are explained in Section 4 while Section 5 is devoted to the database details, experiments and results. Finally, Section 6 draws some conclusions.

Face and iris biometrics are considered in this study to construct the structure of the unimodal and consequently multimodal system. Generally, face recognition and iris recognition are considered as one of the most attractive areas for biometric schemes in the past few years [30–36]. These biometrics are briefly described in the following subsections.

The common processing steps for unimodal face system are image preprocessing, training, testing and matching. The illumination effects of face images are reduced by applying histogram equalization (HE) and mean-and-variance normalization (MVN) [16] on facial images in preprocessing step. The facial features are then extracted in the training and testing stages to be examined by different techniques in matching score level fusion, feature level and/or combination of both fusion levels. Finally in the last step, in order to compute the matching score between train and test feature vectors, Manhattan distance measurement is applied.

The face databases used in different experiments of this study include several variations in images. Pose variations, illumination variations, facial expressions, distance images and occlusions due to glasses, mustache and beard are the variations that appear in the facial images. The availability of these variations on the facial databases used in this study is demonstrated in Table 1.
                         In addition, identification accuracies of the state-of-the-art methods used in this study such as PCA, LDA, spPCA, mPCA and LBP are shown in that comparative table on the facial databases used in this study. Identification accuracies of the state-of-the-art methods are also presented in Table 1 using FERET, BANCA and CASIA-Iris-Distance datasets of facial images. The accuracies are reported using 50 individuals with 2 train and 2 test images on FERET, BANCA datasets and 90 individuals with 5 train and 5 test images on CASIA-Iris-Distance dataset. ORL database includes 40 individuals which are all used in these experiments with 2 train and 2 test images for each individual. The same parameter settings related to the state-of-the-art feature extraction methods are used as in [20]. The results demonstrate that LBP achieves better recognition accuracy compared to the other state-of-the-art feature extractors on all datasets. The highest accuracies are obtained on ORL dataset and the lowest accuracies are reported on CASIA-Iris-Distance dataset due to the distance factor that exist in these images.

In this work, in order to improve the recognition performance of face images, AAM toolbox (Active Appearance Modeling) [17,37] is used to detect face images based on the center position of left and right irises. The toolbox aims to model and annotate human face images and obtain a precise location of facial features such as mouth, nose, eyes, and eyebrows. The precise center position of both irises is achieved by the toolbox and therefore we are able to measure the angle of head roll that may happen during acquisition of a face image. In fact, using the center positions and the measured angle, both eyes can be aligned in the face image.

In this study, in order to extract iris features, Libor Masek’s iris recognition system [11] is applied. The typical processing steps for an iris recognition system are segmentation, normalization, feature encoding, and feature matching [11,23,29]. The automatic segmentation system of Libor Masek’s iris recognition system is based on the Hough transform to localize the circular iris and pupil region, occluding eyelids and eyelashes, and reflections. The extracted iris region is then normalized into a fixed rectangular block. In feature encoding step, 1D Log-Gabor filters are employed to extract the phase information of iris to encode the unique pattern of the iris into a bit-wise biometric template. Finally, the Hamming distance measurement is employed for classification of iris templates [11].

Iris images include several variations such as illumination changes, occlusions due to eyelashes, eyelids and glasses, distance images, different noise factors such as reflections, contrast, luminosity, off angle, rotation, blurring and focus problems. These variations are summarized in Table 2 using the iris databases involved in the experiments of this study. The identification accuracies of the iris recognition system used in this study are also shown in Table 2 on three iris datasets namely CASIA-Iris-Distance-Left, CASIA-Iris-Distance-Right and UBIRIS. In the experiments, 50 individuals from each database with 2 train and 2 test images are used. The recognition accuracy achieved on UBIRIS dataset is better than the accuracies obtained on CASIA-Iris-Distance datasets because of the distance factor available in these images. Additionally, the iris images on CASIA-Iris-Distance datasets are extracted from the face images which decrease the quality of the iris images and consequently, the identification accuracy using these extracted iris images decreases.

On the other hand, the performance of iris recognition system is needed to be improved by using rotation of the iris patterns. In other words, during acquisition of a face image, the iris patterns may rotate frequently because of the user head roll to left or right shoulder that may lead to degrade iris recognition performance [18]. The rotation of face images and consequently iris images causes circular shifting of the iris features and therefore if the rotation angles of the irises are different, the extracted feature codes can be misaligned which affects the recognition accuracy [18]. In this respect, plenty of researches are conducted to solve this problem based on shifting the iris feature codes to perform matching. The authors in [18] proposed a new method by measuring the angle of head roll to shift the iris feature codes. In this study, we apply a similar algorithm as in [18] to improve the iris recognition accuracy.

Development of face-iris multimodal biometric system is one of the most significant steps in this study. This section describes the details of fusion techniques for face and iris modalities in different levels. Generally, fusing different modalities denotes an advantage to enhance the strength of the system especially in the case when one biometric trait of a person becomes defective. In the proposed scheme, feature level fusion and matching score level fusion is considered to authenticate the reality of a person.

In feature level fusion, concatenation of the original feature sets of face and iris modalities is considered and therefore this level of fusion contains richer information about the raw biometric data. In this study, involving five different local and global feature extractor methods to extract the original facial feature sets may lead high dimension vectors resulting to decrease the system performance. Therefore designing a scheme to retain the appropriate information from the fused features of the five algorithms, namely Face Feature Vector Fusion (Face-FVF), with the ability of solving the dimensionality problem has to be considered. In order to overcome the dimensionality problem, we applied two optimization methods separately in 2 different levels as depicted in Fig. 1
                         to select the optimized subsets of methods and features by removing the redundant and irrelevant data [1,13]. In the first proposed method, the well-known optimization technique named PSO [13] is used and then a more recent optimization technique named BSA [41] is applied in order to improve the system accuracy.

PSO that was introduced by Kennedy and Eberhart in 1995 [13] aims to find an optimized solution in a search space. It is initialized with a population of random solutions called particles to be evaluated using a fitness function. Each particle is treated as a point in an n-dimensional feature space. The ith particle is represented as xi
                        
                        =(xi
                        
                        1, 
                        xi
                        
                        2, 
                        …
                        , 
                        xin
                        ). The ability of PSO to memorize the best previous positions at each iteration causes to update each particle by two best values pbest and gbest. Pbest is the position giving the best fitness value of any particle which can be recorded and represented as pi
                        
                        =(pi
                        
                        1, 
                        pi
                        
                        2,…,
                        pin
                        ), where P is the size of the population. The index of the best particle among all the particles in the population is called gbest and represented as pg
                        . The velocity for ith particle is represented as vi
                        
                        =(vi
                        
                        1,
                        vi
                        
                        2,…,
                        vin
                        ). A particle’s new velocity is calculated according to its previous velocity and the distances of its current position from its own best position and from the group’s best experience. The particles are updated as follows [1,4,13]:
                           
                              (3.1)
                              
                                 
                                    
                                       
                                          
                                             
                                                v
                                                i
                                             
                                             =
                                             
                                                wv
                                                i
                                             
                                             +
                                             c
                                             1
                                             ×
                                             
                                             
                                                rand
                                                1
                                             
                                             
                                                (
                                                
                                                   
                                                      0.25
                                                      e
                                                      m
                                                   
                                                   
                                                      0
                                                      e
                                                      x
                                                   
                                                
                                                )
                                             
                                             
                                                (
                                                
                                                   p
                                                   i
                                                
                                                −
                                                
                                                   x
                                                   i
                                                
                                                )
                                             
                                             +
                                             c
                                             2
                                             ×
                                             
                                             
                                                rand
                                                2
                                             
                                             
                                                (
                                                
                                                   
                                                      0.25
                                                      e
                                                      m
                                                   
                                                   
                                                      0
                                                      e
                                                      x
                                                   
                                                
                                                )
                                             
                                             
                                                (
                                                
                                                   p
                                                   g
                                                
                                                −
                                                
                                                   x
                                                   i
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (3.2)
                              
                                 
                                    
                                       
                                          
                                             
                                                x
                                                i
                                             
                                             =
                                             
                                                x
                                                i
                                             
                                             +
                                             
                                                v
                                                i
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (3.3)
                              
                                 
                                    
                                       
                                          
                                             
                                                x
                                                i
                                             
                                             =
                                             
                                                {
                                                
                                                   
                                                      
                                                         
                                                            1
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               if
                                                               
                                                                  
                                                                     0.35
                                                                     e
                                                                     m
                                                                  
                                                                  
                                                                     0
                                                                     e
                                                                     x
                                                                  
                                                               
                                                               
                                                                  1
                                                                  
                                                                     1
                                                                     +
                                                                     
                                                                        
                                                                           e
                                                                        
                                                                        
                                                                           −
                                                                           
                                                                              v
                                                                              i
                                                                           
                                                                        
                                                                     
                                                                  
                                                               
                                                               >
                                                               
                                                                  rand
                                                                  3
                                                               
                                                               
                                                                  (
                                                                  
                                                                     
                                                                        0.25
                                                                        e
                                                                        m
                                                                     
                                                                     
                                                                        0
                                                                        e
                                                                        x
                                                                     
                                                                  
                                                                  )
                                                               
                                                            
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            0
                                                         
                                                      
                                                      
                                                         
                                                            otherwise
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where w is the inertia weight, c1 and c2 are acceleration constants and rand
                        1(), rand
                        2(), rand
                        3() are separate random numbers.

Generally, selecting a proper inertia weight provides a balance between global and local explorations and consequently results in finding sufficient optimal solution faster. The acceleration constants c1 and c2 are used to pull particle toward pbest and gbest. Indeed, employing an appropriate fitness function to optimize the problem in feature selection techniques such as PSO is an important issue. Due to measuring verification and identification performance of biometric systems in this study, the consideration of two different fitness functions in implementing PSO is needed. The fitness function corresponding to identification aims to maximize the recognition rate by computing the distance of a test sample against all the training samples to find the match scores and selecting the lowest distance value to check if it belongs to the same person or not. On the other hand, the verification fitness function considers the distance between training and testing samples to obtain match scores and then computes FAR and its corresponding GAR to maximize them by setting a threshold.

In this study, we set the inertia weight to 1, and the acceleration constants c1 and c2 both to 2 as in the original PSO [13]. Selection of features is based on a bit string of length M, where M is the number of feature extraction methods applied on face unimodal system in level 1 and number of features in level 2 that are taken from the selected feature extraction methods of level 1. In other words, every bit here represents one feature extraction method or one feature in the sense that; value ‘1’ means all features of corresponding feature extraction method are selected and ‘0’ means that they are not selected for level 1 PSO. In level 2 PSO, value ‘1’ means the feature of the corresponding feature extraction method taken from level 1 is selected and ‘0’ means that it is not selected. In our work for level 1 PSO, we assigned the size of the population and iteration as 6. In level 2 PSO, population and iteration size are set to 20 and 30 respectively.

On the other hand, Backtracking Search Algorithm (BSA) is a recent optimization algorithm applied on many numerical optimization benchmark problems. BSA is compared with six widely used algorithms including PSO and it is reported that BSA is more successful than these comparison algorithms [41]. BSA was introduced by Civicioglu in 2013 [41] as a new Evolutionary Algorithm for solving real-valued numerical optimization problems. The algorithm reduces the effects of problems encountered in Evolutionary Algorithms such as excessive sensitivity to control parameters, premature convergence and slow computation. BSA has a single parameter, a simple structure that is effective, fast and capable of solving multimodal problems and it can be adapted to different numerical optimization problems. It includes two new crossover and mutation operators for generating a trial population. BSA has memory that allows it to use experiences from previous generations for generating trial populations.
                        
                     

Backtracking Search Algorithm is a population-based iterative Evolutionary Algorithm and it has five processes namely initialization, selection-I, mutation, crossover and selection-II. The general structure of BSA algorithm is shown in Algorithm 1.

                           
                              
                                 
                                 
                                    
                                       
                                          Algorithm 1. General Structure of Backtracking Search Algorithm [41].
                                    
                                    
                                       1. Initialization
                                    
                                    
                                       
                                          repeat
                                       
                                    
                                    
                                       
                                          2. Selection-I
                                    
                                    
                                       
                                          
                                          Generation of Trial-Population
                                       
                                    
                                    
                                       
                                          
                                          3. Mutation
                                    
                                    
                                       
                                          
                                          4. Crossover
                                    
                                    
                                       
                                          
                                          end
                                       
                                    
                                    
                                       
                                          5. Selection-II
                                    
                                    
                                       
                                          Until stopping conditions are met
                                       
                                    
                                 
                              
                           
                        
                     

In this study, instead of using minimization which is the main goal of BSA, we modified the algorithm to perform maximization. Scaling factor F is set to 1, mix_rate is set to 1, population and iteration size for level 1 BSA is set to 6. In level 2 BSA, population and iteration size are set to 20 and 30 respectively. The stopping condition for level 2 BSA is set to maximum number of iteration, or obtaining the highest recognition rate or failing to update the last best solution after 200 evaluations. If one of these three conditions are satisfied, the algorithm stops. The size of the swarm for weight selection using BSA is set to 20 with maximum iteration of 100. The stopping condition is set to maximum number of iteration, or obtaining the lowest possible EER or failing to update the last best solution after 400 evaluations.

Matching score fusion level provides a rule to combine different scores. In this fusion method, different matchers may produce different scores such as distances or similarity measures with different probability distributions or accuracies [3]. Matching score level fusion can be considered as the classification of the scores into one of two classes, Accept/Reject, or combination of the scores to provide an individual scalar score [19]. Match score level fusion involves several simple or complicated algorithms to combine the scores such as Sum Rule, Weighted Sum Rule, Product Rule, classification using SVM and estimation of scores density. Similar and equivalent performance from the aforementioned combination methods are reported in the recent studies [3,5,20,21]. Involving match score level fusion for this work is arranged to combine the left and right irises of a certain person, facial scores achieved by individual feature extractors, facial scores obtained in the feature level fusion step using PSO in two different levels and finally it is used to combine fused face and iris scores.

In order to normalize the face and iris matching scores produced from Manhattan distance and Hamming distance, tanh normalization technique [14] is applied on the produced matching scores from face and iris images to transfer them into a common domain and range. Tanh normalization is a robust and efficient method that was introduced by Hampel et al. [15] and works very well for noisy training scores. Tanh normalization is represented as
                           
                              (3.4)
                              
                                 
                                    
                                       
                                          
                                             
                                                s
                                                
                                                   k
                                                
                                                ′
                                             
                                             =
                                             
                                                {
                                                tanh
                                                
                                                   (
                                                   0.01
                                                   
                                                      (
                                                      
                                                         
                                                            
                                                               s
                                                               k
                                                            
                                                            −
                                                            
                                                               μ
                                                               GH
                                                            
                                                         
                                                         
                                                            σ
                                                            GH
                                                         
                                                      
                                                      )
                                                   
                                                   )
                                                
                                                +
                                                1
                                                }
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where μGH
                         is the mean and σGH
                         is the standard deviation of the genuine score distribution [14].
                     

In this study we employ Sum Rule and Weighted Sum Rule techniques to combine the face and iris scores. Finding efficient weights to perform the experiments in both verification and identification modes is an important issue that can be effective for performance enhancement. Generally, Weighted Sum Rule is a method that can be used to compute combined matching scores of the individual matchers. Jain and Ross [22] have proposed to compute the weighted sum of scores from different modalities using user-specific weights. Usually, weights are computed using Equal Error Rate (EER), distribution of scores, quality of the individual biometrics or empirical schemes [4]. Weighted Sum Rule (ws) of different score matchers can be written as
                           
                              (3.5)
                              
                                 
                                    
                                       
                                          
                                             ws
                                             =
                                             
                                                w
                                                1
                                             
                                             ×
                                             
                                             
                                                s
                                                1
                                             
                                             +
                                             
                                                w
                                                2
                                             
                                             ×
                                             
                                             
                                                s
                                                2
                                             
                                             +
                                             ⋯
                                             +
                                             
                                                w
                                                n
                                             
                                             ×
                                             
                                             
                                                s
                                                n
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where w
                        1, 
                        w
                        2, …,
                        wn
                         are the assigned weights for different modalities and s
                        1, 
                        s
                        2, …, 
                        sn
                         are the scores obtained using individual biometric systems.

In this study, we consider the idea of using continuous PSO to select the optimized weights to have a better evaluation on the multimodal system. Generally, assigning the proper weights to the scores obtained using individual biometric systems may produce sufficient output to guarantee the improved performance in both verification and identification modes. Each particle, i, representing the weighting vector wi
                        
                        =(wi
                        
                        1, 
                        wi
                        
                        2, …,
                        win
                        ), is randomly initialized between 0 and 1 and then normalized with the constraint 
                           
                              
                                 ∑
                                 
                                    i
                                    =
                                    1
                                 
                                 k
                              
                              
                                 w
                                 i
                              
                              =
                              1
                           
                        , where k is the number of weights. The particle positions are updated using velocity and position functions according to continuous PSO technique. Fitness function is computed as a combination of EER’s of involved scores in the proposed scheme with the weights represented by the particle as in the following equation to be considered as an optimization problem for minimization.
                           
                              (3.6)
                              
                                 
                                    
                                       
                                          
                                             F
                                             
                                                (
                                                
                                                   ∑
                                                   
                                                      i
                                                      =
                                                      1
                                                   
                                                   k
                                                
                                                
                                                   w
                                                   i
                                                
                                                
                                                   EER
                                                   i
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where w
                        1,
                        w 
                        2,…,
                        wn
                         are the optimized weights for different modalities and EER
                        1,
                        EER
                        2,…,
                        EERk
                         (EERi
                        ) are the Equal Error Rates obtained using the corresponding scores in the proposed scheme. A reference database is used to test the proposed optimal weight selection part in order to choose the optimized weights and then the optimum weights are applied on our multimodal system. The size of the swarm in this study is set as 20 with maximum iteration of 100, inertia weight is 1, and the acceleration constants c1 and c2 are both set to 2.

                     

The structure of the proposed schemes is based on a face-iris multimodal biometric system using score-level fusion and feature level fusion. In fact, availability of sufficient information content and the ease in accessing and combining matching scores encouraged us to propose new schemes using matching score fusion techniques along with feature level fusion.

The fusion of face and iris can be done with only one of the irises (left or right) or combination of both irises. This leads to improve the multimodal biometric system performance especially whenever the fusion is done using face and information obtained from combined scores of both irises. Since combining the information of both left and right irises can improve the authentication accuracy even with the images with low quality [24], in the proposed schemes we consider the combination of two irises as demonstrated in Fig. 2. The score fusion of left and right irises is done using Weighted Sum Rule.

On the other hand, we performed feature concatenation on original face feature sets that may result dimensionality and redundancy problems and performance degradation. These kinds of problems can be alleviated using optimization algorithms such as PSO and BSA by selecting an optimized subset of features from original feature sets and removing the redundant and irrelevant data based on a certain objective function. Therefore, we applied two optimization algorithms separately in two different levels. In the first proposed scheme, the well-known PSO algorithm is used for optimization as in [4,42] on face and iris biometrics. Then, a new optimization technique named Backtracking Search Algorithm (BSA) [41] is used in the second proposed method to improve the accuracy of the first proposed method. In the first proposed method, we apply PSO in two different levels to select proper and optimized feature extractors and features. In addition, in order to combine face and iris in each PSO level, several fusion techniques are employed with left iris, right iris and combination of left and right irises as shown in Fig. 3.

Fusion of face and iris explained in the above schemes in Fig. 3 may help to enhance the performance of the overall multimodal biometric system although the need to improve the performance of the system motivated us to find a new design to fuse face and iris information. The proposed scheme considers the combination of facial feature level fusion scores in both levels along with all the facial scores achieved using the five aforementioned feature extractors of an individual to be fused with the combined scores of left and right iris of the same individual. It is needed to state that the combination of all facial scores obtained using the five local and global feature extractors (PCA, LDA, spPCA, mPCA, LBP) is done with Weighted Sum Rule. On the other hand, Weighted Sum Rule fusion is applied on the normalized face and iris scores that guided us to a better performance compared to unimodal and other existing multimodal systems in this study.

                     
                     
                     
                     
                     
                  

The proposed scheme considers the implementation of optimized weights using PSO with the fact that the proper weights are able to reflect the relative difference in unimodal systems performance compared to other fusion techniques [4]. The main idea of the proposed scheme is to fuse scores of all implemented algorithms and techniques on face and iris modalities to take the advantages of each technique on a specific modality for increasing the ability of the multimodal system. The block diagram of the proposed method is presented in Fig. 4. Four different weights are needed to fuse the face and iris scores using Weighted Sum Rule as shown in the block diagram of the proposed method in Fig. 4.

The steps involved in proposed scheme I are used in proposed scheme II, however, instead of using the well-known PSO technique, BSA is used which is a more recent optimization technique. The block diagram of the second proposed method is demonstrated in Fig. 5.

This section covers the detailed explanations about the database, experimental works and results on the unimodal and multimodal systems in order to show the robustness and strength of our proposed system compared to other existing techniques in this study.

The face system used in this study employs five local and global feature extraction methods namely subpattern-based PCA (spPCA), modular PCA (mPCA), Local Binary Patterns (LBP), Principal Component Analysis (PCA) and subspace Linear Discriminant Analysis (LDA) to examine the performance. Implementation of all feature extractors is done using MATLAB. The facial images in subpattern-based PCA and modular PCA are initially partitioned into N
                     2 subimages. Eigenvectors corresponding to 97% of eigenvalues with N
                     =81, where N is the number of partitions, are considered for both spPCA and mPCA. Each facial image before partitioning is resized in order to have equal size for each subimage. PCA algorithm is implemented as described in [9] based on the selection of maximum number of nonzero eigenvectors. In subspace LDA, in order to have dimensionality reduction, PCA is applied on facial images initially and then extracted principal components by PCA are used as inputs to LDA. The numbers of eigenvectors selected in the first and second stages of LDA method are selected as the maximum number of nonzero eigenvectors. For Local Binary Patterns (LBP), the number of partitions used is N
                     =81 as in spPCA and mPCA and (8,2) circular neighborhood is used.

In order to evaluate the performance of our multimodal system, a publicly available database named CASIA-Iris-Distance is used. CASIA-Iris-Distance images were captured by a high resolution camera, so both dual-eye iris and face patterns are included in the image region with detailed facial features for multimodal biometric information fusion [12]. The full database includes 142 subjects and a total number of 2567 images. Face images were acquired at-a-distance of ∼3m from camera [12]. In this study we consider 90 subjects to construct our multimodal face and iris biometric system. The chosen subjects cover the proper information needed for building the multimodal system including the whole face images and clear dual-eye iris patterns. Those images that are not selected do not include some parts of face components such as mouth, forehead or eyebrows. Randomly 10 samples for each subject or individual are selected, 5 samples for training and the rest 5 for testing. Additionally, in order to decide on the four optimized weights applied in the first proposed scheme using PSO technique, three different face and iris subsets are used. These include FERET [25] +UBIRIS [26] dataset with 50 individuals and 4 samples from each database, ORL [27] +UBIRIS dataset with 40 individuals and 4 samples from each database and BANCA [28] +UBIRIS dataset with 50 individuals and 4 samples from each database. These subsets are only employed for weight selection. The averaged optimized selected weights with PSO are as w
                        1
                        =0.10, w
                        2
                        =0.31, w
                        3
                        =0.24 and w
                        4
                        =0.35 where the number of particles is 20 and the dimension of the search space is 400. Similarly, whenever BSA is employed in the second proposed scheme, the average optimized weights become as w
                        1
                        =0.21, w
                        2
                        =0.26, w
                        3
                        =0.22 and w
                        4
                        =0.31. In general, preliminary results comparing state-of-the-art methods used in Section 2 of this study are obtained using all the databases mentioned above and the corresponding results were shown in Tables 1 and 2. The evaluation of the proposed fusion schemes (proposed scheme I and proposed scheme II) and the corresponding comparisons with the state-of-the-art methods are all performed on CASIA-Iris-Distance images and the results are demonstrated in Tables 3–10. Parameter tuning for PSO and BSA optimization techniques are performed on three datasets namely FERET+UBIRIS, ORL+UBIRIS and BANCA+UBIRIS datasets.

@&#EXPERIMENTS AND RESULTS@&#

In this study, the experiments are represented using ROC curves and GAR at FAR 0.01% as verification performance and also recognition rate and CMC curves as identification performance. We first describe the details of experiments performed in identification mode on unimodal and multimodal systems and then go into the details of verification mode.
                        
                     

Face and iris unimodal systems are used to measure the performance of individual systems. All global and local feature extraction methods, Face-Feature Vector Fusion (Face-FVF) with and without PSO, scores combination of all local and global methods using Weighted Sum Rule on facial images and Libor Masek’s iris recognition system together with approaches to combine left and right irises using Sum Rule and Weighted Sum Rule on iris images are used to carry out the experiments. PCA and LDA are global methods that are applied on the whole face images. SpPCA, mPCA and LBP are local feature extractors which are applied by partitioning the images into subregions.

In Face-FVF, all feature sets of all feature extraction methods are concatenated and then Manhattan Distance measurement is used to compare the images. The experiments are shown in Table 3 on original images without aligning face, eyes and without rotating iris patterns, as identification mode, where the best accuracy for face recognition is achieved using the local feature extractor LBP as 80.89% and using Face-FVF with level 2 PSO as 84.00% and combination of left and right irises with Weighted Sum Rule as 44.89% in Table 3. These experiments demonstrate that the fusion of face features/scores or iris scores separately leads to achieve better accuracy for unimodal face and iris systems.

                           
                        

Feature concatenation of some of the face extractors selected by level 1 PSO, which is LDA and LBP for this study, and corresponding features selected by level 2 PSO will improve the recognition accuracy of unimodal and consequently multimodal systems. Fusion of multimodal face and iris systems leads to a higher recognition accuracy compared to the unimodal biometric systems. In this study, we are motivated to use a feature extraction method from face biometric and the left/right or combination of two irises using Weighted Sum Rule of iris scores.

These set of experiments are demonstrated in Table 4 on original images without aligning face, eyes and without rotating iris patterns. In Table 4, fusion of unimodal face scores with the scores resulting from Masek’s iris code using Weighted Sum Rule is illustrated for left and right irises separately. The best result is obtained using LBP for unimodal face scores and left/right iris scores as 81.78% and 80.44%, respectively. Additionally, fusion of face unimodal scores with the scores resulting from Masek’s iris code using Weighted Sum Rule is illustrated for combined irises. In Table 4, the best result is obtained using LBP for face unimodal scores and combined left/right iris scores.

It is clearly shown that the combined left and right iris scores are able to improve the performance accuracy compared to the fusion with only one iris. The best result achieved is 86.44% as seen in Table 4
                           . In order to increase the recognition performance of our multimodal system, we continue to perform experiments for the fusion, however this time, not only with scores obtained using unimodal face or iris, but fused face scores/features are used to be combined with only one iris (left or right) and finally combination of left and right iris scores.

The best accuracies for both left and right irises on original images without aligning face, eyes and without rotating iris patterns as demonstrated in Table 5, are as 86.22% and 84.44% respectively. It can be easily seen that applying PSO to select the proper face feature extraction methods and then selecting the optimized subsets of features of these proper methods together with scores from left or right iris is able to improve the recognition accuracy.

We performed similar experiments with face scores/features and combined left and right irises to observe the effect of fusion on fused face and fused iris and attain their advantage. That is, combining the strength of each modality separately and then by using the available strength and intensity improves the accuracy of the system. As shown in the last column of Table 5, with considering the fused face scores and fused iris scores, the performance (88%) is enhanced compared to the previous experiments. According to the experiments, the proposed method outperforms all other unimodal and multimodal techniques implemented in this study. As demonstrated in Table 5, the proposed scheme involves the fused face scores from all five local and global feature extractors along with the scores obtained from level 1 PSO to select the optimal subsets of feature extractors and the scores achieved from level 2 PSO to choose the optimized subsets of features of the methods selected by level 1 PSO together with the fused left and right iris scores and it achieves the performance of 88.89%. This performance shows the significant improvements of 5.89% as compared to the case when only the best fused face features using level 2 PSO is used and 44% improvement as compared to the case when only fused iris scores using Weighted Sum Rule is used. It is needed to state that the achieved result for proposed method in this part is based on original images without aligning face, eyes and without rotating iris patterns.

The same set of experiments in identification mode are done on cropped and aligned face images and iris images based on the calculated head roll angle in Tables 6–8. As it is shown in the tables, we achieve a comparable result based on the calculated angle. The best accuracy is obtained using LBP feature extractor for face recognition as 90.77% and using Face-FVF with level 2 PSO as 92.77% and combined left and right irises using head roll angle rotation of feature codes as 77.65% in Table 6
                           . In addition, the fusion of aligned and cropped face information and rotated left or right irises or combined left and right irises can improve recognition performance as shown in Table 7 as 93.77%, 93.33% and 96% respectively.

The detected and rotated fused face scores and rotated left/right or fused iris scores are useful to enhance the recognition performance as demonstrated in Table 8. The best result using PSO method is achieved using Face-FVF with level 2 PSO and left, right or combined irises as 96%, 96.66% and 97.55%, respectively.

Proposed scheme I involving the rotated and fused face scores of all five local and global feature extractors and the scores obtained from level 1 PSO and also scores from level 2 PSO together with rotated and fused left and right iris scores achieves the best accuracy among all the experiments as 98% as shown in Table 8.

The results using BSA optimization algorithm are also presented in Table 8. Method selection with BSA presents slightly better results compared to the method selection with PSO. Whenever method selection and feature selection are performed with BSA, the accuracies are improved compared to PSO counterparts as shown in the table. Proposed scheme II which uses BSA for optimization and weight selection improves the recognition accuracy to 98.66% that is slightly better than the first proposed method. Therefore, using BSA instead of PSO achieves improved recognition accuracy for the fusion of face and iris biometrics.

On the other hand, Cumulative Match Characteristics (CMC) curves for unimodal and proposed multimodal systems on aligned and rotated face-iris images are demonstrated in Fig. 6 for proposed scheme I. The performance improvement of proposed scheme I is clearly shown in the curve. Additionally, CMC curves of several multimodal systems and proposed scheme I are compared in Fig. 7
                           . Proposed scheme I is slightly better than the other multimodal schemes in the first five rank levels.

                           
                        

The experiments conducted in verification mode consider proposed scheme I to demonstrate the difference between unimodal systems, multimodal systems and proposed scheme I. The best accuracy achieved for each experiment in the identification mode is used to repeat the experiments in verification context using ROC curves and GAR at FAR 0.01%. EER’s of the methods and techniques are presented separately in this section. As mentioned earlier, 450 training samples (90×5) and 450 testing samples (90×5) are considered in this study. Therefore 450 genuine scores and 40,050 (90×89×5) imposter matching scores are used to validate the verification performance analysis. The proposed face-iris multimodal system presented in the previous sections is compared with unimodal and other existing multimodal systems using ROC analysis. False Acceptance Rate (FAR) and False Rejection Rate (FRR) are used as a function of decision threshold which controls the tradeoff between these two error rates. The probability of FAR versus the probability of FRR is plotted for different values of decision threshold. Tables 9 and 10 demonstrate the verification performance (GAR) of some schemes implemented in this study at FAR=0.01%. Total Error Rates (TER) of the unimodal systems, multimodal systems and the proposed scheme are calculated and presented in Tables 9 and 10 as well to compare the results with the results of the existing systems. Total error rate is the sum of FAR and FRR, which is equal to twice the value of EER. Fig. 8 demon-strates the EER of proposed scheme I without cropping and rota-tion as 3.69%.

The best performance for the unimodal system is presented in Table 9 with GAR=87.33% at FAR 0.01% and TER=5.50% for face biometric whenever level 2 PSO is applied with alignment process. On the other hand, the proposed scheme obtains the performance of GAR=84.96% at FAR0.01% and TER=7.38% on non-aligned and non-rotated face and iris images that has 4.85% improvement compared to the best unimodal system performance as demonstrated in Table 10. The proposed method with alignment and rotation has the best performance of GAR=94.44% at FAR 0.01% and TER=3.78%.

The ROC analysis of unimodal systems and the proposed scheme on non-aligned and non-rotated face and iris images is demonstrated in Fig. 9. On the other hand, Fig. 10 demonstrates the ROC analysis of the proposed scheme and unimodal systems on aligned and rotated face and iris images. The best performance is obtained by the proposed scheme as shown in the figures.

The proposed multimodal schemes are tested on several face and iris spoofing datasets using real and spoof face and iris images. These experiments are performed to demonstrate the robustness of the proposed schemes against spoofing attacks. Several spoofing conditions are studied and the experimental results are presented in Tables 11–16
                           
                            under face spoofing only, iris spoofing only, both face and iris spoofing and no spoof conditions. The experiments are conducted on Print Attack face database, Replay Attack face database, IIIT-Delhi Contact Lens iris database with transparent attack and colored attack images. The results are presented on the tables for both proposed schemes (using PSO and BSA) with and without alignment of the irises on the images. Minimum Total Error Rates (TER) and Genuine Accept Rates (GAR) at 0.01% False Accept Rate (FAR) are shown in the tables. In order to test the robustness of proposed schemes under spoofing attacks, 50 individuals with 10 real access samples and 10 attack samples are extracted from 200 available video clips in Print Attack face database. Consequently, the same number of individuals, real access and attack samples are extracted from Replay Attack face database. Table 11 presents feature level fusion results of five feature extraction methods on unimodal face biometrics. Additionally, we prepared an iris dataset using IIIT-Delhi Contact Lens Iris database with 50 individuals, 10 real access samples for left and right irises and 10 attack samples dealing with Transparent lens for left and right irises and 10 attack samples for colored contact lens for left and right irises. Table 12 presents score level fusion results of left and right irises on unimodal iris biometrics.


                           Tables 11–16 present multimodal face and iris fusion results using two proposed schemes with and without alignment. Table 13 uses Print Attack face database and IIIT-Delhi Contact Lens iris database with transparent attack images while Table 14 presents the results on the same databases with colored attack iris images. Replay Attack face database is used for the experimental results demonstrated in Tables 15 and 16 with IIIT-Delhi Contact Lens iris database. Transparent attack on iris images is used for the experimental study reported in Table 15 and colored attack on iris images is used for the experimental study presented in Table 16. It is clear from these tables that in the presence of spoofing attacks, the total error rates of all considered scenarios are increased for the proposed schemes and Genuine Accept Rates of all considered scenarios are decreased. Therefore, the results show that the proposed multimodal and unimodal schemes are robust against spoofing attacks. In general, for validating the performance of all unimodal and multimodal schemes under spoofing attacks the whole database of 50 individuals is divided into two independent sets. The first set is considered for setting the parameters of PSO and BSA and the second set is used to test the algorithms. In total, 15 individuals are assigned to the first set for parameter setting. We assigned also 35 individuals out of 50 to the second set and then divided this set into two equal partitions providing 5 reference data and five testing data for each. The partitioning for reference and testing has been repeated 10 times without any overlapping. The whole database of 50 individuals is partitioned three more times into the above two sets due to small size of database. As a result, for Holdout cross-validation, totally we have 30 different runs. The results are averaged over 30 runs and reported as means and standard deviations.

@&#CONCLUSION@&#

Fusion of face and iris biometrics is presented using several feature extraction methods with score level and feature level fusion techniques. The proposed methods considered concatenation of different feature sets of local feature extractors, namely spPCA, mPCA and LBP; and global feature extractors such as PCA and LDA for face unimodal biometric systems using PSO in proposed scheme I and BSA in proposed scheme II in two levels. Face images are aligned to a horizontal line, detected and cropped. For iris recognition, a publicly available library was applied to extract iris features. Performance of iris recognition is improved by rotating the iris patterns. The rotation of face images and consequently iris images causes circular shifting of the iris features and therefore if the rotation angles of the irises are different, the extracted feature codes can be misaligned which affects the recognition accuracy. We improved the iris recognition by measuring the angle of head roll to shift the iris feature codes.

The proposed schemes involve consideration of all face and both left and right iris scores along with PSO and BSA to select the optimized subset of features and weights prior to fusion. Several techniques at matching score level and feature level fusion on CASIA Iris Distance database were examined with PSO and BSA techniques to optimize the weights and features. The proposed face-iris multimodal schemes were presented and compared with the existing unimodal and multimodal biometric systems in this study in both verification and identification modes. The main idea of the proposed schemes is to fuse scores of all implemented algorithms and techniques on face and iris modalities to take the advantages of each technique on a specific modality for increasing the ability of the multimodal system. The experimental results demonstrated that the proposed schemes outperformed all existing unimodal and multimodal systems implemented in this study in both verification and identification modes. Additionally, the proposed multimodal schemes are robust against spoofing attacks under all scenarios of attacks applied only on face images, only on iris images and on both face and iris images.

@&#REFERENCES@&#

