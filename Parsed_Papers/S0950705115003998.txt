@&#MAIN-TITLE@&#ODOC-ELM: Optimal decision outputs compensation-based extreme learning machine for classifying imbalanced data

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           The reason of the damage caused by class imbalance for ELM is analyzed in theory.


                        
                        
                           
                           The influence factors about the performance of ELM on skewed data are investigated.


                        
                        
                           
                           An optimal decision outputs compensation-based ELM called ODOC-ELM is presented.


                        
                        
                           
                           Exploring prior data distributions helps improve quality of ELM.


                        
                        
                           
                           Statistical results indicate the superiority of the proposed ODOC-ELM algorithm.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Extreme learning machine

Class imbalance learning

Decision outputs compensation

Prior data distributions

Optimization

@&#ABSTRACT@&#


               
               
                  Extreme learning machine (ELM) has been one widely used learning paradigm to train single hidden layer feedforward network (SLFN). However, like many other classification algorithms, ELM may learn undesirable class boundaries from data with unbalanced classes. This paper first tries to analyze the reason of the damage caused by class imbalance for ELM, and then discusses the influence of several data distribution factors for the damage. Next, we present an optimal decision outputs compensation strategy to deal with the class imbalance problem in the context of ELM. Specifically, the outputs of the minority classes in ELM are properly compensated. For a binary-class problem, the compensation can be regarded as a single variable optimization problem, thus the golden section search algorithm is adopted to find the optimal compensation value. For a multi-class problem, the particle swarm optimization (PSO) algorithm is used to solve the multivariate optimization problem and to provide the optimal combination of compensations. Experimental results on lots of imbalanced data sets demonstrate the superiority of the proposed algorithm. Statistical results indicate that the proposed approach not only outperforms the original ELM, but also yields better or at least competitive results compared with several widely used and state-of-the-art class imbalance learning methods.
               
            

@&#INTRODUCTION@&#

Extreme learning machine (ELM), which is a new learning paradigm for training single hidden layer feedforward network (SLFN), has become popular for solving classification problem due to its high classification accuracy and light computational requirements. Different from the traditional back-propagation (BP) algorithm [1], ELM does not need to tune the parameters of the hidden layer iteratively, but assigns them randomly. Meanwhile, in ELM, the output weights of SLFN are calculated by making use of a least-square method [2–5]. Specifically, in contrast with those conventional training algorithms, ELM is generally more robust as it places emphasis on achieving both the small norm of output weights and the least training errors. It has been found that ELM often provides similar or better classification accuracy at a much faster learning speed than classifiers including BP neural network (BPNN), support vector machine (SVM) and least-square support vector machine (LS-SVM) [2–4]. In recent years, ELM has also been widely adopted in various real-world problems, such as face recognition [6], fashion retailing forecasting [7], sales forecasting [8,9], hyperspectral image classification [10], electricity price forecasting [11], wind power generation forecasting [12], and bioinformatics [13,14].

Like other classifiers, on an imbalanced data set, ELM can produce an undesirable model that is biased toward the majority class and has a low performance on the minority class [15]. In other word, on imbalanced data, the decision boundary of ELM tends to be pushed towards the region of the minority class. Imbalanced data can be found almost everywhere, from biomedical applications [16] to network intrusion detection [17]. In recent years, a number of methods have been developed to deal with class imbalance problem, including resampling [18–23], cost-sensitive learning [24–26], decision boundary moving [27–29], ensemble learning [30–35], active learning [36] and one class classifier [37,38].

In the context of ELM, however, only a few work focused on the class imbalance problem. Zong et al. [15] profited from the idea of cost-sensitive learning to present a weighted ELM classifier (WELM). By designating different penalty factors for the training errors belonging to different categories, the performance of the minority classes could be highlighted. Zhang and Ji [39] presented a similar method called FELM, which changes the distributions of penalty factors by inserting a fuzzy matrix. However, they failed to provide a unified design rule for the fuzzy matrix. Xia et al. [40] proposed a kernel clustering-based possibilistic fuzzy ELM (PFELM) algorithm to deal with class imbalance problem, and found it performs better than FSVM-CIL algorithm [41]. Li et al. [42] provided a solution by embedding WELMs into a modified Boosting framework to automatically update the weights of the training samples, obtaining improved classification performance. Vong et al. [43] adopted a modified random oversampling method named prior duplication to promote the recognition rate of the level of suspended particulate matter. Sun et al. [44] integrated synthetic minority oversampling technology (SMOTE) [18] into a multiple ELMs framework to improve the prediction of corporate life cycle. Mirza et al. [45] proposed an ensemble algorithm of subset online sequential ELM (ESOS-ELM) to implement incremental class imbalance learning. In particular, a change detection mechanism is used in ESOS-ELM algorithm to detect concept drifts.

In this article, we first attempt to analyze the reason of the damage caused by class imbalance for ELM in theory. Also, we try to investigate the influence of several data distribution factors for the damage. Then, we would like to present an optimal decision output compensation-based ELM (ODOC-ELM) for promoting the classification performance of ELM in the scenario of class imbalance. ODOC-ELM can be regarded as a family member of decision boundary moving. In ODOC-ELM, the decision outputs of the minority classes are properly compensated based on the geometric mean (G-mean) performance metric, which pushes the decision boundary back towards the appropriate position of the majority class. For binary-class data, the problem of finding the optimal compensation value can be regarded as a single variable optimization problem, thus we adopt golden section search algorithm [46]. For multiclass data, there are multiple minority classes, thus it is a multivariate optimization problem. Particle swarm optimization (PSO) algorithm [47,48] is used to find the optimal combination of compensations. In addition, readers are also encouraged to replace them with other optimization algorithms in their practical applications.

To present the superiority of the proposed ODOC-ELM algorithm, it has been compared with some popular and state-of-the-art class imbalance learning algorithms on 30 binary-class imbalanced data sets and 12 multiclass imbalanced data sets randomly acquired from the Keel data repository [49]. The statistical results indicated that ODOC-ELM not only outperforms the original ELM, but also yields better or at least competitive results compared with several other methods.

The remainder of this paper is organized as follows. Section 2 provides some preliminary knowledge about ELM, including the basic ELM algorithm and three kinds of bias correction strategies in the context of ELM. Also, in Section 2, we also analyze the reason of the damage caused by class imbalance for ELM in theory, further investigate several data distribution factors which can influence on this damage. The proposed ODOC-ELM algorithm is described in Section 3 in detail. The experimental results and discussions are provided in Section 4. Finally, we conclude in Section 5.

In supervised learning, the learning algorithms always use a finite number of input-output instances for training. Suppose there are N arbitrary distinct training instances (x
                           i
                        , t
                           i
                        ) ∈ R
                           n
                         × R
                           m
                        , where x
                           i
                         is one n×1 input vector and t
                           i
                         is one m×1 target vector. If an SLFN with L hidden nodes can approximate these N samples with zero error, it then implies that there exist βi, ai
                         and bi
                        , such that:

                           
                              (1)
                              
                                 
                                    
                                       f
                                       L
                                    
                                    
                                       (
                                       
                                          x
                                          j
                                       
                                       )
                                    
                                    =
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       L
                                    
                                    
                                       
                                          β
                                          i
                                       
                                       G
                                       
                                          (
                                          
                                             a
                                             i
                                          
                                          ,
                                          
                                             b
                                             i
                                          
                                          ,
                                          
                                             x
                                             j
                                          
                                          )
                                       
                                       =
                                       
                                          t
                                          j
                                       
                                       ,
                                    
                                    
                                    j
                                    =
                                    1
                                    ,
                                    .
                                    .
                                    .
                                    ,
                                    N
                                 
                              
                           
                        where ai
                         and bi
                         denote the weight and bias of the ith hidden layer node, βi
                         is the weight vector connecting the ith hidden node to the output nodes. Then Eq. (1) can be written compactly as:

                           
                              (2)
                              
                                 
                                    H
                                    β
                                    =
                                    T
                                 
                              
                           
                        where

                           
                              (3)
                              
                                 
                                    
                                       
                                       
                                       
                                          
                                             H
                                             (
                                             
                                                a
                                                1
                                             
                                             ,
                                             .
                                             .
                                             .
                                             ,
                                             
                                                a
                                                L
                                             
                                             ,
                                             
                                                b
                                                1
                                             
                                             ,
                                             .
                                             .
                                             .
                                             ,
                                             
                                                b
                                                L
                                             
                                             ,
                                             
                                                x
                                                1
                                             
                                             ,
                                             .
                                             .
                                             .
                                             ,
                                             
                                                x
                                                N
                                             
                                             )
                                          
                                       
                                    
                                    
                                       
                                       
                                       
                                          
                                             
                                             =
                                             
                                                [
                                                
                                                   
                                                      
                                                         
                                                            G
                                                            (
                                                            
                                                               a
                                                               1
                                                            
                                                            ,
                                                            
                                                               b
                                                               1
                                                            
                                                            ,
                                                            
                                                               x
                                                               1
                                                            
                                                            )
                                                         
                                                      
                                                      
                                                         
                                                            .
                                                            .
                                                            .
                                                         
                                                      
                                                      
                                                         
                                                            G
                                                            (
                                                            
                                                               a
                                                               L
                                                            
                                                            ,
                                                            
                                                               b
                                                               L
                                                            
                                                            ,
                                                            
                                                               x
                                                               1
                                                            
                                                            )
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         ⋮
                                                      
                                                      
                                                         
                                                            .
                                                            .
                                                            .
                                                         
                                                      
                                                      
                                                         ⋮
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            G
                                                            (
                                                            
                                                               a
                                                               1
                                                            
                                                            ,
                                                            
                                                               b
                                                               1
                                                            
                                                            ,
                                                            
                                                               x
                                                               N
                                                            
                                                            )
                                                         
                                                      
                                                      
                                                         
                                                            .
                                                            .
                                                            .
                                                         
                                                      
                                                      
                                                         
                                                            G
                                                            (
                                                            
                                                               a
                                                               L
                                                            
                                                            ,
                                                            
                                                               b
                                                               L
                                                            
                                                            ,
                                                            
                                                               x
                                                               N
                                                            
                                                            )
                                                         
                                                      
                                                   
                                                
                                                ]
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (4)
                              
                                 
                                    β
                                    =
                                    
                                       
                                          [
                                          
                                             
                                                
                                                   
                                                      β
                                                      1
                                                      T
                                                   
                                                
                                             
                                             
                                                
                                                   ⋮
                                                
                                             
                                             
                                                
                                                   
                                                      β
                                                      L
                                                      T
                                                   
                                                
                                             
                                          
                                          ]
                                       
                                       
                                          L
                                          ×
                                          m
                                       
                                    
                                    
                                    and
                                    
                                    T
                                    =
                                    
                                       
                                          [
                                          
                                             
                                                
                                                   
                                                      t
                                                      1
                                                      T
                                                   
                                                
                                             
                                             
                                                
                                                   ⋮
                                                
                                             
                                             
                                                
                                                   
                                                      t
                                                      N
                                                      T
                                                   
                                                
                                             
                                          
                                          ]
                                       
                                       
                                          N
                                          ×
                                          m
                                       
                                    
                                 
                              
                           
                        
                     

Here, G(ai, bi, xj
                        ) denotes the activation function which is used to calculate the output of the ith hidden node for the jth training instance. In ELM, many nonlinear activation functions can be used, including sigmoid, sine, hardlimit and radial basis functions [2,3]. H is called hidden layer output matrix of the network, where its ith column denotes the ith hidden node's output vector with respect to inputs x
                        1, x
                        2...xN
                         and its jth row represents the output vector of the hidden layer with respect to input xj
                        . Fig. 1
                         provides the basic structure of a SLFN.

In SLFN, if the number of hidden nodes, L, is less than the number of training samples, N, and if the data distribution is sufficiently complex, then the training error cannot be made exactly zero but can approach a nonzero training error ɛ. ELM differs from other training algorithms in that the hidden node parameters ai
                         and bi are not tuned during training, but are instead assigned with random values according to any continuous sampling distribution 
                        [2,3]. Eq. (2) then becomes a linear system and the output weights β are estimated as:

                           
                              (5)
                              
                                 
                                    
                                       β
                                       ^
                                    
                                    =
                                    
                                       
                                          H
                                       
                                       †
                                    
                                    T
                                 
                              
                           
                        where H† is the Moore–Penrose generalized inverse of the hidden layer output matrix H. 
                           
                              
                                 
                                    H
                                 
                                 †
                              
                              =
                              
                                 
                                    (
                                    
                                       
                                          H
                                       
                                       T
                                    
                                    H
                                    )
                                 
                                 
                                    −
                                    1
                                 
                              
                              
                                 
                                    H
                                 
                                 T
                              
                           
                         if H
                           T
                        H is nonsingular or 
                           
                              
                                 
                                    H
                                 
                                 †
                              
                              =
                              
                                 
                                    H
                                 
                                 T
                              
                              
                                 
                                    (
                                    H
                                    
                                       
                                          H
                                       
                                       T
                                    
                                    )
                                 
                                 
                                    −
                                    1
                                 
                              
                           
                         if HH
                           T
                         is nonsingular. Here, 
                           
                              β
                              ^
                           
                         is the minimum-norm least squares solution of Eq. (2) 
                        [2].

ELM can also be derived in a regularized form for improved train-test generalization. The norm of the output weights ||β|| is closely related with the generalization ability of a neural network [50], ELM can be derived in a form that tries to minimize both 
                           
                              
                                 ∥
                                 H
                                 β
                                 −
                                 T
                                 |
                              
                              
                                 
                                    |
                                 
                                 2
                              
                           
                         and ||β||2 simultaneously, in which case Eq. (2) is replaced [3] by:

                           
                              (6)
                              
                                 
                                    
                                       
                                       
                                       
                                          
                                             
                                                Minimize
                                                :
                                             
                                             
                                             L
                                             
                                                p
                                                
                                                   E
                                                   L
                                                   M
                                                
                                             
                                             =
                                             
                                                1
                                                2
                                             
                                             
                                                ∥
                                                β
                                                |
                                             
                                             
                                                
                                                   |
                                                
                                                2
                                             
                                             +
                                             C
                                             
                                                1
                                                2
                                             
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                N
                                             
                                             
                                                
                                                   ∥
                                                
                                                
                                                   
                                                      ɛ
                                                   
                                                   i
                                                
                                                
                                                   |
                                                
                                                
                                                   
                                                      |
                                                   
                                                   2
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                       
                                          
                                             
                                                Subject
                                                to
                                                :
                                             
                                             
                                             h
                                             
                                                (
                                                
                                                   x
                                                   i
                                                
                                                )
                                             
                                             β
                                             =
                                             
                                                t
                                                i
                                             
                                             −
                                             
                                                
                                                   ɛ
                                                
                                                i
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    ɛ
                                 
                                 i
                              
                              =
                              
                                 [
                                 
                                    
                                       ɛ
                                    
                                    
                                       i
                                       ,
                                       1
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    
                                       ɛ
                                    
                                    
                                       i
                                       ,
                                       m
                                    
                                 
                                 ]
                              
                           
                         is the training error vector of the m output nodes corresponding to the training instance x
                           i
                        , C is the trade-off regularization parameter between the minimization of training errors and the maximization of the marginal distance, and h(xi
                        ) denotes the hidden layer output vector corresponding to the ith sample. The solution of Eq. (6) can be obtained based on the KKT theorem [51]. Given a new instance x, the output function of ELM is obtained [3] by:

                           
                              (7)
                              
                                 
                                    f
                                    
                                       (
                                       x
                                       )
                                    
                                    =
                                    
                                       {
                                       
                                          
                                             
                                                
                                                   h
                                                   
                                                      (
                                                      x
                                                      )
                                                   
                                                   
                                                      H
                                                      T
                                                   
                                                   
                                                      
                                                         (
                                                         
                                                            I
                                                            C
                                                         
                                                         +
                                                         H
                                                         
                                                            H
                                                            T
                                                         
                                                         )
                                                      
                                                      
                                                         −
                                                         1
                                                      
                                                   
                                                   
                                                      T
                                                      ,
                                                      when
                                                   
                                                   
                                                   N
                                                   <
                                                   L
                                                
                                             
                                          
                                          
                                             
                                                
                                                   h
                                                   
                                                      (
                                                      x
                                                      )
                                                   
                                                   
                                                      
                                                         (
                                                         
                                                            I
                                                            C
                                                         
                                                         +
                                                         
                                                            H
                                                            T
                                                         
                                                         H
                                                         )
                                                      
                                                      
                                                         −
                                                         1
                                                      
                                                   
                                                   
                                                      H
                                                      T
                                                   
                                                   
                                                      T
                                                      ,
                                                      when
                                                   
                                                   
                                                   N
                                                   ≥
                                                   L
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              f
                              
                                 (
                                 x
                                 )
                              
                              =
                              [
                              
                                 f
                                 1
                              
                              
                                 (
                                 x
                                 )
                              
                              ,
                              …
                              ,
                              
                                 f
                                 m
                              
                              
                                 (
                                 x
                                 )
                              
                              ]
                           
                         is the output vector. Then users may use the following equation to find out the predicted class label of x:

                           
                              (8)
                              
                                 
                                    l
                                    a
                                    b
                                    e
                                    l
                                    
                                       (
                                       x
                                       )
                                    
                                    =
                                    
                                       
                                          a
                                          r
                                          g
                                          m
                                          a
                                          x
                                       
                                       i
                                    
                                    
                                       f
                                       i
                                    
                                    
                                       (
                                       x
                                       )
                                    
                                    ,
                                    i
                                    ∈
                                    
                                       {
                                       1
                                       ,
                                       …
                                       ,
                                       m
                                       }
                                    
                                 
                              
                           
                        
                     

In this article, we adopt the regularized version of ELM which is described in Eq. (6).

Classifiers that pursue the minimization of training errors are apt to be destroyed by imbalanced class distributions, including Naïve Bayes classifiers [52], K nearest neighbors (KNN) classifiers [53], multilayer perceptrons (MLP) [54] and support vector machines (SVM) [29]. The fact is that the training errors often appear in the overlapping region between two different categories, and the desired classification boundary is the center of the class overlapping region. In this region, however, the number of instances belonging to the majority class is much more than that of the minority class. To guarantee the minimization of training errors, the minority class has to sacrifice more than the majority class. In this article, we try to analyze the reason of the damage caused by class imbalance for ELM and investigates the influence of several data distribution factors for this damage.

Without loss of generality, suppose the classification task is binary, and the target outputs of the minority class and majority class are assigned as 1 and −1, respectively. Consider a small and compact boundary region, there are N majority class instances and 1 minority class sample, where N is much more than 1. The words “small and compact” mean that all the instances in the described region have quite similar inputs, and the imbalance ratio (IR) is N: 1. Let's describe the feature vector of the minority class instance as x0 = (x01, x02,…, x0
                        
                           n
                        ), and the feature vectors of the majority class instances as x
                           i
                         = (x01 + Δx
                           i
                        
                        1, x02 + Δx
                           i
                        
                        2,…, x0
                        
                           n
                         + Δx
                           in
                        ), where 
                           
                              i
                              ∈
                              {
                              1
                              ,
                              2
                              ,
                              …
                              ,
                              N
                              }
                           
                         and Δx
                           ij
                         denotes a small either positive or negative deviation for the jth feature component of the ith instance in comparison with that of the instance x0. Also, we use Δx
                           i
                         = (Δx
                           i
                        
                        1, Δx
                           i
                        
                        2,…, Δx
                           in
                        ) to denote the deviation of the feature vector of the ith instance compared with the minority instance x0. Then based on Eq. (1), the outputs of these instances can be represented as:

                           
                              (9)
                              
                                 
                                    f
                                    
                                       (
                                       
                                          x
                                          j
                                       
                                       )
                                    
                                    =
                                    
                                       {
                                       
                                          
                                             
                                                
                                                   
                                                      ∑
                                                      
                                                         i
                                                         =
                                                         1
                                                      
                                                      L
                                                   
                                                   
                                                      
                                                         β
                                                         i
                                                      
                                                      G
                                                      
                                                         (
                                                         
                                                            a
                                                            i
                                                         
                                                         ,
                                                         
                                                            b
                                                            i
                                                         
                                                         ,
                                                         
                                                            x
                                                            0
                                                         
                                                         )
                                                      
                                                      ,
                                                   
                                                   if
                                                   j
                                                   =
                                                   0
                                                   
                                                
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      ∑
                                                      
                                                         i
                                                         =
                                                         1
                                                      
                                                      L
                                                   
                                                   
                                                      
                                                         β
                                                         i
                                                      
                                                      G
                                                      
                                                         (
                                                         
                                                            a
                                                            i
                                                         
                                                         ,
                                                         
                                                            b
                                                            i
                                                         
                                                         ,
                                                         
                                                            x
                                                            0
                                                         
                                                         +
                                                         Δ
                                                         
                                                            x
                                                            j
                                                         
                                                         )
                                                      
                                                      ,
                                                   
                                                   if
                                                   j
                                                   =
                                                   1
                                                   ,
                                                   …
                                                   ,
                                                   N
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Based on Eq. (9), we can obtain Δf(x
                           j
                        ) which is the variation of output between the jth majority instance x
                           j
                         and the minority instance x0 as follows:

                           
                              (10)
                              
                                 
                                    
                                       
                                          
                                             Δ
                                             f
                                             (
                                             
                                                x
                                                j
                                             
                                             )
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                L
                                             
                                             
                                                
                                                   β
                                                   i
                                                
                                                G
                                                
                                                   (
                                                   
                                                      a
                                                      i
                                                   
                                                   ,
                                                   
                                                      b
                                                      i
                                                   
                                                   ,
                                                   
                                                      x
                                                      0
                                                   
                                                   +
                                                   Δ
                                                   
                                                      x
                                                      j
                                                   
                                                   )
                                                
                                                −
                                                
                                                   ∑
                                                   
                                                      i
                                                      =
                                                      1
                                                   
                                                   L
                                                
                                                
                                                   
                                                      β
                                                      i
                                                   
                                                   G
                                                   
                                                      (
                                                      
                                                         a
                                                         i
                                                      
                                                      ,
                                                      
                                                         b
                                                         i
                                                      
                                                      ,
                                                      
                                                         x
                                                         0
                                                      
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                L
                                             
                                             
                                                
                                                   β
                                                   i
                                                
                                                
                                                   (
                                                   G
                                                   
                                                      (
                                                      
                                                         a
                                                         i
                                                      
                                                      ,
                                                      
                                                         b
                                                         i
                                                      
                                                      ,
                                                      
                                                         x
                                                         0
                                                      
                                                      +
                                                      Δ
                                                      
                                                         x
                                                         j
                                                      
                                                      )
                                                   
                                                   −
                                                   G
                                                   
                                                      (
                                                      
                                                         a
                                                         i
                                                      
                                                      ,
                                                      
                                                         b
                                                         i
                                                      
                                                      ,
                                                      
                                                         x
                                                         0
                                                      
                                                      )
                                                   
                                                   )
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Here, it is not difficult to observe that when the activation function G is a continuous function, such as sigmoid or rbf, meanwhile the deviation Δx
                           j
                        , the norm of hidden layer parameters ||a||, ||b|| and the norm of output weights ||β|| are all small enough, Δf(x
                           j
                        ) is a quite small real number, either positive or negative. In ELM, the hidden layer parameters are previously assigned with random values according to any continuous sampling distribution [2,3], while according to Eq. (6), ||β|| would reduce with the decrease of the trade-off regularization parameter C. That means in ELM, the sampling distribution and regularization parameter can be chosen to make two closely adjacent instances have quite similar outputs.

Suppose β has been determined, then the mean squared training errors of the subset Qsub
                         can be represented as:

                           
                              (11)
                              
                                 
                                    
                                       
                                          
                                             Q
                                             
                                                s
                                                u
                                                b
                                             
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                
                                                   (
                                                   f
                                                   
                                                      (
                                                      
                                                         x
                                                         0
                                                      
                                                      )
                                                   
                                                   −
                                                   1
                                                   )
                                                
                                                2
                                             
                                             +
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                N
                                             
                                             
                                                
                                                   (
                                                   f
                                                   
                                                      (
                                                      
                                                         x
                                                         0
                                                      
                                                      )
                                                   
                                                   +
                                                   Δ
                                                   f
                                                   
                                                      (
                                                      
                                                         x
                                                         i
                                                      
                                                      )
                                                   
                                                   −
                                                   
                                                      (
                                                      −
                                                      1
                                                      )
                                                   
                                                   )
                                                
                                                2
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                
                                                   (
                                                   f
                                                   
                                                      (
                                                      
                                                         x
                                                         0
                                                      
                                                      )
                                                   
                                                   −
                                                   1
                                                   )
                                                
                                                2
                                             
                                             +
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                N
                                             
                                             
                                                
                                                   (
                                                   f
                                                   
                                                      (
                                                      
                                                         x
                                                         0
                                                      
                                                      )
                                                   
                                                   +
                                                   Δ
                                                   f
                                                   
                                                      (
                                                      
                                                         x
                                                         i
                                                      
                                                      )
                                                   
                                                   +
                                                   1
                                                   )
                                                
                                                2
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                          =
                                       
                                       
                                          
                                             f
                                             
                                                
                                                   (
                                                   
                                                      x
                                                      0
                                                   
                                                   )
                                                
                                                2
                                             
                                             −
                                             2
                                             f
                                             
                                                (
                                                
                                                   x
                                                   0
                                                
                                                )
                                             
                                             +
                                             1
                                             +
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                N
                                             
                                             
                                                (
                                                f
                                             
                                             
                                                
                                                   (
                                                   
                                                      x
                                                      0
                                                   
                                                   )
                                                
                                                2
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                       
                                          
                                             +
                                             
                                             2
                                             f
                                             
                                                (
                                                
                                                   x
                                                   0
                                                
                                                )
                                             
                                             
                                                (
                                                Δ
                                                f
                                                
                                                   (
                                                   
                                                      x
                                                      i
                                                   
                                                   )
                                                
                                                +
                                                1
                                                )
                                             
                                             +
                                             Δ
                                             f
                                             
                                                
                                                   (
                                                   
                                                      x
                                                      i
                                                   
                                                   )
                                                
                                                2
                                             
                                             +
                                             2
                                             Δ
                                             f
                                             
                                                (
                                                
                                                   x
                                                   i
                                                
                                                )
                                             
                                             
                                                +
                                                1
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

To minimize the training errors for the subset, we set the quantity to zero:

                           
                              (12)
                              
                                 
                                    
                                       
                                          
                                             
                                                ∂
                                                
                                                   Q
                                                   
                                                      s
                                                      u
                                                      b
                                                   
                                                
                                             
                                             
                                                ∂
                                                f
                                                (
                                                
                                                   x
                                                   0
                                                
                                                )
                                             
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             2
                                             f
                                             
                                                (
                                                
                                                   x
                                                   0
                                                
                                                )
                                             
                                             −
                                             2
                                             +
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                N
                                             
                                             
                                                (
                                                2
                                                f
                                                
                                                   (
                                                   
                                                      x
                                                      0
                                                   
                                                   )
                                                
                                                +
                                                2
                                                
                                                   (
                                                   Δ
                                                   f
                                                   
                                                      (
                                                      
                                                         x
                                                         i
                                                      
                                                      )
                                                   
                                                   +
                                                   1
                                                   )
                                                
                                                )
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                (
                                                2
                                                N
                                                +
                                                2
                                                )
                                             
                                             f
                                             
                                                (
                                                
                                                   x
                                                   0
                                                
                                                )
                                             
                                             +
                                             2
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                N
                                             
                                             
                                                Δ
                                                f
                                                (
                                                
                                                   x
                                                   i
                                                
                                                )
                                             
                                             +
                                             2
                                             N
                                             −
                                             2
                                          
                                       
                                    
                                 
                              
                           
                        
                     

That means the solution for the sub-optimization problem is:

                           
                              (13)
                              
                                 
                                    f
                                    
                                       (
                                       
                                          x
                                          0
                                       
                                       )
                                    
                                    =
                                    
                                       
                                          1
                                          −
                                          N
                                          −
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             N
                                          
                                          
                                             Δ
                                             f
                                             (
                                             
                                                x
                                                i
                                             
                                             )
                                          
                                       
                                       
                                          N
                                          +
                                          1
                                       
                                    
                                 
                              
                           
                        
                     

If 
                           
                              
                                 ∑
                                 
                                    i
                                    =
                                    1
                                 
                                 N
                              
                              
                                 Δ
                                 f
                                 (
                                 
                                    x
                                    i
                                 
                                 )
                              
                           
                         is small enough, f(x0) tends to output a negative value. Moreover, we found that with the increase of the imbalance ratio N, the output value gradually approaches to −1. Therefore, it is not difficult to understand why the classification boundary is usually pushed towards the region of the minority class in ELM.

Next, we wish to investigate the influence of several data distribution factors for ELM. According to Liu et al. [31] and our previous work [52], not all imbalanced classification tasks are harmful, and for those unharmful imbalanced classification tasks, adopting specific class imbalance learning methods might increase unnecessary temporal and/or spatial costs. In fact, the damage is related to multiple potential data distribution factors, including class overlap, imbalance ratio, the size of training instances, noisy data and small disjunctions [52,55–57]. Here, we investigate the influences of the first three factors for ELM.

Suppose there is a two-dimensional binary-class data set, and both classes satisfy different Gaussian distributions. Fig. 2
                         presents the classification boundaries trained by ELM (L = 10, C = 10, sigmoidal G function) with different class overlap proportions, imbalance ratios and the total number of training instances. Fig. 2(a1) shows that when there is a large margin between two classes’ instances, ELM is not damaged by imbalanced class distributions. When class overlap proportion increases, the classification boundary gradually move towards the region of the minority class (see Fig. 2(a2) and (a3)), eventually reaching a situation in which all minority instances are misclassified (Fig. 2(a4)). Fig. 2(b1)–(b4) shows that the higher the imbalance ratio is, the more severe the damage is. If the data set is approximately balanced, there exist almost equal number of instances for both classes in the overlapping region, therefore the risk of misclassification is shared almost equally by both classes. The number of training instances is another important impact factor for ELM. Fig. 2(c1)–(c4) show that an increase in the number of training instances is helpful for improving the recognition accuracy of the minority class.

Besides the three factors shown in Fig. 2, if there exists some noisy information and small disjunctions, the quality of the trained ELM model would further decline. Therefore, for ELM, the damage of class imbalance can be caused by a combination of multiple factors. The appropriate methods and parameters should be determined by investigating the actual data distributions.

Based on Eq. (13), we analyze three kinds of bias correction strategies for ELM in the scenario of class imbalance.

Resampling is a family of algorithms which is widely used to solve the class imbalance problem. As its name indicates, resampling either increases some instances for the minority class or takes away some samples from the majority class. The former is called oversampling, while the latter is called undersampling. The simplest resampling strategies are random oversampling (ROS) and random undersampling (RUS). ROS adds instances by randomly duplicating the existing minority samples, thus it often overfits the data. RUS randomly removes some majority instances to re-balance the training set, causing some information loss. To overcome the drawbacks of ROS and/or RUS, more complicated resampling strategies have been developed. Synthetic minority oversampling technique (SMOTE), proposed by Chawla et al. [18], is the most widely used resampling strategy. Different from the instance duplication used in ROS, SMOTE generates each synthetic minority instance by interpolating between two existing minority samples. Besides RUS, ROS and SMOTE, other resampling strategies include random walk over-sampling (RWOS) [20], MLSMOTE [21], clustering-based undersampling [58] etc.

Resampling has little dependence on the used classifier, as it occurs in the procedure of data preprocessing. Therefore, any resampling strategy can be combined with ELM to deal with a class imbalance problem. After resampling, the distribution of the original boundary instances changes, which causes N in Eq. (13) to approach 1, thus the risk of the misclassification is shared equally by both classes.

Weighted extreme learning machine (WELM) proposed by Zong et al. [15] is also an effective method to cope with the problem of imbalanced data classification. WELM can be regarded as a cost-sensitive learning method, as it offers a larger penalty to the training errors of the minority instances than to those of the majority ones. In WELM, Eq. (6) is rewritten as:

                              
                                 (14)
                                 
                                    
                                       
                                          
                                          
                                          
                                             
                                                
                                                   Minimize
                                                   :
                                                
                                                
                                                L
                                                
                                                   D
                                                   
                                                      E
                                                      L
                                                      M
                                                   
                                                
                                                =
                                                
                                                   1
                                                   2
                                                
                                                
                                                   ∥
                                                   β
                                                   |
                                                
                                                
                                                   
                                                      |
                                                   
                                                   2
                                                
                                                +
                                                C
                                                
                                                   1
                                                   2
                                                
                                                
                                                   ∑
                                                   
                                                      i
                                                      =
                                                      1
                                                   
                                                   N
                                                
                                                
                                                   (
                                                   
                                                      W
                                                      
                                                         i
                                                         i
                                                      
                                                   
                                                   
                                                      ×
                                                      ∥
                                                   
                                                   
                                                      
                                                         ɛ
                                                      
                                                      i
                                                   
                                                   
                                                      |
                                                      
                                                         
                                                            |
                                                         
                                                         2
                                                      
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                          
                                          
                                             
                                                
                                                   Subject
                                                   to
                                                   :
                                                   
                                                   h
                                                   (
                                                
                                                
                                                   x
                                                   i
                                                
                                                )
                                                β
                                                =
                                                
                                                   t
                                                   i
                                                
                                                −
                                                
                                                   
                                                      ɛ
                                                   
                                                   i
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where W is an N×N diagonal matrix associated with each training instance x
                              i
                           . Usually if x
                              i
                            comes from the minority class, the associated weight W
                              ii
                            is relatively larger than those corresponding to the instances from the majority class. WELM's solution to the problem can be understood by supposing that the weight of the minority class is M times more than that of the majority class, then Eq. (11) should be rewritten as:

                              
                                 (15)
                                 
                                    
                                       
                                          
                                             
                                                Q
                                                
                                                   s
                                                   u
                                                   b
                                                
                                             
                                          
                                          
                                             =
                                          
                                          
                                             
                                                M
                                                
                                                   
                                                      (
                                                      f
                                                      
                                                         (
                                                         
                                                            x
                                                            0
                                                         
                                                         )
                                                      
                                                      −
                                                      1
                                                      )
                                                   
                                                   2
                                                
                                                +
                                                
                                                   ∑
                                                   
                                                      i
                                                      =
                                                      1
                                                   
                                                   N
                                                
                                                
                                                   
                                                      (
                                                      f
                                                      
                                                         (
                                                         
                                                            x
                                                            0
                                                         
                                                         )
                                                      
                                                      +
                                                      Δ
                                                      f
                                                      
                                                         (
                                                         
                                                            x
                                                            i
                                                         
                                                         )
                                                      
                                                      −
                                                      
                                                         (
                                                         −
                                                         1
                                                         )
                                                      
                                                      )
                                                   
                                                   2
                                                
                                             
                                          
                                       
                                       
                                          
                                          
                                             =
                                          
                                          
                                             
                                                M
                                                
                                                   
                                                      (
                                                      f
                                                      
                                                         (
                                                         
                                                            x
                                                            0
                                                         
                                                         )
                                                      
                                                      −
                                                      1
                                                      )
                                                   
                                                   2
                                                
                                                +
                                                
                                                   ∑
                                                   
                                                      i
                                                      =
                                                      1
                                                   
                                                   N
                                                
                                                
                                                   
                                                      (
                                                      f
                                                      
                                                         (
                                                         
                                                            x
                                                            0
                                                         
                                                         )
                                                      
                                                      +
                                                      Δ
                                                      f
                                                      
                                                         (
                                                         
                                                            x
                                                            i
                                                         
                                                         )
                                                      
                                                      +
                                                      1
                                                      )
                                                   
                                                   2
                                                
                                             
                                          
                                       
                                       
                                          
                                          
                                             =
                                          
                                          
                                             
                                                M
                                                f
                                                
                                                   
                                                      (
                                                      
                                                         x
                                                         0
                                                      
                                                      )
                                                   
                                                   2
                                                
                                                −
                                                2
                                                M
                                                f
                                                
                                                   (
                                                   
                                                      x
                                                      0
                                                   
                                                   )
                                                
                                                +
                                                M
                                                +
                                                
                                                   ∑
                                                   
                                                      i
                                                      =
                                                      1
                                                   
                                                   N
                                                
                                                
                                                   (
                                                   f
                                                
                                                
                                                   
                                                      (
                                                      
                                                         x
                                                         0
                                                      
                                                      )
                                                   
                                                   2
                                                
                                             
                                          
                                       
                                       
                                          
                                          
                                          
                                             
                                                +
                                                
                                                2
                                                f
                                                
                                                   (
                                                   
                                                      x
                                                      0
                                                   
                                                   )
                                                
                                                
                                                   (
                                                   Δ
                                                   f
                                                   
                                                      (
                                                      
                                                         x
                                                         i
                                                      
                                                      )
                                                   
                                                   +
                                                   1
                                                   )
                                                
                                                +
                                                Δ
                                                f
                                                
                                                   
                                                      (
                                                      
                                                         x
                                                         i
                                                      
                                                      )
                                                   
                                                   2
                                                
                                                +
                                                2
                                                Δ
                                                f
                                                
                                                   (
                                                   
                                                      x
                                                      i
                                                   
                                                   )
                                                
                                                
                                                   +
                                                   1
                                                   )
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

To minimize the weighted training errors, we have:

                              
                                 (16)
                                 
                                    
                                       
                                          
                                             
                                                
                                                   ∂
                                                   
                                                      Q
                                                      
                                                         s
                                                         u
                                                         b
                                                      
                                                   
                                                
                                                
                                                   ∂
                                                   f
                                                   (
                                                   
                                                      x
                                                      0
                                                   
                                                   )
                                                
                                             
                                          
                                          
                                             =
                                          
                                          
                                             
                                                2
                                                M
                                                f
                                                
                                                   (
                                                   
                                                      x
                                                      0
                                                   
                                                   )
                                                
                                                −
                                                2
                                                M
                                                +
                                                
                                                   ∑
                                                   
                                                      i
                                                      =
                                                      1
                                                   
                                                   N
                                                
                                                
                                                   (
                                                   2
                                                   f
                                                   
                                                      (
                                                      
                                                         x
                                                         0
                                                      
                                                      )
                                                   
                                                   +
                                                   2
                                                   
                                                      (
                                                      Δ
                                                      f
                                                      
                                                         (
                                                         
                                                            x
                                                            i
                                                         
                                                         )
                                                      
                                                      +
                                                      1
                                                      )
                                                   
                                                   )
                                                
                                             
                                          
                                       
                                       
                                          
                                          
                                             =
                                          
                                          
                                             
                                                
                                                   (
                                                   2
                                                   N
                                                   +
                                                   2
                                                   M
                                                   )
                                                
                                                f
                                                
                                                   (
                                                   
                                                      x
                                                      0
                                                   
                                                   )
                                                
                                                +
                                                2
                                                
                                                   ∑
                                                   
                                                      i
                                                      =
                                                      1
                                                   
                                                   N
                                                
                                                
                                                   Δ
                                                   f
                                                   (
                                                   
                                                      x
                                                      i
                                                   
                                                   )
                                                
                                                +
                                                2
                                                N
                                                −
                                                2
                                                M
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

To make the quantity as zero, then the solution of Eq. (16) is:

                              
                                 (17)
                                 
                                    
                                       f
                                       
                                          (
                                          
                                             x
                                             0
                                          
                                          )
                                       
                                       =
                                       
                                          
                                             M
                                             −
                                             N
                                             −
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                N
                                             
                                             
                                                Δ
                                                f
                                                (
                                                
                                                   x
                                                   i
                                                
                                                )
                                             
                                          
                                          
                                             N
                                             +
                                             M
                                          
                                       
                                    
                                 
                              
                           
                        

Obviously, when the weight ratio M approximates the imbalance ratio N, the minority instance x0 tends to appear near the classification boundary. Therefore, WELM can effectively solve the class imbalance problem in the context of ELM. In the reference [15], Zong et al. provides two weighted strategies as follows:

                              
                                 (18)
                                 
                                    
                                       
                                          WELM
                                          1
                                       
                                       :
                                       
                                       
                                          W
                                          
                                             i
                                             i
                                          
                                       
                                       =
                                       1
                                       /
                                       #
                                       
                                          (
                                          
                                             t
                                             i
                                          
                                          )
                                       
                                    
                                 
                              
                           
                        

and

                              
                                 (19)
                                 
                                    
                                       
                                          WELM
                                          2
                                       
                                       :
                                       
                                       
                                          W
                                          
                                             i
                                             i
                                          
                                       
                                       =
                                       
                                          {
                                          
                                             
                                                
                                                   
                                                      0.618
                                                      /
                                                      #
                                                      (
                                                      
                                                         t
                                                         i
                                                      
                                                      )
                                                   
                                                
                                                
                                                   
                                                      
                                                         if
                                                         
                                                         #
                                                      
                                                      
                                                         (
                                                         
                                                            t
                                                            i
                                                         
                                                         )
                                                      
                                                      >
                                                      AVG
                                                      
                                                         (
                                                         
                                                            t
                                                            i
                                                         
                                                         )
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      1
                                                      /
                                                      #
                                                      (
                                                      
                                                         t
                                                         i
                                                      
                                                      )
                                                   
                                                
                                                
                                                   
                                                      if
                                                      
                                                      #
                                                      
                                                         (
                                                         
                                                            t
                                                            i
                                                         
                                                         )
                                                      
                                                      ≤
                                                      AVG
                                                      
                                                         (
                                                         
                                                            t
                                                            i
                                                         
                                                         )
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where #(t
                              i
                           ) and AVG(t
                              i
                           ) denote the number of samples belonging to class t
                              i
                            and the average number of examples for each class, respectively.

Besides resampling and weighting, decision boundary moving [27–29] is also an effective method to deal with class imbalance. As described above, a skewed instance distribution tends to push the classification boundary towards the minority class, thus the aim of decision boundary moving is to push the boundary back towards the appropriate position. Some previous work has explored its effectiveness and feasibility in the context of MLP and SVM. Zhou and Liu [27] proposed a method named threshold moving that first adopts 0–1 outputs to train MLP, then normalizes the sum of outputs as 1, and finally multiplies by the corresponding cost for the output of each class. They found that the threshold moving is resource-saving and relatively efficient in class imbalance tasks. Lin and Chen [28] considered boundary moving for SVM classifier, and presented a feasible computational formula which considers the class imbalance ratio in the training set. The method, however, often produces relatively lower classification performance than other learning strategies. Therefore, in our previous work, we have proposed an iterative optimization algorithm to automatically find the optimal position for the hyperplane of SVM [29]. Specifically, the algorithm satisfies both the needs of accuracy and generalization.

For ELM, boundary moving means adding a small positive real value ζ to the decision output of the minority class. Then Eq. (13) can be rewritten as:

                              
                                 (20)
                                 
                                    
                                       f
                                       
                                          (
                                          
                                             x
                                             0
                                          
                                          )
                                       
                                       =
                                       
                                          
                                             1
                                             −
                                             N
                                             −
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                N
                                             
                                             
                                                Δ
                                                f
                                                (
                                                
                                                   x
                                                   i
                                                
                                                )
                                             
                                          
                                          
                                             N
                                             +
                                             1
                                          
                                       
                                       +
                                       ζ
                                    
                                 
                              
                           
                        

If the compensated threshold is large enough, the output of the minority instance x0 becomes positive. Actually, the purpose of decision output compensation is to push the classification boundary back towards the majority class.

In this article, we focused on developing decision boundary moving strategy to deal with class imbalance problem in the context of ELM. According to Eq. (20), we know that in order to determine the optimal classification boundary, we need to find the optimal compensation threshold ζ. Optimality can be measured using the G-mean measure, which is computed by finding the largest G-mean value on the training set. G-mean is useful because, for imbalanced classification tasks, overall accuracy is not an appropriate performance measure any more. F-measure and G-mean can better represent the desired classification behavior than overall accuracy. F-measure and G-mean are functions of the confusion matrix as shown in Table 1.
                  

They are calculated as follows:

                        
                           (21)
                           
                              
                                 
                                    F
                                    −
                                    measure
                                    =
                                 
                                 
                                    
                                       2
                                       ×
                                       Precision
                                       ×
                                       Recall
                                    
                                    
                                       Precision
                                       +
                                       Recall
                                    
                                 
                              
                           
                        
                     
                     
                        
                           (22)
                           
                              
                                 
                                    G
                                    −
                                    mean
                                 
                                 =
                                 
                                    
                                       TPR
                                       ×
                                       TNR
                                    
                                 
                              
                           
                        
                     where Precision, Recall, TPR and TNR are further defined as:

                        
                           (23)
                           
                              
                                 Precision
                                 =
                                 
                                    TP
                                    
                                       TP
                                       +
                                       FP
                                    
                                 
                              
                           
                        
                     
                     
                        
                           (24)
                           
                              
                                 Recall
                                 =
                                 TPR
                                 =
                                 
                                    TP
                                    
                                       TP
                                       +
                                       FN
                                    
                                 
                              
                           
                        
                     
                     
                        
                           (25)
                           
                              
                                 TNR
                                 =
                                 
                                    TN
                                    
                                       TN
                                       +
                                       FP
                                    
                                 
                              
                           
                        
                     
                  

As shown in Eq. (22), G-mean is the geometric mean of recall score of the minority class and that of the majority class, and therefore, unlike overall accuracy, it cannot be high when the minority-class recall rate is low.

Returning to our discussions on setting a proper value of ζ, for a binary-class problem, if we gradually increase the value of ζ from 0 to a pre-assigned upper value, and use G-mean metric to assess their performances, the curve will tend to be U-shaped as follows: it first gradually rises until the peak value ζ
                     *, and then drops to a relatively small value again. The problem is an unconstrained optimization problem of single variable ζ. To rapidly find ζ
                     * which is the optimal value of ζ, we adopt the golden section search algorithm [46] for the optimization process. In addition, considering the target output is either 1 or −1, the value range is pre-assigned between 0 and 2. Fig. 3
                      shows an example using the abalone19 data set acquire from the Keel data repository [49] to present the variation of G-mean with the increase of compensated threshold on the training set (randomly extracted 80% instances) and testing set (the remaining 20% instances), respectively. Although there are some differences between the two curves, a nearly identical position of the peak value is observed. That means the optimal compensation threshold ζ
                     * obtained from the training set produces approximately optimal G-mean value on the testing set, too.

For a multi-class imbalance problem, the problem becomes more complex [59,60]. Except the class with the most instances, the outputs of all other classes need to be compensated by different thresholds. Therefore, it becomes a multivariate optimization problem. To solve this optimization problem, particle swarm optimization (PSO) algorithm [47,48] is adopted. PSO is a population-based stochastic optimization technique, inspired by the social behavior of bird flocking. Specifically, PSO is originally designed to address continuous optimization problem, thus it can be directly used to deal with our problem without any modifications. During the optimization process of PSO, each particle dynamically changes its position and velocity by recalling its historical optimal position (pbest) and observing the position of the optimal particle (gbest). On each round, the position of each particle is updated by:

                        
                           (26)
                           
                              
                                 {
                                 
                                    
                                       
                                          
                                             
                                                v
                                                
                                                   i
                                                   d
                                                
                                                
                                                   k
                                                   +
                                                   1
                                                
                                             
                                             =
                                             
                                                v
                                                
                                                   i
                                                   d
                                                
                                                k
                                             
                                             +
                                             
                                                c
                                                1
                                             
                                             ×
                                             
                                                r
                                                1
                                             
                                             ×
                                             
                                                (
                                                p
                                                b
                                                e
                                                s
                                                t
                                                −
                                                
                                                   x
                                                   
                                                      i
                                                      d
                                                   
                                                   k
                                                
                                                )
                                             
                                             +
                                             
                                                c
                                                2
                                             
                                             ×
                                             
                                                r
                                                2
                                             
                                             ×
                                             
                                                (
                                                g
                                                b
                                                e
                                                s
                                                t
                                                −
                                                
                                                   x
                                                   
                                                      i
                                                      d
                                                   
                                                   k
                                                
                                                )
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                x
                                                
                                                   i
                                                   d
                                                
                                                
                                                   k
                                                   +
                                                   1
                                                
                                             
                                             =
                                             
                                                x
                                                
                                                   i
                                                   d
                                                
                                                k
                                             
                                             +
                                             
                                                v
                                                
                                                   i
                                                   d
                                                
                                                
                                                   k
                                                   +
                                                   1
                                                
                                             
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     where 
                        
                           v
                           
                              i
                              d
                           
                           k
                        
                      and 
                        
                           v
                           
                              i
                              d
                           
                           
                              k
                              +
                              1
                           
                        
                      represent the velocities of the dth dimension of the ith particle in the kth round and the (k+1)st round, while 
                        
                           x
                           
                              i
                              d
                           
                           k
                        
                      and 
                        
                           x
                           
                              i
                              d
                           
                           
                              k
                              +
                              1
                           
                        
                      denote their positions, respectively. c
                     1 and c
                     2 are two nonnegative constants that are called acceleration factors, while r
                     1 and r
                     2 are two random variables in the range of [0, 1]. In this study, the size of particle swarm and the search times are both set as 50, as well c
                     1 and c
                     2 are both set to 1. Meanwhile, the position x is restricted in the range of [0, 2] and the velocity v is restricted between −1 and 1. Fig. 4
                      shows an example of the PSO evolution curve for a three-class data set balance acquired from the Keel data repository [49], where fitness denotes the G-mean metric. From Fig. 4, it can be observed that PSO helps find an approximately optimal combination of compensated thresholds in a limited search time.


                     Fig. 5
                      uses the balance data set as an example to present the variation of G-mean with the variation of the two compensated thresholds on the training set (randomly extracted 80% of instances) and testing set (the remaining 20% of instances), respectively (see Fig. 5). From Fig. 5, it is not difficult to observe that on both sets, the peak values emerge at similar positions, too.

According to what mentioned above, we provide a description for the procedure of the proposed ODOC-ELM algorithm as follows:


                     Input: The training set S, the testing set T, the activation function G, the number of hidden nodes L, trade-off regularization parameter C.


                     Output: The optimal compensated threshold(s), G-mean value on the testing set T.


                     Procedure:

                        
                           1.
                           Generate hidden-layer weights and biases randomly, and then train an ELM classifier on the training set S using Eq. (6);

Obtain the outputs of all training instances;

Adopt the golden section search algorithm for a binary-class problem and the PSO algorithm for a multiclass problem to find the optimal compensated threshold or combination of compensated thresholds, which corresponds to the largest G-mean value on S;

Record the optimal compensated threshold ζ
                              * for a binary-class problem or the optimal combination of compensated thresholds (ζ
                              1
                              *, ζ
                              2
                              *,…, ζm
                              
                              −
                              1
                              *) for a multiclass problem, where m denotes the number of classes;

Get the actual outputs of all samples on the testing set T using the trained ELM;

Compensate the outputs of instances belonging to the minority class(es) using the optimal compensated threshold(s) recoded in step 4;

Calculate and outputting G-mean value on the testing set T.

To present the effectiveness of ODOC-ELM algorithm and its differences with several popular class imbalance learning algorithms, we construct a binary-class artificial data set to show the classification boundaries of various algorithms. On the data set, the minority class has 100 instances which satisfy the Gaussian distribution with mean value μ
                     + = 0.7 and the variance σ = 0.3, while the majority class owns 1000 instances satisfying the Gaussian distribution with mean value μ
                     - = 0.3 and the variance σ = 0.3 (see Fig. 6(
                     a)). Fig. 6(b)–(h) present the classification boundaries trained by seven different algorithms, including ELM, WELM1, WELM2, RUS-ELM, ROS-ELM, SMOTE-ELM and the proposed ODOC-ELM. From Fig. 6, it is not difficult to observe that ELM can be severely destroyed by class imbalance, while all six other classification algorithms can effectively alleviate class imbalance. Different from other algorithms, ODOC-ELM neither undersamples, oversamples, nor re-weights the training set, thus if there are enough training instances, and no concept drift between training set and testing set, then ODOC-ELM can find a better classification boundary than the other algorithms.

@&#RESULTS AND DISCUSSIONS@&#

We compared the proposed ODOC-ELM algorithm with ten other algorithms on 30 binary-class data sets and nine other algorithms on 12 multiclass data sets randomly acquired from the Keel data repository [49]. Detailed information about these data sets are provided in Table 2
                         and Table 3
                        , respectively.

Ten compared learning algorithms on binary-class data sets are listed as follows:

                           
                              1.
                              ELM [3]: The original ELM learning algorithm without any modifications;

WELM1 [15]: The weighted ELM algorithm with using Eq. (18) to assign penalty weights for training instances;

WELM2 [15]: The weighted ELM algorithm with using Eq. (19) to assign penalty weights for training instances;

RUS-ELM [61]: The original ELM algorithm with preprocessing the training data using random undersampling technology;

ROS-ELM [61]: The original ELM algorithm with preprocessing the training data using random oversampling technology;

SMOTE-ELM [18]: The original ELM algorithm with preprocessing the training data using SMOTE technology, where the number of neighbors k in SMOTE is assigned as the default value 5;

RWOS-ELM [20]: The original ELM algorithm with preprocessing the training data using random walk oversampling technology;

BWELM [44]: Boosting weighted ELM algorithm with 50 base classifiers;

PFELM [40]: The kernel clustering-based possibilistic fuzzy ELM algorithm, where all parameters use the default ones;

SVM-OTHR [29]: Our previously proposed support vector machine-based optimized decision threshold adjustment strategy.

As PFELM algorithm is specifically designed for binary-class problem, it doesn't participate in the comparisons on multiclass data sets. However, SVM-OTHR algorithm is added into the comparison on multiclass data sets by adopting one-versus-one decomposition strategy [62]. Also, as we adopt the optimization technologies in our proposed method, to guarantee the impartiality of the comparisons, we use internal five-fold cross validation to determine the optimal sampling level for the four sampling algorithms on all binary-class data sets, where the optimal sampling level acquired from {1, 2,…, IR}. While on multiclass data sets, the sampling level is fixed as class imbalance ratio. In addition, all experiments are implemented in Matlab 2013a environment. ELM uses sigmoid function on the hidden layer, as it has been found performs well in the reference [3]. For each algorithm in the context of ELM, grid search based on five-fold cross-validation is adopted to find the best combination of number of hidden nodes 
                           
                              L
                              ∈
                              {
                              10
                              ,
                              20
                              ,
                              …
                              ,
                              200
                              }
                           
                         and trade-off factor 
                           
                              C
                              ∈
                              {
                              
                                 2
                                 
                                    −
                                    20
                                 
                              
                              ,
                              
                                 2
                                 
                                    −
                                    18
                                 
                              
                              ,
                              …
                              ,
                              
                                 2
                                 20
                              
                              }
                           
                        . For SVM-OTHR algorithm, rbf kernel is used and the optimal combination of parameters is also determined by grid search, where 
                           
                              C
                              ∈
                              [
                              
                                 2
                                 
                                    −
                                    2
                                 
                              
                              ,
                              
                                 2
                                 
                                    −
                                    1
                                 
                              
                              ,
                              …
                              ,
                              
                                 2
                                 10
                              
                              ]
                           
                        , 
                           
                              σ
                              ∈
                              [
                              
                                 2
                                 
                                    −
                                    6
                                 
                              
                              ,
                              
                                 2
                                 
                                    −
                                    5
                                 
                              
                              ,
                              …
                              ,
                              
                                 2
                                 5
                              
                              ]
                           
                        . Moreover, for each data set, we repeatedly conducted 50 random five-fold cross-validation experiments, and then provided the experimental results in the form of mean±standard deviation.

For performance estimation metrics, we adopted F-measure and G-mean to compare the performance of various learning algorithms. For a multiclass problem, the F-measure can be transformed to F-score metric [63]:

                           
                              (27)
                              
                                 
                                    
                                       F
                                       −
                                       score
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             m
                                          
                                          
                                             
                                                F
                                                −
                                                measur
                                             
                                             
                                                e
                                                i
                                             
                                          
                                       
                                       m
                                    
                                 
                              
                           
                        where F-measure
                           i
                         can be calculated by using the following equation:

                           
                              (28)
                              
                                 
                                    
                                       F
                                       −
                                       measur
                                    
                                    
                                       e
                                       i
                                    
                                    =
                                    
                                       
                                          2
                                          ×
                                          Precisio
                                          
                                             n
                                             i
                                          
                                          ×
                                          Recal
                                          
                                             l
                                             i
                                          
                                       
                                       
                                          Precisio
                                          
                                             n
                                             i
                                          
                                          
                                             +
                                             Recal
                                          
                                          
                                             l
                                             i
                                          
                                       
                                    
                                 
                              
                           
                        and G-mean metric can be calculated by:

                           
                              (29)
                              
                                 
                                    
                                       G
                                       −
                                       mean
                                    
                                    =
                                    
                                       
                                          (
                                          
                                             ∏
                                             
                                                i
                                                =
                                                1
                                             
                                             m
                                          
                                          
                                             Ac
                                             
                                                c
                                                i
                                             
                                          
                                          )
                                       
                                       
                                          1
                                          /
                                          m
                                       
                                    
                                 
                              
                           
                        where Acc
                           i
                         denotes the recall of the ith class.


                        Tables 4
                         and 5
                         show the experimental results on 30 binary-class data sets, where bold indicates the best result, underline indicates the second best and italic labels the worst one on each data set.

From Tables 4 and 5, we observe as follows:

                           
                              1.
                              For skewed data, especially for severely imbalanced data sets, the performance of ELM is quite poor. ELM has provided the lowest F-measure and G-mean values on 13 and 20 data sets, respectively. In contrast with ELM, ten other learning algorithms are able to improve the classification performance more or less.

In contrast with other algorithms, WELM2 and RUS-ELM lack stability, although they performed better than other algorithms on some data sets. WELM2 tends to excessively adjust the classification boundary towards the majority class, while RUS-ELM is apt to drop some important classification information, causing randomness of the learned classification boundary.

SMOTE-ELM and RWOS-ELM both perform a little better than ROS-ELM. It is not difficult to understand this result: ROS tends to make the classifier overfit as it only simply duplicates the original instances, while SMOTE and RWOS can both improve the generalization ability of the classifier by creating new synthetic instances. Furthermore, RWOS effectively enlarges the classification boundary of the minority class. In contrast with SMOTE, however, RWOS does not have obvious advantages.

As for two complicated weighted ELM algorithms, BWELM obviously outperforms PFELM. It is easy to explain this phenomenon: PFELM
                                 
                                  only considers the information of the original data distributions, but BWELM can concentrate on those difficult and fallible instances. The adoption of boosting ensemble learning framework can promote the generalization ability of the classifier to a large extent.

ODOC-ELM outperforms all other learning algorithms, as it has provided the best and the second best F-measure values on 7 and 9 data sets, as well presented the best and the second best G-mean values on 11 and 3 data sets, respectively. Compared to SVM-OTHR, ODOC-ELM can obviously improve the classification performance, as more complicated optimization technologies have been embedded within the ODOC-ELM algorithm. ODOC-ELM carefully investigates the real distribution of instances and subtly adjusts the classification boundary, thus if there are enough training data, it can provide an approximately optimal classification boundary.

To further estimate the performance of these learning algorithms, we also provide statistical results. Specifically, Friedman test [64,65] is used to detect statistical differences in a group of results, and Holm post-hoc test [66] is adopted to examine whether the proposed algorithm is distinctive among a 1×N comparison. The post-hoc procedure allows us to know whether a hypothesis of comparison of means could be rejected at a specified level of significance α. The adjusted p-value (APV) is also calculated to denote the lowest level of significance of a hypothesis that results in a rejection. Furthermore, we consider the average rankings of the algorithms in order to measure how good a method is with respect to its partners. This ranking is obtained by assigning a position to each algorithm depending on its performance on each data set. The algorithm which achieves the best accuracy on a specific data set will have the first ranking (value 1), then the algorithm with the second best accuracy is assigned rank 2, and so forth. This task is carried out for all data sets and finally an average ranking is calculated. The statistical results are provided in Table 6.
                     

From Table 6, we observe that ODOC-ELM has obtained the lowest rankingF and rankingG, thus it is the best algorithm. In addition, all APVF except BWELM are lower than a standard level of significance of α=0.05, meaning the null-hypothesis of equality can be rejected in these cases. While for APVG, we can state that ODOC-ELM is significant better than ELM, WELM2, RUS-ELM, ROS-ELM, RWOS-ELM, PFELM and SVM-OTHR at α=0.5, but we cannot say that ODOC-ELM is significant different from WELM1, SMOTE-ELM and BWELM, although it has obviously lower average rankings than both WELM1 and SMOTE-ELM algorithms. In addition, we have to admit that BWELM is also a competitive algorithm to address class imbalance problem.


                        Fig. 7 provides the optimal decision output compensation ζ
                        * on each data set acquired by ODOC-ELM algorithm. From Fig. 7, it is not difficult to observe that the optimal compensation strongly depends on the imbalance ratio of the data set, though there are several exceptions. As discussed in Section 2, imbalance ratio is an important factor influencing the classification performance of learning algorithms. For data sets with the same number of instances and class overlap ratio, the one with the highest imbalance ratio generally pushes the classification boundary farthest towards the minority class, thus it needs a relatively larger compensation.


                        Tables 7 and 8 provide the F-score and G-mean values of various learning algorithms on 12 multiclass data sets.

From these two tables, we observe that ODOC-ELM outperforms the other learning algorithms as it obtains the lowest average ranking on both F-score and G-mean metrics. Specifically, it has obtained the best F-score and G-mean results on 5 data sets, and second best F-score and G-mean results on 3 and 2 data sets, respectively.

In addition, we observe that on those data sets which are seriously destroyed by class imbalance, i.e., yeast, pageblocks, ecoli, lymphography and shuttle, ODOC-ELM can greatly improve the classification performance, while on those data sets which are only a little destroyed by class imbalance, ODOC-ELM can only produce slightly better or at least comparable classification performance compared with ELM.

Next, we found that on those data sets with relatively high imbalance ratio, RUS-ELM always performs worse than those oversampling algorithms, and even worse than original ELM. It is not difficult to understand this phenomenon: in this scenario, as RUS drops excessive useful information, the training process of the classifier would be extremely insufficient. As for oversampling series algorithms, it is obvious that using complex strategies often outperform adopting simple strategies, i.e., RWOS-ELM is better than SMOTE-ELM, while SMOTE-ELM is better than ROS-ELM. That means RWOS can effectively alleviate the effect of overfitting. Moreover, we observe that two weighted ELM algorithms perform rather poor on multiclass imbalanced data sets. BWELM, however, can promote their classification performance to a large extent, indicating ensemble learning is helpful for searching more elaborate and accurate classification boundary. In fact, BWELM performs better than SVM-OTHR which has been proved to be an efficient class imbalance learning algorithm in our previous work [29].

At last, we provide the optimal combination of compensation thresholds acquired by ODOC-ELM (see Table 9). It is clear that the classes with different number of training instances have been compensated by different thresholds. Regardless of whether there are multiple majority categories or multiple minority classes, ODOC-ELM can adaptively find the approximately optimal combination of thresholds.

The experimental results above have indicated that the proposed ODOC-ELM is an effective and efficient algorithm for solving the class imbalance problem. However, due to embedding an optimization search procedure in the algorithm, ODOC-ELM always needs to consume much more training time than those simple learning algorithms. Tables 10 and 11 provide the average running time of various learning algorithms on binary-class and multiclass data sets.

From Table 10, we can observe that the training time of ODOC-ELM is from several times to dozens of times more than that of ELM, WELM1 and WELM2. As the four sampling algorithms adopt internal cross validation to find the optimal sampling level, thus they often consumes more time than ODOC-ELM. Specifically, with the increase of class imbalance ratio, the gap between them will gradually enlarge. While for BWELM and PFELM, their time consumptions are always larger than that of ODOC-ELM, too. It is not difficult to understand the reason that BWELM needs to iteratively adjust weights for each instance, and PFELM consumes much time to acquire the optimal clustering results. At last, SVM-OTHR consumes quite similar time consumptions with ODOC-ELM in that although training SVM classifier is more time-consuming than training ELM classifier, SVM-OTHR adopts simpler optimization algorithm than ODOC-ELM, causing its running time is only related with the number of the initially misclassified instances belonging to the minority class. However, on binary-class data sets, ODOC-ELM only needs to solve single variable optimization problem which has quite small search space. While on multiclass data sets, with the enlargement of search space, ODOC-ELM generally needs to consume hundreds of times’ or even thousands of times’ running time than those simple algorithms (see Table 11). Its time complexity is even much higher than BWELM algorithm and SVM-OTHR algorithm.

During the experiments, we also found that the classification performance of ELM is relatively sensitive to the trade-off regularization parameter C. If we increase the value for the parameter C, the output weights ||β|| increase (see Eq. (6)), consequently causing the difference between two neighborhood instances’ outputs to enlarge (see Eq. (10)). In detail, the training of ELM can be regarded as a whole optimization process for all training instances, thus any two adjacent instances greatly influence each other, i.e., they have to output similar values. Unfortunately, in the class overlap region, the number of majority instances is much more than that of minority samples, thus to minimize the whole training error, the minority classes sacrifice more than the majority classes. However, if we increase the value of the parameter C, the output weights ||β|| increase, making the strength of influence between any two adjacent instances weaken. Therefore, increasing the value of parameter C helps to obtain more discriminative classification boundaries, though it is possible to make the classifier overfit.

In addition, different from resampling and cost-sensitive learning, our proposed ODOC-ELM algorithm is self-adapting, as it investigates the real distribution of training instances in detail. That means that as long as there are enough training data and there is no clear difference between the distribution of training instances and that of testing examples, ODOC-ELM can provide an approximately optimal classification result. Of course, if the two distributions have obvious distinctions, the classification boundary
                         acquired by ODOC-ELM would tend to overfit.

@&#CONCLUSIONS@&#

Extreme learning machine is one of the most popular learning paradigms in the machine learning field. However, it is sensitive to class imbalance distributions. This article has interpreted imbalance sensitivity in theory and reviewed some widely adopted solutions. Furthermore, we presented a decision output compensation algorithm called ODOC-ELM, which embeds an optimization procedure within itself. The optimal compensation thresholds are automatically found by the golden section search algorithm for binary-class problems, and by particle swarm optimization for multiclass problems. Of course, they can also be simply replaced by some other optimization algorithms. Experimental results indicate that ODOC-ELM outperforms many popular and state-of-the-art algorithms in terms of both F-measure and G-mean performance metrics. It is expected to be adopted in practical applications with class imbalance distributions in the near future.

The following research topics deserve further investigation:

                        
                           1.
                           As discussed in Section 4, the proposed ODOC-ELM algorithm is time-consuming, thus we also wish to improve the algorithm for further saving the time consumption.

When there are only a few training instances, ODOC-ELM cannot precisely estimate the real distributions, consequently causing the classifier to be overfitting. In this scenario, perhaps combining the results of several different class imbalance algorithms can further improve the classification performance.

One-class classifiers have been widely used to address class imbalance problem [37,38], so it may be worthwhile to investigate the possibility of adapting ELM as one-class classifier.

@&#ACKNOWLEDGMENTS@&#

This research was partially supported by National Natural Science Foundation of China under grant nos. 61305058, 61473086, 61471182, and 61572242, Natural Science Foundation of Jiangsu Province of China under grant nos. BK20130471 and BK20140566, China Postdoctoral Science Foundation under grant nos. 2013M540404, 2015T80481 and 2014M561586, Jiangsu Planned Projects for Postdoctoral Research Funds under grant no.1401037B, Nature Science Foundation of the Jiangsu Higher Education Institutes of China under grant no.13KJB520003, Qing Lan Project of Jiangsu Province of China, Fundamental Research Funds for the Jiangsu University (13JDG093), and open fund of Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education under grant no.MCCSE2013B01. We also wish to express our appreciation to several anonymous reviewers who have provided lots of valuable suggestions to improve the quality of this article.

@&#REFERENCES@&#

