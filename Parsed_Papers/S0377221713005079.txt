@&#MAIN-TITLE@&#Algorithmic aspects of mean–variance optimization in Markov decision processes

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           The mean and variance of the total reward in Markov decision processes are studied.


                        
                        
                           
                           Randomized or history-based policies can improve performance for these criteria.


                        
                        
                           
                           Computing an optimal policy under a variance constraint is shown to be NP-hard.


                        
                        
                           
                           Pseudopolynomial exact and approximation algorithms are proposed.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Markov processes

Dynamic programming

Control

Complexity theory

@&#ABSTRACT@&#


               
               
                  We consider finite horizon Markov decision processes under performance measures that involve both the mean and the variance of the cumulative reward. We show that either randomized or history-based policies can improve performance. We prove that the complexity of computing a policy that maximizes the mean reward under a variance constraint is NP-hard for some cases, and strongly NP-hard for others. We finally offer pseudopolynomial exact and approximation algorithms.
               
            

@&#INTRODUCTION@&#

The classical theory of Markov decision processes (MDPs) deals with the maximization of the cumulative (possibly discounted) expected reward, to be denoted by W. However, a risk-averse decision maker may be interested in additional distributional properties of W. In this paper, we focus on the case where the decision maker is interested in both the mean and the variance of the cumulative reward (e.g., trying to optimize the mean subject to a variance constraint or vice versa), and we explore the associated computational issues.

Risk aversion in MDPs is of course an old subject. In one approach, the focus is on the maximization of 
                        
                           E
                           [
                           U
                           (
                           W
                           )
                           ]
                        
                     , where U is a concave utility function. Problems of this type can be handled by state augmentation (e.g., Bertsekas, 1995), namely, by introducing an auxiliary state variable that keeps track of the cumulative past reward. In a few special cases, e.g., with an exponential utility function, state augmentation is unnecessary, and optimal policies can be found by solving a modified Bellman equation (Chung & Sobel, 1987). (The exponential utility function is often viewed as a surrogate for trading off mean and variance, on the basis of a single tunable parameter. The difficulty of solving mean–variance optimization problems—which is the focus of this paper—does provide some support for using a surrogate criterion, more amenable to exact optimization.) Another interesting case where optimal policies can be found efficiently involves a “one-switch utility functions” (the sum of a linear and an exponential) Liu and Koenig (2005), or piecewise linear utility functions with a single break point (Liu & Koenig, 2006).

In another approach, the objective is to optimize a so-called coherent risk measure (Artzner, Delbaen, Eber, & Heath, 1999), which turns out to be equivalent to a robust optimization problem: one assumes a family of probabilistic models and optimizes the worst-case performance over this family. In the multistage case (Riedel, 2004), problems of this type can be difficult (Le Tallec, 2007), except for some special cases (Iyengar, 2005; Nilim & El Ghaoui, 2005) that can be reduced to Markov games (Shapley, 1953).

Mean–variance optimization lacks some of the desirable properties of approaches involving coherent risk measures or risk-sensitive utility functions (e.g., exponential utility functions) and sometimes leads to counterintuitive policies. Bellman’s principle of optimality does not hold, and as a consequence, a decision maker who has received unexpectedly large rewards in the first stages, may actively seek to incur losses in subsequent stages in order to keep the variance small. Counterintuitive and seemingly “irrational” behavior (i.e., incompatible with expected utility maximization) can even arise in static problems under a mean–variance formulation: for example, under a variance constraint, one may prefer to forgo a profit which is guaranteed to be positive but has a positive variance. Nevertheless, mean–variance optimization is a common approach in financial decision making e.g., (Luenberger, 1997), especially for static (one-stage) problems. Consider, for example, a fund manager who is interested in the 1-year performance of the fund whose investment strategies will be judged according to the mean and variance of the return. Assuming that the manager is allowed to undertake periodic re-balancing actions in the course of the year, one obtains a Markov decision process with mean–variance criteria, and it is important to know the least possible variance achievable under a set target for the mean return. While the applicability of the financial strategies arising from mean–variance optimization in multi-period fund management can be debated (due to the “irrational” aspects mentioned above), mean–variance optimization is definitely a meaningful objective in various engineering contexts. Consider, for example, an engineering process whereby a certain material is deposited on a surface. Suppose that the primary objective is to maximize the amount deposited, but that there is also an interest in having all manufactured components be similar to each other; this secondary objective can be addressed by keeping the variance of the amount deposited small. In general, the applicability of the formulations studied in this paper will depend on the specifics of a particular application.

Mean–variance optimization problems resembling ours have been studied in the literature. For example, (Guo, Ye, & Yin, 2012) consider a mean–variance optimization problem, but subject to a constraint on the vector of expected rewards starting from each state, which results in a simpler problem, amenable to a policy iteration approach. Collins (1997) provides an apparently exponential-time algorithm for a variant of our problem, and Tamar, Di-Castro, and Mannor (2012) propose a policy gradient approach that aims at a locally optimal solution. Expressions for the variance of the discounted reward for stationary policies were developed in Sobel (1982). However, these expressions are quadratic in the underlying transition probabilities, and do not lead to convex optimization problems. Similarly, much of the earlier literature (see Kawai (1987), Huang & Kallenberg (1994) for a unified approach) on the problem provides various mathematical programming formulations. In general, these formulations either deal with problems that differ qualitatively focusing on the variation of reward from its average (Filar, Kallenberg, & Lee, 1989; White, 1992) from ours or are nonconvex, and therefore do not address the issue of polynomial-time solvability which is our focus. Indeed, we are not aware on any complexity results on mean–variance optimization problems. We finally note some interesting variance bounds obtained by Arlotto, Gans, and Steel (2013).

Motivated by considerations such as the above, this paper deals with the computational complexity aspects of mean–variance optimization. The problem is not straightforward for various reasons. One is the absence of a principle of optimality that could lead to simple recursive algorithms. Another reason is that, as is evident from the formula 
                        
                           var
                           (
                           W
                           )
                           =
                           E
                           [
                           
                              
                                 W
                              
                              
                                 2
                              
                           
                           ]
                           -
                           
                              
                                 (
                                 E
                                 [
                                 W
                                 ]
                                 )
                              
                              
                                 2
                              
                           
                        
                     , the variance is not a linear function of the probability measure of the underlying process. Nevertheless, 
                        
                           E
                           [
                           
                              
                                 W
                              
                              
                                 2
                              
                           
                           ]
                        
                      and 
                        
                           E
                           [
                           W
                           ]
                        
                      are linear functions, and as such can be addressed simultaneously using methods from multicriteria or constrained Markov decision processes (Altman, 1999). Indeed, we will use such an approach in order to develop pseudopolynomial exact or approximation algorithms. On the other hand, we will also obtain various NP-hardness results, which show that there is little hope for significant improvement of our algorithms.

The rest of the paper is organized as follows. In Section 2, we describe the model and our notation. **We also define various classes of policies and performance objectives of interest. In Section 3, we compare different policy classes and show that performance typically improves strictly as more general policies are allowed. In Section 4, we establish NP-hardness results for the policy classes we have introduced. Then, in Sections 5 and 6, we develop exact and approximate pseudopolynomial time algorithms. Unfortunately, such algorithms do not seem possible for some of the more restricted classes of policies, due to strong NP-completeness results established in Section 4. Finally, Section 7 contains some brief concluding remarks.

In this section, we define the model, notation, and performance objectives that we will be studying. Throughout, we focus on finite horizon problems.
                        1
                        Negative complexity results are straightforward to extend to the more general case of infinite horizon problems. Also, some of the positive results, such as the approximation algorithms of Section 6, can be extended to the infinite horizon discounted case; this is beyond the scope of this paper.
                     
                     
                        1
                     
                  

We consider a Markov decision process (MDP) with finite state, action, and reward spaces. An MDP is formally defined by a sextuple 
                           
                              M
                              =
                              (
                              T
                              ,
                              S
                              ,
                              A
                              ,
                              R
                              ,
                              p
                              ,
                              g
                              )
                           
                         where:
                           
                              (a)
                              
                                 T, a positive integer, is the time horizon;


                                 
                                    
                                       S
                                    
                                  is a finite collection of states, one of which is designated as the initial state;


                                 
                                    
                                       A
                                    
                                  is a collection of finite sets of possible actions, one set for each state;


                                 
                                    
                                       R
                                    
                                  is a finite subset of 
                                    
                                       Q
                                    
                                  (the set of rational numbers), and is the set of possible values of the immediate rewards. We let 
                                    
                                       K
                                       =
                                       
                                          
                                             max
                                          
                                          
                                             r
                                             ∈
                                             R
                                          
                                       
                                       |
                                       r
                                       |
                                    
                                 .


                                 
                                    
                                       p
                                       :
                                       {
                                       0
                                       ,
                                       …
                                       ,
                                       T
                                       -
                                       1
                                       }
                                       ×
                                       S
                                       ×
                                       S
                                       ×
                                       A
                                       →
                                       Q
                                    
                                  describes the transition probabilities. In particular, p
                                 
                                    t
                                 (s′∣s,
                                 a) is the probability that the state at time t
                                 +1 is s′, given that the state at time t is s, and that action a is chosen at time t.


                                 
                                    
                                       g
                                       :
                                       {
                                       0
                                       ,
                                       …
                                       ,
                                       T
                                       -
                                       1
                                       }
                                       ×
                                       R
                                       ×
                                       S
                                       ×
                                       A
                                       →
                                       Q
                                    
                                  is a set of reward distributions. In particular, g
                                 
                                    t
                                 (r∣s,
                                 a) is the probability that the immediate reward at time t is r, given that the state and action at time t is s and a, respectively.

With few exceptions (e.g., for the time horizon T), we use capital letters to denote random variables, and lower case letters to denote ordinary variables. The process starts at the designated initial state. At every stage t
                        =0,1,…, T
                        −1, the decision maker observes the current state S
                        
                           t
                         and chooses an action A
                        
                           t
                        . Then, an immediate reward R
                        
                           t
                         is obtained, distributed according to g
                        
                           t
                        (·∣S
                        
                           t
                        ,
                        A
                        
                           t
                        ), and the next state S
                        
                           t+1 is chosen, according to p
                        
                           t
                        (·∣S
                        
                           t
                        ,
                        A
                        
                           t
                        ). Note that we have assumed that the possible values of the immediate reward and the various probabilities are all rational numbers. This is in order to address the computational complexity of various problems within the standard framework of digital computation. Finally, we will use the notation x
                        0:t
                         to indicate the tuple (x
                        0,…, x
                        
                           t
                        ).

We will use the symbol π to denote policies. Under a deterministic policy π
                        =(μ
                        0,…, μ
                        
                           T−1), the action at each time t is determined according to a mapping μ
                        
                           t
                         whose argument is the history H
                        
                           t
                        
                        =(S
                        0:t
                        ,
                        A
                        0:t−1,
                        R
                        0:t−1) of the process, by letting A
                        
                           t
                        
                        =
                        μ
                        
                           t
                        (H
                        
                           t
                        ). We let Π
                        
                           h
                         be the set of all such history-based policies. (The subscripts are used as a mnemonic for the variables on which the action is allowed to depend.) We will also consider randomized policies. Intuitively, at each point in time, the policy can pick an action at random, with the probability of each action determined by the current information (which is H
                        
                           t
                         as well as the outcomes of earlier randomizations). Randomness can always be simulated by using an independent uniform random variable as the seed, which leads to the following formal definition. We assume that there is available a sequence of i.i.d.uniform random variables U
                        0, U
                        1,…, U
                        
                           T−1, which are independent from everything else. In a randomized policy, the action at time t is determined by letting A
                        
                           t
                        
                        =
                        μ
                        
                           t
                        (H
                        
                           t
                        ,
                        U
                        0:t
                        ). Let Π
                        
                           h,u
                         be the set of all randomized policies.

In classical MDPs, it is well known that restricting to Markovian policies (policies that take into account only the current state S
                        
                           t
                        ) results in no loss of performance. In our setting, there are two different possible “states” of interest: the original state S
                        
                           t
                        , or the augmented state (S
                        
                           t
                        ,
                        W
                        
                           t
                        ), where
                           
                              
                                 
                                    
                                       W
                                    
                                    
                                       t
                                    
                                 
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          k
                                          =
                                          0
                                       
                                       
                                          t
                                          -
                                          1
                                       
                                    
                                 
                                 
                                    
                                       R
                                    
                                    
                                       k
                                    
                                 
                                 ,
                              
                           
                        (with the convention that W
                        0
                        =0). Accordingly, we define the following classes of policies: Π
                        
                           t,s
                         (under which A
                        
                           t
                        
                        =
                        μ
                        
                           t
                        (S
                        
                           t
                        )), and Π
                        
                           t,s,w
                         (under which A
                        
                           t
                        
                        =
                        μ
                        
                           t
                        (S
                        
                           t
                        ,
                        W
                        
                           t
                        )), and their randomized counterparts Π
                        
                           t,s,u
                         (under which A
                        
                           t
                        
                        =
                        μ
                        
                           t
                        (S
                        
                           t
                        ,
                        U
                        
                           t
                        )), and Π
                        
                           t,s,w,u
                         (under which A
                        
                           t
                        
                        =
                        μ
                        
                           t
                        (S
                        
                           t
                        ,
                        W
                        
                           t
                        ,
                        U
                        
                           t
                        ). Notice that
                           
                              
                                 
                                    
                                       Π
                                    
                                    
                                       t
                                       ,
                                       s
                                    
                                 
                                 ⊂
                                 
                                    
                                       Π
                                    
                                    
                                       t
                                       ,
                                       s
                                       ,
                                       w
                                    
                                 
                                 ⊂
                                 
                                    
                                       Π
                                    
                                    
                                       h
                                    
                                 
                                 ,
                              
                           
                        and similarly for their randomized counterparts.

Once a policy π and an initial state s is fixed, the cumulative reward W
                        
                           T
                         becomes a well-defined random variable. The performance measures of interest are its mean and variance, defined by 
                           
                              
                                 
                                    J
                                 
                                 
                                    π
                                 
                              
                              =
                              
                                 
                                    E
                                 
                                 
                                    π
                                 
                              
                              [
                              
                                 
                                    W
                                 
                                 
                                    T
                                 
                              
                              ]
                           
                         and V
                        
                           π
                        
                        =var
                           π
                        (W
                        
                           T
                        ), respectively. Under our assumptions (finite horizon, and bounded rewards), it follows that there are finite upper bounds of KT and K
                        2
                        T
                        2, for ∣J
                        
                           π
                        ∣ and V
                        
                           π
                        , respectively, independent of the policy.

Given our interest in complexity results, we will focus on “decision” problems that admit a yes/no answer, except for Section 6. We define the following problem.


                        Problem 
                        mv-mdp(Π):

Given an MDP 
                           
                              M
                           
                         and rational numbers λ, v, does there exist a policy in the set Π such that J
                        
                           π
                        
                        ⩾ 
                        λ and V
                        
                           π
                        
                        ⩽
                        v?

Clearly, an algorithm for the problem mv-mdp(Π) can be combined with binary search to solve (up to any desired precision) the problem of maximizing the expected value of W
                        
                           T
                         subject to an upper bound on its variance, or the problem of minimizing the variance of W
                        
                           T
                         subject to a lower bound on its mean.

Our first step is to compare the performance obtained from different policy classes. We introduce some terminology. Let Π and Π′ be two policy classes. We say that Π is inferior to Π′ if, loosely speaking, the policy class Π′ can always match or exceed the “performance” of policy class Π, and for some instances it can exceed it strictly. Formally, Π is inferior to Π′ if the following hold: (i) if 
                        
                           (
                           M
                           ,
                           c
                           ,
                           d
                           )
                        
                      is a “yes” instance of mv-mdp(Π), then it is also a “yes” instance of mv-mdp (Π′); (ii) there exists some 
                        
                           (
                           M
                           ,
                           c
                           ,
                           d
                           )
                        
                      which is a “no” instance of mv-mdp(Π) but a “yes” instance of mv-mdp(Π′). Similarly, we say that two policy classes Π and Π′ are equivalent if every “yes” (respectively, “no”) instance of mv-mdp(Π) is a “yes” (respectively, “no”) instance of mv-mdp(Π′).

We define one more convenient term. A state s is said to be terminal if it is absorbing (i.e., p
                     
                        t
                     (s∣s,
                     a)=1, for every t and a) and provides zero rewards (i.e., g
                     
                        t
                     (0∣s,
                     a)=1, for every t and a).

Our first observation is that randomization can strictly improve performance. This is not surprising given that we are dealing simultaneously with two criteria, and that randomization is helpful in constrained MDPs e.g., (Altman, 1999). (Clearly, it is not the case that there will always be improvement—consider a case where rewards are identically zero, so that all policy classes offer the same performance. The content of our result is that certain policies are not “equivalent,” meaning that there exist instances for which the resulting performance is different).
                           Theorem 1
                           
                              
                                 
                                    (a)
                                    
                                       Π
                                       
                                          t,s
                                        
                                       is inferior to Π
                                       
                                          t,s,u
                                       
                                       ;
                                    


                                       Π
                                       
                                          t,s,w
                                        
                                       is inferior to Π
                                       
                                          t,s,w,u
                                       
                                       ;
                                    


                                       Π
                                       
                                          h
                                        
                                       is inferior to Π
                                       
                                          h,u
                                       
                                       .
                                    

It is clear that performance cannot deteriorate when randomization is allowed. It therefore suffices to display an instance in which randomization improves performance.

Consider a one-stage MDP (T
                              =1). At time 0, we are at the initial state and there are two available actions, a and b. The mean and variance of the resulting reward are both zero under action a, and both equal to 1 under action b. After the decision is made, the rewards are obtained and the process terminates. Thus W
                              
                                 T
                              
                              =
                              R
                              0, the reward obtained at time 0.

Consider the problem of maximizing 
                                 
                                    E
                                    [
                                    
                                       
                                          R
                                       
                                       
                                          0
                                       
                                    
                                    ]
                                 
                               subject to the constraint that var(R
                              0)⩽1/2. There is only one feasible deterministic policy (choose action a), and it has zero expected reward. On the other hand, a randomized policy that chooses action b with probability p has an expected reward of p and the corresponding variance satisfies
                                 
                                    
                                       var
                                       (
                                       
                                          
                                             R
                                          
                                          
                                             0
                                          
                                       
                                       )
                                       ⩽
                                       E
                                       [
                                       
                                          
                                             R
                                          
                                          
                                             0
                                          
                                          
                                             2
                                          
                                       
                                       ]
                                       =
                                       p
                                       E
                                       [
                                       
                                          
                                             R
                                          
                                          
                                             0
                                          
                                          
                                             2
                                          
                                       
                                       |
                                       
                                          
                                             A
                                          
                                          
                                             0
                                          
                                       
                                       =
                                       b
                                       ]
                                       =
                                       2
                                       p
                                       .
                                    
                                 
                              When 0<
                              p
                              ⩽1/4, such a randomized policy is feasible and improves upon the deterministic one.

Note that for the above instance we have Π
                              
                                 t,s
                              
                              = 
                              Π
                              
                                 t,s,w
                              
                              =
                              Π
                              
                                 h
                              , and Π
                              
                                 t,s,u
                              
                              =
                              Π
                              
                                 t,s,w,u
                              
                              =
                              Π
                              
                                 h,u
                              . Hence the above example establishes all three of the claimed statements.□

We now show that in most cases, performance can improve strictly when we allow a policy to have access to more information. The only exception arises for the pair of classes Π
                        
                           t,s,w,u
                         and Π
                        
                           h,u
                        , which we show in Section 5 to be equivalent (cf. Theorem 6).
                           Theorem 2
                           
                              
                                 
                                    (a)
                                    
                                       Π
                                       
                                          t,s
                                        
                                       is inferior to Π
                                       
                                          t,s,w
                                       
                                       , and Π
                                       
                                          t,s,u
                                        
                                       is inferior to Π
                                       
                                          t,s,w,u
                                       
                                       .
                                    


                                       Π
                                       
                                          t,s,w
                                        
                                       is inferior to Π
                                       
                                          h
                                       
                                       .
                                    


                              
                                 
                                    (a)
                                    Consider the following MDP, with time horizon T
                                       =2. The process starts at the initial state s
                                       0, at which there are two actions. Under action a
                                       1, the immediate reward is zero and the process moves to a terminal state. Under action a
                                       2, the immediate reward R
                                       0 is either 0 or 1, with equal probability, and the process moves to state s
                                       1. At state s
                                       1, there are two actions, a
                                       3 and a
                                       4: under action a
                                       3, the immediate reward R
                                       1 is equal to 0, and under action a
                                       4, it is equal to 1. We are interested in the optimal value of the expected reward 
                                          
                                             E
                                             [
                                             
                                                
                                                   W
                                                
                                                
                                                   2
                                                
                                             
                                             ]
                                             =
                                             E
                                             [
                                             
                                                
                                                   R
                                                
                                                
                                                   0
                                                
                                             
                                             +
                                             
                                                
                                                   R
                                                
                                                
                                                   1
                                                
                                             
                                             ]
                                          
                                       , subject to the constraint that the variance is less than or equal to zero (and therefore equal to zero). Let p be the probability that action a
                                       2 is chosen at state s
                                       0. If p
                                       >0, and under any policy in Π
                                       
                                          t,s,u
                                       , the reward R
                                       0 at state s
                                       0 has positive variance, and the reward R
                                       1 at the next stage is uncorrelated with R
                                       0. Hence, the variance of R
                                       0
                                       +
                                       R
                                       1 is positive, and such a policy is not feasible; in particular, the constraint on the variance requires that p
                                       =0. We conclude that the largest possible expected reward under any policy in Π
                                       
                                          t,s,u
                                        (and, a fortiori, under any policy in Π
                                       
                                          t,s
                                       ) is equal to zero. Consider now the following policy, which belongs to Π
                                       
                                          t,s,w
                                        and, a fortiori, to Π
                                       
                                          t,s,w,u
                                       : at state s
                                       0, choose action a
                                       2; then, at state s
                                       1, choose a
                                       3 if W
                                       1
                                       =
                                       R
                                       0
                                       =1, and choose a
                                       4 if W
                                       1
                                       =
                                       R
                                       0
                                       =0. In either case, the total reward is R
                                       0
                                       +
                                       R
                                       1
                                       =1, while the variance of R
                                       0
                                       +
                                       R
                                       1 is zero, thus ensuring feasibility. This establishes the first part of the theorem.

Consider the following MDP, with time horizon T
                                       =3. At state s
                                       0 there is only one available action; the next state S
                                       1 is either s
                                       1 or s
                                       1′, with probability p and 1−
                                       p, respectively, and the immediate reward R
                                       0 is zero. At either state s
                                       1 or 
                                          
                                             
                                                
                                                   s
                                                
                                                
                                                   1
                                                
                                                
                                                   ′
                                                
                                             
                                          
                                       , there is again only one available action; the next state, S
                                       2, is s
                                       2, and the reward R
                                       1 is zero. At state s
                                       2, there are two actions, a and b. Under action a, the mean and variance of the resulting reward R
                                       2 are both zero, and under action b, they are both equal to 1. Let us examine the largest possible value of 
                                          
                                             E
                                             [
                                             
                                                
                                                   W
                                                
                                                
                                                   3
                                                
                                             
                                             ]
                                             =
                                             E
                                             [
                                             
                                                
                                                   R
                                                
                                                
                                                   2
                                                
                                             
                                             ]
                                          
                                       , subject to the constraint var(W
                                       2)⩽1/2. The class Π
                                       
                                          t,s,w
                                        contains two policies, corresponding to the two deterministic choices of an action at state s
                                       2; only one of them is feasible (the one that chooses action a), resulting in zero expected reward. However, the following policy in Π
                                       
                                          h
                                        has positive expected reward: choose action b at state s
                                       2 if and only if the state at time 1 was equal to s
                                       1 (which happens with probability p). As long as p is sufficiently small, the constraint var(W)⩽1/2 is met, and this policy is feasible. It follows that Π
                                       
                                          t,s,w
                                        is inferior to Π
                                       
                                          h
                                       .□

In this section, we establish that mean–variance optimization in finite horizon MDPs is unlikely to admit polynomial time algorithms, in contrast to classical MDPs.
                        Theorem 3
                        
                           The problem 
                           mv
                           -
                           mdp
                           (Π) is NP-hard, when Π is Π
                           
                              t,s,w
                           
                           , Π
                           
                              t,s,w,u
                           
                           , Π
                           
                              h
                           
                           , or Π
                           
                              h,u
                           
                           .
                        

We will actually show NP-hardness for the special case of mv-mdp(Π), in which we wish to determine whether there exists a policy whose reward variance is equal to zero. (In terms of the problem definition, this corresponds to letting λ
                           =−KT and v
                           =0.) The proof uses a reduction from the Partition problem: Given n positive integers, does there exist a subset B of {1,…, n} such that 
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       ∈
                                       B
                                    
                                 
                                 
                                    
                                       r
                                    
                                    
                                       i
                                    
                                 
                                 =
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       ∉
                                       B
                                    
                                 
                                 
                                    
                                       r
                                    
                                    
                                       i
                                    
                                 
                              
                           ?

Given an instance (r
                           1,…, r
                           
                              n
                           ) of Partition, and for any of the policy classes of interest, we construct an instance of mv-mdp(Π), with time horizon T
                           =
                           n
                           +1, as follows. At the initial state s
                           0, there is only one available action, resulting in zero immediate reward (R
                           0
                           =0). With probability 1/2, the process moves to a terminal state; with probability 1/2, the process moves (deterministically) along a sequence of states s
                           1,…, s
                           
                              n
                           . At each state s
                           
                              i
                            (i
                           =1,…, n), there are two actions: a
                           
                              i
                           , which results in an immediate reward of r
                           
                              i
                           , and b
                           
                              i
                           , which results in an immediate reward of −r
                           
                              i
                           .

Suppose that there exists a set B
                           ⊂{1,…, n} such that 
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       ∈
                                       B
                                    
                                 
                                 
                                    
                                       r
                                    
                                    
                                       i
                                    
                                 
                                 =
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       ∉
                                       B
                                    
                                 
                                 
                                    
                                       r
                                    
                                    
                                       i
                                    
                                 
                              
                           . Consider the policy that chooses action a
                           
                              i
                            at state s
                           
                              i
                            if and only if i
                           ∈
                           B. This policy achieves zero total reward, with probability 1, and therefore meets the zero variance constraint. Conversely, if a policy results in zero variance, then the total reward must be equal to zero, with probability 1, which implies that such a set B exists. This completes the reduction.

Note that this argument applies no matter which particular class of policies is being considered.□

The above proof also applies to the policy classes Π
                     
                        t,s
                      and Π
                     
                        t,s,u
                     . However, for these two classes, a stronger result is possible. Recall that a problem is strongly NP-hard, if it remains NP-hard when restricted to instances in which the numerical part of the instance description involves “small” numbers; see Garey & Johnson (1979) for a precise definition.
                        Theorem 4
                        
                           If Π is either Π
                           
                              t,s
                            
                           or Π
                           
                              t,s,u
                           
                           , the problem 
                           mv
                           -
                           mdp
                           (Π) is strongly NP-hard.
                        

As in the proof of Theorem 3, we will prove the result for the special case of mv-mdp, in which we wish to determine whether there exists a policy under which the variance of the reward is equal to zero. The proof involves a reduction from the 3-Satisfiability problem (3sat). An instance of 3sat consists of n Boolean variables x
                           1,…, x
                           
                              n
                           , and m clauses C
                           1,…, C
                           
                              m
                           , with three literals per clause. Each clause is the disjunction of three literals, where a literal is either a variable or its negation. (For example, 
                              
                                 
                                    
                                       x
                                    
                                    
                                       2
                                    
                                 
                                 ∨
                                 
                                    
                                       
                                          
                                             x
                                          
                                          
                                             ¯
                                          
                                       
                                    
                                    
                                       4
                                    
                                 
                                 ∨
                                 
                                    
                                       x
                                    
                                    
                                       5
                                    
                                 
                              
                            is such a clause, where a bar stands for negation.) The question is whether there exists an assignment of truth values (“true” or “false”) to the variables such that all clauses are satisfied.

Suppose that we are given an instance of 3sat, with n variables and m clauses, C
                           1,…, C
                           
                              m
                           . We construct an instance of mv-mdp(Π) as follows. There is an initial state s
                           0, a state d
                           0, a state c
                           
                              j
                            associated with each clause C
                           
                              j
                           , and a state y
                           
                              i
                            associated with each literal x
                           
                              i
                           . The actions, dynamics, and rewards are as follows:
                              
                                 (a)
                                 Out of state s
                                    0, there is equal probability, 1/(m
                                    +1), of reaching any one of the states d
                                    0,
                                    c
                                    1,…, c
                                    
                                       m
                                    , independent of the action; the immediate reward is zero.

State d
                                    0 is a terminal state. At each state c
                                    
                                       j
                                    , there are three actions available: each action selects one of the three literals in the clause, and the process moves to the state y
                                    
                                       i
                                     associated with that literal; the immediate reward is 1 if the literal appears in the clause unnegated, and −1 if the literal appears in the clause negated. For an example, suppose that the clause is of the form 
                                       
                                          
                                             
                                                x
                                             
                                             
                                                2
                                             
                                          
                                          ∨
                                          
                                             
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      ¯
                                                   
                                                
                                             
                                             
                                                4
                                             
                                          
                                          ∨
                                          
                                             
                                                x
                                             
                                             
                                                5
                                             
                                          
                                       
                                    . Under the first action, the next state is y
                                    2, and the reward is 1; under the second action, the next state is y
                                    4 and the reward is −1; under the third action, the next state is y
                                    5, and the reward is 1.

At each state y
                                    
                                       i
                                    , there are two possible actions a
                                    
                                       i
                                     and b
                                    
                                       i
                                    , resulting in immediate rewards of 1 and −1, respectively. The process then moves to the terminal state d
                                    0.

For the converse direction, suppose that there exists a policy in Π
                           
                              t,s
                           , or more generally, in Π
                           
                              t,s,u
                            under which the variance of the total reward is zero. Since the total reward is equal to 0 whenever the first transition leads to state d
                           0 (which happens with probability 1/(m
                           +1), it follows that the total reward must be always zero. Consider now the following truth assignment: x
                           
                              i
                            is set to be true if and only if the policy chooses action b
                           
                              i
                            at state y
                           
                              i
                           , with positive probability. Suppose that the state visited after the first transition is c
                           
                              j
                           . Suppose that the action chosen at state c
                           
                              j
                            leads next to state y
                           
                              i
                            and that the literal x
                           
                              i
                            appears unnegated in clause C
                           
                              j
                           . Then, the reward at state c
                           
                              j
                            is 1, which implies that the reward at state y
                           
                              i
                            is −1. It follows that the action chosen at y
                           
                              i
                            is b
                           
                              i
                           , and therefore x
                           
                              i
                            has been set to be true. It follows that clause C
                           
                              j
                            is satisfied. A similar argument shows that clause C
                           
                              j
                            is satisfied when the literal x
                           
                              i
                            associated with the chosen action at c
                           
                              j
                            appears negated. In either case, we conclude that clause C
                           
                              j
                            is satisfied. Since every state c
                           
                              j
                            is possible at time 1, it follows that every clause is satisfied, and we have a “yes” instance of 3sat.□

Because the immediate rewards are bounded, it is easily seen that an instance with general rewards is equivalent to one with all positive (or all negative) rewards. It follows that our negative complexity results remain valid even if we restrict to instances in which all rewards are positive (respectively, negative).

The comparison and complexity results of the preceding two sections indicate that the policy classes Π
                     
                        t,s
                     , Π
                     
                        t,s,w
                     , Π
                     
                        t,s,u
                     , and Π
                     
                        h
                      are inferior to the class Π
                     
                        h,u
                     , and furthermore some of them (Π
                     
                        t,s
                     , Π
                     
                        t,s,w
                     ) appear to have higher complexity. Thus, there is no reason to consider them further. While the problem mv-mdp(Π
                     
                        h,u
                     ) is NP-hard, there is still a possibility for approximate or pseudopolynomial time algorithms. In this section, we focus on exact pseudopolynomial time algorithms.

Our approach involves an augmented state, defined by X
                     
                        t
                     
                     =(S
                     
                        t
                     ,
                     W
                     
                        t
                     ). Let 
                        
                           X
                        
                      be the set of all possible values of the augmented state. Let 
                        
                           |
                           S
                           |
                        
                      be the cardinality of the set 
                        
                           S
                        
                     . Let 
                        
                           |
                           R
                           |
                        
                      be the cardinality of the set 
                        
                           R
                        
                     . Recall also that 
                        
                           K
                           =
                           
                              
                                 max
                              
                              
                                 r
                                 ∈
                                 R
                              
                           
                           |
                           r
                           |
                        
                     . If we assume that the immediate rewards are integers, then W
                     
                        t
                      is an integer between −KT and KT. In this case, the cardinality 
                        
                           |
                           X
                           |
                        
                      of the augmented state space 
                        
                           X
                        
                      is bounded by 
                        
                           |
                           S
                           |
                           ·
                           (
                           2
                           KT
                           +
                           1
                           )
                        
                     , which is polynomial. Without the integrality assumption, the cardinality of the set 
                        
                           X
                        
                      remains finite, but it can increase exponentially with T. For this reason, we study the integer case separately in Section 5.2.

In this section, we provide some results on the representation of MDPs in terms of a state-action frequency polytope, thus setting the stage for our subsequent algorithms.

For any policy π
                        ∈
                        Π
                        
                           h,u
                        , and any 
                           
                              x
                              ∈
                              X
                           
                        , 
                           
                              a
                              ∈
                              A
                           
                        , we define the state-action frequencies at time t by
                           
                              
                                 
                                    
                                       z
                                    
                                    
                                       t
                                    
                                    
                                       π
                                    
                                 
                                 (
                                 x
                                 ,
                                 a
                                 )
                                 =
                                 
                                    
                                       P
                                    
                                    
                                       π
                                    
                                 
                                 (
                                 
                                    
                                       X
                                    
                                    
                                       t
                                    
                                 
                                 =
                                 x
                                 ,
                                 
                                    
                                       A
                                    
                                    
                                       t
                                    
                                 
                                 =
                                 a
                                 )
                                 ,
                                 
                                 t
                                 =
                                 0
                                 ,
                                 1
                                 ,
                                 …
                                 ,
                                 T
                                 -
                                 1
                                 ,
                              
                           
                        and
                           
                              
                                 
                                    
                                       z
                                    
                                    
                                       t
                                    
                                    
                                       π
                                    
                                 
                                 (
                                 x
                                 )
                                 =
                                 
                                    
                                       P
                                    
                                    
                                       π
                                    
                                 
                                 (
                                 
                                    
                                       X
                                    
                                    
                                       t
                                    
                                 
                                 =
                                 x
                                 )
                                 ,
                                 
                                 t
                                 =
                                 0
                                 ,
                                 1
                                 ,
                                 …
                                 ,
                                 T
                                 .
                              
                           
                        Let z
                        
                           π
                         be a vector that lists all of the above defined state-action frequencies.

For any family Π of policies, let Z(Π)={z
                        
                           π
                        ∣ π
                        ∈
                        Π}. The following result is well known e.g., (Altman, 1999). It asserts that any feasible state-action frequency vector can be attained by policies that depend only on time, the (augmented) state, and a randomization variable. Furthermore, the set of feasible state-action frequency vectors is a polyhedron, hence amenable to linear programming methods.
                           Theorem 5
                           
                              
                                 
                                    (a)
                                    
                                       We have Z(Π
                                       
                                          h,u
                                       
                                       )
                                       
                                       =
                                       
                                       Z(Π
                                       
                                          t,s,w,u
                                       
                                       ).
                                    


                                       The set Z(Π
                                       
                                          h,u
                                       
                                       ) is a polyhedron, specified by 
                                       
                                          
                                             O
                                             (
                                             T
                                             ·
                                             |
                                             X
                                             |
                                             ·
                                             |
                                             A
                                             |
                                             )
                                          
                                        
                                       linear constraints.
                                    

Note that a certain mean–variance pair (λ,
                        v) is attainable by a policy in Π
                        
                           h,u
                         if and only if there exists some z
                        ∈
                        Z(Π
                        
                           h,u
                        ) that satisfies
                           
                              (1)
                              
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          (
                                          s
                                          ,
                                          w
                                          )
                                          ∈
                                          X
                                       
                                    
                                 
                                 
                                    
                                       wz
                                    
                                    
                                       T
                                    
                                 
                                 (
                                 s
                                 ,
                                 w
                                 )
                                 =
                                 λ
                                 ,
                              
                           
                        
                        
                           
                              (2)
                              
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          (
                                          s
                                          ,
                                          w
                                          )
                                          ∈
                                          X
                                       
                                    
                                 
                                 
                                    
                                       w
                                    
                                    
                                       2
                                    
                                 
                                 
                                    
                                       z
                                    
                                    
                                       T
                                    
                                 
                                 (
                                 s
                                 ,
                                 w
                                 )
                                 =
                                 v
                                 +
                                 
                                    
                                       λ
                                    
                                    
                                       2
                                    
                                 
                                 .
                              
                           
                        Furthermore, since Z(Π
                        
                           h,u
                        )=
                        Z(Π
                        
                           t,s,w,u
                        ), it follows that if a pair (λ,
                        v) is attainable by a policy in Π
                        
                           h,u
                        , it is also attainable by a policy in Π
                        
                           t,s,w,u
                        . This establishes the following result.
                           Theorem 6
                           
                              The policy classes Π
                              
                                 h,u
                               
                              and Π
                              
                                 t,s,w,u
                               
                              are equivalent.
                           

Note that checking the feasibility of the conditions z
                        ∈ 
                        Z(Π
                        
                           h,u
                        ), (1), and (2) amounts to solving a linear programming problem, with a number of constraints proportional to the cardinality of the augmented state space 
                           
                              X
                           
                         and, therefore, in general, exponential in T.

In this section, we assume that the immediate rewards are integers, with absolute value bounded by K, and we show that pseudopolynomial time algorithms are possible. Recall that an algorithm is a pseudopolynomial time algorithm if its running time is polynomial in K and the instance size. (This is in contrast to polynomial time algorithms in which the running time can only grow as a polynomial of log
                        K.)
                        
                           Theorem 7
                           
                              Suppose that the immediate rewards are integers, with absolute value bounded by K. Consider the following two problems:
                              
                                 
                                    (i)
                                    
                                       determine whether there exists a policy in Π
                                       
                                          h,u
                                        
                                       for which (J
                                       
                                          π
                                       
                                       ,
                                       
                                       V
                                       
                                          π
                                       
                                       )
                                       
                                       =
                                       
                                       (λ,
                                       
                                       v), where λ and v are given rational numbers; and,
                                    


                                       determine whether there exists a policy in Π
                                       
                                          h,u
                                        
                                       for which J
                                       
                                          π
                                       
                                       
                                       =
                                       
                                       λ and V
                                       
                                          π
                                       
                                       ⩽
                                       v, where λ and v are given rational numbers.
                                    


                                       these two problems admit a pseudopolynomial time algorithm; and,
                                    


                                       unless P
                                       
                                       =
                                       
                                       NP, these problems cannot be solved in polynomial time.
                                    


                              
                                 
                                    (a)
                                    As already discussed, these problems amount to solving a linear program. In the integer case, the number of variables and constraints is bounded by a polynomial in K and the instance size. The result follows because linear programming can be solved in polynomial time.

This is proved by considering the special case where λ
                                       =
                                       v
                                       =0 and the exact same argument as in the proof of Theorem 3.□

Similar to constrained MDPs, mean–variance optimization involves two different performance criteria. Unfortunately, however, the linear programming approach to constrained MDPs does not translate into an algorithm for the problem mv-mdp(Π
                        
                           h,u
                        ). The reason is that the set
                           
                              
                                 
                                    
                                       P
                                    
                                    
                                       MV
                                    
                                 
                                 =
                                 {
                                 (
                                 
                                    
                                       J
                                    
                                    
                                       π
                                    
                                 
                                 ,
                                 
                                    
                                       V
                                    
                                    
                                       π
                                    
                                 
                                 )
                                 |
                                 π
                                 ∈
                                 
                                    
                                       Π
                                    
                                    
                                       h
                                       ,
                                       u
                                    
                                 
                                 }
                              
                           
                        of achievable mean–variance pairs need not be convex. To bring the constrained MDP methodology to bear on our problem, instead of focusing on the pair (J
                        
                           π
                        ,
                        V
                        
                           π
                        ), we define 
                           
                              
                                 
                                    Q
                                 
                                 
                                    π
                                 
                              
                              =
                              
                                 
                                    E
                                 
                                 
                                    π
                                 
                              
                              [
                              
                                 
                                    W
                                 
                                 
                                    T
                                 
                                 
                                    2
                                 
                              
                              ]
                           
                        , and focus on the pair (J
                        
                           π
                        ,
                        Q
                        
                           π
                        ). This is now a pair of objectives that depend linearly on the state frequencies associated with the final augmented state X
                        
                           T
                        . Accordingly, we define
                           
                              
                                 
                                    
                                       P
                                    
                                    
                                       MQ
                                    
                                 
                                 =
                                 {
                                 (
                                 
                                    
                                       J
                                    
                                    
                                       π
                                    
                                 
                                 ,
                                 
                                    
                                       Q
                                    
                                    
                                       π
                                    
                                 
                                 )
                                 |
                                 π
                                 ∈
                                 
                                    
                                       Π
                                    
                                    
                                       h
                                       ,
                                       u
                                    
                                 
                                 }
                                 .
                              
                           
                        Note that P
                        
                           MQ
                         is a polyhedron, because it is the image of the polyhedron Z(Π
                        
                           h,u
                        ) under the linear mapping specified by the left-hand sides of Eqs. (1) and (2). In contrast, P
                        
                           MV
                         is the image of P
                        
                           MQ
                         under a nonlinear mapping:
                           
                              
                                 
                                    
                                       P
                                    
                                    
                                       MV
                                    
                                 
                                 =
                                 {
                                 (
                                 λ
                                 ,
                                 q
                                 -
                                 
                                    
                                       λ
                                    
                                    
                                       2
                                    
                                 
                                 )
                                 |
                                 (
                                 λ
                                 ,
                                 q
                                 )
                                 ∈
                                 
                                    
                                       P
                                    
                                    
                                       MQ
                                    
                                 
                                 }
                                 ,
                              
                           
                        and is not, in general, a polyhedron.

As a corollary of the above discussion, and for the case of integer rewards, we can exploit convexity to devise pseudopolynomial algorithms for problems that can be formulated in terms of the convex set P
                        
                           MQ
                        . On the other hand, because of the non-convexity of P
                        
                           MV
                        , we have not been able to devise pseudopolynomial time algorithms for the problem mv-mdp(Π
                        
                           h,u
                        ), or even the simpler problem of deciding whether there exists a policy π
                        ∈
                        Π
                        
                           h,u
                         that satisfies V
                        
                           π
                        
                        ⩽
                        v, for some given number v, except for the very special case where v
                        =0, which is the subject of our next result. For a general v, an approximation algorithm will be presented in the next section.
                           Theorem 8
                           
                              
                                 
                                    (a)
                                    
                                       If there exists some π
                                       ∈
                                       Π
                                       
                                          h,u
                                        
                                       for which V
                                       
                                          π
                                       
                                       
                                       =
                                       
                                       0, then there exists some π′∈
                                       Π
                                       
                                          t,s,w
                                        
                                       for which 
                                       
                                          
                                             
                                                
                                                   V
                                                
                                                
                                                   
                                                      
                                                         π
                                                      
                                                      
                                                         ′
                                                      
                                                   
                                                
                                             
                                             =
                                             0
                                          
                                       
                                       .
                                    


                                       Suppose that the immediate rewards are integers, with absolute value bounded by K. Then the problem of determining whether there exists a policy π
                                       ∈
                                       Π
                                       
                                          h,u
                                        
                                       for which V
                                       
                                          π
                                       
                                       
                                       =
                                       
                                       0 admits a pseudopolynomial time algorithm.
                                    


                              
                                 
                                    (a)
                                    
                                       Suppose that there exists some π
                                       ∈
                                       Π
                                       
                                          h,u
                                        
                                       for which V
                                       
                                          π
                                       
                                       
                                       =
                                       
                                       0. By 
                                       
                                          Theorem 6
                                       , π can be assumed, without loss of generality, to lie in Π
                                       
                                          t,s,w,u
                                       
                                       . Let var
                                       
                                          π
                                       
                                       (W
                                       
                                          T
                                       ∣U
                                       
                                          0:T
                                       
                                       ), be the conditional variance of W
                                       
                                          T
                                       
                                       , conditioned on the realization of the randomization variables U
                                       
                                          0:T
                                       
                                       . We have 
                                       
                                          
                                             
                                                
                                                   var
                                                
                                                
                                                   π
                                                
                                             
                                             (
                                             
                                                
                                                   W
                                                
                                                
                                                   T
                                                
                                             
                                             )
                                             ⩾
                                             
                                                
                                                   E
                                                
                                                
                                                   π
                                                
                                             
                                             [
                                             
                                                
                                                   var
                                                
                                                
                                                   π
                                                
                                             
                                             (
                                             
                                                
                                                   W
                                                
                                                
                                                   T
                                                
                                             
                                             |
                                             
                                                
                                                   U
                                                
                                                
                                                   0
                                                   :
                                                   T
                                                
                                             
                                             )
                                             ]
                                          
                                       
                                       , which implies that there exists some u
                                       
                                          0:T
                                        
                                       such that var
                                       
                                          π
                                       
                                       (W
                                       
                                          T
                                       ∣U
                                       
                                          0:T
                                       
                                       
                                       =
                                       
                                       u
                                       
                                          0:T
                                       
                                       )
                                       
                                       =
                                       
                                       0. By fixing the randomization variables to this particular u
                                       
                                          0:T
                                       
                                       , we obtain a deterministic policy, in Π
                                       
                                          t,s,w
                                        
                                       under which the reward variance is zero.
                                    


                                       If there exists a policy under which V
                                       
                                          π
                                       
                                       
                                       =
                                       
                                       0, then there exists an integer k, with ∣k∣⩽
                                       KT such that, under this policy, W
                                       
                                          T
                                        
                                       is guaranteed to be equal to k. Thus, we only need to check, for each k in the relevant range, whether there exists a policy such that (J
                                       
                                          π
                                       
                                       ,
                                       
                                       V
                                       
                                          π
                                       
                                       )
                                       
                                       =
                                       
                                       (k,
                                       
                                       0). By 
                                       
                                          Theorem 7
                                       , this can be done in pseudopolynomial time.
                                       □

The approach in the proof of part (b) above leads to a short argument, but yields a rather inefficient (albeit pseudopolynomial) algorithm. A much more efficient and simple algorithm is obtained by realizing that the question of whether W
                        
                           T
                         can be forced to be k, with probability 1, is just a reachability game: the decision maker picks the actions and an adversary picks the ensuing transitions and rewards (among those that have positive probability of occurring). The decision maker wins the game if it can guarantee that W
                        
                           T
                        
                        =
                        k. Such sequential games are easy to solve in time polynomial in the number of (augmented) states, decisions, and the time horizon, by a straightforward backward recursion. On the other hand a genuinely polynomial time algorithm does not appear to be possible; indeed, the proof of Theorem 3 shows that the problem is NP-complete.

In this section, we deal with the optimization counterparts of the problem mv-mdp(Π
                     
                        h,u
                     ). We are interested in computing approximately the following two functions:
                        
                           (3)
                           
                              
                                 
                                    v
                                 
                                 
                                    ∗
                                 
                              
                              (
                              λ
                              )
                              =
                              
                                 
                                    inf
                                 
                                 
                                    {
                                    π
                                    ∈
                                    
                                       
                                          Π
                                       
                                       
                                          h
                                          ,
                                          u
                                       
                                    
                                    :
                                    
                                       
                                          J
                                       
                                       
                                          π
                                       
                                    
                                    ⩾
                                    λ
                                    }
                                 
                              
                              
                                 
                                    V
                                 
                                 
                                    π
                                 
                              
                              ,
                           
                        
                     and
                        
                           (4)
                           
                              
                                 
                                    λ
                                 
                                 
                                    ∗
                                 
                              
                              (
                              v
                              )
                              =
                              
                                 
                                    
                                       sup
                                    
                                    
                                       {
                                       π
                                       ∈
                                       
                                          
                                             Π
                                          
                                          
                                             h
                                             ,
                                             u
                                          
                                       
                                       :
                                       
                                          
                                             V
                                          
                                          
                                             π
                                          
                                       
                                       ⩽
                                       v
                                       }
                                    
                                 
                              
                              
                                 
                                    J
                                 
                                 
                                    π
                                 
                              
                              .
                           
                        
                     If the constraint J
                     
                        π
                     
                     ⩾
                     λ (respectively, V
                     
                        π
                     
                     ⩽ 
                     v) is infeasible, we use the standard convention v
                     ∗(λ)=∞ (respectively, λ
                     ∗(v)=−∞). Note that the infimum and supremum in the above definitions are both attained, because the set P
                     
                        MV
                      of achievable mean–variance pairs is the image of the polyhedron P
                     
                        MQ
                      under a continuous map, and is therefore compact.

We do not know how to efficiently compute or even generate a uniform approximation of either v
                     ∗(λ) or λ
                     ∗(v) (i.e., find a value v′ between v
                     ∗(λ)−
                     ∊ and v
                     ∗(λ)+
                     ∊, and similarly for λ
                     ∗(v)). In the following two results we consider a weaker notion of approximation that is computable in pseudopolynomial time. We discuss v
                     ∗(λ) as the issues for λ
                     ∗(v) are similar.

For any positive ∊ and ν, we will say that 
                        
                           
                              
                                 v
                              
                              
                                 ˆ
                              
                           
                           (
                           ·
                           )
                        
                      is an (∊,
                     ν)-approximation of v
                     ∗(·) if, for every λ,
                        
                           (5)
                           
                              
                                 
                                    v
                                 
                                 
                                    ∗
                                 
                              
                              (
                              λ
                              -
                              ν
                              )
                              -
                              ∊
                              ⩽
                              
                                 
                                    v
                                 
                                 
                                    ˆ
                                 
                              
                              (
                              λ
                              )
                              ⩽
                              
                                 
                                    v
                                 
                                 
                                    ∗
                                 
                              
                              (
                              λ
                              +
                              ν
                              )
                              +
                              ∊
                              .
                           
                        
                     This is an approximation of the same kind as those considered in Papadimitriou & Yannakakis (2000): it returns a value 
                        
                           
                              
                                 v
                              
                              
                                 ˆ
                              
                           
                        
                      such that 
                        
                           (
                           λ
                           ,
                           
                              
                                 v
                              
                              
                                 ˆ
                              
                           
                           )
                        
                      is an element of the “(∊
                     +
                     ν)-approximate Pareto boundary” of the set P
                     
                        MV
                     . For a different view, the graph of the function 
                        
                           
                              
                                 v
                              
                              
                                 ˆ
                              
                           
                           (
                           ·
                           )
                        
                      is within Hausdorf distance ∊
                     +
                     ν from the graph of the function v
                     ∗(·).

We will show how to compute an (∊,
                     ν)-approximation in time which is pseudopolynomial, and polynomial in the parameters 1/∊, and 1/ν.

We start in Section 6.1 with the case of integer rewards, and build on the pseudopolynomial time algorithms of the preceding section. We then consider the case of general rewards in Section 6.2. We finally sketch an alternative algorithm in Section 6.3 based on set-valued dynamic programming.

In this section, we prove the following result.
                           Theorem 9
                           
                              Suppose that the immediate rewards are integers. There exists an algorithm that, given ∊, ν, and λ, outputs a value 
                              
                                 
                                    
                                       
                                          v
                                       
                                       
                                          ˆ
                                       
                                    
                                    (
                                    λ
                                    )
                                 
                               
                              that satisfies 
                              
                                 (5)
                              
                              , and which runs in time polynomial in 
                              
                                 
                                    |
                                    S
                                    |
                                    ,
                                    
                                    |
                                    A
                                    |
                                    ,
                                    
                                    T
                                    ,
                                    
                                    K
                                    ,
                                    
                                    1
                                    /
                                    ∊
                                 
                              
                              , and 1/ν.
                           

Without loss of generality, and only for the purposes of this proof, we can and will assume that the immediate rewards are nonnegative. Indeed, if the immediate rewards range in [−K,
                              K] we can redefine them, by adding K to the reward at each stage. Then, 
                                 
                                    
                                       
                                          v
                                       
                                       
                                          ˆ
                                       
                                    
                                    (
                                    λ
                                    )
                                 
                               for the original problem will be equal to 
                                 
                                    
                                       
                                          v
                                       
                                       
                                          ˆ
                                       
                                    
                                    (
                                    λ
                                    +
                                    K
                                    )
                                 
                               for the new problem. Since the rewards are bounded by K, we have v
                              ∗(λ)=∞ for λ
                              >
                              KT and v
                              ∗(λ)= 
                              v
                              ∗(0) for λ
                              <0. For this reason, we only need to consider λ
                              ∈[0,KT]. To simplify the presentation, we assume that ∊
                              =
                              ν. We let δ be such that ∊
                              =3δ KT.

The algorithm is as follows. We consider grid points λ
                              
                                 i
                               defined by λ
                              
                                 i
                              
                              =(i
                              −1)δ, i
                              =1,…, n, where n is chosen so that λ
                              
                                 n−1
                              ⩽
                              KT, λ
                              
                                 n
                              
                              >
                              KT. Note that n
                              =
                              O(KT/δ). For i
                              =1,…, n
                              −1, we calculate 
                                 
                                    
                                       
                                          q
                                       
                                       
                                          ˆ
                                       
                                    
                                    (
                                    
                                       
                                          λ
                                       
                                       
                                          i
                                       
                                    
                                    )
                                 
                              , the smallest possible value of 
                                 
                                    E
                                    [
                                    
                                       
                                          W
                                       
                                       
                                          T
                                       
                                       
                                          2
                                       
                                    
                                    ]
                                 
                              , when 
                                 
                                    E
                                    [
                                    
                                       
                                          W
                                       
                                       
                                          T
                                       
                                    
                                    ]
                                 
                               is restricted to lie in [λ
                              
                                 i
                              ,
                              λ
                              
                                 i+1]. Formally,
                                 
                                    
                                       
                                          
                                             q
                                          
                                          
                                             ˆ
                                          
                                       
                                       (
                                       
                                          
                                             λ
                                          
                                          
                                             i
                                          
                                       
                                       )
                                       =
                                       
                                          min
                                       
                                       
                                          
                                             
                                                q
                                                |
                                                ∃
                                                
                                                   
                                                      λ
                                                   
                                                   
                                                      ′
                                                   
                                                
                                                ∈
                                                [
                                                
                                                   
                                                      λ
                                                   
                                                   
                                                      i
                                                   
                                                
                                                ,
                                                
                                                   
                                                      λ
                                                   
                                                   
                                                      i
                                                      +
                                                      1
                                                   
                                                
                                                ]
                                                
                                                s
                                                .
                                                t
                                                .
                                                (
                                                
                                                   
                                                      λ
                                                   
                                                   
                                                      ′
                                                   
                                                
                                                ,
                                                q
                                                )
                                                ∈
                                                
                                                   
                                                      P
                                                   
                                                   
                                                      MQ
                                                   
                                                
                                             
                                          
                                       
                                       .
                                    
                                 
                              We let 
                                 
                                    
                                       
                                          u
                                       
                                       
                                          ˆ
                                       
                                    
                                    (
                                    
                                       
                                          λ
                                       
                                       
                                          i
                                       
                                    
                                    )
                                    =
                                    
                                       
                                          q
                                       
                                       
                                          ˆ
                                       
                                    
                                    (
                                    
                                       
                                          λ
                                       
                                       
                                          i
                                       
                                    
                                    )
                                    -
                                    
                                       
                                          λ
                                       
                                       
                                          i
                                          +
                                          1
                                       
                                       
                                          2
                                       
                                    
                                 
                              , which can be interpreted as an estimate of the least possible variance when 
                                 
                                    E
                                    [
                                    
                                       
                                          W
                                       
                                       
                                          T
                                       
                                    
                                    ]
                                 
                               is restricted to the interval [λ
                              
                                 i
                              ,
                              λ
                              
                                 i+1]. Finally, we set
                                 
                                    
                                       
                                          
                                             v
                                          
                                          
                                             ˆ
                                          
                                       
                                       (
                                       λ
                                       )
                                       =
                                       
                                          
                                             
                                                min
                                             
                                             
                                                i
                                                ⩾
                                                k
                                             
                                          
                                       
                                       
                                          
                                             u
                                          
                                          
                                             ˆ
                                          
                                       
                                       (
                                       
                                          
                                             λ
                                          
                                          
                                             i
                                          
                                       
                                       )
                                       ,
                                       
                                       if
                                       
                                       λ
                                       ∈
                                       [
                                       
                                          
                                             λ
                                          
                                          
                                             k
                                          
                                       
                                       ,
                                       
                                          
                                             λ
                                          
                                          
                                             k
                                             +
                                             1
                                          
                                       
                                       ]
                                       .
                                    
                                 
                              The main computational effort is in computing 
                                 
                                    
                                       
                                          q
                                       
                                       
                                          ˆ
                                       
                                    
                                    (
                                    
                                       
                                          λ
                                       
                                       
                                          i
                                       
                                    
                                    )
                                 
                               for every i. Since P
                              
                                 MQ
                               is a polyhedron, this amounts to solving O(KT/δ) linear programming problems. Thus, the running time of the algorithm has the claimed properties.

We now prove correctness. Let q
                              ∗(λ)=min{q∣ (λ,
                              q)∈
                              P
                              
                                 MQ
                              }, and u
                              ∗(λ)=
                              q
                              ∗(λ)−
                              λ
                              2, which is the least possible variance for a given value of λ. Note that v
                              ∗(λ)=min{u
                              ∗(λ′)∣ λ′⩾
                              λ}.

We have 
                                 
                                    
                                       
                                          q
                                       
                                       
                                          ˆ
                                       
                                    
                                    (
                                    
                                       
                                          λ
                                       
                                       
                                          i
                                       
                                    
                                    )
                                    ⩽
                                    
                                       
                                          q
                                       
                                       
                                          ∗
                                       
                                    
                                    (
                                    
                                       
                                          λ
                                       
                                       
                                          ′
                                       
                                    
                                    )
                                 
                              , for all λ′∈[λ
                              
                                 i
                              ,
                              λ
                              
                                 i+1]. Also, 
                                 
                                    -
                                    
                                       
                                          λ
                                       
                                       
                                          i
                                          +
                                          1
                                       
                                       
                                          2
                                       
                                    
                                    ⩽
                                    -
                                    
                                       
                                          (
                                          
                                             
                                                λ
                                             
                                             
                                                ′
                                             
                                          
                                          )
                                       
                                       
                                          2
                                       
                                    
                                 
                              , for all λ′∈[λ
                              
                                 i
                              ,
                              λ
                              
                                 i+1]. By adding these two inequalities, we obtain 
                                 
                                    
                                       
                                          u
                                       
                                       
                                          ˆ
                                       
                                    
                                    (
                                    
                                       
                                          λ
                                       
                                       
                                          i
                                       
                                    
                                    )
                                    ⩽
                                    
                                       
                                          u
                                       
                                       
                                          ∗
                                       
                                    
                                    (
                                    
                                       
                                          λ
                                       
                                       
                                          ′
                                       
                                    
                                    )
                                 
                              , for all λ′∈[λ
                              
                                 i
                              ,
                              λ
                              
                                 i+1]. Given some λ, let k be such that λ
                              ∈[λ
                              
                                 k
                              ,
                              λ
                              
                                 k+1]. Then,
                                 
                                    
                                       
                                          
                                             v
                                          
                                          
                                             ˆ
                                          
                                       
                                       (
                                       λ
                                       )
                                       =
                                       
                                          
                                             
                                                min
                                             
                                             
                                                i
                                                ⩾
                                                k
                                             
                                          
                                       
                                       
                                          
                                             u
                                          
                                          
                                             ˆ
                                          
                                       
                                       (
                                       
                                          
                                             λ
                                          
                                          
                                             i
                                          
                                       
                                       )
                                       ⩽
                                       
                                          
                                             
                                                min
                                             
                                             
                                                
                                                   
                                                      λ
                                                   
                                                   
                                                      ′
                                                   
                                                
                                                ⩾
                                                
                                                   
                                                      λ
                                                   
                                                   
                                                      k
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             u
                                          
                                          
                                             ∗
                                          
                                       
                                       (
                                       
                                          
                                             λ
                                          
                                          
                                             ′
                                          
                                       
                                       )
                                       ⩽
                                       
                                          
                                             
                                                min
                                             
                                             
                                                
                                                   
                                                      λ
                                                   
                                                   
                                                      ′
                                                   
                                                
                                                ⩾
                                                λ
                                             
                                          
                                       
                                       
                                          
                                             u
                                          
                                          
                                             ∗
                                          
                                       
                                       (
                                       
                                          
                                             λ
                                          
                                          
                                             ′
                                          
                                       
                                       )
                                       =
                                       
                                          
                                             v
                                          
                                          
                                             ∗
                                          
                                       
                                       (
                                       
                                          
                                             λ
                                          
                                          
                                             ′
                                          
                                       
                                       )
                                       ,
                                    
                                 
                              so that 
                                 
                                    
                                       
                                          v
                                       
                                       
                                          ˆ
                                       
                                    
                                    (
                                    λ
                                    )
                                 
                               is always an underestimate of v
                              ∗(λ).

We now prove a reverse inequality. Fix some λ and let k be such that λ
                              ∈[λ
                              
                                 k
                              ,
                              λ
                              
                                 k+1]. Let i
                              ⩾
                              k be such that 
                                 
                                    
                                       
                                          v
                                       
                                       
                                          ˆ
                                       
                                    
                                    (
                                    λ
                                    )
                                    =
                                    
                                       
                                          u
                                       
                                       
                                          ˆ
                                       
                                    
                                    (
                                    
                                       
                                          λ
                                       
                                       
                                          i
                                       
                                    
                                    )
                                 
                              . Let also 
                                 
                                    
                                       
                                          λ
                                       
                                       
                                          ¯
                                       
                                    
                                    ∈
                                    [
                                    
                                       
                                          λ
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    
                                       
                                          λ
                                       
                                       
                                          i
                                          +
                                          1
                                       
                                    
                                    ]
                                 
                               be such that 
                                 
                                    
                                       
                                          q
                                       
                                       
                                          ∗
                                       
                                    
                                    (
                                    
                                       
                                          λ
                                       
                                       
                                          ¯
                                       
                                    
                                    )
                                    =
                                    
                                       
                                          q
                                       
                                       
                                          ˆ
                                       
                                    
                                    (
                                    
                                       
                                          λ
                                       
                                       
                                          i
                                       
                                    
                                    )
                                 
                              . Note that
                                 
                                    (6)
                                    
                                       
                                          
                                             λ
                                          
                                          
                                             i
                                             +
                                             1
                                          
                                          
                                             2
                                          
                                       
                                       -
                                       
                                          
                                             
                                                
                                                   λ
                                                
                                                
                                                   ¯
                                                
                                             
                                          
                                          
                                             2
                                          
                                       
                                       ⩽
                                       
                                          
                                             λ
                                          
                                          
                                             i
                                             +
                                             1
                                          
                                          
                                             2
                                          
                                       
                                       -
                                       
                                          
                                             λ
                                          
                                          
                                             i
                                          
                                          
                                             2
                                          
                                       
                                       =
                                       δ
                                       (
                                       
                                          
                                             λ
                                          
                                          
                                             i
                                          
                                       
                                       +
                                       
                                          
                                             λ
                                          
                                          
                                             i
                                             +
                                             1
                                          
                                       
                                       )
                                       ⩽
                                       2
                                       δ
                                       (
                                       KT
                                       +
                                       δ
                                       )
                                       ⩽
                                       3
                                       δ
                                       KT
                                       .
                                    
                                 
                              Then,
                                 
                                    
                                       
                                          
                                             v
                                          
                                          
                                             ˆ
                                          
                                       
                                       (
                                       λ
                                       )
                                       
                                          
                                             =
                                          
                                          
                                             (
                                             a
                                             )
                                          
                                       
                                       
                                          
                                             u
                                          
                                          
                                             ˆ
                                          
                                       
                                       (
                                       
                                          
                                             λ
                                          
                                          
                                             i
                                          
                                       
                                       )
                                       
                                          
                                             =
                                          
                                          
                                             (
                                             a
                                             )
                                          
                                       
                                       
                                          
                                             q
                                          
                                          
                                             ˆ
                                          
                                       
                                       (
                                       
                                          
                                             λ
                                          
                                          
                                             i
                                          
                                       
                                       )
                                       -
                                       
                                          
                                             λ
                                          
                                          
                                             i
                                             +
                                             1
                                          
                                          
                                             2
                                          
                                       
                                       
                                          
                                             =
                                          
                                          
                                             (
                                             c
                                             )
                                          
                                       
                                       
                                          
                                             q
                                          
                                          
                                             ∗
                                          
                                       
                                       (
                                       
                                          
                                             λ
                                          
                                          
                                             ¯
                                          
                                       
                                       )
                                       -
                                       
                                          
                                             λ
                                          
                                          
                                             i
                                             +
                                             1
                                          
                                          
                                             2
                                          
                                       
                                       
                                          
                                             ⩾
                                          
                                          
                                             (
                                             d
                                             )
                                          
                                       
                                       
                                          
                                             q
                                          
                                          
                                             ∗
                                          
                                       
                                       (
                                       
                                          
                                             λ
                                          
                                          
                                             ¯
                                          
                                       
                                       )
                                       -
                                       
                                          
                                             
                                                
                                                   λ
                                                
                                                
                                                   ¯
                                                
                                             
                                          
                                          
                                             2
                                          
                                       
                                       -
                                       3
                                       δ
                                       KT
                                       
                                          
                                             =
                                          
                                          
                                             (
                                             e
                                             )
                                          
                                       
                                       
                                          
                                             u
                                          
                                          
                                             ∗
                                          
                                       
                                       (
                                       
                                          
                                             λ
                                          
                                          
                                             ¯
                                          
                                       
                                       )
                                       -
                                       3
                                       δ
                                       KT
                                       
                                          
                                             ⩾
                                          
                                          
                                             (
                                             f
                                             )
                                          
                                       
                                       
                                          
                                             v
                                          
                                          
                                             ∗
                                          
                                       
                                       (
                                       
                                          
                                             λ
                                          
                                          
                                             ¯
                                          
                                       
                                       )
                                       -
                                       3
                                       δ
                                       KT
                                       
                                          
                                             ⩾
                                          
                                          
                                             (
                                             g
                                             )
                                          
                                       
                                       
                                          
                                             v
                                          
                                          
                                             ∗
                                          
                                       
                                       (
                                       λ
                                       -
                                       δ
                                       )
                                       -
                                       3
                                       δ
                                       KT
                                       
                                          
                                             ⩾
                                          
                                          
                                             (
                                             h
                                             )
                                          
                                       
                                       
                                          
                                             v
                                          
                                          
                                             ∗
                                          
                                       
                                       (
                                       λ
                                       -
                                       ∊
                                       )
                                       -
                                       ∊
                                       .
                                    
                                 
                              In the above, (a) holds by the definition of i; (b) by the definition of 
                                 
                                    
                                       
                                          u
                                       
                                       
                                          ˆ
                                       
                                    
                                    (
                                    
                                       
                                          λ
                                       
                                       
                                          i
                                       
                                    
                                    )
                                 
                              ; (c) by the definition of 
                                 
                                    
                                       
                                          λ
                                       
                                       
                                          ¯
                                       
                                    
                                 
                              ; and (d) follows from Eq. (6). Equality (e) follows from the definition of u
                              ∗(·). Inequality (f) follows from the definition of v
                              ∗(·); and (g) is obtained because v
                              ∗(·) is nondecreasing and because 
                                 
                                    
                                       
                                          λ
                                       
                                       
                                          ¯
                                       
                                    
                                    ⩾
                                    λ
                                    -
                                    δ
                                 
                              . (The latter fact is seen as follows: (i) if i
                              >
                              k, then 
                                 
                                    λ
                                    ⩽
                                    
                                       
                                          λ
                                       
                                       
                                          k
                                          +
                                          1
                                       
                                    
                                    ⩽
                                    
                                       
                                          λ
                                       
                                       
                                          i
                                       
                                    
                                    ⩽
                                    
                                       
                                          λ
                                       
                                       
                                          ¯
                                       
                                    
                                 
                              ; (ii) if i
                              =
                              k, then both λ and 
                                 
                                    
                                       
                                          λ
                                       
                                       
                                          ¯
                                       
                                    
                                 
                               belong to [λ
                              
                                 k
                              ,
                              λ
                              
                                 k+1], and their difference is at most δ.) Inequality (h) is obtained because of the definition ∊
                              =3δKT, the observation δ
                              <
                              ∊, and the monotonicity of v
                              ∗(·).□


                        Theorem 9 allows us to construct an approximate Pareto boundary. In addition, one may be interested in obtaining corresponding policies. As is common in Markov decision theory, the construction of suitable policies is implicit in value function calculations, and is immediate from the proof Theorem 9, as we now describe. Suppose that are given some λ that happens to lie in some [λ
                        
                           k
                        ,
                        λ
                        
                           k+1]. As in the proof of the theorem, we find some i such that 
                           
                              
                                 
                                    v
                                 
                                 
                                    ˆ
                                 
                              
                              (
                              λ
                              )
                              =
                              
                                 
                                    u
                                 
                                 
                                    ˆ
                                 
                              
                              (
                              
                                 
                                    λ
                                 
                                 
                                    i
                                 
                              
                              )
                              =
                              
                                 
                                    q
                                 
                                 
                                    ˆ
                                 
                              
                              (
                              
                                 
                                    λ
                                 
                                 
                                    i
                                 
                              
                              )
                              -
                              
                                 
                                    λ
                                 
                                 
                                    i
                                    +
                                    1
                                 
                                 
                                    2
                                 
                              
                           
                        . From the definition of 
                           
                              
                                 
                                    q
                                 
                                 
                                    ˆ
                                 
                              
                              (
                              
                                 
                                    λ
                                 
                                 
                                    i
                                 
                              
                              )
                           
                        , there exists some 
                           
                              (
                              
                                 
                                    λ
                                 
                                 
                                    ¯
                                 
                              
                              ,
                              q
                              )
                              ∈
                              
                                 
                                    P
                                 
                                 
                                    MQ
                                 
                              
                           
                         with 
                           
                              
                                 
                                    λ
                                 
                                 
                                    ¯
                                 
                              
                              ∈
                              [
                              
                                 
                                    λ
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    λ
                                 
                                 
                                    i
                                    +
                                    1
                                 
                              
                              ]
                           
                         and 
                           
                              q
                              =
                              
                                 
                                    q
                                 
                                 
                                    ∗
                                 
                              
                              (
                              
                                 
                                    λ
                                 
                                 
                                    ¯
                                 
                              
                              )
                              =
                              
                                 
                                    q
                                 
                                 
                                    ˆ
                                 
                              
                              (
                              
                                 
                                    λ
                                 
                                 
                                    i
                                 
                              
                              )
                           
                        . The key observation is that we can easily find a policy for which 
                           
                              E
                              [
                              
                                 
                                    W
                                 
                                 
                                    T
                                 
                              
                              ]
                           
                         and 
                           
                              E
                              [
                              
                                 
                                    W
                                 
                                 
                                    T
                                 
                                 
                                    2
                                 
                              
                              ]
                           
                         are equal to 
                           
                              
                                 
                                    λ
                                 
                                 
                                    ¯
                                 
                              
                           
                         and q, respectively. This is done by finding a corresponding state-action frequency vector in the polyhedron Z(Π
                        
                           h,u
                        ) (which is a linear programming feasibility problem), and expressing that vector as a convex combination of extreme points of Z(Π
                        
                           h,u
                        ). As is well known, extreme points of Z(Π
                        
                           h,u
                        ) are associated with deterministic policies. The desired policy is a randomized policy obtained by combining these deterministic policies according to the coefficients involved in the convex combination. The policy constructed in this manner has a variance equal to
                           
                              
                                 q
                                 -
                                 
                                    
                                       (
                                       
                                          
                                             λ
                                          
                                          
                                             ¯
                                          
                                       
                                       )
                                    
                                    
                                       2
                                    
                                 
                                 =
                                 
                                    
                                       q
                                    
                                    
                                       ˆ
                                    
                                 
                                 (
                                 
                                    
                                       λ
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 -
                                 
                                    
                                       (
                                       
                                          
                                             λ
                                          
                                          
                                             ¯
                                          
                                       
                                       )
                                    
                                    
                                       2
                                    
                                 
                                 ⩽
                                 
                                    
                                       q
                                    
                                    
                                       ˆ
                                    
                                 
                                 (
                                 
                                    
                                       λ
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 -
                                 
                                    
                                       λ
                                    
                                    
                                       i
                                       +
                                       1
                                    
                                    
                                       2
                                    
                                 
                                 +
                                 3
                                 δ
                                 KT
                                 =
                                 
                                    
                                       v
                                    
                                    
                                       ˆ
                                    
                                 
                                 (
                                 λ
                                 )
                                 +
                                 3
                                 δ
                                 KT
                                 ⩽
                                 
                                    
                                       v
                                    
                                    
                                       ˆ
                                    
                                 
                                 (
                                 λ
                                 )
                                 +
                                 ∊
                                 ,
                              
                           
                        where the first inequality is obtained as in Eq. (6). We have thus found a policy whose performance is within ∊ of the computed approximately optimal performance 
                           
                              
                                 
                                    v
                                 
                                 
                                    ˆ
                                 
                              
                              (
                              λ
                              )
                           
                        .

Similar policy constructions are possible in the other cases considered in this paper (as, for example, in the next section). Given that these constructions do not involve any new ideas, we will not repeat them.

When rewards are arbitrary, we can discretize the rewards and obtain a new MDP. The new MDP is equivalent to one with integer rewards to which the algorithm of the preceding subsection can be applied. This is a legitimate approximation algorithm for the original problem because, as we will show shortly, the function v
                        ∗(·) changes very little when we discretize using a fine enough discretization.

We are given an original MDP 
                           
                              M
                              =
                              (
                              T
                              ,
                              S
                              ,
                              A
                              ,
                              R
                              ,
                              p
                              ,
                              g
                              )
                           
                         in which the rewards are rational numbers in the interval [−K,
                        K], and an approximation parameter ∊. We fix a positive number δ, a discretization parameter whose value will be specified later. We then construct a new MDP 
                           
                              
                                 
                                    M
                                 
                                 
                                    ′
                                 
                              
                              =
                              (
                              T
                              ,
                              S
                              ,
                              A
                              ,
                              
                                 
                                    R
                                 
                                 
                                    ′
                                 
                              
                              ,
                              p
                              ,
                              
                                 
                                    g
                                 
                                 
                                    ′
                                 
                              
                              )
                           
                        , in which the rewards are rounded down to an integer multiple of δ. More precisely, all elements of the reward range 
                           
                              
                                 
                                    R
                                 
                                 
                                    ′
                                 
                              
                           
                         are integer multiples of δ, and for every 
                           
                              t
                              ,
                              s
                              ,
                              a
                              ∈
                              {
                              0
                              ,
                              1
                              ,
                              …
                              ,
                              T
                              -
                              1
                              }
                              ×
                              S
                              ×
                              A
                           
                        , and any integer n, we have
                           
                              
                                 
                                    
                                       g
                                    
                                    
                                       t
                                    
                                    
                                       ′
                                    
                                 
                                 (
                                 δ
                                 n
                                 |
                                 s
                                 ,
                                 a
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          r
                                          :
                                          
                                          δ
                                          n
                                          ⩽
                                          r
                                          <
                                          δ
                                          (
                                          n
                                          +
                                          1
                                          )
                                       
                                    
                                 
                                 
                                    
                                       g
                                    
                                    
                                       t
                                    
                                 
                                 (
                                 r
                                 |
                                 s
                                 ,
                                 a
                                 )
                                 .
                              
                           
                        We denote by J, Q and by J′, Q′ the first and second moments of the total reward in the original and new MDPs, respectively. Let Π
                        
                           h,u
                         and 
                           
                              
                                 
                                    Π
                                 
                                 
                                    h
                                    ,
                                    u
                                 
                                 
                                    ′
                                 
                              
                           
                         be the sets of (randomized, history-based) policies in 
                           
                              M
                           
                         and 
                           
                              
                                 
                                    M
                                 
                                 
                                    ′
                                 
                              
                           
                        , respectively. Let P
                        
                           MQ
                         and 
                           
                              
                                 
                                    P
                                 
                                 
                                    MQ
                                 
                                 
                                    ′
                                 
                              
                           
                         be the associated polyhedra.

We want to to argue that the mean–variance tradeoff curves for the two MDPs are close to each other. This is not entirely straightforward because the augmented state spaces (which include the possible values of the cumulative rewards W
                        
                           t
                        ) are different for the two problems and, therefore, the sets of policies are also different. A conceptually simple but somewhat tedious approach involves an argument along the lines of Whitt (1978), Whitt (1979), generalized to the case of constrained MDPs; we outline such an argument in Section 6.3. Here, we follow an alternative approach, based on a coupling argument.
                           Proposition 1
                           
                              There exists a polynomial function c(K,
                              
                              T) such that the Hausdorf distance between P
                              
                                 MQ
                               
                              and 
                              
                                 
                                    
                                       
                                          P
                                       
                                       
                                          MQ
                                       
                                       
                                          ′
                                       
                                    
                                 
                               
                              is bounded above by 2KT
                              
                                 2
                              
                              δ. More precisely,
                              
                                 
                                    (a)
                                    
                                       For every policy π
                                       ∈
                                       Π
                                       
                                          h,u
                                       
                                       , there exists a policy 
                                       
                                          
                                             
                                                
                                                   π
                                                
                                                
                                                   ′
                                                
                                             
                                             ∈
                                             
                                                
                                                   Π
                                                
                                                
                                                   h
                                                   ,
                                                   u
                                                
                                                
                                                   ′
                                                
                                             
                                          
                                        
                                       such that
                                       
                                          
                                             
                                                
                                                   max
                                                
                                                
                                                   
                                                      
                                                         |
                                                         
                                                            
                                                               J
                                                            
                                                            
                                                               
                                                                  
                                                                     π
                                                                  
                                                                  
                                                                     ′
                                                                  
                                                               
                                                            
                                                            
                                                               ′
                                                            
                                                         
                                                         -
                                                         
                                                            
                                                               J
                                                            
                                                            
                                                               π
                                                            
                                                         
                                                         |
                                                         ,
                                                         
                                                         |
                                                         
                                                            
                                                               Q
                                                            
                                                            
                                                               
                                                                  
                                                                     π
                                                                  
                                                                  
                                                                     ′
                                                                  
                                                               
                                                            
                                                            
                                                               ′
                                                            
                                                         
                                                         -
                                                         
                                                            
                                                               Q
                                                            
                                                            
                                                               π
                                                            
                                                         
                                                         |
                                                      
                                                   
                                                
                                                ⩽
                                                2
                                                
                                                   
                                                      KT
                                                   
                                                   
                                                      2
                                                   
                                                
                                                δ
                                                .
                                             
                                          
                                       
                                    


                                       Conversely, for every policy 
                                       
                                          
                                             
                                                
                                                   Π
                                                
                                                
                                                   h
                                                   ,
                                                   u
                                                
                                                
                                                   ′
                                                
                                             
                                          
                                       
                                       , there exists a policy Π
                                       
                                          h,u
                                        
                                       such that the above inequality again holds.
                                    

We denote by d(r) the discretized value of a reward r, that is, 
                                 
                                    d
                                    (
                                    r
                                    )
                                    =
                                    max
                                    {
                                    n
                                    δ
                                    :
                                    n
                                    δ
                                    ⩽
                                    r
                                    ,
                                    
                                    n
                                    ∈
                                    Z
                                    }
                                 
                              . Let us consider a third MDP 
                                 
                                    
                                       
                                          M
                                       
                                       
                                          ″
                                       
                                    
                                 
                               which is identical to 
                                 
                                    
                                       
                                          M
                                       
                                       
                                          ′
                                       
                                    
                                 
                              , except that its rewards 
                                 
                                    
                                       
                                          R
                                       
                                       
                                          t
                                       
                                       
                                          ″
                                       
                                    
                                 
                               are generated as follows. (We follow the convention of using a single or double prime to indicate variables associated with 
                                 
                                    
                                       
                                          M
                                       
                                       
                                          ′
                                       
                                    
                                 
                               or 
                                 
                                    
                                       
                                          M
                                       
                                       
                                          ″
                                       
                                    
                                 
                              , respectively.) A random variable R
                              
                                 t
                               is generated according to the distribution prescribed by g
                              
                                 t
                              (r∣s
                              
                                 t
                              ,
                              a
                              
                                 t
                              ), and its value is observed by the decision maker, who then incurs the reward 
                                 
                                    
                                       
                                          R
                                       
                                       
                                          t
                                       
                                       
                                          ″
                                       
                                    
                                    =
                                    d
                                    (
                                    
                                       
                                          R
                                       
                                       
                                          t
                                       
                                    
                                    )
                                 
                              . Let 
                                 
                                    
                                       
                                          P
                                       
                                       
                                          MQ
                                       
                                       
                                          ″
                                       
                                    
                                 
                               be the polyhedron associated with 
                                 
                                    
                                       
                                          M
                                       
                                       
                                          ″
                                       
                                    
                                 
                              . We claim that 
                                 
                                    
                                       
                                          P
                                       
                                       
                                          MQ
                                       
                                       
                                          ″
                                       
                                    
                                    =
                                    
                                       
                                          P
                                       
                                       
                                          MQ
                                       
                                       
                                          ′
                                       
                                    
                                 
                              . The only difference between 
                                 
                                    
                                       
                                          M
                                       
                                       
                                          ′
                                       
                                    
                                 
                               and 
                                 
                                    
                                       
                                          M
                                       
                                       
                                          ″
                                       
                                    
                                 
                               is that the decision maker in 
                                 
                                    
                                       
                                          M
                                       
                                       
                                          ″
                                       
                                    
                                 
                               has access to the additional information R
                              
                                 t
                              
                              −
                              d(R
                              
                                 t
                              ). However, this information is incosequential: it does not affect the future transition probabilities or reward distributions. Thus, R
                              
                                 t
                              
                              −
                              d(R
                              
                                 t
                              ) can only be useful as an additional randomization variable. Since 
                                 
                                    
                                       
                                          P
                                       
                                       
                                          MQ
                                       
                                       
                                          ′
                                       
                                    
                                 
                               is the set of achievable pairs using general (history-based randomized) policies, having available an additional randomization variable does not change the polyhedron, and 
                                 
                                    
                                       
                                          P
                                       
                                       
                                          MQ
                                       
                                       
                                          ″
                                       
                                    
                                    =
                                    
                                       
                                          P
                                       
                                       
                                          MQ
                                       
                                       
                                          ′
                                       
                                    
                                 
                              . Thus, to complete the proof it suffices to show that the polyhedra P
                              
                                 MQ
                               and 
                                 
                                    
                                       
                                          P
                                       
                                       
                                          MQ
                                       
                                       
                                          ″
                                       
                                    
                                 
                               are close.

Let us compare the MDPs 
                                 
                                    M
                                 
                               and 
                                 
                                    
                                       
                                          M
                                       
                                       
                                          ″
                                       
                                    
                                 
                              . The information available to the decision maker is the same for these two MDPs (since all the history of reward truncations 
                                 
                                    
                                       
                                          {
                                          
                                             
                                                R
                                             
                                             
                                                τ
                                             
                                          
                                          -
                                          d
                                          (
                                          
                                             
                                                R
                                             
                                             
                                                τ
                                             
                                          
                                          )
                                          }
                                       
                                       
                                          τ
                                          =
                                          1
                                       
                                       
                                          t
                                          -
                                          1
                                       
                                    
                                 
                               is available in 
                                 
                                    
                                       
                                          M
                                       
                                       
                                          ″
                                       
                                    
                                 
                               for the decision at time t). Therefore, for every policy in one MDP, there exists a policy for the other under which (if we define the two MDPs on a common probability space, involving common random generators) the exact same sequence of states 
                                 
                                    (
                                    
                                       
                                          S
                                       
                                       
                                          t
                                       
                                    
                                    =
                                    
                                       
                                          S
                                       
                                       
                                          t
                                       
                                       
                                          ″
                                       
                                    
                                 
                              ), actions 
                                 
                                    (
                                    
                                       
                                          A
                                       
                                       
                                          t
                                       
                                    
                                    =
                                    
                                       
                                          A
                                       
                                       
                                          t
                                       
                                       
                                          ″
                                       
                                    
                                 
                              ), and random variables R
                              
                                 t
                               is realized. The only difference is that the rewards are R
                              
                                 t
                               and d(R
                              
                                 t
                              ), in 
                                 
                                    M
                                 
                               and 
                                 
                                    
                                       
                                          M
                                       
                                       
                                          ″
                                       
                                    
                                 
                              , respectively. Recall that 0⩽
                              R
                              
                                 t
                              
                              −
                              d(R
                              
                                 t
                              )⩽
                              δ. We obtain that for every policy π
                              ∈
                              Π, there exists a policy π″∈
                              Π″ for which 
                                 
                                    0
                                    ⩽
                                    
                                       
                                          W
                                       
                                       
                                          T
                                       
                                    
                                    -
                                    
                                       
                                          W
                                       
                                       
                                          T
                                       
                                       
                                          ″
                                       
                                    
                                    =
                                    
                                       
                                          ∑
                                       
                                       
                                          τ
                                          =
                                          0
                                       
                                       
                                          T
                                          -
                                          1
                                       
                                    
                                    (
                                    
                                       
                                          R
                                       
                                       
                                          t
                                       
                                    
                                    -
                                    d
                                    (
                                    
                                       
                                          R
                                       
                                       
                                          t
                                       
                                    
                                    )
                                    )
                                    ⩽
                                    δ
                                    T
                                 
                              , and therefore, 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   W
                                                
                                                
                                                   T
                                                
                                                
                                                   2
                                                
                                             
                                             -
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               W
                                                            
                                                            
                                                               T
                                                            
                                                            
                                                               ″
                                                            
                                                         
                                                      
                                                   
                                                
                                                
                                                   2
                                                
                                             
                                          
                                       
                                    
                                    ⩽
                                    2
                                    
                                       
                                          KT
                                       
                                       
                                          2
                                       
                                    
                                    δ
                                 
                              . Taking expectations, we obtain ∣J
                              
                                 π
                              
                              −
                              J′′
                                 π
                              ∣⩽
                              Tδ, 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   Q
                                                
                                                
                                                   π
                                                
                                             
                                             -
                                             
                                                
                                                   Q
                                                
                                                
                                                   π
                                                
                                                
                                                   ″
                                                
                                             
                                          
                                       
                                    
                                    ⩽
                                    2
                                    
                                       
                                          KT
                                       
                                       
                                          2
                                       
                                    
                                    δ
                                 
                              . This completes the proof of part (a). The proof of part (b) is identical.□


                              There exists an algorithm that, given ∊, ν, and λ, outputs a value 
                              
                                 
                                    
                                       
                                          v
                                       
                                       
                                          ˆ
                                       
                                    
                                    (
                                    λ
                                    )
                                 
                               
                              that satisfies 
                              
                                 (5)
                              
                              , and which runs in time polynomial in 
                              
                                 
                                    |
                                    S
                                    |
                                 
                              
                              , 
                              
                                 
                                    |
                                    A
                                    |
                                    ,
                                    
                                    T
                                    ,
                                    
                                    K
                                    ,
                                    
                                    1
                                    /
                                    ∊
                                 
                              
                              , and 1/ν.
                           

Assume for simplicity that ν
                              =
                              ∊. Given the value of ∊, let δ be such that ∊/2=2KT
                              2
                              δ, and construct the discretized MDP 
                                 
                                    
                                       
                                          M
                                       
                                       
                                          ′
                                       
                                    
                                 
                              . Run the algorithm from Theorem 9 to find an (∊/2,∊/2)-approximation 
                                 
                                    
                                       
                                          v
                                       
                                       
                                          ˆ
                                       
                                    
                                 
                               for 
                                 
                                    
                                       
                                          M
                                       
                                       
                                          ′
                                       
                                    
                                 
                              . Using Proposition 1, it is not hard to verify that this yields an (∊,
                              ∊)-approximation of v
                              ∗(λ).□

There are two general approaches for constructing approximation algorithms. (i) One can discretize the problem, to obtain an easier one, and then apply an algorithm specially tailored to the discretized problem; this was the approach in the preceding subsection. (ii) One can design an exact (but inefficient) algorithm for the original problem and then implement the algorithm approximately. This approach will work provided the approximations do not build up excessively in the course of the algorithm. In this subsection, we elaborate on the latter approach.

We defined earlier the polyhedron P
                        
                           MQ
                         as the set of achievable first and second moments of the cumulative reward starting at time zero at the initial state. We extend this definition by considering intermediate times and arbitrary (intermediate) augmented states. We let
                           
                              (7)
                              
                                 
                                    
                                       C
                                    
                                    
                                       t
                                    
                                 
                                 (
                                 s
                                 ,
                                 w
                                 )
                                 =
                                 {
                                 (
                                 λ
                                 ,
                                 q
                                 )
                                 :
                                 ∃
                                 π
                                 ∈
                                 
                                    
                                       Π
                                    
                                    
                                       h
                                       ,
                                       u
                                    
                                 
                                 
                                 s
                                 .
                                 t
                                 .
                                 
                                 
                                    
                                       E
                                    
                                    
                                       π
                                    
                                 
                                 [
                                 
                                    
                                       W
                                    
                                    
                                       T
                                    
                                 
                                 |
                                 
                                    
                                       S
                                    
                                    
                                       t
                                    
                                 
                                 =
                                 s
                                 ,
                                 
                                 
                                    
                                       W
                                    
                                    
                                       t
                                    
                                 
                                 =
                                 w
                                 ]
                                 =
                                 λ
                                 
                                 and
                              
                           
                        
                        
                           
                              
                                 
                                    
                                       E
                                    
                                    
                                       π
                                    
                                 
                                 [
                                 
                                    
                                       W
                                    
                                    
                                       T
                                    
                                    
                                       2
                                    
                                 
                                 |
                                 
                                    
                                       S
                                    
                                    
                                       t
                                    
                                 
                                 =
                                 s
                                 ,
                                 
                                 
                                    
                                       W
                                    
                                    
                                       t
                                    
                                 
                                 =
                                 w
                                 )
                                 =
                                 q
                                 }
                                 .
                              
                           
                        Clearly, C
                        0(s,0)=
                        P
                        
                           MQ
                        . Using a straightforward backwards induction, it can be shown that C
                        
                           t
                        (·,·) satisfies the set-valued dynamic programming recursion
                           2
                           If X and Y are subsets of a vector space and α a scalar, we let α X
                              ={αx∣x
                              ∈
                              X} and X
                              +
                              Y
                              ={x
                              +
                              y∣x
                              ∈
                              X, y
                              ∈
                              Y}. Furthermore, if for every 
                                 
                                    a
                                    ∈
                                    A
                                 
                              , we have a set X
                              
                                 a
                              , then 
                                 
                                    
                                       
                                          conv
                                       
                                       
                                          a
                                          ∈
                                          A
                                       
                                    
                                    {
                                    
                                       
                                          X
                                       
                                       
                                          a
                                       
                                    
                                    }
                                 
                               is the convex hull of the union of these sets.
                        
                        
                           2
                        
                        
                           
                              (8)
                              
                                 
                                    
                                       C
                                    
                                    
                                       t
                                    
                                 
                                 (
                                 s
                                 ,
                                 w
                                 )
                                 =
                                 
                                    
                                       conv
                                    
                                    
                                       a
                                       ∈
                                       A
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   
                                                      
                                                         s
                                                      
                                                      
                                                         ′
                                                      
                                                   
                                                   ∈
                                                   S
                                                
                                             
                                          
                                          
                                             
                                                p
                                             
                                             
                                                t
                                             
                                          
                                          (
                                          
                                             
                                                s
                                             
                                             
                                                ′
                                             
                                          
                                          |
                                          s
                                          ,
                                          a
                                          )
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   r
                                                   ∈
                                                   R
                                                
                                             
                                          
                                          
                                             
                                                g
                                             
                                             
                                                t
                                             
                                          
                                          (
                                          r
                                          |
                                          s
                                          ,
                                          a
                                          )
                                          
                                             
                                                C
                                             
                                             
                                                t
                                                +
                                                1
                                             
                                          
                                          (
                                          
                                             
                                                s
                                             
                                             
                                                ′
                                             
                                          
                                          ,
                                          w
                                          +
                                          r
                                          )
                                       
                                    
                                 
                                 ,
                              
                           
                        for every 
                           
                              s
                              ∈
                              S
                              ,
                              w
                              ∈
                              R
                           
                        , and for t
                        =0,1, 2, …, T
                        −1, initialized with the boundary conditions
                           
                              (9)
                              
                                 
                                    
                                       C
                                    
                                    
                                       T
                                    
                                 
                                 (
                                 s
                                 ,
                                 w
                                 )
                                 =
                                 {
                                 (
                                 w
                                 ,
                                 
                                    
                                       w
                                    
                                    
                                       2
                                    
                                 
                                 )
                                 }
                                 .
                              
                           
                        A simple inductive proof shows that the sets C
                        
                           t
                        (s,
                        w) are polyhedra; this is because C
                        
                           T
                        (s,
                        w) is either empty or a singleton and because the sum or convex hull of finitely many polyhedra is a polyhedron. Thus, the recursion involves a finite amount of computation, e.g., by representing each polyhedron in terms of its finitely many extreme points. In the worst case, this translates into an exponential time algorithm, because of the possibly large number of extreme points. However, such an algorithm can also be implemented approximately. If we allow for the introduction of an O(∊/T) error at each stage (where error is measured in terms of the Hausdorf distance), we can work with approximating polyhedra that involve only O(1/∊) extreme points, while ending up with a O(∊) total error; this is because we are approximating polyhedra in the plane, as opposed to higher dimensions where the dependence on ∊ would have been worse dependence. The details are straightforward but somewhat tedious and are omitted. On the other hand, in practice, this approach is likely to be faster than the algorithm of the preceding subsection.

@&#CONCLUSIONS@&#

We have shown that mean–variance optimization problems for MDPs are typically NP-hard, but sometimes admit pseudopolynomial approximation algorithms. We only considered finite horizon problems, but it is clear that the negative results carry over to their infinite horizon counterparts. Furthermore, given that the contribution of the tail of the time horizon in infinite horizon discounted problems (or in “proper” stochastic shortest path problems as in Bertsekas (1995)) can be made arbitrarily small, our approximation algorithms can also yield approximation algorithms for infinite horizon problems.

Two more problems of some interest deal with finding a policy that has the smallest possible, or the largest possible variance. There is not much we can say here, except for the following:
                        
                           (a)
                           The smallest possible variance is attained by a deterministic policy, that is,
                                 
                                    
                                       
                                          
                                             
                                                min
                                             
                                             
                                                π
                                                ∈
                                                
                                                   
                                                      Π
                                                   
                                                   
                                                      h
                                                      ,
                                                      u
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             V
                                          
                                          
                                             π
                                          
                                       
                                       =
                                       
                                          
                                             
                                                min
                                             
                                             
                                                π
                                                ∈
                                                
                                                   
                                                      Π
                                                   
                                                   
                                                      h
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             V
                                          
                                          
                                             π
                                          
                                       
                                       .
                                    
                                 
                              This is proved using the inequality 
                                 
                                    
                                       
                                          var
                                       
                                       
                                          π
                                       
                                    
                                    (
                                    
                                       
                                          W
                                       
                                       
                                          T
                                       
                                    
                                    )
                                    ⩾
                                    
                                       
                                          E
                                       
                                       
                                          π
                                       
                                    
                                    [
                                    
                                       
                                          var
                                       
                                       
                                          π
                                       
                                    
                                    (
                                    
                                       
                                          W
                                       
                                       
                                          T
                                       
                                    
                                    |
                                    
                                       
                                          U
                                       
                                       
                                          0
                                          :
                                          T
                                       
                                    
                                    )
                                    ]
                                 
                              .

Variance will be maximized, in general, by a randomized policy. To see this, consider a single stage problem and two actions with deterministic rewards, equal to 0 and 1, respectively. Variance is maximized by assigning probability 1/2 to each of the actions. The variance maximization problem is equivalent to maximizing the concave function q
                              −
                              λ
                              2 subject to (λ,
                              q)∈
                              P
                              
                                 MQ
                              . This is a quadratic programming problem over the polyhedron P
                              
                                 MQ
                               and therefore admits a pseudopolynomial time algorithm, when the rewards are integer.

Our results suggest several interesting directions for future research, which we briefly outline below.

First, our negative results apply to general MDPs. It would be interesting to determine whether the hardness results remain valid for specially structured MDPs. One possibly interesting special case involves multi-armed bandit problems: there are n separate MDPs (“arms”); at each time step, the decision maker has to decide which MDP to activate, while the other MDPs remain inactive. Of particular interest here are index policies that compute a value (“index”) for each MDP and select an MDP with maximal index; such policies are often optimal for the classical formulations (see Gittins (1979) and Whittle (1988)). Obtaining a policy that uses some sort of an index for the mean–variance problem or alternatively proving that such a policy cannot exist would be interesting.

Second, a number of complexity questions have been left open. We list a few of them:
                        
                           (a)
                           Is there a pseudopolynomial time algorithm for computing v
                              ∗(λ) or λ
                              ∗(v) exactly?

Is there a polynomial or pseudopolynomial time algorithm that computes v
                              ∗(λ) or λ
                              ∗(v) within a uniform error bound ∊?

Is the problem of computing 
                                 
                                    
                                       
                                          v
                                       
                                       
                                          ˆ
                                       
                                    
                                    (
                                    λ
                                    )
                                 
                               with the properties in Eq. (5) NP-hard?

Is there a pseudopolynomial time algorithm the smallest possible variance in the absence of any constraints on the mean cumulative reward?

Third, bias-variance tradeoffs may pay an important role in speeding up certain control and learning heuristics, such as those involving control variates (Meyn, 2008). Perhaps mean–variance optimization can be used to address the exploration/exploitation tradeoff in model-based reinforcement learning, with variance reduction serving as a means to reduce the exploration time (see Sutton & Barto (1998) for a general discussion of exploration–exploitation in reinforcement learning). Of course, in light of the computational complexity of bias-variance tradeoffs, incorporating bias-variance tradeoffs in learning makes sense only if experimentation is nearly prohibitive and computation time is cheap. Such an approach could be particularly useful if a coarse, low-complexity, approximate solution of a bias-variance tradeoff problem can result in significant exploration speedup.

Fourth, we only considered mean–variance tradeoffs in this paper. However, there are other interesting and potentially useful criteria that can be used to incorporate risk into multi-stage decision making. For example, Liu and Koenig (2005) consider a utility function with a single switch. Many other risk aware criteria have been considered in the single stage case. It would be interesting to develop a comprehensive theory for the complexity of solving multi-stage decision problems under general (monotone convex or concave) utility function and under risk constraints. This is especially interesting for the approximation algorithms presented in Section 6.

Finally, it is reasonable to expect that our positive results (on approximation algorithms) can be extended to problems involving continuous states and actions and/or unbounded rewards, by first discretizing the problem, truncating the rewards, and then applying our algorithms to a discrete problem. Of course, one would have to deal with the generic issues that arise in discretizing MDPs Whitt (1978), Whitt (1979); we expect this line of work to be tedious without offering any substantial new insights, and have refrained from pursuing it in this paper.

@&#ACKNOWLEDGMENTS@&#

The authors are grateful to the reviewers for their constructive comments. This research was partially supported by the Israel Science Foundation (contract 890015), a Horev Fellowship, and the National Science Foundation under grant CMMI-0856063. A preliminary version of this paper appeared at the 28th International Conference on Machine Learning.

@&#REFERENCES@&#

