@&#MAIN-TITLE@&#Road traffic density estimation using microscopic and macroscopic parameters

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We propose an algorithm for road traffic congestion estimation from video scenes.


                        
                        
                           
                           We compare between macroscopic and microscopic parameters in terms of accuracy.


                        
                        
                           
                           The method proposed is accurate, and it is computationally inexpensive.


                        
                        
                           
                           It does not require segmentation or tracking of vehicles.


                        
                        
                           
                           It is robust towards illumination changes.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Road traffic density estimation

Microscopic and macroscopic traffic parameters

Motion detection and tracking

KNN

LVQ

SVM

@&#ABSTRACT@&#


               
               
                  In this paper we present a comparative study of two approaches for road traffic density estimation. The first approach uses the microscopic parameters which are extracted using both motion detection and tracking methods from a video sequence, and the second approach uses the macroscopic parameters which are directly estimated by analyzing the global motion in the video scene. The extracted parameters are applied to three classifiers, the K Nearest Neighbor (KNN) classifier, the LVQ classifier and the SVM classifier, in order to classify the road traffic in three categories: light, medium and heavy. The methods are compared based on their robustness to the classification of different road traffic states. The goal of this study is to propose an algorithm for road traffic density estimation with a high precision.
               
            

@&#INTRODUCTION@&#

The recent technological advances have made vehicles a lot safer, but in return the road environment has become more complex. This is mainly due to the rapid increase in the number of vehicles and the resulting consequences, such as traffic accidents, road congestions… etc.

Road traffic congestion is a serious problem resulting in a large number of negative effects both economically and ecologically. Accidents, construction works, bad weather and poor traffic signal timing are likely to cause the traffic congestion.

Intelligent transport systems (ITS) are the most recent solutions for this problem. More recently, the use of camera networks has shown promise in traffic monitoring. In contrast to loop detectors, video based systems are less disruptive, less costly to install and allow for a more detailed understanding of traffic flow patterns.

Most existing approaches for classifying road traffic videos use a combination of segmentation and tracking [1–10]. The general procedure consists of the following three steps: (i) motion detection, (ii) tracking of the individual vehicles, and (iii) combining trajectories' information to derive an overall description of the traffic flow. The vehicle tracking framework has, as a disadvantage, an accuracy that is dependent on the quality of the segmentation. The segmentation task becomes more difficult with the presence of adverse environmental conditions, such as lighting (e.g. overcast, glare, shadows, occlusion, and blurring). Furthermore, segmentation cannot be performed reliably on low resolution images where the vehicles are only represented by a few pixels. Tracking algorithms also have problems when there are many objects in the scene, which is typically the case for highway scenes with congestion.

L. Huang and M. Barth [1] proposed a multi-vehicle tracking approach, which combines both local feature tracking and a global color probability model. In cases with low occlusion, corner features detection and tracking algorithms can be used to estimate vehicle positions and trajectories [2]. When there is a high degree of occlusion, corner features can be tracked to provide position estimates of moving objects, then a color probability can be calculated in the occluded area to determine which object each pixel belongs to. This approach is adapted to both stationary and moving video cameras.

Y. Kee and H.Yo-Sung [3] proposed a vehicle tracking algorithm that takes a new occlusion reasoning approach. They considered two different types of occlusions: explicit occlusion and implicit occlusion. Explicit occlusion represents the following situation: after two or more vehicles appear separately, they are merged due to some occlusion conditions. Implicit occlusion represents the initial occlusion where two or multiple vehicles appear as a single object. The authors also proposed a traffic flow extraction method with the velocity and trajectory of the moving vehicles. The proposed vehicle tracking system is composed of three parts: vehicle segmentation, vehicle tracking, and traffic parameter extraction. The vehicle segmentation part separates moving vehicles from their background. They employed the adaptive background approach, which does not update the background of moving objects. They also designed a 2D token-based algorithm for vehicle tracking, using Kalman filtering which has a low computational complexity. The traffic parameters extraction part estimates the traffic parameters, such as the vehicle count and the average speed. It also extracts the traffic flow. They then evaluated the proposed algorithm with some MPEG-7 test sequences.

J. B. Kim, and C. W. Lee [4] proposed a wavelet-based vehicle tracking system for automatic traffic surveillance. In order to meet real-time requirements they used an adaptive thresholding and Wavelet-based Neural Network (NN). First, moving regions are extracted by performing a frame difference analysis on two consecutive frames using adaptive thresholding. Second, the Wavelet-based NN is used for recognizing the vehicles in the extracted moving regions. Wavelet Transform (WT) is adopted to decompose an image, and a particular frequency band is selected as an input for the NN, for vehicles recognition. Third, vehicles are tracked by using position coordinates and wavelet features difference values for correspondence in recognized vehicle regions.

G. Mo and S. Zhang [5] adopted a multiple video object segmentation algorithm for vehicle detection. The algorithm consists of a training step and a segmentation step. At the training step, a codebook method is used for training: First, a scale-invariant feature transform method [6] is used to extract the features from vehicle image samples. Secondly, the images are segmented into small patches to cluster. Third, a new way is used to activate the codebook that can largely reduce the numbers of vehicles sample images: the key idea is to automatically learn a relatively large number of simple and compact appearance prototypes and represent the complex appearance distribution in relation to them. To obtain a set of informative locations for each image an interest point detector is applied. The amount of data to be processed can be reduced by extracting features in those locations, while the interest point detector preference for certain structures assures that “similar” regions are sampled on different objects. At the segmentation step, an implicit shape model is used to combine the recognition knowledge and the segmentation knowledge together. Finally, the authors used this algorithm to implement a prototype of a traffic flow analysis system, and achieved the expected results.

In the work of F. Bardet and T. Chateau [7] multi-vehicles are tracked through a Markov Chain Monte-Carlo Particle Filter (MCMCPF) method. They have shown that integrating a simple vehicle kinematic model within this tracker allows for estimating the trajectories of a set of vehicles, with a moderate number of particles. Their paper also addresses vehicle tracking.

Alternatively, several approaches have attempted to recover a holistic representation (macroscopic view) of traffic flow information directly, thereby avoiding the need for detecting and tracking individual moving objects. In Ref. [11] F. Porikli and X. Li proposed an unsupervised, low-latency traffic congestion estimation algorithm that operates on the MPEG video data. They extracted congestion features directly in the compressed domain, and employed Gaussian Mixture Hidden Markov Models (GM–HMM) to detect traffic condition. First, they constructed a multi-dimensional feature vector from the parsed DCT coefficients and motion vectors. Then, they trained a set of left-to-right HMM chains corresponding to traffic patterns and used a Maximum Likelihood (ML) criterion to determine the state from the outputs of the separate HMM chains. They then calculated a confidence score to assess the reliability of the detection results. The proposed method is computationally efficient and modular. Their tests prove that the feature vector is invariant to different illumination conditions, e.g. sunny, cloudy. Furthermore, they did not have to impose different models for different camera setups, thus they significantly reduced the system initialization workload and improved its adaptability. However the video should be recorded in MPEG format.

A. B. Chan and N. Vasconcelos [12] proposed a method to model the traffic flow in a video using a holistic generative model that does not require segmentation or tracking. In particular, they adopted the dynamic texture model, an auto-regressive stochastic process, which encodes the appearance and the underlying motion separately into two probability distributions. With this representation, retrieval of similar video sequences and classification of traffic congestion can be performed using the Kullback–Leibler divergence and the Martin distance [13]. A drawback of this approach is the large computational load in fitting the model. As a result analysis is limited to relatively small image patches and might make this approach impractical for application to real-time traffic monitoring.

K.G. Derpanis and R.P. Wildes in Ref. [14] described a system for classifying traffic congestion videos based on their observed visual dynamics. The proposed system treats traffic flow identification as an instance of dynamic texture, i.e. as spatiotemporal image patterns, best characterized in terms of the aggregate dynamics of a set of constituent elements, rather than in terms of the individuals (cf. spatial texture [15]). In particular, traffic patterns were classified directly in terms of measures of their dynamics aggregated over regions of image space–time, (x, y, t), rather than via the analysis of individual vehicles. Towards that end, an approach was developed that is based solely on observed dynamics (i.e., excluding purely spatial appearance cues). For such purposes, local spatiotemporal orientation is of fundamental descriptive power, as it captures the first-order correlation structure of the data, irrespective of its origin (i.e., irrespective of the underlying visual phenomena), allowing for the discrimination of pattern differences (e.g., levels of congestion). Correspondingly, each traffic scene is associated with a distribution (histogram) of measurements that indicates the relative presence of a particular set of 3D orientations in visual space–time, (x, y, t), as captured by a bank of spatiotemporal filters, and recognition was performed by matching such distributions. On the other hand, this approach has shown a limited ability to distinguish between completely stopped traffic and an empty roadway.

To our knowledge, none of the above methods have focused on the comparison between the microscopic and macroscopic features based approaches to estimate road traffic density. This study provides quantitative comparisons between these two approaches with the focus emphasis on determining the best feature combination that provides the highest classification accuracy of traffic videos. Firstly we estimate the traffic parameters with both approaches. Then road traffic classification is achieved using a set of parameter combinations, extracted from the video stream, which feeds a K-Nearest Neighbor (KNN), a Learning Vector Quantization (LVQ), or a Support Vector Machine (SVM) classifier.

Our goal is to compare the accuracy of the two methods, and also to propose an algorithm which provides the highest accuracy with a low number of traffic parameters. The proposed algorithm is compared against an existent method which used the same database in terms of classification accuracy and computational complexity.

Microscopic parameters are obtained by averaging the parameters of all the individual vehicles on the road. Usually, these parameters are estimated by detecting and tracking each vehicle on the road. We consider three kinds of microscopic parameters: traffic velocity, road occupancy rate and traffic flow.

The velocity is the most important parameter for describing the vehicle's behavior and the traffic conditions. The traffic velocity is obtained by averaging all the individual vehicles' speeds.

To measure a vehicle's speed we need first to extract it by a motion detection method; for this we use the median approximated detection method [16], for its simplicity and efficiency in this application.

A common approach to identifying the moving objects is background subtraction, where each video frame is compared against a reference or background model. Pixels in the current frame that deviate significantly from the background are considered to be moving objects. These “foreground” pixels are further processed for object localization and tracking.

Due to the success of the non-recursive median filtering, N. Mcfarlane and C. Schoeld proposed a simple recursive filter to estimate the median, called the approximated median filter [17]. This technique has also been used in background modeling for urban traffic monitoring [18]. It finds the difference of the current pixel intensity value and the median of some recent pixel intensity values. It uses a buffer of size n, where n is the number of the last frames whose pixel values are considered for calculating the median value for the background model. A pixel is considered as a foreground pixel if it satisfies the following inequality:
                              
                                 (1)
                                 
                                    
                                       
                                          
                                             I
                                             
                                                i
                                                j
                                             
                                             −
                                             Med
                                             
                                                i
                                                j
                                             
                                          
                                       
                                       >
                                       Th
                                       .
                                    
                                 
                              
                           
                        

The median value is updated for the last n recent pixel values. ‘I’ represents the current frame and ‘Med’ is the median of the last n frames, Th is a threshold defined by the Otsu method. For each pixel (i,j), the difference of its value with the pixel value of the median of the last n frames is used to decide whether it is foreground or background.

The vehicles are tracked from one frame to another by a kind of data association method that assigns the same label to the detections in consecutive frames, emanating from the same vehicle. In this paper we use a simple data association method. Considering that the trajectory of each vehicle is rectilinear, during some say n few consecutive frames, we use the correlation coefficient, defined in Eq. (2), to associate the detections of a given vehicle in these frames.


                           
                              
                                 (2)
                                 
                                    
                                       R
                                       =
                                       
                                          
                                             n
                                             
                                                
                                                   ∑
                                                
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                n
                                             
                                             
                                                x
                                                i
                                             
                                             
                                                y
                                                i
                                             
                                             −
                                             
                                                
                                                   ∑
                                                
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                n
                                             
                                             
                                                x
                                                i
                                             
                                             
                                                
                                                   ∑
                                                
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                n
                                             
                                             
                                                y
                                                i
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      n
                                                      
                                                         
                                                            ∑
                                                         
                                                         
                                                            i
                                                            =
                                                            1
                                                         
                                                         n
                                                      
                                                      
                                                         x
                                                         i
                                                         2
                                                      
                                                      −
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     ∑
                                                                  
                                                                  
                                                                     i
                                                                     =
                                                                     1
                                                                  
                                                                  n
                                                               
                                                               
                                                                  x
                                                                  i
                                                               
                                                            
                                                         
                                                         2
                                                      
                                                   
                                                
                                             
                                             
                                             
                                                
                                                   
                                                      n
                                                      
                                                         
                                                            ∑
                                                         
                                                         
                                                            i
                                                            =
                                                            1
                                                         
                                                         n
                                                      
                                                      
                                                         y
                                                         i
                                                         2
                                                      
                                                      −
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     ∑
                                                                  
                                                                  
                                                                     i
                                                                     =
                                                                     1
                                                                  
                                                                  n
                                                               
                                                               
                                                                  y
                                                                  i
                                                               
                                                            
                                                         
                                                         2
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where n represents the number of frames and (x
                           i, y
                           i) are the coordinates of the gravity center of the detected vehicle in frame i.

R is maximum and equals 1 when the points (x
                           i, y
                           i) lie on a line. If R exceeds a threshold equal to 0.95, we consider that the n detections are likely to belong to the same vehicle [10]. Furthermore to enhance the data association we restrict the speed of a vehicle to be lower than a given maximum speed.

The search for the best path, which corresponds to an object, is performed on three consecutive frames. This task must satisfy a constraint of the uniqueness association, which stems from the fact that a point of the image can represent only one physical point. This prohibits associating a detected object to several trajectories. Moreover, in some frames there may be no match between any detection and a confirmed trajectory, because some objects appear or disappear. So a point or a path may have either one association or none. Fig. 1
                            shows an example of vehicle tracking.

The distance traveled by a vehicle is measured in pixels and should be converted into meter. To avoid a prior calibration of the camera for this, we like in Ref. [10] evaluate a scale factor at each row of the image as follows: For each vehicle detected at a given row we calculate a scale factor as the ratio between the vehicle mean length and the length of this vehicle in pixels. An example of these calculated scale factors is shown in Fig. 2
                           .

Assuming that the scale factor is a linear function of the row number, we model it by the following equation:
                              
                                 (3)
                                 
                                    
                                       q
                                       
                                          v
                                       
                                       =
                                       mv
                                       +
                                       b
                                       ,
                                    
                                 
                              
                           where q represents the scale factor in meter/pixel, v the row number and m and b parameters that may be determined from the data set by a curve fitting, using the least squares criterion. This is illustrated in Fig. 2.

The distance d traveled between two positions (v1
                           , v2
                           ) is evaluated by:
                              
                                 (4)
                                 
                                    
                                       d
                                       =
                                       
                                          
                                             
                                                ∫
                                                
                                                   v
                                                   1
                                                
                                                
                                                   v
                                                   2
                                                
                                             
                                             
                                                q
                                                
                                                   v
                                                
                                                dv
                                             
                                          
                                       
                                       .
                                    
                                 
                              
                           
                        

Finally, having an estimate of the distance traveled, we use the inter-frame sample time (∆t) to estimate the vehicle speed:
                              
                                 (5)
                                 
                                    
                                       
                                          s
                                          ^
                                       
                                       =
                                       
                                          d
                                          Δt
                                       
                                       .
                                    
                                 
                              
                           
                        

In this work we estimate the road occupancy rate as the ratio between the total surface of the road and the surface of all the vehicles present on the road. To calculate the road surface we need first to detect the road region. Many techniques for road detection and segmentation have been proposed with more or less success. They are usually based on the detection of markers, using the Hough transform for example. This also can be achieved by using a segmentation method based on extracting the regional maxima of the image, if there is no road marking as in our case.

To extract the regional maxima of a numerical function f, which represents in our case the image, we apply a thresholding to the difference between f and its geodesic reconstruction, which is accomplished by a geodesic dilation of (f
                           −
                           1) under f 
                           [19], as illustrated in Fig. 3
                           .


                           
                              
                                 (6)
                                 
                                    
                                       Max
                                       
                                          f
                                       
                                       =
                                       
                                          T
                                          1
                                       
                                       
                                          
                                             f
                                             −
                                             
                                                δ
                                                ∞
                                             
                                             
                                                
                                                   f
                                                   ,
                                                   f
                                                   −
                                                   1
                                                
                                             
                                          
                                       
                                       ,
                                    
                                 
                              
                           where δ∞
                           (f,f
                           −
                           1) presents the infinite size geodesic dilation operation and Th (f) is the thresholding function of f.
                              
                                 (7)
                                 
                                    
                                       
                                          T
                                          h
                                       
                                       
                                          f
                                       
                                       =
                                       
                                          
                                             x
                                             \
                                             f
                                             
                                                x
                                             
                                             ≥
                                             h
                                          
                                       
                                       .
                                    
                                 
                              
                           
                        


                           Fig. 4
                            shows a road region segmented with this method.

Regional maxima are flat zones that are connected to pixels of lower value while regional minima are flat zones that are connected to pixels of higher value. In the case of a dark road texture we have to extract the regional minima.

The extraction of the regional minima of (f) uses the same process, but applied to (−
                           f). We can also perform a geodesic reconstruction by erosion of (f
                           +
                           1) over f 
                           [19].
                              
                                 (8)
                                 
                                    
                                       Min
                                       
                                          f
                                       
                                       =
                                       T
                                       1
                                       
                                          
                                             
                                                ϵ
                                                ∞
                                             
                                             
                                                
                                                   f
                                                   ,
                                                   f
                                                   +
                                                   1
                                                
                                             
                                             −
                                             f
                                          
                                       
                                       .
                                    
                                 
                              
                           
                        


                           Fig. 5
                            shows a dark road texture segmented with the minima regional.

Since the vehicle occupies an area on the road during a period of time, we may consider that the area occupied by vehicles in the case of congested road traffic is larger than the area occupied in the case of light traffic. Therefore, the ratio between the total objects (vehicles) area and the road area defined in Eq. 9 may be used to infer the traffic density.
                              
                                 (9)
                                 
                                    
                                       Occupancy
                                       
                                       rate
                                       =
                                       
                                          
                                             
                                                
                                                   ∑
                                                   
                                                      objects
                                                      
                                                      area
                                                   
                                                
                                             
                                             /
                                             road
                                             
                                             area
                                          
                                       
                                       ∗
                                       100
                                       .
                                    
                                 
                              
                           
                        

The road area is estimated by calculating the number of white pixels which represent the road region. To estimate the area occupied by the vehicles we use the motion detection method, described previously, followed by some morphological operations to enhance the moving objects extraction, mainly the holes filling operation.

Road traffic flow is estimated by calculating the difference between the inflow and outflow, which are respectively, the average number of vehicles entering in the visual field of the camera, during a period of time, and the number of vehicles which leave it during the same period.

We have estimated the number of vehicles passing by in a given region by using the same idea as the one proposed in Ref. [20]. Firstly a road region, delimited by two parallel lines located at the beginning of the road, is selected, secondly the approximate median filter motion detection method is applied and finally the following algorithm is used to detect any vehicle that enters the region of interest.

To identify a vehicle in the selected region the following condition must be satisfied:
                              
                                 (10)
                                 
                                    
                                       
                                          
                                             d
                                             
                                                
                                                   j
                                                   −
                                                   1
                                                
                                             
                                             >
                                             S
                                             &
                                             &
                                             d
                                             
                                                j
                                             
                                             <
                                             =
                                             S
                                          
                                       
                                       ,
                                       j
                                       =
                                       2
                                       ,
                                       …
                                       ,
                                       NC
                                       −
                                       10
                                    
                                 
                              
                           where S is a threshold that depends on the minimum size of the vehicles (in this paper we used S=60), j is the column index, d(j) is the area of a region from column j to j+10, and NC is the columns number.

The area of a region is measured by the number of white pixels that it contains (Fig. 6
                           ).

If a region satisfies the above condition, we can assume that it is a vehicle's template in the current frame, as shown in Fig. 7
                           .

The number of vehicles is counted as follows:
                              
                                 1)
                                 If the vehicle template identified in the current image is matched four times or more continuously in the next few frames, this indicates that a car is passing by, and the total number of vehicles is incremented by one.

If the vehicle template identified in the current image is not matched four times continuously in the next few frames, this indicates that a car has passed by, and the vehicle template is deleted.

If the vehicle template identified in the current image is not matched in the previous frames, this indicates that a new car is appearing, so this new car is taken as the vehicle template.

A road flow can be likened to a liquid flowing in a channel. Macroscopic representation of traffic borrows the notions of density and velocity specific to fluid mechanics. These two variables can be estimated by using a method based on estimating the optical flow field by a block matching algorithm.

Block matching technique is commonly used for estimating the block motion vector [21], due to its simplicity. The matching error between the block at position (x, y) in the current image, I
                           t
                        , and the candidate block at position (x
                        +
                        u, y
                        +
                        v) in the reference image, It−1, is often defined as the Sum of Absolute Difference (SAD):
                           
                              (11)
                              
                                 
                                    
                                       SAD
                                       
                                          x
                                          y
                                       
                                    
                                    
                                       u
                                       v
                                    
                                    =
                                    
                                       
                                          ∑
                                          
                                             i
                                             =
                                             0
                                          
                                          
                                             B
                                             −
                                             1
                                          
                                       
                                       
                                          
                                             
                                                ∑
                                                
                                                   j
                                                   =
                                                   0
                                                
                                                
                                                   B
                                                   −
                                                   1
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         I
                                                         t
                                                      
                                                      
                                                         
                                                            x
                                                            +
                                                            i
                                                            ,
                                                            y
                                                            +
                                                            j
                                                         
                                                      
                                                      −
                                                      
                                                         I
                                                         
                                                            t
                                                            −
                                                            1
                                                         
                                                      
                                                      
                                                         
                                                            x
                                                            +
                                                            u
                                                            +
                                                            i
                                                            ,
                                                            y
                                                            +
                                                            v
                                                            +
                                                            j
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    ,
                                 
                              
                           
                        where B is the block size.

The best estimate of the block motion vector 
                           
                              
                                 
                                    u
                                    ^
                                 
                                 
                                    v
                                    ^
                                 
                              
                              
                           
                         is the one having the minimum matching error:
                           
                              (12)
                              
                                 
                                    
                                       
                                          u
                                          ^
                                       
                                       
                                          v
                                          ^
                                       
                                    
                                    =
                                    arg
                                    
                                    mi
                                    
                                       n
                                       
                                          u
                                          v
                                       
                                    
                                    
                                       SAD
                                       
                                          x
                                          y
                                       
                                    
                                    
                                       u
                                       v
                                    
                                 
                              
                           
                        
                     

We used a search window of size (2R×2R) for blocks of size (R×R). The value R=16 pixels was found to achieve a good compromise between accuracy and complexity.

The length of a motion vector represents the moving distance of the corresponding block. An example of motion vectors estimated by block matching is given in Fig. 8
                        , for two different frames.

Road traffic density and velocity are estimated by the density and the mean velocity of the non-zero motion vectors.

There is no unified standard for the definition of traffic congestion, and each nation has a different definition. For example, the ministry of public security in China [22] defines congestion at crossings and sections as follows: for crossings, a distinction is made between a crossing with the control of a signaling beacon and a crossing without. The former is considered as congested if the vehicles cannot pass it during 3 times of green light show, while the latter is considered as congested if it is difficult for the vehicles to cross it and if the length of vehicles line is over 250m. A section is defined as congested when the vehicles in the lanes are blocked up and the vehicles line is longer than 1km.

Using a combination of the traffic parameters extracted from a video clip the traffic congestion is classified as light, medium, or heavy, using one of the three classifiers: KNN, LVQ, SVM, which are well documented in the specialized literature [22–25].

@&#EXPERIMENTAL RESULTS@&#

The performances of the proposed algorithm were assessed and compared to that of the method proposed in Ref. [12], using the UCSD traffic video data set [26], which consists of video sequences of daytime highway traffic in Seattle, Washington, totaling 20min of video footage, under different conditions (e.g., raining, overcast and sunny). Each video has a resolution of 320×240 pixels with 42 to 52 frames captured at 10 frames per second. Hand-labeled ground truth is provided that describes the amount of traffic congestion in each sequence. In total there are 254 videos sequences, grouped into three classes of traffic congestion: 44 instances of heavy traffic (slow or stop and go speeds), 45 videos of medium traffic (reduced speed), and 165 of light traffic (normal speed). Different weather conditions are also considered: 75 instances of rainy weather (with drops on the lens), 162 instances of overcast weather, and 17 videos of sunny weather. Fig. 9
                      shows a representative set of clips from this database.

The SVM parameters were selected using 3-fold cross-validation over the training set. The LIBSVM software package [27] was used for training and testing.

For the LVQ classifier, the number of neurons used in the input layer corresponds to the number of traffic parameters used for the classification; the number of neurons in the competition layer is twice as the number of neurons in the input layer plus one, and the output layer has three neurons. We used a maximum of 700 iterations and a learning rate equal to 0.1.

Two scenarios were considered. In the first scenario, the three classifiers were trained using 61 sequences, spanning 4h from the first day, and tested on 193 sequences, spanning 15h from the following day.


                     Table 1
                      shows the classification results for many combinations of the traffic parameters. The SVM classifier was tested with both one versus one and one versus all schemes, to handle the multi-class problem, and both a linear kernel and an RBF kernel were considered.

S: speed by detection and tracking, SBM: speed with block matching, O: occupancy, F: traffic flow, D: density.

We can observe from this table that traffic parameters classification by SVM is globally more accurate than the KNN or the LVQ. It can also be observed that it is better to use the linear kernel with microscopic parameters and the RBF kernel with macroscopic parameters, which means that the microscopic parameters are linearly separable and the macroscopic ones are not.

The microscopic parameters were able to achieve a correct classification rate equal to 86%.

The best classification rate (96.37%) was obtained by the SVM with the RBF kernel and one versus one scheme, using the macroscopic parameters SBM+D parameters. We present in Fig. 10
                      the evolution of traffic congestion estimated by this system.

However a good rate (95.85%) has been obtained with the same parameters, using the simpler NN classifier. This means that when the used parameters are discriminant enough a simple classifier may suffice.

The microscopic parameters have produced a lower accuracy compared to macroscopic parameters. This is because the vehicle tracking framework has the disadvantage that its accuracy is dependent on the quality of the segmentation (motion detection step). The segmentation task becomes more difficult with the presence of adverse environmental conditions, such as lighting (e.g. overcast, glare, and shadows). This was approved by some results of the motion-detection stage in different weather and traffic density conditions. In the case of a clear weather and light traffic, the precision of the segmentation obtained is 80%, and in the case of bad conditions like an overcast weather and a medium traffic density the precision was 76%, this rate has been highly decreased to 46% in the case of heavy traffic and a clear weather, and to 30% in the case of heavy traffic with a rainy weather and drops on lens. Tracking algorithms also have problems when there are many objects in the scene.

The proposed system (SMB+D) which achieves the best classification rate has produced 7 misclassified video scenes including 4 scenes captured in rainy weather (57.1%), 2 videos in an overcast weather (28.6%), and 1 scene captured in a clear weather (14.3%).

In the other hand, the microscopic system (S+O+F) which produced a good accuracy (86%) generates 27 misclassified videos, including 14 videos on rainy weather (51.8%), 11 videos captured in an overcast weather (40.7%) and 2 scenes of clear weather(7.4%).

We note that the same videos misclassified by the macroscopic system (SMB+D) were also misclassified by the microscopic system (S+O+F).

For a thorough evaluation of the SBM+D and SVM system a second scenario was considered, where all experimental results were averaged over four trials; in each trial the dataset was split differently with 75% used for training and cross-validation, and 25% reserved for testing.

We have obtained an overall rate of correct classification of 95.28%. Table 2
                      shows the corresponding confusion matrix.

As shown in this table the misclassifications occur only between neighboring classes (light vs. medium and medium vs. heavy).


                     Table 3
                      presents a comparison between our method and the dynamic texture method proposed in Ref. [12] for the two scenarios. We also give in this table a complexity comparison between the two methods, measured in terms of the mean computational time required for the classification of one video, using MATLAB version 7.7 running on laptop with a 2.16GHz Intel® dual core and 1GB of RAM.

This result shows that the proposed method outperforms the dynamic texture method in terms of both accuracy and complexity.

The accuracy of the proposed method is the same as that of the method presented in Ref. [14] for the second scenario. However no results are presented for this method in the case of the first scenario. Also according to the confusion matrix presented in Ref. [14] and unlike in our method, the misclassification may occur between the non-neighboring classes (light vs heavy and heavy vs light).

To assess the accuracy of our algorithm in the case of illumination changes we have tested it using some videos which are captured at night. In total there are 19 videos: 12 videos of light traffic, 5 videos of medium traffic and 2 instances of heavy traffic. We present in Fig. 11
                      some instances of this nighttime database. The classification results obtained by our proposed method are given in Table 4
                     .

Only 2 videos from the 19 videos were not properly classified, which represents an accuracy rate of 89.47%. The misclassification occurs also only between neighboring classes.

@&#CONCLUSION@&#

The work presented in this paper concerns the problem of automatic road traffic classification from a digital video sequence. The aim of the automation of this task is to provide, in real time, drivers with information about road traffic, that may help them to avoid difficult traffic situations. The classification is based on traffic parameters extracted rom the video sequence. We have compared, in terms of accuracy, between both microscopic parameters, whose extraction requires detecting and tracking vehicles in the scene individually, and macroscopic parameters, which are extracted by analyzing the video scene as a whole. We have also tested several classification methods, namely the KNN, the LVQ and the SVM.

The performances of the investigated methods were assessed and compared by using a publicly available data set assembled from real world data. The best performance was obtained by using SVM with the vehicles mean speed and the density macroscopic parameters, extracted by the block matching technique. This proposed method has several advantages:
                        
                           It achieves state-of-the-art performance, with a correct classification rate around 96%.

It is computationally inexpensive.

It does not require segmentation or tracking of vehicles.

It is robust towards illumination changes.

@&#REFERENCES@&#

