@&#MAIN-TITLE@&#A framework for a real time intelligent and interactive Brain Computer Interface

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A framework for a real time implementation of a Brain Computer Interface.


                        
                        
                           
                           Implementation & comparison of different feature extraction methods and classifiers.


                        
                        
                           
                           Accuracy & processing time comparison for detection of event related potentials-ERP.


                        
                        
                           
                           An implementation of a prototype system using the proposed BCI framework.


                        
                        
                           
                           Real time EEG data collection and classification of ERPs using Hex-O-Speller.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Electroencephalography (EEG)

Data collection

Brain–computer interface (BCI)

Event-related potentials (ERP)

Classification

@&#ABSTRACT@&#


               
               
                  This research proposes a framework for a real time implementation of a Brain Computer Interface (BCI). This interface is designed with a future objective of providing a testing platform as an interactive and intelligent Image Search and Retrieval tool that allows users, disabled or otherwise, to browse and search for images using non-invasive electroencephalography (EEG) signals. As a proof of concept, a prototype system was designed and implemented to test real time data collection and navigation through the interface by detection and classification of event-related potentials (ERPs). A comparison of different feature extraction methods and classifiers for the detection of ERPs is performed on a standard data set to determine the best fit for the real time implementation of the BCI. The preliminary results of the real time implementation of the prototype BCI system are quantified by the success rate of the user/subject in navigating through the interface and spelling a search keyword using the mental-typewriter Hex-O-Speller.
               
            

@&#INTRODUCTION@&#

Assistive devices and technologies are essential in enhancing the quality of life for individuals. Significant research in developing assistive smart devices and technologies for disabled individuals to improve upon motor movements, speech, touch and bio-signals has been done. Most of these systems depend on some residual motor movement or speech. The Brain Computer Interface (BCI) systems completely bypass any motor-output by decoding the brain state of an individual, which can be an emotion, attention, an event related potential (ERP) or an imagined movement. These BCIs can help disabled individuals that have minimal to no voluntary muscle/movement control, thereby attempting to give some autonomy to individuals by providing the brain with alternate ways of communication.

In [1], the authors comprehensively describe the existing Brain Computer Interface trends and applications and its use as an augmenting and alternative communication system. It categorizes the BCI systems based on the inputs i.e. the stimulus presentation paradigms and the elicited EEG responses. It details further in the effective and commonly used preprocessing and classification techniques. Matrix Speller, Hex-O-Speller and Rapid Serial Visual Presentation are the existing visuospatial presentation techniques (described in Section 2.1) for eliciting event related potentials (ERPs). It also expands upon motor imagery-movements induced responses as a control paradigm for communication in BCI.

This framework for the BCI is an attempt to use and augment the existing EEG based research and understand how it can be applied to make devices such as smart-phones, tablets or computers, and technologies smarter and more interactive. Today the dependencies and reliance on smart devices, computers and applications; especially the ones that use the information available on internet, are indisputable. Modern users expect the applications and technologies to be smart and learn from the usage and the choices made by the user. The proposed study will further reinforce this notion, by taking into consideration and understanding their thoughts/actions by decoding their brain state, thus making them more user friendly and intelligent. Multiple studies in the psychophysiology and neuroscience fields have been done to understand the relation of emotions [2], attention [3], interest [4] and motor movements [5] to the brain activity and EEG responses [6]. However, there are relatively fewer real world applications that use these methods and findings. The research presented in [7] compares the performance of Brain Computer Interface (BCI) to Eye Tracking Interface (ETI) in terms of how fast a decision/classification could be made for a visual stimulus targets on a smaller screen. It observed that BCI performed better for small screen targets compared to ETI, and recommended the use of BCI over ETI in small screen applications like smart phones.

Research in using the EEG signals to enhance and augment the retrieval of meaningful information from images and videos has been done. Similar studies to the one proposed use EEG signals and image processing to find similar images or specific objects in images [8]. These reinforce an image search by leveraging the robust and invariant object recognition capabilities of the human brain [9]. In [10,11], rapid visual presentation has been used to categorize objects in images and do image search respectively. The study in [12] describes the discriminatory EEG signals in response to events of interest in a surveillance video. The above referenced BCI research uses a single type of visual stimulus i.e. RSVP. The proposed framework for the BCI is an attempt to use different stimuli such as the Hex-O-Speller along with RSVP to give more flexibility and autonomy while doing image searches. This will result in a more enhanced image browsing, search and retrieval process. The framework for the BCI focuses only on the visual stimulus, but it can be further extended to auditory stimuli like music or sounds. The proposed framework because of its modular setup can be used to create experiments, collect data with different type of stimulus, train and classify in real time.

The principal challenge with this research is that there is no defined model or relation that relates a user’s interest to the contents of an image. In order to formulate a relation, experiments as proposed in Section 6 
                     [13] can be designed and performed using the proposed BCI. Another challenge is that a large amount of training data is needed to be collected, thereby a longer training time, for getting higher accuracy for a single-trial EEG signal classification [14,15]. Moreover, in a real time implementation of a BCI, timing synchronization between the generation of visual stimuli and data collection is challenging. Also the EEG recording devices are a bit uncomfortable and less fashionable. However, there have been significant advances in wearable technologies and new devices like the Emotive research headset [16] and EEG head bands that collect wireless EEG data and are relatively comfortable and fashionable. The proposed modular framework for the BCI can be used to address and research the aforementioned challenges individually. Nonetheless, this study is a step towards better understanding the use of brain signals as a feedback to devices and applications to understand the user better.

The remainder of this document is organized as follows: Section 2 describes the background information for this work. Section 3 describes the proposed approach for the BCI framework. It elaborates various aspects of the BCI system, explains different feature extraction methods and classifiers used for detecting event related potentials on the standard dataset. Preliminary results of the BCI system and comparison of different methods for classifying event related potentials are presented and discussed in Section 4. Conclusions are drawn in Section 5, and the future work is discussed in Section 6.

@&#BACKGROUND@&#

Understanding the brain function is a challenge that has inspired a lot of research from researchers and scientists from multiple disciplines. This resulted in the emergence of computational neuroscience, the principle theoretical approach in understanding the workings of the nervous system [14]. Part of this research involves decoding the brain-state/human intentions by using the electroencephalogram (EEG) signals. This branch of the field is greatly influenced by the development of effective communication interfaces connecting the human-brain and the computer [14]. This section describes the background information used as a reference and guide for this work.

In general every Brain Computer Interface (BCI) requires a medium of communication represented by an event that evokes an EEG response. This event can be a visual, muscle movement, audio or touch. In order to get a valid EEG signal representing an event of a specific human brain state, the EEG response is evoked by recreating a similar setup which normally would evoke such a response. This is called a ‘Stimulus’.

In this research, a visual stimulus is used for eliciting event related potentials i.e. ERPs that are EEG responses that occur when the subject focuses their attention on symbols or images shown and tries to distinguish between what he/she wants to select. This notion is called the ‘oddball paradigm’ [15,14] in which the subject focuses on the target images/symbols and ignores other non-target images.

The most commonly researched example of this stimulus is the attention based mental typewriter. This mental typewriter is used to spell words using the EEG signals. The following subsection explains some of the visual stimuli.

The Matrix Speller consists of a 6×6 character matrix, as shown in Fig. 1
                           , where characters are arranged within rows and columns [17]. Throughout the stimulus generation process, the rows and columns are intensified (‘flashed’) in a random order one after the other. The subject is asked to focus on the target character for the span of stimulus and EEG response corresponding to each intensification is recorded. As the EEG response can be modulated by attention, the response for the target character is different from the non-target characters.

The Hex-O-Speller is a variant of the Matrix speller [14]. It is represented with six circles placed at the corners of an invisible hexagon as shown in Fig. 2
                           . Each circle is intensified (‘flashed’) in a random order multiple times. Intensification is realized by up-sizing the circles by 40% for 100ms. In this speller, the choice is made in two recurrent steps. At the first step, the group containing the target symbol/character is selected, followed by the selection of the target symbol itself at the second step.

Rapid Serial Visual Presentation (RSVP) is an experimental setup used to understand temporal characteristics of attention. In this setup images are flashed or shown for a fixed duration at specific intervals as show in Fig. 3
                           . This form of stimulus is used for representing visual information such as pictures to evoke a visual event.

The Psychtoolbox 3.0 tool can be used to generate the Stimulus [18,19]. It allows control over the graphics for the stimulus. The Stimulus for Hex-O-Speller and RSVP are generated using the Psychtoolbox. It allows control and flexibility to record the time for events which helps generate decently accurate event markers necessary for data segmentation(epoching or shelving). Every display screen has a display buffer and a ‘Vertical Blank Interval’ time [20] also know as VBlank, that it takes to completely display a new image. This toolbox gives a two stage flexibility to change a display on a monitor. The first step is filing the display buffer with the desired image and second is ‘Flipping’ the screen i.e. clearing the previous display and showing the new display. The refresh rate of a monitor depends on the type of monitor, operating system and the graphics card on the computer. This rate remains the same for the stimulus running on a specific system. This refresh rate say 
                              
                                 
                                    
                                       T
                                    
                                    
                                       refresh
                                    
                                 
                              
                           , becomes the unit time for change of display on a monitor.

The VBlank interval for each image varies based upon the size and resolution of the image. The VBlank interval is represented as 
                              
                                 VBlank
                                 =
                                 n
                                 ∗
                                 
                                    
                                       T
                                    
                                    
                                       refresh
                                    
                                 
                              
                           , where n being the time taken to show the image in the image buffer onto the screen in units 
                              
                                 
                                    
                                       T
                                    
                                    
                                       refresh
                                    
                                 
                              
                           . For all images to be displayed this VBlank time can be fixed to the maximum possible time taken for the biggest image to be displayed during the stimulus. This results in a uniform screen change rate, thus making the synchronization of stimulus generation and data collection better.

Data collection is an integral part of any Brain Computer Interface. Data collection requires proper setup of electrodes and bio-signal collection devices. These devices generally contain buffers and amplifiers to amplify the bio-signal data and transmit it to the computer. It is extremely important for proper synchronization between stimulus generation for the event that evokes the EEG response and its collection, to correctly segregate (also known as epoching or shelving) the EEG response corresponding to the event. The non-invasive EEG signals data collection is done by placing electrodes on the scalp. For that various EEG caps, headsets and head bands are available. The EEG caps generally use the standard 10–20 placement system as shown in Fig. 4
                        . These caps can have 14, 20, 32, 64 or 128 channels depending on the EEG cap. Some headsets like the Emotive have 14 channels, and some headbands have only four. There are various data-collection hardware available such as Bio-Radio 150 [21], Bio-Semi [22], B-Alert [23], Emotive head set [16] that collect the data. According to the Bio-Radio 150 manual [21], the data is collected and wirelessly transmitted via blue-tooth in serialized data packets. The Bio-Radio 150 provides libraries so that its functions can be called and the data collected parsed and put into a proper matrix, say 
                           
                              
                                 
                                    X
                                 
                                 
                                    bioradio
                                 
                              
                           
                        . 
                           
                              •
                              
                                 
                                    
                                       
                                          
                                             X
                                          
                                          
                                             bioradio
                                          
                                       
                                    
                                  of dimensions (c
                                 ×
                                 p) where c is the number Of channels/electrodes connected and p number of data points in the time series of the signal received at each channel/electrode.

It also allows the system to configure the data sampling rate and resolution of the data collected. A maximum of eight channels can be connected to a single Bio-radio 150. As more than 8 channels are required for getting sufficient information to represent an Event Related Potential (ERP), programs for recording EEG signals from multiple Bio-Radio 150 modules simultaneously are implemented.

The EEG signals recorded using a non-invasive procedure are noisy. Before generation of features that would represent the characteristics of an EEG response, the raw data signal is to be cleaned, corrected and segmented (epoched/shelved). The types of noise present in an EEG response are skin impedance noise, data collection instrument noise, eye movement and blink artifacts i.e. Electrooculogram (EOG) (if they are not considered as useful information for the objective of the Brain Computer Interface classification), Electrocardiogram (ECG) noise, and other muscle movement noise (voluntary or involuntary).

The skin impedance noise can generally be removed by ensuring proper conductivity between the scalp and the electrode. As mentioned in Section 2.2, the 20 channel EEG cap used has passive electrode i.e. it requires conductive gel to be inserted as cushion between the scalp and the electrodes in the EEG cap. For that the Bio-Capture Software [21] is used along with a testing program implemented in this system to ensure that minimum skin impedance (also known as ‘contact noise’) is present. Generally this noise affects the DC offset i.e. the zero Hz frequency of the raw EEG signal.

Instrument noise present in the raw signal can be either because of the 60Hz power noise, noise over the wireless communication or if the electrode cable is not connected properly to the Bio-radio 150. The main frequency bands of the EEG signal approximately range from 0.5Hz to 30Hz [24]. In order to remove the DC noise and the 60Hz noise, the raw signal can be filtered using a band-pass filter. A sixth order Butterworth band pass filter with cut-off frequencies as 0.1Hz and 30Hz can be used [15].

The raw EEG data is corrupted by other bio-signals that are generated involuntarily by the subject during data collection and are undesirable. These signals are called Artifacts. The biggest contributor of these artifacts are eye movements and blinks. For removal of these artifacts the most commonly used method is Independent Component Analysis as explained in [25,26]. The Winsorizing method (also known as Clipping) is also used to reduce the effects of large amplitude outliers which are representative of these artifacts. In Perseh and Sharafat (2012) this method is used to dampen the effects of these artifacts [15]. Both methods are described briefly below:

ICA based artifact correction can separate and remove wide variety of artifacts from EEG data by linear decomposition. However, there are certain assumptions that are made [25,26]. They are: (1) Spatially stable mixtures of activities of temporally independent brain and artifact sources, (2) Summation of potentials from different channels/sensors is linear at the electrodes, (3) Propagation delays from the source of the electrodes is negligible.

For analyzing the EEG signals, let X be the input response matrix where,
                              
                                 •
                                 
                                    X of dimensions (c
                                    ×
                                    p), where c is the number of channels and p is the number of data points.

The ICA finds an unmixing matrix W which linearly decomposes the multi-channel EEG data into sum of temporally independent and spatially fixed components.
                              
                                 (1)
                                 
                                    U
                                    =
                                    W
                                    ·
                                    X
                                 
                              
                           where the rows of U are the time courses of activation of ICA components. The columns of the inverse matrix, 
                              
                                 
                                    
                                       W
                                    
                                    
                                       -
                                       1
                                    
                                 
                              
                            gives the relative projection strengths of the respective components at each electrode. Columns of 
                              
                                 
                                    
                                       W
                                    
                                    
                                       -
                                       1
                                    
                                 
                              
                            are used to determine which components are to be selected. Say a set of a components are selected, then clean data can be represented as
                              
                                 (2)
                                 
                                    CleanData
                                    =
                                    
                                       
                                          W
                                       
                                       
                                          -
                                          1
                                       
                                    
                                    (
                                    :
                                    ,
                                    a
                                    )
                                    ∗
                                    U
                                    (
                                    a
                                    ,
                                    :
                                    )
                                 
                              
                           where the dimensions of each matrices are
                              
                                 •
                                 
                                    
                                       
                                          CleanData
                                       
                                    : (c
                                    ×
                                    p),


                                    
                                       
                                          
                                             
                                                W
                                             
                                             
                                                -
                                                1
                                             
                                          
                                          (
                                          :
                                          ,
                                          a
                                          )
                                       
                                    : (c
                                    ×length of a),


                                    
                                       
                                          U
                                          (
                                          a
                                          ,
                                          :
                                          )
                                       
                                    : (length of a
                                    ×
                                    p).

Winsorizing is a method used for attenuating and reducing the affect of outliers. A percentile of data say 
                              
                                 
                                    
                                       p
                                    
                                    
                                       th
                                    
                                 
                              
                            percentile is to be attenuated. Then for a data vector say of size N, the first and last 
                              
                                 
                                    
                                       (
                                       0.5
                                       ∗
                                       p
                                       )
                                    
                                    
                                       th
                                    
                                 
                              
                            percentile of the sorted values of the data vector are replaced by the value nearest to the side of the sorting order. For a sorted data vector say X,
                              
                                 •
                                 
                                    
                                       
                                          X
                                          =
                                          
                                             
                                                X
                                             
                                             
                                                1
                                             
                                          
                                          ,
                                          
                                             
                                                X
                                             
                                             
                                                2
                                             
                                          
                                          ,
                                          
                                             
                                                X
                                             
                                             
                                                3
                                             
                                          
                                          ,
                                          
                                             
                                                X
                                             
                                             
                                                4
                                             
                                          
                                          ,
                                          
                                             
                                                X
                                             
                                             
                                                5
                                             
                                          
                                          ,
                                          …
                                          ,
                                          
                                             
                                                X
                                             
                                             
                                                99
                                             
                                          
                                          ,
                                          
                                             
                                                X
                                             
                                             
                                                100
                                             
                                          
                                       
                                    , 
                                       
                                          10
                                          th
                                       
                                     percentile winsorizing will result in,


                                    
                                       
                                          
                                             
                                                X
                                             
                                             
                                                clipped
                                             
                                          
                                          =
                                          
                                             
                                                X
                                             
                                             
                                                6
                                             
                                          
                                          ,
                                          
                                             
                                                X
                                             
                                             
                                                6
                                             
                                          
                                          ,
                                          
                                             
                                                X
                                             
                                             
                                                6
                                             
                                          
                                          ,
                                          
                                             
                                                X
                                             
                                             
                                                6
                                             
                                          
                                          ,
                                          
                                             
                                                X
                                             
                                             
                                                6
                                             
                                          
                                          ,
                                          
                                             
                                                X
                                             
                                             
                                                6
                                             
                                          
                                          ,
                                          
                                             
                                                X
                                             
                                             
                                                7
                                             
                                          
                                          ,
                                          …
                                          ,
                                          
                                             
                                                X
                                             
                                             
                                                93
                                             
                                          
                                          ,
                                          
                                             
                                                X
                                             
                                             
                                                94
                                             
                                          
                                          ,
                                          
                                             
                                                X
                                             
                                             
                                                94
                                             
                                          
                                          ,
                                          
                                             
                                                X
                                             
                                             
                                                94
                                             
                                          
                                          ,
                                          
                                             
                                                X
                                             
                                             
                                                94
                                             
                                          
                                          ,
                                          
                                             
                                                X
                                             
                                             
                                                94
                                             
                                          
                                          ,
                                          
                                             
                                                X
                                             
                                             
                                                94
                                             
                                          
                                       
                                    , where X and 
                                       
                                          
                                             
                                                X
                                             
                                             
                                                clipped
                                             
                                          
                                       
                                     have the same length.

The corrected data is segmented (epoched/shelved) corresponding to the response of each stimulus. In order to keep the ratio of meaningful signal to that of noise high i.e. a high SNR, multiple trials of the same stimulus are performed and data collected. These trials are averaged in order to attenuate the noise. Each data sample has data points corresponding to time interval prior to the display of the stimulus and data points after the display of the stimulus. This signal collected for the time interval is called Stimulus Onset Asynchrony (SOA). Lets say the time interval for SOA is 
                              
                                 
                                    
                                       T
                                    
                                    
                                       SOA
                                    
                                 
                              
                           . The mean of the data points corresponding to the time course of 
                              
                                 
                                    
                                       T
                                    
                                    
                                       SOA
                                    
                                 
                              
                            is subtracted from the rest of the data sample. This is called ‘Baseline Correction’. The data collection for a stimulus is continuous during a trial. Baseline correction is done to remove any change of offset in a response due to the response to the previous stimulus in a trial.

Following the guidelines of recording EEG signals and ERP data as given in Picton et al. (2000), the EEG signal data can be re-referenced and baseline corrected [27].

Feature extraction is performed after the raw EEG data is corrected, segmented/epoched, re-referenced, averaged and baseline corrected. Event-related potentials (ERPs) in the collected EEG signals are to be identified and classified. The property of an ERP lies in both spatial and temporal domain. Spatial domain is the representation of the signal generated at different channels(electrodes/sensors) and the contribution of each channel to successfully characterize an ERP. Temporal domain represents the response times and intervals of generation of ERP after the stimulus is shown. This response time varies from subject to subject. Some subjects might respond faster or slower than other subjects. However, it has been noted that for an average person the ERP response, specifically the P-300 response happens 300ms after the stimulus event occurs [17,15]. Hence the nomenclature of P-300 Matrix Speller, as it tries to identify this specific data point response in an EEG response to a stimulus.

Not all channels contribute equally to the distinguishable characteristics of an ERP response to target and non-target stimuli. In Perseh and Sharafat (2012) the channel selection is done by sorting the channels using Bhattacharyya Distance as defined in [15]. The efficiency of each channel is measured by the ability to discriminate target and non-target patterns in the training dataset of EEG responses. At each data point in the time series of the response a statistical measure i.e. the Bhattacharyya Distance (BD) is used between the two respective target and non target patterns.

Similar to the spatial selection certain time intervals in the EEG response from each selected channels contain more discriminative information for target patterns and non target patterns. This approach is used to select clusters of timing data points for generation of features [14]. The statistical measure is a biserial correlation coefficient ‘
                              
                                 r
                              
                           ’ as defined in [14].

Following the identification of discriminative spatial channels and temporal data points, features representing them can be generated. The temporal characteristics of the EEG signal can be identified and represented, both in time and frequency domain.

Band powers are sum of Fast Fourier Transform (FFT) coefficients for specific EEG bands. This feature extraction method explores the frequency domain features of the signal. For each channel of an EEG response the signal is passed though various band-pass filters with cut-off frequencies representing different bands of the EEG signal [24] and the corresponding FFT is taken and the absolute values of the FFT coefficients are used to represent each channel. These coefficients are summed at fixed equally spaced intervals and vectorized to represent a feature vector for all channels.

This feature extraction method explores the time domain features of the signal. For each channel of an EEG response the discriminating timing intervals are identified, and summed for each channel. For 
                              
                                 
                                    
                                       n
                                    
                                    
                                       tau
                                    
                                 
                              
                            number of timing intervals identified for c number of channels, a feature vector of size (
                              
                                 
                                    
                                       n
                                    
                                    
                                       tau
                                    
                                 
                              
                           
                           ×
                           c) is generated for each data sample. Let a data sample be:
                              
                                 •
                                 
                                    
                                       
                                          
                                             
                                                x
                                             
                                             
                                                i
                                             
                                          
                                       
                                     of dimensions (c
                                    ×
                                    p) for 
                                       
                                          i
                                          ∈
                                          [
                                          1
                                          ,
                                          
                                             
                                                N
                                             
                                             
                                                1
                                             
                                          
                                          +
                                          
                                             
                                                N
                                             
                                             
                                                2
                                             
                                          
                                          ]
                                       
                                    , where c is the number of selected channels and p is the number of data points in the signal and 
                                       
                                          
                                             
                                                N
                                             
                                             
                                                1
                                             
                                          
                                       
                                     and 
                                       
                                          
                                             
                                                N
                                             
                                             
                                                2
                                             
                                          
                                       
                                     are the number of samples in the target and non-target pattern signals respectively.

Let 
                                       
                                          
                                             
                                                τ
                                             
                                             
                                                ijk
                                             
                                          
                                       
                                     represent a set of time interval where 
                                       
                                          j
                                          ∈
                                          [
                                          1
                                          ,
                                          c
                                          ]
                                       
                                    and 
                                       
                                          k
                                          ∈
                                          [
                                          1
                                          ,
                                          
                                             
                                                n
                                             
                                             
                                                tau
                                             
                                          
                                          ]
                                       
                                    , and 
                                       
                                          
                                             
                                                n
                                             
                                             
                                                tau
                                             
                                          
                                       
                                     is the number of time intervals identified. Fig. 5
                                     shows an example of timing intervals being selected.

Let X be the feature set representing x. Then 
                                       
                                          
                                             
                                                X
                                             
                                             
                                                i
                                             
                                          
                                          =
                                          
                                             
                                                
                                                   
                                                      
                                                         X
                                                      
                                                      
                                                         ijk
                                                      
                                                   
                                                   ∀
                                                   j
                                                   ∈
                                                   [
                                                   1
                                                   ,
                                                   c
                                                   ]
                                                   ,
                                                   k
                                                   ∈
                                                   [
                                                   1
                                                   ,
                                                   
                                                      
                                                         n
                                                      
                                                      
                                                         tau
                                                      
                                                   
                                                   ]
                                                
                                             
                                          
                                       
                                    , where 
                                       
                                          
                                             
                                                X
                                             
                                             
                                                ijk
                                             
                                          
                                       
                                     is the sum of all data points in the time interval 
                                       
                                          
                                             
                                                τ
                                             
                                             
                                                ijk
                                             
                                          
                                       
                                     for channel j of data sample i.


                                    
                                       
                                          
                                             
                                                X
                                             
                                             
                                                ijk
                                             
                                          
                                          =
                                          
                                             
                                                ∑
                                             
                                             
                                                
                                                   
                                                      τ
                                                   
                                                   
                                                      k
                                                   
                                                
                                             
                                          
                                          
                                             
                                                x
                                             
                                             
                                                ij
                                             
                                          
                                       
                                    , where 
                                       
                                          
                                             
                                                x
                                             
                                             
                                                ij
                                             
                                          
                                       
                                     is the signal of data sample 
                                       
                                          i
                                          ∈
                                          [
                                          1
                                          ,
                                          
                                             
                                                N
                                             
                                             
                                                1
                                             
                                          
                                          +
                                          N
                                          2
                                          ]
                                       
                                     from channel 
                                       
                                          j
                                          ∈
                                          [
                                          1
                                          ,
                                          c
                                          ]
                                       
                                    , and 
                                       
                                          
                                             
                                                τ
                                             
                                             
                                                k
                                             
                                          
                                       
                                     is the 
                                       
                                          
                                             
                                                k
                                             
                                             
                                                th
                                             
                                          
                                          ∈
                                          [
                                          1
                                          ,
                                          
                                             
                                                n
                                             
                                             
                                                tau
                                             
                                          
                                          ]
                                       
                                     time interval.

This feature extraction method explores both the time and frequency domain features of the signal. Wavelet analysis can be performed either in continuous mode (CWT) or in discrete mode (DWT). Discrete Wavelet Transforms have been used in Perseh and Sharafat (2012) for representing the EEG data sample [15]. Wavelet decomposition for each channel using the Daubechies family of wavelets up to six levels is performed, and the approximation and detail coefficients for each level recorded. These coefficients are then used to represent the feature vector for an EEG data sample.

Generally, in order to develop a Brain Computer Interface (BCI) involves four major steps. Firstly, the generation of an EEG signal using some form of ‘Stimulus’. The EEG response can be evoked using either a visual, auditory, motor, touch or speech stimulus. Here in this research a visually evoked response is used to generate a type of EEG signal called the Event Related Potential (ERP). The EEG response is collected and stored corresponding to the event that evoked it. Thus the setup and collection of the EEG data is the second step in the development of a BCI. The first two steps occur concurrently, hence synchronization and timing is a crucial part for effective data collection. Thirdly, the pre-processing and representation of the EEG data. For effective representation of EEG data, the data is cleaned and is represented by a reduced subset of characteristic features. The last step involves generation of models that can classify the EEG signals representing different events. Once the EEG response to an event is classified/identified, subsequent action based on the purpose of the BCI can be taken.

The proposed framework is designed for a real time BCI as an interactive and intelligent image search and retrieval tool [13]. It describes the implementation of simultaneous data collection and timing synchronization during the visual stimulus for the BCI. It also describes the implementation of an ERP detection and classification system integrated with the BCI for real time navigation. It discusses how training and testing of EEG data is done using the proposed framework. A description of the workings and model of the framework is given below.

The BCI allows the user to navigate, and spell search queries in the user interface by detecting ERPs and/or motor imaginary movements. In order to perform a web image search, an initial search query spelled by the user using a Hex-O-Speller is used and the resulting images are retrieved. These images are then displayed in a Rapid Serial Visual Presentation (RSVP) and EEG response is recorded corresponding to each image shown. This data is then passed to the ERP interest score generator that takes into account the interest/attention shown in an image by the user and the images are sorted based on the interest.

The proposed BCI system is modeled as a Model View Controller (MVC) architecture as shown in Fig. 6
                        . The working of the system is described in detail as follows:

The View controls the display to the user, hence is responsible for the user interface of the system that generates the visual stimulus with the pertaining information (explained in detail below) for the user to choose. The visual stimulus used is a hexagonal placement of six circles that contain information, which are intensified by up-sizing in a random order for a short amount of time [14]. There are four different types of visual stimuli; Training, Navigation, Hex-O-Speller and RSVP.
                              
                                 •
                                 
                                    Training: The Training generates different visual stimuli for ERP detection, motor imaginary movements and eye movements during the training of the system for a specific user.


                                    Navigation: The Navigation displays different options for easy navigation through the interface either by using ERP detection or through motor imaginary movements.


                                    Hex-O-Speller: The Hex-O-Speller is used to type search queries which are then passed to the Controller for image retrieval from the web (see Section 2.1.2).


                                    Rapid Serial Visual Presentation (RSVP): After the images are retrieved and processed these images are shown to the user in a Rapid Serial Visual Presentation (RSVP) (see Section 2.1.3).

The Stimulus Generator handles the change and the placement of proper information in all of these displays.The rate of change and timing of the visual stimulus is synchronized with the Controller (Section 3.1.3). The Stimulus Generator also provides the necessary information about the visual stimulus to the Controller for tracking and organizing the data representing the EEG response corresponding to the visual stimulus.

The Model contains different classifiers giving the classification needed as per the stimuli provided and thus updating the View. It provides the information needed by the stimulus generator to change and update the display based on the EEG response corresponding to the previous stimuli. The model contains sub-systems for classifying motor imaginary movements, detecting ERPs, generating ERP scores, content based image similarity maps and ranking/queuing the images and also refining the search queries. There are two different types of ERP classifiers; one classifies a Target/Non-target i.e. a Yes/No choice, used in Navigation and the Hex-O-Speller; and the other generates an ERP interest score for the images displayed during the RSVP. The score along with the similarity map is passed to the Image Queuing/Ranking sub-system that determines the subset of images that the user has shown interest in. Using the names of the images, it finds similar keywords and adds/refines the initial search query.
                              
                                 •
                                 
                                    Motor Imaginary Movement Algorithm This sub-system generates the training model for classifying left,right,up and down movements as shown in [5,28]. Using this classification the user can control and navigate through the interface, emulating the control paradigm for a Mouse or a Joystick. The Motor Imaginary Movement Classifier is an alternate way of controlling the interface other than the ERPs.


                                    ERP Detection Algorithm This sub-system identifies the event related potentials (ERPs) in a EEG response. It filters, removes the artifacts and noise using Independent Component Analysis (ICA) and generates the features for representing the components of ERPs (see Section 2.3.3).


                                    ERP Yes/No Algorithm This subsystem generates the training model for classifying the ERPs as Target and Non Target, thus emulating a ’Yes/No’ choice from the user. This classification is used in Navigation to emulate a click, and in Hex-O-Speller to select the alphabets to type the search query [14].


                                    ERP Score Generation Algorithm This subsystem uses the EEG response to define an objective measure for the interest shown by the user in an image during the Rapid Serial Visual Presentation (RSVP). It generates the features needed by the Image Queuing/Ranking subsystem to select the subset of images representing the user’s interest.


                                    Content Based Image Similarity Map Generation Algorithm This subsystem compares and analyzes all the images retrieved from the web in terms of similar content. The criteria of similar content can be both local and global features such as shapes, color, texture, edges or any other information that can be retrieved from the images [29–31]. In [9] a graph based representation has been achieved based on the similarity of these images.


                                    Image Queuing/Ranking and Search Query Refinement Algorithm This subsystem combines the results of Similarity map generation and the ERP score and selects the subset of images representing the user’s interest. The metadata i.e. the name of an image from the web can be used to refine the search query by matching similar keywords.

The Controller acts as an intermediary between the View and the Model. It collects the EEG data and also synchronizes the rate of change of the display in the View to that of the data collection. It also organizes the data corresponding to each stimulus and retrieves images from the web, thus providing the necessary data to the Model and also chooses the sub-system or classifier to be used. The data is represented as a structure that contains the information of the visual stimuli and the data collected corresponding to it. This is explained in detail in the following section.

The data collection is needed to be done simultaneously and synchronously with the visual stimulus shown. During training or testing the data is to be collected and stored along with the information corresponding to the visual stimulus. Moreover, the information of event timings; here the intensification of circles containing the pertinent information in a visual stimulus, is also stored along with the data. The event information is necessary in order to extract the EEG response corresponding to each event in a visual stimulus. The data sample for each stimulus is stored and organized in a data structure.

The block diagram representing the flow of information and working of the View and Controller for data collection during Training is shown in Fig. 7
                        . The timing diagram representing the tasks performed during Data Collection is shown in Fig. 8
                        . 
                           
                              Task
                              1
                           
                         represents the buffering of data, 
                           
                              Task
                              2
                           
                         shows the display of the visual stimulus as controlled by the Stimulus Generator (see Section 3.1.1), 
                           
                              Task
                              3
                           
                         records the events taking place during a visual stimulus and 
                           
                              Task
                              4
                           
                         organizes and stores the data collected for the visual stimulus. The workings of the Controller subsystems i.e. the data buffering, organization and synchronization is discussed in detail below:

This sub-system interfaces with the data-collection hardware i.e. Bio-Radio 150 [21] and reads the bio-signal data. This data is stored in a buffer of a fixed size. The size of the buffer can be controlled and is determined by the time taken by a single visual stimulus.
                              
                                 •
                                 The buffer, say 
                                       rawWindow
                                     is a defined matrix of size (c
                                    ×
                                    p), where c are the Number of Channels and p is the Collection Interval, i.e.

Collection Interval 
                                       
                                          p
                                          =
                                          
                                             
                                                F
                                             
                                             
                                                s
                                             
                                          
                                          ∗
                                          
                                             
                                                T
                                             
                                             
                                                collection
                                             
                                          
                                       
                                    , where 
                                       
                                          
                                             
                                                F
                                             
                                             
                                                s
                                             
                                          
                                       
                                     is the Sampling Rate and the 
                                       
                                          
                                             
                                                T
                                             
                                             
                                                collection
                                             
                                          
                                       
                                     is the duration of collection. Here in the given system,


                                    
                                       
                                          
                                             
                                                F
                                             
                                             
                                                s
                                             
                                          
                                          =
                                          960
                                       
                                     samples/s, 
                                       
                                          
                                             
                                                T
                                             
                                             
                                                collection
                                             
                                          
                                       
                                    = 3s. Therefore 
                                       
                                          p
                                          =
                                          960
                                          ∗
                                          3
                                          =
                                          2880
                                       
                                     data points.

The buffer is constantly updated and the data appended as long as the entire BCI system is running. The rawWindow is transferred to the Data Organizer at the end of the stimulus (Section 3.2.3) when the Time Synchronizer invokes the Read Event (Section 3.2.2). Hence, the continuous bio-signal response for the entire duration of the visual stimulus is recorded and transferred to the Data Organizer. This process is represented as 
                              
                                 Task
                                 1
                              
                            in the timing diagram (refer Fig. 8).

This sub-system records the times of the events in the visual stimulus. These times of the events are crucial for epoching/segmenting the bio-signal data collected for the entire duration of the stimulus. The timings and the working of different tasks performed by the subsystems are represented in Fig. 8. This process is represented as 
                              
                                 Task
                                 3
                              
                            in the timing diagram.

There are eight different events tracked during a visual stimulus. The significance of each event is explained as follows:
                              
                                 •
                                 
                                    Start Event This event signifies the start of the visual stimulus. This event is important as the data from the start event to the beginning of the first up-sizing/intensification of the circle i.e. Event 1 in the rawWindow can be used for baseline correction [27]. In the timing diagram (Fig. 8) the times shown for a single visual stimulus (
                                       
                                          Task
                                          2
                                       
                                    ) span the duration of collection i.e. 3s, and the Start Event happens at 0.4s. For a duration of 0.2s after the Start Event there is no up-sizing/intensification in the visual stimulus.


                                    Events 1 to 6 These Events represent the up-sizing/intensification of the six circles. These event times are important for identifying the epochs i.e. the EEG response for each up-sizing circle that may contain event related potentials (ERP). The stimulus onset i.e. the time between each upsizing/intensification here is 0.25s. The up-sizing lasts for 0.1s and 0.15s is the time taken before the next up-sizing. The stimulus onset can be controlled and modified as per the need of the experiment during training or the reaction time of the subject.


                                    Stop Event This event signifies the end of the visual stimulus. This event happens 0.8s after the last up-sizing of circles. In order to include the epoch response of the last event, delay of 0.8s is given. In the timing diagram (Fig. 8) the time for the stop event shown is at 2.9s. Immediately after, the Time Synchronizer triggers the Read Event asking the Data Collector to transfer the buffer to the Data Organizer.


                                    Read Event This event signifies the exact time of transfer of the raw data from the buffer to the Data Organizer. The reason for separating the Stop Event and the Read Event is to compensate for any processing delay that could happen, as these processes are controlled by different subsystems. In the timing diagram (Fig. 8) the time for the read event shown is at 3s, i.e. the last data point of the rawWindow. This is used as the reference for the times of the previous events.

This sub-system organizes each data sample for a single visual stimulus and stores it into a data structure. The fields of this structure are described as follows:
                              
                                 •
                                 
                                    Order This field contains information of the sequence in which the six circles in a visual stimulus are up-sized. In case of RSVP it has the order/queue of the images.


                                    Association This field contains the information to be displayed in the visual stimulus. This information can be the options during Navigation, Alphabets in Hex-O-Speller, image pointers during RSVP or sequences of a Training trial. During training the association is predetermined.


                                    Raw EEG Data contains the raw data received from the data collection system. Epoching/Segmentation is performed on this data and is stored in a 4-Dimensional matrix say rawData of dimensions (c
                                    ×
                                    w
                                    ×
                                    e
                                    ×
                                    t).
                                       
                                          – Number of Channels (
                                             c
                                             ): number of locations of the electrodes from where the data is recorded.

– Size of Epoch Window (
                                             w
                                             ): The Epoch Window represents the set of data points to be selected from the rawWindow representing each epoch i.e. the range of data points that will contain the response for each up-sizing of circle in a visual stimulus. The position of the epoch window is determined by the Events i.e Event 1 to 6 (see Fig. 18). Here the size is for a duration is 1s =960 datapoints. The Epoch Window is placed such that all the data points representing 0.2s prior and 0.8s after the Events i: (i
                                             =1 to 6).

– Number of Events (
                                             e
                                             ): for 6 events, as there are 6 circles up-sizing in a single visual stimulus.

– Number of Trials (
                                             t
                                             ): In case of ERP recording, multiple trials of the same visual stimulus are taken and averaged after artifact removal to suppress noise [27,15].


                                    Trigger information contains information of the time of the events in a visual stimulus (see Fig. 8).


                                    Label represents the result of a classifier on the data. For Navigation or Hex-O-Speller, it is labeled as Up/Down/Left/Right or Yes/No depending on the classifier used. In case of RSVP, an ERP score is assigned. This field is predetermined during training.


                                    Inference Based on the Labels determined after the classification results from the Model, the action or the choice of the user is inferred. For example,in case of a Hex-O-Speller or Navigation, the choice of the user; and during RSVP the ranking of the image.The Stimulus Generator generates the next visual Stimulus based on this inference. During Training this attribute is predetermined, and is used to instruct the subject what option to choose (as shown in Fig. 17).

The Data Organizer performs different tasks during training and testing modes of the system. During training the data structures are stored and written to a file corresponding to each visual stimulus.Where as during testing the data structure is passed to the Model for classification. During training the labels and inferences are pre-determined and the Model reads these files and generates the training model needed for classification.

In order to test and understand the behavior of event related potentials, ERP Detection System and the Yes/No (2-Class Target/non-Target) classifier were implemented for a standardized data set of P-300 Speller responses [17]. Fig. 9
                         shows the block diagram of the implemented system. This system is a part of the Model. There are two modes in which this system works; training and testing. During training multiple data samples are collected and shelved in response to the stimulus provided by the View, and the training model for the selected classifier is generated. During testing each sample is collected and classified using the generated training model. The data from the data set is first segmented/epoched into a 4-Dimensional matrix, say D.
                           
                              •
                              
                                 D is of size (p
                                 ×
                                 c
                                 ×
                                 r
                                 ×
                                 t), where p is the Number of data points in a signal per channel i.e. time/sampling rate, c is the number of channels, r is the number of responses depending on the stimuli (P-300 has twelve, where as a Hex-O-Speller has six, see Section 2.1) and t is the number of trials (see Section 3.2).

Each data sample for a response and trial is filtered using a band pass filter with 0.1Hz and 30Hz cut-off frequencies. This data sample is then winsorized with a 
                           
                              
                                 
                                    90
                                 
                                 
                                    th
                                 
                              
                           
                         percentile. This data is then placed into a structure as described in Section 3.2.3.

After the data is shelved in a structure, Target and Non-Target samples are extracted. For the all the data samples for a character, the data matrix 
                           
                              
                                 
                                    D
                                 
                                 
                                    k
                                 
                              
                           
                         for 
                           
                              
                                 
                                    k
                                 
                                 
                                    th
                                 
                              
                           
                         character, is concatenated across all channels, resulting in a matrix say M.
                           
                              •
                              
                                 M is of size (c
                                 ×
                                 m), where m is the length of the concatenated points, which is 
                                    
                                       m
                                       =
                                       p
                                       ∗
                                       r
                                       ∗
                                       t
                                    
                                 .

Following this ICA is performed on 
                           
                              
                                 
                                    M
                                 
                                 
                                    avg
                                 
                              
                           
                         where
                           
                              •
                              
                                 
                                    
                                       
                                          
                                             M
                                          
                                          
                                             avg
                                          
                                       
                                       =
                                       mean
                                       (
                                       
                                          
                                             M
                                          
                                          
                                             k
                                          
                                       
                                       )
                                    
                                  : 
                                    
                                       ∀
                                    
                                  Characters, resulting in the unmixing matrix 
                                    
                                       
                                          
                                             W
                                          
                                          
                                             avg
                                          
                                       
                                    
                                 .

Then a subset of all
                           
                              •
                              
                                 
                                    
                                       
                                          
                                             M
                                          
                                          
                                             j
                                          
                                       
                                    
                                  : j 
                                    
                                       ∈
                                    
                                  Random Set of Characters, is chosen and ICA is performed resulting in the unmixing matrix 
                                    
                                       
                                          
                                             W
                                          
                                          
                                             j
                                          
                                       
                                    
                                 .

For all selected 
                           
                              
                                 
                                    W
                                 
                                 
                                    j
                                 
                              
                           
                        , correlation with 
                           
                              
                                 
                                    W
                                 
                                 
                                    avg
                                 
                              
                           
                         is performed and the components are selected using a threshold. Following which the components that are to be kept are identified, say a set a. The resulting truncated unmixing matrix 
                           
                              
                                 
                                    W
                                 
                                 
                                    trunc
                                 
                              
                              =
                              
                                 
                                    W
                                 
                                 
                                    avg
                                 
                              
                              (
                              a
                              ,
                              :
                              )
                           
                         is stored, as the ICA parameter and the rest of the data is corrected as explained in Section 2.3.3. The corrected data is then re-referenced, averaged and base-line corrected. Spatial and temporal features are identified, features extracted, features reduced using Principal Component Analysis and the training models are generated. The training models are generated for four different classifiers: Linear Discriminant Analysis (LDA), optimized LDA, Support Vector Machine (SVM) and Neural Networks (NN). The aforementioned classifiers are the most commonly used classifiers in the BCI systems [32]. An EEG response for an ERP can be characterized in both time and frequency domain. Hence, three different types of feature extraction methods Band Power (BP), Timing Intervals/Segments (TS) and Wavelet Decomposition (WD) are explored. Band powers solely represent the frequency domain features of the EEG response (Section 2.4.3), time segments represent time domain features (Section 2.4.4) and wavelets represent both time and frequency domain features (Section 2.4.5).

The combination of these three feature extraction methods and four classifiers results in 12 different ERP detection and classification models for a sample data set. The results of this system for determining consistently accurate and fast training model for the BCI, are presented in the following Section 4.

@&#RESULTS@&#

This section discusses the results obtained from the ERP detection and the Yes/No (2-Class Target/non-Target) classifiers described in Section 3.3. It explains the experiment setup and methodology used in the collection of data. It describes the training stimulus and navigation through the BCI. It also discusses the results obtained from simultaneous data collection and stimulus generation for the implemented prototype BCI system.

The ERP detection and the classifier were tested on the P300 Matrix Speller standard dataset from the BCI Competition 2005 [17] for two subjects. The P300 Matrix Speller stimulus results in finding two target responses form the twelve responses to the stimulus. Each target response corresponds to a row and column in the matrix of symbol, thus identifying the targeted symbol. This setup is different from the Hex-O-Speller, as it requires one target response for 6 stimuli. In P300 speller the goal is to identify 1 character out of 36 characters from the 6×6 matrix, whereas for Hex-O-Speller the goal is to identify group of characters or character out of 6 choices, depending on the step (Section 2.1.2).

The dataset is recorded for two subjects, Subject A and Subject B. For each subject EEG responses from 64 channels/sensors are collected at the rate of 
                              
                                 
                                    
                                       F
                                    
                                    
                                       s
                                    
                                 
                              
                            is 240 samples/s for 15 trials per character. The EEG electrodes are placed using the 10–20 electrode placement system as shown in Fig. 4. The recorded EEG is band-passed from 0.1Hz to 60Hz. As the ERP relevant information can be found in the frequency range of less than 30Hz, this data is band passed further during epoching/segmentation. The training and testing datasets contain 85 and 100 characters, respectively for each subject.

Hence the total number of data samples/epochs, say 
                              
                                 
                                    
                                       N
                                    
                                    
                                       train
                                    
                                 
                              
                            in training is.
                              
                                 •
                                 
                                    
                                       
                                          
                                             
                                                N
                                             
                                             
                                                train
                                             
                                          
                                          =
                                          k
                                          ∗
                                          r
                                          ∗
                                          t
                                       
                                    , where k is the number of characters r is the number of responses in 1 trial and t is the number of trials.

Hence, 
                                       
                                          
                                             
                                                N
                                             
                                             
                                                train
                                             
                                          
                                          =
                                          85
                                          ∗
                                          12
                                          ∗
                                          15
                                          =
                                          15300
                                       
                                    ; and testing, say 
                                       
                                          
                                             
                                                N
                                             
                                             
                                                test
                                             
                                          
                                          =
                                          100
                                          ∗
                                          12
                                          ∗
                                          15
                                          =
                                          18
                                          ,
                                          000
                                       
                                    .


                           Fig. 10
                            gives an idea of how the ERP signal looks like and behaves over the time course for a data sample/epoch. The graph in Fig. 10 shows the time course of the ERPs at channel Cz and the average of channels Po7 and Po8. The baseline is taken from −100ms to 0ms as the Stimulus Onset Asynchrony i.e the time between each intensification (row or column stimulus) is 100ms.

Each ICA component is a representation of the contribution of all the channels. The artifacts that are introduced in the signal are highly correlated and generally contributed from the extreme frontal and temporal region channels of the brain. An example topographic map of a the contribution of each channel in the rejected components for Subject A, B as a 2-D circular view is shown in Fig. 11
                           . Fig. 11 shows that these components have a higher contribution from the frontal lobe and are highly likely to contribute as an eye blink or eye movement artifacts [25,26]. For Subject A, 42 independent components were selected, and for Subject B, 54 components were selected. The threshold for correlation for identifying the artifact channels was chosen as 0.5 where 0 and 1 represent highly uncorrelated and correlated respectively. In order to perform the ICA the FastICA algorithm was used [33]. It can be observed that Subject B data is less riddled with noise compared to Subject A as more components were retained in the former. This can also be observed in the final classification results in Table 3.

For feature extraction, spatial and temporal discriminating criterion is identified using Bhattacharyya Distance (BD) and signed-
                              
                                 
                                    
                                       r
                                    
                                    
                                       2
                                    
                                 
                              
                            respectively (see Section 2.3).

The results for the channel selection using BD are shown in Figs. 12 and 13
                           
                           . In Figs. 12 and 13, the figure on the left column shows the discriminating behavior for each data point as a 2-D color mapped image, and the bar graph on the right shows the mean BD for the entire time course of a data sample/epoch. For the 2D-color map the x-axis is the index of each channel as shown in Fig. 4, and the y-axis is the data points ranging from 1 to 168, that correspond to 0–700ms given 
                              
                                 
                                    
                                       F
                                    
                                    
                                       s
                                    
                                 
                                 =
                                 240
                              
                            samples/s data sampling rate. Table 1
                            lists the selected 32 channels sorted in descending order of the BD. Fig. 14
                            shows the topographic map for the channels selected for Subject A and B. It can be seen that the most significant channels are present in the parietal region.

The results for the selection of time segments using signed-
                              
                                 
                                    
                                       r
                                    
                                    
                                       2
                                    
                                 
                              
                            values are shown in Figs. 15 and 16
                           
                           . In Figs. 15 and 16, figure on the left column shows the discriminating behavior for each channel as a 2-D color mapped image, and the bar graph on the right shows the mean signed-
                              
                                 
                                    
                                       r
                                    
                                    
                                       2
                                    
                                 
                              
                            for the selected channels shown in Table 1. For the 2D-color map the y-axis is the index of each channel as shown in Fig. 4, and the x-axis is the data points ranging from 1 to 168, that correspond to 0–700ms given 
                              
                                 
                                    
                                       F
                                    
                                    
                                       s
                                    
                                 
                                 =
                                 240
                              
                            samples/s, the data sampling rate. Table 2
                            lists the selected 8 time ranges. The intervals of the time segments are selected by finding the peaks and valleys in the averaged signed-
                              
                                 
                                    
                                       r
                                    
                                    
                                       2
                                    
                                 
                              
                            values for all the selected channels. It can be seen that the most prominent time segment lie between 0.2s (48 data point) and 0.6s (144 data point).

The objective of performing this comparison was to determine the best combination of the feature extraction method and a classifier that was accurate, consistent and fast. Three different feature extraction methods were used to generate features for the given P300 sample dataset. They were Band Powers (BP) i.e. sum of FFT coefficients for the EEG bands, sum of data points over the selected Time Segments/Intervals (TS) and Wavelet Decomposition (WD) (Section 2.3). Each of these features were classified using three different classifiers: Linear Discriminant Analysis (LDA), Support Vector Machine (SVM) and Neural Networks(NN). An optimized variation of LDA (oLDA) was also used as a classifier. The objective of the optimized LDA was to find the best subset of samples for each class in training using the scatter matrices as the optimization criterion. A fivefold cross-validation with 50% of the training data set was performed and the mean performance parameters were noted. Table 3
                            list all the combinations of the results for both subjects. The performance parameters with the target (positive) and non-target (negative) class are true positives (TP), false positives(FP), false negatives (FN) and true negatives (TN). Other derived performance parameters were calculated as follows [15]:
                              
                                 (3)
                                 
                                    Accuracy
                                    =
                                    
                                       
                                          TP
                                          +
                                          TN
                                       
                                       
                                          TP
                                          +
                                          TN
                                          +
                                          FP
                                          +
                                          FN
                                       
                                    
                                 
                              
                           
                           
                              
                                 (4)
                                 
                                    TruePositiveRate
                                    
                                    (
                                    TPR
                                    )
                                    =
                                    
                                       
                                          TP
                                       
                                       
                                          TP
                                          +
                                          FN
                                       
                                    
                                 
                              
                           
                           
                              
                                 (5)
                                 
                                    FalsePositiveRate
                                    
                                    (
                                    FPR
                                    )
                                    =
                                    
                                       
                                          FP
                                       
                                       
                                          TN
                                          +
                                          FP
                                       
                                    
                                 
                              
                           
                           
                              
                                 (6)
                                 
                                    TrueDetectionRate
                                    
                                    (
                                    TDR
                                    )
                                    =
                                    
                                       
                                          TP
                                       
                                       
                                          TP
                                          +
                                          FP
                                          +
                                          FN
                                       
                                    
                                 
                              
                           The training models subsequently generated were used to classify the test data sets of Subjects A and B that contained data epochs for 100 characters. The second last column of Table 3 lists the number of characters accurately detected. As there are two ERP responses for every 12 response, correct classification of two responses simultaneously was required to accurately detect the target character. The last column lists the number of correctly detected row or column. For every row or column detected correctly, 0.5 was added to the total of the target detected. The performance of the ERP classification in comparison to other research is shown in Table 5. The performance is the average of correctly detected characters for Subject A and Subject B. In [15], the author compares the results by using different sets of EEG channels and wavelet decomposition features representing the ERPs to identify the ones giving the highest accuracy. Binary Particle Swarm optimization is used to search for the best combination of wavelet features and channels using TDR as the fitness criterion [15]. Here in this system, no optimization for selecting the combination of EEG channels and wavelet features was done in order to reduce the processing time.

The number of features generated and the processing time for each of the aforementioned combinations of feature extraction methods and classifiers is compared in Table 4
                           
                           , in order to choose the best fit during its integration with a real time BCI. The first two columns of the Table 4 show the number of features generated and the features reduced as a result of Principal Component Analysis (PCA) i.e. keeping 99% of the contributing components. Average processing times during the three phases of feature generation, training and classification were measured.

Accuracy and processing (computation) speed are the criterion to determine the best combination of the feature extraction method and the classifier. As the number of non-target samples is five times that of the target samples (12 responses=10 non target response+2 target response), the TPR and TDR are better criteria to judge the performance of a classifier model than the Accuracy 
                           [15]. It can be observed in Table 3 that Band Powers (BP) has a poor performance compared to Time Segments (TS) and Wavelet Decomposition(WD) methods for all the classifiers. The performance during validation for the WD and TS are very similar but WD does not produce consistent results when tested on the 100 character test dataset (Section 4.1.1). The TS in combination with LDA produces the best results for both Subjects A and B i.e. 79 and 91 characters out of 100 respectively, giving an average of 85% accuracy. The results of the combination of TS with Neural Networks (NN) are also good but are not selected because the NN has a longer training and classification processing times compared to LDA (Table 4). It can be observed that the combination of Time Segments with Linear Discriminant Analysis yielded the fastest processing times.

As described in Section 3 a real time implementation of the proposed BCI framework was done by integrating the ERP System for generating training models and classifying ERPs; and the simultaneous data collection and stimulus generation. The Stimulus Generation controlled by the ‘View’, timing synchronization for data collection and stimulus generation controlled by the ‘Controller’ and ERP detection and the Yes/No(2 class target and non-target) classifier subsystems for the ‘Model’ were implemented. In order to test the workings of the ‘Controller’, initially electromyogram (EMG) signals from the wrist are used. This is followed by collecting EEG signals from 10 channels as per the 10–20 electrode placement system. A classifier model using Time Segments/Intervals (TS) as the feature extraction method and Linear Discriminant Analysis (LDA) as the classifier is generated. Navigation though the interface is tested in real time.

The EEG data is collected using a standard 10/20 placement EEG cap and multiple Bio-Radio 150 data collection systems [21]. The ERP training of the system involves collection of EEG data samples for the visual stimuli with feedback from the user (if not physically disabled) as he/she concentrates on a choice and reaffirms it using the Spacebar key on the keyboard. The tools used for making the prototype are MATLAB for collecting, analyzing and processing the data and the Psych Toolbox [18] for the generation and synchronization of the visual stimulus. For faster processing, the Parallel Processing Toolbox in MATLAB is used sparsely. The system can be made faster by making efficient use of this toolbox.

The generation of visual stimulus during training can be tailored. Fig. 17
                            shows two different types of visual stimulus used to collect ERP related data. Fig. 17(a) shows a choice based stimulus where a polygon is positioned in one of the circles. The position of the polygon displayed changes in random order during multiple trials and the ERP response is collected and stored. The Hex-O-Speller based stimulus instructs the subject to spell predefined words and the responses are stored. As shown in Fig. 17(b) in order to spell the letter ‘A’, the Hex-O-Speller displays the choice to be made at the top-left corner of the screen. Before the beginning of each trial in training, a fixation time is allotted to the user to concentrate on the desired area of the screen. During that time, the training system also makes the user aware of the character to be spelled and its corresponding choice by displaying the character for a very brief time in the center of the screen (before moving to the top-left corner) and flashing/up-sizing the related choice. This information regarding the choice is stored in the inference and label fields of the data structure. During testing these fields are assigned after classification by the Model (see Section 3.1.2). During testing the navigation interface for the BCI is used to evoke EEG responses from the user.

Here as a proof of concept for testing the workings of the data collection system EMG signals from the wrist were used and the corresponding data collected and segmented. Fig. 18
                            shows the raw data collected and the epochs extracted. The red line represents the start of the visual stimulus, the green lines represent the end of up-sizing of a circles in the stimulus and the black line that almost coincides with end of the raw data window represents the stop event. The green lines that represent the events are spaced on an average time of 0.22s. The timing diagram shown in Fig. 8 the events are spaced at 0.25s, which is the maximum time allowed between the up-sizing of the circles. Ideally the spacing between the up-sizing events should be 0.2s,but a delay error of range 0.05s happens due to processing delays.Hence, the stimulus onset varies from 0.20–0.25s. But as the epoching/segmentation is done with respect to event times, it is safe to assume that the epochs extracted adequately represent the response for each up-sizing of the circle with minimal delays.

This data collection system allows the user to control and change various parameters during data collection. As this system can interface with multiple Bio-Radio 150s, the number of channels (c) can be increased. The size of the collection interval (p) can be changed based on the time taken by a single visual stimulus. The user can also add events and vary their stimulus onset during a visual stimulus. The size and placement of the epoch window (e) can be changed as per the need of the experiment or reaction time of the subject to be tested (Section 3.2.3). The number of repeated trials (t) needed for averaging to suppress ERP related noise can be set.

The EEG signals are collected using two Bio-Radio 150 from 10 channels at 960 samples/s data sampling rate. The channels used are P3, P4, C3, C4, F3, F4, O1, O2, Cz and Pz (see Fig. 4). During training for each stimulus 10 trials are recorded of the same epoch. As shown in Fig. 17(a), three different polygons were randomly placed for 3 repetitions and 10 trials of each repetition were recorded. This generated 9 selection responses. This was followed by the Hex-O-Speller, where the text ‘
                              
                                 MABLRIT
                              
                           ’ was spelled. This text is relatively small as compared to the dataset used in [14]. This generates 16 responses as each letter is spelled in two recurrent steps. This visual stimuli is followed by a Rapid Serial Visual Presentation, where the subject tries to identify a ’red car’ from a series of images queued [9]. The RSVP is repeated for 10 times, in order to get 10 trials for the EEG response corresponding to an image. This results in the collection of data for different types of visual stimuli. This data is stored and organized and used in generating a training model for the ERP.

After collection of the EEG responses, followed by pre-processing and data correction target and non-target epochs were extracted. Fig. 19
                            shows the mean ERP response of the target and non-target epochs. The graph in Fig. 19 shows the time course of the ERPs at channel Cz and the average of channels P3 and P4. The data shown is base-line corrected.

The collected EEG data is then passed to the ERP detection and classification system (Section 3.3) for the generation of a training model. The classifier was integrated with the system to detect ERPs in real time. In order to measure the success of the working of the BCI in real time the user/subject was asked to perform a task of navigating through the interface and spelling the word ‘FOX’. This task required 6 consecutive correct detection and classification of the ERPs, as the selection of each alphabet required 2 recurrent steps in the Hex-O-Speller. This task was achieved 6 times out of 10 tries to spell the word ‘FOX’ i.e., sixty percent of the time the subject was able to spell the word ‘FOX’ correctly.

@&#CONCLUSION@&#

The data collection system for the proposed BCI framework is adequately able to collect data in sync with the visual stimulus. However, due to the lack of data from more than one user/subject, comment cannot be made on the accuracy of the Timing Synchronization. It allows some flexibility to control various aspects and parameters of data collection. It also organizes the data during training and records the related visual stimulus and event information in a well defined data structure. This system can be used for performing future experiments and collecting data for various subjects; both healthy and disabled.

The results obtained from classification of ERP’s on the standardized data set show that the data obtained from Subject B was less noisy, and it resulted in higher accuracy compared to Subject A. For feature extraction, the temporal domain information for ERP’s is important and is preserved in the Time Segment feature extraction method. Band powers resulted in the worst performance for ERP classification. Wavelet Decomposition results were relatively better compared to the Band Powers. Even though the results for performance during validation was high, it did not necessarily give good results during testing. In comparison to the Wavelet Decomposition the Time Segments were more consistent. The classifier model with Linear Discriminant Analysis in combination with Time Segment features resulted in the best classification accuracy for both the subjects and produced consistent results. This model also had the fastest processing (computation) speed during feature extraction, training and classification. The framework for a real time implementation of the BCI was successfully implemented using this classifier model, resulting in a decent 60 percent success rate for spelling the three letter keyword ‘FOX’ using the Hex-O-Speller. This accuracy result implies the working of the proposed framework for the real time implementation of a BCI and with further data collections and optimization shows the possibility of further improvement.

@&#FUTURE WORK@&#

Approval from the Institutional Review Board (IRB) to perform data collection using the proposed BCI system on a group of 15–30 subjects; both healthy and disabled is being sought. Pending approval this system will be tested on the participating subjects. Using this system training models for these subjects will be generated. The User Interface will be optimized and changed based on the feedback from the subjects regarding the ease of navigation, representation of information and the options available. Also an analysis on time synchronization of the EEG data collected to the visual stimuli and the processing time of the system will be done. For collecting a larger set of ERP data during Hex-O-Speller, instead of spelling ‘
                        
                           FOX
                        
                     ’ or ‘
                        
                           MABLRIT
                        
                     ’, a text such as ‘
                        
                           PACKMYBOXWITHFIVEDOZENLIQUORJUGS
                        
                     ’ that contains all the alphabet will be used. The detection and spelling of this text will be a better representative of a measure for accuracy for the working of the ERP classifier and the proposed BCI.

Other Classifier models for detecting motor imaginary movements, eye movements, etc. will be implemented. Training of the user for detecting eye blinks and movements will be done in order to remove artifacts or noisy sample data. For training of the motor imaginary movement, an object will be animated to move left, right, up and down and the EEG data collected. In order to understand the relation of the EEG scores to the selected images and define a formulation, data from multiple subjects (healthy and disabled) will be collected and analyzed. Each subject will be shown a series of semantically similar images and the EEG data will be recorded. The subject will then be asked to rank the images according to his/her liking. They will also give a valence score from 1 to 5 for each image shown to determine the criteria and the reason for their interest. They will score an image in terms of it being pleasant, unpleasant, and neutral [2]; or was it recognized as something similar or familiar [4,34].

@&#ACKNOWLEDGMENTS@&#

The authors would like to thank Rochester Institute of Technology for the Effective Access Technologies Grant for this work and the staff in Multi-Agent Bio-robotics Laboratory (MABL) for their valuable inputs. The authors are grateful to Ryan Bowen and Daniel Nystrom for their help and views during the design of this BCI framework.

Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.compeleceng.2015.03.024.


                     
                        
                           Supplementary video
                           
                        
                     
                  

@&#REFERENCES@&#

