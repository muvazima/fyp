@&#MAIN-TITLE@&#Identification of Heat Shock Protein families and J-protein types by incorporating Dipeptide Composition into Chou's general PseAAC

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We develop a predictor for classification of Heat Shock Proteins and J-proteins.


                        
                        
                           
                           It is the combination of Dipeptide Composition and SVM.


                        
                        
                           
                           Two datasets were evaluated using jackknife test.


                        
                        
                           
                           Best results are reported so far in the literature.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Heat Shock Proteins

J-protein

SVM

KNN

PNN

@&#ABSTRACT@&#


               
               
                  Heat Shock Proteins (HSPs) are the substantial ingredients for cell growth and viability, which are found in all living organisms. HSPs manage the process of folding and unfolding of proteins, the quality of newly synthesized proteins and protecting cellular homeostatic processes from environmental stress. On the basis of functionality, HSPs are categorized into six major families namely: (i) HSP20 or sHSP (ii) HSP40 or J-proteins types (iii) HSP60 or GroEL/ES (iv) HSP70 (v) HSP90 and (vi) HSP100. Identification of HSPs family and sub-family through conventional approaches is expensive and laborious. It is therefore, highly desired to establish an automatic, robust and accurate computational method for prediction of HSPs quickly and reliably. Regard, a computational model is developed for the prediction of HSPs family. In this model, protein sequences are formulated using three discrete methods namely: Split Amino Acid Composition, Pseudo Amino Acid Composition, and Dipeptide Composition. Several learning algorithms are utilized to choice the best one for high throughput computational model. Leave one out test is applied to assess the performance of the proposed model. The empirical results showed that support vector machine achieved quite promising results using Dipeptide Composition feature space. The predicted outcomes of proposed model are 90.7% accuracy for HSPs dataset and 97.04% accuracy for J-protein types, which are higher than existing methods in the literature so far.
               
            

@&#INTRODUCTION@&#

Heat Shock Proteins (HSPs) are essential constituent for the cell growth and survival, which exists in all living organisms ranging from bacteria to archaea [1–3]. They were first discovered in 1962 [1,2]. The main function of HSPs is to control the folding and unfolding process of proteins [4,5]. It is also called stress proteins because they prevent the unalterable aggregation of misfolded proteins under stress conditions like stress or high temperature, etc. [4]. HSPs play a vital role in protecting cellular homeostatic processes from environmental and physiologic stress [6] and in protein–protein interaction like unwanted protein disaggregation and establish proper protein conformation. The expression levels of HSPs that are functionally related proteins increased when the cells are exposed to physiological and environmental stress condition such as high temperature, infection and swelling [7,8]. They are also involved in a large variety of cellular processes like protein assembly, maturation, assisting protein folding, degradation of misfolded proteins, intercellular trafficking, modulating signaling pathways and regulating immune responses [3,9,10]. To control the quality of newly synthesized proteins, HSPs take part in cellular homeostasis [10]. More recently, HSPs have been implicated as a biological factor in anti-tumor immunity, oncogenesis, type 1 diabetes mellitus, atherosclerosis, multiple sclerosis and other autoimmune reactions [11–14]. HSPs are categorized into six different families on the basis of nature and functionality [2,15]. These families are: (i) HSP20 or sHSP (Small Heat Shock Proteins), (ii) HSP40 or J-proteins, (iii) HSP60 or GroEL/ES, (iv) HSP70, (v) HSP90, and (vi) HSP100 [2,3,15]. HSP40 or J-proteins is further divided into four different types, that is, Type-1, Type-2, Type-3 and Type-4 [3,16]. Owing to the diversified nature and vast repertoire functions of HSPs, they have got considerable attention of the researchers. In this regards, extensive studies have been carried out in order to distinguish HSPs from other proteins. A series of efforts have been performed for identification of HSPs families from other proteins and reasonable results have been achieved [2]. In contrast, owing to huge exploration of proteins sequences and lack of recognized structures in databases, conventional methods like nuclear magnetic resonance (NMR) were inadequate for identifying HSPs families [17–20]. In such circumstances, computational methods were utilized for the identification of HSPs and J-protein types. In last few decades, numerous methods have been utilized for identification of HSPs and J-protein types [2]. More recently the notion of reduce alphabet amino acid composition method (RAAAC) was applied by different researchers and achieved remarkable results. Further it has been used in various area of computational biology, such as prediction of DNA-binding proteins [21], prediction of defensin family and subfamilies [22] and prediction of bioluminescent proteins [23]. Similarly, Feng et al. [2] have used RAAAC and Support Vector Machine (SVM) for prediction of HSPs families and obtained maximum overall accuracy 87.82%. In addition, Feng et al. also developed the first automated method for the discrimination of J-protein types and obtained the overall accuracy of 94% [16]. As summarized and demonstrated by a series of publication [2,24–32], in compliance with Chou's 5-step rule [33], to develop a convenient sequence-based prediction model for a biological system, the following procedures are necessary to consider: the first step is to choose or establish a valid benchmark dataset in order to train and test the predictor; the second step is to express the biological sequences into effective mathematical formula that can truly represent their intrinsic correlation with the target to be predicted; the third step is to introduce or develop a powerful learning algorithm; the fourth step is to perform statistical cross-validation tests to assess the performance of the predictor; and finally develop a user-friendly and publicly accessible web-server for the predictor that is accessible to the public.

In this study, we develop a high throughput computational model for the prediction of HSPs families and J-protein types. Three discrete protein sample representation methods namely: Pseudo Amino Acid Composition (PseAAC), Dipeptide Composition and Split Amino Acid Composition (SAAC) were used to extract the numerical values from biological protein sequences of HSPs and J-proteins types. Various classification methods were utilized to select the high throughput algorithm for computational model. Cross validation test leave one out is used to evaluate the performance of proposed model.

The remaining paper is organized as follows: Section 2 describes materials and methods, Section 3 represents performance measures, Section 4 describes the results and discussion and conclusion has been drawn in the Section 5 of the paper.

@&#MATERIAL AND METHODS@&#

The preliminary step for constructing a promising computational model is to have a valid benchmark dataset that train the model effectively. Moreover, the recent comprehensive review [34] suggested that it is not essential to split the dataset into training and testing data for evaluation of a predictor performance. In this regard, we have used two different datasets, the first dataset S
                        HSP of HSPs families and the second dataset S
                        JP is of J-protein types [2,16]. Total numbers of protein sequences in S
                        HSP are 2225, details are given in Table 1
                        . Similarly, total numbers of protein sequences in S
                        JP are 1245, details are given in Table 2
                        . The sequences of HSPs are available from the HSRIP dataset (http://pdslab.biochem.iisc.ernet.in/hspir), which currently composed of 9902 protein sequences including 277 genomes ranging from prokaryotes to eukaryotes. CD-HIT [35] was applied to reduce homologous bias and redundancy and those sequences were removed which have ≥40% pairwise sequence identity.

Similarly the sequence of J-proteins is available at HSPIR database at http://pdslab.biochem.iisc.ernet.in/hspir/, which currently contains 3901 protein sequences. J-proteins have ≥40 pairwise sequences identity and CD-HIT [35] program was used to reduce dimensionality and homologous bias. The dataset S
                        JP can be freely downloaded from http://lin.uestc.edu.cn/server/iJPred/data.
                           
                              (1)
                              
                                 
                                    
                                       S
                                       
                                          H
                                          S
                                          P
                                       
                                    
                                    =
                                    
                                       S
                                       1
                                    
                                    ∪
                                    
                                       S
                                       2
                                    
                                    ∪
                                    
                                       S
                                       3
                                    
                                    ∪
                                    
                                       S
                                       4
                                    
                                    ∪
                                    
                                       S
                                       5
                                    
                                    ∪
                                    
                                       S
                                       6
                                    
                                 
                              
                           
                        
                        
                           
                              (2)
                              
                                 
                                    
                                       S
                                       
                                          JP
                                       
                                    
                                    =
                                    
                                       S
                                       
                                          type1
                                       
                                    
                                    ∪
                                    
                                       S
                                       
                                          type2
                                       
                                    
                                    ∪
                                    
                                       S
                                       
                                          type3
                                       
                                    
                                    ∪
                                    
                                       S
                                       
                                          type4
                                       
                                    
                                 
                              
                           
                        
                     

Feature extraction is one of the main processes of machine learning, which is used for extraction of prominent features from protein sequences [36]. In the features extraction process, numerical attributes are calculated of protein sequences because the statistical model uses only numerical attributes for training. One of the essential but also most challenging problems in computational biology and Bioinformatics is how to express a biological sequence with a discrete model or a vector. This is because all the existing operation engines, such as support vector machine (SVM) and neural network (NN), can only handle vector but not sequence instances [37]. On the other hand, expressing protein sequences using discrete model may loses all sequence order information. In order to overcome this issue, the concept of Pseudo Amino Acid Composition (PseAAC) was proposed [38,39]. It has been adopted into many biomedicine and drug development areas [40], and nearly all the areas of computational proteomics [41–47]. Because it has been extensively applied, recently three powerful open access softwares, called ‘PseAAC-Builder’ [38], ‘propy’ [39], and ‘PseAAC-General’ [47], were developed: the former two are for generating various modes for Chou's special PseAAC: while the 3rd one for those of Chou's general PseAAC [33], including not only “Functional domain” mode [12], “Gene Ontology” mode [12], and “Sequential Evolution” or “PSSM” mode [12]. Encouraged by the successes of using PseAAC to deal with protein/peptide sequences, three web-servers [48–50] were developed for generating various feature vectors for DNA/RNA sequences. Particularly, recently a powerful web-server called Pse-in-one [30] has been established that can be used to generate and desired feature vectors for protein/peptide and DNA/RNA sequences according to the need of user's studies. In this study, the following features extraction methods have been used.

Dipeptide Composition is a discrete method uses for protein sequences representation. It considers the neighbor sequence information for training the classifiers. It denotes the frequency of two adjacent amino acids in a 400D vector [51,52]. It describes the global sequence information about protein sequences. The main advantage of using Dipeptide Composition over conventional amino acid composition (AAC) is that it considers the global information about proteins while AAC considers only the occurrence frequency of amino acids in protein sequences. Dipeptide Composition feature vector can be calculated as [22,51]:
                           
                              (3)
                              
                                 
                                    Dipeptide
                                     
                                    Composition
                                     
                                    (
                                    i
                                    )
                                    =
                                    
                                       
                                          Total
                                           
                                          number
                                           
                                          of
                                           
                                          D
                                          P
                                          (
                                          i
                                          )
                                       
                                       
                                          Total
                                           
                                          number
                                           
                                          of
                                           
                                          all
                                           
                                          D
                                          P
                                       
                                    
                                 
                              
                           
                        where Dipeptide Composition (i) denotes the fraction of ith dipeptide out of 400 dipeptides.

A protein sequence is a combination of 20 amino acids [53]. These 20 amino acids are arranged differently forming a proteins sequence. Thus, a protein sequence is represented with 20 discrete values which is the normalized occurrence frequency of these 20 native amino acids in proteins [42,53]. The AAC of 20 amino acids can be expressed in 20-D vector.
                           
                              (4)
                              
                                 
                                    X
                                    
                                       i
                                    
                                    =
                                    
                                       
                                          n
                                          (
                                          i
                                          )
                                       
                                       L
                                    
                                 
                              
                           
                        where i
                        =1, 2, 3, …, 20. Here n is the occurrence frequency of amino acid i, and L is the length of protein sequence. It can be formulated as [54,55]:
                           
                              (5)
                              
                                 
                                    P
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   p
                                                   1
                                                
                                                ,
                                                
                                                   p
                                                   2
                                                
                                                ,
                                                
                                                   p
                                                   3
                                                
                                                ,
                                                …
                                                ,
                                                
                                                   p
                                                   
                                                      20
                                                   
                                                
                                             
                                          
                                       
                                       T
                                    
                                 
                              
                           
                        where p
                        1, represents the frequency of A, p
                        2 is the frequency of C and so on and T is the transpose. However, AAC only calculates the frequency of amino acids, it does not preserve sequence order information and sequence length. To avoid this issue, the concept of PseAAC was introduced by Chou [56]. In PseAAC concept, the amino acids are represented in (20+
                        λ). The concept of PseAAC has been widely used in the fields of bioinformatics, proteomics and system biology [56,57]. The mathematical expression of PseAAC is as [32,42,53,56]:
                           
                              (6)
                              
                                 
                                    P
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   p
                                                   1
                                                
                                                ,
                                                
                                                   p
                                                   2
                                                
                                                ,
                                                
                                                   p
                                                   3
                                                
                                                ,
                                                …
                                                ,
                                                
                                                   p
                                                   
                                                      20
                                                   
                                                
                                                ,
                                                
                                                   p
                                                   
                                                      20
                                                      +
                                                      1
                                                   
                                                
                                                ,
                                                …
                                                ,
                                                
                                                   p
                                                   
                                                      20
                                                      +
                                                      l
                                                   
                                                
                                             
                                          
                                       
                                       T
                                    
                                 
                              
                           
                        where p
                        1, p
                        2, p
                        3, …, p
                        20 represents the relative occurrence frequency of 20 amino acids and the rest are the different correlation factors of amino acids. These correlation factors are determined on the basis of hydrophobicity, hydrophilicity, charge and polarity [42,58]. In the current study, λ
                        =10 for both J-protein types and Heat Shock Proteins.

Split Amino Acid Composition (SAAC) is the successor of amino acid composition. It has been successfully used by researchers for the prediction of proteins function [59]. In SAAC, information is extract from fragments i.e. the protein sequence is decomposed into various parts and composition of each part is computed separately [60,61]. In this work, the protein sequence is divided into three parts: (i) 15 amino acids of N termini, (ii) 15 amino acids of C termini, and (iii) region between these two terminuses. The resultant feature vector is a 60D instead of 20D as in case of AAC [61]. The feature vector of SAAC is represented as [59]:
                           
                              (7)
                              
                                 
                                    P
                                    =
                                    
                                       
                                          
                                             f
                                             1
                                             N
                                          
                                          ,
                                          …
                                          ,
                                          
                                             f
                                             
                                                20
                                             
                                             N
                                          
                                          ,
                                          
                                             f
                                             1
                                             
                                                int
                                             
                                          
                                          ,
                                          …
                                          ,
                                          
                                             f
                                             
                                                20
                                             
                                             
                                                int
                                             
                                          
                                          ,
                                          
                                             f
                                             1
                                             C
                                          
                                          ,
                                          …
                                          ,
                                          
                                             f
                                             
                                                20
                                             
                                             C
                                          
                                       
                                    
                                 
                              
                           
                        where N, int and C represents the N-terminus, integral segment, C-terminus, respectively.

K-nearest neighbor (KNN) is the most popular classification algorithm also called Lazy learning algorithm [62,63]. This algorithm is mostly applied in data mining, image processing, speech recognition, statistics, machine learning and bioinformatics [62,64,65]. KNN has achieved substantial performance compared to many other learning algorithms [42]. It requires less time to train the model while it takes more time to test the model. KNN is substantial for self-motivated data that changes and updates rapidly [66,67]. The distance of nearest neighbor is calculated by using the Euclidean Distance Formula:
                              
                                 (8)
                                 
                                    
                                       
                                          E
                                          
                                             d
                                             i
                                             s
                                          
                                       
                                       (
                                       
                                          x
                                          1
                                       
                                       ,
                                       
                                          x
                                          2
                                       
                                       )
                                       =
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          n
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      (
                                                      
                                                         x
                                                         
                                                            i
                                                            1
                                                         
                                                      
                                                      −
                                                      
                                                         x
                                                         
                                                            i
                                                            2
                                                         
                                                      
                                                      )
                                                   
                                                   2
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

Euclidean sort the value as Di
                           ≤
                           Di
                           +1, where i
                           =1, 2, 3, …, N. KNN applies mean or voting on the data according to the nature of data. The number of nearest neighbor or k value large when the data is quite large and keep it small if the data is limited [68].

Probabilistic neural network (PNN) is a feed forward neural network, which was introduced by Specht in the early 1990s [34,69]. PNN is based on Bayes theory and uses the radial basis function as kernel [42]. It interprets the network structure in the form of probability density function and usually performs better compared to other neural networks [42]. Practical advantage of the PNN is that, unlike many other neural networks, it operates completely in parallel without a need of feedback from the individual neurons to the inputs. In PNN, the operations are categorized into four layers namely input layer, pattern layer, summation layer and output layer as shown in Fig. 1
                           .

The response to an input pattern is processed from one layer to the next, without feedback paths to previous layers. The input units supply the same values to all pattern units. The pattern units form a dot product between the input pattern vector x and a weight vector w
                           
                              i
                            (z
                           
                              i
                           
                           =
                           xw
                           
                              i
                           ), which is followed by the non-linear neuron activation function:
                              
                                 (9)
                                 
                                    
                                       g
                                       (
                                       
                                          z
                                          i
                                       
                                       )
                                       =
                                       exp
                                       
                                          
                                             −
                                             
                                                
                                                   
                                                      
                                                         (
                                                         
                                                            w
                                                            i
                                                         
                                                         −
                                                         
                                                            x
                                                            i
                                                         
                                                         )
                                                      
                                                      ′
                                                   
                                                   (
                                                   
                                                      w
                                                      i
                                                   
                                                   −
                                                   
                                                      x
                                                      i
                                                   
                                                   )
                                                
                                                
                                                   2
                                                   
                                                      ∂
                                                      2
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

The summation units simply sum the inputs from the pattern units, corresponding to the category from which the training patterns were selected. Repeating this procedure for each class, the un-normalized density functions g k(x), for k
                           =1, 2, y, k were estimated. The Bayesian probability that the case was from class k is as follows:
                              
                                 (10)
                                 
                                    
                                       P
                                       (
                                       X
                                       ∈
                                       k
                                       )
                                       =
                                       
                                          
                                             g
                                             i
                                             (
                                             x
                                             )
                                          
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                k
                                             
                                             
                                                g
                                                i
                                                (
                                                x
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

The output units have a competitive transfer function that picks the maximum of the probabilities and produces 1 for one class and 0 for the other [70–72]. PNN has the ability to train on scarce datasets. Moreover, it has the ability to classify data into specific categories [73]. Faster computational time and robust to noise is the key advantage of the PNN compared to other classifiers.

SVM is the most popular and extensively used learning algorithm in the area of Machine learning, pattern recognition and bioinformatics [29,74–76]. It is based on statistical theory. It was first introduced by Cortes and Vapnik in 1995 [29,74]. Later on, it was modified in 1999 [76]. Initially, SVM was proposed for binary classification problems but later it was extended to classify multiclass problems. To deal with multiclass problem different variations, “one verses one” (OVO) and “one verses rest” (OVR), is applied to the traditional SVM [76,77]. It transfers the data to a high dimensional feature space and seeking a hyperplane, which separates the positive and negative instances [2,74,78]. For the optimization of SVM model, various types of kernel functions like linear, polynomial, RBF and sigmoid are used. In this study, the ‘OVR’ strategy was employed using radial base function (RBF) as kernel [28,31]. The regularization parameter C and the kernel width parameter γ were determined via an optimization procedure using a grid search approach, and the actual values obtained in this way for the current study were C= and γ= for Heat Shock Proteins and C
                           =3 and γ
                           =0.005 for J-proteins types.
                              
                                 (11)
                                 
                                    
                                       K
                                       (
                                       
                                          x
                                          i
                                       
                                       ,
                                       
                                          x
                                          j
                                       
                                       )
                                       =
                                       exp
                                       
                                          
                                             −
                                             γ
                                             
                                                
                                                   
                                                      
                                                         
                                                            x
                                                            i
                                                         
                                                         −
                                                         
                                                            x
                                                            j
                                                         
                                                      
                                                   
                                                
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

The parameter γ is the width of RBF function and the values for RBF is calculated via a grid search approach. This software is available free for download at http://www.csie.ntu.edu.tw/∼cjlin/libsvm.

Multi-layer perceptron (MLP) is a feed forward artificial neural network that accepts a set of input values and produces a set of appropriate output [42]. MLP utilizes the back propagation supervised technique for training the model that enables the system to classify the non-linear separable data [42]. MLP consists of two or more input layer, output layer and one or more hidden layer. Every node of layer has its own weight w
                           
                              ij
                            
                           [79].

Looking at the importance of HSPs and J-proteins Types, a high throughput computational model is indispensable. In this regards, a quite promising and robust computational model is developed to classify families of HSPs and J-protein types with high precision. In this model, three powerful feature extraction methods namely: Split Amino Acid Composition (SAAC), Pseudo Amino Acid Composition (PseAAC) and Dipeptide Composition were used to extract valuable features from the datasets S
                           HSP and S
                           JP. The extracted features were provided to four different classification algorithms including KNN, PNN, SVM and MLP in order to select the best classification algorithm for proposed model. Jackknife test was applied to evaluate the prediction performance of the classification algorithms [51]. Our proposed model yielded higher performance compared to the existing methods in the literature so far. The framework of the proposed model has been illustrated in Fig. 2
                           .

The prediction performance of various learning algorithms is assessed using different performance measures. These performance measures are based on both the correct values and incorrect values. Therefore, during the classification process both correct and incorrect values are stored with the help of confusion matrix. Mostly accuracy is used as a performance measure, but sometime accuracy is not sufficient. Therefore, we have used several performance measures, which are listed below [2,25–27,31,65,80]:
                           
                              i
                              Accuracy:
                                    
                                       (12)
                                       
                                          
                                             Accuracy
                                             =
                                             
                                                
                                                   T
                                                   P
                                                   +
                                                   T
                                                   N
                                                
                                                
                                                   T
                                                   P
                                                   +
                                                   F
                                                   P
                                                   +
                                                   F
                                                   N
                                                   +
                                                   T
                                                   N
                                                
                                             
                                             ×
                                             100
                                          
                                       
                                    
                                 
                              

Sensitivity:
                                    
                                       (13)
                                       
                                          
                                             Sensitivity
                                             =
                                             
                                                
                                                   T
                                                   P
                                                
                                                
                                                   T
                                                   P
                                                   +
                                                   F
                                                   N
                                                
                                             
                                             ×
                                             100
                                          
                                       
                                    
                                 
                              

Specificity:
                                    
                                       (14)
                                       
                                          
                                             Specificity
                                             =
                                             
                                                
                                                   T
                                                   N
                                                
                                                
                                                   F
                                                   P
                                                   +
                                                   T
                                                   N
                                                
                                             
                                             ×
                                             100
                                          
                                       
                                    
                                 
                              

Mathews Correlation Coefficient (MCC):
                                    
                                       (15)
                                       
                                          
                                             MCC
                                             (
                                             i
                                             )
                                             =
                                             
                                                
                                                   T
                                                   P
                                                   ×
                                                   T
                                                   N
                                                   −
                                                   F
                                                   P
                                                   ×
                                                   F
                                                   N
                                                
                                                
                                                   
                                                      
                                                         [
                                                         T
                                                         P
                                                         +
                                                         F
                                                         P
                                                         ]
                                                         [
                                                         T
                                                         P
                                                         +
                                                         F
                                                         N
                                                         ]
                                                         [
                                                         T
                                                         N
                                                         +
                                                         F
                                                         P
                                                         ]
                                                         [
                                                         T
                                                         N
                                                         +
                                                         F
                                                         N
                                                         ]
                                                      
                                                   
                                                
                                             
                                             ×
                                             100
                                          
                                       
                                    
                                 
                              

F-measure:
                                    
                                       (16)
                                       
                                          
                                             F
                                             -measure
                                             =
                                             2
                                             ×
                                             
                                                
                                                   Precision
                                                   ×
                                                   Recall
                                                
                                                
                                                   Precision
                                                   +
                                                   Recall
                                                
                                             
                                          
                                       
                                    
                                 
                              

It should be noted that the four metrics as formulated in Eqs. (12)–(15) are not quite intuitive and easier-to-understand, particularly the Mathew's correlation coefficient. To avoid this problem, we have adopted the formula proposed in recent publication [23,25,27,30,81] which is based on the Chou's symbol and definition [82].
                           
                              (17)
                              
                                 
                                    Sn
                                    =
                                    1
                                    −
                                    
                                       
                                          
                                             N
                                             −
                                             +
                                          
                                       
                                       
                                          
                                             N
                                             +
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (18)
                              
                                 
                                    Sp
                                    =
                                    1
                                    −
                                    
                                       
                                          
                                             N
                                             +
                                             −
                                          
                                       
                                       
                                          
                                             N
                                             −
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (19)
                              
                                 
                                    Acc
                                    =
                                    1
                                    −
                                    
                                       
                                          
                                             N
                                             −
                                             +
                                          
                                          +
                                          
                                             N
                                             +
                                             −
                                          
                                       
                                       
                                          
                                             N
                                             +
                                          
                                          +
                                          
                                             N
                                             −
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (20)
                              
                                 
                                    MCC
                                    =
                                    
                                       
                                          1
                                          −
                                          
                                             
                                                
                                                   
                                                      
                                                         N
                                                         −
                                                         +
                                                      
                                                      +
                                                      
                                                         N
                                                         +
                                                         −
                                                      
                                                   
                                                   
                                                      
                                                         N
                                                         +
                                                      
                                                      +
                                                      
                                                         N
                                                         −
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            1
                                                            +
                                                            
                                                               
                                                                  
                                                                     N
                                                                     +
                                                                     −
                                                                  
                                                                  −
                                                                  
                                                                     N
                                                                     −
                                                                     +
                                                                  
                                                               
                                                               
                                                                  
                                                                     N
                                                                     +
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      1
                                                      +
                                                      
                                                         
                                                            
                                                               N
                                                               −
                                                               +
                                                            
                                                            −
                                                            
                                                               N
                                                               +
                                                               −
                                                            
                                                         
                                                         
                                                            
                                                               N
                                                               −
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

It should be noted that the metrics given in Eq. (12)–(15) and Eq. (17)–(20) are valid only for the single-label system as in the current system. For the multi-label system whose existence has become more frequent in system biology [83], and system medicine [84], a completely different set of metrics is used as defined in [85].

@&#RESULTS AND DISCUSSIONS@&#

To examine, the quality and effectiveness of a predictor, there are three most popular cross-validation methods namely independent dataset test, sub-sampling test, and jackknife test are used in bioinformatics [86]. Among these tests, jackknife test is deemed to be the most effective and objective as it always produces a unique result for a given dataset [28,51]. Accordingly, jackknife test was applied to assess the prediction performance of the proposed method. The prediction performance of various classification algorithms using three protein sequence representation methods; Dipeptide Composition, PseAAC and SAAC for two benchmark datasets i.e. S
                     HSP and S
                     JP is described below.

The success rates of classification algorithms using Dipeptide Composition feature space are listed in Table 3
                         for S
                        HSP. In case of S
                        HSP dataset, SVM obtained the highest success rates of 90.7% accuracy, 92.43% sensitivity, 86.85% specificity, 0.76 MCC and 0.73 F-measure, among various classification algorithms. Similarly, MLP obtained the second highest results of 84.41% accuracy, 84.41% sensitivity, 96.30% specificity, 0.73 MCC and 0.70 F-measure. On the other hand, PNN and KNN yielded relatively similar results. The prediction performance of classification algorithms on S
                        JP is reported in Table 4
                        . On S
                        JP dataset, SVM still obtained quite promising results compared to other using classification algorithms. SVM yielded 97.01% accuracy, 85.70% sensitivity, 98% specificity, 0.80 MCC and 0.80 F-measure. Likewise, PNN achieved the second highest results of 86.70% accuracy, 76.45% sensitivity, 86.70% specificity, 0.64 MCC and 0.86 F-measure. In contrast, the accuracies of MLP and KNN are comparatively similar but the specificity of MLP is worse compared to KNN.

The success rates of classification algorithms using PseAAC feature space is given in Tables 5 and 6
                        
                         on S
                        HSP and S
                        JP datasets, respectively. Using benchmark dataset of S
                        HSP, MLP obtained the highest success rates of 79.68% accuracy, 87.52% sensitivity, 79.43% specificity, 0.68 MCC and 0.79 F-measure, among various classification algorithms. Whereas KNN obtained the second highest results of 78.96% accuracy, 76.75% sensitivity, 78.75% specificity, 0.44 MCC and 0.53 F-measure. Similarly, PNN yielded 76.04% accuracy, 75.63% sensitivity, 74.24% specificity, 0.39 MCC and 0.48 F-measure. On the other hand, the performance of SVM is recorded worse compared to other used classification algorithms. Whereas in case of S
                        JP dataset still MLP obtained quite good results compared to other classification algorithms. The obtained results of MLP are 97.75% accuracy, 87.33% sensitivity, 91.90% specificity, 0.78 MCC and 0.92 F-measure. Likewise, PNN achieved the second highest results of 84.36% accuracy, 72.60% sensitivity, 86.70% specificity, 0.64 MCC and 0.86 F-measure, while the performance of SVM and KNN is considerable.

The prediction outcomes of various classification algorithms using SAAC feature space are shown in Tables 7 and 8
                        
                         on the datasets of S
                        HSP and S
                        JP, respectively. Using S
                        HSP benchmark dataset, SVM obtained the highest success rates of 81.61% accuracy, 79.83% sensitivity, 82.0% specificity, 0.50 MCC and 0.58 F-measure, among various classification algorithms. Similarly, KNN obtained second highest results of 79.01% accuracy, 77.31% sensitivity, 78.54% specificity, 0.44 MCC and 0.53 F-measure. Whereas, the performance of MLP is 77.57% accuracy, 85.91% sensitivity, 77.60% specificity, 0.64 MCC and 0.77 F-measure. In contrast, the success rates of PNN are 77.0% accuracy, 80.67% sensitivity, 74.40% specificity, 0.42 MCC and 0.51. In case of S
                        JP dataset, still SVM obtained enhanced results compared to other classification methods. It performance is 95.02% accuracy, 85.01% sensitivity, 95.85% specificity, 0.69 MCC and 0.71 F-measure. Whereas, KNN achieved the results of 91.25% accuracy, 95.01% sensitivity, 90.95% specificity, 0.61 MCC and 0.60 F-measure. Likewise, MLP achieved an accuracy of 90.97%, sensitivity of 84.90%, specificity of 91.10%, MCC of 0.76, and F-measure of 0.91. On the other hand, PNN obtained worst results of 87.56% accuracy, 77.01% sensitivity, 87.61% specificity, 0.65 MCC and 0.87 F-measure among using classification methods. The empirical results revealed that the performance of Dipeptide Composition feature space is enhanced compared to PseAAC and SAAC feature spaces. Its mean that Dipeptide Composition has clearly defined the motif of proteins and extracts salient features from protein sequences. Among classification algorithms SVM achieved quite promising results compared to other classification algorithms.

Performance comparison of proposed method with the existing methods in the literature has been drawn in Tables 9 and 10
                        
                         for S
                        HSP and S
                        JP datasets, respectively. The pioneer work on dataset S
                        HSP has been carried out by Peng et al. [2]. The predicted outcomes of their predictor iHSP-PseRAAAC are 87.8% accuracy, 75.02% sensitivity, 94.40% specificity and 0.70 MCC. In contrast, our proposed method has achieved 91.7% accuracy, 92.43% sensitivity, 86.85% specificity and 0.76 MCC. As results, the success rates of our proposed model are 2.87% accuracy and 0.6 MCC higher than that of Peng et al. model. Similarly, in case of S
                        JP dataset, the pioneer work has also been performed by Peng et al. [16]. The predictor outcome of Feng et al. is 94.06% accuracy, 63.50% sensitivity, 88.55% specificity and 0.59 MCC. In contrast, the success rates of our proposed method are 97.01% accuracy, 85.70% sensitivity, 98.01% specificity and 0.79 MCC. So, the performance of our proposed model is higher than that of Peng et al. model in all performance measures and so far in the literature. This achievement is ascribed with extraction of valuable and salient features through Dipeptide Composition from protein sequences and the best classification algorithm SVM. Dipeptide Composition considers the global information about protein sequences. Whereas SVM has more discrimination power because it converts low dimension space into high dimension space and draw a hyperplane to separate the classes. SVM training process always seeks a global optimized solution and avoids over-fitting, because the most remarkable characteristics of SVM are; the absence of local minima, and the sparseness of the solution. So it has the ability to deal with a large number of features. As demonstrated in a series of recent publications [2,24–32], in developing new prediction methods, user-friendly and publicly accessible web-servers are more significant to enhance the impact of the proposed predictor [37]. In future work, we will make efforts to provide a web-server for the prediction of Heat Shock Proteins and J-proteins types.

@&#CONCLUSION@&#

In this study, a reliable, robust and high throughput computational model has been developed for the identification of HSPs families and J-protein types. The proposed method used three discrete sample representation methods including Dipeptide Composition, Pseudo Amino Acid Composition and Split Amino Acid Composition to extract numerical values from protein sequences. Various classification algorithms namely: KNN, PNN, SVM and MLP were utilized to find out the best classification algorithm among these algorithms for the identification of HSPs families and J-protein types. The discrimination power and quality of the proposed method was investigated using two datasets i.e. one for HSPs and second for J-protein types. The empirical results revealed that, SVM achieved highest performance in conjunction with Dipeptide Composition on both SSHP and S
                     JP, respectively. The success rates of SVM are 90.7% accuracy on SSHP dataset and 97.01% accuracy on S
                     JP dataset. It is ascertained that our proposed model might be useful for academia and research community.

The authors declare that they have no conflict of interest.

@&#REFERENCES@&#

