@&#MAIN-TITLE@&#How different types of users develop trust in technology: A qualitative analysis of the antecedents of active and passive user trust in a shared technology

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           This paper investigated antecedents of trust in technology for active/passive users.


                        
                        
                           
                           A list of system features that influenced trust was derived from qualitative analysis.


                        
                        
                           
                           Different antecedents of trust related to trust/distrust differently.


                        
                        
                           
                           Active/passive users evaluated trust according to similar sets of system features.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Passive user

Trust in technology

Shared technology

Team

@&#ABSTRACT@&#


               
               
                  The aim of this study was to investigate the antecedents of trust in technology for active users and passive users working with a shared technology. According to the prominence-interpretation theory, to assess the trustworthiness of a technology, a person must first perceive and evaluate elements of the system that includes the technology. An experimental study was conducted with 54 participants who worked in two-person teams in a multi-task environment with a shared technology. Trust in technology was measured using a trust in technology questionnaire and antecedents of trust were elicited using an open-ended question. A list of antecedents of trust in technology was derived using qualitative analysis techniques. The following categories emerged from the antecedent: technology factors, user factors, and task factors. Similarities and differences between active users and passive user responses, in terms of trust in technology were discussed.
               
            

@&#INTRODUCTION@&#

Trust is a fundamental factor in all relationships (Montague, 2010). In social-technical systems, there are three types of trust that are critical for optimal system outcomes: interpersonal trust, trust between two or more people (Larzelere and Huston, 1980), institutional trust, a person's trust with an organization (Castelfranchi and Falcone, 2001), and technological trust, a person's trust with a technology or device (Muir, 1987). Specifically, trust in technology is “the attitude that an agent will help achieve an individual's goals in a situation characterized by uncertainty and vulnerability” (Lee and See, 2004). Research in trust in technology continues to grow in the field of Human Factors and Ergonomics as it plays a vital role in human–technology interaction (Madhavan and Wiegmann, 2007; Parasuraman and Wickens, 2008).

Previous research has found that a user's level of trust in technology influences the user's strategy towards the use of the technology (Bagheri and Jamieson, 2004; Lee and Moray, 1994; Muir, 1987). Inappropriate trust in technology can potentially lead to misuse and disuse of the technology (Parasuraman and Riley, 1997). Over trust of technology often results in misuse of technology that leads to complications and errors in the work system. On the other hand, lack of trust in technology prevents the user from utilizing the system to its full extent, and can lead to a decrease in productivity. Recently, there have been efforts to integrate the concept of trust in technology with the technology acceptance model (Ghazizadeh et al., 2012; Pavlou, 2003; Wang and Benbasat, 2005) to predict a user's intent or behavior to adopt technologies. In addition, an individual's trust in technology may also influence his/her trust in other elements of the system, such as interpersonal trust and institutional trust (Muir, 1994). This issue is critical for industries such as healthcare (Montague and Lee, 2012) and e-commerce (Lee and Turban, 2001) where interpersonal trust and institutional trust are important.

Human factors researchers have investigated the means for trust calibration (Lee and See, 2004) and how users develop an appropriate level of trust toward the technology. In order to develop effective means for trust calibration, one needs to first identify the antecedents of trust in technology. Researchers have identified a wide variety of factors that influence an individual user's level of trust in technology. Among those factors, reliability of the technology was widely cited in trust in automation studies (Bisantz and Seong, 2001; Lee and Moray, 1992; Lewandowsky et al., 2000; Madhavan et al., 2006). Factors related to interface design were also identified, such as etiquette (Cassell and Bickmore, 2000; Parasuraman and Miller, 2004), usability (Corritore et al., 2003; Koufaris and Hampton-Sosa, 2002), social presence (Hassanein and Head, 2004), and visual design (Fogg et al., 2003; Kim and Moon, 1998; Weinstock et al., 2012). Furthermore, factors related to individual difference, such as age (Sanchez et al., 2004) and propensity to trust technology (Merritt and Ilgen, 2008), were also investigated.

The concept of trust in technology has been previously explored by researchers in work on trust between users and automation (Lee and See, 2004), information technology (Marsh and Dibben, 2005), and the world wide web (Egger, 2001; Wang and Emurian, 2005). However, the majority of current research on trust in technology focuses on situations that involve an individual user's interactions with a technology. When multiple people or groups use a technology, trust in technology could be a factor that influences how the technology is used and collaboration between group members. For example, whether or not the team trusts the technology appropriately can affect overall team performance (Bowers et al., 1996).

Trust in technology at the group level is especially relevant for multi-user shared technologies, such as health technologies shared by physicians, nurses, and patients, or interactive interfaces shared by customer service representatives and customers, or robots used by a military team. Under such shared technology scenarios, the users of the technology sometimes act as active users, who have direct control over the technology, or passive users, who do not have direct control but interact with both the technology and the active users (Inbar and Tractinsky, 2009, 2012; Montague and Xu, 2012; Xu and Montague, 2012). A related concept discussed in the use of technology on the individual level is supervisory control (Sheridan, 2002). Furthermore, if the primary task of the user is monitoring task, then the user is considered to be a passive process operator (Persson et al., 2001). This role of the individual user is characterized by indirect control of task process through automated systems. At the group level the role of individual users can be further differentiated where a passive user of a shared technology is characterized by indirect control of the technology through the active user. For example, in a face-to-face customer service encounter, the customer service representative plays the active user role and the customer plays the passive user role (Inbar and Tractinsky, 2010). The case is similar for the patient and the clinician's roles with regards to the use of computers in a clinical encounter (Montague and Xu, 2012). In other circumstances, an individual could be an active user for some aspects of the shared technology while a passive user to other aspects. An example of this could be the roles of two pilots in a commercial aircraft cockpit. Also the active/passive user roles may switch among individuals during the interaction process; collaboration in robotic surgeries (Hanly et al., 2006) could be an example of such cases.

Previous research has found that active users and passive users have different ways of calibrating trust. Specifically, active users' trust is influenced primarily by trust in the co-user and by direct interaction with the technology, while passive users' trust is influenced by the communication with the active users (Montague and Xu, 2012). However, little research had been conducted to investigate antecedents of trust in technology in the use of a shared technology. It is unclear that previous identified antecedents of trust in technology in single-user scenarios can be applied to multi-user scenarios. Thus, further research is needed to better understand trust in technology from differing perspectives in multi-user systems.

As previously reviewed, there are a variety of factors that could potentially influence a user's trust in technology. In the human–computer interaction domain, the prominence-interpretation theory proposed by Fogg (2003) could provide a framework for understanding how different factors affect user's trust. As shown in Fig. 1
                        , the process of trust calibration involves two elements, namely prominence and interpretation. Prominence refers to the likelihood of a specific system element being perceived by a user. Interpretation refers to how a user evaluates the system element in terms of trust. The overall trust of the user towards the technology is the combined effect of the factors that are perceived by the user and the user's corresponding evaluation of the system factors. The prominence and interpretation are related to subjective perceptions of the user about the technology, and these perceptions are influenced by objective factors, such as the task being performed, user expertise, and individual differences, etc. (Fogg, 2003). So the collection of system elements that could potentially affect a user's trust in technology becomes a pool of potential antecedents of trust in technology. However, these system elements have to be perceived and evaluated by the user in order to have impact on the user's trust. In addition, the users' roles in the group (whether active or passive user) may affect the users' prominence as well as interpretation of system elements since different users interact with the technology in different ways.

The purpose of this experimental study was to understand the antecedents of users' trust in technology in a multi-user system involving active users and passive users. First, as a comparison of general levels of trust in technology of the active user and passive user, a quantitative scale for trust in technology was used in the experiment to answer the following question:
                           
                              (a)
                              
                                 What is the effect of being an active user or a passive user of a shared technology under varied technological/task conditions on the ratings of trust in technology?
                              

To further understand the antecedents of trust in technology in such a setting, qualitative data was collected and analyzed. Prominence and interpretation related to a user's trust in technology was investigated through open ended questions about the factors (prominence) that led the user to trust/distrust (interpretation) the technology. The following research questions were addressed:
                           
                              (b1)
                              
                                 What are the antecedents of trust in technology reported by the users?
                              


                                 What are the similarities and differences in reported antecedents of trust in technology from the active users and the passive users?
                              


                                 How do the technological/task conditions influence the type of antecedents of trust in technology reported by the user?
                              

Finally, the quantitative data and qualitative data were integrated to answer the following research question:
                           
                              (c)
                              
                                 What is the relationship between the rating of trust in technology and the reported antecedents of trust in technology?
                              

@&#METHOD@&#

Participants of the study were recruited from a human factors introductory course at a large Midwestern university in the US through voluntary sign up. The sample size was 54 in 27 two-person teams. 36 participants (66.7%) were male. The age of the participants ranged from 19 to 29, with the average of 21.6 (SD = 1.6). As compensation for participating in the study, participants were given extra credit points in a specific course. As an incentive for the study, there was a $20 reward for each of the team members in the best performing team in this study.

A modified version of the Multi Attribute Task Battery (MATB) program (Comstock and Arnegard, 1992) was used in this study. The participants had to perform three tasks simultaneously in MATB: the monitoring task, the tracking task, and the resource management task. The monitoring task required the participants to respond to two lights and fluctuations of four dials as quickly as possible. The tracking task required the participants to keep a moving target in the center of the screen using a joystick. The resource management task required the participants to control eight pumps to maintain optimum liquid level in two tanks.

@&#EXPERIMENTAL DESIGN@&#

Participants worked as two-person teams in the experiment. Teams consisted of one passive user and one active user. These roles were randomly assigned. The active user had full access to the control devices of the computer, including the keyboard and the joystick. The passive user did not have access to any of the control devices but he/she could monitor the tasks and communicate with the active user to assist with the task. The participants were instructed that all of the responsibilities of the task, such as decision making and planning, except physical control, would be shared equally between the two team members.

Each of the teams went through three technological/task conditions in three trials in the experiment–normal condition (N), hard condition (H), and low reliability condition (L). The normal condition was characterized by moderate task difficulty and high technology reliability. The hard condition was characterized by increased difficulty in the monitoring and tracking tasks, compared to the normal condition. In the monitoring task, a more frequent response was required for both the lights and the fluctuation of the dials. In the tracking task, the random movement of the target was increased in both speed and direction change. The low reliability condition was characterized by low reliability of the technology being used – some of the pumps in the resource management task were out of control and it was not possible to maintain optimum liquid level in one of the tanks. The difficulty levels of the other two sub-tasks in this condition were the same as those found in the normal condition. Each trial took the participants six minutes to complete. To prevent order effects related to how the user completed the task, the conditions were counter-balanced across teams with different orders: N-H-L, H-L-N, L-N-H, N-L-H, H-N-L, and L-H-N.

@&#PROCEDURE@&#

Once both team members arrived to participate in the experiment, the experiment process was briefly explained. The participants were told that the goal of the experiment was to investigate performance in teams with shared technology. Informed consents were obtained from the participants. The role of active and passive user was randomly assigned with a brief explanation of both. The active user had full access to the control devices of the computer, including a keyboard and a joystick. The passive user did not have access to any of the control devices but he/she could monitor the tasks and communicate with the active user to assist with the task. The participants were told that though they were in different roles, they would share the responsibility for performance. Instruction documents about the MATB and the task were given to the participants to read. The documents included instructions about how to control the computer and the MATB, and the goal of each task in MATB. Next, the program was displayed on the computer and the tasks were explained again orally by the experimenter. Then the participants went through a six-minute individual training session with the program using separate computers.

After the training had been completed, the first trial of the task began. The two participants in a team sat next to each other and shared a computer screen to perform the tasks. There were three trials in total. Each trial terminated automatically after six minutes. After each trial, both the active and passive user completed questionnaires separately. After all three task trials had been completed, the participants were given a debriefing statement about the background and purpose of the study.

Questionnaires were given after each task trial in order to gather participants' subjective experiences of the technology. A trust in technology scale (Jian et al., 2000) was used for measuring trust quantitatively and an open ended question was used for eliciting why the participants trust or distrust the technology qualitatively. The question was phrased as follows: “Do you trust the technology to perform the task? Why or why not?” At the end of the experiment, the participants were asked to fill out a questionnaire which included demographic information questions and a 14-item task motivation scale. The task motivation scale was derived from the Dundee Stress State Questionnaire (Matthews et al., 2002).

For the quantitative data analysis, all data analyses were conducted using R (R Development Core Team, 2011) with various packages, including nlme (Pinheiro and Bates, 2009), MASS (Venables and Ripley, 2002), and ggplot2 (Wickham, 2009). Specific statistical techniques used will be described in the results section.

From the open ended question, specific antecedents of trust (prominence) and binary trust/distrust (interpretation) could be derived. An approach similar to grounded theory was used to derive the antecedents of trust (Corbin and Strauss, 2008; Glaser and Strauss, 1967). There were three stages in the process. First, two researchers (AD and KL) open coded 10% of the data line-by-line individually. Second, the two coding researchers and one additional researcher (JX) met and discussed the coding and came to a consensus about the final codes to be used. After the meeting AD and KL continued and coded another 45% of the data. Then AD, KL, and JX met again and discussed the coding and achieved consensus about all of the codes. This process was repeated for the remaining 45% of the data. Definitions and descriptions of the codes were developed. Finally, the three researchers organized the codes into different categories as they represented issues in different elements of the system, such as the technology element, the user element, and the task element. Saturation was considered to be achieved in the analysis that no new codes emerged for the last 45% of the data.

@&#RESULTS@&#

Manipulation check for the role assignments of the participants in a team was performed directly through observation and indirectly through questionnaire. By examining the video recording of the experiment, it was confirmed that the active users did not turn over the controls to the passive users. In addition, all of the passive users assisted the active user through verbal communication as expected. Paired t-test was performed for the motivation ratings to compare the different users' perceived motivation during the experiment. The results indicated that the difference between the active users and the passive users was not significant (t(26) = 0.39, p = 0.70). This implied that there was no systematic difference in task engagement between the two types of users.

Performance measures were used as manipulation check for the technological/task conditions. Linear mixed effect (LME) models were fitted to the data to examine if the teams' performances in different tasks were consistent with the expected results of the manipulations. The mean reaction times in the monitoring task in conditions N, H, and L were 3.50 s, 3.62 s, and 3.64 s, respectively. The differences among these means were not significant (F(2, 52) = 0.55, p = 0.58) as expected. The means of the root mean square error (RMSE) in the tracking task in condition N, H, and L were 94.27, 127.90, and 90.94, respectively. The differences were significant (F(2, 52) = 70.75, p < 0.00) that the performance in the H condition was lower than both the N condition (t(52) = 9.78, p < 0.00) and the L condition (t(52) = 10.75, p < 0.00), as expected. The means of the absolute deviation in the resource management task in condition N, H, and L were 643.60, 786.80, and 2542.00, respectively. The differences were significant (F(2, 52) = 148.32, p < 0.00) such that the performance in the L condition was lower than both the N condition (t(52) = 15.46, p < 0.00) and the H condition (t(52) = 14.30, p < 0.00), as expected.

Practice effect was found in the performance for the monitoring task (F(2, 52) = 40.82, p < 0.00) such that the mean reaction times significantly decreased over time. No practice effect was found for the tracking task and the resource management task. Further investigation showed that the task sequence effect was not found for trust in technology ratings (F(2, 103) = 1.05, p = 0.35). In addition, trust in technology ratings were not significantly correlated with reaction times in the monitoring task (F(1, 103) = 0.09, p = 0.78).

Technological/task conditions, roles, and technological/task conditions X roles interaction were entered into a linear mixed effect (LME) model to predict average trust in technology rating. Participants nested in teams were entered as a random intercept. Significant main effects for technological/task conditions were found (F(2, 104) = 11.85, p < 0.00). Specifically, mean average trust in technology ratings was significantly lower in the low reliability condition than that in the normal condition (t(104) = 2.49, p = 0.01). No significant effects were found for roles and technological/task conditions X roles interaction. The results are visualized in Fig. 2
                        .

The open ended question regarding why the participants did or did not trust the technology was analyzed qualitatively. A total number of 16 codes emerged from the data. These codes were labeled as first level factors. The first level factors were further organized into six second level factors according to their similarities. Finally, the second level factors were organized into three third level factors to represent different aspects of a system. The three third level factors included technology, user, and task factors. The technology factor focused on the computer being used. This involved three second level factors, including usability, competence, and appearance/aesthetics of the technology. The user factor included the participant's personality and confidence in using the technology. The task factor included the tasks being performed, emphasizing what the demands of the tasks were, as well as the outcomes of the tasks. Table 1
                            also shows the definitions of the codes and how they were categorized into different levels of factors. An example of quotes from the participants was also provided for each of the codes in the table.


                           Table 2
                            shows the distribution of codes for trust or distrust the technology as reported by the active users and passive users.

In the subsequent analysis, second level factors were used to represent the reported antecedents of trust in technology, in order to balance amount of information and simplicity of the analysis. Fig. 3
                            shows frequency counts of the coded second level factors as they related to reported trust/distrust in technology. To test the relationship between second level factors and trust/distrust in technology, a generalized linear mixed model (GLMM) was fitted to the data. Trust/distrust in technology was entered as the response variable with binomial distribution and logit link function. Second level factors were entered as predictor variables. In addition, roles, technological/task conditions, and roles X technological/task conditions interaction were also entered as predictor variables. Since there were instances where multiple second level factors were coded from the answer of one trial, the structure of the random intercept was that trials nested in participants and in turn nested in teams. Wald test for the model coefficients indicated that there was a significant second level factors effect on trust/distrust in technology (χ
                           2(df = 6) = 1.50 × 1012, p < 0.00). This result suggested that different antecedents of trust in technology related to the dichotomous report of trust or distrust in technology to a different extent. On the other hand, the effects of roles (t(26) = 1.20, p = 0.26) and technological/task conditions (χ
                           2(df = 2) = 2.60, p = 0.27) were not significant.


                           Fig. 4
                            shows the frequency counts of the second level factors at different levels of the two independent variable – roles and technological/task conditions.

To explore the relationship between the second level factors and the independent variables, a technique known as surrogate Poisson model analysis (Venables and Ripley, 2002) was used. First, a “minimal model” was fitted to the data. The model was specified as a generalized linear model with Poisson distribution and log link function for the response variable. Frequency counts were used as the response variable. The predictor variables include roles, technological/task conditions, roles X technological/task conditions interaction, and second level factors. AIC of the minimal model was 162.19. Residual deviance of the model was 26.20 with 30 degrees of freedom.

Second, two predictor variables – roles X second level factors interaction and technological/task conditions X second level factors interaction – were added to the minimal model to test if there was a significantly better fit. If there was a significantly better model fit, it indicates that there should be a relationship between the second level factors and the independent variables, and this relationship affects the distribution of the frequency count. The resultant model by adding roles X second level factors interaction had an AIC of 163.07. The resultant model by adding technological/task conditions X second level factors interaction had an AIC of 176.13. None of the new models suggested a better fit to the data. So no relationship was found between the second level factors and the independent variables in this analysis.

The purpose of the analysis in this step was to explore the relationship between the reported antecedents of trust in technology and trust in technology rating. Fig. 5
                         shows the mean trust in technology ratings at different categories of the second level factors given different roles and technological/task conditions.

An LME model was fitted to the data using trust in technology rating was the response variable and second level factors as predictor variables. Technological/task conditions, roles, and technological/task conditions X roles interaction were also entered into the model as predictor variables in order to control for their effect. Trials nested in participants that were nested in teams were entered as a random intercept. The results indicated that the effect of second level factors on trust in technology ratings were not significant (F(6, 26) = 2.07, p = 0.09). Unfortunately, due to the limited sample size, reliable tests about the relationship between roles and second level factors and the relationship between technological/task conditions and second level factors with regards to trust in technology ratings could not be performed.

@&#DISCUSSION@&#

The analysis of overall trust in technology ratings provided insight into research question (a). The finding was consistent with previous research (Montague and Xu, 2012) that there was no significant difference between the active user and passive user on average. That is to say, on average, the passive users reported a similar level of trust in technology as the active users reported under varied technological/task conditions. It appears that the active user and passive user in a team calibrated their trust in technology to a similar extent through interaction, even though they may or may not have explicitly discussed the trustworthiness of the technology. Future research could explore the content of the communications between the active user and the passive user in a team and examine how these communications contribute to the calibration of trust at the team level. For example, Xu and Montague (2013) found that group polarization happened after group discussion of the trustworthiness of the technology. Moods, which are heavily influenced by social interactions, are also found to be significantly related to trust in automation (Merritt, 2011; Stokes et al., 2010). In a meta-analysis, Hancock et al. (2011) found that factors of team collaboration, such as culture, communication, and shared mental model, influence trust in robots of individuals in a team. These studies indicated that social interaction can be a source for trust calibration. This future direction also corresponds to the recent development of “mesoergonomics” (Karsh et al., 2014) where human factors constructs are investigated across different levels. In this case, trust in technology is investigated at both individual level and group/team level.

The results presented in Tables 1 and 2 answered research question (b1). As seen in Table 2, there was variation in responses from the participants. Also the frequency counts of the factors reported by the participants varied, as seen in Figs. 3 and 4. Specifically, the frequency counts of usability and competence were significantly higher than the other factors. This is consistent with the prominence-interpretation theory that a variety of elements of the system could be potential antecedents of trust in technology for the users, and some of them are more easily perceived than others (Fogg, 2003; Fogg et al., 2003). In this particular setting, the participants reported that they evaluated the trustworthiness of the technology based on usability, competence, and appearance of the technology, individual characteristics of the user herself/himself, and demand and outcome of the task. Extensive empirical research has been conducted to investigate the factors related to usability and competence (Madhavan et al., 2006; Seong and Bisantz, 2008; Yuviler-Gavish and Gopher, 2011). However, more research is needed for some of the other factors, such as demand and outcome of the task (Ezer et al., 2005; Rice and Keller, 2009; Schwark et al., 2010). These results also showed that qualitative methodology is able to uncover a wide range of factors that can be potentially critical for trust calibration. Future research can use this technique in different specific settings such as healthcare or driving to explore specific antecedents of trust.

The statistical analysis of the relationship between second level factors and trust/distrust evaluation is also consistent with the prominence-interpretation theory. Different elements of the system to be perceived may relate to different evaluations of trust. For example, in this study, appearance of the technology was related to trust but not distrust for passive users. Previous studies found that aesthetics in interface design influence users' trust in online banking systems (Kim and Moon, 1998) and overall perceived credibility of information provided on a website (Robins and Holmes, 2008). In a study of automated navigation systems, Weinstock et al. (2012) found that although interface aesthetics were not significantly related to perception of trust, they related to perceived annoyance of the system. It was found that when the automation is perfect, the annoyance rating was lower in the aesthetic condition compared to the non-aesthetic condition. It was possible that aesthetics influenced trust by making the user perceive the automation to be less annoyance, but this effect was not detected due to the ceiling effect, which was a result of the perfectness of the automation.

The statistical analysis aimed to answer research question (b2) and (b3) suggested that, the type of antecedents of trust in technology reported by the users is independent of roles (i.e. active/passive user) and technological/task conditions. However, there should be caution about this result since the frequency counts of some of the factors were very low (even 0 in some cases) thus the analysis may not be reliable. Fig. 4 suggests that the active users reported usability and individual differences as antecedents of trust more frequently than the passive users. Appearance of the technology was only mentioned by passive users. There were also differences in the first level factors between the active users and passive users. In addition, even though some of the responses were assigned the same code, the actual response from the active user and passive user were different. For example, when mentioning the control of the technology, the active users may discuss their own experience with the control device; while the passive users may describe what they think the active users were experiencing: “… yes except the joystick. My partner [had a] problem … control[ling] it within the range.”

For research question (c), it is interesting that the mean trust in technology rating did not differ significantly under different antecedents, given that antecedents relates to the binary trust/distrust response significantly. There are two potential causes of this result. First, this could be a result of limited sample size and the unbalanced number of cases of each antecedent. Second, there may be a measurement problem. Under the prominence-interpretation theory, this research assumed that the binary trust/distrust measures the interpretation of a certain antecedent and the trust in technology rating scale measures the overall level of trust in technology. This assumption may not be valid. Future research should develop better instruments to assess a range of trust ratings to measure the constructs in this theory.

In this study, the active and passive users had limited prior experience of working with each other and had limited time to interact with each other. Thus, the results may be applicable to temporary groups where swift trust is to be developed (Goodman and Goodman, 1976; Meyerson et al., 1996; Wildman et al., 2012). In work settings, active and passive users may have both long and short term relationships. In patient-clinician relationships, for example, patients and providers can have long term relationships (for example, primary care settings), or short-term relationships (for example, emergency care settings). The large time differences in the relationships and interactions in the real world may lead to different developments of trust in technology perceptions. In short term experimental studies, it is unknown that if longer task period would lead to significant different trust levels. It is possible that some elements of the system would influence a user's trust only if the interaction time is long enough. Future studies could systematically manipulate the length of task period and investigate its effect on trust.

@&#CONCLUSION@&#

This study investigated the antecedents of trust in technology for active users and passive users in a multi-user system. The results indicated that the users evaluated the trustworthiness of the technology based on a variety of factors related to the technology, the user, and the task. The evaluation of trust given different antecedents differs, which supports the prominence-interpretation theory. The results also suggest that there are similarities between active users and passive users in terms of perceived antecedents of trust, though notable differences exist.

@&#REFERENCES@&#

