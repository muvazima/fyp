@&#MAIN-TITLE@&#Visualizing in vivo brain neural structures using volume rendered feature spaces

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Interactive visualization software for 3D microscope images is developed.


                        
                        
                           
                           A new transfer function design using volume rendered feature spaces is proposed.


                        
                        
                           
                           Multidimensional features are directly utilized for volume exploration.


                        
                        
                           
                           Two-photon microscope images of live mice are applied to the developed software.


                        
                        
                           
                           Soma, dendrites and apical dendrites are visualized using 3D feature spaces.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Volume visualization

Feature analysis

Neural structures

Two-photon microscopy

@&#ABSTRACT@&#


               
               
                  Background
                  Dendrites of cortical neurons are widely spread across several layers of the cortex. Recently developed two-photon microscopy systems are capable of visualizing the morphology of neurons within deeper layers of the brain and generate large amounts of volumetric imaging data from living tissue.
               
               
                  Method
                  For visual exploration of the three-dimensional (3D) structure of dendrites and the connectivity among neurons in the brain, we propose a visualization software and interface for 3D images based on a new transfer function design using volume rendered feature spaces. This software enables the visualization of multidimensional descriptors of shape and texture extracted from imaging data to characterize tissue. It also allows the efficient analysis and visualization of large data sets.
               
               
                  Results
                  We apply and demonstrate the software to two-photon microscopy images of a living mouse brain. By applying the developed visualization software and algorithms to two-photon microscope images of the mouse brain, we identified a set of feature values that distinguish characteristic structures such as soma, dendrites and apical dendrites in mouse brain. Also, the visualization interface was compared to conventional 1D/2D transfer function system.
               
               
                  Conclusions
                  We have developed a visualization tool and interface that can represent 3D feature values as textures and shapes. This visualization system allows the analysis and characterization of the higher-dimensional feature values of living tissues at the micron level and will contribute to new discoveries in basic biology and clinical medicine.
               
            

@&#BACKGROUND@&#

Given the complex three-dimensional (3D) architecture of the brain, it is essential to explore the morphology and activity of neurons in all layers of the cortex. However, this can often be challenging because the dendrites of cortical neurons are widely spread across several layers, including deeper layers that are difficult to observe by confocal or light microscopy. The length of the dendrites can vary from 20μm to 1mm, and the width and branching of the dendrites depends on the distance from the soma. This suggests that spatial differences in brain morphology relate to the functionality of the neuron including characteristics of the dendrite and synaptic efficiency [1,2].

To understand the 3D structure of dendrites and the connectivity among neurons in the brain, in vivo imaging and visualization have significant roles. Recently, the improved performance of microscopy systems enables the acquisition of large amounts of slice images from living tissues. In comparison with confocal or other optical microscopy systems, two-photon microscopy has an advantage in visualizing the morphology of neurons within deeper layers of living mouse brain [3–6]. Since the structures of tissues are stored as volume data, volume visualization techniques [7,8] are focused on the interactive exploration of the 3D images. When visualizing unknown features in the deeper layers of the brain, prior knowledge of the morphology of tissues [9,10] cannot be used. Furthermore, microscopic images are affected by optical characteristics such as scattering within tissues and the presence of image noise within deeper regions of the images. Because large amounts of volume data are obtained through two-photon microscopy, there is a demand for efficient visualization of local internal structures and characteristic intensity distributions.

Volume rendering has been widely used for visualizing volume data, where the rendered image is generated from the data by simulating optical properties such as radiation and absorption [11,12]. In comparison with surface rendering, the volume rendering technique generates a projection by converting the scalar values of sampling points in 3D space into color and opacity (RGBA values) based on transfer functions and integrating them along the viewing direction. The user can interactively explore the micro-level structures included in the volume data while changing the camera parameter, modifying the transfer functions [13–15] or generating the cross-section of the 3D images. Unlike the pattern recognition approach [16–18], visualization does not involve algorithm-based detection for specific objects. In other words, the transformations when visualizing the 3D image as a projection on the screen are only defined, and modifications of the visualization parameters and final judgments about the structures observed are left to the user.

The final quality of projections obtained with volume rendering significantly depends on the definition of the transfer functions. For this reason, the design of transfer functions is regarded as an important area of research for volume visualization [15,19,20]. So far, one-dimensional (1D) transfer functions based on the histogram of voxel intensity or its gradient [11,13] have been commonly used. However, with 1D transfer functions, it is not possible to achieve a visualization that distinguishes structures with the same intensity values and gradient information. By defining transfer functions based on feature values with higher dimensionality, it is possible to visualize changes in texture and morphological characteristics included in the images [19–22], but the high degree of freedom in multidimensional transfer functions makes it difficult for users to obtain the visualization result through manual parameter settings. For these issues, much effort has been made in the past years to ease the transfer function design. Wu et al. [23] proposed an interactive framework that allows editing and combining of images rendered by different transfer functions. Semiautomatic/automatic transfer function generation [24,25] has been also studied. Läthén et al. [25] presented automatic tuning techniques based on local intensity shift in vessel visualization domain [26]. These approaches can assist the user׳s visualization process based on the feature descriptors specified for the target structures. However, in the case of visual exploration, there are many situations for which feature descriptors have not been formulated [27]. Some researchers have focused on this issue and investigated methods for exploring high-dimensional feature space. PCA, ICA and clustering techniques are commonly used for dimensionality reduction of the feature space [28]. Julia et al. [27] presented a high-dimensional clustering approach to support classification tasks for knowledge-assisted visualization [29]. On the other hand, our focus is interactive visualization of brain neural structures in the biological domain. The deep layers of brain tissue contain a variety of neural structures with complex shapes such as soma, dendrites and white matter. Unlike in clinical CT/MRI images, numerous minute or thin structures are closely observed with optical scattering noise in microscopic images, which makes it challenging for volume visualization. As far as we surveyed, there are no reports on interactive feature exploration software and interface for visualizing multi-layer neural structures in microscopic images.

The main goal of our research was to investigate multidimensional features for exploring brain neural structures based on volume visualization. To do this, we designed a practical software system and interface for visual exploration of feature descriptors. It offers a means for intuitive observation of multidimensional features on shape and texture included in the volume data. This framework generates a feature space, which defines the 3D distribution of feature values contributing to structural classification within the image. Furthermore, it supports exploration for internal structures and setting of arbitrary visualization parameters through a user interface that enables interactive exploration and selection within the feature space generated. A final rendering is then generated based on the multidimensional transfer functions that define the color and opacity of voxels corresponding to the specified region of interest (ROI). We applied the proposed approach to two-photon microscopy images obtained from live mice. Using the developed system, we have elucidated a set of feature values that perform classified visualization of soma, dendrites and hippocampus. In addition, in comparison with conventional approaches, the proposed feature-based visualization successfully visualized complex apical dendrite structures.

The purpose of the proposed framework is to optimize the efficient visualization of features such as minute vascular and neural structures or weak tonal changes where boundaries are indistinct in 3D images (scalar volume data) obtained by two-photon microscopy. 
                     Fig. 1(a) shows the proposed data structure and algorithm flow, taking the volume data to be visualized as the starting point. The sequence of algorithms consists of the following three steps: (1) calculate the localized feature values and generate a 3D feature space representing the feature value distribution, (2) set the ROI in the visualized feature value distribution and its color and opacity, and (3) volume visualization based on the multidimensional transfer functions.

In the first step, we evaluated volume data locally based on various descriptors using filter operations and obtained feature values 
                        
                           
                              C
                           
                           
                              k
                           
                        
                      (k=1, …, n) with equivalent size to that of the volume data. In this research, we focused on a number of feature values that quantify both texture and shape, generating an n-dimensional feature space H with mapped statistical information. This process is applied once to the volume data as preprocessing.

Next, by selecting three axes arbitrarily from n axes in the feature space H, the user can visualize the 3D feature values obtained and observe them as a whole. Here, the feature value distribution functions not only as a global map for volume searches but also as an interface for setting visualization parameters. The second step involves managing user interaction in the feature space and helping the user to select regions in the feature value distribution of interest and to set visualization parameters for each region.

Finally, the third step involves generating visualization results based on multidimensional transfer function η reflecting the visualization parameters set. The majority of these processes are executed in parallel at high speed by the graphical processing unit (GPU), so the user can adjust the transfer function η interactively and obtain new visualization results.

By repeating these three steps, the user can interactively explore the characteristic structures and tonal changes included in the volume data, obtaining visual feedback from both the feature value map and the results of volume visualization.

This section explains the method of calculating the feature values of textures and shapes and how to generate the feature space. First, we convert the intensity pattern around each voxel and the probability distribution of texture feature values into numerical values. To enable visualization using the variation in the texture as an index of the feature values obtained in relation to the local region of the image, we calculated 10 feature values: local average, standard deviation, kurtosis, skewness, slice correlation in each axial x, y, and z-directions and three eigenvalues of the Hessian matrix. To perform visualization that distinguishes neural structures, we investigated a significant set of features configured by biological scientists.

In microscopic images, soma have a spherical structure. Dendrites have a linear shape, are often aligned in the same direction (the z-direction in this paper), and have complex minute fringe structures called spines. In deeper layers of the cortex, we can observe white matter, which can be noisy because of scattering effects. To achieve simultaneous visualization of several distinct structures, we used the local average of intensity μ and standard deviation σ, the slice correlation r
                        
                           z
                         in the z axis, which is the long axis of the dendrites, and labeled the image with the background region removed from the original image. The average intensity μ and standard deviation σ were also used by Haidacher et al. [22], who demonstrated that this method can enable visualization that is resistant to noise.

Slice correlation is used for classifying lung tumors and pulmonary vasculature in the field of medical image processing, and it has been confirmed to be effective [18]. Because it is mainly used for identifying and observing minute structures that are close to the limit of resolution with microscopic images, this technique is useful for our analysis. The diameter of the visualized soma and dendrites are extremely small, in the range of one or two voxels in some places, and they can appear non-contiguous in the image. Unlike the eigenvalue of the Hessian [15,17] that is also considered effective for classifying shapes, slice correlation does not require differential operations. This enables visualization of locations with many closely grouped minute structures in microscopic images, with less influence from surrounding structures. The coefficient of correlation is found with Eqs. (1)–(4).
                           
                              (1)
                              
                                 
                                    
                                       r
                                    
                                    
                                       z
                                    
                                 
                                 =
                                 
                                    
                                       σ
                                    
                                    
                                       f
                                       g
                                    
                                 
                                 /
                                 
                                    
                                       σ
                                    
                                    
                                       f
                                    
                                 
                                 
                                    
                                       σ
                                    
                                    
                                       g
                                    
                                 
                                 ,
                              
                           
                        
                        
                           
                              (2)
                              
                                 
                                    
                                       σ
                                    
                                    
                                       f
                                       g
                                    
                                 
                                 =
                                 
                                    1
                                    
                                       
                                          
                                             T
                                          
                                          2
                                       
                                    
                                 
                                 
                                    ∑
                                    
                                       x
                                       =
                                       0
                                    
                                    T
                                 
                                 
                                    
                                       ∑
                                       
                                          y
                                          =
                                          0
                                       
                                       T
                                    
                                    
                                       (
                                       f
                                       (
                                       x
                                       ,
                                       y
                                       )
                                       −
                                       
                                          
                                             m
                                          
                                          
                                             f
                                          
                                       
                                       )
                                       (
                                       g
                                       (
                                       x
                                       ,
                                       y
                                       )
                                       −
                                       
                                          
                                             m
                                          
                                          
                                             g
                                          
                                       
                                       )
                                    
                                 
                                 ,
                              
                           
                        
                        
                           
                              (3)
                              
                                 
                                    
                                       σ
                                    
                                    
                                       f
                                    
                                 
                                 =
                                 
                                    
                                       
                                          1
                                          T
                                       
                                       
                                          ∑
                                          
                                             x
                                             =
                                             0
                                          
                                          T
                                       
                                       
                                          
                                             ∑
                                             
                                                y
                                                =
                                                0
                                             
                                             T
                                          
                                          
                                             
                                                
                                                   (
                                                   f
                                                   (
                                                   x
                                                   ,
                                                   y
                                                   )
                                                   −
                                                   
                                                      
                                                         m
                                                      
                                                      
                                                         f
                                                      
                                                   
                                                   )
                                                
                                                2
                                             
                                          
                                       
                                    
                                 
                                 ,
                              
                           
                        
                        
                           
                              (4)
                              
                                 
                                    
                                       σ
                                    
                                    
                                       g
                                    
                                 
                                 =
                                 
                                    
                                       
                                          1
                                          T
                                       
                                       
                                          ∑
                                          
                                             x
                                             =
                                             0
                                          
                                          T
                                       
                                       
                                          
                                             ∑
                                             
                                                y
                                                =
                                                0
                                             
                                             T
                                          
                                          
                                             
                                                
                                                   (
                                                   g
                                                   (
                                                   x
                                                   ,
                                                   y
                                                   )
                                                   −
                                                   
                                                      
                                                         m
                                                      
                                                      
                                                         g
                                                      
                                                   
                                                   )
                                                
                                                2
                                             
                                          
                                       
                                    
                                 
                                 ,
                              
                           
                        where 
                           f
                           (
                           x
                           ,
                           y
                           )
                         and 
                           g
                           (
                           x
                           ,
                           y
                           )
                         are the intensity of pixels at position 
                           (
                           x
                           ,
                           y
                           )
                         in the z and z+k slice images in the z axial direction, respectively, and 
                           
                              
                                 m
                              
                              
                                 f
                              
                           
                           ,
                           
                              
                                 m
                              
                              
                                 g
                              
                           
                         is the average pixel value in the filter region 
                           T
                           ×
                           T
                         centered on position 
                           (
                           x
                           ,
                           y
                           )
                        . The calculated coefficient of correlation is defined as the evaluation value for quantifying the local shape through normalization.

Next, a feature value distribution is expressed by plotting the three evaluation values obtained for each voxel unit in 3D space.When the feature values are defined as 
                           
                              
                                 C
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 C
                              
                              
                                 2
                              
                           
                           ,
                           
                              
                                 C
                              
                              
                                 3
                              
                           
                        , the frequency of appearance of the 
                           (
                           
                              
                                 C
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 C
                              
                              
                                 2
                              
                           
                           ,
                           
                              
                                 C
                              
                              
                                 3
                              
                           
                           )
                         set obtained by scanning the whole volume is registered in its 3D position corresponding to 
                           X
                           =
                           (
                           
                              
                                 C
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 C
                              
                              
                                 2
                              
                           
                           ,
                           
                              
                                 C
                              
                              
                                 3
                              
                           
                           )
                        . In other words, the feature space is a 3D histogram when the intensity volume is evaluated with the three statistics above. Therefore, even in cases where classification is not possible with only intensity values, it is possible to achieve visualization with each voxel classified statistically based on variations in localized feature values. In these cases, the local average for voxel μ, standard deviation σ, and slice correlation r were used as feature values, and because there are three dimensions, the feature space H is discretized as 3D volume data comprising 2563 voxels. Each voxel 
                           H
                           (
                           X
                           )
                         comprising H is quantized by an 8-bit scalar value. The volume data obtained with this procedure are called feature volumes.


                        Fig. 1(b) shows the feature space H of the lung field in the CT data obtained from the chest. In this research, by volume visualization of the feature space and by defining a color map for frequency of feature values that appear, it is possible to observe the total 3D distribution and frequency of the feature values in the ROI. For example, if several tissues with similar feature values are present in the ROI and if the evaluation values are clearly different from other regions, then feature value distribution clusters are formed corresponding to each tissue in the feature space. If the boundaries of this group of clusters can be defined and spatially zoned, clear structural classification is possible, but in many cases, it is difficult to establish clear boundaries in the actual images, even when using several evaluation axes.

In addition to mechanically classifying these feature spaces, we also aimed to provide the user with a global map for observing the total feature value distributions in the image. This would allow the user to explore using the space itself as an interface and explore structures of interest interactively. Feature value distributions with characteristic structures present in the body overlap and are spatially offset from the distributions of the majority of normal tissue. Because of the tendency of feature values to be distributed around the margins of the main cluster, this method provides a framework for achieving visualization according to the user׳s intention and exploration with a high degree of freedom.

This section describes a visualization method using the feature volume generated by defining the color and opacity of characteristic structures and tone distributions in the volume data. Visualization is achieved by converting the RGBA values from 
                           X
                           =
                           (
                           μ
                           ,
                           σ
                           ,
                           r
                           )
                        , the 3D statistic obtained from the voxel units. In this research, we introduce a multidimensional transfer function η that defines the conversion from each feature space point 
                           H
                           (
                           X
                           )
                         to RGBA space points.
                           
                              (5)
                              
                                 η
                                 :
                                 H
                                 (
                                 X
                                 )
                                 →
                                 R
                                 G
                                 B
                                 A
                              
                           
                        The distribution of each point 
                           H
                           (
                           X
                           )
                         in the feature space is visualized with volume rendering. 
                        Fig. 2(a) is an image of the multidimensional transfer function η settings using the visualized feature space. In this example, two regions, D
                        1 and D
                        2, are specified in the feature space, and each region is assigned a color and opacity. Fig. 2(b) shows the results when the color and opacity of each region are applied to each point in the feature space. In this way, the RGBA value assigned to point 
                           H
                           (
                           X
                           )
                         is determined. The three-dimensional distribution of 
                           H
                           (
                           X
                           )
                         with the assigned RGBA values directly expresses the transfer function η.

In the rendering process, the normal 1D transfer function where intensity volume I is converted into an RGBA value is used concurrently, and that setting is used preferentially in relation to the voxels for which the RGBA value is defined in (4). This determination and conversion is performed by the GPU in the rendering process. Specifically, each volume data containing the evaluation values 
                           μ
                           ,
                           σ
                           ,
                           r
                         is sent to the memory in the GPU as a 3D texture. In the rendering process, when deciding the value that each voxel contributes in generating the final image, the color and opacity given to a single point 
                           H
                           (
                           X
                           )
                         in the feature space corresponding to the relevant voxel is used.

The transfer function η is expressed by color and opacity values assigned to the three-dimensional distribution of feature values in the volumetric feature space. In order to configure η efficiently, it is desirable for the user to be able to specify regions intuitively and simply. In our system, we provided an interface with two options for specifying ROIs in the feature space using a generic two-dimensional pointing device such as a mouse or a touchscreen.

When a single point on the rendered feature space is specified by a pointing operation (e.g. a mouse click), a set of voxels X with positive values is set as the selected region D in a spherical or a cubic region with a radius l centered on a three-dimensional position 
                           P
                           ∈
                           
                              
                                 R
                              
                              3
                           
                        . This framework is simply described by
                           
                              (6)
                              
                                 {
                                 X
                                 ∈
                                 D
                                 |
                                 
                                 |
                                 X
                                 −
                                 P
                                 |
                                 ≤
                                 l
                                 ∧
                                 H
                                 (
                                 X
                                 )
                                 >
                                 0
                                 }
                              
                           
                        
                     

In order to determine P in 2D operation, when the user indicates a pixel on the rendered image, our framework estimates the corresponding voxel in the volumetric space by accumulating voxel values (i.e. frequency values in the 3D histogram) at sampled feature points in the eye direction. This process is similar to the ray casting protocol [11] commonly used in the volume rendering scheme. Once the accumulated values exceed a threshold, we simply assume that the voxel at the current feature point 
                           X
                           =
                           (
                           μ
                           ,
                           σ
                           ,
                           r
                           )
                         has been selected by the user. The radius l can be set with the mouse wheel. By detecting the position in each rendering as above, the continuous regions of the adjacent isosurface of the feature value distribution can be acquired by changing the pointing position by dragging with the mouse.

To select a wide area that includes the inside of the feature value distribution, a cuboid bounding box is used. Each side of the cuboid is parallel with the axes of the feature space, and the center and the length of each side can be controlled with six parameters. The center of the bounding box 
                           P
                           ∈
                           
                              
                                 R
                              
                              3
                           
                         can also be specified by direct pointing on the rendered image as described in the pointing-based interface.


                        
                        Fig. 3 shows the appearance of the user interface. The interface consists of three main areas, with the feature volume displayed at the bottom right and the rendering results with color and opacity assigned at the left. At the bottom right are the controls for setting the parameters used for specifying the region of the feature space. After specifying a ROI in the feature space with (1) or (2) above, the user sets the RGBA values specified in the region with a color palette. The user can check the effect of the settings in the visualization results and if necessary, perform other operations such as adjusting the region and the RGBA values, or adding a region.

@&#RESULTS@&#

We implemented the sequence of algorithms using C++, OpenGL, GLSL (Open GL Shader Language), and NVIDIA CUDA (Compute Unified Device Architecture) as a software package called FeatureVis. The software, the user guide and the tutorial movies are available at our web site. We applied the developed software to two-photon microscopic images and verified the visualization of the characteristic structures and intensity distribution included in the images. For the 3D microscopic image and visualization of its feature volume, we used the texture-based rendering scheme [12] to achieve high-speed volume rendering using the texture interpolation and synthesis functions of the GPU. For validation and testing, we used a PC with the following specifications: OS: Windows 7 Ultimate (64 bit), CPU: Intel Core i7 870 (2.93GHz), memory: 16.0GB, and GPU: NVIDIA GeForce 580GTX.

For verification, we used three volume data sets taken from live, genetically modified mice, using a Nikon two-photon microscope (A1MP+). The neurons in the second and fifth layers of the cortex were labeled by a fluorescent protein (GFP, green fluorescent protein). The study was carried out in accordance with the recommendations in the Guidelines for the Care and Use of Laboratory Animals of the Animal Research Committee. The protocol was approved by the Committee on the Ethics of Animal Experiments.

These data capture a tomogram with a depth close to 1.4mm from the surface layer of the cortex [4], which, to the best of our knowledge, is the deepest visualized layer that has been obtained. This is important because the visualization of these deep layers have applications in the biological and medical fields. Together, these data verify that we successfully visualized fluorescently-labeled neurons from the second and fifth cortical layers from a live mouse. The visualization results for the whole volume using the earlier 1D transfer function is shown in the left panel of Fig. 3. The volume data have a size of 512×512×325 voxels, and the range of capture is 512×512×1300μm3. For reference, we sampled the brain region from clinical MR imaging data (512×512×512 voxels) of the head and built feature volumes. We used a 7×7×7 filter size for this test, bearing in mind the structures to be explored in the volume data, and k=3 for calculating the slice correlation.

The feature volumes generated are shown in 
                        Fig. 4. It shows that the brighter the tone of each voxel, the higher the frequency of appearance of the feature value. In Fig. 4(a) and (b), the feature volumes generated from the two-photon microscopic image are shown, where the feature values are distributed broadly in 3D. All axes contribute to the evaluation of the local features in the image. Fig. 4(c) shows the feature volume in the brain region of the MR image for reference. As the brain tissue has homogeneous pixel values, its feature volume has a biased distribution. A feature of the proposed method is that the feature volume enables the user to completely observe the trend in distribution of characteristics in the image, while defining the color and opacity of this distribution supports the discovery of characteristic structures in the image.

To examine the effectiveness of interactive searching using the proposed feature volume and visualization interface, we used the microscopic image in 
                        Fig. 5(a) to set transfer functions. First, we set the selection range of the μ axial direction for the bounding box from 0.16 to 1.00 and the selection range of the r axial direction from 0.05 to 1.00, and assigned red to the relevant region. The resulting characteristic volume and visualization results can be seen in Fig. 5(a). As with conventional two-dimensional (2D) transfer functions, the soma and their surrounding structures are visualized simultaneously. With the μ axis and r axial direction selection range fixed, Fig. 5(b) shows the image when the σ axial direction selection range is changed from 0.16 to 1.00, and (c) shows it changed from 0.36 to 1.00. The structure of interest is largely restricted by μ and r, and visualization is achieved with the soma distinguished by σ. The same result could not be obtained with only μ and r, but a 3D transfer function with σ added enabled visualization of the soma.

Next, 
                        Fig. 6(a) shows the result with the selection range of the μ axial direction set from 0.0 to 0.10, and the selection range of the σ axial direction set from 0.05 to 1.00. Fig. 6(b) and (c) shows the state in (a) with the selection range for the μ axial direction fixed, and the selection range of both the σ and r axial directions changed. By adjusting the selection range while checking the visualization results, we successfully visualized the apical dendrites, distinguished from their periphery. Throughout the image, σ and r were effective for evaluating fluorescence in microstructures.


                        
                        Fig. 7 shows another comparison result between intensity-based visualization and the proposed feature-based visualization. As shown in Fig. 7(b), we succeeded in visualizing the apical dendrite structures by making fine adjustments to the selection range in the σ and r axial directions. This indicates that searching with a focus on the marginal regions of feature value distributions leads to the discovery of biologically significant structures. It is difficult to obtain visualization parameters that meet these conditions through trial and error in images where prior knowledge cannot be used. Therefore, this is an example of a situation that allows the transfer functions to be changed interactively while observing the feature value distribution contributing to the discovery of characteristic structures.


                        
                        Fig. 8 shows visualization examples of multiple structures based on 3D transfer function settings configured by biological scientists. We could distinguish the dendrites, apical dendrites, soma, and white matter in the cortex, as well as hippocampal regions. We also could simultaneously visualize multiple structures by maintaining the changes to color and opacity defined for each feature value 
                           (
                           μ
                           ,
                           σ
                           ,
                           r
                           )
                         in the feature volume. While analyzing the transfer functions, it is possible to change additional settings and adjust the characteristics of interest. Fig. 8 shows the results of making all feature values of everything other than the soma and hippocampus transparent, and making the feature values of the dendrites opaque. This is achieved when the mouse is clicked on a point on the feature volume, the surface point detected by ray casting [11] is set as the start position, and a contiguous region related to the color and opacity in the characteristic volume is obtained.

We have confirmed that the visualization results reflecting adjustments of the transfer function are obtained at 18–21 frames per second, and natural, interactive operations are possible while re-rendering the scene. In the pre-computation process, the software generates the feature volumes from the initial intensity volume. A total of ten feature values are calculated in our case, and it takes 25–30s through per-voxel parallel processing of CUDA. This is reasonable computation time for visual exploration of three-dimensional microscopic images.

@&#DISCUSSION@&#

In this study, a practical system and interface for defining the conversion of each feature value to colors and opacities has been introduced and applied to volume visualization. Applying this approach to two-photon microscope images of the mouse brain enabled visualization that distinguished characteristic structures such as soma, dendrites and apical dendrites, which are difficult to visualize with earlier 1D or 2D transfer functions. Additionally, it appears that through characteristic volumes, the user can evaluate the total distribution of feature values in the image. In this way, by searching the marginal regions, the user can discover parameters that enable visualization of characteristic structures.

The limitation of the developed system and interface is that the number of features is obviously limited to three in feature space visualization. In the experiments, we have discussed with biological researchers and have chosen three feature values that can generate visually appropriate visualization results. Although interactive visualization is possible after selecting feature values, this also implies that trial-and-error process is needed to explore high-dimensional feature space. To make this process more systematic, incorporating PCA, ICA and other clustering techniques [29] into the developed software would be interesting. Actually, we have confirmed that the eigenvalues of Hessian [17,18] and the indices for the size of the structures [19] were also effective to visualize overlapping neural structures. Quantitative analysis of feature values of microscopic images and searches that combine the visualization of a wider range of structures with effective feature values are areas of future work.

@&#CONCLUSIONS@&#

This paper proposed a visualization software and interface for 3D microscopic images using the generation of a 3D feature space and a color and opacity interface using volume rendered feature spaces. It allows for the total observation of multiple indices of shape and texture included in the volume data. It also provides an environment in which the user can achieve the desired visualization while interactively and intuitively setting the parameters for multidimensional transfer functions. However, with the feature values used in this study, it was not possible to clearly visualize the microstructures in the white matter or the complex end structures of the apical dendrites. By optimizing visualization algorithms and interfaces that can handle feature values of three dimensions or more, we aim to establish visualization systems that contribute to new discoveries in biology, medicine and molecular modeling through the analysis of feature values of tissue at the micron level.

The authors report no conflicts of interest.

@&#ACKNOWLEDGMENTS@&#

This work was supported by Platform for Dynamic Approaches to Living System from the Ministry of Education, Culture, Sports, Science and Technology, Japan. This research was also supported by a Japan Society for the Promotion of Science (JSPS), Grant-in-Aid for Scientific Research for Young Scientists, Number 24680059.

@&#REFERENCES@&#

