@&#MAIN-TITLE@&#Word sense discrimination in information retrieval: A spectral clustering-based approach

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We propose a new unsupervised method that uses word sense discrimination in IR.


                        
                        
                           
                           The re-ranking method is based on spectral clustering.


                        
                        
                           
                           The effectiveness over queries with ambiguous terms is proved on TREC corpora.


                        
                        
                           
                           Our interest regards improving the precision after 5, 10 and 30 retrieved documents.


                        
                        
                           
                           The method improves results for queries with poor baseline performance.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Information retrieval

Word sense disambiguation

Word sense discrimination

Spectral clustering

High precision

@&#ABSTRACT@&#


               
               
                  Word sense ambiguity has been identified as a cause of poor precision in information retrieval (IR) systems. Word sense disambiguation and discrimination methods have been defined to help systems choose which documents should be retrieved in relation to an ambiguous query. However, the only approaches that show a genuine benefit for word sense discrimination or disambiguation in IR are generally supervised ones. In this paper we propose a new unsupervised method that uses word sense discrimination in IR. The method we develop is based on spectral clustering and reorders an initially retrieved document list by boosting documents that are semantically similar to the target query. For several TREC ad hoc collections we show that our method is useful in the case of queries which contain ambiguous terms. We are interested in improving the level of precision after 5, 10 and 30 retrieved documents (P@5, P@10, P@30) respectively. We show that precision can be improved by 8% above current state-of-the-art baselines. We also focus on poor performing queries.
               
            

@&#INTRODUCTION@&#

According to Lin (1997), “given a word, its context and its possible meanings, the problem of word sense disambiguation (WSD) is to determine the meaning of the word in that context”.
                        1
                        For a complete discussion of state-of-the-art WSD see the monograph (Agirre & Edmonds, 2006).
                     
                     
                        1
                      Although WSD is generally easy for humans, it represents an issue for computers. The problem becomes even more difficult to solve when an ambiguous word occurs in short chunks of texts, such as a query in an information retrieval (IR) system.

Applying WSD to improve IR results is a well studied problem, but with controversial results as evidenced in the literature. Several authors have concluded that WSD in IR does not lead to significant retrieval performance improvement (Guyot, Falquet, Radhouani, & Benzineb, 2008; Sanderson, 1994). Various studies (Krovetz & Croft, 1992; Uzuner, Katz, & Yuret, 1999; Voorhees, 1993) have argued that the main problem in improving retrieval performance when using WSD is the inefficiency of the existing disambiguation algorithms, a problem which increases in the case of short queries.

In more recent years the issue remained “as to whether less than 90% accurate automated WSD can lead to improvements in retrieval effectiveness” (Stokoe, Oakes, & Tait, 2003). This remark refers primarily to the traditional task of WSD which identifies the meaning of the ambiguous word in context. This type of WSD is generally based on external sources, such as dictionaries or WordNet(WN)-like knowledge bases for labeling senses (Carpineto & Romano, 2012; Guyot et al., 2008) and is therefore knowledge-based.

Attempts to use knowledge-based WSD in IR have been numerous. In (Gonzalo, Verdejo, Chugur, & Cigarran, 1998) as well as in (Mihalcea & Moldovan, 2000) positive results were reported. These studies made use of semantic indexing based on WN synsets. However, they were all conducted on small data sets. As commented in (Ng, 2011), the evaluation is scaled up to a large test collection in (Stokoe et al., 2003) but the reported improvements are from a weak baseline. Positive results are also reported in (Kim, Seo, & Rim, 2004), although the quantum of improvements is small.


                     Zhong and Ng (2012) are among the few authors who more recently have expressed a growing belief in the benefits brought by WSD to IR – when using a supervised WSD technique. They constructed their supervised WSD system directly from parallel corpora. Experimental results on standard TREC collections show that, using the word senses tagged by this supervised WSD system, significant improvements over a state-of-the-art IR system can be obtained (Zhong & Ng, 2012). However, it is well known that supervised WSD cannot be used on a large scale in practice due to the absence of the necessary annotated/parallel corpora.

In contrast to all these authors, we are suggesting and investigating the usage of an unsupervised WSD technique. In this paper, we present an approach that aims at identifying clusters from similar contexts, where each cluster shows a polysemous word being used for a particular meaning. It is our belief that IR is an application for which this type of analysis is useful. Our approach is therefore not concerned with performing a straightforward WSD, but rather with differentiating among the meanings of an ambiguous word. Considering word sense discrimination rather than straightforward WSD avoids the use of external sources such as dictionaries or WN type synsets which are commonly used (Carpineto & Romano, 2012).

In this paper, we propose a new word sense discrimination method for IR based on spectral clustering. This state of the art clustering technique is now a hot topic; for example, Takacs and Demiris (2009) studied the use of spectral clustering in multi-agent systems while Borjigin and Guo (2012) recently discussed the cluster number determination in spectral clustering. Spectral clustering has been used in WSD for the first time by Popescu and Hristea (2011) who point out the importance of the clustering method used in unsupervised WSD.

We hereby show that WS discrimination based on spectral clustering outperforms the baseline when no WS discrimination is applied and also when using another unsupervised method (Naïve Bayes).

The present paper is organized as follows: in Section 2 we present the related works on WSD in IR; the focus is on unsupervised methods. Section 3 presents word sense discrimination based on spectral clustering. Section 4 presents the two step IR process using the proposed WS discrimination model. The evaluation is presented in Section 5. A more thorough analysis of the obtained results is performed in Section 6. Section 7 lays out the impact of automatically generated context on our proposed method. Section 8 concludes this paper.

@&#RELATED WORK@&#

Word sense ambiguity is a central concern in natural language processing (NLP). SENSEVAL defined the first evaluation framework for word sense disambiguation (WSD) in NLP (Kilgarriff, 1997). According to Kilgarriff and Rosenzweig (2000), SENSEVAL participants defined systems that can be classified into two categories: supervised systems, which use training instances of sense-tagged words and non-supervised systems. According to (Navigli, 2009), supervised systems are typically employed when a restricted number of words have to be disambiguated, while this type of system encounters more difficulties when all open-class words from a text have to be disambiguated. In addition to general WSD, many recent papers consider disambiguation of individuals (Artiles, Gonzalo, & Sekine, 2007; D’Angelo, Giuffrida, & Abramo, 2011; Piskorski, Wieloch, & Sydow, 2009) and disambiguation of place names (Leidner, 2007). Indeed, WSD has many applications, such as text processing, machine translation and information retrieval (IR), for which this type of disambiguation – proper names – can be useful (although not sufficient).


                     Krovetz and Croft (1992) were among the first to conduct a thorough analysis of ambiguity in IR. They used the CACM and TIME test collections and compared query word sense with word senses in retrieved documents. They found that sense mismatch occurs more often when the document is non-relevant to the query and when there are few common words bridging the query and the retrieved document. Another large scale study of word sense disambiguation in IR was conducted by Voorhees (1993). The automatic indexing process she developed used the “is-a” relations from WN and constructed vectors of senses to represent documents and queries. This approach was compared to a stem-based approach for 5 small collections (CACM, CISI, CRAN, MED, TIME). The results showed that the stem-based approach was superior overall, although the sense-based approach improved the results for some queries (Voorhees, 1993). Sanderson (1994) used the Reuters collection in his experiments and showed that disambiguation accuracy should be of at least 90% in order for it to be of practical use. He used pseudo-words in his experiments.

Schütze introduced word sense discrimination in IR (Schütze & Pedersen, 1995; Schütze, 1998). Moreover, Schütze considers that, in some cases, WSD can be defined as a two-stage process: first sense discrimination, then sense labeling. Sense discrimination aims at classifying the occurrences of a word into categories that share the same word sense. This type of approach is quite distinct from the traditional task of WSD, which, as already mentioned, classifies words relative to existing senses. Schütze and Pedersen (1995), Schütze (1998) created a lexical co-occurrence based thesaurus. They associated each ambiguous term with a word vector where coordinates correspond to co-occurring term frequencies. Words with the same meaning were assumed to have similar vectors. Word vectors were clustered together to determine the word uses. Similarity was based on the cosine measure. The application in IR consisted of modifying the standard word-based vector-space model. The words from the “bag of words” text representation were replaced by word senses. Evaluation of TREC 1 showed that average precision is improved when using sense-based retrieval rather than word-based retrieval. Combining word and sense-based retrieval improves precision as well. They were the first to demonstrate that disambiguation, even if imperfect, can indeed improve text retrieval performance (Schütze & Pedersen, 1995).


                     Schütze (1998) context group discrimination uses a form of average link clustering known as McQuitty’s Similarity Analysis. Schütze adapts LSI/LSA so that it represents entire contexts rather than single word types using second-order co-occurrences of lexical features. The created clusters are made up of contexts that represent a similar or related sense. In Schütze (1998) it is again shown that unlabeled clusters of occurrences of a word representing the same sense result in improved IR.

Unlike that described in Schütze and Pedersen (1995), Schütze (1998), the method we propose in the present paper is based on re-ranking and not on modifying document representation.

Much more recently, Chifu and Ionescu (2012) also show that the combination of word-based ranking and sense-based ranking is beneficial for improving IR performance, but on the lowest precision queries only (Chifu & Ionescu, 2012). They used a classical clustering technique based on the Naïve Bayes model (for which a WN-based feature selection is performed). However, in the case of their best obtained result, the highest difference between this result and the baseline under consideration was 0.1091.

The present paper will investigate a similar type of technique for IR that uses spectral clustering. Our aim is not only to restate the benefits of unsupervised WSD in IR, but also to point out the importance of the clustering technique involved in this task. While Chifu and Ionescu (2012), in spite of performing WN-based feature selection, were not able to move beyond the baseline when considering all queries, and therefore only targeted the lowest precision ones, we hereby show that, when using spectral clustering (that performs its own feature weighting) the same baseline is, in most cases, surpassed.

Analysis of the results provided by the newly proposed method will be carried out (see Section 5.6) against these two major approaches existing in the literature (Chifu & Ionescu, 2012; Schütze & Pedersen, 1995). The obtained results will be shown as promising in sustaining the concept of sense discrimination being beneficial for IR applications, especially when used from a re-ranking perspective.

Word sense discrimination can be considered as a clustering problem since a way to solve it is to group the contexts of an ambiguous word into a number of groups and to discriminate between these groups without labeling them. As is well known, linguistic data is structurally highly complex, thus turning clustering into a difficult task. Recently, a variety of clustering algorithms have been proposed in order to deal with situations where the data is not linearly separable and the clusters are non-convex. In particular, two related families of methods, kernel and spectral methods, have proven to be very effective in solving different tasks.

In computational linguistics, spectral clustering has been used for machine translation (Gangadharaiah, Brown, & Carbonell, 2006; Zhao, Xing, & Waibel, 2005), name disambiguation for author citation identification (Han, Zha, & Giles, 2005), and in unsupervised WSD (Popescu & Hristea, 2011).

Spectral clustering has been used in WSD for the first time by Popescu and Hristea (2011) who point out the importance of the clustering method used in unsupervised WSD. Spectral clustering has been shown (Popescu & Hristea, 2011) as strong enough to make up for the lack of external knowledge of all types, solving many problems on its own, including that of feature selection for WSD. Disambiguation results, after using an unsupervised algorithm based on spectral clustering (that uses its own feature weighting) were superior to those obtained using a classical unsupervised algorithm (with an underlying Naïve Bayes model, for which feature selection was performed) for all parts of speech (Popescu & Hristea, 2011).

The disambiguation accuracy obtained when using spectral clustering in unsupervised WSD, relative to all parts of speech, encouraged us to adopt this clustering technique for sense discrimination in the context of IR.

The method of spectral clustering is briefly presented here. For more details and justification of the method the reader is referred to von Luxburg (2007) and Hastie, Tibshirani, and Friedman (2008).

Given a set of observations 
                           
                              
                                 
                                    x
                                 
                                 
                                    1
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    x
                                 
                                 
                                    n
                                 
                              
                           
                         and some notion of similarity 
                           
                              
                                 
                                    s
                                 
                                 
                                    ij
                                 
                              
                              ⩾
                              0
                           
                         between all pairs of observations 
                           
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                           
                         and 
                           
                              
                                 
                                    x
                                 
                                 
                                    j
                                 
                              
                           
                        , the intuitive goal of clustering is to divide the observations into several groups such that observations in the same group are similar and observations in different groups are dissimilar to each other. One possible way to represent the pairwise similarities between observations is via an undirected similarity graph 
                        
                           
                              G
                              =
                              (
                              V
                              ,
                              E
                              )
                           
                        . The vertices of the graph represent the observations (the vertex 
                           
                              
                                 
                                    v
                                 
                                 
                                    i
                                 
                              
                           
                         represents the observation 
                           
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                           
                        ). Two vertices are connected if the similarity 
                           
                              
                                 
                                    s
                                 
                                 
                                    ij
                                 
                              
                           
                         between the corresponding observations 
                           
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                           
                         and 
                           
                              
                                 
                                    x
                                 
                                 
                                    j
                                 
                              
                           
                         is positive (or exceeds some threshold). The edges are weighted by the 
                           
                              
                                 
                                    s
                                 
                                 
                                    ij
                                 
                              
                           
                         values. The problem of clustering can then be reformulated as a graph-partition problem, where we identify connected components with clusters. Our intention is to find a partition of the graph such that the edges between different groups have very low weights (which means that observations in different clusters are dissimilar to each other) and the edges within a group have high weights (which means that observations within the same cluster are similar to each other).

An important element in spectral clustering is to construct similarity graphs that reflect the local neighborhood relationships between observations. Starting from a similarity matrix, there are many ways (Maier, Hein, & von Luxburg, 2009; von Luxburg, 2007) to define a similarity graph that reflects local behavior: 
                           
                              ε
                           
                        -neighborhood graph, k-nearest neighbor graphs, fully connected graph. One of the most popular graphs and the one that we will use for unsupervised WSD, is the mutual k-nearest-neighbor graph. The vertex 
                           
                              
                                 
                                    v
                                 
                                 
                                    i
                                 
                              
                           
                         is connected to the vertex 
                           
                              
                                 
                                    v
                                 
                                 
                                    j
                                 
                              
                           
                         if, according to the similarity matrix 
                           
                              
                                 
                                    s
                                 
                                 
                                    ij
                                 
                              
                           
                        , the observation 
                           
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                           
                         is among the k-nearest neighbors of the observation 
                           
                              
                                 
                                    x
                                 
                                 
                                    j
                                 
                              
                           
                         or the observation 
                           
                              
                                 
                                    x
                                 
                                 
                                    j
                                 
                              
                           
                         is among the k-nearest neighbors of the observation 
                           
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                           
                        . The weight of the edge 
                           
                              
                                 
                                    v
                                 
                                 
                                    i
                                 
                              
                              
                                 
                                    v
                                 
                                 
                                    j
                                 
                              
                           
                         will be 
                           
                              
                                 
                                    w
                                 
                                 
                                    ij
                                 
                              
                              =
                              
                                 
                                    s
                                 
                                 
                                    ij
                                 
                              
                           
                         in this case.

In order to formally present the method of spectral clustering we introduce the following notations.

Let 
                           
                              G
                              =
                              (
                              V
                              ,
                              E
                              )
                           
                         be an undirected graph with vertex set 
                           
                              V
                              =
                              
                                 
                                    v
                                 
                                 
                                    1
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    v
                                 
                                 
                                    n
                                 
                              
                           
                        . In the following we assume that the graph G is weighted, that each edge between two vertices 
                           
                              
                                 
                                    v
                                 
                                 
                                    i
                                 
                              
                           
                         and 
                           
                              
                                 
                                    v
                                 
                                 
                                    j
                                 
                              
                           
                         carries a non-negative weight 
                           
                              
                                 
                                    w
                                 
                                 
                                    ij
                                 
                              
                              ⩾
                              0
                           
                        . The weighted adjacency matrix of the graph is the matrix 
                           
                              W
                              =
                              
                                 
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          ij
                                       
                                    
                                    )
                                 
                                 
                                    i
                                    ,
                                    j
                                    =
                                    1
                                    ,
                                    …
                                    ,
                                    n
                                 
                              
                           
                        . 
                           
                              
                                 
                                    w
                                 
                                 
                                    ij
                                 
                              
                              =
                              0
                           
                         means that the vertices 
                           
                              
                                 
                                    v
                                 
                                 
                                    i
                                 
                              
                           
                         and 
                           
                              
                                 
                                    v
                                 
                                 
                                    j
                                 
                              
                           
                         are not connected by an edge. As G is undirected, we require that 
                           
                              
                                 
                                    w
                                 
                                 
                                    ij
                                 
                              
                              =
                              
                                 
                                    w
                                 
                                 
                                    ji
                                 
                              
                           
                        . The degree of a vertex 
                           
                              
                                 
                                    v
                                 
                                 
                                    i
                                 
                              
                              ∈
                              V
                           
                         is defined as 
                           
                              
                                 
                                    d
                                 
                                 
                                    i
                                 
                              
                              =
                              
                                 
                                    ∑
                                 
                                 
                                    j
                                    =
                                    1
                                 
                                 
                                    n
                                 
                              
                              
                                 
                                    w
                                 
                                 
                                    ij
                                 
                              
                           
                        . The degree matrix D will be the diagonal matrix with the degrees 
                           
                              
                                 
                                    d
                                 
                                 
                                    1
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    d
                                 
                                 
                                    n
                                 
                              
                           
                         on the diagonal.

Given a subset of vertices 
                           
                              A
                              ⊆
                              V
                           
                        , we denote its complement 
                           
                              V
                              ⧹
                              A
                           
                         by 
                           
                              
                                 
                                    A
                                 
                                 
                                    ‾
                                 
                              
                           
                         and its cardinal by 
                           
                              |
                              A
                              |
                           
                        . For two not necessarily disjoint sets 
                           
                              A
                              ,
                              B
                              ⊆
                              V
                           
                         we define
                           
                              (1)
                              
                                 W
                                 (
                                 A
                                 ,
                                 B
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          
                                             
                                                v
                                             
                                             
                                                i
                                             
                                          
                                          ∈
                                          A
                                          ,
                                          
                                             
                                                v
                                             
                                             
                                                j
                                             
                                          
                                          ∈
                                          B
                                       
                                    
                                 
                                 
                                    
                                       w
                                    
                                    
                                       ij
                                    
                                 
                                 .
                              
                           
                        
                     

We can now formulate the graph-partition problem in relation to spectral clustering. For a given number k of subsets (clusters) there is a partition of V 
                        
                           
                              
                                 
                                    A
                                 
                                 
                                    1
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    A
                                 
                                 
                                    k
                                 
                              
                           
                         which minimizes
                           2
                           The “RatioCut” is not the only objective function optimized in spectral clustering. See (von Luxburg, 2007) for other variants such as “Ncut”.
                        
                        
                           2
                        :
                           
                              (2)
                              
                                 RatioCut
                                 (
                                 
                                    
                                       A
                                    
                                    
                                       1
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    
                                       A
                                    
                                    
                                       k
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       2
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          k
                                       
                                    
                                 
                                 
                                    
                                       W
                                       (
                                       
                                          
                                             A
                                          
                                          
                                             i
                                          
                                       
                                       ,
                                       
                                          
                                             
                                                
                                                   A
                                                
                                                
                                                   ‾
                                                
                                             
                                          
                                          
                                             i
                                          
                                       
                                       )
                                    
                                    
                                       |
                                       
                                          
                                             A
                                          
                                          
                                             i
                                          
                                       
                                       |
                                    
                                 
                              
                           
                        
                     

Unfortunately, the above optimization problem is NP hard (von Luxburg, 2007). Spectral clustering solves a relaxed version of this problem. Relaxing “RatioCut” leads to unnormalized spectral clustering.

The unnormalized graph Laplacian matrix of a similarity graph G is defined as:
                           
                              (3)
                              
                                 L
                                 =
                                 D
                                 -
                                 W
                              
                           
                        Spectral clustering finds the m eigenvectors 
                           
                              
                                 
                                    U
                                 
                                 
                                    n
                                    ×
                                    m
                                 
                              
                           
                         that correspond to the m smallest eigenvalues of L (ignoring the trivial constant eigenvector corresponding to the eigenvalue 0). Using a standard method like K-means, the rows of U are clustered, giving a clustering of the original observations.

The unnormalized spectral clustering algorithm is summarized in Algorithm 1.
                           Algorithm 1
                           Unnormalized spectral clustering algorithm 
                                 
                                    
                                       
                                       
                                          
                                             Input: Similarity matrix 
                                                   
                                                      S
                                                      ∈
                                                      
                                                         
                                                            R
                                                         
                                                         
                                                            n
                                                            ×
                                                            n
                                                         
                                                      
                                                   
                                                , number k of clusters to construct.
                                          
                                          
                                             
                                                
                                                   
                                                      •
                                                      Construct a similarity graph in one of the standard ways, for example by using the mutual k-nearest-neighbor graph. Let W be its weighted adjacency matrix.
                                                   
                                                   
                                                      •
                                                      Compute the unnormalized Laplacian L.
                                                   
                                                   
                                                      •
                                                      Compute the first 
                                                            
                                                               k
                                                               -
                                                               1
                                                            
                                                          eigenvectors 
                                                            
                                                               
                                                                  
                                                                     u
                                                                  
                                                                  
                                                                     1
                                                                  
                                                               
                                                               ,
                                                               …
                                                               ,
                                                               
                                                                  
                                                                     u
                                                                  
                                                                  
                                                                     k
                                                                     -
                                                                     1
                                                                  
                                                               
                                                            
                                                          of L corresponding to the 
                                                            
                                                               k
                                                               -
                                                               1
                                                            
                                                          smallest eigenvalues of L (ignoring the trivial constant eigenvector corresponding to the eigenvalue 0).
                                                   
                                                   
                                                      •
                                                      Let 
                                                            
                                                               U
                                                               ∈
                                                               
                                                                  
                                                                     R
                                                                  
                                                                  
                                                                     n
                                                                     ×
                                                                     (
                                                                     k
                                                                     -
                                                                     1
                                                                     )
                                                                  
                                                               
                                                            
                                                          be the matrix containing the vectors 
                                                            
                                                               
                                                                  
                                                                     u
                                                                  
                                                                  
                                                                     1
                                                                  
                                                               
                                                               ,
                                                               …
                                                               ,
                                                               
                                                                  
                                                                     u
                                                                  
                                                                  
                                                                     k
                                                                     -
                                                                     1
                                                                  
                                                               
                                                            
                                                          as columns.
                                                   
                                                   
                                                      •
                                                      For 
                                                            
                                                               i
                                                               =
                                                               1
                                                               ,
                                                               …
                                                               ,
                                                               n
                                                            
                                                         , let 
                                                            
                                                               
                                                                  
                                                                     y
                                                                  
                                                                  
                                                                     i
                                                                  
                                                               
                                                               ∈
                                                               
                                                                  
                                                                     R
                                                                  
                                                                  
                                                                     k
                                                                     -
                                                                     1
                                                                  
                                                               
                                                            
                                                          be the vector corresponding to the i-th row of U.
                                                   
                                                   
                                                      •
                                                      Cluster the points 
                                                            
                                                               
                                                                  
                                                                     (
                                                                     
                                                                        
                                                                           y
                                                                        
                                                                        
                                                                           i
                                                                        
                                                                     
                                                                     )
                                                                  
                                                                  
                                                                     i
                                                                     =
                                                                     1
                                                                     ,
                                                                     …
                                                                     ,
                                                                     n
                                                                  
                                                               
                                                            
                                                          in 
                                                            
                                                               
                                                                  
                                                                     R
                                                                  
                                                                  
                                                                     k
                                                                     -
                                                                     1
                                                                  
                                                               
                                                            
                                                          with the k-means algorithm into clusters 
                                                            
                                                               
                                                                  
                                                                     C
                                                                  
                                                                  
                                                                     1
                                                                  
                                                               
                                                               ,
                                                               …
                                                               ,
                                                               
                                                                  
                                                                     C
                                                                  
                                                                  
                                                                     k
                                                                  
                                                               
                                                            
                                                         .
                                                   
                                                
                                             
                                          
                                          
                                             Output: Clusters 
                                                   
                                                      
                                                         
                                                            A
                                                         
                                                         
                                                            1
                                                         
                                                      
                                                      ,
                                                      …
                                                      ,
                                                      
                                                         
                                                            A
                                                         
                                                         
                                                            k
                                                         
                                                      
                                                   
                                                 with 
                                                   
                                                      
                                                         
                                                            A
                                                         
                                                         
                                                            i
                                                         
                                                      
                                                      =
                                                      {
                                                      j
                                                      |
                                                      
                                                         
                                                            y
                                                         
                                                         
                                                            j
                                                         
                                                      
                                                      ∈
                                                      
                                                         
                                                            C
                                                         
                                                         
                                                            i
                                                         
                                                      
                                                      }
                                                   
                                                .
                                          
                                       
                                    
                                 
                              
                           

There are a number of issues that must be dealt with when applying spectral clustering in practice. One must choose how to compute the similarity between observations and how to transform these similarities into a similarity graph. In the case of the mutual k-nearest-neighbor graph, the parameter k, representing the number of nearest neighbors, must be set. In the light of all these issues we follow the approach adopted by Popescu and Hristea (2011) where spectral clustering was used in WSD for the first time.

In unsupervised WSD, the observations are represented by contexts of the ambiguous word. The contextual features are given by the actual “neighboring” content words of the target (ambiguous) word. They occur in a fixed position near the target, in a window of fixed length, centered or not centered on the target. A window of size n denotes the consideration of n content words to the left and n content words to the right of the target, whenever possible. The total number of words considered for disambiguation is therefore 
                           
                              2
                              n
                              +
                              1
                           
                        . When not enough features are available, the entire sentence in which the target word occurs represents the context window. The classical window size, generally used in WSD, and also in the present paper, is 25 
                           
                              (
                              n
                              =
                              25
                              )
                           
                        . Within this representation, the value of a feature is given by the number of occurrences of the corresponding word in the given context window. Thus, a context is represented as a feature vector and the similarity between two contexts is given by the value of the dot product of the corresponding feature vectors. The dot product was chosen as the measure of similarity between feature vectors because of the success of the linear kernel in supervised WSD (Màrquez, Escudero, Martínez, & Rigau, 2006).

As a method for building the similarity graph from the similarity matrix we use the mutual k-nearest-neighbor graph method. This involves the choice of the parameter k, the number of neighbors. As in Popescu and Hristea (2011), we use a value of 30 for the number of neighbors.
                           3
                           See (Popescu & Hristea, 2011) for a justification concerning the choice of this number of neighbors.
                        
                        
                           3
                        
                     

The main contribution of this paper is the proposal of a new spectral clustering-based method for performing word sense discrimination for IR. This method aims to increase the top level precision for queries which contain ambiguous words. Our suggested approach reorders an initially retrieved document list by pushing to the fore documents that are semantically similar to the target query.

To start with, for each query we retrieve a set of documents by means of a state of the art search engine; documents are ordered according to their scores. Our objective is then to pinpoint the documents which are more relevant to the information need because they share the term sense with the query; and to enable these documents to improve their position at the top of the document list.

The first phase in the method is based on clustering the retrieved document set with respect to the senses of each ambiguous word and to decide which documents share the query term sense. Spectral clustering is used in this phase. In the second phase, we reorder the initially retrieved document list by boosting documents belonging to the selected cluster.

Before we can apply the WSD technique described in Section 3.2 to the queries, we need to process the data within a preprocessing step. The preprocessing step identifies the polysemous words of a query (as a result of their occurrence in multiple WN synsets).

Term discrimination uses a sub-set of documents. More precisely, for each query, we consider the first n documents retrieved by the IR system. For each document we thus know the score which indicates how similar that document is to a specific query. The query is added to this set of retrieved documents as if it was another document. The feature set for each document is then calculated by creating an incidence matrix with rows representing the documents and columns representing the features. Each element of this matrix is either 1 or 0, depending whether or not the feature indicated by the column index is present in the document indicated by the row index. The number of WN senses and the incidence matrix obtained after data preprocessing is used as input for the WSD algorithm. Thus, the WSD process is performed on 
                           
                              n
                              +
                              1
                           
                         documents (n initially retrieved documents and the query itself).

Let us now describe in more details the entire WSD process in relation to a single polysemous target word.

The first step is to build the corresponding feature set. The first processing step is to eliminate the stopwords. The remaining words are stemmed using the Porter stemmer algorithm (Porter, 1980). The stem corresponding to the target word is not retained, while the remaining stems, alphabetically ordered, represent the final set of features that are used in the WSD process.

The second step is to build the incidence matrix that indicates what features occur in each document. We determine the position of the target word within each of the documents. In our experiments we used a context window of size 25, as suggested in Hristea, Popescu, and Dumitrescu (2008) in order to obtain the best possible disambiguation accuracy. The features that occur in the context window are stored in the row of the matrix that corresponds to the analyzed document. If a certain document contains the target several times, we only consider its first occurrence. This is done in accordance with the “one-sense-per-discourse” heuristic (Gale, Church, & Yarowsky, 1992) which is largely used in WSD and which states the tendency of a word to preserve its meaning across all its occurrences in a given discourse.

For each query and for each ambiguous term occurring in that query, we cluster the retrieved documents into a number of clusters equal to the number of senses the ambiguous term has, according to a lexical database (WN), which will be used as sense inventory. The final task of the WSD process for a given polysemous word is to determine the document clusters relative to that word. Each obtained cluster corresponds to a specific sense of the polysemous target word, with one of the clusters containing the query itself. Note that two documents are similar (and thus belong to the same cluster), from the WSD point of view, if the polysemous word has the same sense in both documents. Therefore, disambiguating a polysemous term results in retaining only those documents occurring in the same cluster as the query.

A query can contain several ambiguous terms. In this case, as many clusters of documents as the number of ambiguous words in the query are retained. In order to form a unique list of documents, we fuse these sets of documents; we consider the initial values obtained by the search engine to be the document scores. Various fusion functions that can be used for this purpose have been defined in the literature (Shaw & Fox, 1995). In order to obtain a unique cluster per query, we apply the fusion function CombMNZ (Shaw & Fox, 1995).

The CombMNZ function computes the final document scores as follows:
                           
                              (4)
                              
                                 
                                    
                                       S
                                    
                                    
                                       f
                                    
                                    
                                       i
                                    
                                 
                                 =
                                 
                                    
                                       c
                                    
                                    
                                       i
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          j
                                          =
                                          0
                                       
                                       
                                          
                                             
                                                c
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                S
                                             
                                             
                                                j
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                 
                                 ,
                              
                           
                        where 
                           
                              
                                 
                                    S
                                 
                                 
                                    f
                                 
                                 
                                    i
                                 
                              
                           
                         represents the final score for each document 
                           
                              
                                 
                                    d
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    S
                                 
                                 
                                    j
                                 
                                 
                                    i
                                 
                              
                           
                         represents the score of the document 
                           
                              
                                 
                                    d
                                 
                                 
                                    i
                                 
                              
                           
                         from the cluster j (if the document 
                           
                              
                                 
                                    d
                                 
                                 
                                    i
                                 
                              
                           
                         does not exist in one particular cluster j, then 
                           
                              
                                 
                                    S
                                 
                                 
                                    j
                                 
                                 
                                    i
                                 
                              
                              =
                              0
                           
                        ), and 
                           
                              
                                 
                                    c
                                 
                                 
                                    i
                                 
                              
                           
                         represents the number of nonzero scores for each document i (
                           
                              
                                 
                                    c
                                 
                                 
                                    i
                                 
                              
                              =
                              k
                           
                         if the document 
                           
                              
                                 
                                    d
                                 
                                 
                                    i
                                 
                              
                           
                         occurs in k clusters).

We should point out that our unsupervised WSD method performs word sense discrimination and therefore does not give the actual word sense (since we do not know which cluster refers to a specific sense). However, it is not necessary to pair clusters with senses, as document clusters are sufficient for explicit automatic disambiguation in IR.

Our approach aims to improve the top retrieved document list. The first phase in the discussed method leads to a set of documents extracted by the search engine, corresponding to each query, and in the clusters of documents obtained as described in Section 4.1. Our main purpose for using a WSD technique in IR is to find the most probable relevant documents and to assign them a higher rank in the initial document list. The second phase of the method thus corresponds to a re-ranking method (Meister, Kurland, & Kalmanovich, 2011).

In our approach, the way to reach this goal is to modify the order of the retrieved documents by pushing those documents to the fore that are semantically similar to the query, as defined by the WSD results. To do this, we fuse the initial set of documents with those obtained as a result of clustering. According to the method discussed above, the documents obtained by the search engine and the set of documents obtained after clustering have different levels of importance in the final results. We therefore use a parameter to assign a weight to the fusion function.

This function has the following structure:
                           
                              (5)
                              
                                 
                                    
                                       
                                       
                                          
                                             
                                                
                                                   S
                                                
                                                
                                                   f
                                                
                                                
                                                   i
                                                
                                             
                                             =
                                             
                                                
                                                   S
                                                
                                                
                                                   1
                                                
                                                
                                                   i
                                                
                                             
                                             +
                                             α
                                             
                                                
                                                   S
                                                
                                                
                                                   2
                                                
                                                
                                                   i
                                                
                                             
                                             
                                             with
                                             :
                                          
                                       
                                    
                                    
                                       
                                       
                                          
                                             
                                                
                                                   S
                                                
                                                
                                                   1
                                                
                                                
                                                   i
                                                
                                             
                                             =
                                             score
                                             (
                                             
                                                
                                                   d
                                                
                                                
                                                   i
                                                
                                             
                                             )
                                          
                                       
                                    
                                    
                                       
                                       
                                          
                                             
                                                
                                                   S
                                                
                                                
                                                   2
                                                
                                                
                                                   i
                                                
                                             
                                             =
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               score
                                                               (
                                                               
                                                                  
                                                                     d
                                                                  
                                                                  
                                                                     i
                                                                  
                                                               
                                                               )
                                                               ,
                                                            
                                                            
                                                               if
                                                               
                                                               
                                                                  
                                                                     d
                                                                  
                                                                  
                                                                     i
                                                                  
                                                               
                                                               
                                                               exists
                                                               
                                                               in
                                                               
                                                               Clust
                                                            
                                                         
                                                         
                                                            
                                                               0
                                                               ,
                                                            
                                                            
                                                               otherwise
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                             ,
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    S
                                 
                                 
                                    f
                                 
                                 
                                    i
                                 
                              
                           
                         represents the final score of a document 
                           
                              
                                 
                                    d
                                 
                                 
                                    i
                                 
                              
                           
                        , 
                           
                              score
                              (
                              
                                 
                                    d
                                 
                                 
                                    i
                                 
                              
                              )
                           
                         represents the score of that document 
                           
                              
                                 
                                    d
                                 
                                 
                                    i
                                 
                              
                           
                         when considered in the initially retrieved document set, Clust is the document cluster containing the query itself and 
                           
                              α
                              ∈
                              [
                              0
                              ,
                              1
                              ]
                           
                         represents the weight of the clustering method for the final results.

As reported in the evaluation section (Section 5), we started with 
                           
                              α
                              =
                              0
                           
                         and then increased this parameter by 0.01 at each trial.

Finally, in Section 6, the method is used on subgroups of queries with the purpose of analyzing its behavior with regard to different types of queries. The criterion for creating these subgroups of queries is their performance after being sent to the search engine.

To evaluate the described method we have used data collections from the TREC competition. TREC (Text REtrieval Conference) is an annual workshop hosted by the US government’s National Institute of Standards and Technology which provides the necessary infrastructure for the large-scale evaluation of text retrieval methods.
                           4
                           
                              http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&tid=10667.
                        
                        
                           4
                         The TREC ad hoc tasks allow us to investigate the performance of systems that search a static set of documents using new information needs (called topics). We opted for three collections for use in the ad hoc task: TREC7, TREC8 and WT10G. For TREC7 and TREC8, the competition provided approximately 2gigabytes worth of documents and a set of 50 natural language topic statements (per collection). The documents were articles from newspapers like the Financial Times, the Federal Register, the Foreign Broadcast Information Service and the LA Times. The WT10G collection provided approximately 10gigabytes worth of Web/Blog page documents.

TREC distinguishes between a statement of information need (the topic) and the text that is actually processed by a retrieval system (the query). The TREC test collections provide topics. What is now considered the “standard” format of a TREC topic statement comprises a topic ID, a title, a description and a narrative. The title contains two or three words that represent the key words a user could have used to send a query to a search engine. The description contains one or two sentences that describe the topic area. The narrative part gives a concise description of what makes a document relevant (Voorhees & Harman, 1998). Both the descriptive and the narrative parts can offer clues about the word senses used in the title part.

In our approach, the ambiguity of a query was evaluated with reference to the title part of the topic. The ambiguous terms were detected using the WN knowledge database. If the term occurred in multiple WN synsets, then it was considered as an ambiguous term. A query is defined as ambiguous if it contains at least one ambiguous term. The queries from the three collections contained from zero to four ambiguous words, as presented in Table 1
                        , with most of them being nouns.

In TREC, ad hoc tasks are evaluated using the trec_eval package. This package provides various performance measures, including some single valued summary measures that are derived from the two basic measures in IR: recall and precision. The precision is the fraction of the retrieved documents that are relevant, while the recall represents the fraction of the documents relevant to the query that are successfully retrieved. The average precision is defined as:
                           
                              (6)
                              
                                 AP
                                 
                                    
                                       
                                          q
                                       
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             r
                                             =
                                             1
                                          
                                          
                                             R
                                          
                                       
                                       [
                                       p
                                       (
                                       r
                                       )
                                       rel
                                       (
                                       r
                                       )
                                       ]
                                    
                                    
                                       relev
                                       (
                                       q
                                       )
                                    
                                 
                              
                           
                        where 
                           
                              relev
                              (
                              q
                              )
                           
                         represents the number of documents relevant to the query 
                           
                              q
                              ,
                              R
                           
                         is the number of retrieved documents, r is the rank, 
                           
                              p
                              (
                              r
                              )
                           
                         is the precision of the top r retrieved documents and 
                           
                              rel
                              (
                              r
                              )
                           
                         equals 1 if the rth document is relevant and 0 otherwise. The MAP (Mean Average Precision) stands for the mean of the average precision scores for each query. The trec_eval package also implements the precision at certain cut-off levels. A cut-off level is a rank that defines the retrieved set. For example, a cut-off level of ten defines the retrieved set as the top ten documents in the ranked list (P@10).

This study uses three cut-off levels: P@5, P@10 and P@30, which are high precision measures.

The present study is based on runs constructed by Terrier. Terrier is an open source search engine that implements state-of-the-art indexing and retrieval functionalities. Terrier was developed at the School of Computing Science, University of Glasgow.
                           5
                           
                              http://www.terrier.org.
                        
                        
                           5
                         The sets of documents retrieved by Terrier are the first 1000 ranked documents returned by the search engine. We tried several configurations of the Terrier parameters. For each collection, we chose as our baselines the settings with the highest MAP. The MAP values for our baselines are consistent with the literature (He & Ounis, 2005; Zhong & Ng, 2012). Runs (associated with the baselines), which determine the set of documents to be used by our WSD method, were constructed as additional baselines.

The best configuration for TREC7 was the following: a two step indexation (both direct and inverted index), with active indexation by block and the use of the BB2 (parameter 
                           
                              c
                              =
                              1
                           
                        ) as a weighting model. As a query expansion model our choice was the parameter-free KL model (KLbfree). The configuration required 3 documents to be used for the query expansion. A term has to occur in two documents in order to be considered relevant. Finally, the number of terms to be added to the query for the process of query expansion was set at 10. The queries use all the three topic parts (title, descriptive and narrative). However, for TREC8 and WT10G, the parameter configuration with the best results in terms of MAP stays in place, except for two differences: the weighting model which is changed with the DFRee model (no parameters) and the narrative part of the topic which is not taken into account when the query is constructed. The values of the MAP corresponding to the best initial results for the data collections we use are briefly introduced in Table 2
                        , in addition to some collection features, such as the number of topics and the number of documents.

It is worth mentioning that, since we re-ordered an initial retrieved document list, we were unable to retrieve documents that would not have initially been retrieved (no recall improvement). We target high precision improvements.

For each query, the set of the top 1,000 documents retrieved by the best settings for Terrier was considered. These 1,000 documents are the documents considered as the most similar to the information need, sorted by their obtained score, in descending order. The method we promote aims to filter and reorder those documents before retrieving them for the user.

The target terms for the described WSD process are taken from the title part of the TREC topic only. However, our approach needs to have a context for the term. Since topic titles are generally too short, in order to form the context window for the ambiguous terms in the title, all three parts of the topic were used (title, narrative and descriptive). The stopwords from the resulting text were removed (Terrier’s stopwords list is used).

@&#RESULTS@&#

The results as compared to the baselines are presented in Fig. 1
                        . The graphs illustrate the manner in which the top levels of precision evolve with respect to the alpha parameter. Alpha is the parameter which gives a greater or smaller level of importance, in the final score, to the scores of the documents in the cluster, as presented in Section 4.2 (Eq. (5)). On the first row, the results for P@5 are shown, in comparison with the three collections. The next two rows present the results for P@10 and P@30 respectively. For each cut-off level, the vertical axis is recalibrated in order to obtain a clearer view.


                        Fig. 1 shows that the best results were obtained when the value of the alpha parameter was between 0.02 and 0.20. This observation holds for all the top levels of precision (P@5, P@10 and P@30) and for all the three collections involved in the study. p-values smaller than 
                           
                              
                                 
                                    10
                                 
                                 
                                    -
                                    6
                                 
                              
                           
                         of T-Tests have confirmed the statistical significance of our results. T-Tests have used the two following populations: the baseline value (fixed across the alpha parameter) and our results per alpha, respectively. It is also noticeable that for an alpha parameter which is greater than 0.2, the results usually fall below the baselines. Since alpha is the parameter that assigns a weight to the clustering method for the final results, one can notice that it is best for the cluster documents to participate with not more than 
                           
                              1
                              /
                              5
                           
                         in the final document score. The alpha parameter is similar to lambda in the case of RM3, since the optimal lambda values for the short queries vary between 
                           
                              0.01
                           
                         and 
                           
                              0.4
                           
                         (Zhai & Lafferty, 2004). The baselines were surpassed by the obtained results for each collection and for each top level of precision. While for P@5 and P@10 the difference between the results and the baselines is clear, for P@30 the curve representing the results remains closer to the baseline. The best improvement occurred for the WT10G collection in the case of P@10. A precision value of 0.2937 was obtained (the baseline was 0.2688), which represents an improvement of 8.48%, and is statistically significant with a p-value 
                           
                              <
                              
                                 
                                    10
                                 
                                 
                                    -
                                    6
                                 
                              
                           
                         (T-Test).

Analysis of the results has been carried out against the two major approaches existing in the literature. We compare our results with those obtained when implementing the disambiguation method proposed in Schütze and Pedersen (1995), as well as with those reported in Chifu and Ionescu (2012). The latter authors also test over the TREC7 benchmark, while employing a Naïve Bayes clustering technique. The baselines are therefore identical and represent the performance of the best runs (see Section 5.4), for all the 35 ambiguous queries considered in our study.

In the case of the terms co-occurrence-based method (Schütze & Pedersen, 1995) we have reimplemented this method and have organized the same testing setup as the one originally used by Schütze and Pedersen (1995). The stop words have been removed (Terrier stop-words list) and the target term-centered context window was set to size 40 (20 terms before the target term, 20 terms after the target term). We have identified 1,466,983 unique terms that induced the 1,466,983×1,466,983 sparse term co-occurrence matrix. We mention that, due to the high number of vocabulary terms, the co-occurrence matrix is difficult to handle from a computational point of view. The SVD for the co-occurrence matrix was set to 100 dimensions and the reduced matrix was computed using the irlba package of R
                        
                           6
                           
                              http://www.r-project.org/.
                        
                        
                           6
                         (with 100 iterations). To classify the context vectors we used the Buckshot algorithm implemented using R (packages hc and kmeans of R), with 
                           
                              10
                              %
                           
                         sampling for the initial hierarchical clustering step. The query terms that occurred less than 100 times in the corpus were not considered ambiguous since, according to its authors (Schütze & Pedersen, 1995), the method uses 
                           
                              f
                              /
                              50
                           
                         as the number of senses, with f denoting the occurrence number of the target term in the corpus. Creating context vectors for each target word occurrence, as well as reindexing after replacing words with their senses (for each set of queries), also represent time and resource consuming operations.

In Table 3
                         we present the results of our comparison in terms of high precision. Best Run represents the best run obtained with Terrier, treated as baseline (see Section 5.4) and also treated as the word-based retrieval for the terms co-occurrence-based method. Naïve Bayes represents the method from Schütze and Pedersen (1995). Spectral Clustering is the method proposed in this article. Sense-based represents the terms co-occurrence-based method of Schütze and Pedersen (1995) and CombRank represents the modified co-occurrence method, also presented in Schütze and Pedersen (1995), which considers the sum of ranks from the word-based retrieval and from the sense-based retrieval as the final rank for a retrieved document. It was reported (Schütze & Pedersen, 1995) as better than using the sense-based method alone. CMNZ-WB-SB represents the combined document list resulting from the word-based and sense-based retrievals, using the CombMNZ function (Shaw & Fox, 1995). CMNZ-WB-SB-
                        alpha represents the CMNZ-WB-SB results with the sense-based retrieved list weighted by an alpha parameter. We tested various alpha values. The best turned out to be 
                           
                              0.1
                           
                        .

The various combinations help to improve the initial performance of Sense-based results, although the baseline results (Best Run) are not surpassed. The number of the improved queries with respect to Best Run was also computed for the CMNZ-
                        
                           
                              0.1
                           
                         (which is the best alpha for co-occurrence-based results) and for the Spectral Clustering results, with the same alpha value of 
                           
                              0.1
                           
                        . The results are presented in Table 4
                        . Only very few queries are improved by the terms co-occurrence-based method.

As opposed to the Sense-based model, the peak results of Spectral Clustering outperform the Best Run baseline, for all the levels of high precision (from 1.01% to 3.73%). The average results do not overcome the baseline due to the performance decrease after a certain value of alpha (see Fig. 1).

The Naïve Bayes peak results outperform the Best Run only for P@10 and P@30 (
                           
                              0.51
                              %
                           
                         and 
                           
                              1.85
                              %
                           
                        , respectively). In addition, our method outperforms the Naïve Bayes method both on average and on peak results. The average is computed across all alpha parameter values, considering all the ambiguous queries. This again suggests the importance of the clustering technique used in unsupervised WSD for IR. We hereby conclude that spectral clustering is an appropriate clustering method for the purpose of sense discrimination in IR.

The present method was also tested for 5000 document runs, but the results were not improved. We think the reason for this is that, once more documents per run are taken into account, a significant amount of noise is also introduced and the re-ranking method cannot reach efficiency at the top level of precision (P@5, P@10 and P@30). We also considered a two cluster model in which documents could either be clustered in the query cluster if similar enough to the query, or in the non-query cluster. Results were better when as many clusters as WN senses were considered.

This section aims to deepen the analysis of the results obtained when considering two types of query clusters: those based on the query performance and those taking into account the number of ambiguous terms per query.

Following previous research showing that results can differ according to query difficulty (Bigot, Chrisment, Dkaki, Hubert, & Mothe, 2011) and with the purpose of observing where the proposed method behaves most accurately, (independently for each collection), all the results from all the three test collections were gathered into a single data set. All the runs corresponding to the 104 ambiguous queries were divided into 5 groups, according to the baseline precision (0.0–0.2, 0.2–0.4, …, 0.8–1.0) and for each of the top levels of precision being investigated (P@5, P@10 and P@30, respectively).

The P@5 results for all the intervals are depicted in Fig. 2
                        . For the queries with low or very low performance (intervals 0.0–0.2 and 0.2–0.4) we obtained significant improvements. This fact suggests that the reordering of the poorly performing list of documents retrieved by the search engine puts more relevant documents at the top of the list. On the other hand, for the queries with a good or very good performance (0.6–0.8 and 0.8–1.0), our re-ranking method could not bring more relevant documents to the fore because the search engine’s results were either already as good as possible (0.8 for the interval 0.6–0.8, or 1.0 for the interval 0.8–1.0), or very close to this. The results can hence overcome the baseline only by chance. All the comparisons are statistically significant, with the p-values 
                           
                              <
                              
                                 
                                    10
                                 
                                 
                                    -
                                    6
                                 
                              
                           
                         (T-Test for columns Baseline vs. Peak Result and Baseline vs. Average Result, respectively). The conclusions for P@5 are also consistent with P@10. The good results for P@10 can be explained by the fact that there is a higher chance of obtaining new relevant documents in a list of 10 documents than in a list of 5. However, for P@30, the improvements were not as significant as for the other top levels of precision. In order to improve the performance in a list of 30 retrieved documents it would be necessary to bring more than 1 or 2 new relevant documents from the re-ranking process. (For P@5, 1 new relevant document represents a 20% improvement, while for P@30, 1 new relevant document represents only a 3.33% improvement).

In Table 5
                         we present the peak (the best results) and average improvements for each baseline precision interval in the case of P@5 and P@10 respectively.

The queries from the data set utilized in this study contain from 0 to 4 ambiguous terms (see Table 1). A high number of ambiguous terms also suggests an increased level of query difficulty caused by multiple possible combinations of senses between terms. Keeping this aspect in mind, we investigated the behavior of our method over clusters of queries classified by the number of ambiguous terms. We proceeded as in Section 6.1 (independently for each collection) by grouping all three data sets into a single one. All of the 104 ambiguous queries were divided into three classes: queries that contain 1 ambiguous term, 2 ambiguous terms and 3 ambiguous terms respectively. Our method was not applied to the queries that contained no ambiguous terms (see Section 5.2). The population for the cluster corresponding to queries with 4 ambiguous terms was very weak (only one query) and therefore was also not taken into account.

The peak results and the percentages of improvements for each cluster, by the top levels of precision, are presented in Table 6
                        .

The highest values were obtained for the clusters of queries containing 3 ambiguous terms, which suggests that our method best improves the most ambiguous queries. For P@30 the improvement was almost 
                           
                              8
                              %
                           
                        . The results are statistically significant with p-values 
                           
                              <
                              
                                 
                                    10
                                 
                                 
                                    -
                                    6
                                 
                              
                           
                         (T-Test for columns Baseline vs. Peak Res.). It is also worth mentioning that constant improvements were also obtained for the other two clusters being investigated.

The method we propose in this paper uses all the three parts of the TREC topics, title, description and narrative (TDN), as disambiguation context. TDN implies the assumption that a context exists for the query, which is not the case in real world applications. For this reason, in this section we automatically build a context in order to validate our approach. This automatic context is not optimal (weaker performance than for TDN) and it is not optimized since our point was only to validate that our method still works with automatic context. We present the automatic contextualization method and we discuss the obtained results.

We chose a straightforward pseudo relevance feedback (PRF) approach in order to obtain the context (Attar & Fraenkel, 1977; Buckley, Salton, Allan, & Singhal, 1994). The option for this type of contextualization is motivated by the assumption that the first retrieved documents have high chances to be relevant and thus they presumably contain the target words with the correct sense.

First of all, we run retrieval on the initial query (title part of the TREC topic) over the TREC document collections and we retain the first three retrieved documents, as it was done in the baseline (see Section 5.4). This parameter value is used in query expansion models (He & Ounis, 2009) based on the assumption that, when taking into account more than five documents, the probability of treating irrelevant documents increases. Having these top documents, we concatenate the texts, we remove the stopwords and we search for the presence of at least two query terms in a moving context window of 50 words. If this presence occurs, we keep the text in the context window and add it to our context. The search for at least two query terms together is motivated by the assumption that two ambiguous words tend to disambiguate each other when found together, for example “java” and “island” (Andrews, Pane, & Zaihrayeu, 2011).

External sources such as Wikipedia were avoided when building the context due to differences in terms of actuality. Moreover, the relevance judgments were constructed considering the information in the description and narrative parts of the topic, suggesting some kind of a closed circuit. For instance, supposing that we have obtained a context with senses for target words different than the senses suggested for pooling, this would lead the evaluation of the disambiguation process to complete failure.

The usage of our PRF-based context and insights regarding the performance are presented in the following subsection.

@&#EXPERIMENTS AND RESULTS@&#

In order to prove the effectiveness of our method in the case of automatically generated context we created four TREC runs, as follows:
                           
                              •
                              
                                 Title: retrieved documents when the query represents only the title part of the topic;


                                 Title
                                 
                                 +
                                 
                                 Context: retrieved documents when the query represents the title part of the topic, together with the automatically built context;


                                 Spectral-Title: the re-ranked documents after applying the spectral clustering method, when the query is represented only by the title part of the topic;


                                 Spectral-Title
                                 
                                 +
                                 
                                 Context: the re-ranked documents after using the automatic context as WSD context for the spectral clustering method.

For few queries in each collection, our method was not able to provide any context either due to a title part of the TREC topic formed only by one term, or due to the complete nonexistence of co-occurrences of at least two terms in the context window, in the retained text. Hence, we considered only the queries containing ambiguous terms and for which the automatic method was able to provide a context, as follows: 29 out of 35 ambiguous queries in TREC7, 35 out of 35 ambiguous queries in TREC8 and 28 out of 32 ambiguous queries in WT10G, respectively.


                        Tables 7–9
                        
                        
                         provide precision values at 5, 10 and 30 retrieved documents after evaluating the above mentioned runs, for each collection. For comparison we recall the results obtained using the reformulated TD(N) runs (from Section 5). We mention that the queries without automatic context were also removed from the TD(N) evaluations, in order to maintain the same comparison basis. The best values per collection are written in bold. Statistical significance of results (T-Test between the basic Title run and Spectral-Title
                        
                        +
                        
                        Context) is also marked with asterisks in the tables (p-value 
                           
                              <
                              
                                 
                                    10
                                 
                                 
                                    -
                                    6
                                 
                              
                           
                        ).

In terms of top level precision, the Spectral-Title+context run is better than the Title run, which is better than the Title
                        
                        +
                        
                        Context run. This suggests that the generated context is harmful for the retrieval process itself but beneficial for the spectral clustering method (Spectral-Title
                        
                        +
                        
                        Context run is generally better than Spectral-Title). We believe that this is due to the amount of “noisy” terms in the context. Unlike the feature selection process using a Naïve Bayes technique (Chifu & Ionescu, 2012), spectral clustering automatically selects its useful features, hence “noise” filters out and it remains less of a problem than for the retrieval process.

We notice that in 
                           
                              89
                              %
                           
                         of cases the Spectral-Title
                        
                        +
                        
                        Context run has the greatest performance. Even if the relative improvement (0.6–6.5%) is not very high, this improvement allows us to state that our method remains effective even with automatic contextualization. In addition, the results we obtained in Section 5 show that using a better context would improve the results even more.

The least improved results are to be noticed in the case of WT10G (Table 9). Here the initial retrieval (Title only) has the lowest performance among all three considered collections, therefore the context quality decreases, since the P@3 is relatively low (TREC7: 0.5977, TREC8: 0.5619, WT10G: 0.3810). Having a poor context implies a less performing WSD process.

@&#CONCLUSIONS@&#

This paper presents a re-ranking method for IR. It shows a remarkable improvement in high rank precision for ambiguous queries. We believe that this represents a very important aspect, considering the fact that IR systems are predisposed to failure in the case of this particular type of queries (Stokoe et al., 2003; Mothe & Tanguy, 2007).

Several previous studies (Sanderson, 1994; Guyot et al., 2008) have failed to prove the usefulness of WSD in IR. On the contrary, we show that unsupervised WSD, namely WS discrimination, can improve IR results. We are of the opinion that WS discrimination is sufficient in IR and that WS disambiguation is not compulsory, as opposed to text translation, for example. Analysis of the obtained results has been carried out by us with respect to the two major approaches existing in the literature (Chifu & Ionescu, 2012; Schütze & Pedersen, 1995), as detailed in Section 5. In the only existing related approach, when using a Naïve Bayes-based clustering technique, Chifu and Ionescu (2012) also demonstrated that WS discrimination can improve IR performance. However, in their work they only recorded very small improvement and only on some sub-cases, hence the importance of the clustering technique used for WS discrimination in IR, another point which we have made here.

Our method exploits TREC topic definitions which have a level of detail in their description, a level which is not normally available in an IRS; the topic definition is used to provide a context to the query. Other works from the literature use these structural elements and most often the Descriptive part of the query helps in improving the results (He & Ounis, 2004). However, using the complete statement of the topic could lead to valid criticism of experiments such as ours because we exploit this detail. In this paper, we were interested in proving that, if this level of detail is available, then it can be used in a beneficial way to improve retrieval effectiveness, and we have achieved this goal. However, even if our goal was not to develop mechanisms which can capture in an optimal way the needed level of detail, we do propose a method to capture the context of the query and show that our own method for WS discrimination in IR remains useful. Such a method of contextualization, namely the usage of PRF (Buckley et al., 1994), has been employed by us in Section 7 for validating our conclusions in the presence of automatically generated context. Indeed, contextualizing short texts, such as tweet contextualization (SanJuan, Bellot, Moriceau, & Tannier, 2011) and query expansion (Ogilvie, Voorhees, & Callan, 2009) is an active research domain and we think it will be worth considering new contextualization techniques in our WS discrimination method as a future goal.

Our future work will also concentrate on query difficulty prediction, which is already an active research area (Carmel & Yom-Tov, 2010; Mothe & Tanguy, 2005; Pehcevski, Thom, Vercoustre, & Naumovski, 2010). The fact that our method rather improves poor performing queries (Section 6.1), especially those with multiple ambiguous terms (Section 6.2), should drive in-depth research along this path.

@&#ACKNOWLEDGEMENTS@&#

The authors would like express their gratitude to Taoufiq Dkaki from the University of Toulouse and Radu Ionescu from University of Bucharest for their useful comments and discussions, as well as to the ANR agency who partially funded this work.

@&#REFERENCES@&#

