@&#MAIN-TITLE@&#FoDoSu: Multi-document summarization exploiting semantic analysis based on social Folksonomy

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A novel multi-document summarization system that employs the tag cluster on Flickr.


                        
                        
                           
                           The FoDoSu detects meaningful words by exploiting tag cluster for summarizing multi-documents.


                        
                        
                           
                           We demonstrate the superiority of FoDoSu through experiments on TAC2008 and TAC2009.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Multi-document summarization

Folksonomy

Tag cluster

Semantic analysis

@&#ABSTRACT@&#


               
               
                  Multi-document summarization techniques aim to reduce documents into a small set of words or paragraphs that convey the main meaning of the original document. Many approaches to multi-document summarization have used probability-based methods and machine learning techniques to simultaneously summarize multiple documents sharing a common topic. However, these techniques fail to semantically analyze proper nouns and newly-coined words because most depend on an out-of-date dictionary or thesaurus. To overcome these drawbacks, we propose a novel multi-document summarization system called FoDoSu, or Folksonomy-based Multi-Document Summarization, that employs the tag clusters used by Flickr, a Folksonomy system, for detecting key sentences from multiple documents. We first create a word frequency table for analyzing the semantics and contributions of words using the HITS algorithm. Then, by exploiting tag clusters, we analyze the semantic relationships between words in the word frequency table. Finally, we create a summary of multiple documents by analyzing the importance of each word and its semantic relatedness to others. Experimental results from the TAC 2008 and 2009 data sets demonstrate the improvement of our proposed framework over existing summarization systems.
               
            

@&#INTRODUCTION@&#

The rapid growth of the Internet and smart multimedia devices such as smart phones and tablet PCs allow users to find information easily through diverse media (e.g., documents, images, videos, and music). In particular, in the Web 2.0 environment, users can obtain general and common information from a Folksonomy system, such as Wikipedia, Flickr, or del.ici.ous. Folksonomy systems are classification systems derived from social tags assigned to multimedia content by everyday users. Web 2.0 has led to the development of large Web-based communities that support and facilitate collaboration among Internet users (Huang, Lin, & Chan, 2012). Folksonomies became popular on Web 2.0 as part of social tagging applications, such as social bookmarking and image annotations uploaded by users. Tagging, which is a defining characteristic of Web 2.0 services, allows users to collectively classify and find information. Some websites include tag clouds as a way to visualize tags in a Folksonomy. The most important and distinctive feature of the Folksonomy system is the creation of content by general users without restriction. Users are now able to create, share, and search multimedia content anytime and anywhere using their smart devices. As a result, the number of documents (news, blogs, Web pages, e-mails, etc.) created on the Web has been rapidly increasing day by day. In this environment, to find the necessary information, users must manually review all of the searched documents without any assistance from search engines, which requires too much time and effort. To address this problem, various document summarization techniques have been studied to efficiently summarize the core of a single original document. More recently, multi-document summarization techniques have been researched to summarize multiple documents sharing a common topic at the same time. This technique allows users to easily understand a topic by looking at a short summary.

Over the past few years, multi-document summarization has received more attention and made much progress. However, various evaluations indicate that multi-document summarization is highly complex and demanding, and there is still much work to be done before automatic summarizers catch up with human beings (Dang & Owczarzak, 2008). Most existing multi-document summarization techniques analyze semantic relationships between words in a document by exploiting probability theory, machine learning techniques, and external knowledge-bases such as WordNet.
                        1
                        
                           http://wordnet.princeton.edu/.
                     
                     
                        1
                      However, these techniques suffer from high computational costs in the learning and summarization processes. Furthermore, WordNet-based approaches fail to analyze proper nouns (e.g., a person’s name or the name of a product or firm) and newly-coined words because these words are not present in WordNet.

In this paper, we propose a novel multi-document summarization system, called FoDoSu (Folksonomy-based Multi-Document Summarization), that employs the tag clusters used in Folksonomy systems to detect key sentences in multiple documents. For this purpose, we exploit the tag clusters used by Flickr, one of the most representative Folksonomy systems, for summarizing multiple documents. Using tag clusters, we analyze the importance of each word and the semantic relatedness among them, and finally make a summary of the documents.

The rest of this paper is organized as follows: In Section 2, we briefly review the related work. The proposed multi-document summarization system is presented in Section 3. In Section 4, we describe the experimental results using the TAC 2008 and 2009 datasets. Finally, we conclude our work in Section 5.

@&#RELATED WORK@&#

In Section 2.1, we first review the related work on various multi-document summarization techniques. Then, we exploit the HITS (Kleinberg, 1999) algorithm for analyzing the semantics and contributions of words in documents. Therefore, we briefly introduce HITS for a better understanding of the algorithm in Section 2.2.

In the literature, the development of multi-document summarization has been largely promoted by the Document Understanding Conferences (DUC)
                           2
                           
                              http://duc.nist.gov/.
                        
                        
                           2
                         and Text Analysis Conferences (TAC).
                           3
                           
                              http://www.nist.gov/tac/.
                        
                        
                           3
                        
                     

Multi-document summarization techniques can be classified into two approaches. One is the extractive summarization approach and the other is the abstractive summarization approach (Mani, 2001). The extractive summarization approach involves assigning saliency scores to some units (e.g., sentences, paragraphs) of the documents and extracting those units with the highest scores. In contrast, the abstractive summarization approach takes the essence of the source document to build a summary by using natural language processing techniques. Although the abstraction-based method can summarize a document more accurately than the extraction-based method, it is much more difficult and complex because it requires the use of high-cost natural language processing technologies, such as information fusion (Barzilay, McKeown, & Elhadad, 1999), sentence compression (Knight & Marcu, 2002), and reformulation (McKeown, Klavans, Hatzivassiloglou, Barzilay, & Eskin, 1999).

Traditionally, research on extractive summarization is based on the position of a sentence in a document which measures the overall frequency of the words they contain such as the TF-IDF technique (Luhn, 1958; Edmundson, 1969; Brandow, Mitze, & Rau, 1995). However, this technique only measures word-frequency in the documents, and does not consider the semantics of the relationships between the words and sentences (Chali, Hasan, & Joty, 2011).


                        Hennig and Labor (2009) proposed a multi-document summarization method based on Probabilistic Latent Semantic Analysis (PLSA), which represents sentences and queries as probability distributions over latent topics. They combine query-focused features and thematic sentence features into an overall sentence score. Arora and Ravindran (2008) used Latent Dirichlet Allocation (LDA) to capture the topics of the given documents and form a summary with sentences representing these topics. Wan and Yang (2008) proposed novel summarization models to make use of a theme cluster in the document set. The proposed model incorporates the cluster information in a conditional Markov random walk model. Authors detect theme clusters in the document set using popular clustering algorithms, such as Kmeans Clustering, Agglomerative Clustering, and Divisive Clustering. Although these models perform well, they require a high processing cost and more time when performing the clustering algorithm in the preprocessing step. The probability method employed also requires complex computation.


                        Wan (2008) also proposed a document summarization model using the Hypertext Induced Topic Search (HITS) algorithm. He considers the documents and sentences to be hubs and authorities respectively, and then the HITS algorithm is applied on the document-to-sentence bipartite graph to compute the saliency scores of the sentences. As we will explain in next section, our approach also uses the HITS algorithm. However, we consider the words in the document to be hubs and the tag clusters to be authorities, and then exploit them to analyze the semantics of the word.


                        Dang and Luo (2008) proposed a method to detect key sentences using keyword extraction based on statistics and synsets. They used WordNet for extracting the most relevant sentences from the original document. However, it is not easy to analyze the relationship between the semantics of the words in Web documents because there are many proper nouns and newly-coined words in the original documents which are not defined in WordNet.


                        Zhu et al. (2009) proposed a tag-oriented Web document summarization approach using both the document itself and the tags annotated on the document. This approach limits the semantic analyses between words in the document because it only analyzes the relationships between users and tags in the Folksonomy system.

To overcome these drawbacks, we propose a novel multi-document summarization system FoDoSu that employs tag clusters of the Folksonomy system to detect the key sentences in multiple documents. For this purpose, we exploit the tag clusters used by Flickr, one of the most representative Folksonomy systems, for summarizing multiple documents. Using tag clusters, we analyze the importance of each word as well as the semantic relatedness among them, and make a summary.

The HITS algorithm is a very popular and effective algorithm for link analysis between web pages that has been successfully used to improve web retrieval. The HITS algorithm classifies each Web page as a hub or an authority. An authority is a page with many in-links. Fig. 1
                         shows an authority page and a hub page. The idea is that an authority page may have good or authoritative content on some topic and thus many people trust it and link to it. In contrast, a hub is a page with many out-links. The hub page serves as an organizer of the information on a particular topic and points to many good authority pages on a given topic.

Authorities and hubs are assigned respective scores. The two types of scores are extracted by iterations consisting of the following two operations. The scores are computed in a mutually reinforcing way, and the algorithm iteratively proceeds as follows:
                           
                              
                                 
                                    
                                       a
                                    
                                    
                                       p
                                    
                                 
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          q
                                          ∈
                                          B
                                          (
                                          p
                                          )
                                       
                                    
                                 
                                 
                                    
                                       h
                                    
                                    
                                       q
                                    
                                 
                                 
                                 
                                    
                                       h
                                    
                                    
                                       p
                                    
                                 
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          q
                                          ∈
                                          F
                                          (
                                          p
                                          )
                                       
                                    
                                 
                                 
                                    
                                       a
                                    
                                    
                                       q
                                    
                                 
                                 .
                              
                           
                        
                     

Let ap
                         and hp
                         represent the authority and hub scores of page p, respectively. B(p) and F(p) denote the set of referrer and reference pages of page p, respectively. Therefore, authorities and hubs exhibit what could be called mutually reinforcing relationships. A good hub represents a page that points to many good authorities, and a good authority represents a page that is linked to by many good hubs. In this paper, we consider the word in the document as a hub and the tag clusters as authorities to exploit the HITS algorithm for analyzing the semantics of the words.


                     Fig. 2
                      shows the framework of our multi-document summarization system FoDoSu. Given multiple documents that need to be summarized, we first perform a pre-processing step so that the documents can be analyzed at different granularities (i.e., word-level and sentence-level). Afterwards, the words analysis module calculates the word frequency and analyzes the semantic importance of each word by exploiting the tag clusters from Flickr. Then we compute the contribution of each word using the HITS algorithm. The computed semantic importance score and contribution are used for rating each sentence in multiple documents. Finally, the sentence analysis module generates the final summary of the documents by selecting the top n ranked sentences. The main phase of the framework will be described in the following subsection.

The preprocessing module extracts sentences from the input documents, and then performs tokenization. Next, the stop words are removed using a popular stop word list.
                           4
                           
                              http://www.lextek.com/manuals/onix/stopwords1.html.
                        
                        
                           4
                         A set of words generated after preprocessing is used by the word analysis module to compute the semantic importance and contribution of each word in the document. Also, the extracted sentences are given a weighted score based on the importance and contributions of the words in each sentence.

To analyze how much each word contributes to the document, we construct a Word Frequency Table (WFT) using tag clusters from Flickr, and calculate the contribution of each word in the WFT by exploiting the HITS algorithm. To build the WFT, in Section 3.2.1, we count all the words in the documents, and then find the words with strong semantic relationships using their tag clusters from Flickr. If words with strong semantic relationships are found, the WFT is updated to WFT′. In Section 3.2.2, we calculate the contribution of each word using the HITS algorithms with WordCluster. The WordCluster consists of a set of the high relationship words in the WFT′. The following sections explain more specifically how to construct the WFT′ and analyze the semantics of each word.

First, we construct a WFT to calculate the frequency of each word in the documents, which can be represented as
                              
                                 (1)
                                 
                                    WFT
                                    =
                                    
                                       
                                          {
                                          (
                                          
                                             
                                                w
                                             
                                             
                                                1
                                             
                                          
                                          ,
                                          
                                             
                                                c
                                             
                                             
                                                1
                                             
                                          
                                          )
                                          ,
                                          (
                                          
                                             
                                                w
                                             
                                             
                                                2
                                             
                                          
                                          ,
                                          
                                             
                                                c
                                             
                                             
                                                2
                                             
                                          
                                          )
                                          ,
                                          …
                                          ,
                                          (
                                          
                                             
                                                w
                                             
                                             
                                                n
                                             
                                          
                                          ,
                                          
                                             
                                                c
                                             
                                             
                                                n
                                             
                                          
                                          )
                                          }
                                       
                                       
                                          T
                                       
                                    
                                    .
                                 
                              
                           
                        

In Eq. (1), wi
                           
                           =(w
                           1, w
                           2,…,
                           wn
                           ) is the i-th word in the documents, and ci
                           
                           =(c
                           1, c
                           2,…,
                           cn
                           ) is the frequency of the i-th word. After construction of the WFT, it is sorted by the frequency c. However, it is hard to be certain that a word with high frequency is important in the documents. Therefore, in addition to the frequency, we exploit the tag cluster, a Folksonomy system that can be obtained from Flickr, to analyze the semantics of the words. AFolksonomyis a system of classification derived from the practice ofcollaborativelycreating and managing tagsannotated by users. Flickr, a popular collaborative tagging application for pictures, provides tag clusters which are groups consisting of a tag and its related tags. To achieve this, the Flickr system statistically measures which tags coincide with other tag words when a user tags media. Although these tag clusters are continuously updated and refined, there is still a gap between the tags and the actual content of the images (Liu, Hua, Wang, & Zhang, 2010). For example, when exploring Flickr tag clusters for the word “apple”, four different categories are returned: laptop, fruit, smartphone, and NYC (Barbuto, Contaldi, & Senatore, 2012). However, when analyzing the semantics of the words, there are many proper nouns and newly-coined words in the documents such as the names of people and products. It is hard to analyze the semantics of these words using WordNet because it does not cover proper nouns and newly-coined words. For this reason, we use the Flickr tag cluster instead of WordNet when analyzing the semantics of proper nouns and newly-coined words.


                           Table 1
                            shows an example of a tag cluster from Flickr. A tag cluster consists of words highly related to a given tag. This has the benefit of providing useful information for the semantic analysis of words. As shown in Table 1, the tag “airbus” has two categories. The first category has highly semantically related words such as “airport”, “plane” and “a380”. On the other hand, the second category has less related words such as “heathrow” and “London”. Also, the tag cluster “a380” has one category that includes many semantically related words such as “airbus”, “aircraft”, “plane”, and “aviation”. As this example shows, we can identify a strong semantic association between the tags “airbus” and “a380”. However, the tag clusters for “France” have four different categories with a variety of meanings, and they have no words semantically related to “airbus” or “a380”. The additional benefit of tag clusters comes from the fact that they cover most words, including proper nouns and newly-coined words that are not defined in WordNet. For instance, the word “a380” is the model number of the airbus, which is a kind of aircraft, but is not defined in WordNet. Therefore, it is hard to analyze the semantics of the word “a380” using WordNet.


                           Fig. 3
                            shows the procedure used to construct a WFT. Given the initial word frequency table, we obtain tag clusters of each word in the WFT from Flickr. For example, the tag cluster of the word “airbus” contains social tags such as “airport”, “plane”, and “a380”. This indicates that “airport”, “plane”, and “a380” have a high semantic closeness with “airbus” in the tag space. Then, we collect every tag that corresponds to each word existing in the original WFT and count the frequency of the collected tags to update the frequency of each word in the WFT. The tags in each tag cluster are also stored together in the updated word frequency of the WFT. Although FoDoSu seems to gather tag clusters with different meanings for a given word, these tags are filtered out before analyzing the semantics of the words if they do not exist in the original WFT. For example, in Table 1, the word “airbus” has two tag categories. The first category has highly semantically related words such as “airport”, “plane”, “a380”, and so on. Whereas, the second category has less related words such as “heathrow” and “London”. These words are filtered out and not used because they do not exist in the original WFT. Likewise, in Fig. 3, FoDoSu does not gather the tag cluster “France” for analyzing semantics because the words in the tag cluster “France” do not exist in the original WFT. The updated word frequency table (WFT′) in Fig. 3 shows the final table generated by this procedure, in which the count and WordCluster information is updated. The final word frequency table WFT’ is represented as follows:
                              
                                 (2)
                                 
                                    
                                       
                                          WFT
                                       
                                       
                                          ′
                                       
                                    
                                    =
                                    
                                       
                                          {
                                          (
                                          
                                             
                                                w
                                             
                                             
                                                1
                                             
                                          
                                          ,
                                          
                                             
                                                c
                                             
                                             
                                                1
                                             
                                          
                                          ,
                                          
                                             
                                                wc
                                             
                                             
                                                1
                                             
                                          
                                          )
                                          ,
                                          (
                                          
                                             
                                                w
                                             
                                             
                                                2
                                             
                                          
                                          ,
                                          
                                             
                                                c
                                             
                                             
                                                2
                                             
                                          
                                          ,
                                          
                                             
                                                wc
                                             
                                             
                                                2
                                             
                                          
                                          )
                                          ,
                                          …
                                          ,
                                          (
                                          
                                             
                                                w
                                             
                                             
                                                n
                                             
                                          
                                          ,
                                          
                                             
                                                c
                                             
                                             
                                                n
                                             
                                          
                                          ,
                                          
                                             
                                                wc
                                             
                                             
                                                n
                                             
                                          
                                          )
                                          }
                                       
                                       
                                          T
                                       
                                    
                                    ,
                                 
                              
                           where wci
                            is the WordCluster of wi. WFT′ is used to analyze the contribution of each word in the documents in Section 3.3.

To calculate the contribution of each word in the WFT’, we apply the Hypertext Induced Topic Search (HITS) algorithm to our system. Originally, the HITS was a type of link analysis algorithm to rate Web pages. The HITS algorithm classifies each Web page as a hub or authority. A good hub is a page that points to many good authorities, and a good authority is a page that is linked to by many good hubs. To analyze the contribution of each word, we consider the WordCluster as an authority, and the words in the WFT′ as a hub.


                           Fig. 4
                            shows the relationship between each WordCluster and each word in the WFT′ as an authority and a hub in the HITS algorithm. In an authority, the WordCluster of a particular word includes words in the WFT′ which have a high semantic relationship to it. A hub contains a particular word included in the WordCluster of the other word in the WFT′. The HITS calculates a contribution score for every word in the WFT′. Eq. (3) shows the formula for calculating the HITS of word wi
                           ,
                              
                                 (3)
                                 
                                    HITS
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          i
                                       
                                    
                                    )
                                    =
                                    a
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          i
                                       
                                    
                                    )
                                    +
                                    h
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          i
                                       
                                    
                                    )
                                    ,
                                 
                              
                           where a(wi
                           ) and h(wi
                           ) are the authority score and hub score of word wi
                           . The above equations can be written as
                              
                                 (4)
                                 
                                    a
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          i
                                       
                                    
                                    )
                                    =
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             
                                                
                                                   w
                                                
                                                
                                                   i
                                                
                                             
                                             ,
                                             
                                                
                                                   w
                                                
                                                
                                                   j
                                                
                                             
                                             ∈
                                             
                                                
                                                   WFT
                                                
                                                
                                                   ′
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          Auth
                                       
                                       
                                          
                                             
                                                w
                                             
                                             
                                                i
                                             
                                          
                                          ,
                                          
                                             
                                                w
                                             
                                             
                                                j
                                             
                                          
                                       
                                    
                                    
                                    
                                       
                                          Auth
                                       
                                       
                                          
                                             
                                                w
                                             
                                             
                                                i
                                             
                                          
                                          ,
                                          
                                             
                                                w
                                             
                                             
                                                j
                                             
                                          
                                       
                                    
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      1
                                                   
                                                
                                                
                                                   
                                                      0
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             if
                                             
                                             
                                                
                                                   w
                                                
                                                
                                                   j
                                                
                                             
                                             ∈
                                             
                                                
                                                   Cluster
                                                
                                                
                                                   
                                                      
                                                         w
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             Otherwise
                                          
                                       
                                    
                                 
                              
                           
                           
                              
                                 (5)
                                 
                                    h
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          i
                                       
                                    
                                    )
                                    =
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             
                                                
                                                   w
                                                
                                                
                                                   i
                                                
                                             
                                             ,
                                             
                                                
                                                   w
                                                
                                                
                                                   j
                                                
                                             
                                             ∈
                                             
                                                
                                                   WFT
                                                
                                                
                                                   ′
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          Hub
                                       
                                       
                                          
                                             
                                                w
                                             
                                             
                                                i
                                             
                                          
                                          ,
                                          
                                             
                                                w
                                             
                                             
                                                j
                                             
                                          
                                       
                                    
                                    
                                    
                                       
                                          Hub
                                       
                                       
                                          
                                             
                                                w
                                             
                                             
                                                i
                                             
                                          
                                          ,
                                          
                                             
                                                w
                                             
                                             
                                                j
                                             
                                          
                                       
                                    
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      1
                                                   
                                                
                                                
                                                   
                                                      0
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             if
                                             
                                             
                                                
                                                   w
                                                
                                                
                                                   i
                                                
                                             
                                             ∈
                                             
                                                
                                                   Cluster
                                                
                                                
                                                   
                                                      
                                                         w
                                                      
                                                      
                                                         j
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             Otherwise
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        

Eq. (4) shows an authority measurement of the word wi
                           . If word wj
                            in the WFT′ exists in the WordCluster of word wi
                           , the authority of a(wi
                           ) increases by 1, otherwise it is zero. Then the number of words in the WFT′ that have words in the WordCluster of word wi
                            is summed. The authority of word wi
                            has a high score, indicating that the WordCluster of the word wi
                            is an important word in a given document because there are many words that have a high semantic relationship with it. In contrast, Eq. (5) shows a hub measuring the word wi
                            for the case that the WordCluster of wj
                            consists of word wi
                            in the WFT′. Then, the hub score h(wi
                           ) increases by 1, otherwise it is zero. The number of words in the WFT′ having words wj
                            in the WFT′ is summed. Analysis of the hub of word wi
                            is quite similar to the analysis of the authority of wi
                           . If the hub of wi
                            has a high score, the WordCluster of wi
                            makes a high contribution to the given document because it refers to many other words in the WFT′ that have a high semantic relationship with word wi
                           . Finally, the HITS of each word represents its importance and contribution to a given document by summing its authority and hub scores.

In Section 3.2, we analyzed the contribution of words by constructing a WFT′ using tag clusters from Flickr and exploiting the HITS algorithm to summarize the documents. After analyzing the importance and contribution of each word, we calculate the sentence score and rank each sentence with WordCluster. To calculate the sentence score, we define the rel-gram as a contiguous sequence of k words without duplicates.


                        Table 2
                         shows the difference between the N-gram and rel-gram. The N-gram considers the sequence of words. Thus, when k
                        =2, “AB” and “BA” are different. However, in the case of the rel-gram, they are the same 2-gram because the rel-gram does not consider the sequence of words.


                        Fig. 5
                         shows an illustrative example of computing the relationship between words. Assume that there are three words, “A”, “B”, and “C”, and each word has its own WordCluster, “ClusterA
                        ”, “ClusterB
                        ”, and “ClusterC
                        ”. As shown in Fig. 5, ClusterA
                         includes word “B”, but does not include word “C”. ClusterB
                         includes both words “A” and “B”, while ClusterC
                         contains no words. In this case, words “A” and “B” have a high semantic relationship because their WordClusters include both words. However, words “B” and “C” have a medium semantic relationship because only ClusterB
                         includes “C”. Words “A” and “C” have no relationship because their WordClusters do not include each other. We calculate the semantic relationship between the rel-grams as follows:
                           
                              (6)
                              
                                 Rel
                                 (
                                 rel
                                 -
                                 gram
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          ,
                                          j
                                          ∈
                                          
                                             
                                                WFT
                                             
                                             
                                                ′
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       Relate
                                    
                                    
                                       i
                                       ,
                                       j
                                    
                                 
                              
                           
                        where
                           
                              (7)
                              
                                 
                                    
                                       Relate
                                    
                                    
                                       i
                                       ,
                                       j
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   1
                                                
                                             
                                             
                                                
                                                   0
                                                
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       
                                          if
                                          
                                          
                                             
                                                w
                                             
                                             
                                                j
                                             
                                          
                                          ∈
                                          
                                             
                                                Cluster
                                             
                                             
                                                
                                                   
                                                      w
                                                   
                                                   
                                                      i
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          Otherwise
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

We then compute the score of a sentence as follows:
                           
                              (8)
                              
                                 Score
                                 (
                                 s
                                 ,
                                 rel
                                 -
                                 gram
                                 )
                                 =
                                 
                                    
                                       
                                          α
                                          ∗
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   i
                                                   ∈
                                                   
                                                      
                                                         WFT
                                                      
                                                      
                                                         ′
                                                      
                                                   
                                                
                                             
                                          
                                          TF
                                          ·
                                          IDF
                                          (
                                          
                                             
                                                w
                                             
                                             
                                                i
                                             
                                          
                                          )
                                       
                                    
                                 
                                 +
                                 
                                    
                                       
                                          β
                                          ∗
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   i
                                                   ∈
                                                   
                                                      
                                                         WFT
                                                      
                                                      
                                                         ′
                                                      
                                                   
                                                
                                             
                                          
                                          HITS
                                          (
                                          
                                             
                                                w
                                             
                                             
                                                i
                                             
                                          
                                          )
                                       
                                    
                                 
                                 +
                                 γ
                                 ∗
                                 Rel
                                 (
                                 rel
                                 -
                                 gram
                                 )
                                 ,
                              
                           
                        where s is the sentence that includes the rel-gram, TF
                        ⋅
                        IDF(wi
                        ) is value of TF-IDF of wi
                        , HITS(wi
                        ) is the contribution score for each word, Rel(rel-gram) is the score of the semantic relationship between rel-grams, and “α”, “β”, and “γ” are the weights of each term (where α
                        +
                        β
                        +
                        γ
                        =1).


                        
                           
                              
                                 
                                 
                                    
                                       
                                          
                                             Algorithm 1 Sentence Scoring
                                       
                                    
                                    
                                       
                                          Input: multi-documents
                                       
                                    
                                    
                                       
                                          Output: summarized documents
                                       
                                    
                                    
                                       
                                          1:
                                          
                                          
                                             for 
                                             n
                                             =1 to k 
                                             // k
                                             
                                             =
                                             the number of rel-gram
                                       
                                    
                                    
                                       
                                          2:
                                          
                                          
                                          if(n == 1)
                                       
                                    
                                    
                                       
                                          3:
                                          
                                          
                                          
                                          take one word w in WFT’
                                          
                                       
                                    
                                    
                                       
                                          4:
                                          
                                          
                                          
                                          
                                             for 
                                             each sentence s
                                          
                                       
                                    
                                    
                                       
                                          5:
                                          
                                          
                                          
                                          
                                          find s includes w
                                          
                                       
                                    
                                    
                                       
                                          6:
                                          
                                          
                                          
                                          
                                          ScoreTable=Score(s,w)
                                       
                                    
                                    
                                       
                                          7:
                                          
                                          
                                          
                                          
                                             end for
                                          
                                       
                                    
                                    
                                       
                                          8:
                                          
                                          
                                          else
                                       
                                    
                                    
                                       
                                          9:
                                          
                                          
                                          
                                          compute rel-gram in WFT’
                                          
                                       
                                    
                                    
                                       
                                          10:
                                          
                                          
                                          
                                          for each sentence s
                                          
                                       
                                    
                                    
                                       
                                          11:
                                          
                                          
                                          
                                          
                                          find s includes rel-gram
                                       
                                    
                                    
                                       
                                          12:
                                          
                                          
                                          
                                          
                                          ScoreTable=Score(s,rel-gram)
                                       
                                    
                                    
                                       
                                          13:
                                          
                                          
                                          
                                          
                                             end for
                                          
                                       
                                    
                                    
                                       
                                          14:
                                          
                                          
                                          
                                          
                                             end if
                                          
                                       
                                    
                                    
                                       
                                          15:
                                          
                                          
                                          
                                             end for
                                          
                                       
                                    
                                    
                                       
                                          16:
                                          
                                          
                                          Sort(ScoreTable) by SentenceScore
                                       
                                    
                                 
                              
                           
                        Algorithm 1 shows the procedure for computing the score of a sentence in a given document. The importance of a sentence is determined by its corresponding score, where s is a sentence in the given multi-document. First, Algorithm 1 calculates the score of each sentence s that includes a single word (lines 2–7). Then, it calculates the scores of sentences that contain k rel-grams (lines 9–13). The sentences are stored in a Score Table along with their sentence scores (line 6 and line 12). To summarize multiple documents, these sentences are sorted by their scores to select those that contribute the most (line 16). The most remarkable feature of our sentence scoring method is that it considers the semantic relationships among the words in a sentence.


                        Fig. 6
                         illustrates an example of Algorithm 1 in the FoDoSu system. For pre-processing, the FoDoSu system splits the sentences in the given documents and constructs the WFT′ using the words in the documents and tag clusters from Flickr. Algorithm 1 assigns a score to each sentence by calculating its frequency, the HITS of the words it contains, and by computing the relationships between words. The FoDoSu system summarizes documents using sentences assigned the highest scores. As shown in Fig. 6, Sentence 6 from the input documents was assigned the highest sentence score by Algorithm 1. Sentence 6 contains “airbus”, “a380”, and “plane”, which are words included in the WFT′. The FoDoSu system calculated the frequency and the HITS score of “airbus”, “a380”, and “plane”, and computed the relationships between the words. Sentences 25 and 70 were assigned the second and the third highest scores using the same process. Finally, our system generated a final summary of the given documents based on the top n scored sentences in the Score Table.

@&#EXPERIMENTS@&#

In this section, we present the results of our evaluation of multi-document summarization. All experiments were done on a core 2 personal computer equipped with a dual-core Intel E8400 @, 3.00GHz CPU, 8 gigabytes RAM, and 1 terabyte hard disk which had been used in a desk-top system. The program was implemented using Eclipse, which can be used to develop applications in JAVA on a Microsoft Windows 7 PC.

We used the TAC 2008 and TAC 2009 data sets to test our proposed method empirically. Both data sets are open benchmark data sets from the Text Analysis Conference (TAC) for automatic summarization evaluation. TAC 2008 provides 48 document sets, and TAC 2009 provides 44 document sets. We used the ROUGE (Lin & Hovy, 2003) toolkit for evaluations, which has been widely adopted by the Document Understanding Conference (DUC) for automatic summarization evaluation. It measures the quality of document summarization by counting overlapping units such as n-grams, word sequences, and word pairs between the candidate summary (a summary by summarization techniques) and the reference summary (a summary by human experts). Several automatic evaluation methods are implemented in ROUGE, such as ROUGE-1 (recall against unigrams), ROUGE-2 (recall against bigrams), ROUGE-L (longest common subsequence), and ROUGE-SU (skip bigram plus unigram). ROUGE-N is an n-gram recall measure computed as follows:
                           
                              (9)
                              
                                 ROUGE
                                 -
                                 N
                                 =
                                 
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             S
                                             ∈
                                             {
                                             Ref
                                             }
                                          
                                       
                                       
                                          
                                             ∑
                                          
                                          
                                             n
                                             -gram
                                             ∈
                                             S
                                          
                                       
                                       
                                          
                                             Count
                                          
                                          
                                             match
                                          
                                       
                                       (
                                       n
                                       -gram
                                       )
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             S
                                             ∈
                                             {
                                             Ref
                                             }
                                          
                                       
                                       
                                          
                                             ∑
                                          
                                          
                                             n
                                             -
                                             garm
                                             ∈
                                             S
                                          
                                       
                                       Count
                                       (
                                       n
                                       -gram
                                       )
                                    
                                 
                                 .
                              
                           
                        
                     

In Eq. (9), n is the length of the n-gram, and Ref stands for the reference summaries, Countmatch
                        (n-gram) is the maximum number of n-grams co-occurring in a candidate summary and the set of reference summaries. Count (n-gram) is the number of n-grams in the reference summaries. ROUGE metrics at the 95% confidence level are computed by running ROUGE-1.5.5, where ROUGE-2 and ROUGE-SU4 are automatic ROUGE evaluation scores. We show ROUGE-2 and ROUGE-SU4 metrics in the experimental results.

In this section, we optimize the three parameters, α, β, and γ, used in Eq. (8) and Algorithm 1. To investigate how the combined weights influenced summarization performance, we varied the weights of α, β, and γ in Eq. (8) from 0 to 1. Figs. 7–10
                        
                        
                        
                         show the ROUGE-2 and ROUGE-SU4 on the TAC 2008 and TAC 2009 datasets, respectively. In all figures, the values of ROUGE-2 and ROUGE-SU4 gradually increase until the value of α reaches 0.3. In case of TAC 2008, both ROUGE-2 and ROUGE-SU4 reach the peak at α
                        =0.3, β
                        =0.6, and γ
                        =0.1. On the other hand, in case of TAC 2009, both ROUGE-2 and ROUGE-SU4 reach the peak α
                        =0.3, β
                        =0.5, and γ
                        =0.2. This relationship indicates that both the first term α (TF-IDF) and the second term β (HITS) have great impact on the summarization performance. We can also see that the quality of document summarization is mainly influenced by the contributions of words and the semantic relationships between them.

In the second experiment, we set the weights α, β, and γ to the values that showed the best performance in the previous experiment on TAC 2008 (α
                        =0.3, β
                        =0.6, γ
                        =0.1). Then, we varied the value k of the rel-gram from 3 to 5. We also varied the number of words for which the rel-gram was computed from 10 to 20. Figs. 11
                         and 12
                         demonstrate the effect of changing the number of rel-grams on the summarization. On TAC 2008, our proposed system showed good performance when k
                        =3 and the number of words was 15.

We set the weights α, β, and γ to the values that showed the best performance in the previous experiment on TAC 2009 (α
                        =0.3, β
                        =0.5, γ
                        =0.2), and varied the value of k in the rel-gram from 3 to 5. We also varied the number of words for which the rel-gram was computed from 10 to 20. Figs. 13
                         and 14
                         demonstrate the effect of changing the number of rel-grams on summarization. Using TAC 2009, our proposed system showed good performance when k
                        =4 and the number of words was 10.

The proposed system exploits the tag clusters used in Folksonomy to analyze the semantics of the words with low computational cost. Therefore, it is necessary to demonstrate how the tag cluster affects document summarization. That is, to investigate how the HITS algorithm helps the summarization performance of FoDoSu, we have investigated the following three cases. In the first case, we considered only the frequencies of words in the documents (OnlyFREQ). In the second case, we considered only TF-IDF of words in the documents (TF-IDF). In the third case, the HITS algorithm was not used (WithoutHITS), and in the final case, the HITS algorithm (WithHITS) was used.


                        Figs. 15
                         and 16
                         show the F-measure of ROUGE-2 and ROUGE-SU4 on datasets TAC 2008 and TAC 2009. OnlyFREQ shows the results when we considered only the word frequency (e.g., α
                        =1, β
                        =
                        γ
                        =0 in Eq. (8)). TF-IDF shows the results when we considered TF-IDF of the words instead of the word frequency (e.g., α
                        =1, β
                        =
                        γ
                        =0 in Eq. (7)). On the other hand, WithoutHITS shows the results when we consider TF-IDF of the words and rel-gram scores (e.g., α
                        =
                        γ
                        =1, β
                        =0 in Eq. (7)) and WithHITS shows the results when we considered all the parameters (TF-IDF of words, HITS scores, and rel-gram scores). As depicted in Figs. 15 and 16, OnlyFREQ show the worst performance in all cases. As expected, this means that it is not sufficient to consider only the frequencies of words when summarizing documents. On the other hand, the result of TF-IDF can always perform better than that of OnlyFREQ on both datasets. This results show that TF-IDF is more effective than the word frequency when extracting the important words from documents. As seen in Fig. 15, WithHITS shows the best performance in all cases. The performance of WithHITS on TAC 2008 and TAC 2009 was improved by 8.9–11.5% compared to OnlyFREQ, and by 2.7–4.1% compared to WithoutHITS. This is due to the fact that, in case of WithHITS, the analysis of important words using the HITS algorithm with tag clusters was performed when summarizing documents. FoDoSu detects the sentences that consist of highly semantically related words by calculating the rel-gram score, and then analyzes the meaningful words in the documents using the HITS algorithm with tag clusters. We also found similar results using ROUGE-SU4 in Fig. 16. That is, WithHITS performs best in all cases. The performance of WithHITS on TAC 2008 and TAC 2009 was improved by 7.1–9.5% compared to OnlyFREQ, and by 2.1–5.2% compared to WithoutHITS. From these experiments, we confirmed that FoDoSu effectively selects the sentences containing significant words by exploiting the HITS algorithm with tag clusters when summarizing documents.


                        Tables 3 and 4
                        
                         show the results when comparing our proposed system FoDoSu, which uses TF-IDF and HITS algorithm together, with the related multi-document summarization techniques on TAC 2008 and TAC 2009. In these experiments, we had compared our system with the methods called DocHITS and ClusterHITS as baseline systems that used the HITS algorithm for summarizing documents. DocHITS considers the documents and sentences as hubs and authorities in the HITS algorithm. On the other hand, ClusterHITS first detects theme clusters in the document set using popular clustering algorithms and then considers these theme clusters and sentences as hubs and authorities, respectively, in the HITS algorithm. We had also compared our system with the systems NIST, ceaList1, LPN1, Veness Team1 which results were provided by TAC 2008 and TAC 2009. As shown in Tables 3 and 4, our proposed system FoDoSu always performs better than the other systems on both datasets. Furthermore, the results show that the proposed system using the HITS algorithm is more effective than other the HITS algorithm based systems for document summarization.

We had computed the statistical significance of the difference in ROUGE scores for the best-performing FoDoSu against the other systems on both TAC 2008 and TAC 2009 datasets and found that the differences are statistically significant with P
                        <0.05 (using T-Test). The results demonstrate the following advantages. First, when analyzing the semantics of words in the document, FoDoSu performed with low computational cost. Second, FoDoSu analyzed proper nouns and newly-coined words, such as the names of people and products.


                        Fig. 17
                         shows the ratio of words used to analyze semantics in TAC 2008 and TAC 2009 using WordNet and tag clusters from Flickr. To analyze the words, we selected the top 20 words in the WFT′ of each Flickr’s tag clusters. As shown in Fig. 17, 87% of the words in the documents that had their semantics analyzed existed in both WordNet and Flickr’s tag clusters. The majority of these words were common nouns that could be used to easily analyze the meaning of the words. A total of 6% of all the words were included in Flickr’s tag clusters. On the other hand, just 4% of all words were included in WordNet. The remaining words were difficult to analyze semantically using WordNet and the tag clusters, including “number” and “symbol”. The words found only in the tag clusters were most proper nouns and newly-coined words, so these words could not be used to analyze the meaning of a word using WordNet. In contrast, the existing words in WordNet consisted of gerunds and participles, parts of speech that are not semantically important. Therefore, the semantic analysis technique using WordNet created difficulty in analyzing the semantics of proper nouns and newly-coined words. However, identification was possible using tag clusters in the Folksonomy system, which leverages the collective intelligence of the users in analyzing the semantics of the words in the document.

@&#CONCLUSIONS@&#

In this paper, we propose FoDoSu which is a novel multi-document summarization system using tag clusters from a Folksonomy system to detect the key sentences in multiple documents. Our proposed system uses Flickr to acquire tag clusters to analyze the semantics of words. It efficiently summarizes the documents by detecting meaningful words and their semantic relatedness. The advantages of our approach for multi-document summarization are as follows. First, when analyzing the semantics of words in the document, FoDoSu performed with low computational cost. Second, FoDoSu analyzed proper nouns and newly-coined words in the document, such as the names of people and products. Through various experiments on TAC 2008 and TAC 2009 datasets, our proposed system FoDoSu always performs better than the other systems. Also, we identified the ratio of words that could be analyzed in TAC 2008 and TAC 2009 using WordNet and Flickr’s tag clusters. We have shown that it is useful in efficiently analyzing the semantics of proper nouns and newly-coined words. Finally, we demonstrated that analysis using tag clusters from the Folksonomy greatly affected document summarization. In future work, we will explore better methods for analyzing the semantics of words that are difficult to analyze, such as proper nouns and newly-coined words. In addition, we will improve our proposed multi-document summarization.

@&#ACKNOWLEDGEMENT@&#

This research was supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (2013R1A1A2059663).

@&#REFERENCES@&#

