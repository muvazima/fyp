@&#MAIN-TITLE@&#An efficient low bit-rate compression scheme of acoustic features for distributed speech recognition

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A low bit-rate source coding scheme for distributed speech recognition (DSR) systems is proposed.


                        
                        
                           
                           The algorithm is based on weighted least squares (W-LS) polynomial approximation.


                        
                        
                           
                           The efficiency of the algorithm is tested with the noisy Aurora-2 database, for bit-rates ranging from 1400 bps to 1925 bps.


                        
                        
                           
                           The obtained results generally outperform the ETSI-AFE encoder for clean training and provide similar performance, at 1925 bps, for multi-condition training.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Distributed speech recognition

Weighted least squares fitting

Aurora-2 database

Low bit-rate source coding

@&#ABSTRACT@&#


               Graphical Abstract
               
                  
                     
                        
                           Image, graphical abstract
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

During the last few years, the implementation of client-server architecture has received more attention for the practical speech recognition systems, especially for mobile applications. In the client-server speech recognition, also known as distributed speech recognition (DSR) [1], the front-end client is included in the terminal and it is connected over a data channel to a remote recognition server (back-end). DSR provides particular benefits for mobile terminal services such as giving access from different points of network with a guaranteed level of recognition performance. The Mel frequency cepstral coefficients (MFCCs) are the commonly used feature components for DSR front-ends [2–4]. These features are extracted and quantized at the client side, and then transmitted through an error protected data channel to a hidden Markov model-based (HMM) speech recognition system.

The introduction of new mobile services sometimes tends to produce new saturations in the network, as the available channel bandwidth is relatively limited. One solution is to quantize the feature vectors using the least amount of bits, which can be supported by the available channel bandwidth, while keeping the recognition performance that is as close as possible to that of unquantized feature vectors. In fact, several techniques for compressing MFCCs, in DSR systems, have been designed. Most of the state of the art approaches exploit inter and/or intra-frame correlations across consecutive MFCC components. This offers the capability to efficiently design low bit-rate source coding schemes. Among these methods, one can cite the work in Ref. [5], where eight temporally consecutive 14-dimensional MFCC vectors are grouped and then processed by the discrete cosine transform (DCT). The achieved compression bit-rate is around 4200 bps. In the same manner as for one-dimensional DCT, a two-dimensional DCT (2D-DCT) has been addressed in Ref. [6] where both inter and intra-frame correlations are exploited. In this method, for each 12×12
 block of consecutive MFCC frames a 2D-DCT is applied and only the DCT components with the highest energy are quantized while the rest of components are set to zero. No significant performance degradation is obtained with bit-rates as low as 624 bps for speaker dependent isolated digit recognition.

The European telecommunication standards institute (ETSI) [2–4] defines split vector quantization (SVQ) scheme [7], where MFCC coefficients are grouped into pairs and each pair is quantized using its own vector quantization (VQ) codebook. The resulting MFCC encoding bit-rate is 4400 bps without including channel error protection.

A novel bits allocation scheme for ETSI front-end (FE) [2] has been successfully applied in Ref. [8]. The quantization bits are allocated proportionally to the mutual information measures between FE sub-vectors, where the greater portion of the total bit-rate is assigned to the lower MFCCs. This method has yielded significant performance improvements for clean speech data. The authors in Ref. [9] presented a scalable predictive approach in which each feature is independently quantized using a scalar predictive coding. The scalability allows providing flexibility in optimizing the DSR bit-rate, in terms of recognition performance, to the changing bandwidth requirement and server load.

The half frame rate (HFR) front-end algorithm [10,11] investigates the redundancies in the full frame rate (FFR) features of ETSI-FE, where the source coding bit-rate is reduced from ETSI-FE 4400 bps to 2200 bps. The HFR algorithm has been evaluated on the Aurora-2 [12] clean speech. The comparison of achieved performance accuracy levels are close to ETSI-FE compression algorithm. Another DSR encoder has been proposed in Ref. [13], called packetization and variable bit-rate compression scheme. This encoder has the property of being compatible with various VQ-based DSR encoders. The coded MFCC frames are grouped using the group of pictures (GoPs) structure taken from video coding, and then a Huffman coding is applied for each group. The packetization and variable bit-rate method provides lossless compression at 3400 bps for Aurora-2 clean data, whereas the GoP grouping of ETSI-FE coded frames requires an additional algorithmic delay.

Moreover, a series of quantization techniques have been described in Ref. [14], where both inter and intra-frame MFCC correlations are exploited. One of the most discussed methods is the multi-frame Gaussian mixture model-based (GMM) block quantizer [15]. Evaluated on Aurora-2 clean speech, the GMM-based encoder achieved the best recognition performance at lower bit-rates, exhibiting a negligible 1% degradation at 800 bps. The GMM-based method has been extended to quantize MFCCs in noisy environments [16]; however, the obtained results showed a degraded recognition performance by increasing the noise level.

More recently, the authors in Ref. [17] have proposed a new bandwidth reduction scheme based on Haar wavelet decomposition. Experiments are performed on Aurora-2 noisy speech under clean training condition. Compared with the baseline system, when there is no packet loss (i.e. source coding), the bandwidth can be reduced to 50% without degrading the recognition performance. However, a graceful degradation is obtained when the bandwidth is reduced to 25% of the baseline. In addition, the work in Ref. [18] presents a series of low bit-rate quantization methods based on differential vector quantization (DVQ) algorithms. The performance is evaluated for two different tasks (Aurora-2 and Aurora 4 [19] for small and large vocabulary tasks, respectively) using only clean speech. Results obtained show that the DVQ-based schemes can be an efficient compression method at very low bit-rate, in particular for small vocabulary DSR applications. Generally speaking, most of the previously proposed low bit-rate DSR encoders suffer from degraded performance under noisy conditions.

The method we propose in this paper focuses on reducing the source coding bit-rate of MFCC vectors in a DSR system, using weighted least squares approximation. According to the DSR limitations in terms of bandwidth, memory, and computational requirements, our ultimate aim is twofold: (i) the compression task should not cause any significant loss on recognition performance, in particular under noisy conditions and (ii) the computational complexity and memory requirements should be moderate. The key idea behind this method is that we do not have to transmit every set of extracted MFCCs to the decoder (back-end); instead, we could transmit only the coefficients of the polynomial that approximates these MFCCs. At the server side, the MFCC components are reconstructed using the de-quantized polynomial coefficients. However, the performance will depend not only on the amount of allocated bits but also on both polynomial degree and weighting values.

A set of temporally consecutive MFCC frames are extracted from the speech utterance and grouped into blocks, where each block row corresponds to the time trajectory of a particular cepstral feature. By means of exploiting both the slow evolution and correlation characteristics across MFCC frames for dimensionality reduction purpose, each block row is approximated by low degree polynomial, through a weighted least-squares sense. The method used to calculate the weighting coefficient of each block column is inspired from the variable frame rate (VFR) algorithm proposed in Ref. [20]. The calculation of weights is based on the log energy parameter in which the larger weight is assigned to the more noise robust MFCC frame. Furthermore, QR factorization is used for solving the weighted least-squares problem.

In earlier work, we introduced the idea of applying the unweighted least squares (U-LS) approximation (i.e. all features have the same weight) to encode MFCCs [21]. The promising initial results obtained have shown that the approach could be further explored. Here, we present an extension to the U-LS-based encoder at lower bit-rates through the introduction of a weighting approach along with improved recognition performance. A quantization scheme based on tree structured vector quantization (TSVQ) [22] is also adopted to considerably decrease the code-vector full search. In addition, in order to reduce the approximation error for low degree polynomial fitting, the length of the approximation interval (i.e. block row dimension) used in this work is smaller than the one applied previously in Ref. [21].

It is worth noting that the issue of channel coding is a challenging task which has attracted interest from researchers in the last decades. However, it should be noted that the main objective of this paper is not to study the effects of packet loss in DSR but to propose a low bit-rate source coding scheme.

The rest of the paper is organized as follows. In Section 2, a general overview of ETSI DSR standards and Aurora-2 framework is presented. Section 3 provides a detailed description of the proposed weighted least squares-based source coding scheme. The experimental results and discussions are given in Section 4. Finally, in Section 5, the conclusion summarizes the principal results, as well as further work that need to be completed.

The basic concept of DSR consists of distributing the automatic speech recognition (ASR) system between the local front-end terminal and the back-end recognizer (see Fig. 1
                        ). Compared to the network speech recognition (NSR) system in which the features are extracted from the decoded speech signal at the server side [23], a DSR system provides particular benefits for mobile terminal services, such as (i) improved recognition performance, (ii) less complicated architecture, (iii) low bit-rate transmission over data channel, and (iv) access from different points of network with a guaranteed level of performance.

In the conventional ETSI-FE [2] the speech features (i.e. MFCCs), used in the front-end part, are derived from the extracted speech frames at frame length of 25 ms with frame shift of 10 ms, using Hamming windowing. Then, a Fourier transform is performed and followed by Mel filter bank with 23 frequency bands in the range from 64 Hz up to 4 kHz. The extracted coefficients are the first 12 MFCCs C
                        1 to C
                        12, the zeroth cepstral coefficient C
                        0, and the log energy log 
                        E in each frame. The different blocks of the MFCC front-end extraction algorithm are depicted in Fig. 2.
                        
                     

The abbreviation with a brief description of each block is listed as follows:

                           
                              •
                              
                                 ADC: Analog to digital converter. The possible output sampling rates of the ADC block are 8, 11, and 16 kHz.


                                 Offcom: Offset compensation. A notch filtering is applied to remove the direct current (DC) offset component of the waveform signal.


                                 Framing: Consists of splitting the waveform signal samples into frames of constant length of 25 ms with frame shift of 10 ms.


                                 Log E: The logarithmic frame energy measure (natural logarithm).


                                 PE: Pre-emphasis filter. In the ETSI-FE standard, the pre-emphasis factor is set to 0.97.


                                 W: Hamming windowing.


                                 FFT: Fast Fourier transform (only magnitude components are considered).


                                 MF: Mel filter bank with 23 frequency bands (from 64 Hz up to 4 kHz).


                                 LOG: Nonlinear transformation. The output of Mel filtering is subjected to a logarithm function.


                                 DCT: Discrete cosine transform. Process of transforming the log filter bank amplitudes into cepstral coefficients.


                                 MFCC: Mel frequency cepstral coefficient.

Furthermore, a detailed description of the MFCC calculation is given in Ref. [2].

In the compression task, the 14-dimensional feature vector is split into seven sub-vectors (i.e. pairs), and each of them is quantized with its own 2-dimensional VQ. The weighted mean squared error (MSE) distortion is used for the energy pair [C
                        0, log 
                        E] while the unweighted MSE is used for the remaining pairs. In total, 44 bits are assigned to each MFCC vector to achieve a source bit-rate of 4400 bps. At the back-end recognition server, delta and delta–delta coefficients (i.e. first and second derivatives) are calculated and appended to the 13 static features
                           
                              
                              [
                              
                                 
                                    C
                                    1
                                 
                                 −
                                 
                                    C
                                    12
                                 
                                 ,
                                 
                                 
                                    C
                                    
                                       0
                                       
                                    
                                 
                                 or
                                 
                                 l
                                 o
                                 g
                                 
                                 E
                              
                              ]
                           
                        , to give a total of 39 elements in each feature vector.

In the ETSI extended front-end (ETSI-EFE) standard [3], additional parameters such as voicing class and fundamental frequency are extracted at the front-end; this allows reconstructing the speech signal at the back-end side. Therefore, the transmission of these additional components increases relatively the source coding bit-rate.

The advanced front-end version (ETSI-AFE) [4] provides considerable improvements in recognition performance in the presence of background noise. In ETSI-AFE feature extraction part, noise reduction is performed first, which is based on Wiener filtering theory. Then, waveform processing is applied to the denoised signal and cepstral features are calculated. Voice activity detection (VAD) is also implemented in the front-end feature extractor. The VAD parameter is used for excluding the non-speech frames from the recognition task.

ETSI-AFE frames are quantized using the same bits allocation scheme as in the conventional ETSI-FE, except for [C
                        11, C
                        12] pair which is quantized with 5 bits while the VAD flag is transmitted as single bit. Table 1
                         shows the number of bits assigned to each sub-vector including the VAD parameter [4].

On the server side, unlike the conventional ETSI-FE standard where the derivatives are calculated from the recognition HMM toolkit (HTK), ETSI-AFE includes additional scripts, in which the derivatives are calculated from the MFCC samples contained in a 9-frame window. In addition, in ETSI-AFE back-end side, both energy coefficients C
                        0 and log 
                        E are combined in the following way:

                           
                              (1)
                              
                                 
                                    
                                       C
                                       comb
                                    
                                    =
                                    
                                       
                                          0.6
                                       
                                       23
                                    
                                    
                                       C
                                       0
                                    
                                    +
                                    0.4
                                    
                                    l
                                    o
                                    g
                                    
                                    E
                                    .
                                 
                              
                           
                        
                     

Aurora-2 database consists of isolated and connected digits task. It provides speech samples and scripts to perform speaker independent speech recognition experiments in clean and noisy conditions. This database has been prepared by down-sampling from the original 20 kHz sampling frequency to 8 kHz with an ideal low pass filter. An additional filtering is applied with the two standard frequency characteristics: G.712 and modified intermediate reference system (MIRS).

Aurora-2 contains two training sets of 8440 utterances for each one (clean and multi-condition sets), and three testing sets (set A, set B, and set C). The clean training set is filtered with the G.712 characteristic without any noise added. In multi-condition training set, the same clean utterances are equally split into 20 subsets and are then corrupted by four noises (subway, babble, car, and exhibition hall) at five different signal-to-noise ratio (SNR) levels (clean, 20, 15, 10, and 5 dB).

The first testing set called test set A, consists of 28028 utterances filtered with the G.712 characteristic using four different noises, namely subway, babble, car, and exhibition hall. In total, this test set consists of 28 subsets where the noises are added at seven different SNR levels (clean, 20, 15, 10, 5, 0, and −5 dB). Test set A contains the same noises to those used in the multi-condition training set; this leads to a high match between training and testing data. The second testing set called test set B, which is created by the same way as test set A but by using four different noises, restaurant, street, airport, and train station. The third testing set called test set C with 14014 utterances distributed into 14 subsets, where two different types of noises are considered, subway and street. In test set C, speech and noises are first filtered with the MIRS (i.e. simulating the frequency characteristics received from the terminal device), and then these noises are added using the same SNR levels as in set A and set B.

The HTK toolkit is principally designed for building HMM-based speech processing tools, in particular recognizers. It consists of a set of library modules and tools available from http://htk.eng.cam.ac.uk/. These tools provide sophisticated facilities for speech analysis, HMM training, testing, and results analysis.

In Aurora-2 task, the model set contains eleven whole word HMM models (digits 0 to 9 and “oh”) which are linear left-to-right with no skips over states. Two silence models are defined, i.e. “sil” (silence) and “sp” (short pause). The “sil” model has three emitting states and each state has six mixtures, while the “sp” model has only a single state. Each word model has sixteen states with three Gaussian mixtures per state (in HTK structure, two dummy states are added at the beginning and at the end of the given set of states).

The recognition performance is measured in terms of word accuracy rate (WAR) which can be formulated as follows [24]:

                           
                              (2)
                              
                                 
                                    WAR
                                    
                                       (
                                       %
                                       )
                                    
                                    =
                                    
                                       
                                          N
                                          −
                                          S
                                          −
                                          D
                                          −
                                          I
                                       
                                       N
                                    
                                    ×
                                    100
                                    ,
                                 
                              
                           
                        where N is the total number of words in the test set, S is the number of substitution errors, D is the number of deletion errors, and I is the number of insertion errors.

One should note that, in Aurora-2 framework, test sets A and B have twice as many utterances as test set C and therefore they should be given twice the weighting when calculating the WAR average [12,25]. Hence, the overall accuracy is expressed as

                           
                              (3)
                              
                                 
                                    WAR
                                    
                                       (
                                       Overall
                                       )
                                    
                                    =
                                    
                                    
                                       
                                          [
                                          
                                             2
                                             
                                                (
                                                
                                                   WA
                                                   
                                                      R
                                                      A
                                                   
                                                   
                                                   +
                                                   
                                                   WA
                                                   
                                                      R
                                                      B
                                                   
                                                
                                                )
                                             
                                             
                                             +
                                             
                                             WA
                                             
                                                R
                                                C
                                             
                                          
                                          ]
                                       
                                       5
                                    
                                    .
                                 
                              
                           
                        
                     

The overall WAR of recognition experiments, conducted on Aurora-2 task using ETSI-AFE standard, are summarized in Tables 2
                         and 3
                         
                        [25]. These experiments are performed under different training modes, with and without MFCCs quantization, and without including the optional VAD parameter.

The statistical properties of MFCCs were investigated in Ref. [14], in particular the temporal correlation across consecutive MFCC vectors (inter-frame dependencies). These properties have a direct influence on the rate distortion performance of any compression scheme, in which they can be exploited in different ways to form an efficient least squares fitting-based MFCCs encoder.

In this section we provide a detailed description of the proposed MFCC source encoder. However, before presenting the algorithm, we first give an overview of the topic of polynomial least squares fitting that plays an important role in computational and applied mathematics.

Given a set of n data points,
                           
                              
                              
                                 (
                                 
                                    
                                       x
                                       1
                                    
                                    ,
                                    
                                    
                                       y
                                       1
                                    
                                 
                                 )
                              
                              ,
                              
                              
                              …
                              ,
                              
                              
                              
                                 (
                                 
                                    
                                       x
                                       i
                                    
                                    ,
                                    
                                    
                                       y
                                       i
                                    
                                 
                                 )
                              
                              ,
                              …
                              ,
                              
                              
                                 (
                                 
                                    
                                       x
                                       n
                                    
                                    ,
                                    
                                    
                                       y
                                       n
                                    
                                 
                                 )
                              
                           
                        , where xi
                         are distinct real points called nodes and
                           
                              
                              
                                 y
                                 i
                              
                              ∈
                              R
                           
                        . Now, we consider the problem of finding a polynomial of degree p that fits these points, such that

                           
                              (4)
                              
                                 
                                    
                                    
                                       y
                                       i
                                    
                                    =
                                    
                                       a
                                       0
                                    
                                    
                                    +
                                    
                                    
                                       a
                                       1
                                    
                                    
                                       x
                                       i
                                    
                                    
                                    +
                                    
                                    
                                       a
                                       2
                                    
                                    
                                       x
                                       i
                                       2
                                    
                                    +
                                    …
                                    +
                                    
                                       a
                                       p
                                    
                                    
                                       x
                                       i
                                       p
                                    
                                    .
                                 
                              
                           
                        
                     

This can be written in matrix notation as

                           
                              (5)
                              
                                 
                                    
                                       [
                                       
                                          
                                             
                                                1
                                             
                                             
                                                
                                                   x
                                                   1
                                                
                                             
                                             
                                                
                                                   x
                                                   1
                                                   2
                                                
                                             
                                             
                                                …
                                             
                                             
                                                
                                                   x
                                                   1
                                                   p
                                                
                                             
                                          
                                          
                                             
                                                1
                                             
                                             
                                                
                                                   x
                                                   2
                                                
                                             
                                             
                                                
                                                   x
                                                   2
                                                   2
                                                
                                             
                                             
                                                …
                                             
                                             
                                                
                                                   x
                                                   2
                                                   p
                                                
                                             
                                          
                                          
                                             
                                                ⋮
                                             
                                             
                                                ⋮
                                             
                                             
                                                ⋮
                                             
                                             
                                                ⋱
                                             
                                             
                                                ⋮
                                             
                                          
                                          
                                             
                                                1
                                             
                                             
                                                
                                                   x
                                                   n
                                                
                                             
                                             
                                                
                                                   x
                                                   n
                                                   2
                                                
                                             
                                             
                                                …
                                             
                                             
                                                
                                                   x
                                                   n
                                                   p
                                                
                                             
                                          
                                       
                                       ]
                                    
                                    
                                       [
                                       
                                          
                                             
                                                
                                                   a
                                                   0
                                                
                                             
                                          
                                          
                                             
                                                
                                                   a
                                                   1
                                                
                                             
                                          
                                          
                                             
                                                ⋮
                                             
                                          
                                          
                                             
                                                
                                                   a
                                                   p
                                                
                                             
                                          
                                       
                                       ]
                                    
                                    =
                                    
                                       [
                                       
                                          
                                             
                                                
                                                   y
                                                   1
                                                
                                             
                                          
                                          
                                             
                                                
                                                   y
                                                   2
                                                
                                             
                                          
                                          
                                             
                                                ⋮
                                             
                                          
                                          
                                             
                                                
                                                   y
                                                   n
                                                
                                             
                                          
                                       
                                       ]
                                    
                                    
                                    o
                                    r
                                    
                                    X
                                    a
                                    =
                                    y
                                    .
                                 
                              
                           
                        
                     

The matrix X here is called a Vandermonde matrix, or triangular Vandermonde in case of p < n. This linear system is overdetermined which usually has no exact solution [26]. Therefore, for a suitable choice of polynomial degree p, the system can be solved in the least squares sense by the following minimization:

                           
                              (6)
                              
                                 
                                    
                                       
                                          
                                             
                                                min
                                                
                                                   
                                                      
                                                         0.33
                                                         e
                                                         m
                                                      
                                                      
                                                         0
                                                         e
                                                         x
                                                      
                                                   
                                                   a
                                                   ∈
                                                   
                                                      
                                                         
                                                            R
                                                         
                                                      
                                                      
                                                         (
                                                         p
                                                         
                                                            
                                                               0.33
                                                               e
                                                               m
                                                            
                                                            
                                                               0
                                                               e
                                                               x
                                                            
                                                         
                                                         +
                                                         
                                                            
                                                               0.33
                                                               e
                                                               m
                                                            
                                                            
                                                               0
                                                               e
                                                               x
                                                            
                                                         
                                                         1
                                                         )
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   ∥
                                                   Xa
                                                   −
                                                   y
                                                   ∥
                                                
                                                2
                                             
                                             .
                                          
                                       
                                    
                                 
                              
                           
                        
                     

In general, a least squares problem is solved either by using normal equations or QR factorization of the coefficients of matrix  X. As comparison between both methods, the normal equations method is the fastest one but at the same time numerically unstable. However, the QR method is more accurate and stable, as well as more widely applicable than normal equations [26,27].

Before calculating 
                           
                              a
                              =
                              
                                 
                                    [
                                    
                                       
                                          a
                                          
                                             0
                                             
                                          
                                       
                                       ,
                                       …
                                       ,
                                       
                                          a
                                          p
                                       
                                    
                                    ]
                                 
                                 T
                              
                           
                        , the choice of appropriate nodes 
                           
                              
                                 x
                                 i
                              
                              
                                 (
                                 
                                    i
                                    =
                                    1
                                    ,
                                    …
                                    ,
                                    n
                                 
                                 )
                              
                           
                         can contribute significantly to the improvement of numerical properties of the polynomial fitting algorithm. The simplest way is to start by equidistant nodes located in the interval 
                           
                              [
                              −
                              1
                              ,
                              1
                              ]
                           
                        . In case of high polynomial degrees it is better to use points which are more distributed at the boundaries of the interval 
                           
                              [
                              −
                              1
                              ,
                              1
                              ]
                           
                         (e.g. Chebyshev nodes). However, the freedom of choosing the interpolation points is not always given, and sometimes equidistant points are what is needed [27].

Suppose some of the data points are known to be more significant than others. In this case, the basic form of least squares is extended to the weighted least squares, where the weights W are introduced in the measurement of the error using the minimization

                           
                              (7)
                              
                                 
                                    
                                       min
                                       
                                          a
                                          ∈
                                          
                                             R
                                             
                                                (
                                                
                                                   p
                                                   +
                                                   1
                                                
                                                )
                                             
                                          
                                       
                                    
                                    
                                       
                                          ∥
                                          
                                             W
                                             
                                                (
                                                
                                                   X
                                                   a
                                                   −
                                                   y
                                                
                                                )
                                             
                                          
                                          ∥
                                       
                                       2
                                    
                                 
                              
                           
                        where

                           
                              (8)
                              
                                 
                                    W
                                    =
                                    diag
                                    
                                       [
                                       
                                          
                                             w
                                             1
                                          
                                          ,
                                          …
                                          ,
                                          
                                             w
                                             n
                                          
                                       
                                       ]
                                    
                                    .
                                    
                                 
                              
                           
                        
                     

The weighted least squares problem can also be expressed in the following form:

                           
                              (9)
                              
                                 
                                    
                                       
                                          
                                             
                                                min
                                                
                                                   
                                                      
                                                         0.33
                                                         e
                                                         m
                                                      
                                                      
                                                         0
                                                         e
                                                         x
                                                      
                                                   
                                                   a
                                                   ∈
                                                   
                                                      
                                                         
                                                            R
                                                         
                                                      
                                                      
                                                         (
                                                         p
                                                         +
                                                         1
                                                         )
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      ∥
                                                   
                                                   
                                                      X
                                                      ˜
                                                   
                                                   a
                                                   −
                                                   
                                                      y
                                                      ˜
                                                   
                                                   
                                                      ∥
                                                   
                                                
                                                2
                                             
                                             ,
                                          
                                       
                                    
                                 
                              
                           
                        where

                           
                              (10)
                              
                                 
                                    
                                       X
                                       ˜
                                    
                                    =
                                    W
                                    X
                                    
                                    and
                                    
                                    
                                       y
                                       ˜
                                    
                                    =
                                    W
                                    y
                                    .
                                 
                              
                           
                        
                     

For convenience, we use the abbreviation W-LS to denote the proposed weighted least squares encoder. The flowchart of the W-LS algorithm, without quantization, is illustrated in Fig. 3
                        . For each speech utterance, the 14-dimensional MFCC vectors are extracted by using ETSI-AFE extraction algorithm. The first step consists of arranging the extracted MFCCs into non-overlapping blocks of eight temporally consecutive MFCC frames each (matrix of 14 rows and 8 columns). Then, for each block row we estimate the coefficients of a polynomial of degree p, in a weighted least squares sense. By the fact that the MFCC entries are extracted at regular time intervals (i.e. every 10 ms), the interpolation points 
                           
                              
                                 x
                                 
                                    i
                                    
                                 
                              
                              
                                 (
                                 
                                    i
                                    =
                                    1
                                    ,
                                    …
                                    ,
                                    8
                                 
                                 )
                              
                              
                           
                        are considered as uniformly distributed in the interval
                           
                              
                              [
                              
                                 −
                                 1
                                 ,
                                 
                                 1
                              
                              ]
                           
                         such that
                           
                              
                              
                              
                                 x
                                 i
                              
                              =
                              −
                              1
                              +
                              
                                 (
                                 
                                    i
                                    −
                                    1
                                 
                                 )
                              
                              h
                              
                              
                                 (
                                 
                                    i
                                    =
                                    1
                                    ,
                                    …
                                    ,
                                    
                                    
                                    n
                                 
                                 )
                              
                           
                        , with 
                           
                              h
                              =
                              2
                              /
                              (
                              
                                 n
                                 −
                                 1
                              
                              )
                           
                         and 
                           
                              n
                              =
                              8
                           
                        .

To solve the weighted least squares problem we use the QR factorization of matrix 
                           
                              
                                 X
                                 ˜
                              
                              
                              
                                 (
                                 
                                    n
                                    
                                    ×
                                    
                                    p
                                 
                                 )
                              
                           
                         which is defined as

                           
                              (11)
                              
                                 
                                    
                                       X
                                       ˜
                                    
                                    =
                                    Q
                                    R
                                    ,
                                 
                              
                           
                        where Q is an orthogonal matrix and R is an upper triangular matrix. In this work, the commonly used Householder triangularization [27] is applied to compute the QR factorization.

The system of equations 
                           
                              
                                 X
                                 ˜
                              
                              a
                              =
                              
                                 y
                                 ˜
                              
                           
                         can be reduced to an upper-triangular system by introducing the QR factorization, we obtain

                           
                              (12)
                              
                                 
                                    Q
                                    R
                                    a
                                    =
                                    
                                       y
                                       ˜
                                    
                                    .
                                 
                              
                           
                        
                     

A left-multiplication by QT
                         results in

                           
                              (13)
                              
                                 
                                    R
                                    a
                                    =
                                    b
                                    ,
                                 
                              
                           
                        where

                           
                              (14)
                              
                                 
                                    
                                       
                                          
                                             b
                                             =
                                             
                                                
                                                   Q
                                                
                                                T
                                             
                                             
                                                y
                                                ˜
                                             
                                             
                                                
                                                   0.16
                                                   e
                                                   m
                                                
                                                
                                                   0
                                                   e
                                                   x
                                                
                                             
                                             
                                                with
                                             
                                             
                                                
                                                   0.16
                                                   e
                                                   m
                                                
                                                
                                                   0
                                                   e
                                                   x
                                                
                                             
                                             
                                                
                                                   Q
                                                
                                                T
                                             
                                             Q
                                             =
                                             I
                                             .
                                          
                                       
                                    
                                 
                              
                           
                        
                     

The upper-triangular system 
                           
                              R
                              a
                              =
                              b
                           
                         is then solved by performing back substitution [27] which allows computing
                           
                              
                              
                                 R
                                 
                                    −
                                    1
                                 
                              
                              b
                           
                        .

One obvious reason of choosing the data points as the row vectors rather than column vectors (in each block) is motivated by the slow evolution or small range of variation in a set of consecutive MFCC coefficients. This will lead to more accurate W-LS polynomial approximation, in particular in the case of lower order polynomial fitting (i.e. p ≪ n).

Additionally, it can be observed that the proposed scheme gives more flexibility to fine tune the suitable bit-rate by varying the two parameters n (block row dimension) and p (interpolation degree). The best low bit-rate W-LS configuration is the one with the good compromise of low polynomial degree and high block row dimension (i.e. block size); however, increasing the block row dimension will lead to a relatively more algorithmic delay. Therefore, with carful investigation and based on previous related research [5,17], the block size was fixed to 14× 8.

From the computational point of view, the work of the W-LS algorithm is generally dominated by the cost of QR factorization. As found in Ref. [27], in case of employing Householder triangularization for the calculation of QR factorization of a matrix X(n×p), we retrieve approximately
                           
                              
                              2
                              p
                              
                                 n
                                 2
                              
                              −
                              
                                 2
                                 3
                              
                              
                                 n
                                 3
                              
                           
                         floating point operations.

The method employed to calculate the weights is derived from the VFR analysis approach proposed in Ref. [20]. The authors have calculated an energy weighted distance of two adjacent MFCC frames where the frames which reveal changes but are low in energy are discarded, since they may not be noise robust. VFR processing has been used by several researchers to improve the performance of ASR systems (e.g. the work in Ref. [28]). By applying the same principle as in Ref. [20], the weights are calculated in such a manner that the larger weight is assigned to the more noise robust MFCC frame in the block. This allows giving more importance to the frames that are less affected by noise. For each MFCC frame, the corresponding weighting coefficient
                           
                              
                              
                                 w
                                 i
                              
                              
                              
                                 (
                                 
                                    i
                                    =
                                    1
                                    ,
                                    …
                                    ,
                                    
                                    8
                                 
                                 )
                              
                           
                         is calculated based on log 
                        E
                        
                           i 
                        coefficient from the following formulas:

                           
                              (15)
                              
                                 
                                    
                                       w
                                       i
                                    
                                    =
                                    l
                                    o
                                    g
                                    
                                    
                                       E
                                       i
                                    
                                    −
                                    
                                       (
                                       
                                          
                                             
                                                l
                                                o
                                                g
                                                
                                                
                                                   E
                                                   i
                                                
                                                
                                                   (
                                                   L
                                                   )
                                                
                                             
                                             ¯
                                          
                                          /
                                          β
                                       
                                       )
                                    
                                    
                                    
                                       (
                                       
                                          i
                                          =
                                          1
                                          ,
                                          …
                                          ,
                                          
                                          8
                                       
                                       )
                                    
                                    ,
                                 
                              
                           
                        where

                           
                              (16)
                              
                                 
                                    
                                       
                                          l
                                          o
                                          g
                                          
                                          
                                             E
                                             i
                                          
                                          
                                             (
                                             L
                                             )
                                          
                                       
                                       ¯
                                    
                                    =
                                    
                                       1
                                       
                                          (
                                          
                                             2
                                             L
                                             +
                                             1
                                          
                                          )
                                       
                                    
                                    
                                       ∑
                                       
                                          j
                                          =
                                          −
                                          L
                                       
                                       L
                                    
                                    l
                                    o
                                    g
                                    
                                    
                                       E
                                       
                                          i
                                          −
                                          j
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

In Ref. [20], the whole utterance was used to compute the log energy average 
                           
                              
                                 l
                                 o
                                 g
                                 
                                 
                                    E
                                    i
                                 
                                 
                                    (
                                    L
                                    )
                                 
                              
                              ¯
                           
                         and the β parameter was set to 1.5. Here, 
                           
                              
                                 l
                                 o
                                 g
                                 
                                 
                                    E
                                    i
                                 
                                 
                                    (
                                    L
                                    )
                                 
                              
                              ¯
                           
                         is estimated over a window of 
                           
                              2
                              L
                              +
                              1
                           
                         fixed samples (weighting window length) centered around logE
                        
                           i 
                        ; however, different values of β, from 1.5 to 2.8, are evaluated. Fig. 4
                         illustrates in detail how the windowing is performed to calculate the weights from logE coefficient, where the length of interpolation window is set to 8 and the length of weighting window is set to 9. One can deduce that for each block of 8 frames the weighting scheme requires 8 additional frames, in which only logE parameter is considered (4 frames in the beginning and 4 frames in the end of the block). This means that the algorithm, in its weighted form, will operate over two different blocks: (i) interpolation non-overlapping blocks of 14 × 8 samples and (ii) weighting blocks of 1  ×16 samples with 8 overlapping log 
                        E coefficients (see Fig. 5
                        ).

The reason for using a centered short time weighting window, in particular 9-frame window length (
                           
                              L
                              =
                              4
                           
                        ), rather than the whole utterance is twofold: (i) calculating the weights from a limited amount of samples allows reducing considerably the algorithmic delay, especially in real time applications, with relatively more computational complexity, and (ii) as applied in ETSI-AFE to estimate the derivatives [4], it can be deduced that 9-frame length is somewhat sufficient to capture such dynamic changes over logE coefficients. This is further supported by an experiment, conducted in Section 4.2, which shows that short time weighting window gives better results than the whole utterance.

In the quantization step, each polynomial vector
                           
                              
                              a
                              =
                              
                                 
                                    [
                                    
                                       
                                          a
                                          
                                             0
                                             
                                          
                                       
                                       ,
                                       …
                                       ,
                                       
                                          a
                                          p
                                       
                                    
                                    ]
                                 
                                 T
                              
                           
                        , extracted from each MFCC block row, is quantized separately using its own VQ codebook (by considering low dimensional vectors i.e. p ≪ n). To achieve this, two scenarios can be distinguished: (i) In the case of using codebook with small size (i.e. lower bit-rates), the iterative Linde–Buzo–Gray (LBG) algorithm [29] is applied in one step to the training data set, where the resulting code-vectors constitute the VQ codebook. The nearest code-vector is selected by the criterion of minimum MSE which is defined as

                           
                              (17)
                              
                                 
                                    MSE
                                    
                                       (
                                       
                                          a
                                          ,
                                          
                                             a
                                             ^
                                          
                                       
                                       )
                                    
                                    =
                                    
                                       
                                          (
                                          
                                             a
                                             −
                                             
                                                a
                                                ^
                                             
                                          
                                          )
                                       
                                       T
                                    
                                    
                                       (
                                       
                                          a
                                          −
                                          
                                             a
                                             ^
                                          
                                       
                                       )
                                    
                                    ,
                                 
                              
                           
                        where a and 
                           
                              a
                              ^
                           
                         are the input vector and the code-vector, respectively. (ii) In the case of codebook with larger size, a TSVQ technique is adopted in order to reduce the code-vector search [22]. The proposed TSVQ scheme focuses on jointly designing the tree and the VQ codebook to provide a reasonable compromise between memory and complexity. We used a binary search strategy with a balanced tree structure. First we apply the iterative LBG to get two code-vectors (root node) within the entire training set. In the next step, the training set is divided into two training sub-sets, where data are assigned to the resulting code-vectors, and then each sub-set is trained separately. This process of splitting is repeated until the desired number of terminal nodes is obtained. Finally, we apply VQ-LBG to the training sub-set belonging to each terminal node. An example of 10 bits codebook is illustrated in Fig. 6
                        , where the rate vector 
                           
                              R
                              =
                              (
                              
                                 1
                                 ,
                                 
                                 1
                                 ,
                                 
                                 1
                                 ,
                                 
                                 7
                              
                              )
                           
                        .

@&#EXPERIMENTAL RESULTS@&#

The proposed W-LS algorithm is evaluated on the Aurora-2 database (set A, set B, and set C), under both clean and multi-condition training modes. Since the polynomial degree and weighting coefficients have a considerable influence over the recognition performance, a series of experiments are first conducted in order to analyze their effectiveness as well as to assess the suitable parameters for low bit-rate coding. Then, quantization results at various bit-rates are illustrated.

Speech recognition experiments are carried out using HTK ver. 3.4 speech recognizer engine [24]. In the recognition task, the zeroth cepstral coefficient C
                     0 and logarithmic frame energy log 
                     E coefficients are substituted by the coefficient C
                     comb as indicated in formula (1). The HTK configuration parameter type “MFCC_E_D_A” is applied; this means that deltas plus delta–delta coefficients are appended to the 13 static features 
                        
                           [
                           
                              
                                 C
                                 1
                              
                              −
                              
                                 C
                                 12
                              
                              ,
                              
                              
                                 C
                                 comb
                              
                           
                           ]
                        
                      
                     [4,24].

Various experiments are carried out for assessing the appropriate interpolation degree p with respect to the overall WAR. In order to analyze the effects of polynomial degree independently from the weighting coefficients, a U-LS algorithm is first implemented without including quantization. From Fig. 7
                         it can be derived that generally there is no significant degradation for all p ≥ 2. For example, if we take 
                           
                              p
                              =
                              2
                              
                           
                        we measure a degradation of 0.2% and 0.16% for clean and multi-condition training, respectively. This can be interpreted by the fact that a polynomial curve fitting could be viewed as a noise smoothing filter [30] which can contribute to reducing the noise from MFCCs. Therefore, one might suspect that the distortion due to the approximation function can be partly compensated by the smoothing function which allows denoising MFCCs.

In addition to the effects of interpolation degree p, there are also the effects related to the length of interpolation window (i.e. the parameter n). The best choice of these two parameters (n, p) is based on the principle of removing as much noise as possible without, at the same time, excessively degrading the underlying information [30]. However, in the proposed algorithm it is better to kept the interpolation window as short as possible, because using shorter duration (i) is more appropriate for real time applications by the fact that the algorithmic delay and complexity are reduced and (ii) tends to decrease the degree of polynomial (i.e. the parameter p) which is more suitable for low bit-rate coding.

Although the obtained results are encouraging, the performance may be significantly degraded when considering quantization error. Therefore, the ultimate aim is to improve the performance of the unquantized scheme (baseline) as much as possible, before applying quantization.

The weighting coefficients depend essentially on the β parameter value and the weighting window length (
                           
                              2
                              L
                              +
                              1
                           
                        ). As stated in Section 3.3 the weighting window length is set to 9. In order to select the best β parameter, we conducted a second experiment in which β is varied from 1.5 to 2.8. In this study, for low bit-rate coding purpose, we only consider 
                           
                              p
                              =
                              2
                              
                           
                        and 
                           
                              p
                              =
                              3
                           
                        .

From Figs. 8
                         and 9
                        , it can be shown that the inclusion of weighting coefficients has a considerable impact on recognition performance with respect to the U-LS results (refer to Fig. 7). The best performance is achieved by setting 
                           
                              β
                              =
                              2
                              
                           
                        in both training conditions. Further results are given in Table 4
                        . In general, results surpassed the ETSI-AFE baseline system under both training conditions, except for the case of 
                           
                              p
                              =
                              2
                           
                         under multi-condition training where we notice a negligible degradation of 0.08%.

Here, the role of weighting function is to discriminate between noisy (assigning smallest weight) and underlying MFCCs (assigning larger weight). Therefore, the underlying MFCCs will be much more considered in the polynomial approximation. In other words, this allows reducing noise from the more affected frames.

Further, we conducted a similar experiment where the weighting coefficients are calculated over the whole utterance. This can justify our choice in using short time weighting window rather than the whole utterance. By analyzing results shown in Tables 4 and 5
                        , it can be concluded that short time weighting window analysis is more convenient for the proposed W-LS scheme.

The efficiency of the W-LS depends on a large number of parameters. However, it remains a difficult task to find a good compromise between the weighted least squares parameters (w, n, p) and the quantization scheme.

Various quantization and bits allocation schemes are applied for different bit-rates. We only considered the case of
                           
                              
                              p
                              =
                              2
                           
                        , in order to reduce as much as possible the amount of allocated bits. Whereas the β parameter is set to 
                           
                              β
                              =
                              2
                           
                        . Table 6
                         illustrates the bits allocation and quantization scheme for each bit-rate coding.

We performed a series experiments on the quantized MFCCs. The results we obtained, at various bit-rates, are compared with the following algorithms: (i) ETSI-AFE baseline system or unquantized MFCCs (refer to results in Table 2). (ii) ETSI-AFE encoder with quantized MFCCs at 4300 bps, i.e. by excluding the 1 bit/frame used for VAD parameter (refer to ETSI-AFE bits allocation in Table 1 and results in Table 3). (iii) The recently proposed Wavelet-based (WT) encoder [17]. Without including the effects of packet loss, two bit-rates coding are considered, such as 2150 bps using level-1 WT decomposition and 1075 bps using level-2 WT decomposition. Quantization results are given in detail in Table 7.
                        
                     

The quantization bit-rate varies from 1050 bps to 1925 bps, which means that the bandwidth is reduced to (i) around 25% for 1050 bps and (ii) around 45% for 1925 bps of the baseline, respectively. The measure of degradation is expressed as the difference between the WAR using the quantized features and the WAR of the baseline system. A difference with a positive sign means improvement whereas the one with a negative sign means degradation.

The overall performance show that the proposed W-LS encoder slightly exceeds the ETSI advanced front-end (ETSI-AFE) baseline system for bit-rates ranging from 1400 bps to 1925 bps when using clean training mode. However, a negligible degradation is observed in case of multi-condition training mode (around 0.6% and 0.2% at 1400 bps and 1925 bps, respectively). Furthermore, the performance of the W-LS-based encoder generally outperforms the ETSI-AFE source encoder at 4400 bps under clean training and provides similar performance, at 1925 bps, under multi-condition training.

In this work, a low bit-rate source coding method for compressing MFCCs has been proposed. Based on a weighted least squares approximation, it has been shown that the proposed scheme is capable of significantly reducing the bit-rate for a DSR system, without making drastic performance degradation. The method was evaluated, using Aurora-2 database, under both clean and noisy environments at different SNR levels. Overall, the results indicate that the performance of the proposed W-LS algorithm is slightly better than the ETSI-AFE baseline system, under clean condition training. However, there is a graceful degradation obtained under multi-condition training (around 0.6% and 0.2% at 1400 bps and 1925 bps, respectively). Compared with ETSI-AFE encoder at 4300 bps (i.e. without VAD), the performance generally outperforms ETSI-AFE for clean training and provides similar performance, at 1925 bps, for multi-condition training. Moreover, we achieved comparable performance to that of the recently proposed wavelet-based DSR encoder, in case of excluding packet loss effects and under clean condition training.

Future work will be devoted to: (i) improving the performance of the proposed W-LS algorithm by investigating new noise robust weighting techniques (ii) analyzing the effects of packet loss on the coded MFCCs.

@&#ACKNOWLEDGEMENTS@&#

The authors would like to thank the LCPTS laboratory team for providing contribution and making many suggestions, which have been exceptionally helpful in carrying out this research work.

@&#REFERENCES@&#

