@&#MAIN-TITLE@&#Effect of acoustic and linguistic contexts on human and machine speech recognition

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           We compared the automatic speech recognition (ASR) with that of humans (HSR).


                        
                        
                           
                           HSR improves dramatically when given some preceding words context.


                        
                        
                           
                           More contexts improve human word prediction, but do not recognition so much.


                        
                        
                           
                           ASR without any language model were much inferior to HSR.


                        
                        
                           
                           Thus the acoustic models in ASR should be improved rather than language models.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Continuous speech recognition

Human speech recognition ability

Acoustic model

Language model

@&#ABSTRACT@&#


               
               
                  We compared the performance of an automatic speech recognition system using n-gram language models, HMM acoustic models, as well as combinations of the two, with the word recognition performance of human subjects who either had access to only acoustic information, had information only about local linguistic context, or had access to a combination of both. All speech recordings used were taken from Japanese narration and spontaneous speech corpora.
                  Humans have difficulty recognizing isolated words taken out of context, especially when taken from spontaneous speech, partly due to word-boundary coarticulation. Our recognition performance improves dramatically when one or two preceding words are added. Short words in Japanese mainly consist of post-positional particles (i.e. wa, ga, wo, ni, etc.), which are function words located just after content words such as nouns and verbs. So the predictability of short words is very high within the context of the one or two preceding words, and thus recognition of short words is drastically improved. Providing even more context further improves human prediction performance under text-only conditions (without acoustic signals). It also improves speech recognition, but the improvement is relatively small.
                  Recognition experiments using an automatic speech recognizer were conducted under conditions almost identical to the experiments with humans. The performance of the acoustic models without any language model, or with only a unigram language model, were greatly inferior to human recognition performance with no context. In contrast, prediction performance using a trigram language model was superior or comparable to human performance when given a preceding and a succeeding word. These results suggest that we must improve our acoustic models rather than our language models to make automatic speech recognizers comparable to humans in recognition performance under conditions where the recognizer has limited linguistic context.
               
            

@&#INTRODUCTION@&#

The performance of automatic speech recognition (ASR) systems using large-vocabulary continuous speech recognizers (LVCSR) is sufficient for read speech, and such systems are used every day in dictation systems, broadcasting systems (Imai et al., 2000, 2007; Matsoukas et al., 2006; Boulianne et al., 2006; Ortega et al., 2009), etc. Good recognition of spontaneous speech, such as dialogs or lectures, is also required, but ASR system performance is still insufficient for these tasks because spontaneous speech involves phenomena such as much greater variation in the speed of speech, higher rates of speech, and wider variation in pronunciation, caused by coarticulation, than read speech. In this paper we use the term “coarticulation” to describe the influence of preceding and following sounds on pronunciation.

ASRs use local acoustic information about sub-word units such as phones and syllables, along with the local linguistic context. Similar to the way in which ASRs function, human speech recognition (HSR) is also believed to use sub-word acoustic knowledge to discriminate the phones or syllables, and local linguistic information about the target word and its contexts, to predict the acoustic phenomena. In this paper we focus on evaluating the performance of automatic speech recognition based on conventional HMM acoustic and n-gram language models, by comparing them with human speech recognition based on a combination of acoustic knowledge and information about local context. More specifically, we compare the performance of an automatic speech recognition system using n-gram language models, HMM acoustic models, and combinations of the two, with the word recognition performance of human subjects who have access to only acoustic information, have information only about local linguistic context, or have access to a combination of both. All of the speech recordings used are taken from Japanese narration and spontaneous speech corpora.

For large-vocabulary speech recognition, hidden Markov models (HMM) and n-gram models (typically with small n values, such as bigrams (n
                     =2) or trigrams (n
                     =3)) are used as acoustic and language models, respectively.

HMM statistical models focus on acoustic features of sub-words, such as phones or syllables, and are usually designed to consider just the preceding or following sub-words in order to deal with short-term coarticulation.

On the other hand, n-gram statistical models use local linguistic properties such as a conditional probability when given the preceding n
                     −1 words. N-gram models work well with machine speech recognition algorithms, and can easily constrain the search space in an algorithm.


                     Lippmann (1997) compared the overall performance of machine recognition systems with human recognition under various conditions, and revealed that machines were quite inferior to humans, especially under adverse conditions such as recognition with background noise, etc. For example, the human transcription error rates of read sentences from the Wall Street Journal, and of spontaneous conversations recorded over the telephone, were less than 0.4% and 4%, respectively (Ebel and Picone, 1995; Lippmann, 1997), error rates are far beyond the ability of machines. The recognition performance of humans and machines was also compared in a digit string recognition task (Lippmann, 1997). The string recognition error rate of the machine recognizer was 0.72% for wide-band speech, while that of humans was 0.105% for LPC synthesized speech, and humans recognized digit strings almost perfectly (the error rate was 0.009%). These results suggest that enough acoustic information is present in the feature parameters for recognition. Lippmann (1997) concluded that further work was needed to explore the differences between spontaneous and read speech, but he did not clearly explain the difference in linguistic-level performance.

Braida et al. investigated the factors which determine clarity of speech for hearing-impaired listeners by comparing clear and conversational speech (Picheny et al., 1985), and acoustically analyzed these types of speech (Picheny et al., 1986). Then they compared HMM consonant recognition with human recognition under various noisy conditions (Sroka and Braida, 2004). Their research, however, dealt only with isolated sub-word recognition and did not treat continuous speech recognition.

In May 2007, a special issue of “Speech Communication” was published with the theme “Bridging the gap between human and automatic speech recognition”, and comparison of human and machine speech recognition was revisited. In that issue, Scharenborg (2007) reviewed research comparing human and machine speech recognition, and pointed out that acoustic and linguistic models could be improved by incorporating knowledge obtained from human speech recognition research.


                     Shinozaki and Furui (2003) also studied human and machine recognition of words in short word contexts and concluded that the major factors in machine misrecognition, which were caused by estimation errors of language models and acoustic models, included acoustic variation, ambiguity, etc. Our aim in this study is to discover the weaknesses of acoustic and linguistic models in current speech recognizers, especially for spontaneous speech, by comparing human and machine recognition in a way similar to their research, using speech recognition algorithms to indicate more clearly what we should focus on in state-of-the-art speech recognition research.


                     Dusan and Rabiner (2005) compared the general characteristics of both human and automatic speech recognition from the viewpoints of architecture, analysis methods, information processing methods, etc., and concluded that radical modifications were needed before automatic speech recognition could approach human levels. But no clear direction for such modification has been found. For example, humans apparently use more global information, such as longer contexts, co-occurrence information with target words, and domain knowledge. Humans also use syntax and lexical, semantic, and interpretative contexts (Tyler and Frauenfelder, 1987; Tanenhaus and Lucas, 1987). Thus, efforts have been made to model such information, but a general method for using such global information has not yet been developed. One way of thinking about this approach to improving machine speech recognition performance is that the improvement of acoustic and/or language modeling would be a quick (and wonderful) short-cut.


                     Sommers and Danielson (1999) and Pichora-Fuller et al. (1995) compared human identification scores for words presented in isolation and in low- and high-predictability sentences, and showed that predictability greatly affects identification/recognition scores. Nittrouer and Boothroyd (1990) also noted the contribution of word and sentence contexts on the perception of phonemes within words, and of words within sentences, respectively. N-gram models raise the prediction ability of machine recognizers in a similar manner. Our experiments in this paper assess the capability of n-grams by comparing them with human speech recognition.

In this paper, as mentioned above, we focus on evaluating the performance of widely used HMM acoustic and n-gram language models by comparing them with a combination of human acoustic knowledge and information about local context. The comparisons are done under various conditions by altering context lengths, target word lengths, and speaking styles, such as read speech vs. spontaneous speech, and we discuss the performance of such local acoustic and linguistic models. By examining the differences between ASR and human speech recognition (HSR), and by investigating the strengths and weaknesses of automatic speech recognizers, we hope to understand how ASR could profit from more knowledge about HSR. We then discuss how our experiments described in this paper contribute to this understanding. Through experimentation, we evaluate the impact of acoustic and linguistic information on human and machine speech recognition, and reveal where the models used in machine recognition can be improved. Finally, we point out that we must improve acoustic models first, because the difference in the capability of acoustic models vs. human use of acoustic information is larger than the capability gap of language models. These experiments highlight the weak points in such frameworks, and help us determine the best direction for future research in automatic speech recognition.

This paper is organized as follows: in Section 2, we evaluate human speech recognition performance when given (or not given) local linguistic context and acoustic information. Section 3 describes the performance of an automatic speech recognizer when given almost the same information as given to humans in Section 2. In Section 4, we briefly summarize the experimental results shown in Sections 2 and 3, and discuss the capabilities of acoustic and language models from the viewpoint of information theory. Section 5 concludes the paper.

@&#METHODOLOGY@&#

In this section, we investigate human speech recognition performance by looking at how humans utilize acoustic knowledge and linguistic information. This paper compares human recognition performance with state-of-the-art acoustic modeling using Hidden Markov models and language modeling using n-grams. Thus, we consider information about local context as linguistic information.

In order to make this comparison, we conducted the following experiments
                           1
                        
                        
                           1
                           As for the evaluation of human prediction performance with syllable context information only, there was no acoustic information and too little linguistic information and thus such evaluation could not be performed.
                        :
                           
                              •
                              Evaluation of human word recognition performance by having subjects listen to word utterances with word contexts, which investigates the effect of linguistic information and acoustic information to resolve coarticulation through speech input.

Evaluation of human word recognition performance by having subjects listen to word utterances with syllable contexts, which investigates the effect of acoustic information in resolving coarticulation through speech input.

Evaluation of human word prediction performance when subjects know linguistic information which investigates the effect of linguistic information on predicting/constraining a current word through text input.

The details of the experimental setups are shown in the next section.

@&#EXPERIMENTS@&#

We evaluated the word recognition performance of humans when a word context was given, using a Japanese continuous speech corpora.

We used two sets of speech as test sets. The first was speech read aloud from selected passages of the JNAS (Japanese Newspaper Article Sentences) speech corpus (Itou et al., 1999). We randomly selected 100 words from the corpus, ignored some parts of speech, and excluded fillers and unknown words omitted from the vocabulary used in speech recognition experiments in Section 3.
                           2
                        
                        
                           2
                           Unlike stress-accented languages like English, Japanese is a pitch-accented language, and thus accenting scarcely affects the strength and length of syllables. So the stress in a word had little effect on our experiments and thus we did not consider the stress of the word in the sentence to select the targets.
                         The second speech set was derived from four lectures (A01M0074, A01M0035, A01M0007, A05M0031) included in the Corpus of Spontaneous Japanese (CSJ) (Maekawa, 2003), which we used as samples of spontaneous speech. These were oral presentations given at conference meetings in Japan related to speech. All of the speakers were male, and the number of speakers was 23 and 4 for JNAS and CSJ, respectively. We randomly chose 25 words as targets from each of the CSJ lectures, so this second set also contained 100 words. A histogram of word length (in terms of number of syllables
                           3
                        
                        
                           3
                           Since Japanese is also a mora-counting language and every syllable corresponds to one mora in most cases, word length can be approximately measured by the number of syllables.
                        ) is shown in Fig. 1
                        . The JNAS test set included 31 function words out of 100 words, and the CSJ test set included 34 function words out of 100 words.

Target words were extracted from continuous speech
                           4
                        
                        
                           4
                           When segmenting speech signals, chopping can often introduce artifacts; for example, sudden onsets and offsets can sound like plosives. In Japanese, however, the set of available syllables consists of many CV pairs (ka, ki, ku, ke, ko, etc.) and five V sounds (a, i, u, e, o), and thus such problems rarely occur in natural speech. Segmentation was done by a student engaged in speech research, who carefully listened to each segment and cut it so that the correct syllable could be heard. For this reason, we believe that our speech samples can be appropriately used in our experiment.
                         together with the context sections below:
                           
                              No-context:
                              Target word only;

Target word with one preceding word;

Target word with two preceding words;

Target word with one preceding and one succeeding word;

Target word with two preceding and two succeeding words.

Examples of the instructions for the above contexts are shown in Fig. 2
                        .

Here /·/ denotes standard Japanese pronunciation. Trigrams can be compared in the case of ASR systems, but speech is influenced by preceding and succeeding sounds (which, we call “coarticulation”). Humans cannot make use of acoustic knowledge without the context of the succeeding word, and thus acoustic variation caused by coarticulation unreasonably degrades human recognition performance only in the “two preceding words” context. In this paper, we consider coarticulation to occur only at word boundaries, which are affected by knowledge of the lexical context. So we also conducted a 1-word-each test in which each context had two words. If we assume that the same number of neighboring context words provides comparable quantities of linguistic information, these two conditions should be comparable. We also tested the 2-words-each context as a new context not used in current ASR systems.

Five subjects listened to each segment with prior knowledge of the word sequence context, but no a priori knowledge was given in the case of ‘no-context.’ They then wrote down the target word they heard.

Target words in each corpus were randomly arranged to preclude the effects of information from other targets and their local contexts. Subjects could repeatedly listen to each word,
                           5
                        
                        
                           5
                           Some ASR systems use several passes to scan speech data in order to locate the best path. In this study we investigated the best human recognition performance based on this type of multiple scan system.
                         but were prohibited from referring back to words they had heard before.

Since all the subjects were students enrolled in a master's program in a field related to speech processing, they were presumed to be familiar with the specific areas of expertise covered by the lectures, such as technical terms and the academic manner of speaking. Familiarity with a specific subject area corresponded to language models which have been adapted to a lecture speech recognition system.
                           6
                        
                        
                           6
                           The human subjects were identical for both experiments using read and spontaneous speech. This may be inconsistent with the experiments in Section 3, in which we used language models for read speech to recognize read speech, and those for lecture speech to recognize spontaneous speech. Although humans can adapt to different speaking styles very easily, automated systems cannot. This ability, which must be investigated in the future, is not discussed in this paper.
                        
                     

Since the same target words were used under all conditions, there was a possibility that this might influence the results. To avoid this, we were especially careful about the order of the tests and the intervals between tests. We arranged the tests as follows: (1) no-context, (2) 1-preceding word, (3) 2-preceding-words, (4) 1-word-each, and (5) 2-words-each. Just after taking the 2-preceding-words test, the subject might remember the context, and use the two preceding words and the succeeding word to recognize the target word. So at least two weeks passed between the 2-preceding-words test and the 1-word-each test to avoid any influence of the former on the latter. In other words, the context of the former test was partially included in the context of the latter test to recognize the same word, and thus more information could have been given to the subjects than intended. On the other hand, we did not impose a long waiting period between the no-context, 1-preceding-word, and 2-preceding-words tests, nor between the 1-word-each and 2-words-each tests. This was because the no-context-word test had no context, so the result of the 1-preceding-word test was not influenced by the no-context test, and likewise, the 2-preceding-words test was not influenced by the 1-preceding-words test. In the same way, the contexts of the 1-word-each test were completely included in the contexts of the 2-words-each test, and thus the result of the 2-words-each test was not influenced by the 1-word-each context test, even if the subjects temporarily memorized the contexts presented in the 1-word-each test.

We applied some corrections to the test results. Some homonym pairs were considered to be the same word due to similar meanings. We also considered variations in pronunciation. For example, the syllable /zu/ is sometimes pronounced /tsu/, but since this variation does not affect the understanding of the words, we judged the answer as correct even if a subject replaced /zu/ with /tsu/ in their answer.

We conducted significance tests of the mean difference of binomial distributions (Nakagawa, 1999) based on the binomial test (Siegel, 1988) on some comparisons of the experimental results.

@&#RESULTS@&#


                        Table 1
                         shows human word recognition results for words of various lengths (in terms of number of syllables), and for various word contexts in terms of the number of preceding and/or succeeding words. We can see the effect of word length on recognition performance. For both read and spontaneous speech, performance under the 1-preceding-word and 2-preceding-words conditions was much better than under the no-context condition. Under the no-context condition, the more syllables in a word, the better performance tended to be. Short words were difficult to recognize due to the small amount of acoustic information, but the constraints of linguistic information were used effectively. This linguistic information seems analogous to n-gram language models (in this case, a bigram and a trigram)
                           7
                        
                        
                           7
                           In this paper, we assume that the amount of linguistic information is proportional to the length of the word context. Strictly speaking, the quantity of information may differ among contexts of the same word length, but here we discuss the statistical tendencies of information quantity based simply on context word length.
                         used in machine speech recognition.

An n-gram is a model that incorporates the statistics of language using the following approximations:


                        
                           
                              (1)
                              
                                 P
                                 (
                                 W
                                 )
                                 =
                                 P
                                 (
                                 
                                    w
                                    1
                                 
                                 ,
                                 
                                    w
                                    2
                                 
                                 ,
                                 …
                                 ,
                                 
                                    w
                                    n
                                 
                                 )
                              
                           
                        
                        
                           
                              (2)
                              
                                 =
                                 P
                                 (
                                 
                                    w
                                    1
                                 
                                 )
                                 P
                                 (
                                 
                                    w
                                    2
                                 
                                 |
                                 
                                    w
                                    1
                                 
                                 )
                                 P
                                 (
                                 
                                    w
                                    3
                                 
                                 |
                                 
                                    w
                                    1
                                 
                                 ,
                                 
                                    w
                                    2
                                 
                                 )
                                 ⋯
                              
                           
                        
                        
                           
                              (3)
                              
                                 =
                                 
                                    ∏
                                    
                                       i
                                       =
                                       1
                                    
                                    n
                                 
                                 P
                                 (
                                 
                                    w
                                    i
                                 
                                 |
                                 
                                    w
                                    1
                                 
                                 ,
                                 …
                                 ,
                                 
                                    w
                                    
                                       i
                                       −
                                       1
                                    
                                 
                                 )
                              
                           
                        
                        
                           
                              (4)
                              
                                 ≈
                                 
                                    ∏
                                    
                                       i
                                       =
                                       1
                                    
                                    n
                                 
                                 P
                                 (
                                 
                                    w
                                    i
                                 
                                 |
                                 
                                    w
                                    
                                       i
                                       −
                                       n
                                       +
                                       1
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    w
                                    
                                       i
                                       −
                                       1
                                    
                                 
                                 )
                                 .
                              
                           
                        
                     

Thus, a long context including preceding words is approximated by a short one (typically 2 words). Although an approximation is applied to the transformation from Eq. (3) and (4), such modeling has achieved good performance in read speech recognition when used in combination with HMM acoustic models.
                           8
                        
                        
                           8
                           We evaluated variations in contexts such as 1-word-each and 2-words-each. Here, we evaluate not only n-gram-like contexts 
                                 P
                                 (
                                 
                                    w
                                    i
                                 
                                 |
                                 
                                    w
                                    
                                       i
                                       −
                                       n
                                       +
                                       1
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    w
                                    
                                       i
                                       −
                                       1
                                    
                                 
                                 )
                               appearing in Eq. (4), but also in other contexts as shown below:
                                 
                                    (5)
                                    
                                       P
                                       (
                                       
                                          w
                                          i
                                       
                                       |
                                       
                                          W
                                          i
                                          I
                                       
                                       )
                                       =
                                       P
                                       (
                                       
                                          w
                                          i
                                       
                                       |
                                       
                                          w
                                          1
                                       
                                       ,
                                       …
                                       ,
                                       
                                          w
                                          
                                             i
                                             −
                                             1
                                          
                                       
                                       
                                          w
                                          
                                             i
                                             +
                                             1
                                          
                                       
                                       ,
                                       …
                                       ,
                                       
                                          w
                                          I
                                       
                                       )
                                    
                                 
                              
                              
                                 
                                    (6)
                                    
                                       ≈
                                       P
                                       (
                                       
                                          w
                                          i
                                       
                                       |
                                       
                                          w
                                          
                                             i
                                             −
                                             n
                                             +
                                             1
                                          
                                       
                                       ,
                                       …
                                       ,
                                       
                                          w
                                          
                                             i
                                             −
                                             1
                                          
                                       
                                       ,
                                       
                                          w
                                          
                                             i
                                             +
                                             1
                                          
                                       
                                       ,
                                       …
                                       ,
                                       
                                          w
                                          
                                             i
                                             +
                                             n
                                             −
                                             1
                                          
                                       
                                       )
                                       .
                                    
                                 
                              
                           
                        
                     


                        Table 2
                         shows the perplexities
                           9
                        
                        
                           9
                           Word probabilities are predefined for all words, and we obtained the perplexity to calculate the inverse number of the geometric mean of the probabilities. Perplexities obtained from test sets are called test set perplexities. For example, when using trigrams, perplexity 
                                 PP
                                 =
                                 
                                    
                                       
                                          ∏
                                          
                                             
                                                w
                                                i
                                             
                                             ∈
                                             W
                                          
                                       
                                       P
                                       (
                                       
                                          w
                                          i
                                       
                                       |
                                       
                                          w
                                          
                                             i
                                             −
                                             2
                                          
                                       
                                       ,
                                       
                                          w
                                          
                                             i
                                             −
                                             1
                                          
                                       
                                       )
                                    
                                    
                                       −
                                       1
                                       /
                                       n
                                    
                                 
                              , where W is the set of target words and n
                              =|W|.
                         for each word length with language models for read and spontaneous speech as described in Section 3.2. Perplexity expresses a branching factor based on “entropy,” so it is expected to correlate with predictability. Words with only one syllable in Japanese mainly consist of post-positional particles and auxiliary verbs, which are function words located just after content words such as nouns and verbs. The predictability of particles tends to be high when using a bigram or a trigram. The prediction performance of target words with word context information only (that is, without acoustic information) is shown in Table 3
                        . In this experiment, contexts were only shown to the subjects on paper. Both the 1-word-each and 2-preceding-words contexts have two words of context, but prediction performance in the 1-word-each context was superior to that in the 2-preceding-words context.

Human word recognition performance under the 1-word-each condition was also superior to the 2-preceding-words condition (see Table 1). One possible difference between the 2-preceding and 1-word-each conditions was knowledge about inter-word coarticulation between the target and succeeding words. To see the effect of this, we removed the linguistic context. Table 4
                         compares recognition performances with and without preceding and succeeding syllables. An example is given in Fig. 3
                        .

We showed subjects the 1-syllable-each context on paper (in most cases, a syllable corresponded to a Japanese hiragana character), and the subjects listened to the segment of the target word with these preceding and succeeding syllables. Because hiragana characters are phonograms, the subjects could not understand any meaning based on one hiragana syllable alone. However, they could use knowledge of acoustic coarticulation at the beginning and end of the word in combination with this information. Comparing the improvement in recognition from the no-context condition to the 1-syllable-each condition, and from the 1-syllable-each condition to the 1-word-each condition, we can observe that acoustic information has a larger effect on performance improvement (no-context vs. 1-syllable) than linguistic information (1-syllable-each vs. 1-word-each). The results from Tables 1, 2, and 4 are summarized in Fig. 4
                        , in which we can see the relationship between recognition performance and quantities of acoustic and linguistic information. In particular, in the case of “no-context,” the recognition rate for 2-syllable words in spontaneous speech was much lower than in read speech. But the rates were almost identical between read and spontaneous speech in the one-syllable-each context. Coarticulation related to neighboring syllables strongly affected human recognition performance. The recognition rate of 1-syllable-words in spontaneous speech, however, is inferior to that of read speech, even though the perplexities of spontaneous speech are less than those of read speech (see Table 2). This suggests that segment lengths of one syllable do not provide enough acoustic information, and that the lack of acoustic information causes the lower recognition rates for one-syllable-words in spontaneous speech as compared to read speech (Table 1). To separate the effects of acoustic and linguistic information, we conducted another recognition experiment involving typical syllable recognition in read speech, and compared the results with those in Table 4. We prepared 30 random syllable sequences consisting of 10 syllables each. These sequences had no linguistic constraints other than the normal phonotactic syllable restrictions in Japanese. Three male speakers each read ten of these nonsense syllable sequences, selected exclusively from the above 30 sequences. Consequently, we obtained 30 unique utterances from which we randomly selected 100 syllables as targets (excluding the first and last syllables in each sequence, since for these syllables the 1-syllable-each context condition was not possible). The selected syllables were manually segmented with either no context or with only one preceding and one succeeding syllable. Five subjects listened to the segments and noted what targets they heard under identical conditions as in the no-context and 1-syllable-each experiments from Table 4. Results are shown in Table 5
                        . The samples can be considered read speech, and the recognition rate with the 1-syllable-each condition was indeed a little lower than in Table 4 (see the column “word length = 1 syllable”), but still better than with spontaneous speech. However, the recognition rates under the no-context condition in Table 5 were not very high, being almost equal to those of read and spontaneous speech in Table 4. These results indicate that acoustic contexts drastically improved recognition performance, and that unigram language information improved it even further.

Note that the case of word length = 1 in the 1-word-each condition in Table 3 corresponds to the effect of the linguistic information that the target word is actually a word, whereas the 1-syllable-each condition in Table 5 corresponds to the effect of acoustic information about the target. Comparison of these results indicates that acoustic information about the target word improved recognition/prediction performance more than linguistic information about it.

Having discovered the effect of acoustic contexts on human syllable/word recognition, significant performance differences still remain between the 1-syllable-each and 1-word-each conditions. If we assume that inter-word coarticulation affected only the acoustics of neighboring syllables, then the above difference was caused solely by the difference in the quantity of linguistic information. Performance differences between these two conditions were almost identical for read and spontaneous speech, therefore we believe that the large difference was caused by the amount of linguistic information. In Table 1 we also found that increasing the amount of linguistic information even marginally improves recognition performance (i.e., when comparing the 1-word-each and 2-words-each conditions). As shown in Table 3, with only linguistic information, there was remarkable improvement in prediction performance when the context became larger. However, 1-word-each linguistic information was sufficient for short words and acoustic information was relatively more important than linguistic information for long words. Thus, increasing linguistic information serves to improve human recognition performance much less than increasing acoustic information, as far as using local word contexts.

We then tested an automatic speech recognizer under almost the same conditions as in the human speech recognition experiments described in Section 2.

@&#METHODOLOGY@&#

We chose the same target words used in Section 2 and manually segmented each word with a preceding and succeeding word. We calculated the acoustic likelihood for each word in the vocabulary with a concatenated word HMM, as shown in Fig. 5
                        . We set the correct context words at preceding and succeeding positions and set each word in the vocabulary in the target position. Then we added the linguistic score (in the case of trigram 
                           P
                           (
                           
                              w
                              i
                           
                           |
                           
                              w
                              
                                 i
                                 −
                                 2
                              
                           
                           ,
                           
                              w
                              
                                 i
                                 −
                                 1
                              
                           
                           )
                        , where 
                           
                              w
                              i
                           
                         was the target word, for example), with an appropriate weight, to the acoustic score to obtain the total score. Word boundaries were automatically and optimally decided by the acoustic models, and thus we were able to avoid the effects of manual segmentation, and the issue of differences in optimal boundaries between humans and machines.

The aim of this experiment was to discover the upper limit of ASR performance in the framework of a combination of acoustic and linguistic scores. Therefore we used various weighting balances to combine these scores and then selected the factor with the best recognition rate for each combination.

Speech data were sampled at a sampling frequency of 16kHz, and the signal was pre-emphasized by a factor of 0.97. A Hamming window of 25ms in length was applied and shifted at 10ms intervals. 39 dimensional feature vectors were used, including 12-dimensional MFCCs, log power, and their first and second deviation coefficients, and the first and second deviations. MFCCs were derived from speech using 24-dimensional Mel filter banks with an HTK tool ver. 3.4.1 (Young et al., 2000).

For read speech, 928 left-context-dependent HMMs (116 syllables × 8 left-contexts consisting of 5 vowels, /N/, silence, and short pause) were trained using 27,992 utterances read by 175 male speakers (JNAS corpus (Itou et al., 1999)). For spontaneous speech, we trained the context-dependent HMMs using lecture speech (part of the CSJ corpus (Maekawa, 2003) consisting of 814 lectures presented by male speakers). Each continuous HMM had five states, four of which had pdfs of output probability. Each pdf consisted of 32 Gaussians with diagonal covariance matrices. Both HMM sets were speaker-independent, and no speaker adaptation was applied.

We used two trigram language models, both of which consisted of 20 K vocabulary words. The model for read speech was trained with 90 million words from 45 months of text from the Japanese newspaper “Mainichi”. For spontaneous speech, we used the trial version of a model trained at Kyoto University using lecture speech transcriptions, which was provided with the CSJ corpus (7 million words). Each model covered all the target words in each test condition.
                           10
                        
                        
                           10
                           In this experiment, humans were at a disadvantage because they have a much larger vocabulary. But generally, ASR performance with a larger vocabulary is not much inferior to ASR performance with a relatively smaller vocabulary (Kawahara et al., 2000). In the ASR prediction experiment, performance is not affected by vocabulary size if word coverage by the vocabulary does not change or if perplexity changes only slightly (in our experiment there were no unknown words). For these reasons, the results of the investigation should not change if the size of the vocabulary is increased. Also note that humans cannot adapt to domains with short contexts, which is a disadvantage for them.
                        
                     

@&#RESULTS@&#


                        Table 6
                         shows recognition results using an automatic speech recognizer for words with various numbers of syllables, and for various combinations of acoustic and language models (acoustic models only, language models only, and their combinations – using only the language models corresponds strictly to the prediction results). We also incorporated parts of the results in Table 6 into Fig. 6
                        ,
                           11
                        
                        
                           11
                           Note that the tendencies of graphs in our previous work with context-independent HMMs (Kitaoka et al., 2003) were almost the same as those shown in Fig. 6 and thus the results shown in this figure are very likely to be reliable.
                         along with perplexities from Table 2.

The above results can be compared with human speech recognition/prediction performance as described in Tables 1, 3 and 4.

Using only acoustic models (AS: corresponding to the no-context or 1-syllable-each conditions in Tables 1 and 4), we achieved 60.0% and 57.0% recognition rates for read and spontaneous speech, respectively. When we also used a unigram (AS+LS(uni.)), bigram (AS+LS(bi.)), or trigram (AS+LS(tri.)), we obtained gradual improvements in performance over “acoustic models only” approach.

We measured the prediction performance of the language models using a method similar to the one used in the prediction experiments with text only for humans (see Table 3). The system selected the word with the highest conditional probability within a given context.
                           12
                        
                        
                           12
                           We provided the language model with the correct context and compared recognition performance with the correct context between humans and machines. ASRs sometimes have to predict words using incorrect contexts, and thus “misrecognition chains” may occur, a phenomenon we do not discuss in this paper. Zhou et al. (1996) reported that 1-best and 10-best word prediction rates using correct contexts were 17% and 52%, respectively, whereas rates using contexts generated by an ASR with an approximately 70% recognition rate were 17% and 47%, respectively, so the actual difference was not so large.
                         When a unigram model (LS(uni.)) was used, the system always predicted the same word,
                           13
                        
                        
                           13
                           The Japanese possessive particle “no” (almost equivalent to “of” in English) was selected.
                         and the prediction rate with text only was identical to the rate of appearance of the word in the texts. The system using the trigram model only (LS(tri.)) achieved 24.0% and 27.0% prediction rates for read and spontaneous speech, respectively.

Comparing the results of AS and AS+LS(uni.) in Table 6 with the no-context results in Table 1, the absolute performance of the recognition system was a little inferior to that of humans except in the case of AS+LS(uni.) for spontaneous speech. However, since the speech segments of target words with preceding and succeeding words were aligned using the acoustic models illustrated in Fig. 5, and the acoustic models were context-dependent, we should compare these results with the 1-syllable-each context in Table 4.

In contrast, prediction performance using trigram language models only (LS(tri.)) surpassed humans under the 2-preceding words and the 1-word-each conditions in Table 3. In general, humans predicted words more accurately when given longer contexts, but the performance of n-gram models was not improved with a larger n (Owens et al., 1999). Goodman (2001) showed that higher-order n-grams only resulted in small improvements in machine recognition, which is therefore not of practical importance. Thus, even though prediction performance under the 2-words-each condition was much better than 2-preceding-words and the 1-word-each conditions when using only text, we cannot expect improvement in the performance of the n-gram model with greater n values. Indeed, good prediction performance under the 2-words-each condition led to improvement in recognition performance over the 1-word-each condition in Table 1, but the difference between the results of AS+LS(uni.) and AS+LS(tri) in Fig. 6 shows the degree of improvement achieved by using the language model, and implies that low AS performance has more impact on total recognition performance.


                        Table 7
                         shows the details of a comparison between recognition using only acoustic models and using a combination of acoustic and linguistic models. In the case of read speech recognition, the combination of the unigram language model and the acoustic models slightly exceeded recognition using acoustic models alone, but many samples which were only recognized correctly by the acoustic models were misrecognized when both acoustic models and the unigram model were used, and vice versa. Therefore, the total recognition rates were almost the same. The recognition rate was improved using the trigram model, which correctly recognized 60.0% of misrecognized words that occurred when using acoustic models alone (0.60 = 24.0/40.0), but a few samples correctly recognized using acoustic models alone were misrecognized when using a combination of acoustic and trigram models (only 5%; 0.05 = 3.0/60.0).

Regarding spontaneous speech recognition, 7% of correctly recognized words using the acoustic models were misrecognized by the trigram model (0.07 = 4.0/57.0). These results indicate that the current combination of models while leading to some improvement, also results in poorer recognition performance in certain cases. Table 8
                         compares the prediction performance of the language models with the recognition performance of the combination of language and acoustic models. In this case, the recognition rate using a combination of models was much superior to the prediction rate using a language model alone, because the combined models correctly recognized 78.1% of incorrectly predicted words (0.781 = 57.0/73.0). But a few samples predicted by the language model were misrecognized by the combination of models, e.g., 14% (0.14 = 4.0/27.0) of words correctly predicted by the trigram were misrecognized during spontaneous speech recognition. We can see the relationship between the confidence level of the acoustic scores, the confidence level of the language scores (corresponding to perplexities), and the effect of language models by comparing AS and AS+LS(tri.) in Table 6. Generally, as words become longer, acoustic scores tend to become more confident because of the rich quality of acoustic information. However, longer words tend to have greater perplexities (see Table 2), thus lowering the confidence level of the language scores. For this reason, improvements using the language model diminish for the recognition of longer words. There are obviously problems with the current combination method which point to a need for the development of a new method.

@&#DISCUSSION@&#

Human and machine syllable and word recognition performance, together with word prediction performance with text only, are summarized in Table 9
                        . First we compare the performance of ASR and HSR, and then we discuss the possibility of improvement of ASR performance based on these results.

In comparing human syllable recognition performance with machine recognition performance, as discussed in previous studies regarding read speech (Nakagawa et al., 1999), our experiments showed that the syllable recognition performance of the automatic speech recognizer was inferior to human syllable recognition performance. These results only detail the amount of acoustic information used for recognition, as no linguistic information was provided. On the other hand, prediction performance of the ASR using the trigram language model only, which varied the quantity of linguistic information used, was superior to the prediction performance of humans who knew the two preceding words, the one preceding and one succeeding words, or the two words preceding and the two words succeeding a target word. This result was consistent with Owens et al. (1999) which proved that when only an n-gram model was used, the prediction performance of an n-gram language model was superior to humans who knew n words of context when n was large.

The literature on psycholinguistics points out that word frequencies play a prominent role in the rate at which humans correctly understand words.
                           14
                        
                        
                           14
                           This condition corresponds to the ASR experiments using unigram language models.
                         Contexts also affect the accuracy of human word recognition. For example, the word four is a higher-frequency word than ford, but the latter may be much more likely in a conversation about cars, and thus the word tends to be recognized correctly in conversations on cars (Cutler, 2012). This phenomenon is well supported by the result of our prediction experiments above. This is also similar to the constraint by the n-gram model in ASR, but n-gram model prediction performnance was superior to that of humans. This may be because the n-gram model is obtained using strict statistics, and thus machine prediction performance is mathematically optimized. The prediction performance can be seen as an upper bound in “a few preceding words” contexts.

However, when combinations of acoustic and linguistic information were provided (indicated as ‘word’ in Table 9, under the 2-preceding words condition and the 1-word-each condition for humans, and as AS+LS(tri.) for the machine), the performance of the recognizer was greatly inferior to that of humans. Comparing these two human recognition conditions, recognition under the 1-word-each condition (1-1) was superior to that under the 2-preceding-words (trigram) condition.

Previously, we conducted almost the same experiments using different stimuli and different acoustic models (Kitaoka et al., 2003), and our findings reveals the same tendency as in this paper. This supports our belief that experimental results are highly reliable.

Triphone acoustic modeling has proven to be effective for speech recognition in most languages (Gales et al., 2007; Schwartz et al., 2004). N-gram language modeling has also been proven to be effective, not only for Indo-European languages, but also for others (Schwartz et al., 2004; Kirchhoff et al., 2006), including Altaic languages such as Japanese, whose grammars are very different from Indo-European languages. The tendency that the effectiveness of n-gram language models is saturated at n
                        =3 or 4 is common to most languages. And the performance of HSR is almost always superior to that of ASR, regardless of language (Meyer et al., 2006; Scharenborg and Cooke, 2008). Based on these facts, we believe that the comparison of ASR and HSR, and the comparison of acoustic and language models, can be generally applied to most languages.

We may be able to improve language models, as suggested by the high human prediction performance under the 2-word-each context (see Table 3), but due to the difficulty in controlling the decoder when using such linguistic contexts, and the insufficiency of the data for estimating the statistics, the trigram language model seems to be a more suitable method to adopt.

It is clear that acoustic models, especially, must be improved. The results in Section 2.3 indicate that we must carefully model inter-word coarticulation caused by neighboring syllables during spontaneous speech. Part-of-speech dependent modeling, or position-in-a-word dependent modeling (Isogawa et al., 2002; Higuchi et al., 2001), for example, are other approaches that may also help upgrade performance. Thus, we readily conclude that acoustic modeling and the combination method have to be improved.

Regardless, our results support the argument (Nakagawa, 1999, 2005) that much more information is available from acoustic models than from language models, thus acoustic models should be improved independently, whether language models are improved or not.

So far, we have discussed the potential usefulness of acoustic and language models based on a comparison of human speech recognition performance and machine speech recognition performance. Here we discuss speech recognition using syllable-based acoustic models and trigram language models from the viewpoint of information theory.

We calculate the mutual information between vocabulary words V and observation vectors Y, or language models L, as follows:


                        
                           
                              (7)
                              
                                 I
                                 (
                                 V
                                 ;
                                 Y
                                 )
                                 =
                                 H
                                 (
                                 V
                                 )
                                 −
                                 H
                                 (
                                 V
                                 |
                                 Y
                                 )
                              
                           
                        
                        
                           
                              (8)
                              
                                 ≈
                                 H
                                 (
                                 V
                                 )
                                 −
                                 
                                    ∑
                                    i
                                 
                                 H
                                 (
                                 
                                    C
                                    i
                                 
                                 |
                                 
                                    Y
                                    i
                                 
                                 )
                                 ,
                              
                           
                        
                        
                           
                              (9)
                              
                                 I
                                 (
                                 V
                                 ;
                                 L
                                 )
                                 =
                                 H
                                 (
                                 V
                                 )
                                 −
                                 H
                                 (
                                 V
                                 |
                                 L
                                 )
                                 ,
                              
                           
                        where V
                        =
                        C
                        1
                        C
                        2
                        ⋯
                        C
                        
                           m
                         and C
                        
                           i
                         denote a syllable.

Now we assume the vocabulary size as 20,000,
                           15
                        
                        
                           15
                           When assuming the vocabulary size is 80,000, Eq. (10) becomes H(V)=−log
                              2(1/80, 000)=16.3. If we assume that each word consists of three syllables and that the Japanese language has 120 syllables, H(V)≈−log
                              2(1/1203)=20.7. The difference between these assumptions only adds a bias to H(V), and thus reaches the same conclusion.
                         so


                        
                           
                              (10)
                              
                                 H
                                 (
                                 V
                                 )
                                 =
                                 −
                                 
                                    log
                                    2
                                 
                                 
                                    
                                       
                                          
                                             1
                                             
                                                20
                                                ,
                                                000
                                             
                                          
                                       
                                    
                                 
                                 =
                                 14.3
                                 
                                 
                                    [
                                    bits
                                    ]
                                 
                                 .
                              
                           
                        
                     


                        H(V|L) is a logarithm of perplexity P, that is, log
                        2(P). When assuming P
                        =200 (as in the speech recognition experiments using trigram language models in this paper (see Table 2), entropy becomes:
                           
                              (11)
                              
                                 H
                                 (
                                 V
                                 |
                                 L
                                 )
                                 =
                                 7.6
                                 
                                 
                                    [
                                    bits
                                    ]
                                 
                                 .
                              
                           
                        However, the number of syllables in Japanese is approximately 120, and the syllable recognition rate was about 75% (Yamamoto and Nakagawa, 2000; Nakagawa et al., 1999). Here we assume that each word consists of three syllables. If a given syllable is mistaken for a particular syllable (that is, the most disproportionate case), the entropy per syllable H(C
                        
                           i
                        |Y
                        
                           i
                        ) is:


                        
                           
                              
                                 
                                    H
                                    min
                                 
                                 (
                                 
                                    C
                                    i
                                 
                                 |
                                 
                                    Y
                                    i
                                 
                                 )
                                 =
                                 −
                                 0.75
                                 ×
                                 
                                    log
                                    2
                                 
                                 (
                                 0.75
                                 )
                                 −
                                 0.25
                                 ×
                                 
                                    log
                                    2
                                 
                                 (
                                 0.25
                                 )
                                 =
                                 0.81
                                 
                                 
                                    [
                                    bits
                                    ]
                                 
                                 ,
                              
                           
                        or, conversely, when the syllable is randomly mistaken for another syllable (with 25% probability):


                        
                           
                              
                                 
                                    H
                                    max
                                 
                                 (
                                 
                                    C
                                    i
                                 
                                 |
                                 
                                    Y
                                    i
                                 
                                 )
                                 =
                                 −
                                 0.75
                                 ×
                                 
                                    log
                                    2
                                 
                                 (
                                 0.75
                                 )
                                 −
                                 0.25
                                 ×
                                 
                                    log
                                    2
                                 
                                 
                                    
                                       
                                          0.25
                                          ×
                                          
                                             1
                                             119
                                          
                                       
                                    
                                 
                                 =
                                 2.54
                                 
                                 
                                    [
                                    bits
                                    ]
                                 
                                 .
                              
                           
                        Thus entropy per word H(V|Y) without any language information must satisfy the following (Nakagawa, 1992):


                        
                           
                              (12)
                              
                                 3
                                 ×
                                 
                                    H
                                    min
                                 
                                 (
                                 
                                    C
                                    i
                                 
                                 |
                                 
                                    Y
                                    i
                                 
                                 )
                                 =
                                 2.43
                                 ≤
                                 H
                                 (
                                 V
                                 |
                                 Y
                                 )
                                 ≤
                                 3
                                 ×
                                 
                                    H
                                    max
                                 
                                 (
                                 
                                    C
                                    i
                                 
                                 |
                                 
                                    Y
                                    i
                                 
                                 )
                                 =
                                 7.60
                                 
                                 [
                                 bits
                                 ]
                                 .
                              
                           
                        If the recognition rate in N-best candidates with small N is high, entropy should be small. For example, assuming the N-best syllable recognition rates shown in Fig. 7
                        ,
                           16
                        
                        
                           16
                           This assumption is based on our continuous syllable recognition results (Oshikawa et al., 2004).
                         the maximum value of H
                        
                           upper
                        (C
                        
                           i
                        |Y
                        
                           i
                        ) equals −∑
                           k
                        
                        p(k)log
                        2
                        p(k), where p(k) is the rate at which the kth syllable candidate is correct (Shannon, 1951). Then H(V|Y) must satisfy:


                        
                           
                              (13)
                              
                                 2.43
                                 ≤
                                 H
                                 (
                                 V
                                 |
                                 Y
                                 )
                                 ≤
                                 3
                                 ×
                                 
                                    H
                                    upper
                                 
                                 (
                                 
                                    C
                                    i
                                 
                                 |
                                 
                                    Y
                                    i
                                 
                                 )
                                 =
                                 4.52
                                 
                                 [
                                 bits
                                 ]
                                 .
                              
                           
                        Thus, the mutual information becomes
                           17
                        
                        
                           17
                           This entropy corresponds to the linguistic information for isolated word recognition with a vocabulary size of 1000–2000 words. Speech recognition with such a limited vocabulary size achieves nearly perfect performance using state-of-the-art recognition techniques. This comparison gives an intuitive understanding of the effect of the quantity of information.
                        
                     


                        
                           
                              (14)
                              
                                 I
                                 (
                                 V
                                 ;
                                 Y
                                 )
                                 ≈
                                 9.8
                                 ∼
                                 11.9
                                 
                                 [
                                 bits
                                 ]
                                 ,
                              
                           
                        from Eqs. (7), (10), and (13), and


                        
                           
                              (15)
                              
                                 I
                                 (
                                 V
                                 ;
                                 L
                                 )
                                 ≈
                                 6.7
                                 
                                 [
                                 bits
                                 ]
                                 .
                              
                           
                        from Eqs. (9)–(11). Comparing Eqs. (14) and (15) reveals I(V
                        ;
                        Y)>
                        I(V
                        ;
                        L), which means that acoustic information is more informative than linguistic information.

Supposing that we obtain 10.3bits of mutual information between V and Y, that is, H(V|Y)=4.0. To obtain the same quantity of information to improve I(V
                        ;
                        L) in Eq. (15), we have to use a language model with 16.0 (=214.3−10.3) of perplexity; but this is impossible, even if machines use a longer context as humans do (Nakagawa, 1992; Shannon, 1951). Cover and King (1978) estimated the entropy of English at 1.3bits/letter in an alphabet prediction experiment. Considering that one word consists of approximately five to six phonemes (Nakagawa, 1992), entropy per word is estimated to be from 6.5 to 7.8bits (a perplexity of about 100–200). Hence, a perplexity of 16.0 corresponds to a very limited task/domain. This result is consistent with our conclusion in Section 3.3 that we cannot expect improvement in recognition performance with greater n of an n-gram language model than the n
                        =3 of a trigram, even though prediction performance is improved (for example, if the perplexity of a language model becomes 64, H(V|L)=6.0 and I(V
                        ;
                        L)=8.3). This means I(V
                        ;
                        Y)≥
                        I(V
                        ;
                        L) even with an improved language model, because the improvement of mutual information between V and L is relatively smaller than that between V and Y in speech recognizers that use acoustic information Y and linguistic information L.

@&#CONCLUSION@&#

In this paper, we investigated human speech recognition performance using experiments where listeners used acoustic knowledge and information about the local context to recognize words and word sequences taken from Japanese read and spontaneous speech corpora. We discussed the current performance of machines and compared it with human performance.

Short words were difficult for humans to recognize out of context. When given one or two preceding word(s), however, human speech recognition rates improved dramatically, showing that linguistic constraints worked well. These results relate to the high predictability of short words, which mainly consist of particles, when using bigram and trigram language models in machine speech recognition. They also relate to the improvement in human prediction performance when given only one or two preceding word(s) without any acoustic information. As for the one-word-each and two-words-each context conditions, prediction performance improved significantly, but recognition performance did not improve very much. Thus, increasing linguistic information has a relatively small effect on improving human speech recognition performance. The recognition rate of words segmented without any context in spontaneous speech was greatly inferior to that for read speech, but almost the same as in a 1-syllable-each context with no linguistic information. These results suggest that careful modeling of inter-word coarticulation due to neighboring syllables would improve spontaneous speech recognition.

We also conducted machine recognition experiments using an automatic speech recognizer under almost identical conditions as the human speech recognition experiments. Recognition performance of the recognizer was far inferior to human recognition performance when only acoustic information was provided. In contrast, prediction performance of an ASR using the trigram language model was superior to human performance. However, even when acoustic and language models were combined, we could not sufficiently improve the performance of the automatic speech recognizer. This means that if the subjects recognized speech in the same way that machines do, the humans seemed to be using superior acoustic models. This difference in performance could also be interpreted to suggest that humans have a superior means of exploiting context that does not rely on artificial differentiation between a language model and an acoustic model. Investigation of this hypothesis is one of our future projects. Whatever the case may be, we must improve our acoustic models independently in order to improve speech recognition performance, whether or not language models are improved in the future.

@&#REFERENCES@&#

