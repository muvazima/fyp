@&#MAIN-TITLE@&#Locally linear representation Fisher criterion based tumor gene expressive data classification

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Based on the class information, an intra-class graph and inter-class graph are constructed.


                        
                        
                           
                           In the inter-class graph, the reconstruction error denotes the shortest inter-class distance.


                        
                        
                           
                           In the intra-class graph, the reconstruction error means the intra-class data compactness.


                        
                        
                           
                           Experiments on some tumor gene expressive data validate LLRFC׳s superiority.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Dimensionality reduction

Tumor gene expressive data

Feature extraction

Supervised learning

@&#ABSTRACT@&#


               
               
                  Tumor gene expressive data are characterized by a large amount of genes with only a small amount of observations, which always appear with high dimensionality. So it is necessary to reduce the dimensionality before identifying their genre. In this paper, a discriminant manifold learning method, named locally linear representation Fisher criterion (LLRFC), is applied to extract features from tumor gene expressive data. In LLRFC, an inter-class graph and an intra-class graph are constructed based on their genre information, where any tumor gene expressive data in the inter-class graph should select k nearest neighbors with different class labels and in the intra-class graph the k nearest neighbors for any tumor gene expressive data must be sampled from those with the same class. And then the locally least linear reconstruction is introduced to optimize the corresponding weights in both graphs. Moreover, a Fisher criterion is modeled to explore a low dimensional subspace where the reconstruction errors in the inter-class graph can be maximized and the reconstruction errors in the intra-class graph can be minimized, simultaneously. Experiments on some benchmark tumor gene expressive data have been conducted with some related algorithms, by which the proposed LLRFC has been validated to be efficient.
               
            

@&#INTRODUCTION@&#

With the emergence of tumor gene expressive data collected from DNA microarray, it comes true to simultaneously monitor expression of all genes in the genome, which contributes to make insight into biological processes and mechanisms of human diseases. However, how to interpret tumor gene expressive data still needs further demonstration. Up to now, many studies have been reported on tumor gene expressive data analysis [1–8], where key tumor genes selection and molecular classification of cancer are mainly concentrated on. It is the fact that tumor gene expressive data are always characterized by a large amount of variables (genes) with a small amount of observations (samples), thus before carrying out classification on them, some methods are recommended to reduce their dimensionality or extract features.

The popular linear methods involved in tumor gene expressive data analysis are principal component analysis (PCA) [41], partial least squares (PLS) [11,40] and independent component analysis (ICA) [9,10]. However, Pochet et al. systematically proved that nonlinear models are superior to those linear ones on many tumor gene expressive data sets in 2004 [12]. So how to nonlinearly mine the tumor gene expressive data has been attracting a lot of attention and some nonlinear models are presented. Alexandridis et al. put forward a nonlinear method with finite mixture distribution for tumor analysis [13]. Meanwhile, Martella et al. propose a nonlinear factor mixture model, where both factor factorization and normal mixture are integrated [14]. Moreover, other nonlinear feature extraction methods such as kernel methods and manifold learning have also been advanced for tumor gene expressive data analysis.

Unlike kernel methods, which nonlinearly extract features by a kernel transformation, manifold learning is straightforward to explore the inherent nonlinear structure hidden in the high dimensional space. Firstly manifold learning methods approach local manifold structures using k nearest neighbors (KNN), where any point and its k nearest neighbors will be viewed on a local super-plane. Then the locality can be well modeled by handling the linear computational rules in the local patch. At last, manifold learning pursues low dimensional embeddings of the original data by locality preserving. When mapping all the localities into a global framework, although the local geometry is linear, the corresponding global structure still shows its nonlinearity. In the last decade, some classical manifold learning algorithms have been presented. Among them, isometric feature mapping (ISOMAP) [15], Laplacian eigenmaps (LE) [18], locally linear embedding (LLE) [16,17] and their extensions are widely used for feature extraction or dimensionality reduction. They have yielded impressive results on artificial and real world datasets [19–21,42].

LLE is an effective method for data visualization. However, it exposes some limitations when applied to data classification. One is out-of-sample problem [22]. Another limitation is that the classical LLE does not take into account class information of the training samples, which displays negative impacts on the recognition accuracy.

In order to avoid the problem mentioned above, more and more supervised versions of LLE have been presented to deal with data classification. In the original LLE, the manifold local geometry is usually explored using KNN, where Euclidean distance is involved. In most cases, some points with different labels may also have a shorter Euclidean distance than those with the same class, which results in wrong neighborhoods for classification because some nearest neighbors are from those data with different classes. To address the problem, a method is brought forward to adjust neighborhood weights using class information, where the distance between any two points belonging to different classes is defined to be relatively larger than its Euclidean distance while those distances between points with the same label are preserved. The work is first presented by de Ridder et al. [23]. Instead of enlarging the between-class distances, Wen et al. utilize a nonlinear function to shrink the within-class distances [24], which shows similar impacts on recognition performance. These methods either enlarge between-class distances or shrink within-class distances. Thus Zhang poses an enhanced supervised model of LLE by reducing within-class distance and expanding between-class distance simultaneously [25]. Later, Zhang and Zhao define a probability-based distance that can enlarge the Euclidean distance for labeled and unlabeled points [29,30]. Combining to the class information, these methods endeavor to increase the accuracy of LLE by adjusting the distances between neighborhood points rather than by selecting the neighborhoods points. Thus Hui et al. [26] and Zhao et al. [27] impose a strict constraint that only points with the same class can be considered to be k nearest neighbors. But it must be noted that the neighborhoods points determined by the method mentioned above will be not enough to explore the manifold geometry structure when they are not densely sampled. Therefore, Han et al. propose a method to make a supplement [28]. According to the ascending Euclidean distances, the same class samples are predefined as neighborhood points, and then the remaining neighbors are searched from those with different classes. Moreover, to overcome out-of-sample problem, Kokiopouloua et al. propose an orthogonal neighborhood preserving projection (ONPP) method, which introduces a linear transformation to minimize the reconstruction errors in low dimensional space [43]. Later, Kokiopouloua et al. define a repulsion graph to extract supervised features, where an objective function is constructed to minimize the weighted difference of the reconstruction errors to distances between any two points with different labels in low dimensional space [44]. Similar to Kokiopouloua, Zhang et al. also design an intra-class graph and expect to explore a subspace with the minimum weighted difference of the reconstruction errors in the intra-class graph to distances between any two differently labeled points [45]. On the basis of ONPP, some other methods are presented to set the weights between nodes adaptively [46,47]. However, these modified versions mainly take advantage of class information to adjust the distances between points or to select the neighborhood points in KNN graph, where more parameters are introduced with the augment of the application difficulty.

In addition, some other supervised LLE algorithms combined with LDA have also been boomed. Based on the projection distances of the preprocessed points in LDA subspace, Pang et al. select the k minimum-distance points as the neighbors for each data point and then apply LLE [31]. This method can be viewed as the mode of LDA+LLE because LLE is introduced to extract features from those data handled by LDA. Zhang et al. present a unified framework of LLE and LDA [32,33]. This framework essentially equals to LLE+LDA, where LLE is firstly used to project the original data into a subspace and then LDA is employed to extract features discriminatively. Pang et al. also bring forward an integrated model which is linearly constructed by the objective functions of LLE and LDA under some constraints [34]. The model can be changed into LLE or LDA when the coefficient is one or zero, respectively. Furthermore, a local Fisher embedding (LFE) is put forward by de Ridder et al. [35], where local geometry and global class information are absorbed into a Fisher formulation. Li et al. also propose a supervised LLE algorithm named local linear discriminant embedding (LLDE) based on the fact that the embeddings cost function is invariant to translation and rescaling under sum-to-one constraint to the reconstruction weights in LLE, where the translations and the rescalings can be optimized with a modified LDA [36]. In above methods, the class information is globally involved because LDA is introduced to extract features. However, manifold learning is a nonlinear approach by locality learning. Thus it will contribute to explore the local structure discriminatively using the local label information associated to the corresponding points contained in local patch.

In this paper, a discriminant manifold learning method is applied to extract relevant biological correlations or “molecular logic” from tumor gene expression data. In the method, we have taken advantage of genre information of tumor gene expressive data, by which an intra-class graph and an inter-class graph can be constructed, respectively. In the intra-class graph, any point and its k nearest neighbors should be sampled from the same class points. On the contrary, for any points in the inter-class graph, it must select those with different classes to it as its k nearest neighbors. At last a Fisher criterion can be reasoned to find the optimal projection, which cam maximize the reconstruction errors in the inter-class graph and minimize the reconstructions errors in the intra-class graph in the low dimensional space, simultaneously.

The rest of paper is organized as follows. Section 2 describes classical LLE algorithm. Section 3 presents the proposed algorithm. Some experimental results and simulations are offered in Section 4. Then the whole paper is finished with conclusions in Section 5.

Let 
                        X
                        =
                        [
                        
                           
                              X
                           
                           
                              1
                           
                        
                        ,
                        
                           
                              X
                           
                           
                              2
                           
                        
                        ,
                        ..
                        .
                        ,
                        
                           
                              X
                           
                           
                              n
                           
                        
                        ]
                        ∈
                        
                           
                              R
                           
                           
                              D
                              ×
                              n
                           
                        
                      be 
                        n
                      points in high dimensional space. The data are well sampled from a nonlinear manifold. The goal of LLE is to map the high dimensional data into a low dimensional manifold space with dimensionality 
                        d
                     (
                        d
                        ⪡
                        D
                     ). Let us denote the corresponding set of 
                        n
                      points in the embedding space as 
                        Y
                        =
                        [
                        
                           
                              Y
                           
                           
                              1
                           
                        
                        ,
                        
                           
                              Y
                           
                           
                              2
                           
                        
                        ,
                        ..
                        .
                        ,
                        
                           
                              Y
                           
                           
                              n
                           
                        
                        ]
                        ∈
                        
                           
                              R
                           
                           
                              d
                              ×
                              n
                           
                        
                     . The outline of LLE can be summarized as follows:
                        
                           Step 1: For each data point 
                                 
                                    
                                       X
                                    
                                    
                                       i
                                    
                                 
                              , identify its 
                                 k
                               nearest neighbors by KNN.

Step 2: Compute the optimal reconstruction weights which can minimize the error of linearly reconstructing 
                                 
                                    
                                       X
                                    
                                    
                                       i
                                    
                                 
                               by its 
                                 k
                               nearest neighbors.

Step 3: Calculate the low-dimensional embedding 
                                 Y
                               for 
                                 X
                               that best preserves the local geometry represented by the reconstruction weights and the corresponding k nearest neighbors.

In Step 1 Euclidean distance is always used to define neighborhood, which is composed of k points with the sorted bottom Euclidean distances to 
                        
                           
                              X
                           
                           
                              i
                           
                        
                     . Moreover, some sophisticated criteria may also be used, such as Euclidean distance in kernel space or cosine distance.

Step 2 seeks the best reconstruction weights. Optimality is achieved by minimizing the local linear reconstruction error of 
                        
                           
                              X
                           
                           
                              i
                           
                        
                      by its k nearest neighbors,
                        
                           (1)
                           
                              
                                 
                                    ε
                                 
                                 
                                    i
                                 
                              
                              (
                              W
                              )
                              =
                              arg
                              
                              min
                              
                              
                                 
                                    |
                                    |
                                    
                                       
                                          
                                             X
                                          
                                          
                                             i
                                          
                                       
                                       −
                                       
                                          ∑
                                          
                                             j
                                             =
                                             1
                                          
                                          k
                                       
                                       
                                          
                                             
                                                W
                                             
                                             
                                                i
                                                j
                                             
                                          
                                          
                                             
                                                X
                                             
                                             
                                                j
                                             
                                          
                                       
                                    
                                    |
                                    |
                                 
                                 2
                              
                           
                        
                     
                  

Step 3 computes the optimal low dimensional embedding 
                        Y
                      based on the weight matrix 
                        W
                      obtained from Step 2.
                        
                           (2)
                           
                              ε
                              (
                              Y
                              )
                              =
                              arg
                              
                              min
                              
                              
                                 
                                    |
                                    |
                                    
                                       
                                          
                                             Y
                                          
                                          
                                             i
                                          
                                       
                                       −
                                       
                                          ∑
                                          
                                             j
                                             =
                                             1
                                          
                                          k
                                       
                                       
                                          
                                             
                                                W
                                             
                                             
                                                i
                                                j
                                             
                                          
                                          
                                             
                                                Y
                                             
                                             
                                                j
                                             
                                          
                                       
                                    
                                    |
                                    |
                                 
                                 2
                              
                              =
                              
                              min
                              
                              t
                              r
                              {
                              Y
                              M
                              
                                 
                                    Y
                                 
                                 T
                              
                              }
                           
                        
                     
                  

Then a sparse, symmetric and positive semi-definite matrix 
                        M
                      can be defined as follows:
                        
                           (3)
                           
                              M
                              =
                              
                                 
                                    (
                                    I
                                    −
                                    W
                                    )
                                 
                                 T
                              
                              (
                              I
                              −
                              W
                              )
                           
                        
                     
                  

Thus LLE can find that the low dimensional embeddings are the corresponding eigenvectors related to 
                        d
                      bottom eigenvalues except zero of 
                        M
                     .

@&#METHOD@&#

@&#MOTIVATION@&#

The original LLE is often applied for data visualization because it can probe the manifold intrinsic structure embedded in high dimensional data space. However, LLE cannot efficiently extract the features for classification. The reason probably lies in that LLE nonlinearly mines the high dimensional data by preserving the local manifold structure without considering data class information. Due to the characteristic of locality preserving in manifold learning, the local geometry will keep unchanged in the low dimensional space where the embeddings of any point and its k nearest neighbors also consist of the local neighborhood with the same reconstruction weights. If the nearest neighbors of any point contain data labeled different classes in high dimensional space, their embeddings are also among the neighborhood in the low dimensional space, which will results in great difficulties in data recognition.

Therefore, both manifold locality preserving and class information should be considered. On the one hand, the locality should be preserved, which helps to mine the inherent structure in manifold distributed data; on the other hand, the class information can be introduced to supervise selection of k nearest neighbors, which benefits to learn the local structure discriminatively. Similar to constructions of the local graph in Refs. [37–39], an inter-class graph and an intra-class graph characterizing the corresponding data are constructed on the basis of data labels. In the inter-class graph, the nearest neighbors of any point are sampled from points with different labels. While in the intra-class graph, any neighborhood composes of points with the same class. Moreover, KNN is also employed to determine k nearest neighbors either in the inter-class graph or in the intra-class graph, which can approximately approach the locality of manifold. Thus, both class information and manifold locality preserving can be well integrated for feature extraction.

On the basis of the data labels, an intra-class graph can be constructed. For any sample, its k nearest neighbors should have the same label as the sample point; meanwhile, those with the first k bottom Euclidean distances to the sample point are selected as the neighbors. In the intra-class graph, due to local linearity, the sample can be well reconstructed by its k nearest neighbors with the optimal weights, which is stated as follows:
                           
                              (4)
                              
                                 ε
                                 (
                                 
                                    
                                       W
                                    
                                    
                                       intra
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       min
                                    
                                    
                                       
                                          
                                             X
                                          
                                          
                                             j
                                          
                                       
                                       ∈
                                       I
                                       n
                                       t
                                       r
                                       a
                                       N
                                       (
                                       
                                          
                                             X
                                          
                                          
                                             i
                                          
                                       
                                       )
                                    
                                 
                                 
                                    
                                       |
                                       |
                                       
                                          
                                             X
                                          
                                          
                                             i
                                          
                                       
                                       −
                                       
                                          ∑
                                          
                                             j
                                             =
                                             1
                                          
                                          k
                                       
                                       
                                          
                                             (
                                             
                                                
                                                   W
                                                
                                                
                                                   intra
                                                
                                             
                                             )
                                          
                                          
                                             i
                                             j
                                          
                                       
                                       
                                          
                                             X
                                          
                                          
                                             j
                                          
                                       
                                       |
                                       |
                                    
                                    2
                                 
                              
                           
                        where 
                           I
                           n
                           t
                           r
                           a
                           
                           N
                           (
                           
                              
                                 X
                              
                              
                                 i
                              
                           
                           )
                         denotes the intra-class neighborhood of point 
                           
                              
                                 X
                              
                              
                                 i
                              
                           
                        .

Meanwhile, the inter-class graph can also be established as follows. For any point 
                           
                              
                                 X
                              
                              
                                 i
                              
                           
                        , the other points are sorted according to their ascending Euclidean distances to point 
                           
                              
                                 X
                              
                              
                                 i
                              
                           
                         and the bottom k points with different labels to that of point 
                           
                              
                                 X
                              
                              
                                 i
                              
                           
                         are selected as its inter-class neighbors. The same process is repeated to achieve the optimal reconstruction weights in the inter-class graph.
                           
                              (5)
                              
                                 ε
                                 (
                                 
                                    
                                       W
                                    
                                    
                                       inter
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       min
                                    
                                    
                                       
                                          
                                             X
                                          
                                          
                                             j
                                          
                                       
                                       ∈
                                       I
                                       n
                                       t
                                       e
                                       r
                                       N
                                       (
                                       
                                          
                                             X
                                          
                                          
                                             i
                                          
                                       
                                       )
                                    
                                 
                                 
                                    
                                       |
                                       |
                                       
                                          
                                             
                                                X
                                             
                                             
                                                i
                                             
                                          
                                          −
                                          
                                             ∑
                                             
                                                j
                                                =
                                                1
                                             
                                             k
                                          
                                          
                                             
                                                
                                                   (
                                                   
                                                      
                                                         W
                                                      
                                                      
                                                         inter
                                                      
                                                   
                                                   )
                                                
                                                
                                                   i
                                                   j
                                                
                                             
                                             
                                                
                                                   X
                                                
                                                
                                                   j
                                                
                                             
                                          
                                       
                                       |
                                       |
                                    
                                    2
                                 
                              
                           
                        where 
                           I
                           n
                           t
                           e
                           r
                           
                           N
                           (
                           
                              
                                 X
                              
                              
                                 i
                              
                           
                           )
                         stands for the inter-class neighborhood of point 
                           
                              
                                 X
                              
                              
                                 i
                              
                           
                        .

As mentioned above, both the intra-class graph and the inter-class graph can be constructed with the corresponding weights between nodes. Moreover, LLRFC aims to explore a low dimensional subspace with better classification accuracy, where the data with the same label should be more clustered and the points with different labels should be more apart. In the inter-class graph, the reconstruction error between any point and its k nearest neighbors represents the variance of different classes. In other words, it also means the inter-class distance because the point is differently labeled to its nearest neighbors. At the same time, the distance is the shortest one because its nearest neighbors are involved in calculating the weighted mean. If the shortest inter-class distance can be mapped far in the low dimensional space, other inter-class distances will naturally locate farther. Meanwhile, the reconstruction error in the intra-class graph displays the minimum distance between any point to the weighted mean of its k nearest neighbors with the same classes, which demonstrates the compactness of local intra-class data. Thus to minimize the reconstruction error will contribute to cluster those data in the intra-class graph.

Based on the above analysis, a Fisher criterion can be modeled to maximize the inter-class reconstruction errors and to minimize the intra-class reconstruction errors in the low dimensional space, simultaneously.
                           
                              (6)
                              
                                 S
                                 (
                                 Y
                                 )
                                 =
                                 
                                 max
                                 
                                 
                                    
                                       
                                          ∑
                                          i
                                       
                                       
                                          
                                             
                                                |
                                                |
                                                
                                                   
                                                      
                                                         Y
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   −
                                                   
                                                      ∑
                                                      
                                                         j
                                                         =
                                                         1
                                                      
                                                      k
                                                   
                                                   
                                                      
                                                         
                                                            (
                                                            
                                                               
                                                                  W
                                                               
                                                               
                                                                  inter
                                                               
                                                            
                                                            )
                                                         
                                                         
                                                            i
                                                            j
                                                         
                                                      
                                                      
                                                         
                                                            Y
                                                         
                                                         
                                                            j
                                                         
                                                      
                                                   
                                                
                                                |
                                                |
                                             
                                             2
                                          
                                       
                                    
                                    
                                       
                                          ∑
                                          i
                                       
                                       
                                          
                                             
                                                |
                                                |
                                                
                                                   
                                                      
                                                         Y
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   −
                                                   
                                                      ∑
                                                      
                                                         j
                                                         =
                                                         1
                                                      
                                                      k
                                                   
                                                   
                                                      
                                                         
                                                            (
                                                            
                                                               
                                                                  W
                                                               
                                                               
                                                                  intra
                                                               
                                                            
                                                            )
                                                         
                                                         
                                                            i
                                                            j
                                                         
                                                      
                                                      
                                                         
                                                            Y
                                                         
                                                         
                                                            j
                                                         
                                                      
                                                   
                                                
                                                |
                                                |
                                             
                                             2
                                          
                                       
                                    
                                 
                              
                           
                        
                     

With matrix form, Eqn. (6) can be represented to
                           
                              (7)
                              
                                 S
                                 (
                                 Y
                                 )
                                 =
                                 
                                 max
                                 
                                 
                                    
                                       t
                                       r
                                       (
                                       Y
                                       
                                          
                                             M
                                          
                                          
                                             inter
                                          
                                       
                                       
                                          
                                             Y
                                          
                                          T
                                       
                                       )
                                    
                                    
                                       t
                                       r
                                       (
                                       Y
                                       
                                          
                                             M
                                          
                                          
                                             intra
                                          
                                       
                                       
                                          
                                             Y
                                          
                                          T
                                       
                                       )
                                    
                                 
                              
                           
                        where
                           
                              (8)
                              
                                 
                                    
                                       M
                                    
                                    
                                       intra
                                    
                                 
                                 =
                                 
                                    
                                       (
                                       I
                                       −
                                       
                                          
                                             W
                                          
                                          
                                             intra
                                          
                                       
                                       )
                                    
                                    T
                                 
                                 (
                                 I
                                 −
                                 
                                    
                                       W
                                    
                                    
                                       intra
                                    
                                 
                                 )
                              
                           
                        
                        
                           
                              (9)
                              
                                 
                                    
                                       M
                                    
                                    
                                       inter
                                    
                                 
                                 =
                                 
                                    
                                       (
                                       I
                                       −
                                       
                                          
                                             W
                                          
                                          
                                             inter
                                          
                                       
                                       )
                                    
                                    T
                                 
                                 (
                                 I
                                 −
                                 
                                    
                                       W
                                    
                                    
                                       inter
                                    
                                 
                                 )
                              
                           
                        
                     

For Eq. (7), a linear transformation 
                           Y
                           =
                           
                              
                                 A
                              
                              T
                           
                           X
                         is introduced to overcome out-of-sample problem and Eq. (7) can be deduced to the following:
                           
                              (10)
                              
                                 S
                                 (
                                 Y
                                 )
                                 =
                                 
                                 max
                                 
                                 
                                    
                                       t
                                       r
                                       (
                                       
                                          
                                             A
                                          
                                          T
                                       
                                       X
                                       
                                          
                                             M
                                          
                                          
                                             inter
                                          
                                       
                                       
                                          
                                             X
                                          
                                          T
                                       
                                       A
                                       )
                                    
                                    
                                       t
                                       r
                                       (
                                       
                                          
                                             A
                                          
                                          T
                                       
                                       X
                                       
                                          
                                             M
                                          
                                          
                                             intra
                                          
                                       
                                       
                                          
                                             X
                                          
                                          T
                                       
                                       A
                                       )
                                    
                                 
                              
                           
                        So Lagrange multiplier method can also be adopted to find the linear transformation 
                           A
                         from the following eigendecomposition.
                           
                              (11)
                              
                                 X
                                 
                                    
                                       M
                                    
                                    
                                       inter
                                    
                                 
                                 
                                    
                                       X
                                    
                                    T
                                 
                                 A
                                 =
                                 λ
                                 X
                                 
                                    
                                       M
                                    
                                    
                                       intra
                                    
                                 
                                 
                                    
                                       X
                                    
                                    T
                                 
                                 A
                              
                           
                        
                     

It can be concluded that 
                           A
                         is spanned by the eigenvectors corresponding to top 
                           d
                         eigenvalues of the above generalized eigendecomposition.

LLRFC includes three processes, i.e. selection of neighborhood, calculation of the least reconstruction weights and exploring the linear transformation. When determining the neighborhood in both graphs, the cost is 
                           O
                           (
                           D
                           
                              
                                 N
                              
                              2
                           
                           )
                         for computing nearest neighbors. However, it will expense the computational scales as 
                           O
                           (
                           D
                           N
                           
                              
                                 k
                              
                              3
                           
                           )
                         to obtain the least reconstruction weights in both graphs. At last, the linear transformation 
                           A
                         can be achieved with scales as 
                           O
                           (
                           d
                           
                              
                                 N
                              
                              2
                           
                           )
                        .

Thus for any unknown label point 
                           
                              
                                 X
                              
                              
                                 t
                              
                           
                        , we use the transformation matrix 
                           A
                        , which can be obtained based on the training samples, to find its low dimensional embedding, i.e. 
                           
                              
                                 Y
                              
                              
                                 t
                              
                           
                           =
                           
                              
                                 A
                              
                              T
                           
                           
                              
                                 X
                              
                              
                                 t
                              
                           
                        . At last, a classifier can be adopted to predict its label.

The outline can be concluded as follows:
                           
                              Step 1: For each tumor gene expressive data point 
                                    
                                       
                                          X
                                       
                                       
                                          i
                                       
                                    
                                 , identify it 
                                    k
                                  nearest neighbors with the same class and with varied classes by KNN, respectively.

Step 2: Compute the reconstruction weights of each tumor gene expressive data point 
                                    
                                       
                                          X
                                       
                                       
                                          i
                                       
                                    
                                  to minimize the linear reconstruction error of 
                                    
                                       
                                          X
                                       
                                       
                                          i
                                       
                                    
                                  with its intra-class 
                                    k
                                  nearest neighbors based on 
                                    ε
                                    (
                                    
                                       
                                          W
                                       
                                       
                                          intra
                                       
                                    
                                    )
                                    =
                                    
                                       
                                          min
                                       
                                       
                                          
                                             
                                                X
                                             
                                             
                                                j
                                             
                                          
                                          ∈
                                          I
                                          n
                                          t
                                          r
                                          a
                                          N
                                          (
                                          
                                             
                                                X
                                             
                                             
                                                i
                                             
                                          
                                          )
                                       
                                    
                                    
                                       
                                          |
                                          |
                                          
                                             
                                                
                                                   X
                                                
                                                
                                                   i
                                                
                                             
                                             −
                                             
                                                ∑
                                                
                                                   j
                                                   =
                                                   1
                                                
                                                k
                                             
                                             
                                                
                                                   
                                                      (
                                                      
                                                         
                                                            W
                                                         
                                                         
                                                            intra
                                                         
                                                      
                                                      )
                                                   
                                                   
                                                      i
                                                      j
                                                   
                                                
                                                
                                                   
                                                      X
                                                   
                                                   
                                                      j
                                                   
                                                
                                             
                                          
                                          |
                                          |
                                       
                                       2
                                    
                                 .

Step 3: Compute the reconstruction weights of each tumor gene expressive data point 
                                    
                                       
                                          X
                                       
                                       
                                          i
                                       
                                    
                                  to minimize the linear reconstruction error of 
                                    
                                       
                                          X
                                       
                                       
                                          i
                                       
                                    
                                  with its inter-class 
                                    k
                                  nearest neighbors based on 
                                    ε
                                    (
                                    
                                       
                                          W
                                       
                                       
                                          inter
                                       
                                    
                                    )
                                    =
                                    
                                       
                                          min
                                       
                                       
                                          
                                             
                                                X
                                             
                                             
                                                j
                                             
                                          
                                          ∈
                                          I
                                          n
                                          t
                                          e
                                          r
                                          N
                                          (
                                          
                                             
                                                X
                                             
                                             
                                                i
                                             
                                          
                                          )
                                       
                                    
                                    
                                       
                                          |
                                          |
                                          
                                             
                                                
                                                   X
                                                
                                                
                                                   i
                                                
                                             
                                             −
                                             
                                                ∑
                                                
                                                   j
                                                   =
                                                   1
                                                
                                                k
                                             
                                             
                                                
                                                   
                                                      (
                                                      
                                                         
                                                            W
                                                         
                                                         
                                                            inter
                                                         
                                                      
                                                      )
                                                   
                                                   
                                                      i
                                                      j
                                                   
                                                
                                                
                                                   
                                                      X
                                                   
                                                   
                                                      j
                                                   
                                                
                                             
                                          
                                          |
                                          |
                                       
                                       2
                                    
                                 .

Step 4: Repeat steps 1, 2 and 3 to all the tumor gene expressive data points and obtain the weighted matrix 
                                    
                                       
                                          W
                                       
                                       
                                          inter
                                       
                                    
                                  and 
                                    
                                       
                                          W
                                       
                                       
                                          intra
                                       
                                    
                                 .

Step 5: Construct the matrix 
                                    
                                       
                                          M
                                       
                                       
                                          intra
                                       
                                    
                                    =
                                    
                                       
                                          (
                                          I
                                          −
                                          
                                             
                                                W
                                             
                                             
                                                intra
                                             
                                          
                                          )
                                       
                                       T
                                    
                                    (
                                    I
                                    −
                                    
                                       
                                          W
                                       
                                       
                                          intra
                                       
                                    
                                    )
                                 and 
                                    
                                       
                                          M
                                       
                                       
                                          inter
                                       
                                    
                                    =
                                    
                                       
                                          (
                                          I
                                          −
                                          
                                             
                                                W
                                             
                                             
                                                inter
                                             
                                          
                                          )
                                       
                                       T
                                    
                                    (
                                    I
                                    −
                                    
                                       
                                          W
                                       
                                       
                                          inter
                                       
                                    
                                    )
                                 , respectively;

Step 6: Construct the matrix 
                                    X
                                    
                                       
                                          M
                                       
                                       
                                          inter
                                       
                                    
                                    
                                       
                                          X
                                       
                                       T
                                    
                                  and 
                                    X
                                    
                                       
                                          M
                                       
                                       
                                          intra
                                       
                                    
                                    
                                       
                                          X
                                       
                                       T
                                    
                                 , respectively;

Step 7: Compute the 
                                    d
                                  largest generalized eigenvalues and the corresponding eigenvector matrix 
                                    A
                                  of 
                                    (
                                    X
                                    
                                       
                                          M
                                       
                                       
                                          inter
                                       
                                    
                                    
                                       
                                          X
                                       
                                       T
                                    
                                    ,
                                    
                                    X
                                    
                                       
                                          M
                                       
                                       
                                          intra
                                       
                                    
                                    
                                       
                                          X
                                       
                                       T
                                    
                                    )
                                  and obtain the d-dimensional embedding 
                                    Y
                                    =
                                    
                                       
                                          A
                                       
                                       T
                                    
                                    X
                                 .

Step 8: Adopt a classifier to predict the genre of unlabeled tumor gene expressive data.

@&#EXPERIMENTS@&#

In this section, LLRFC is compared with several related dimensionality reduction methods including PCA, ONPP, ONPP-R, DONPP and LLDE, which is also a supervised version of the original LLE. After employing PCA, ONPP, ONPP-R, DONPP, LLDE and LLRFC for feature extraction, the nearest neighbor (NN) classifier is adopted for label prediction, where for any unlabeled tumor gene expressive data, it will have the same label to that with the shortest Euclidean distance to it. Moreover, considering the characteristics of small sample size and high dimensionality for tumor gene expressive data, SVM and probabilistic linear discriminant analysis (PLDA) [48] are also employed to classify those tumor gene expressive data including DLBCL and FL data, high-grade glioma data, prostate data, acute lymphoblastic leukemia (ALL) data and central nervous system tumors data. When conducting experiments using SVM, Gaussian kernel is adopted, where the parameter 
                        σ
                      is tuned by leave one out cross validation (LOO-CV) on training data. The experiments are repeated 20 times by randomly selecting training samples each time.

In ONPP, ONPP-R, DONPP, LLDE and LLRFC, KNN is used to construct the corresponding local graphs, where parameter k is introduced. In the experiments, we train k using LOO-CV on training samples. When conducting experiment on DLBCL and FL data, high-grade glioma data, prostate data and acute lymphoblastic leukemia (ALL) data, k is 6, 5, 11 and 5, respectively. However, for central nervous system tumors data, the minimum training samples is 3, thus we set k to 2 directly.

Moreover, because the minimum training samples in high-grade glioma data and central nervous system tumors data are 7 and 3, which is too small to vary k greatly, we just test the impact of parameter k on the performance by conducting experiments 5 times on DLBCL and FL data, prostate data and ALL data, respectively.

Diffuse large B-cell lymphomas (DLBCL) and follicular lymphomas (FL) are two B-cell lineage malignancies that have very different clinical presentations, natural histories and response to therapy. DLBCL is a cancer of B-cell, a type of white blood cell responsible for producing antibodies, which is the most common type of non-Hodgkin lymphoma among adults. The second most common type of non-Hodgkin lymphoma among adults is FL. It is also a B-cell type of lymphoma which describes how the lymphoma cells group together in clusters in a lymph node or other tissues. However, FLs frequently evolve over time and acquire the morphologic and clinical features of DLBCLs and some subsets of DLBCLs have chromosomal translocations characteristic of FLs. In the experiment, DLBCL and FL contain 58 and 19 examples with 7129 genes, respectively. The gene-expression based classification model is built to distinguish DLBCL and FL data, where 20 DLBCL samples and 12 FL samples are randomly selected as trainings and the rest 38 DLBCL and 7 FL as test, respectively.

The mean accuracy and variance at the corresponding dimensions are shown in 
                        Table 1. It is found that the proposed method outperforms the other techniques such as PCA+NN, ONPP+NN, ONPP-R+NN, DONPP+NN, LLDE+NN, PLDA and SVM.

Based on the fact the 20 DLBCL samples and 12 FL samples are randomly selected as trainings, we set k to 4, 5, 6, 7, 8, 9, 10 and 11. The corresponding mean performances are plotted in 
                        Fig. 1, where the proposed LLRFC can achieve better results when k equals to 6 and 7.

A glioma is a tumor name of the glial cells, which can arise in any part of the brain and may also be found in the spinal cord. There are different types of gliomas, where high grade glioma means that the glioma is growing rapidly. High-grade glioma samples were carefully selected including 28 glioblastomas and 22 anaplastic oligodendrogliomas. In the experiments, a total of 21 classic tumors are selected as training including 14 glioblastomas and 7 anaplastic oligodendrogliomas, and the remaining 29 samples containing 14 glioblastomas and 15 anaplastic oligodendrogliomas are used for test. The goal here is to separate the glioblastomas from the anaplastic oligodendrogliomas, which allows appropriate therapeutic decisions and prognostic estimation.

The results by using LLRFC+NN, PCA+NN, ONPP+NN, ONPP-R+NN, DONPP+NN, LLDE+NN, PLDA and SVM are shown in 
                        Table 2. The statistics results validate that LLRFC+NN is of the better performance compared to PCA+NN, ONPP+NN, ONPP-R+NN, DONPP+NN, LLDE+NN, PLDA and SVM.

Prostate is another form of cancer which shows development in the prostate, a gland in the male reproductive system. Most prostate cancers are slow growing, however, some grow relatively fast. The cancer cells may spread from the prostate to other parts of the body, particularly the bones and lymph nodes. Prostate samples have two classes, where 77 samples and 59 samples are contained, respectively. In the experiments, 52 samples and 50 samples are randomly selected as trainings from the original Prostate and the remained 25 samples and 9 samples as test set.

The corresponding performances comparison using LLRFC+NN, PCA+NN, ONPP+NN, ONPP-R+NN, DONPP+NN, LLDE+NN, PLDA and SVM is shown in 
                        Table 3, where statistics data including mean accuracy and variance are listed with the corresponding dimensions. Moreover, Table 3 also reveals the proposed method׳s superiority.


                        
                        Fig. 2 exhibits the mean performance curve of LLRFC with varied k, which is changed from 5 to 37 at step 1. When k is 11, the performance is on the top. Later, the performance changes with few vibrations.

ALL is a fast-growing cancer of a type of white blood cells called lymphocytes that crowds out bone marrow and prevents it from making the normal red blood cells, white blood cells and platelets ALL dataset contains multi-class tumor gene expressive data with 12625 genes in 248 samples including BCR-ABL (15 cases), PBX1 (27 cases), hyperdiploid>50 (64 cases), MLL (20 cases), T-ALL (43 cases) and TEL-AML1 (79 cases). The summary of the training subset and test subset for each class are stated in 
                        Table 4.

The separation results by using LLRFC+NN, PCA+NN, ONPP+NN, ONPP-R+NN, DONPP+NN, LLDE+NN, PLDA and SVM are shown in 
                        Table 5. The superiority of LLRFC+NN can be found from Table 5.

In 
                        Fig. 3, the mean performance curve of LLRFC is stated with different k, where k is varied from 3 to 8. The performance decreases after k is 6.

Central nervous system tumors data set contains 34 samples representing four distinct morphologies, which is composed of four types of central nervous system embryonal tumors. It contains 5597 genes representing four distinct morphologies: 10 classic medulloblasomas, 10 malignant gliomas, 10 rhabdoids and 4 normals. In the experiment, we randomly select 5 medulloblasomas, 5 malignant gliomas, 5 rhabdoids and 3 normals as training set, and use the rest samples as test data, which are concluded in 
                        Table 6.

The mean and the corresponding variance of performances are listed in 
                        Table 7 by LLRFC+NN, PCA+NN, ONPP+NN, ONPP-R+NN, DONPP+NN, LLDE+NN, PLDA and SVM. Table 7 shows that the proposed method can gain the best recognition result on central nervous system tumors data.

@&#CONCLUSIONS@&#

In this paper, a discriminant manifold learning method, namely locally linear representation Fisher criterion (LLRFC), is proposed for tumor gene expressive data classification. The proposed algorithm uses the label information to construct the inter-class graph and the intra-class graph respectively and then local linear reconstruction is introduced in both graphs. At last a Fisher criterion is constructed to explore the discriminate subspace for classification based on the reconstruction errors in the inter-class graph and the reconstruction errors in the intra-class graph. So the proposed algorithm becomes more suitable for the tasks of classification. This result is validated by experiments on some benchmark tumor gene expressive data sets.

We declare that we have no financial and personal relationships with other people or organizations that can inappropriately influence our work, there is no professional or other personal interest of any nature or kind in any product, service and/or company that could be construed as influencing the position presented in, or the review of, the manuscript entitled, “Locally Linear Representation Fisher Criterion Based Tumor Gene Expressive Data Classification”.

@&#ACKNOWLEDGMENTS@&#

This work was partly supported by the Grants of the Natural Science Foundation of China (61100106, 61273303, 61273225 and 61373109), China Postdoctoral Science Foundation (20100470613 and 201104173), Natural Science Foundation of Hubei Province (2010CDB03302), the Research Foundation of Education Bureau of Hubei Province (Q20121115), the Program of Wuhan Subject Chief Scientist (201150530152), Hong Kong Scholars Program (XJ2012012) and the Open Project Program of the National Laboratory of Pattern Recognition (201104212).

@&#REFERENCES@&#

