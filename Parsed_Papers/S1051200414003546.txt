@&#MAIN-TITLE@&#On feature extraction for noninvasive kernel estimation of left ventricular chamber function indices from echocardiographic images

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Color-Doppler M-mode images are used to characterize left ventricular function.


                        
                        
                           
                           Estimation of end-systolic peak elastance and time-constant of relaxation rate.


                        
                        
                           
                           Comparison and interpretation of different linear estimators.


                        
                        
                           
                           Conditions where non-linear estimators outperform linear ones.


                        
                        
                           
                           High-fidelity study on mini-pigs with echocardiographic images and invasive measures.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Color Doppler M-mode imaging

Kernel methods

Left ventricular function

Elastance

LV relaxation

Cosine transform

@&#ABSTRACT@&#


               
               
                  Two reference indices used to characterize left ventricular (LV) global chamber function are end-systolic peak elastance (
                        
                           
                              E
                           
                           
                              max
                           
                        
                     ) and the time-constant of relaxation rate (τ). However, these two indices are very difficult to obtain in the clinical setting as they require invasive high-fidelity catheterization procedures. We have previously demonstrated that it is possible to approximate these indices noninvasively by digital processing color-Doppler M-mode (CDMM) images. The aim of the present study was twofold: (1) to study which feature extraction from linearly reduced input spaces yields the most useful information for the prediction of the haemodynamic variables from CDMM images; (2) to verify whether the use of nonlinear versions of those linear methods actually improves the estimation. We studied the performance and interpretation of different linearly transformed input spaces (raw image, discrete cosine transform (DCT) coefficients, partial least squares, and principal components regression), and we compared whether nonlinear versions of the above methods provided significant improvement in the estimation quality. Our results showed that very few input features suffice for providing a good (medium) quality estimator for 
                        
                           
                              E
                           
                           
                              max
                           
                        
                      (for τ), which can be readily interpreted in terms of the measured flows. Additional covariates should be included to improve the prediction accuracy of both reference indices, but especially in τ models. The use of efficient nonlinear kernel algorithms does improve the estimation quality of LV indices from CDMM images when using DCT input spaces that capture almost all energy.
               
            

@&#INTRODUCTION@&#

Characterization of left ventricular (LV) systolic and diastolic chamber function is still a pending issue in the clinical setting. In experimental physiology, peak end-systolic elastance (
                        
                           
                              E
                           
                           
                              max
                           
                        
                     ) is well established as the best available index to measure systolic performance of the LV chamber. In turn, the time-constant of LV relaxation (τ) is accepted as the gold standard method accounting for the rate of relaxation of the chamber, one of the main diastolic properties of LV function. Measuring 
                        
                           
                              E
                           
                           
                              max
                           
                        
                      requires complex measurements of instantaneous pressure and volume inside the LV chamber as well as preload intervention maneuvers. Measuring τ requires invasive catheterization of the LV using high-fidelity micromanometers. For these reasons, neither 
                        
                           
                              E
                           
                           
                              max
                           
                        
                      nor τ are only measured in patients for research purposes.

A number of noninvasive methods have been developed to obtain surrogate indices that correlate with 
                        
                           
                              E
                           
                           
                              max
                           
                        
                      and τ. Among them, most research has focused on Doppler-echocardiography, because it is a fully noninvasive, non ionizing, cheap and readily available at the patient's bedside. In a previous work we have shown that τ and 
                        
                           
                              E
                           
                           
                              max
                           
                        
                      can be reasonably approximated from CDMM images. Using a fluid-dynamic approach we have shown that 
                        
                           
                              E
                           
                           
                              max
                           
                        
                      correlates closely to the peak-ejection pressure difference developed inside the ventricle, which can be computed by solving Euler's equation from the CDMM velocity data [27]. Similarly, τ can be approximated by the peak reverse end-ejection pressure difference with reasonable accuracy [26]. Importantly, using a learning from samples approach we have obtained similar approximations without the need of complex fluid-dynamic modeling [13]. Hence, an experimental (animal) setup was used to simultaneously measure the catheter-based curves (pressure and flow) and acquire the CDMM images; and a machine-learning model was designed for straightforward estimation of τ and 
                        
                           
                              E
                           
                           
                              max
                           
                        
                      parameters from an input space given by the diastolic period of the digitized CDMM image.

In that precedent work, a linear estimator was used for the image input space, which raises several questions. On the one hand, nonlinear relations between CDMM images and indices can be expected, as the haemodynamic variables in the cardiac circulatory system are known to be mostly interrelated by nonlinear fluid dynamic equations. On the other hand, linear kernel estimators are often suggested in the machine learning literature as the most appropriate choice for high-dimensional input spaces, and they also provide with easier to interpret, black-box models than their nonlinear counterparts. Therefore, our aim was to test whether alternative algorithms on the machine learning specifications could improve the prediction of invasive indices 
                        
                           
                              E
                           
                           
                              max
                           
                        
                      and τ from CDMM images. First, we wanted to study which feature extraction from linearly reduced input spaces yields the most useful information for the prediction of the haemodynamic variables from CDMM images. Second, we wanted to verify whether the use of nonlinear algorithmic versions of those linear methods actually improves the estimation. Accordingly, we benchmarked the performance of several linear kernel estimators, in terms of linear feature extraction transformations, and in addition we analyzed the physical and clinical meaning of the relevant features in these transformed spaces, when possible. We also benchmarked the nonlinear kernel versions of the above analyzed estimators, hence determining the actual improvement obtained by the consideration of nonlinearity in the estimation kernel machine. For this purpose, we chose several kernel methods, namely, Support Vector Regression (SVR), Principal Component Regression (PCR), and Partial Least Squares (PLS), according to different levels of algorithm complexity in terms of the multidimensional output estimation from the multidimensional input. SVR performs one dimensional output robust estimation, PCR performs dimensionality reduction and multidimensional output estimation, and PLS performs a dimensionality reduction according input–output covariance.

The rationale for the chosen input features was as follows. First, the RAW input space conveys all the image information, hence it represents a necessary benchmarking. Also, a linear machine working on the input space will be easy to interpret, in terms of the relative temporal and spatial position of the linear weights. Second, DCT input space is a widespread used frequency transform in image problems, and given the smoothness and low-pass frequency content of CDMM images, it can be expected to work well from an image information compression point of view. Third, PCR provides us with features from an intrinsic image decomposition (different from frequency decompositions), with a decoupled regression stage. And finally, PLS provides us with features from an intrinsic image decomposition in which the regression output quality is an embedded optimization criterion.

The scheme of the paper is as follows. In the next section, the fundamental theory of the multidimensional kernel machines is summarized for the SVR, PCR, and PLS algorithms. Then, a detailed set of experiments is presented for benchmarking and interpretation of linear vs nonlinear kernel versions of the estimators. Finally, conclusions are drawn.

This section first describes the basic equations of SVR, PCR and PLS. These methods allow both linear and nonlinear estimation without explicitly extracting features from the images. Both PCR and PLS implicitly extract features (components or latent vectors) previous to the estimation problem. PCR performs a feature extraction such every new feature captures as much as possible of the remaining variance of the input data, where PLS extracts features that maximize the covariance of the input data and target variables.

SVR is a well-studied technique that allows nonlinear mappings of the input space and works well with high dimensional spaces like images [20]. Given a training set 
                           {
                           (
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                           ,
                           
                              
                                 y
                              
                              
                                 i
                              
                           
                           )
                           ,
                           
                           i
                           =
                           1
                           ,
                           …
                           ,
                           n
                           }
                         where 
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                           ∈
                           
                              
                                 R
                              
                              
                                 d
                              
                           
                         and 
                           
                              
                                 y
                              
                              
                                 i
                              
                           
                           ∈
                           R
                        , the SVR finds a function f that estimates 
                           
                              
                                 y
                              
                              
                                 i
                              
                           
                         as
                           
                              (1)
                              
                                 
                                    
                                       
                                          
                                             y
                                          
                                          
                                             ˆ
                                          
                                       
                                    
                                    
                                       i
                                    
                                 
                                 =
                                 f
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       ϕ
                                    
                                    
                                       T
                                    
                                 
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 w
                                 +
                                 b
                                 =
                                 
                                    
                                       y
                                    
                                    
                                       i
                                    
                                 
                                 +
                                 
                                    
                                       e
                                    
                                    
                                       i
                                    
                                 
                              
                           
                         where 
                           ϕ
                           :
                           
                              
                                 R
                              
                              
                                 d
                              
                           
                           →
                           H
                         is in general a nonlinear mapping to the feature space 
                           H
                        ; 
                           
                              
                                 (
                                 ⋅
                                 )
                              
                              
                                 T
                              
                           
                         is the matrix transpose operator; w is the weight column vector in this space; b is a bias term; and 
                           
                              
                                 e
                              
                              
                                 i
                              
                           
                         is the residual error. Function f is found by minimizing a functional with a regularization term and a loss term, as follows:
                           
                              (2)
                              
                                 L
                                 =
                                 
                                    1
                                    2
                                 
                                 
                                    
                                       ‖
                                       w
                                       ‖
                                    
                                    
                                       2
                                    
                                 
                                 +
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    n
                                 
                                 L
                                 (
                                 
                                    
                                       e
                                    
                                    
                                       i
                                    
                                 
                                 )
                              
                           
                         where 
                           L
                         is in our case the robust ϵ-Huber loss function [13], which increases the flexibility modeling outliers, given by
                           
                              (3)
                              
                                 L
                                 (
                                 
                                    
                                       e
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 =
                                 
                                    {
                                    
                                       
                                          
                                             0
                                          
                                          
                                             |
                                             
                                                
                                                   e
                                                
                                                
                                                   i
                                                
                                             
                                             |
                                             ≤
                                             ϵ
                                          
                                       
                                       
                                          
                                             
                                                1
                                                
                                                   2
                                                   δ
                                                
                                             
                                             (
                                             |
                                             
                                                
                                                   e
                                                
                                                
                                                   i
                                                
                                             
                                             
                                                
                                                   |
                                                   −
                                                   ϵ
                                                   )
                                                
                                                
                                                   2
                                                
                                             
                                          
                                          
                                             ϵ
                                             ≤
                                             |
                                             
                                                
                                                   e
                                                
                                                
                                                   i
                                                
                                             
                                             |
                                             ≤
                                             ϵ
                                             +
                                             δ
                                             C
                                          
                                       
                                       
                                          
                                             C
                                             (
                                             |
                                             
                                                
                                                   e
                                                
                                                
                                                   i
                                                
                                             
                                             |
                                             −
                                             ϵ
                                             )
                                             −
                                             
                                                1
                                                2
                                             
                                             δ
                                             
                                                
                                                   C
                                                
                                                
                                                   2
                                                
                                             
                                          
                                          
                                             ϵ
                                             +
                                             δ
                                             C
                                             ≤
                                             |
                                             
                                                
                                                   e
                                                
                                                
                                                   i
                                                
                                             
                                             |
                                          
                                       
                                    
                                 
                              
                           
                         where ϵ is the insensitive-zone parameter (no loss for errors lower than ϵ), and δC controls the size of the quadratic zone of the loss function. Finally, we minimize the following convex problem:
                           
                              (4)
                              
                                 L
                                 =
                                 
                                    1
                                    2
                                 
                                 
                                    
                                       ‖
                                       w
                                       ‖
                                    
                                    
                                       2
                                    
                                 
                                 +
                                 
                                    1
                                    
                                       2
                                       δ
                                    
                                 
                                 
                                    ∑
                                    
                                       i
                                       ∈
                                       
                                          
                                             I
                                          
                                          
                                             1
                                          
                                       
                                    
                                 
                                 
                                    (
                                    
                                       
                                          ξ
                                       
                                       
                                          i
                                       
                                       
                                          2
                                       
                                    
                                    +
                                    
                                       
                                          ξ
                                       
                                       
                                          i
                                       
                                       
                                          ⁎
                                          2
                                       
                                    
                                    )
                                 
                                 +
                                 C
                                 
                                    ∑
                                    
                                       i
                                       ∈
                                       
                                          
                                             I
                                          
                                          
                                             2
                                          
                                       
                                    
                                 
                                 
                                    (
                                    
                                       
                                          ξ
                                       
                                       
                                          i
                                       
                                    
                                    +
                                    
                                       
                                          ξ
                                       
                                       
                                          i
                                       
                                       
                                          ⁎
                                       
                                    
                                    )
                                 
                                 −
                                 
                                    ∑
                                    
                                       i
                                       ∈
                                       
                                          
                                             I
                                          
                                          
                                             2
                                          
                                       
                                    
                                 
                                 
                                    
                                       δ
                                       
                                          
                                             C
                                          
                                          
                                             2
                                          
                                       
                                    
                                    2
                                 
                              
                           
                         with respect to 
                           w
                           ,
                           b
                           ,
                           
                              
                                 ξ
                              
                              
                                 i
                              
                           
                           ,
                           
                              
                                 ξ
                              
                              
                                 i
                              
                              
                                 ⁎
                              
                           
                        , taking into account the following convex constraints:
                           
                              (5)
                              
                                 
                                    
                                       y
                                    
                                    
                                       i
                                    
                                 
                                 −
                                 
                                    
                                       ϕ
                                    
                                    
                                       T
                                    
                                 
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 w
                                 −
                                 b
                                 ≤
                                 ϵ
                                 +
                                 
                                    
                                       ξ
                                    
                                    
                                       i
                                    
                                 
                              
                           
                        
                        
                           
                              (6)
                              
                                 
                                    
                                       ϕ
                                    
                                    
                                       T
                                    
                                 
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 w
                                 +
                                 b
                                 −
                                 
                                    
                                       y
                                    
                                    
                                       i
                                    
                                 
                                 ≤
                                 ϵ
                                 +
                                 
                                    
                                       ξ
                                    
                                    
                                       i
                                    
                                    
                                       ⁎
                                    
                                 
                              
                           
                        
                        
                           
                              (7)
                              
                                 
                                    
                                       ξ
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       ξ
                                    
                                    
                                       i
                                    
                                    
                                       ⁎
                                    
                                 
                                 ≥
                                 0
                              
                           
                         for 
                           i
                           =
                           1
                           ,
                           …
                           ,
                           n
                        , and where 
                           
                              
                                 ξ
                              
                              
                                 i
                              
                           
                           ,
                           
                              
                                 ξ
                              
                              
                                 i
                              
                              
                                 ⁎
                              
                           
                         are positive slack variables to penalize the positive and negative errors, and 
                           
                              
                                 I
                              
                              
                                 1
                              
                           
                         and 
                           
                              
                                 I
                              
                              
                                 2
                              
                           
                         are respectively the sets of samples that are in the quadratic and linear loss zone.

Following the same procedure that solves standard SVR [20], we obtain the following solution:
                           
                              (8)
                              
                                 
                                    
                                       
                                          
                                             y
                                          
                                          
                                             ˆ
                                          
                                       
                                    
                                    
                                       t
                                    
                                 
                                 =
                                 f
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       t
                                    
                                 
                                 )
                                 =
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    n
                                 
                                 
                                    (
                                    
                                       
                                          α
                                       
                                       
                                          i
                                       
                                    
                                    −
                                    
                                       
                                          α
                                       
                                       
                                          i
                                       
                                       
                                          ⁎
                                       
                                    
                                    )
                                 
                                 K
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       x
                                    
                                    
                                       t
                                    
                                 
                                 )
                                 +
                                 b
                              
                           
                         where 
                           K
                           (
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                           ,
                           
                              
                                 x
                              
                              
                                 t
                              
                           
                           )
                           =
                           ϕ
                           
                              
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 )
                              
                              
                                 T
                              
                           
                           ϕ
                           (
                           
                              
                                 x
                              
                              
                                 t
                              
                           
                           )
                         is a Mercer kernel function, which is usually constructed without explicitly projecting in 
                           H
                         (i.e., without explicit knowledge of 
                           ϕ
                        ) and 
                           C
                           ≥
                           
                              
                                 α
                              
                              
                                 i
                              
                           
                           ,
                           
                              
                                 α
                              
                              
                                 i
                              
                              
                                 ⁎
                              
                           
                           ≥
                           0
                         are the Lagrange multipliers for the restrictions (5), (6). After optimization, some 
                           
                              
                                 α
                              
                              
                                 i
                              
                           
                           ,
                           
                              
                                 α
                              
                              
                                 i
                              
                              
                                 ⁎
                              
                           
                         have a non-zero value and their associated sample is named support vector (SV), because it influences function f. Three parameters 
                           (
                           C
                           ,
                           ϵ
                           ,
                           δ
                           )
                         need to be tuned for linear SVR (as well as the kernel width σ for the Gaussian case). Linear kernel is defined as 
                           
                              
                                 k
                              
                              
                                 L
                              
                           
                           (
                           
                              
                                 x
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 x
                              
                              
                                 2
                              
                           
                           )
                           =
                           
                              
                                 x
                              
                              
                                 1
                              
                              
                                 T
                              
                           
                           
                              
                                 x
                              
                              
                                 2
                              
                           
                        , and Gaussian kernel is defined as 
                           
                              
                                 k
                              
                              
                                 G
                              
                           
                           (
                           
                              
                                 x
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 x
                              
                              
                                 2
                              
                           
                           )
                           =
                           
                              
                                 e
                              
                              
                                 −
                                 
                                    
                                       
                                          ‖
                                          
                                             
                                                x
                                             
                                             
                                                1
                                             
                                          
                                          −
                                          
                                             
                                                x
                                             
                                             
                                                2
                                             
                                          
                                          ‖
                                       
                                       
                                          2
                                       
                                    
                                    
                                       2
                                       
                                          
                                             σ
                                          
                                          
                                             2
                                          
                                       
                                    
                                 
                              
                           
                        .

PCR [10] performs a Principal Component Analysis (PCA) on the zero-mean 
                           n
                           ×
                           d
                         predictor matrix X, and then applies Ordinary Least Squares (OLS) to the resulting g principal components (scores) 
                           T
                           =
                           [
                           
                              
                                 t
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 t
                              
                              
                                 2
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 t
                              
                              
                                 g
                              
                           
                           ]
                         and the zero-mean 
                           n
                           ×
                           k
                         dependent variables matrix Y, where k is the number of dependent variables. X can be factorized as:
                           
                              (9)
                              
                                 X
                                 =
                                 T
                                 
                                    
                                       P
                                    
                                    
                                       T
                                    
                                 
                              
                           
                         where 
                           P
                           =
                           [
                           
                              
                                 p
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 p
                              
                              
                                 2
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 p
                              
                              
                                 r
                              
                           
                           ]
                         is the loadings matrix (
                           P
                           
                              
                                 P
                              
                              
                                 T
                              
                           
                           =
                           I
                        ) and r, is the rank of X. Any number 
                           g
                           ≤
                           r
                         of components can be selected in order to estimate Y, however, these components should be selected carefully. Massy [10] proposes using the variance of the components or the dependence (correlation with Y) depending on the purpose of the analysis, but author warns also that low variance components can be useful for predicting Y. Jolliffe [7] and Hadi and Ling [5] discourage the direct application of the high-variance rule for selection components.

Predictions for X and 
                           
                              
                                 X
                              
                              
                                 t
                              
                           
                         are done by using the following matrix relations:
                           
                              (10)
                              
                                 Y
                                 =
                                 T
                                 B
                                 +
                                 F
                              
                           
                        
                        
                           
                              (11)
                              
                                 
                                    
                                       B
                                    
                                    
                                       ˆ
                                    
                                 
                                 =
                                 
                                    
                                       (
                                       
                                          
                                             T
                                          
                                          
                                             T
                                          
                                       
                                       T
                                       )
                                    
                                    
                                       −
                                       1
                                    
                                 
                                 
                                    
                                       T
                                    
                                    
                                       T
                                    
                                 
                                 Y
                              
                           
                        
                        
                           
                              (12)
                              
                                 
                                    
                                       Y
                                    
                                    
                                       ˆ
                                    
                                 
                                 =
                                 T
                                 
                                    
                                       B
                                    
                                    
                                       ˆ
                                    
                                 
                                 =
                                 X
                                 P
                                 
                                    
                                       B
                                    
                                    
                                       ˆ
                                    
                                 
                              
                           
                        
                        
                           
                              (13)
                              
                                 
                                    
                                       
                                          
                                             Y
                                          
                                          
                                             ˆ
                                          
                                       
                                    
                                    
                                       t
                                    
                                 
                                 =
                                 
                                    
                                       X
                                    
                                    
                                       t
                                    
                                 
                                 P
                                 
                                    
                                       B
                                    
                                    
                                       ˆ
                                    
                                 
                              
                           
                         where (10) is the regression model based on the principal components; (11) is the OLS estimator for 
                           
                              
                                 B
                              
                              
                                 ˆ
                              
                           
                        ; and (12) and (13) are the predictions.

As described in [15,17], the nonlinear kernel PCR (KPCR) performs kernel PCA [19] and then OLS using the principal components in the feature space. The same concerns appear in KPCR as in PCR, low variance components could be useful for prediction, as shown in [14]. One parameter (g) needs to be tuned for PCR (and the kernel width for Gaussian KPCR).

PLS for regression (see [16,24] for a description and an overview) are used for modeling relations between blocks of variables (e.g., a block of d explanatory variables and another block of k response variables), as well as for dimensionality reduction. PLS extracts orthogonal latent vectors (also called score vectors) by maximizing the covariance between blocks of variables; then PLS projects the observed data to its latent structure and use the latent vectors to perform regression of the response variables. PLS techniques assume that the observed data are generated by a process driven by a small number of latent (not directly observed) components.

PLS models the zero-mean 
                           n
                           ×
                           d
                         explanatory (predictor) matrix X and the zero-mean 
                           n
                           ×
                           k
                         dependent variables Y as:
                           
                              (14)
                              
                                 X
                                 =
                                 T
                                 
                                    
                                       P
                                    
                                    
                                       T
                                    
                                 
                                 +
                                 E
                              
                           
                        
                        
                           
                              (15)
                              
                                 Y
                                 =
                                 U
                                 
                                    
                                       Q
                                    
                                    
                                       T
                                    
                                 
                                 +
                                 F
                                 =
                                 T
                                 D
                                 
                                    
                                       Q
                                    
                                    
                                       T
                                    
                                 
                                 +
                                 
                                    (
                                    H
                                    
                                       
                                          Q
                                       
                                       
                                          T
                                       
                                    
                                    +
                                    F
                                    )
                                 
                                 =
                                 T
                                 
                                    
                                       C
                                    
                                    
                                       T
                                    
                                 
                                 +
                                 
                                    
                                       F
                                    
                                    
                                       ⁎
                                    
                                 
                              
                           
                         where 
                           T
                           =
                           [
                           
                              
                                 t
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 t
                              
                              
                                 2
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 t
                              
                              
                                 g
                              
                           
                           ]
                         and 
                           U
                           =
                           [
                           
                              
                                 u
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 u
                              
                              
                                 2
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 u
                              
                              
                                 g
                              
                           
                           ]
                         contain the extracted latent vectors as columns, and g is the number of latent components extracted; 
                           P
                           =
                           [
                           
                              
                                 p
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 p
                              
                              
                                 2
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 p
                              
                              
                                 g
                              
                           
                           ]
                         and 
                           Q
                           =
                           [
                           
                              
                                 q
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 q
                              
                              
                                 2
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 q
                              
                              
                                 g
                              
                           
                           ]
                         are the loading matrices; and E and F represent residual matrices. The second equality in (15) comes from the assumption of T columns being good predictors of Y and 
                           U
                           =
                           T
                           D
                           +
                           H
                        , where D is a diagonal matrix.

In order to find score vectors 
                           
                              
                                 t
                              
                              
                                 i
                              
                           
                         and 
                           
                              
                                 u
                              
                              
                                 i
                              
                           
                        , a procedure based on NIPALS algorithm [23] finds 
                           
                              
                                 w
                              
                              
                                 i
                              
                           
                           ,
                           
                              
                                 c
                              
                              
                                 i
                              
                           
                         vectors such that
                           
                              (16)
                              
                                 cov
                                 
                                    
                                       (
                                       
                                          
                                             t
                                          
                                          
                                             i
                                          
                                       
                                       ,
                                       
                                          
                                             u
                                          
                                          
                                             i
                                          
                                       
                                       )
                                    
                                    
                                       2
                                    
                                 
                                 =
                                 cov
                                 
                                    
                                       (
                                       X
                                       
                                          
                                             w
                                          
                                          
                                             i
                                          
                                       
                                       ,
                                       Y
                                       
                                          
                                             c
                                          
                                          
                                             i
                                          
                                       
                                       )
                                    
                                    
                                       2
                                    
                                 
                                 =
                                 
                                    max
                                    
                                       
                                          
                                             ‖
                                             r
                                             ‖
                                             =
                                             1
                                          
                                       
                                       
                                          
                                             ‖
                                             s
                                             ‖
                                             =
                                             1
                                          
                                       
                                    
                                 
                                 ⁡
                                 cov
                                 
                                    
                                       (
                                       X
                                       r
                                       ,
                                       Y
                                       s
                                       )
                                    
                                    
                                       2
                                    
                                 
                              
                           
                         where 
                           cov
                           (
                           
                              
                                 t
                              
                              
                                 i
                              
                           
                           ,
                           
                              
                                 u
                              
                              
                                 i
                              
                           
                           )
                         is the sample covariance between vector 
                           
                              
                                 t
                              
                              
                                 i
                              
                           
                         and vector 
                           
                              
                                 u
                              
                              
                                 i
                              
                           
                        . The procedure starts by a random selection of 
                           
                              
                                 u
                              
                              
                                 i
                              
                           
                         vector (it can be initialized as 
                           
                              
                                 u
                              
                              
                                 i
                              
                           
                           =
                           Y
                         if 
                           k
                           =
                           1
                        ). Then, the following steps are repeated until convergence:
                           
                              1.
                              
                                 
                                    
                                       
                                          w
                                       
                                       
                                          i
                                       
                                    
                                    =
                                    
                                       
                                          X
                                       
                                       
                                          T
                                       
                                    
                                    
                                       
                                          u
                                       
                                       
                                          i
                                       
                                    
                                    /
                                    ‖
                                    
                                       
                                          X
                                       
                                       
                                          T
                                       
                                    
                                    
                                       
                                          u
                                       
                                       
                                          i
                                       
                                    
                                    ‖
                                 
                              


                                 
                                    
                                       
                                          t
                                       
                                       
                                          i
                                       
                                    
                                    =
                                    X
                                    
                                       
                                          w
                                       
                                       
                                          i
                                       
                                    
                                 
                              


                                 
                                    
                                       
                                          c
                                       
                                       
                                          i
                                       
                                    
                                    =
                                    
                                       
                                          Y
                                       
                                       
                                          T
                                       
                                    
                                    
                                       
                                          t
                                       
                                       
                                          i
                                       
                                    
                                    /
                                    ‖
                                    
                                       
                                          Y
                                       
                                       
                                          T
                                       
                                    
                                    
                                       
                                          t
                                       
                                       
                                          i
                                       
                                    
                                    ‖
                                 
                              


                                 
                                    
                                       
                                          u
                                       
                                       
                                          i
                                       
                                    
                                    =
                                    Y
                                    
                                       
                                          c
                                       
                                       
                                          i
                                       
                                    
                                 
                              

The estimated 
                           
                              
                                 Y
                              
                              
                                 ˆ
                              
                           
                         and 
                           
                              
                                 
                                    
                                       Y
                                    
                                    
                                       ˆ
                                    
                                 
                              
                              
                                 t
                              
                           
                         for the training data X and test data 
                           
                              
                                 X
                              
                              
                                 t
                              
                           
                         are respectively given by:
                           
                              (19)
                              
                                 B
                                 =
                                 W
                                 
                                    
                                       (
                                       
                                          
                                             P
                                          
                                          
                                             T
                                          
                                       
                                       W
                                       )
                                    
                                    
                                       −
                                       1
                                    
                                 
                                 
                                    
                                       C
                                    
                                    
                                       T
                                    
                                 
                              
                           
                        
                        
                           
                              (20)
                              
                                 
                                    
                                       Y
                                    
                                    
                                       ˆ
                                    
                                 
                                 =
                                 X
                                 B
                                 =
                                 T
                                 
                                    
                                       C
                                    
                                    
                                       T
                                    
                                 
                              
                           
                        
                        
                           
                              (21)
                              
                                 
                                    
                                       
                                          
                                             Y
                                          
                                          
                                             ˆ
                                          
                                       
                                    
                                    
                                       t
                                    
                                 
                                 =
                                 
                                    
                                       X
                                    
                                    
                                       t
                                    
                                 
                                 B
                              
                           
                         where W, P and C respectively are the matrices with 
                           
                              
                                 w
                              
                              
                                 i
                              
                           
                        , 
                           
                              
                                 p
                              
                              
                                 i
                              
                           
                         and 
                           
                              
                                 c
                              
                              
                                 i
                              
                           
                         as columns.

To extend PLS to nonlinear problems, it is common to apply the kernel-trick [18] to the predictor matrix X and then perform a linear PLS in the feature space [17]. The NIPALS-based kernel version results:
                           
                              1.
                              
                                 
                                    
                                       
                                          t
                                       
                                       
                                          i
                                       
                                    
                                    =
                                    K
                                    
                                       
                                          u
                                       
                                       
                                          i
                                       
                                    
                                    /
                                    ‖
                                    K
                                    u
                                    ‖
                                 
                              


                                 
                                    
                                       
                                          c
                                       
                                       
                                          i
                                       
                                    
                                    =
                                    
                                       
                                          Y
                                       
                                       
                                          T
                                       
                                    
                                    
                                       
                                          t
                                       
                                       
                                          i
                                       
                                    
                                 
                              


                                 
                                    
                                       
                                          u
                                       
                                       
                                          i
                                       
                                    
                                    =
                                    Y
                                    
                                       
                                          c
                                       
                                       
                                          i
                                       
                                    
                                    /
                                    ‖
                                    Y
                                    
                                       
                                          c
                                       
                                       
                                          i
                                       
                                    
                                    ‖
                                 
                              

@&#EXPERIMENTS@&#

The experimental design for data processing is as follows. In Experiment 1, linear SVR was built from raw input space (CDMM images without transformations), both for estimating 
                        
                           
                              E
                           
                           
                              max
                           
                        
                      and τ. Attention was paid to the free parameters search. Error analysis and model diagnostic is given. Weights are plotted and analyzed from a physiological perspective. In Experiment 2, we used DCT coefficients of raw images as input space. Linear SVR was used to analyze the resulting images and the dependence on the retained energy and two normalization schemes were examined. In Experiment 3, we analyzed raw images using PCR. This method was one of the simplest alternatives to SVR as it reduces the dimensionality with PCA and then estimates using OLS. We compared the supervised and unsupervised alternatives to select the principal components. In Experiment 4 raw images were analyzed using PLS. We can see PLS as a dimensionality reduction method guided by the variable to be estimated, followed by a prediction using OLS, and accordingly, we wanted to compare the gain of doing supervised dimensionality reduction as opposed to PCR. In Experiment 5 we tested whether PCR and PLS methods, which include an implicit dimensionality reduction, could be benefited by using, as input space, the DCT coefficients of the CDMM images. Finally, in Experiment 6 we compared linear methods with their nonlinear (Gaussian) counterparts.

To compare the performance of different algorithms, we follow a leave-one-individual-out procedure, which has as many iterations as individuals and is explained as follows: 1) each individual is selected as test in one iteration and is only used to evaluate the estimator capabilities. 2) The test individual is used neither to train the estimator nor to search for its optimal parameters. 3) The rest of individuals (training individuals) are used to search for the optimal parameters of the model by using cross-validation, where each individual is assigned only to a fold. 4) All the training individuals are used to create a model with the best parameters of the previous step. 5) The test individual is used to evaluate that model.

This evaluation procedure, which follows best machine learning practices, provides appropriate error estimates as test individuals are not known for the training procedure and controls overfitting by not mixing individuals in the cross-validation folds, which induces conservative selection of parameters.

We denote the mean of the absolute error (MAE) of the estimation of index Y for individual m as:
                        
                           (22)
                           
                              
                                 
                                    
                                       
                                          E
                                       
                                       
                                          ¯
                                       
                                    
                                 
                                 
                                    m
                                 
                              
                              =
                              1
                              /
                              
                                 
                                    N
                                 
                                 
                                    m
                                 
                              
                              
                                 ∑
                                 
                                    i
                                    =
                                    1
                                 
                                 
                                    
                                       N
                                    
                                    
                                       m
                                    
                                 
                              
                              
                                 |
                                 
                                    
                                       
                                          
                                             Y
                                          
                                          
                                             ˆ
                                          
                                       
                                    
                                    
                                       m
                                    
                                 
                                 [
                                 i
                                 ]
                                 −
                                 
                                    
                                       Y
                                    
                                    
                                       m
                                    
                                 
                                 [
                                 i
                                 ]
                                 |
                              
                              ,
                           
                        
                      where 
                        
                           
                              N
                           
                           
                              m
                           
                        
                      is the number of cases recorded for individual m, 
                        
                           
                              Y
                           
                           
                              m
                           
                        
                        [
                        i
                        ]
                      and 
                        
                           
                              
                                 
                                    Y
                                 
                                 
                                    ˆ
                                 
                              
                           
                           
                              m
                           
                        
                        [
                        i
                        ]
                      are respectively the measured and estimated indices. We use as error figure 
                        
                           
                              E
                           
                           
                              ¯
                           
                        
                     , defined as the mean of the error defined by (22) for all individuals.

This study used data from an experimental protocol with 20 minipigs, whose left ventricular outflow tract (LVOT) flow velocity profile is similar to normal adult humans. Both CDMM images and invasive tracings (using catheters) were simultaneously acquired for each animal in a high fidelity setup. Animals were anesthetized and their heart conditions were manipulated (drugs and vena cava occlusion) to get different values for 
                           
                              
                                 E
                              
                              
                                 max
                              
                           
                         and τ. All measures where averaged by performing the experiment three times on each pig. For 
                           
                              
                                 E
                              
                              
                                 max
                              
                           
                         estimation, 9 pigs where considered that jointly sum up to 274 beats. For τ estimation, 20 pigs were considered that jointly sum up to a total of 1362 beats.

We detail further the images. CDMM images (DICOM format) were obtained from an epicardial approach using a phase-array broadband 2.0–4.0 MHz transducer on a Sequoia C-256 system (Siemens AG) [27]. These CDMM tracings display the 1-dimensional spatiotemporal map of ejection flow velocity along the long axis of the LV. Velocity values were obtained from the color values of each pixel using a previously validated decoding and de-aliasing algorithm [1]. Spatial and temporal calibration was obtained from the DICOM metadata, while velocity calibration was obtained by reading the images scale limits. The beginning and end of ejection, as well as the positions of the LV apex and outflow tract, were visually identified in each image. CDMM recordings were then aligned and cropped slightly beyond these limits, interpolated using a bivariate tensor product spline, and then downsampled to a grid of 
                           32
                           ×
                           32
                         pixels. The details of the experimental setup have been reported in [27].

We considered this data in an animal per animal basis, i.e., we always considered together the beats of an animal whether it is used for training or for testing purposes. Our data for each animal consist in the set of its measurements for each beat, namely, CDMM image and invasively measured values (
                           
                              
                                 E
                              
                              
                                 max
                              
                           
                         and τ).

In this experiment, we analyzed the raw 
                           32
                           ×
                           32
                         CDMM images, in a leave-one-individual-out scheme, with linear SVR. All the images were standardized by dividing them by the constant that made its maximum absolute value one.

A relevant matter is parameter search. We followed a sequential grid search approach where: (1) a parameter was optimized in a grid while the remaining parameters were kept static; (2) the parameter was set to the value providing minimum error; (3) the next parameter was considered. The search finished when no parameter changed after a round. The SVR parameters were sampled in the following ranges: 
                           ν
                           ∈
                           [
                           0.05
                           ,
                           1
                           ]
                        , 
                           C
                           ∈
                           [
                           
                              
                                 e
                              
                              
                                 −
                                 4
                              
                           
                           ,
                           
                              
                                 e
                              
                              
                                 4
                              
                           
                           ]
                         and 
                           δ
                           ∈
                           [
                           
                              
                                 10
                              
                              
                                 −
                                 2
                              
                           
                           ,
                           1
                           ]
                           /
                           C
                           ×
                           max
                           ⁡
                           (
                           |
                           
                              
                                 E
                              
                              
                                 max
                              
                           
                           |
                           )
                        . Given that it can be shown that δC is the length of the quadratic segment of the loss function (3), 
                           δ
                           C
                           ∈
                           [
                           
                              
                                 10
                              
                              
                                 −
                                 2
                              
                           
                           ,
                           1
                           ]
                           ×
                           max
                           ⁡
                           (
                           |
                           
                              
                                 E
                              
                              
                                 max
                              
                           
                           |
                           )
                        . We defined 
                           
                              
                                 δ
                              
                              
                                 ⁎
                              
                           
                           =
                           δ
                           C
                           /
                           max
                           ⁡
                           (
                           |
                           
                              
                                 E
                              
                              
                                 max
                              
                           
                           |
                           )
                        , which varies between 0.01 and 1, and relates to the length of the quadratic zone of (3) with the maximum of the target variable. As 
                           
                              
                                 E
                              
                              
                                 max
                              
                           
                         is standardized to be zero-mean and unit-variance previously to create the model, this selection of δC provides a quadratic penalty in the loss function that can be selected to be as small 
                           0.01
                           max
                           ⁡
                           (
                           |
                           
                              
                                 E
                              
                              
                                 max
                              
                           
                           |
                           )
                         or as high as 
                           max
                           ⁡
                           (
                           |
                           
                              
                                 E
                              
                              
                                 max
                              
                           
                           |
                           )
                        . Larger lengths for the quadratic zone are not considered as we do not want quadratic loss for large residuals that are even clearly outliers or samples that cannot be modeled.

We started with the prediction of 
                           
                              
                                 E
                              
                              
                                 max
                              
                           
                         by using the raw CDMM images. The average result (for all individuals) of the linear search of SVR parameters is shown in Fig. 1
                        , which also shows the boxplot of the best values for each individual. Parameter ν seemed to have little impact on the accuracy, and only C and 
                           
                              
                                 δ
                              
                              
                                 ⁎
                              
                           
                         seemed to have some effect. However, when looking at the optimal parameters, ν was close to zero for the prediction of 7 out of nine animals, C had a little variation, and 
                           
                              
                                 δ
                              
                              
                                 ⁎
                              
                           
                         was close to 1 for almost all cases, which informs that almost all residuals were penalized in the quadratic zone of the loss function.

The prediction and the residuals are shown in Fig. 2
                        , where we observe that the SVR was accurately following 
                           
                              
                                 E
                              
                              
                                 max
                              
                           
                        . The residuals were not Gaussian, as we see heavy tails in the histogram. The Bland–Altman (BA) plot compares the true value, i.e. the gold standard, to the difference between the true and estimated values, with the aim of detecting systematic errors, biases, heteroscedasticity or outliers. In the BA plot for 
                           
                              
                                 E
                              
                              
                                 max
                              
                           
                         we observe that SVR trended to underestimate values above 9 mmHg/ml, which shows a mixture of heteroscedasticity and bias. The averaged absolute error and its standard deviation is shown in Table 1
                        . SVR-LIN was the best linear predictor for raw images. 
                           
                              
                                 E
                              
                              
                                 max
                              
                           
                         had a mean of 4.2 mmHg/ml and a standard deviation of 3.1 mmHg/ml. The average estimation error was about a 20% (30%) of the mean (standard deviation) of 
                           
                              
                                 E
                              
                              
                                 max
                              
                           
                        , and the maximum absolute error was 5.4 mmHg/ml.

With linear kernel, predictor f 
                        (1) becomes a linear combination of the training images, and its absolute value for each image pixel represents the importance of that pixel for the prediction of the haemodynamical variable. The regression weights, represented as images, for each test case are shown in Fig. 3
                        (a), and their mean and standard deviation in Fig. 4
                        (a). A strong agreement on these weights is found in most machines, where we see that the center and lower center of the image is contrasted with its surroundings, hence, the predictor contrasted the region near the apex and near the LVOT at the beginning and near the end of the normalized ejection time with the region near the LVOT in the middle of the normalized ejection time.

For τ prediction, we used the same ranges for the parameters as for 
                           
                              
                                 E
                              
                              
                                 max
                              
                           
                         prediction. We show the average result of the linear search of SVR parameters and the boxplot of the best values in Fig. 1. In this case, the boxplot shows than both ν and 
                           
                              
                                 δ
                              
                              
                                 ⁎
                              
                           
                         varied dramatically from predicting each case, and on the other hand, C was almost the same for all cases. The three parameters seemed to have impact in the accuracy, but variations of ν from 0.2 to 1 had almost no effect.

The regression weights for τ, represented as images, are shown in Fig. 3(b) and their mean and standard deviation in Fig. 4(b). There were several patterns in the regression weights that depended at least on the selected training parameters: patterns like the ones for predicting animals 1, 2, 9, 12, 13, 14, 17, 19 resulted from training with 
                           ν
                           <
                           0.15
                         (most samples do not penalize) and 
                           
                              
                                 δ
                              
                              
                                 ⁎
                              
                           
                           >
                           0.5
                         values (the quadratic zone of the loss function is large); patterns like 3, 6, 7, 10, 11, 15, 18, 20 resulted for training with 
                           ν
                           >
                           0.5
                         (most samples penalize) and 
                           
                              
                                 δ
                              
                              
                                 ⁎
                              
                           
                           <
                           0.06
                         values (the quadratic zone of the loss function is very small so almost all the residuals were penalized with a linear cost). When looking at the coefficients themselves, we see a great agreement just above the lower-left corner where the maximum of the coefficients are for all images. There is also quite agreement on the zones with the larger negative coefficients. Both observations are observed in Fig. 4. The prediction and the residuals are shown in Fig. 2. We observe that the estimation provided by the SVR-LIN for τ was less accurate than for 
                           
                              
                                 E
                              
                              
                                 max
                              
                           
                        . In addition, the BA plot shows an almost-linear behavior in the difference of prediction and gold standard for high τ values. This systematic error dependent of the value of τ (that could be modeled by a line and partially removed) demonstrates the inability of the model to follow high τ values and the need to include other covariates. The average absolute error and its standard deviation is shown in Table 1. SVR-LIN was again the best linear predictor for raw images. Measured τ had a mean of 53.2 ms and a standard deviation of 16.6 ms. The average estimation error was about a 18% (59%) of the mean (standard deviation) of τ, and the maximum absolute error was 162.6 ms which was an outlier as we can see in Fig. 2, the next maximum error was just above 60 ms.

We also tested the estimation of 
                           
                              
                                 E
                              
                              
                                 max
                              
                           
                         and τ using the SVR with the Discrete Cosine Transform (DCT) coefficients instead of the raw images. DCT coefficients represent the frequency content of CDMM images, which resulted smooth and could be efficiently represented with few coefficients, e.g., 
                           {
                           70
                           ,
                           75
                           ,
                           80
                           ,
                           85
                           ,
                           90
                           ,
                           95
                           }
                           %
                         of the energy was respectively retained with only 
                           {
                           4
                           ,
                           6
                           ,
                           9
                           ,
                           15
                           ,
                           37
                           ,
                           176
                           }
                         out of 1024 coefficients in the case of 
                           
                              
                                 E
                              
                              
                                 max
                              
                           
                         (9 individuals), and with 
                           {
                           4
                           ,
                           5
                           ,
                           7
                           ,
                           11
                           ,
                           23
                           ,
                           110
                           }
                         out of 1024 coefficients in the case of τ (20 individuals). To obtain the coefficients needed to preserve an energy amount, we first calculated the DCT transform of each image. Then, we obtained the energy in the DCT domain and accumulated the energy content of all images in frequency domain. Finally, we sorted the frequencies in descending energy-content order and kept the minimum number of coefficients that provided the required amount of energy.


                        Fig. 5
                         shows a contour plot with the cumulative energy content of the DCT coefficients in decreasing order. We observe that energy-content was located in the low frequencies in most images. In addition, it shows the absolute value of DCT coefficients in log scale. We observe that there were 4 orders of magnitude (
                           
                              
                                 e
                              
                              
                                 −
                                 9
                              
                           
                        ) among maximum and minimum coefficient.

DCT coefficients were standardized (zero-mean, unit-variance) as a preprocessing step to the SVR. Alternatively, it was also considered to subtract the mean but not changing the variance. The first approach is inspired in typical preprocessing of independent variables, which is common practice in machine learning and statistics. The second approach implicitly makes the assumption that information is related to variance, thus avoiding emphasizing low variance components, which might be regarded as noise. Both approaches are compared below. A closer analysis of the DCT coefficients shown in Fig. 5 in the 
                           
                              
                                 E
                              
                              
                                 max
                              
                           
                         case reveals, on the one hand, that only 22 (152) DCT coefficients (frequencies) absolute means were above the maximum absolute mean divided by 100 (1000); on the other hand, only 40 (282) frequencies variance were above the maximum variance divided by 100 (1000). Considering both facts together, to use a zero-mean unit-variance standardization has the drawback of amplifying the importance of many frequencies with little contribution on the data, which can lead to poor modeling, especially in the case of small datasets. Table 2
                         shows this behavior with large errors when more than 176 components were considered. By subtracting only the mean, low energy components are not emphasized, and to model with all components does not penalize the results. The problem of the later approach is that higher-variance components have more influence in the model, but variance does not necessarily mean information for the estimation at hand.

For 
                           
                              
                                 E
                              
                              
                                 max
                              
                           
                         we focused on the DCT coefficients that capture 75% of the energy (this figure provided the best prediction for the zero-mean unit-variance standardization). The optimum parameters of the search of SVR parameters (see Fig. 6
                        ) were different than the ones obtained using the raw images, as the input space has now only 6 dimensions. We found, however, some similarities, described below. In the parameter search for 
                           
                              
                                 E
                              
                              
                                 max
                              
                           
                        , ν showed an approximately flat error from 0.2 to 1 (it was almost flat from 0 to 1 in Fig. 1) and the most chosen value was approximately 0.5 (it was close to 0 in Fig. 1); C showed a minimum around 
                           
                              
                                 10
                              
                              
                                 −
                                 1
                              
                           
                         (same shape that in Fig. 1) and its most chosen optimum value was close to 
                           
                              
                                 e
                              
                              
                                 −
                                 2.4
                              
                           
                           ≈
                           0.09
                         (lower than in Fig. 1); 
                           
                              
                                 δ
                              
                              
                                 ⁎
                              
                           
                         seemed to have more effect than in Fig. 1 and presented a minimum around 0.5, which was also the value most selected as optimum (compare with 1, which was most selected in Fig. 1). Therefore, the models for DCT with 75% of the energy had in general more support vectors, i.e., were more complex (higher ν), had a shorter quadratic zone in the loss function (lower 
                           
                              
                                 δ
                              
                              
                                 ⁎
                              
                           
                        ) and less penalty to errors (lower C) than the models used for raw images.

The mean for all individuals of the regression weights to be applied to the selected 6 DCT frequencies are shown as images on Fig. 7
                        . In addition, to have a better understanding of the predictor, we show the mean for all individuals of the regression coefficients to be applied to the original images. These coefficients were obtained from combining (with the SVR α weights) the images calculated as the IDCT (inverse DCT) of the standardized DCT coefficients of the samples that became support vectors. The prediction is (roughly) made by adding the beginning and the end of the ejection and subtracting mid-ejection. The prediction and the residuals (not shown) had a great agreement with the obtained from the raw images, yielding a small increase in the maximum error.

For τ we focused on the DCT coefficients that captured 85% of the energy (again, the one that provided the best prediction for the zero-mean unit-variance standardization). Parameters ν and 
                           
                              
                                 δ
                              
                              
                                 ⁎
                              
                           
                         (see Fig. 6) seemed to be the more influential ones in the search process, which explain the large variation of the optimal values selected for C. Models using DCT coefficients with 85% of energy had higher ν (more support vectors), higher C (more penalty to errors) and lower 
                           
                              
                                 δ
                              
                              
                                 ⁎
                              
                           
                         (smaller quadratic zone) than the models used for raw images.

The mean for all individuals of the regression weights to be applied to the selected 11 DCT coefficients are shown in Fig. 7, where we also show the regression coefficients to be applied to the original images. The pattern, a linear combination of low frequency DCT image basis components, is a low pass approximation to the one shown in Fig. 4. The prediction error of the DCT predictor is slightly higher than the one of the raw predictor, but DCT predictor is easier to interpret. We observe that the predictor for τ roughly contrasted the flow at the beginning and end of normalized ejection time near the apex with the flow at the beginning of ejection time near the LVOT, the mid-ejection time towards the LVOT, and the 1/4 and 3/4 of ejection time towards the apex. The prediction and the residuals were similar to the ones resulted when using linear SVR applied to raw images (see Fig. 2).


                        Table 2 compares SVR-LIN on raw images vs SVR-LIN on DCT coefficients (with both standardization schemes discussed above). SVR-LIN on the raw images performed the best both for predicting 
                           
                              
                                 E
                              
                              
                                 max
                              
                           
                         and τ. However, DCT1 (zero-mean, unit-variance normalization) seemed to be very efficient, as its error was close to the one attained using raw images for 75% (85% for τ) of the energy using only 6 (11 for τ) coefficients instead of 1024 used by raw images. On the other hand, by using DCT2 (zero-mean normalization), we also see that low energy components can increase the accuracy, as in the case of 
                           
                              
                                 E
                              
                              
                                 max
                              
                           
                        .

In this experiment, we estimated the reference indices from raw CDMM images with PCR and SPCR using the same methodology of previous sections. No preprocessing was applied to the raw CDMM image in order to make results comparable to the ones obtained by SVR (dividing the images by a constant makes no difference for PCR). We started by considering the prediction of 
                           
                              
                                 E
                              
                              
                                 max
                              
                           
                         with PCR. The search of the optimum number of components is shown in Fig. 8
                        . In all cases, the optimum number of components, from the minimum error criterion, was equal to or lower than 3, which was the mode. This was a considerable reduction of dimensionality, as we end adjusting a model with 3 variables (instead of 1024). The principal components are shown in Fig. 9
                        . The first one contrasts the normalized mid-ejection time with the beginning of the ejection time region from the middle towards the apex; the second one contrasts two instants around the mid-ejection time; and the third one contrasts the region that surrounds (left, down, right) the middle of the image with the rest of it, especially with the end of the ejection time. The mean of the regression coefficients of the predictors for each individual are shown in Fig. 9. We observe that the predictor was dominated by the first principal component. In addition, the predictor had some similarities with the Fig. 4 of SVR on raw predictions, both cases had a clear negative zone on the mid-ejection time, and a clear positive zone in the upper left of the figure. In addition, both predictors contrasted the middle of the figure with its surroundings. On the other hand, PCR predictor was smoother than the SVR predictor, which makes it easier to interpret. This predictor was also similar to the obtained from DCT shown in Fig. 7, and their performance was almost the same.

Finally the prediction, residuals and BA plot (not shown) were similar to the ones obtained with SVR (Fig. 2), but PCR exhibited a heavier right tail in the residuals histogram.

Another possibility to select the principal components uses the correlation with the variable to estimate (SPCR). Following this approach, we show the search for best number of components in Fig. 8(c). Here we observe that the selected number of components ranged from 2 to 24, being higher than 10 in 3 out of nine cases. Therefore, we found the optimal number of components highly dependent with the individual that was left-out, which shows the sensitivity of SPCR and its need for more individuals to become more stable. The mean of principal components extracted from the data to train each individual predictor are shown in Fig. 10
                        . We see that, in most cases, the first two principal components were similar to the ones obtained by PCR. The rest of components were highly variable from one predictor to another, except the third component, which was similar in most cases (note its variance increase with respect to the other two components). The mean of regression coefficients are shown Fig. 10, where we see that predictors changed significantly from one individual to another (note the high variance). The mean of the regression coefficients had some similarity to the mean of SVR-LIN predictors shown in Fig. 4. Finally, the prediction, residuals and BA plot (not shown) were similar to the ones given by PCR, the left tail of the SPCR residuals histogram was heavier and reached 
                           −
                           4
                            
                           mmHg
                           /
                           ml
                        ; the slight increase in error of SPCR when compared to PCR was due to over-estimations.

Later, we considered the prediction of τ with PCR. We show the search for the optimum number of components in Fig. 8; we found the optimum number of components equals to 2 for all but one of the individuals' predictors, which equals to one. The first 2 principal components are shown in Fig. 9. The first one was similar to the first component that resulted in the 
                           
                              
                                 E
                              
                              
                                 max
                              
                           
                         analysis; the second component was different and roughly contrasted the regions above and below the main diagonal, that is, the first part of the ejection time towards the LVOT is contrasted with the second part of the ejection time towards the apex. The mean of the regression components for all individuals are shown in Fig. 9; there was a strong agreement for all predictors and they were roughly a fatter version of the first component. This predictor was different to both the resulting from DCT+SVR-LIN and RAW+SVR-LIN. Finally, the prediction, residuals and BA plot (not shown) were similar to previous cases, the residuals had heavier tails than DCT+SVR-LIN and RAW+SVR-LIN.

We also considered the results of predicting τ with SPCR. The search for optimum coefficients is shown in Fig. 8. There was again a great variance in the optimum number of components that ranges from 1 to 17, being above 5 for 6 out 20 predictors and 1 the mode. The mean of the principal components extracted from the data to train each individual predictor are shown in Fig. 10, where we observe that only the first component was uniformly the common factor (note the high variance of the second component). The mean of these predictors is shown in Fig. 10, which was similar to the one provided by RAW+SVR-LIN (see Fig. 4). The prediction, residuals, and BA plot (not shown) were similar to the previous ones, but the prediction was worse than previous approaches, having heavier residual tails.

A summary table for error comparison among methods is shown in Table 1, where we observe that both PCR and SPCR accuracies are worse than SVR-LIN. On the other hand, PCR provides easier to interpret predictions. SPCR performs worse than PCR, which we explore below. Table 3
                         shows a comparison of the mean absolute error on each individual provided by PCR and SPCR for 
                           
                              
                                 E
                              
                              
                                 max
                              
                           
                        . SPCR gave better accuracy on 5 out of 9 individuals, in one of these cases SPCR selected a 2-components model and PCR a 3-components, which shows SPCR ability to choose better components for the estimation at hand. It is interesting to note that in the cases where SPCR performed noticeably worse (animals 6, 8 and 9), at least it doubled the number of components used by PCR. The biggest relative difference was in animal 6, 3 components in PCR and 16 in SPCR, which resulted in 19% error increase.


                        Table 4
                         shows the same comparison for τ. Overall SPCR only provided better estimations for 8 out of 20 animals. Taking into account that PCR chose 2 components for all animals but one, for which chose 1; one interesting aspect was that SPCR chose 1 component for 7 animals. We can compare PCR, and SPCR means of principal components in Figs. 9 and 10, where we see that first principal component for τ is almost the same for both methods and it was uniformly chosen by all predictors. Therefore, SPCR was not selecting more components because the second component with highest correlation with the output (that correspond to the component from 7-th to 18-th highest variance depending on the case) overfitted the output and was kept in the growing sets of components to evaluate. On the other hand, when PCR and SPCR agree on the number of components (animals 8 and 16), SPCR performs better. In the case of animal 8 both principal components are almost the same, the one in PCR has obtained with all data, as the component selection does not involve the variable to be estimated; but only the training data was used to obtain SPCR components. In the case of animal 16 the improvement could be ascribed to the same reason, or to the convenience in this case of the correlation with the target instead of the variance. When the number of components selected by SPCR was higher than the selected by PCR, the performance of SPCR was higher only in 3 out of 11 animals (5, 9 and 14). In these three cases the number of selected components (5, 3 and 3) was relatively low compared to the case of animals 3, 19 or 15 (49%, 40% and 26% increase of error, respectively).

In this experiment, we estimated the LV indices from raw CDMM images with PLS using the same methodology of previous sections. We first considered the prediction of 
                           
                              
                                 E
                              
                              
                                 max
                              
                           
                        . Fig. 11
                         shows the search for the optimal number of dimensions, which turned to be 2 for all cases being independent of the individual left out. This shows how efficiently PLS is able to find the subspace useful for prediction. The mean of the loadings obtained for each predictor is shown in Fig. 12
                        . The first loading was almost the same for all predictors, the second loading was similar for all predictors with little variations. The mean of the predictors for the 9 individuals is shown in Fig. 12, which shows a strong similarity among them and their main dependence on the first loading. The predictor was roughly contrasting the normalized mid-ejection time (emphasis near the LVOT) with its surroundings (emphasis at the beginning of the normalized ejection time near the apex and after the normalized mid-ejection time near the LVOT). The prediction, residuals and BA plot is shown in Fig. 13
                        . Prediction is similar to the one provided by SVR-LIN (see Fig. 2) but slightly higher error, as we see one sample above 6 mmHg/ml in the BA plot.

The τ case was similar, Fig. 11 shows the search for the optimal number of dimensions, which turned to be one for all cases. The mean of the loadings is shown in Fig. 12, where we can see a great agreement for all individuals. The same happened with the predictors. The mean of all predictors was similar to the one obtained by PCR (see Fig. 9). The prediction, residuals and BA (not shown) were similar to previous results. The error was compared with the other methods applied to raw CDMM images in Table 1, attaining the best accuracy after SVR-LIN.

In this experiment we estimated the reference indices from the DCT coefficients of the CDMM images using the same methodology as in previous sections. To decide which coefficients to preserve, we used the same energy targets that in Experiment 2. In addition, we also preserved all the coefficients. The coefficients variance was untouched to not distort the images. The results of the prediction of 
                           
                              
                                 E
                              
                              
                                 max
                              
                           
                         and τ are summarized in Table 5
                        . When all the energy is preserved, the prediction results of using the DCT coefficients were the same as using the raw images. This can be interpreted as a basis change that do not affect the results as both PCR and PLS construct a new basis from the input. However, if a fixed amount of energy is preserved, which could be interpreted as some type of filtering in frequency, PCR and PLS could be benefited. For instance, we observe in Table 5 for 
                           
                              
                                 E
                              
                              
                                 max
                              
                           
                         prediction that PCR did slightly better using 90% of the energy given by DCT than using all the energy, which shows that PCR could be benefited by filtering. On the other hand, PLS attained its best result when keeping all the energy. τ prediction, which is harder, showed that DCT pre-filtering improved the results both for PCR (75% of the energy) and PLS (85%) of the energy.

In this experiment, we tested the nonlinear counterparts of the algorithms of the previous experiments applying the same methodology. The results are summarized in Table 6
                        . If we consider the performance of 
                           
                              
                                 E
                              
                              
                                 max
                              
                           
                         estimators using raw CDMM images (Table 1 vs Table 6), we observe that linear methods outperformed their nonlinear counterparts when estimating 
                           
                              
                                 E
                              
                              
                                 max
                              
                           
                        , both method by method and globally. On the other hand, if we consider instead estimators that use DCT coefficients (Table 2 vs Table 6), we found that DCT+SVM-RBF keeping 90% of the energy outperformed all linear and nonlinear methods.

However, we observe that nonlinear methods outperformed their linear counterparts, both method by method and globally, in the estimation of τ using raw CDMM images (Table 1 vs Table 6). The same happened in the DCT space (Table 2 vs Table 6). Globally, the best result for τ was found by DCT+SVM-RBF keeping 85% of the energy, and very close to it by SVM-RBF on the raw CDMM images after zero-mean, unit-variance normalization.

Finally, we compared best linear and nonlinear methods performance for each individual. Fig. 14
                         compares RAW+SVM-LIN vs DCT2+SVM-RBF. We observe that nonlinear method did much better in the case of τ; 16 out of 20 individuals were better estimated by the nonlinear method. For 
                           
                              
                                 E
                              
                              
                                 max
                              
                           
                        , nonlinear method was also better (5 out of 9 cases), but the difference among both methods was only clear for 1 out of 9 individuals, where linear was noticeably worse than nonlinear kernel.

@&#DISCUSSION AND CONCLUSION@&#

Among the kernel methods community, it is generally accepted that linear Support Vector Machine (SVM) is better than nonlinear SVM for high dimensional input spaces like text categorization [25] or fMRI data [4,9,12]. This is supported by two theoretical facts: Cover's Theorem [3] and Vapnik–Chervonenkis VC-dimension [22]. Cover's Theorem discusses the probability of separating a set of n samples into two sets in a general setting. He showed that the natural capacity of a transformation having d degrees of freedom is 2d. In other words, the expected number of points that can be separated into two sets with high probability by a transformation of d degrees of freedom is 2d. The VC-dimension of a family of surfaces is the number of points that the family can classify into two classes no matter the points' labels. VC dimension of SVM with linear kernel (hyperplane) in d dimensions is 
                                 d
                                 +
                                 1
                               and VC-dimension of a SVM with Gaussian kernel is ∞. The capacity of SVM with Gaussian kernel is very high which might lead to overfitting.

In our case, we have CDMM images that were resampled to have 
                                 d
                                 =
                                 1024
                               pixels. So we can assume there is high probability that a hyperplane classifies up to 
                                 n
                                 =
                                 2048
                               points in this space. Actually, as VC dimension expresses, we can separate any configuration of 1025 points with hyperplanes. We have 274 and 1362 samples respectively for 
                                 
                                    
                                       E
                                    
                                    
                                       max
                                    
                                 
                               and τ. Therefore, this dimension is enough for a classification task with a linear kernel and the number of samples might be too small to project these samples in a higher-dimension space and then classify them using a hyperplane, which is what SVM with Gaussian kernel does. Therefore, it is easy to fall in overfitting when using nonlinear SVM and the input space is already very high, as the learner is too flexible.

We can qualitatively borrow these ideas and expect a similar behavior in our estimation (regression) problem. VC-dimension has also been proposed for regression linear methods [2]. The question is how to address an estimation problem where the input space is higher than or comparable to the number of samples. One possibility is to follow the ideas of Structural Risk Minimization [22], i.e., create nested subsets of increasing complexity where to find the regressor function. This approach is naturally followed by PCR and PLS regression methods, where there is a step to decide the number of latent components in the data, and then there is a step of search for the best regressor using this number of components. Instead of bounds we used cross-validation to select the adequate number of components. For SVR, we have to find the hyperparameters, but it is not direct to find the parameter sets that form the aforementioned nested subsets of increasing complexity. Therefore, we used the usual cross-validation to select SVR hyperparameters.

The estimation results, though good for 
                                 
                                    
                                       E
                                    
                                    
                                       max
                                    
                                 
                              , are still heteroscedastic (see BA plots for SVR, PCR and PLS), which shows that the model could be improved with extra variables. In the case of τ, the heteroscedasticity is considerable, rendering the method unable to accurately estimate high values of τ. We have to admit that the estimation quality needs to be improved, but still can be of utility in clinical practice. Other variables like heart rate, peak LV pressure, LV ejection fraction and their squares that could also be estimated noninvasively have shown to improve τ estimation [13].

One of the main strengths of linear methods, in addition to simplicity, is the possibility of interpret as images what the estimator is doing. These images can be analyzed by an expert to assess whether the method is doing something “reasonable” with the data. Also, these estimator images could give some insights of LV function suggesting new research hypotheses. For 
                                 
                                    
                                       E
                                    
                                    
                                       max
                                    
                                 
                               the estimators given by SVR-LIN and SPCR are quite rough in comparison with the ones given by PCR and PLS. Therefore as the accuracy is almost the same (less than 1% increased error), PLS estimator could be easier to interpret and preferable as it obtained with only two latent factors. For τ the estimator given by PCR and PLS are smoother than the one given by SVR-LIN, however this one estimates much better (error decrease of 9%). So, in this case it could be worthy to interpret SVR-LIN estimator instead. Anyway, as the estimation is not so good, the results should be interpreted with caution.

DCT representation seems a good choice for CDMM images, as it gives an insight on their frequency content and allows for effective compression. We represented the estimator as images including standardization. The estimator obtained using DCT+SVR-LIN is somewhat similar to a very smooth version of SVR-LIN estimator, both in case of 
                                 
                                    
                                       E
                                    
                                    
                                       max
                                    
                                 
                               or τ, where their estimation capabilities are really close to SVR-LIN (3% more error for 
                                 
                                    
                                       E
                                    
                                    
                                       max
                                    
                                 
                               and 1% for τ). DCT+SVR-LIN estimators use much less coefficients (6 for 
                                 
                                    
                                       E
                                    
                                    
                                       max
                                    
                                 
                              , 11 for τ), therefore, it could be worthy to interpret their estimators instead of SVR-LIN ones, especially for τ, where PCR and PLS are not so good. In addition, we showed that DCT pre-filtering can improve the results of PCR and PLS, especially in the case of hard prediction problems, such τ, and therefore it can be applied previously to other dimensionality reduction techniques as PCR and PLS.

DCT coefficients standardization should be taken with care. We remove the mean of all coefficients as we do not expect it would be useful for estimation. It could be useful, as an additional criterion to the variance, to select which coefficients to keep. If we decide to standardize the variance of the coefficients with the aim of giving the same importance to all the frequencies, we must be careful selecting the frequencies to include for not amplifying the noise.

SPCR could lead to improvements over PCR but we found this improvements more likely in well characterized problems (as 
                                 
                                    
                                       E
                                    
                                    
                                       max
                                    
                                 
                               estimation). Problems where the estimation is hard (as τ estimation) were more suitable for PCR. We conjecture that one of the problems of SPCR is that very low variance components can, by chance, have high correlations with the target variable but no real meaning in the data. A previous filter based on the variance on the number of components that removes components below a threshold could be useful for SPCR. Another important aspect to consider is the way we select the sets of components. We restricted ourselves to nested component-sets, where each set with g components is formed by the g components with highest variance (PCR) or correlation (SPCR). Wrapper feature selection methods could give better component sets at the price of higher search times. The overfitting example given by SPCR on raw data for τ estimation reminds us how careful we should be when the estimation problem at hand uses data in high dimension and we only have a number of points comparable to the dimension. Being conservative in this type of cases (use the variance to select the components as PCR) it is safer and provides better results because we cannot select noisy components.

We have also experimentally shown that nonlinear methods could outperform linear methods when a previous feature extraction step is considered to estimation. This behavior is also found in other high-dimensional datasets (e.g. [11,21]). We got better estimations by using DCT+SVM-RBF than using linear methods. We conjecture, taking into account our previous discussion on complexity, that convenient feature extraction in nonlinear estimation problems enables taking advantage of the flexibility of nonlinear estimation methods by limiting their overfitting capabilities and reducing the search space. Also, not mixing individuals in the cross-validation folds helps to prevent overfitting by inducing conservative parameter selections.

In addition, we observed that transformations of the variable to be estimated could lead to better estimations. The residuals of 
                                 
                                    
                                       E
                                    
                                    
                                       max
                                    
                                 
                               and τ estimation provided by RAW+SVR-LIN have heavy-tails (see Fig. 2). Therefore, we tried to estimate 
                                 log
                                 ⁡
                                 (
                                 
                                    
                                       E
                                    
                                    
                                       max
                                    
                                 
                                 )
                               and 
                                 log
                                 ⁡
                                 (
                                 τ
                                 )
                               instead and we found a 6% increase of accuracy in the case of 
                                 
                                    
                                       E
                                    
                                    
                                       max
                                    
                                 
                              , and a 4% decrease in accuracy for τ. This is a simple example of how a simple transformation could increase accuracy.

We can conclude that CDMM image-based noninvasive estimates of reference indices of LV function can be improved when using nonlinear machines. This increase in performance involves the selection of alternative lower dimension input spaces where the power of nonlinear machines can be exploited without falling in overfitting. Overfitting becomes an issue in estimation if flexible architectures are used in high dimensional input spaces. We have shown how DCT input space could be a good alternative to raw images in echocardiography that could benefit from the use of nonlinear machines.

On the other hand in exploratory analysis and when the understanding of the estimator is a must, we found PLS to have a good balance between accuracy and simplicity enabling knowledge gain and expert feedback about the estimator.

@&#ACKNOWLEDGEMENTS@&#

This study was partially supported by research projects TEC2010-19263 (EXCALIVUR) from Ministerio de Ciencia e Innovación, TEC2013-48439-C4-1-R (PRINCIPIAS) from Ministerio de Economía y Competitividad, PRIN13_IYA12 from Universidad Rey Juan Carlos and grants PIS09/02603, PS09/02602, and RD12/0042 (Red de Investigación Cardiovascular), from the Plan Nacional de Investigación Científica, Desarrollo e Innovación Tecnológica (I+D+I), Instituto de Salud Carlos III–Ministerio de Economía y Competitividad, Spain, and Prometeo Project of the Secretariat for Higher Education, Science, Technology and Innovation of Ecuador. Author R. Santiago-Mozos is supported by the Juan de la Cierva Program (Ref: JCI-2011-11150) of the Ministerio de Ciencia e Innovación, Spain.

Supplementary material related to this article can be found online at http://dx.doi.org/10.1016/j.dsp.2014.12.012.

The following is the Supplementary material related to this article.


                     
                        
                           MMC 1
                           
                              This is a MATLAB file of Fig. 1a.
                           
                           
                        
                     
                     
                        
                           MMC 2
                           
                              This is a MATLAB file of Fig. 1c.
                           
                           
                        
                     
                     
                        
                           MMC 3
                           
                              This is a MATLAB file of Fig. 2a.
                           
                           
                        
                     
                     
                        
                           MMC 4
                           
                              This is a MATLAB file of Fig. 2b.
                           
                           
                        
                     
                     
                        
                           MMC 5
                           
                              This is a MATLAB file of Fig. 2c.
                           
                           
                        
                     
                     
                        
                           MMC 6
                           
                              This is a MATLAB file of Fig. 2d.
                           
                           
                        
                     
                     
                        
                           MMC 7
                           
                              This is a MATLAB file of Fig. 2e.
                           
                           
                        
                     
                     
                        
                           MMC 8
                           
                              This is a MATLAB file of Fig. 2f.
                           
                           
                        
                     
                     
                        
                           MMC 9
                           
                              This is a MATLAB file of Fig. 3a.
                           
                           
                        
                     
                     
                        
                           MMC 10
                           
                              This is a MATLAB file of Fig. 3b.
                           
                           
                        
                     
                     
                        
                           MMC 11
                           
                              This is a MATLAB file of Fig. 4a (down).
                           
                           
                        
                     
                     
                        
                           MMC 12
                           
                              This is a MATLAB file of Fig. 4a (up).
                           
                           
                        
                     
                     
                        
                           MMC 13
                           
                              This is a MATLAB file of Fig. 4b (down).
                           
                           
                        
                     
                     
                        
                           MMC 14
                           
                              This is a MATLAB file of Fig. 4b (up).
                           
                           
                        
                     
                     
                        
                           MMC 15
                           
                              This is a MATLAB file of Fig. 5d (l).
                           
                           
                        
                     
                     
                        
                           MMC 16
                           
                              This is a MATLAB file of Fig. 5d (r).
                           
                           
                        
                     
                     
                        
                           MMC 17
                           
                              This is a MATLAB file of Fig. 5u (l).
                           
                           
                        
                     
                     
                        
                           MMC 18
                           
                              This is a MATLAB file of Fig. 5u (r).
                           
                           
                        
                     
                     
                        
                           MMC 19
                           
                              This is a MATLAB file of Fig. 6a.
                           
                           
                        
                     
                     
                        
                           MMC 20
                           
                              This is a MATLAB file of Fig. 6c.
                           
                           
                        
                     
                     
                        
                           MMC 21
                           
                              This is a MATLAB file of Fig. 7a.
                           
                           
                        
                     
                     
                        
                           MMC 22
                           
                              This is a MATLAB file of Fig. 7b.
                           
                           
                        
                     
                     
                        
                           MMC 23
                           
                              This is a MATLAB file of Fig. 7c.
                           
                           
                        
                     
                     
                        
                           MMC 24
                           
                              This is a MATLAB file of Fig. 7d.
                           
                           
                        
                     
                     
                        
                           MMC 25
                           
                              This is a MATLAB file of Fig. 8a.
                           
                           
                        
                     
                     
                        
                           MMC 26
                           
                              This is a MATLAB file of Fig. 8b.
                           
                           
                        
                     
                     
                        
                           MMC 27
                           
                              This is a MATLAB file of Fig. 8c.
                           
                           
                        
                     
                     
                        
                           MMC 28
                           
                              This is a MATLAB file of Fig. 8d.
                           
                           
                        
                     
                     
                        
                           MMC 29
                           
                              This is a MATLAB file of Fig. 9a.
                           
                           
                        
                     
                     
                        
                           MMC 30
                           
                              This is a MATLAB file of Fig. 9b.
                           
                           
                        
                     
                     
                        
                           MMC 31
                           
                              This is a MATLAB file of Fig. 9c.
                           
                           
                        
                     
                     
                        
                           MMC 32
                           
                              This is a MATLAB file of Fig. 9d.
                           
                           
                        
                     
                     
                        
                           MMC 33
                           
                              This is a MATLAB file of Fig. 10a.
                           
                           
                        
                     
                     
                        
                           MMC 34
                           
                              This is a MATLAB file of Fig. 10b.
                           
                           
                        
                     
                     
                        
                           MMC 35
                           
                              This is a MATLAB file of Fig. 10c.
                           
                           
                        
                     
                     
                        
                           MMC 36
                           
                              This is a MATLAB file of Fig. 10d.
                           
                           
                        
                     
                     
                        
                           MMC 37
                           
                              This is a MATLAB file of Fig. 11a.
                           
                           
                        
                     
                     
                        
                           MMC 38
                           
                              This is a MATLAB file of Fig. 11b.
                           
                           
                        
                     
                     
                        
                           MMC 39
                           
                              This is a MATLAB file of Fig. 12a.
                           
                           
                        
                     
                     
                        
                           MMC 40
                           
                              This is a MATLAB file of Fig. 12b.
                           
                           
                        
                     
                     
                        
                           MMC 41
                           
                              This is a MATLAB file of Fig. 12c.
                           
                           
                        
                     
                     
                        
                           MMC 42
                           
                              This is a MATLAB file of Fig. 12d.
                           
                           
                        
                     
                     
                        
                           MMC 43
                           
                              This is a MATLAB file of Fig. 13l.
                           
                           
                        
                     
                     
                        
                           MMC 44
                           
                              This is a MATLAB file of Fig. 13r.
                           
                           
                        
                     
                     
                        
                           MMC 45
                           
                              This is a MATLAB file of Fig. 14l.
                           
                           
                        
                     
                     
                        
                           MMC 46
                           
                              This is a MATLAB file of Fig. 14r.
                           
                           
                        
                     
                  

@&#REFERENCES@&#

