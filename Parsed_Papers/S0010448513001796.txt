@&#MAIN-TITLE@&#GaFinC: Gaze and Finger Control interface for 3D model manipulation in CAD application

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A multi-modal control method using finger and gaze for 3D manipulation is proposed.


                        
                        
                           
                           Independent gaze pointing interface increases the intuitiveness of the zooming task.


                        
                        
                           
                           The performance of GaFinC is applicable to actual CAD tools.


                        
                        
                           
                           Interviews of user experience report higher intuitiveness than a mouse.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

User interface

Multi-modal

Gaze tracking

Hands tracking

Gesture recognition

Manipulation

@&#ABSTRACT@&#


               Graphical abstract
               
                  
                     
                        
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

Recently much research into HCI: human–computer interaction has been conducted and its developments for better interfaces have been directed towards making HCI more natural and intuitive. Also in the CAD field, many HCI interfaces have been actively developed, of which the hand gesture control interface for CAD modeling tasks is one. There have been studies using wearable hardware, sensing hand coordinates and gestures  [1,2]. But as special wearable hardware was an obstacle to becoming popular, vision based gesture tracking control  [3–5] became one of the main topics. Recently, hand tracking and skeleton recognition studies using depth sensing cameras such as ‘Kinect’ have been introduced  [6–9]. These hand gesture control interfaces are generally designed based on the metaphor of the real world and its intuitiveness makes it user friendly. However, in spite of its intuitiveness and familiarity, its usability for actual 3D CAD modeling applications is not that comfortable compared with a conventional interface such as a mouse. Most importantly, previous hand gesture control interfaces were not applicable for long time operation in the aspect of increasing users’ physical fatigue. In this paper, we suggest an improved gesture control interface showing conventional interface level usability with low fatigue while maintaining a high level of intuitiveness. As an order of priority, we focused on 3D model manipulation tasks, which are among the most frequent in conducting 3D CAD modeling. Through analyzing problems of previous hand gesture control in manipulation tasks, we achieved our approaches as follows.
                        
                           (1)
                           Precise hand and finger tracking control.

Easy application control with finger gestures.

Gaze tracking as an independent pointing interface.

To achieve these, we developed the multi-modal hand gesture control interface ‘GaFinC’: Gaze and Finger Control interface. The problems in the analysis of previous works and our approaches will be mentioned in detail in the next section.

@&#BACKGROUND@&#

In most CAD applications, manipulation tasks can be divided into three tasks, translation, rotation and zooming. So we will extract the problems of previous research separately for each manipulation task. The definition of terms is shown in Fig. 1
                     .
                  

The translation task is a manipulation method of a 3D model by a parallel translation in the 
                        X
                        Y
                      plane to reveal hidden information outside the camera field of view. In general CAD applications, translation is done by dragging a mouse in the 
                        X
                        Y
                      plane and the position of the model view camera is adjusted. In previous hand gesture control interfaces, after selecting the translation mode, the user changes the position of the model view camera by moving one hand in the 
                        X
                        Y
                      plane  [5,10,11]. In those research efforts, the end point of the user’s hand played the role of pointing, like a mouse pointer, and the user moved his whole arm to adjust the position. In this process, the user has to keep his arm floating in the air against gravity, causing serious physical fatigue for long time CAD work. Furthermore, a human arm is not appropriate for controlling the precise position due to its fundamental biological structure. To make up for this inadequate accuracy and to increase hand pointing resolution, the hand recognition system requires a wider hand moving region for coordinate mapping between the hand and the display. It causes more frequent and larger arm movements than a conventional mouse, which needs only wrist and finger movements.

The rotation task is a manipulation method making the 3D model rotate on the origin of the world axis or the model axis to reveal hidden information on the back or side of the model. The rotation task is done by dragging the mouse in the 
                        X
                        Y
                      plane and the angle of the 3D model is adjusted similar to translation. Implementing as a one-hand rotation control  [12,11], the displacement of the hand position after triggering the command adjusts the rotation angle. As these methods are based on the metaphor of rotation manipulation with a mouse, it is not intuitive for the user from the aspect of reflecting the real world. Two-hand rotation interfaces  [5,10,9] were also introduced and can be evaluated as more intuitively reflecting the behavior of real-world rotation action. However, regardless of one-hand or two-hand rotation control, the same problems as in the translation task, namely physical fatigue on the floating arm against gravity and frequent and large arm movements, are still found.

The zooming task is a manipulation method to obtain detailed information of the 3D model by moving the camera closer to the point of interest. Therefore, pre-setting of point of interest, the target where the model view camera moves towards, is necessary. With a mouse, the user moves the mouse pointer to the point of interest and controls the mouse wheel to decide the zooming level. Park et al.  [11] developed a two-hand zooming control adjusting the zooming parameter by the distance between the two hands. However, they skipped setting a point of interest and used a fixed point of interest at the center of display. Fiorentino et al.  [7] used the same method of control, but they used the midpoint of the two hands as the point of interest. In the research of  [13], Stellmach et al. changed the zooming parameter by moving a hand in the 
                        Z
                     -axis (towards display direction) using a depth sensing camera while getting the position of interest from a hand position in the 
                        X
                        Y
                      plane. However, it has the problem of the position escaping from the target in the 
                        X
                        Y
                      plane as the user moves his hand in along the 
                        Z
                     -axis to adjust the zooming parameter. Although a conventional mouse has the same problem, the behavior for adjusting the zooming parameter is a wheel control causing a small movement and it hardly makes a target pointing error. Also, the pointing task is completed through continuous position error feedback between the target and the current position. However, when the user is doing the pointing task with a hand, the user must consider another position error between their hand and the current position as well as the current and target position. This problem can be solved by being accustomed with hand pointing, but it is hard for users because they also have to endure a floating arm and frequent and large arm movement problems.

Most of all, switching from one manipulation task to another must be easy. At least three another command expressions are needed for translation, rotation and zooming. However, in previous research, there were limited variations of hand expressions due to the low ability for hand gesture recognition, such as closed hands or opened hands  [7,11]. They were able to solve it by attaching additional buttons to the display, but it was not available in actual CAD manipulation for task changing frequently.

Through reviewing previous research, we summarize the problems of previous hand gesture control as in Fig. 2. To solve these problems, we set our approach as follows.
                        
                           (1)
                           Minimizing floating body and hand movement.

Changing the manipulation task only with hand gestures.

An additional independent pointing interface having simple position error feedback.

To satisfy this approach, we developed the multi-modal 3D model manipulation interface ‘GaFinC’: Gaze and Finger Control. The GaFinC interface is composed of another two interfaces: a finger gesture control interface and a gaze tracking pointing interface. First, to minimize floating body movements and recognize various finger gesture expressions for switching the manipulation task, we applied high-resolution hand tracking and a finger gesture recognition library, ‘3Gear system’  [14]. Also we added a gaze tracker as an additional independent pointing interface to avoid interference between the behaviors of pointing and parameter adjustment. There can be other types of pointing method for setting the point of interest. However, considering the reason for setting the point of interest, we try to achieve detailed information of a certain point on the 3D model ‘visually’. Therefore, applying a ‘gaze tracker’ as a pointing interface can be evaluated as the most natural and intuitive interface. Also, with respect to the position error feedback, both the current pointer and the position error data are accomplished by ‘eye’, thus decreasing the user effort in position error feedback. Finally, the user experiences much less fatigue moving their eyes than their hands or other parts of body. Many authors have studied multi-modal control interfaces combining gaze tracking pointing and hand gesture control before  [15,16,12,17]. Pouke et al.  [12] used an eye tracking interface for selecting a model and controlled it using wearable hardware on the hand. But this system provided limited control functions for movement and rotation that are insufficient to apply to CAD applications. Lee et al.  [17] developed a similar system using a depth sensing camera for tracking hands. Although this system gives various functions for control combining two hand gestures, it caused user fatigue by requiring large hand movements to track the hand position from the human skeleton.

In this section, gestures for three manipulation tasks are designed. The basic premise of gesture design is reflecting user behavior in the real-world while minimizing physical fatigue. Designed hand gestures and their description are shown in Figs. 3 and 4
                     
                     .

At first, the hand gestures for the neutral state, which means idle, is designed as an all hands opened status.

The translation task is designed to be controlled by one hand. When users try to move something in the real-world, first they grab it then move it. For a user-friendly design, the pinch gesture serves the function of grabbing the model view camera and also plays the role of a trigger signal for the translation task. Then the displacement from the pinch gesture triggered position is used as a parameter for the amount of translation. 
                        
                           
                              (1)
                              
                                 
                                    
                                       
                                          
                                             A
                                          
                                          
                                             
                                                cam
                                             
                                          
                                          
                                             ′
                                          
                                       
                                    
                                    
                                       ⃗
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             A
                                          
                                          
                                             
                                                cam
                                             
                                          
                                       
                                    
                                    
                                       ⃗
                                    
                                 
                                 +
                                 
                                    
                                       
                                          d
                                       
                                       
                                          
                                             P
                                          
                                          
                                             
                                                cam
                                             
                                          
                                       
                                    
                                    
                                       ⃗
                                    
                                 
                              
                           
                           
                              (2)
                              
                                 
                                    
                                       
                                          d
                                       
                                       
                                          
                                             P
                                          
                                          
                                             
                                                cam
                                             
                                          
                                       
                                    
                                    
                                       ⃗
                                    
                                 
                                 =
                                 
                                    
                                       k
                                    
                                    
                                       
                                          pan
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             P
                                          
                                          
                                             
                                                hand
                                             
                                          
                                       
                                    
                                    
                                       ⃗
                                    
                                 
                                 =
                                 
                                    
                                       k
                                    
                                    
                                       
                                          pan
                                       
                                    
                                 
                                 
                                    (
                                    
                                       
                                          
                                             
                                                P
                                             
                                             
                                                
                                                   disp
                                                
                                             
                                          
                                       
                                       
                                          ⃗
                                       
                                    
                                    −
                                    
                                       
                                          
                                             
                                                P
                                             
                                             
                                                
                                                   org
                                                
                                             
                                          
                                       
                                       
                                          ⃗
                                       
                                    
                                    )
                                 
                                 .
                              
                           
                        
                     
                  

In the case of the rotation task, we designed a two-hands interface. In the real world, when the user tries to get hidden information through rotation, first they grab the model with two hands then rotate it. A pinch gesture of both hands is the behavior of grabbing the 3D model as well as the trigger signal for rotation. The angular variation 
                        
                           
                              θ
                           
                           
                              
                                 rotate
                              
                           
                        
                      between the vector 
                        
                           
                              P
                           
                           
                              
                                 org
                              
                           
                        
                        
                           (
                           x
                           ,
                           y
                           ,
                           z
                           )
                        
                     , where rotating triggered positions, and the other vector 
                        
                           
                              P
                           
                           
                              
                                 disp
                              
                           
                        
                        
                           (
                           x
                           ,
                           y
                           ,
                           z
                           )
                        
                     , where the current hand position is used as a parameter for the rotation. The final angle of the 3D model 
                        
                           
                              θ
                           
                           
                              
                                 rotate
                              
                           
                        
                      is expressed as a combination of 
                        
                           
                              θ
                           
                           
                              x
                           
                        
                        ,
                        
                           
                              θ
                           
                           
                              y
                           
                        
                      and 
                        
                           
                              θ
                           
                           
                              z
                           
                        
                      using quaternion algebra. The equations for 
                        
                           
                              θ
                           
                           
                              x
                           
                        
                      are expressed in (4). In particular, the user can rotate the 3D model in any axis at a time with the GaFinC interface, which is difficult with a mouse interface adjusting 2D parameters in the 
                        X
                      and 
                        Y
                      directions. Also, most systems have axis-constrained rotation and free rotation is notoriously difficult, requiring additional commands for setting the center of rotation. But with GaFinC, the user can choose the center of the rotation just by gazing without troublesome effort. Therefore, we can expect improvement of usability as well as intuitiveness through GaFinC.


                     
                        
                           
                              (3)
                              
                                 
                                    
                                       θ
                                    
                                    
                                       
                                          rotate
                                       
                                    
                                 
                                 =
                                 
                                    
                                       θ
                                    
                                    
                                       x
                                    
                                 
                                 +
                                 
                                    
                                       θ
                                    
                                    
                                       y
                                    
                                 
                                 +
                                 
                                    
                                       θ
                                    
                                    
                                       z
                                    
                                 
                              
                           
                           
                              (4)
                              
                                 
                                    
                                       θ
                                    
                                    
                                       x
                                    
                                 
                                 =
                                 
                                    
                                       sin
                                    
                                    
                                       −
                                       1
                                    
                                 
                                 
                                    (
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         (
                                                         
                                                            
                                                               P
                                                            
                                                            
                                                               
                                                                  org
                                                               
                                                            
                                                         
                                                         )
                                                      
                                                   
                                                   
                                                      y
                                                      z
                                                   
                                                
                                             
                                             
                                                ⃗
                                             
                                          
                                          ⋅
                                          
                                             
                                                
                                                   
                                                      
                                                         (
                                                         
                                                            
                                                               P
                                                            
                                                            
                                                               
                                                                  disp
                                                               
                                                            
                                                         
                                                         )
                                                      
                                                   
                                                   
                                                      y
                                                      z
                                                   
                                                
                                             
                                             
                                                ⃗
                                             
                                          
                                       
                                       
                                          
                                             |
                                             
                                                
                                                   
                                                      
                                                         
                                                            (
                                                            
                                                               
                                                                  P
                                                               
                                                               
                                                                  
                                                                     org
                                                                  
                                                               
                                                            
                                                            )
                                                         
                                                      
                                                      
                                                         y
                                                         z
                                                      
                                                   
                                                
                                                
                                                   ⃗
                                                
                                             
                                             |
                                          
                                          
                                             |
                                             
                                                
                                                   
                                                      
                                                         
                                                            (
                                                            
                                                               
                                                                  P
                                                               
                                                               
                                                                  
                                                                     disp
                                                                  
                                                               
                                                            
                                                            )
                                                         
                                                      
                                                      
                                                         y
                                                         z
                                                      
                                                   
                                                
                                                
                                                   ⃗
                                                
                                             
                                             |
                                          
                                       
                                    
                                    )
                                 
                                 .
                              
                           
                        
                     
                  

Finally, for the zooming task, when the user wants to know detailed information by zooming, he approaches the point of interest in the real world. Avoiding interference in hand gesture controls and enhancing intuitiveness, the method of setting the point of interest is separated from the hand gesture control. What the hand gesture does is act as a trigger signal and adjust the zooming parameter, while the setting of the point of interest is done by the gaze tracker. Among many possible gestures, we used the pinch zoom gesture which many users are accustomed to through smart phone applications. So, the zooming task is performed with two hands control. If the user makes both hands pointing hands, the zooming task command is triggered and the setting of the point of interest is controlled by the user’s eyes. Then the user zooms in or out by widening or narrowing the gap between two pointing fingers.
                        
                           (5)
                           
                              
                                 
                                    l
                                 
                                 
                                    
                                       model
                                    
                                 
                                 
                                    ′
                                 
                              
                              =
                              
                                 
                                    k
                                 
                                 
                                    
                                       zoom
                                    
                                 
                              
                              
                                 (
                                 
                                    |
                                    
                                       
                                          
                                             
                                                P
                                             
                                             
                                                
                                                   disp
                                                
                                             
                                          
                                       
                                       
                                          ⃗
                                       
                                    
                                    |
                                 
                                 /
                                 
                                    |
                                    
                                       
                                          
                                             
                                                P
                                             
                                             
                                                
                                                   org
                                                
                                             
                                          
                                       
                                       
                                          ⃗
                                       
                                    
                                    |
                                 
                                 )
                              
                              
                                 
                                    l
                                 
                                 
                                    
                                       model
                                    
                                 
                              
                              .
                           
                        
                     
                  

The GaFinC interface consists of three parts: the gaze tracker, the hand and finger gesture recognizer, and the data integration center. Using the GaFinC interface, the user moves their hands and fingers and changes the gaze point. These control data are recognized by the finger gesture recognizer and the gaze tracker. The recognized data is transmitted to the data integration center, where it is converted to the proper format for the target applications. The overview of the GaFinC interface is described in Fig. 5
                     .

For the gaze tracking system, we use the customized head mounted type gaze tracker. A webcam with an IR removal filter is used as an eye image capture camera and tens of IR LEDs are installed for IR lighting. For the pupil extraction algorithm, we modified the libraries suggested by John H  [18] and added proper signal filters to improve the accuracy. For coordinate transformation from the pupil movement boundary to the target display boundary, the bilinear coordinate transform method is used, prior to which the user needs to calibrate four boundary points.

The evaluate the accuracy of the gaze tracker, gaze pointing tests for 25 reference points were conducted while the user was located 0.8 m from the 640 mm×400 mm monitor. Coordinate matching calibration was done on 9 points and this unit test was repeated 25 times. The average error was 9.9 mm in distance and 0.71 degree in angle. The standard deviation was 1.9 mm in distance and 0.13 degree in angle.  If needed, the accuracy could be improved using high-performance hardware or a commercial gaze tracking system. Fig. 6
                     (2) shows the results of the evaluation test.

In order to recognize finger level gestures and positions, we utilized ‘3Gear system’, which uses two depth sensing cameras (‘Kinect’) installed above the user. It enables the reconstruction of a finger-precise representation of what both hands are doing. This allows one to build simple and intuitive interactions that leverage small, comfortable gestures: pinching and small wrist movements instead of sweeping arm motions, for example. With proper calibration by the user, this system provides millimeter-level accuracy of the user’s hands  [14]. Before using it, all types of unit finger gestures have to be registered: open hand, pinching and pointing (Fig. 7
                     ).

An experiment evaluating an applied finger tracking interface was conducted to compare the tracking result with an actual finger trajectory. In our experiment, subjects were asked to move their fingers freely within the workspace, and the movement was tracked through our system and recorded along with the time. Simultaneously, for comparison, the movement was also captured through ‘Vicon Bonita 10’ motion capture cameras, which are accurate to 0.5 mm, with fiducial markers attached to the fingers. For the analysis, the tracking data and the motion capture data were synchronized in their coordinates and time interval, and compared together in their trajectories over time. As shown in the selected result in Fig. 8
                     , the experiment showed that the system can follow the motion with reasonable accuracy: 6–9 mm on average, and 20–30 mm at maximum, for various types of motions. Further, it was stable enough to track sudden and rapid changes in finger movements (e.g., around the peaks in Fig. 8).

A data integration center was also developed. The user’s hand control and gaze pointing data are sent to here and converted to a certain data format appropriate to the target CAD application. Even if the target CAD application is changed, what the developer does is just modify the output data format in the data integration center. Fig. 5 shows the detailed process in the data integration center.

To verify the performance of the GaFinC interface, two kinds of test were conducted: fast information finding and accurate manipulation tests (see Fig. 9
                     ). In total eight males ranging from 26 to 33 years of age, all familiar with CAD applications, took part in the test, with each participant practicing using the GaFinC interface for at least 5 min to become accustomed to it. ‘Solidworks’  [19] was used as the CAD application. For the fast information finding test, users were asked to find three different numbers from 1 to 9 written on a 3D model using translation, rotation and zooming, and the elapsed time to identify all numbers was measured. A series of 30 tasks was given, which users did without rest for about 15 min—typical of long periods inducing fatigue. They also completed another series of tasks using a conventional mouse for comparison. As a result, it required an average of 14.20 s for each task using the GaFinC and an average of 8.54 s using a mouse, that was 66% more time (with a standard deviation of 2.02 s and 4.9 s for mouse and GaFinC, respectively) (see Fig. 10
                     (4)). From this result, GaFinC might be considered insufficient compared to a mouse. However, taking into account that users are familiar with a mouse interface, it can be expected the time performance would be improved with increased familiarity with the GaFinC interface. Actually, the fastest times recorded by the GaFinC developer were an average of 11.16 s (36% longer than the mouse) with a standard deviation of 1.86 s. Also, general CAD application UI are optimized for a mouse owing to their prevalence. Therefore, customized functionality to support gesture control in the CAD application could further improve the time performance. For accurate manipulation tests, users were asked to match a current 3D model with a ground truth model (match yellow one with blue in Fig. 9(2)) and the error of translation, rotation and zooming were recorded (see Fig. 10(1–3)). There was no time limit and participants tried to match it as accurately as they could. For translation, GaFinC recorded an average of 7.6% error compared with 3.5% for the mouse. For rotation, GaFinC recorded a 5.2° error against a 3.6 degree error for the mouse. For zooming, the difference of distance from the model view camera from the model was measured. Although GaFinC recorded an average of −0.16, against −0.02 for a mouse, the standard deviation was 0.48 and 0.40 for mouse and GaFinC respectively. By ANOVA analysis with 95% CI, GaFinC recorded lower accuracy than a mouse in all manipulation control, showing 
                        p
                     -values of 0.000, 0.002 and 0.043 for translation, rotation and zooming. This means the user could control the zooming task most accurately while control of translation was worst using GaFinC. By monitoring the tests, we found the error in translation and rotation came from the releasing action of the pinch command. Actually the tracking point of each hand placed on the pointing finger. In releasing the pinch command, the tracking points moved slightly and this made errors. In contrast, for zooming command, the position of the pointing fingers are fixed and did not make any releasing error. This problem can be solved through improving the hand tracking interface in future research.

In the final stage of the study, the intuitiveness and comfort levels of the GaFinC and mouse were scored by means of user interviews. All participants who took part in the performance test scored it on a scale of 1 ‘very bad’ to 7 ‘very good’. The results of the ANOVA analysis are shown in Fig. 11
                     . Regarding intuitiveness, GaFinC scored better than the mouse in all manipulation tasks. The GaFinC scored an average of over 5 points, scoring highest in translation and lowest in rotation. The score difference of GaFinC and mouse was greatest for zooming and least for rotation. Actually, the pinch-zoom gesture used in the zooming can be considered to be least reflective of the real-world behavior. However, the use of gaze tracking to select the point of interest enhanced the naturalness of the gaze–gesture zooming behavior combination, resulting in its high intuitiveness rating. The participants reported that it felt very natural to pick the point of interest using their own gaze. In contrast, with regard to comfort, the GaFinC scored worse than a mouse in all manipulation tasks. The GaFinC received an average score of 5 for translation and 4 for rotation and zooming, whereas the mouse scored over 6 for all tasks. Although the GaFinC interface could be considered to possess a normal level of comfort in use, it was still not nearly as comfortable as the mouse. The comfort score differences of the GaFinC and mouse were greatest for zooming and least for translation. The three gestures ranked in order of comfort from lowest to highest are one hand, two hands, and two hands + gaze, indicating that when compared to the mouse controlled by one hand, the comfort of the GaFinC gets worse as additional control interfaces are added. The participants reported that they felt uncomfortable using two hands instead of one even, when the two hands were not floating in the air.

@&#CONCLUSIONS@&#

We proposed a multi-modal interface GaFinC: Gaze and Finger gesture Control for 3D model manipulation tasks. It contains a precise hand tracking and finger gesture recognition interface and an independent gaze tracker for setting the point of interest. In tests to verify the performance, the GaFinC interface demonstrated insufficient performance in accuracy and time compared to the mouse. Although the GaFinC scored better in overall intuitiveness in user interviews, it still needs to be improved in comfort. However, even if these results are not as good as the conventional interfaces, we need to experiment much before we will find new efficient interfaces for CAD applications. For future works, to enhance comfort of the GaFinC, we plan to apply the one-hand gesture control interface based on the participant interviews indicating a two-hand interface interfered with comfort. Recently, various tools optimized for tracking fingers, such as ‘leap motion’  [20], have been introduced. Because of its high tracking resolution and speed, the integration of this finger tracking technology into the GaFinC can be expected to improve performance as well. Finally, owing to the success of the multi-modal interface using gaze tracking for point of interest selection, we will continue to improve this component of the interface using a fixed type commercial gaze tracker for user convenience and high resolution and speed.

@&#REFERENCES@&#

