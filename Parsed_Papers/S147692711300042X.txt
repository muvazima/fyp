@&#MAIN-TITLE@&#A computational model for enhancing recombinant Penicillin G Acylase production from Escherichia coli DH5α
            

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           A computational model to maximize Penicillin G Acylase production is proposed.


                        
                        
                           
                           Bioprocess is modeled using ANN and optimized using ACO.


                        
                        
                           
                           Effect of applying local search techniques within global ACO process is analyzed.


                        
                        
                           
                           Computationally obtained optimized solution was successfully verified in the lab.


                        
                        
                           
                           Proposed method can be employed for modeling and optimization of other bioprocesses.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Penicillin G Acylase production

Ant colony optimization

Artificial neural network

Local search

Preferential local search

@&#ABSTRACT@&#


               
               
                  An attempt was made to develop a computational model based on artificial neural network and ant colony optimization to estimate the composition of medium components for maximizing the productivity of Penicillin G Acylase (PGA) enzyme from Escherichia coli DH5α strain harboring the plasmid pPROPAC. As a first step, an artificial neural network (ANN) model was developed to predict the PGA activity by considering the concentrations of seven important components of the medium. Design of experiments employing central composite design technique was used to obtain the training samples. In the second step, ant colony optimization technique for continuous domain was employed to maximize the PGA activity by finding the optimal inputs for the developed ANN model. Further, the effect of a combination of ant colony optimization for continuous domain with a preferential local search strategy was studied to analyze the performance. For a comparative study, the training samples were fed into the response surface methodology optimization software to maximize the PGA production. The obtained PGA activity (56.94U/mL) by the proposed approach was found to be higher than that of the obtained value (45.60U/mL) with the response surface methodology. The optimum solution obtained computationally was experimentally verified. The observed PGA activity (55.60U/mL) exhibited a close agreement with the model predictions.
               
            

@&#INTRODUCTION@&#

Penicillin G Acylase (PGA) also known as Penicillin amidohydrolases (EC 3.5.1.11) (Cai et al., 2004; McDonough et al., 1999) is a commercially vital enzyme that cleaves Penicillin G into 6-amino penicillanic acid (6-APA) and phenyl acetic acid (PAA). It is widely used for the production of 6-APA which is a key constituent in the production of various semi synthetic penicillin products.

Usually PGA is obtained from microbial sources such as Escherichia coli (Cole, 1969), Arthrobacter viscous (Ohashi et al., 1989), Alcaligenes faecalis (Verhaert et al., 1997), Providencia rettgeri (Klei et al., 1995), Bacillus megaterium (Martin et al., 1995) and Achromobacter xylosoxidans (Cai et al., 2004). The enzyme is secreted either as intercellular or extracellular or inclusion bodies. Since the success of any fermentation process depends on the composition and concentration of medium components, a set of experiments has to be carefully designed in vitro to quantify the effect of each medium component so as to maximize the production.

One of the popular techniques used in the design of experiments is the central composite design (CCD) (Box et al., 1978). The experimental runs of the CCD can be used as samples to generate a mathematical model relating the medium components with the enzyme activity. Generation of mathematical models may be done using a variety of techniques such as statistical methods, response surface methodology (Fannin et al., 1981; Banerjee and Bhattacharyya, 1992; Coninck et al., 2000), artificial neural networks (Chhatpar et al., 2003) and genetic algorithm (Singh et al., 2009). The pac gene found expressed in E. coli strain DH5α in the LB media yielded the maximum recombinant enzyme activity of 1820U/L (Rajendhran and Gunasekaran, 2007).

In this study, an attempt was made to optimize the PGA enzyme productivity by computationally optimizing the composition of media components using artificial neural network and ant colony optimization. In the first phase, two different models were designed which would accept seven important medium components as input parameters to predict the PGA activity as output. During the second phase, the optimal composition of the medium components of the developed models was determined to maximize the productivity of PGA enzyme using ant colony optimization algorithm for continuous domain.

Generally various local search techniques may be combined with global optimization process to generate the best quality solution quickly by minimizing the number of solutions to be surfed over (Neumann et al., 2008). To reduce the computational cost, preferential local search methods can be integrated with the optimization techniques (Bhuvana and Aravindan, 2011) to identify the promising solutions and iteratively deepen the local search on them. In this work, this preferential local search strategy was combined with ant colony optimization algorithm and the performance was analyzed.

For a comparative study, the experimental samples were fed into the response surface methodology optimization software with an objective of maximizing PGA activity.

Based on the preliminary experiments results, the media components glucose, fructose, yeast extract, disodium hydrogen phosphate, potassium dihydrogen phosphate, sodium chloride and magnesium sulfate were found to be the major elements in PGA production. The present work was carried out according to the Circumscribed CCD. The coded levels for independent variables are given in Table 1
                           . Some additional experiments were manually added to the CCD designed 152 experiments to obtain an uniform distribution and widening of the search space. As a result, a total of 180 experiments were performed to measure the enzyme activities. (Table 8 in Appendix A). Experiments were carried out in duplicate. The data have ±1% error.

Since the early development of recombinant DNA technology, E. coli have been widely used as a host to obtain a high level expression of recombinant proteins. The strain used in this work was recombinant E. coli DH5α harboring recombinant construct pPROPAC (pET-30b carrying pac gene from Bacillus badius with its own promoter). The nucleotide sequences of the B. badius 16S rRNA and pac genes were deposited in the GenBank database under the accession numbers AY803745 and DQ115799. The strain was obtained from the Culture Bank, Department of Microbial Technology, Centre for Advanced Studies in Functional Genomics, School of Biological Sciences (Madurai Kamaraj University, Madurai, Tamil Nadu, India).

The strain E. coli DH5α containing B. badius pac gene was recovered from glycerol stock by inoculating it in 2 mL LB medium containing kanamycin (kan) (30μg/mL) kept overnight at 37̊C, 170rpm. LB agar (kan) plates were prepared and streaked with the overnight culture to develop single colonies at 37̊C for 24h. For inoculum, 150mL fermentation media (kan (30μg/mL)) was subcultured with a single colony grown from overnight in LB agar plate. Absorbence above 0.50 was taken for inoculations.

The medium contained 10.00g/L fructose, 10.00g/L glucose, 6.00g/L yeast extract, 1.85mL/L MgSO4·7H2O (100mM), 9.00g/L disodium hydrogen phosphate, 4.00g/L potassium dihydrogen phosphate, 0.75g/L sodium chloride, 2.00mL/L calcium chloride (10mM) and 2.00mL/L thiamine-HCl (100mM). Some components such as MgSO4·7H2O, thiamine-HCl and calcium chloride were autoclaved separately due to their tendency to precipitate. Fructose and glucose were filter sterilized separately (0.22μm, Steritop, Millipore Corporation, Billerica, USA). Remaining components such as yeast extract, disodium hydrogen phosphate, potassium dihydrogen phosphate and sodium chloride were autoclaved together before mixing with the autoclaved components. The medium was further supplemented with kan (30μg/mL) (all components of the media were obtained from Himedia chemicals, India). An inoculum (1μL/mL) from the culture was grown for 24h (28̊C, pH 7) in a refrigerated shaker (New Brunswick Scientific Classic Series, C76, USA) at 170rpm.

Cells from the sample (10mL) were recovered by centrifugation at 1574×
                           g for 10min (Hitachi high speed centrifuge, CF15R, Japan). The pellet was resuspended in 2mL of 50mM sodium phosphate buffer (pH 7.0). The cells were lysed by sonication (B. Braun, Labsonic2000, Germany), and centrifuged at 9838 ×
                           g for 30min. The clear supernatant liquid was assayed with an equal volume of Penicillin G (20mg/mL) as the substrate. The assay was carried out at 50̊C for an hour. To 0.50mL of reaction mix, 3.50mL of Balasingham reagent was added (Balasingham et al., 1972) and incubated at room temperature for 5min. Absorbence was measured at 415nm (Hitachi UV-VIS Spectrophotometer, U2800, Japan). One unit of PGA activity is defined as the amount of enzyme required to release 1μmol of 6-APA/h at pH 7 and 50̊C.

@&#METHODS@&#

Artificial neural networks (ANN) is an information processing system that provides optimum performance characteristics in common with the biological neural networks (Hopfield, 1982). ANN is an interconnected group of artificial neurons that resides in different set of layers comprising an input layer, output layer and one or more optional hidden layers. It uses a computational model for processing;the general architecture of multilayer neural net is shown (Fig. 1
                           ). In supervised learning, the identified mapping function ‘f’ maps the input data with a predictable output result. All neural algorithms minimize the error ie. the mismatch between target and network outputs.

Back propagation neural network (BPN) is a multilayer feed forward supervised neural network which uses back propagation learning algorithm to train the neural network. Levenberg Marquardt network (LMN) is yet another multilayer feed forward artificial neural network which uses Levenberg Marquardt algorithm (Levenberg, 1944) to train the network. Both the above methods were used in the present work.

Ant colony optimization (ACO) (Dorigo and Stuzle, 2006) is a metaheuristic method inspired by the behavior of real ants for solving hard combinatorial optimization problems.

Real ants deposit chemical substance called pheromone on the ground while traversing from nest to food. The followers can sense the pheromone to find the path probabilistically. Pheromone gets evaporated slowly in due course of time which makes the ants to forget the suboptimal path to which they converge so that the new and shorter one can be discovered and learned. Thus, ACO finds good solutions for the given optimization problem by forming a colony of artificial ants that cooperates among themselves through stigmergy. It can solve the problems that are characterized by a finite set of states each decision variable may assume.

ACO can also be extended to continuous domains as Ant Colony Optimization algorithm for Continuous domains (ACOR) (Socha and Dorigo, 2008). The fundamental idea underlying ACOR is the shift from using a discrete probability distribution to a continuous probability density function (PDF). In ACO, for combinatorial problems, the pheromone values associated with a finite set of discrete values are represented in the form of a pheromone table. But ACOR uses rather a solution archive as the way of describing the pheromone distribution over the search space. The solution archive contains a number of complete solutions to the problem.

The basic stream of the ACOR algorithm is as follows. ACOR keeps a history of its search process by storing a set of solutions in a solution archive T of dimension k. For a given n-dimensional continuous optimization problem with randomly generated initial k solutions, ACOR stores the k solutions and their objective function values in a sorted order based on their quality in T. At each iteration, a set of m new solutions are generated by the m ants. All the (k + m) solutions are sorted based on their objective function value and stored in T. From this set of (k + m) solutions, the m worst solutions are pruned. In this way, the search process aims to find out the best solution.

Each solution in T can be denoted as S
                           1, S
                           2, …, S
                           
                              k
                           . Each S
                           
                              i
                            can be represented as S
                           
                              i
                            = (
                              
                                 s
                                 i
                                 1
                              
                              ,
                              
                                 s
                                 i
                                 2
                              
                              ,
                              …
                              ,
                              
                                 s
                                 i
                                 n
                              
                           ) where 
                              
                                 s
                                 i
                                 j
                              
                            represents the value for the jth variable in the ith solution. The weights 
                              
                                 w
                                 1
                              
                              ,
                              …
                              ,
                              
                                 w
                                 k
                              
                            are calculated for each solution using Eq. (1) using Gaussian function g(1, qk).


                           
                              
                                 (1)
                                 
                                    
                                       
                                          w
                                          l
                                       
                                       =
                                       
                                          
                                             1.0
                                          
                                          
                                             q
                                             k
                                             
                                                
                                                   2
                                                   π
                                                
                                             
                                          
                                       
                                       
                                          e
                                          
                                             (
                                             −
                                             
                                                
                                                   (
                                                   l
                                                   −
                                                   1
                                                   )
                                                
                                                2
                                             
                                             /
                                             2
                                             
                                                q
                                                2
                                             
                                             
                                                k
                                                2
                                             
                                             )
                                          
                                       
                                    
                                 
                              
                           where q is a parameter which controls the diversification of the search process of the algorithm.

Construction of a new solution by an ant is accomplished in an incremental manner, variable by variable. As the first step, to perform the sampling process, the ant chooses one of the solutions in the solution archive T probabilistically; choosing the lth solution S
                           
                              l
                            is:


                           
                              
                                 (2)
                                 
                                    
                                       p
                                       x
                                    
                                    =
                                    
                                       
                                          
                                             w
                                             x
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             k
                                          
                                          
                                             
                                                w
                                                i
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

The ant treats each decision variable i
                           =1, …, n separately. It takes the value 
                              
                                 s
                                 l
                                 i
                              
                            of the variable i of the chosen lth solution as mean (μ) and computes the standard deviation (σ) as the average distance between the ith variable of the solution S
                           
                              l
                            and the ith variables of the other solutions in the archive, multiplied by the learning parameter ξ:


                           
                              
                                 (3)
                                 
                                    σ
                                    =
                                    ξ
                                    
                                       ∑
                                       
                                          e
                                          =
                                          1
                                       
                                       k
                                    
                                    
                                       
                                          
                                             |
                                             
                                                s
                                                e
                                                i
                                             
                                             −
                                             
                                                s
                                                l
                                                i
                                             
                                             |
                                          
                                          
                                             k
                                             −
                                             1
                                          
                                       
                                    
                                 
                              
                           
                        

Then the sampling is done using a Gaussian distribution function:
                              
                                 (4)
                                 
                                    
                                       P
                                       (
                                       x
                                       )
                                       =
                                       
                                          
                                             1.0
                                          
                                          
                                             σ
                                             
                                                
                                                   2
                                                   π
                                                
                                             
                                          
                                       
                                       
                                          e
                                          
                                             (
                                             −
                                             
                                                
                                                   (
                                                   x
                                                   −
                                                   μ
                                                   )
                                                
                                                2
                                             
                                             /
                                             2
                                             
                                                σ
                                                2
                                             
                                             )
                                          
                                       
                                    
                                 
                              
                           This whole process is repeated for each decision variable/dimension i
                           =1, …, n of a solution which in turn will be repeated for each of the m ants. This optimization process will get terminated until the criteria chosen for the problem is satisfied.

Metaheuristics for continuous optimization are often hybridized with any one of the local search methods. This allows ACO to focus on global optimization, while the local search methods, such as gradient-based or direct search methods, help to find the local optimal values. Once the solutions have been constructed and before updating pheromones further, an improvement of the solutions could be made with the use of local search algorithms. The locally optimized solutions are then used to decide which pheromones are to be updated. In this work, the local search has been performed by using gradient ascent and Levenberg Marquardt algorithms.

Gradient ascent is the simplest first-order optimization algorithm (Davidon, 1976) which uses gradient measure to find the local maximum in a function. This algorithm takes steps in proportion to the positive of the gradient function f at the current point x
                              
                                 i
                               to approach the local maximum of that function and the updating rule:


                              
                                 
                                    (5)
                                    
                                       
                                          x
                                          
                                             i
                                             +
                                             1
                                          
                                       
                                       =
                                       
                                          x
                                          i
                                       
                                       +
                                       λ
                                       
                                          f
                                          ′
                                       
                                       (
                                       
                                          x
                                          i
                                       
                                       )
                                    
                                 
                              
                           

This algorithm (Levenberg, 1944) provides a solution to the problem of minimizing or maximizing a non linear function. In order to approach the local maximum of the function f, it uses both gradient and second order derivatives at the current point x
                              
                                 i
                              . The updating rule of the algorithm is a blend of Gradient ascent and Gauss Newton algorithm and given by:


                              
                                 
                                    (6)
                                    
                                       
                                          
                                             x
                                             
                                                i
                                                +
                                                1
                                             
                                          
                                          =
                                          
                                             x
                                             i
                                          
                                          +
                                          
                                             
                                                (
                                                H
                                                +
                                                λ
                                                I
                                                )
                                             
                                             
                                                −
                                                1
                                             
                                          
                                          ×
                                          
                                             f
                                             ′
                                          
                                          (
                                          
                                             x
                                             i
                                          
                                          )
                                       
                                    
                                 
                              where H represents Hessian matrix. The damping factor λ adjusted in each iteration guides the optimization process.

Response surface methodology (RSM) is a collection of statistical and mathematical techniques to develop, improve, and optimize a given process. The most expensive applications of RSM are in the particular situations where several input variables potentially influence some performance measure or quality characteristic of the process. Thus, performance measure or quality characteristic is called the response. The input variables are called as independent variables which could be controlled by the scientist or engineer during production.

RSM a downloadable trail version (http://www.statease.com) is widely used for multivariate optimization studies in several biotechnological processes such as optimization of media and fermentation based on multiple linear regression.

The developed ANN–BPN model using BPN contains an input layer with seven neurons representing the composition of individual medium components, two hidden layers and an output layer with single neuron which represents the enzyme activity. Sigmoid function was used as the activation function in all layers. Data set with 180 patterns was normalized from −1 to +1 using min-max normalization method. Weights were initialized using Nguyen-Widrow initialization method (Nguyen and Widrow, 1990). Race package (Birattari et al., 2002) (Race-0.1.58) in R was used for the purpose of parameter tuning of ANN–BPN model.

The parameters considered for tuning ANN–BPN model were the number of neurons in the hidden layers and the learning rate. Experiments were set up in the following fashion. The number of neurons in the first hidden layer did not exceed double the size of input layer (Heaton, 2008); the range of possible number of neurons in the first hidden layer was taken from 3 to 13. The possible number of neurons in the second hidden layer was based on the neurons in layer one and input layer (Masters, 1993), the obtained size was approximately m/2 where m represented the size of the first hidden layer. So the choice for the probable number of neurons in the second hidden layer was considered from 1 to m/2. Since the learning rate value was a continuous one, its value was taken in a discrete form ranging from 0.10 to 0.90 with an interval of 0.10 each.

To validate the performance of the trained classifier, while running each experiment a 10-fold cross-validation was applied. In 10-fold cross-validation, total pattern set was divided into ten subsets. In each of the total of ten iterations, one of the ten subsets was used as the test set, while the remaining nine subsets were merged to form the training set. The objective fixed for validating each experiment was to minimize the squared error percentage (SEP) function (Eq. (7)) for the set of test patterns.


                           
                              
                                 (7)
                                 
                                    
                                       SEP
                                       =
                                       100
                                       
                                          
                                             
                                                O
                                                
                                                   max
                                                
                                             
                                             −
                                             
                                                O
                                                
                                                   min
                                                
                                             
                                          
                                          
                                             
                                                n
                                                o
                                             
                                             
                                                n
                                                p
                                             
                                          
                                       
                                       
                                          
                                             ∑
                                          
                                          
                                             p
                                             =
                                             1
                                          
                                          
                                             
                                                n
                                                p
                                             
                                          
                                       
                                       
                                          
                                             ∑
                                          
                                          
                                             i
                                             =
                                             1
                                          
                                          
                                             
                                                n
                                                0
                                             
                                          
                                       
                                       
                                          
                                             (
                                             
                                                t
                                                i
                                                p
                                             
                                             −
                                             
                                                o
                                                i
                                                p
                                             
                                             )
                                          
                                          2
                                       
                                    
                                 
                              
                           where O
                           max and O
                           min represent the maximum and minimum values of the output signals of the output neurons, n
                           
                              p
                            represents the number of patterns, n
                           0 is the number of output neurons, and 
                              
                                 t
                                 i
                                 p
                              
                            and 
                              
                                 o
                                 i
                                 p
                              
                           , respectively, with regard to the expected and actual values of output neuron i for pattern p.

The identified optimized parameters for the ANN–BPN model were nine neurons in the first hidden layer and one neuron in the second hidden layer with the learning rate as 0.10. With this network configuration, the model was developed by taking the entire data as the training set. To measure the performance of the developed ANN model, a hold out method was applied on the data set. The obtained result was compared statistically with the expected result using two-tail paired t-test at 0.05 level.

The parameters considered for tuning the ANN–LMN model was the number of neurons in the two hidden layers. Experiments were set up in a similar fashion as defined for ANN–BPN model. The optimized parameters for the ANN–LMN model comprised thirteen neurons in the first hidden layer and six neurons in the second hidden layer. Sigmoid function was used as the activation function in all layers. With this network configuration, the model was generated by taking the entire data set as the training set. The performance with ANN–LMN model was also measured in the same way as the ANN–BPN model.

The input space of the two developed models (ANN–LMN and ANN–BPN) was optimized using ACOR, ACOR with Gradient Ascent method as local search (ACOR + GA), ACOR with Levenberg Marquardt method as local search (ACOR + LM). Seven continuous decision variables were chosen indicating the seven medium components.

Gaussian function was used to perform the sampling process for each ant. The values of the two parameters (i.e. the number of solutions in the archive (k) and the number of ants (m)) were chosen based on the following experiment: eight sets of different initial populations were taken. For each initial population, experiments were conducted by varying the k value from 50 to 150 in steps of 25 and for each k, the value of m was taken as 2, 5, 10, 15 and 20, respectively. For 50% of the population sets, the parameters (k = 50, m = 2) yielded the optimal output with minimum number of iterations than other combinations; 25% of population sets gave output with minimum number of iterations for the parameter (k = 50, m = 5), while for the remaining population sets the best pair of parameters were k = 50, m = 15. From the results of the above computation, the parameters k and m were chosen with the values 50 and 2, respectively. Based on the preliminary experiments, the following parameters were set with the appropriate values during the optimization process. The locality of search process parameter q was assigned with 0.02 and the speed of convergence parameter ξ was assigned with the value 0.85. The stopping criteria used was either the required accuracy |f(s)−
                           f
                           max| where f(s) is the value of the best solution found so far in the algorithm and f
                           max is the maximum value of the optimal expected solution or up to the maximum of a specified number of iterations. The maximum number of iterations to be performed during the optimization process was taken as 5000.

While applying gradient ascent local search, point x
                           
                              i
                            in Eq. (5) represents a vector with seven components indicating the inputs of the neural network model and the function to maximize locally is nothing but the neural network model itself. The updating factor to be used in the gradient ascent updating rule can be represented in the generic way as explained below.


                           Fig. 2
                            shows the general multilayer feedforward network model comprising an input layer with L neurons (X
                           1, X
                           2, …, X
                           
                              L
                           ), hidden layer with m neurons (Y
                           1, Y
                           2, …, Y
                           
                              m
                           ) and an output layer with k neurons (Z
                           1, Z
                           2, …, Z
                           
                              k
                           ). The 
                              
                                 w
                                 ij
                              
                            and 
                              
                                 v
                                 ki
                              
                            parameters represent the weights between the hidden layer unit i and input unit j and the output unit k and the hidden unit i, respectively. Thus the L dimension input vector X can be denoted as X = (x
                           1, x
                           2, …, x
                           
                              L
                           ) and the gradient ∇X can be represented as


                           
                              
                                 (8)
                                 
                                    ∇
                                    X
                                    =
                                    (
                                    ∇
                                    
                                       
                                          x
                                          1
                                       
                                    
                                    ,
                                    ∇
                                    
                                       
                                          x
                                          2
                                       
                                    
                                    ,
                                    …
                                    ,
                                    ∇
                                    
                                       
                                          x
                                          L
                                       
                                    
                                    )
                                 
                              
                           where


                           
                              
                                 (9)
                                 
                                    ∇
                                    
                                       
                                          x
                                          p
                                       
                                    
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   ∂
                                                   
                                                      
                                                         z
                                                         1
                                                      
                                                   
                                                
                                                
                                                   ∂
                                                   
                                                      
                                                         x
                                                         p
                                                      
                                                   
                                                
                                             
                                             +
                                             
                                                
                                                   ∂
                                                   
                                                      
                                                         z
                                                         2
                                                      
                                                   
                                                
                                                
                                                   ∂
                                                   
                                                      
                                                         x
                                                         p
                                                      
                                                   
                                                
                                             
                                             +
                                             …
                                             +
                                             
                                                
                                                   ∂
                                                   
                                                      
                                                         z
                                                         k
                                                      
                                                   
                                                
                                                
                                                   ∂
                                                   
                                                      
                                                         x
                                                         p
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

Here ∂z
                           
                              j
                           /∂x
                           
                              p
                            is computed using the formula


                           
                              
                                 (10)
                                 
                                    
                                       
                                          ∂
                                          
                                             
                                                z
                                                j
                                             
                                          
                                       
                                       
                                          ∂
                                          
                                             
                                                x
                                                p
                                             
                                          
                                       
                                    
                                    =
                                    
                                       f
                                       
                                          
                                             z
                                             j
                                          
                                       
                                       ′
                                    
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       m
                                    
                                    
                                       f
                                       
                                          
                                             y
                                             i
                                          
                                       
                                       ′
                                    
                                    ×
                                    
                                       w
                                       ip
                                    
                                    ×
                                    
                                       v
                                       ji
                                    
                                    ;
                                    
                                    j
                                    =
                                    1
                                    ,
                                    …
                                    ,
                                    k
                                    ,
                                    
                                    
                                    p
                                    =
                                    1
                                    ,
                                    …
                                    ,
                                    L
                                 
                              
                           where f′ indicates the derivative of the activation function used in the corresponding layer.

When using GA as local search, the quality of the solution was improved until the absolute difference between the objective function value of previous iteration was achieved with less than 0.01 or up to the maximum of hundred iterations. In the case of LM as local search, improvement was done until an increase in the objective function value of the current iteration solution over the previous iteration or up to the maximum of fifty iterations. In LM, the damping factor value was chosen as 0.001 and the value 10 was taken as the alteration factor for the damping factor. In both cases the local search was applied for all solutions in the initial population and whenever a new solution is generated by each ant.

The methodology chosen for preferential local search was to carry out the local search only to the set of good quality solutions. Implementation of the above algorithms were done in R language.

@&#RESULTS AND DISCUSSION@&#

In order to evaluate the performance of the developed ANN models, the hold out method was used by randomly selecting 10%, 20% and 30% of data as the test set and the remaining as training set. Under each category, four different test data sets were randomly selected to conduct the experiments. Analysis of differences between the expected output and ANN model output for the test data sets was done using the two-tail paired t-test. The obtained t-values for the four different test data sets in each category are tabulated (Table 2
                        ).

The statistical tests confirmed that the differences between the experimental results and ANN estimation values obtained for the test data sets are not statistically significant.

Fifty sets of different initial solution archives were taken as the starting point for the optimization process analysis.

ACOR was applied to optimize the input parameters of the ANN–BPN model by taking each solution archive set as a starting point. In addition, the local search algorithms GA and LM were used with ACOR. ACOR, ACOR with GA, ACOR with LM were applied for the optimization process of fifty sets of experiments and results were obtained by taking average (Table 3
                           ).

The average enzyme activity obtained through the optimization process from all fifty experiments was approximately 53.40U/mL. Application of local search techniques GA and LM with ACOR did not improve the value of optimum solution. While using local search, the number of solutions surfed over the search space/iterations got more reduced than with the iterations of ACOR but the time taken for convergence was higher than with ACOR. Therefore a new strategy was needed to balance the number of solutions surfed and the time taken for convergence. Hence a preferential local search strategy was tried.

While doing the experiments, to reach an optimum value of 53.40U/mL, ACOR took an average of 1082.76 iterations within 9.26s. Only a negligible improvement of 0.0019U/mL could be observed at an additional cost of 2123 iterations and 13.96s. The same scenario prevailed with other experiments too. Since all the three techniques converged to the same solution at 53.40U/mL, instead of taking the number of iterations as the terminating condition for the optimization process, the optimal value to the optimization process (f
                           max) was set to this value and considered as the termination point. With this setup, the same set of fifty experiments were repeated and the obtained average values are tabulated (Table 4
                           ). The same setup was followed for other experiments also.

While applying the preferential local search, local search was applied to the best x solutions in the solution archive before the start of the next iteration in the optimization process. Also the maximum number of iterations y ought to be performed inside the local search process was reduced. A set of experiments with preferential local search GA and LM were carried out for all the 50 different initial solution archives by varying the parameters x and y (Tables 5 and 6
                           
                           ). It is interesting to note that reducing the value of x leads to a reduction in the convergence time to reach the optimal but reducing the y value results in more iterations for convergence (Tables 5 and 6).

From the results given (Tables 4 and 5), it is clear that while applying ACOR alone for optimization, an average of 1082.76 iterations with 9.26s were required to find the optimum solution. With GA as local search, the number of iterations required to reach optimum solution was reduced by 44.50% but the time taken increased by 58.10%. When x value is 1 and y value is 2 in the preferential local search, the optimum solution was obtained with 20.50% less number of iterations as compared with ACOR and 47.40% reduction in time as compared with ACOR with GA.

With LM as local search, the number of iterations required to reach optimum solution was reduced by 42.76% but the time increased to 32.66% when compared with ACOR results (Tables 4 and 6). When using the preferential local search, the optimum solution was obtained with 7.53% reduction in the number of iterations when compared with ACOR and 7.93% reduction in time over ACOR with LM by taking both x and y value as 1. Therefore, the preferential local search is the best option among the various methods investigated.

The input space of ANN–LMN model was optimized using ACOR, ACOR with GA, ACOR with LM with the same set of initial solution archives as the ANN–BPN model (Table 7
                           ).

The average enzyme activity obtained through the optimization process from all fifty experiments was 56.94U/mL. Both the local searches GA and LM did not yield positive effect over ACOR both in terms of number of solutions to be surfed over the solution space and the time for convergence.

The solutions obtained by the optimization of the input space of ANN–BPN and ANN–LMN model were tested in vitro. The solution predicted by the ANN–LMN model yielded 55.60U/mL in the wet lab experiment. The composition of medium obtained by ANN–LMN model optimized with ACOR comprise of disodium hydrogen phosphate: 13.84g/L, sodium chloride: 2.45g/L, potassium dihydrogen phosphate: 1.08g/L, fructose: 29.74g/L, glucose: 25.56g/L, yeast-extract: 5.47g/L and magnesium sulfate: 0.11g/L with the PGA enzyme activity of 56.94U/mL. Thus the results obtained in silico and in vitro are in close agreement.

@&#CONCLUSION@&#

In this work, an increase in the productivity of PGA enzyme was achieved by optimizing the composition of media components using the artificial neural network and ant colony optimization techniques. To predict the enzyme activity, two models based on ANN were developed. The optimal composition of medium components was identified through ACOR along with a preferential local search technique for effective optimization. With the developed computational model, the optimized PGA enzyme activity was 56.94U/mL. The result was experimentally verified (55.60U/mL) and found in close agreement with the computational result. Also, the PGA activity obtained by this technique is found to be higher than that obtained through response surface methodology (45.60U/mL).

The approach presented in this paper is sufficiently general and thus can also be employed for modeling and optimization of other bioprocesses.

@&#ACKNOWLEDGEMENTS@&#

The authors are grateful to thank the anonymous reviewers for their useful comments and directions for the improvement of the manuscript. The authors also thank their respective institutes for the support in conducting the wet lab experiments.

See Table 8
                     .

@&#REFERENCES@&#

