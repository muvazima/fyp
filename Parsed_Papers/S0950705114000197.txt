@&#MAIN-TITLE@&#A multi-instance learning wrapper based on the Rocchio classifier for web index recommendation

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We propose a new multi-instance learning algorithm for web index recommendation.


                        
                        
                           
                           Compared to previously proposed methods, our algorithm has low computational cost.


                        
                        
                           
                           Our method outperforms state-of-the-art solutions on benchmark data, achieving in particular a high precision.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Multi-instance learning

Multi-instance wrapper

Recommender system

Rocchio classifier

Web mining

@&#ABSTRACT@&#


               
               
                  Web index recommendation systems are designed to help internet users with suggestions for finding relevant information. One way to develop such systems is using the multi-instance learning (MIL) approach: a generalization of the traditional supervised learning where each example is a labeled bag that is composed of unlabeled instances, and the task is to predict the labels of unseen bags. This paper proposes a multi-instance learning wrapper method using the Rocchio classifier to recommend web index pages. The wrapper implements a new way to relate the instances with the class labels of the bags. The proposed method has low computational cost and the experimental study on benchmark data sets shows that it performs better than the state-of-the-art methods for this problem.
               
            

@&#INTRODUCTION@&#

Multi-instance learning (MIL) is a generalization of traditional supervised learning. Since it was formalized by Dietterich et al. [8] in 1997, the interest of the machine learning scientific community in this approach has grown rapidly because of its suitability for solving certain types of complex problems. Such problems are characterized by the fact that each example has multiple forms of representation or, alternatively, consists of multiple parts [7], or represents multiple samples from a stochastic process [2]. Consequently, an example is not described by a single feature vector, but by a bag of feature vectors which are called instances. The class labels of instances are not known, only those of the bags. The goal is to classify new bags based on the description of its instances.

In this paper, we apply the MIL approach to the development of recommendation systems for web index pages. This is a kind of web mining task aimed at assisting Internet users with web page suggestions. Web index pages are Internet pages that only provide titles or brief summaries while leaving the detailed presentation to their linked pages. The recommendation system learns a model for deciding whether a new web index page is relevant to one user on the basis of other web index pages this user has already browsed and classified as relevant or irrelevant, to provide the user with new potentially interesting web index pages.

The web index recommendation (WIR) problem is difficult because the information available about the user is ambiguous: we know whether the user is interested in a web index, but we do not know why, i.e., we do not know which linked pages make that web index relevant for the user. The relationship between a web index and its linked pages makes this problem a natural candidate for the MIL approach. Concretely, web indexes are considered as bags and their linked pages make up the individual instances.

The WIR problem was introduced in the context of MIL by Zhou et al. [37]. Fretcit-kNN [37], based on the well known kNN classifier, was the first MIL method designed specifically for WIR. More recently, two grammar-guided genetic programming methods, G3P-MI [33] and MOG3P-MI [32], were developed for this problem.

Our paper presents a new MIL method for the WIR problem that develops a wrapper algorithm using the Rocchio classifier [23]. The wrapper algorithm uses a new heuristic to assign class labels to instances and thus transforms the data of the MIL problem so that traditional simple-instance learning methods can be used. The Rocchio classifier is a classic method in the information retrieval field [1,25,26]. which has the advantage to be very fast and suitable for textual problems. We learn a Rocchio classifier from the instances that have been labeled by the wrapper, and use it to classify the instances of a new bag. We compare our proposed method with Fretcit-kNN and MOG3P-MI in an experimental study on nine WIR benchmark data sets introduced in [37], which demonstrates that our method exhibits excellent performance. We also show how our method can be optimized to take into account the imbalance between positive and negative bags present in some of the data sets.

The rest of this paper is organized as follows. In the next section, we recall formal aspects that relate to the MIL problem, and give a brief overview of MIL approaches. Section 3 introduces the WIR problem and summarizes representation forms and solution methods that have been used in previous studies to solve this problem. Section 4 presents the details of the method we propose. In Section 5, experimental results and discussions are provided. Finally, we offer concluding remarks in Section 6.

This section introduces the fundamentals of multi-instance learning (MIL). Our MIL definition is stated in Section 2.1; assumptions that drive an algorithm to the solution of MIL problems, and that are relevant to this paper, are examined in Section 2.2; and finally, we present a categorization of MIL methods in Section 2.3.

MIL is a kind of supervised learning. In this setting, we consider a training set 
                           
                              D
                              =
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      1
                                                   
                                                
                                                ,
                                                f
                                                
                                                   
                                                      
                                                         
                                                            
                                                               x
                                                            
                                                            
                                                               1
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       ,
                                       …
                                       ,
                                       
                                          
                                             
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      n
                                                   
                                                
                                                ,
                                                f
                                                
                                                   
                                                      
                                                         
                                                            
                                                               x
                                                            
                                                            
                                                               n
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                         of labeled examples, where 
                           
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                           
                         is the description of the i-th example and 
                           
                              f
                              
                                 
                                    
                                       
                                          
                                             x
                                          
                                          
                                             i
                                          
                                       
                                    
                                 
                              
                              ∈
                              L
                           
                         the label attached to it.

In the traditional (simple-instance) supervised learning scenario, each example 
                           
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                           
                         consists of a single instance represented by a fixed-length vector of features, i.e., 
                           
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              ∈
                              X
                           
                        , where 
                           
                              X
                              =
                              
                                 
                                    X
                                 
                                 
                                    1
                                 
                              
                              ×
                              ⋯
                              ×
                              
                                 
                                    X
                                 
                                 
                                    d
                                 
                              
                           
                         is called the instance space. Conversely, in MIL, each example is a multi-set (or colloquially a bag) 
                           
                              
                                 
                                    b
                                 
                                 
                                    i
                                 
                              
                              ∈
                              
                                 
                                    N
                                 
                                 
                                    X
                                 
                              
                           
                         of instances with a single label 
                           
                              f
                              
                                 
                                    
                                       
                                          
                                             b
                                          
                                          
                                             i
                                          
                                       
                                    
                                 
                              
                           
                        . The bag 
                           
                              
                                 
                                    b
                                 
                                 
                                    i
                                 
                              
                           
                         consists of 
                           
                              
                                 
                                    n
                                 
                                 
                                    i
                                 
                              
                           
                         unlabeled instances 
                           
                              
                                 
                                    x
                                 
                                 
                                    ij
                                 
                              
                              ∈
                              X
                              ,
                              j
                              =
                              1
                              ,
                              …
                              ,
                              
                                 
                                    n
                                 
                                 
                                    i
                                 
                              
                           
                        . While in the traditional supervised learning the objective is to approximate the function 
                           
                              f
                              :
                              X
                              →
                              L
                           
                        , in MIL the function to approximate is 
                           
                              f
                              :
                              
                                 
                                    N
                                 
                                 
                                    X
                                 
                              
                              →
                              L
                           
                        .

In this paper, we restrict ourselves to two-class classification, i.e., 
                           
                              L
                              =
                              
                                 
                                    
                                       0
                                       ,
                                       1
                                    
                                 
                              
                           
                        , where 0 stands for the negative class and 1 for the positive class.

A key element in the design of a MIL algorithm is the hypothesis it assumes regarding the relationship between instances in a bag and bag-level class labels. The first MIL algorithms only considered the assumption that a bag is positive if and only if at least one of its instances is positive. This hypothesis was called standard MIL assumption by Weidmann et al. [28]. Under this assumption, the Boolean mapping function 
                           
                              
                                 
                                    f
                                 
                                 
                                    S
                                 
                              
                           
                         is defined as 
                           
                              
                                 
                                    f
                                 
                                 
                                    S
                                 
                              
                              (
                              b
                              )
                              
                              ⇔
                              
                              ∃
                              x
                              ∈
                              b
                              :
                              
                                 
                                    c
                                 
                                 
                                    I
                                 
                              
                              (
                              x
                              )
                           
                        , where b is a bag and 
                           
                              
                                 
                                    c
                                 
                                 
                                    I
                                 
                              
                              ∈
                              C
                           
                         is a concept from a concept space 
                           
                              C
                           
                        . The standard MIL assumption has been applied to various problems such as drug activity prediction [8], image retrieval [3,19,36], and text categorization [3].

Weidmann et al. [28] also presented various generalizations to the standard MIL assumption. According to these, a set of underlying concepts 
                           
                              C
                              ⊂
                              C
                           
                         is used, each of which contributes to the classification. One of the generalizations is the threshold based assumption, in which the mapping function 
                           
                              
                                 
                                    f
                                 
                                 
                                    T
                                 
                              
                           
                         requires a minimum number of instances of each concept; formally, 
                           
                              
                                 
                                    f
                                 
                                 
                                    T
                                 
                              
                              (
                              b
                              )
                              
                              ⇔
                              
                              ∀
                              
                                 
                                    c
                                 
                                 
                                    i
                                 
                              
                              ∈
                              C
                              :
                              Δ
                              
                                 
                                    
                                       b
                                       ,
                                       
                                          
                                             c
                                          
                                          
                                             i
                                          
                                       
                                    
                                 
                              
                              ⩾
                              
                                 
                                    t
                                 
                                 
                                    i
                                 
                              
                           
                        , where 
                           
                              Δ
                              :
                              
                                 
                                    N
                                 
                                 
                                    X
                                 
                              
                              ×
                              C
                              →
                              N
                           
                         is a counting function which counts the members of a given concept in a bag, and 
                           
                              
                                 
                                    t
                                 
                                 
                                    i
                                 
                              
                           
                         is the lower threshold of concept 
                           
                              
                                 
                                    c
                                 
                                 
                                    i
                                 
                              
                           
                        . The standard MIL assumption can be seen as a particular case of the threshold based assumption where 
                           
                              C
                              =
                              
                                 
                                    
                                       
                                          
                                             c
                                          
                                          
                                             I
                                          
                                       
                                    
                                 
                              
                           
                         and 
                           
                              
                                 
                                    f
                                 
                                 
                                    S
                                 
                              
                              (
                              b
                              )
                              
                              ⇔
                              
                              Δ
                              
                                 
                                    
                                       b
                                       ,
                                       
                                          
                                             c
                                          
                                          
                                             I
                                          
                                       
                                    
                                 
                              
                              ⩾
                              1
                           
                        .

Other MIL assumptions include those based on bag-level distances [27], instance-level distances [24], the collective assumption [11], and more specific MIL assumptions. A good review of the most important MIL assumptions is presented in [12].


                        Table 1
                         summarizes important MIL methods which have been developed to date, categorizing them according to their most distinctive features. Xu’s taxonomy [29], placed in the columns of the table, divides MIL methods into instance-based approaches and metadata-based approaches. Instance-based approaches make the assumption that instances have hidden class labels. The set of hidden instance-level class labels may or may not correspond to the set of bag-level class labels. Methods in this category typically try to estimate a function that assigns class labels to instances, and then use that function to make a prediction at bag level. This category is further subdivided according to the specific MIL assumption used. The most basic division is into those algorithms implementing the standard MIL assumption and those implementing other assumptions. The importance of the standard MIL assumption is that it was the first approach to MIL and there are many algorithms that implement it.

Methods that use the metadata approach rely on the assumption that the class labels of bags are determined by some meta-level information that describes the examples. Algorithms of this type generally apply a transformation that maps bags into a new simple-instance feature space, where the features consist of some metadata extracted from the bag. A simple-instance learning algorithm can then be applied to the instances in this space in order to make predictions. Most metadata algorithms are data-oriented in the sense that the metadata is extracted directly from the data. The alternative to data orientation is the model-oriented approach, where some model is built on the data, and the metadata is extracted from this model rather than from the original data set.

Another way to classify the MIL methods is by the way they relate to traditional learning methods [11]. In this regard, there are three categories which we place in the rows of the table. First are those which are not related to traditional methods, they are purpose-built algorithms designed specifically to learn MIL concepts. The second category includes upgraded simple-instance learners that have been modified to learn directly from multi-instance data. Finally, there are wrapper algorithms that convert MIL problems into simple-instance problems, thereby allowing existing simple-instance learners to be applied directly.

In this work we present a novel instance-based wrapper that implements a variant of the threshold based assumption. Any simple-instance learner can be plugged into this wrapper. As a solution to the WIR problem we propose to combine the wrapper with the Rocchio classifier as this is a very efficient algorithm that performs well in textual domains.

In 2005, Zhou et al. [37] presented the web index recommendation (WIR) problem as a MIL problem. From the standpoint of the MIL approach, in the WIR problem a bag is a web index page that we will call index page for short. In turn, instances of a bag are the web pages linked to the index page. We will call them linked pages.

Zhou et al. collected and prepared data, described in more detail in Section 5.1, that have become a benchmark to test new methods of solution to WIR. The benchmark data consist only of textual information; hypermedia information such as audio, images and videos was discarded to simplify the analysis. Moreover, the data was preprocessed using feature selection, removing textual terms with little semantic content. In the benchmark data, trivial terms such as a, the, is, have been discarded.
                           1
                           The list of used stop words is available from: http://lamda.nju.edu.cn/data_MILWEB.ashx.
                        
                        
                           1
                        
                     

To solve the WIR problem, Zhou et al. [37] proposed the MIL algorithm Fretcit-kNN. Subsequently, new MIL algorithms from the genetic programming family were introduced in [32,33] to solve it. The following subsections describe these MIL approaches, along with the way in which each of them represents the WIR data.

Fretcit-kNN is based on the Citation-kNN 
                        [27] algorithm, which in turn is an adaptation of the popular kNN to the MIL approach. Instead of the traditional Euclidean metric, Citation-kNN uses a variant of the Hausdorff metric [9] to measure distance between bags. In order to increase robustness with respect to noise, Wang and Zucker [27] defined the minimal Hausdorff distance between bags 
                           
                              b
                              =
                              
                                 
                                    
                                       
                                          
                                             x
                                          
                                          
                                             1
                                          
                                       
                                       ,
                                       …
                                       ,
                                       
                                          
                                             x
                                          
                                          
                                             
                                                
                                                   n
                                                
                                                
                                                   b
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                         and 
                           
                              
                                 
                                    b
                                 
                                 
                                    ′
                                 
                              
                              =
                              
                                 
                                    
                                       
                                          
                                             y
                                          
                                          
                                             1
                                          
                                       
                                       ,
                                       …
                                       ,
                                       
                                          
                                             y
                                          
                                          
                                             
                                                
                                                   n
                                                
                                                
                                                   
                                                      
                                                         b
                                                      
                                                      
                                                         ′
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                         as
                           
                              (1)
                              
                                 
                                    
                                       H
                                    
                                    
                                       min
                                    
                                 
                                 
                                    
                                       
                                          b
                                          ,
                                          
                                             
                                                b
                                             
                                             
                                                ′
                                             
                                          
                                       
                                    
                                 
                                 =
                                 
                                    
                                       
                                          min
                                       
                                       
                                          x
                                          ∈
                                          b
                                          ,
                                          y
                                          ∈
                                          
                                             
                                                b
                                             
                                             
                                                ′
                                             
                                          
                                       
                                    
                                 
                                 
                                 
                                    
                                       
                                          x
                                          -
                                          y
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    
                                       x
                                       -
                                       y
                                    
                                 
                              
                           
                         is the distance under some norm between points x and y (usually Euclidean distance). To label a new bag, Citation-kNN takes into account the R-nearest references and the C-nearest citers to the bag. The reference concept is equivalent to the neighbor concept in classic kNN. Meanwhile, an example (a bag) b is a C-nearest citer of 
                           
                              
                                 
                                    b
                                 
                                 
                                    ′
                                 
                              
                           
                        , if 
                           
                              
                                 
                                    b
                                 
                                 
                                    ′
                                 
                              
                           
                         is among the C nearest neighbors of b.

Just like classic kNN, Citation-kNN uses a vectorial representation of the instance space. Zhou et al. [37] adapted Citation-kNN to work with a different type of representation used in the data sets built for the WIR problem. They represent each instance (i.e., each linked page) as a term set 
                           
                              T
                              =
                              
                                 
                                    
                                       
                                          
                                             t
                                          
                                          
                                             1
                                          
                                       
                                       ,
                                       
                                          
                                             t
                                          
                                          
                                             2
                                          
                                       
                                       ,
                                       …
                                       ,
                                       
                                          
                                             t
                                          
                                          
                                             n
                                          
                                       
                                    
                                 
                              
                           
                        , where 
                           
                              
                                 
                                    t
                                 
                                 
                                    i
                                 
                              
                           
                         (
                           
                              i
                              =
                              1
                              ,
                              …
                              ,
                              n
                           
                        ) is one of the n most frequent terms appearing in the corresponding linked page, and they replace the usual Euclidean distance with a new distance measure that takes into account the number of shared terms between instances.
                           2
                           In this paper textual terms always refer to single words.
                        
                        
                           2
                         The distance between two instances x and y is defined as:
                           
                              
                                 fret
                                 -
                                 distance
                                 
                                 (
                                 x
                                 ,
                                 y
                                 )
                                 =
                                 1
                                 -
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          
                                             
                                                
                                                   i
                                                   ,
                                                   j
                                                   =
                                                   1
                                                
                                                
                                                   
                                                      
                                                         x
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   =
                                                   
                                                      
                                                         y
                                                      
                                                      
                                                         j
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          n
                                       
                                    
                                 
                                 
                                    
                                       1
                                    
                                    
                                       n
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                           
                         represents the i-th term of instance x. Fretcit-kNN then refers to Citation-kNN where 
                           
                              
                                 
                                    
                                       x
                                       -
                                       y
                                    
                                 
                              
                              =
                              fret
                              -
                              distance
                              
                              
                                 
                                    
                                       x
                                       ,
                                       y
                                    
                                 
                              
                           
                        .

As Fretcit-kNN is a minor variation of Citation-kNN, it maintains the same computational complexity. As members of the lazy learning family, they have the drawback of deferring the main computational burden to the generalization stage. While its training time is 
                           
                              O
                              (
                              N
                              )
                           
                        , where N is the number of training examples (bags), the generalization time is 
                           
                              O
                              
                                 
                                    
                                       
                                          
                                             N
                                          
                                          
                                             2
                                          
                                       
                                       log
                                       N
                                    
                                 
                              
                           
                         because they need to perform a sort operation (
                           
                              O
                              
                                 
                                    
                                       N
                                       log
                                       N
                                    
                                 
                              
                           
                        ) for every training example in order to compute the C-nearest citer of the example to be classified.

Zhou et al. [37] report results of Fretcit-kNN on the benchmark data with term sets of size 5, 8, 10, 12 and 15. Fretcit-kNN was compared to a simplified version of the Rocchio classifier they call TFIDF [16], as well as with a classic kNN implementation and an updated kNN approach that considers both the references and the citers of an unseen object in prediction, like Citation-kNN but with Euclidean distance. These three algorithms were directly applied to the index pages while ignoring their linked pages. However, since most information of the index pages is delivered by their linked pages, their corresponding performances were very poor [37]. The classic and the updated kNN were therefore modified in order to consider the linked page information, by considering that all the instances in a bag have the label of that bag, and replacing Euclidean distance with 
                           
                              fret
                              -
                              distance
                           
                         so as to enable the application to textual frequent terms. Fretcit-kNN was superior in all of these comparisons [37].

Zafra et al. [33] introduced a grammar-guided genetic programming approach to solve the WIR problem. By contrast to Zhou et al.’s approach, they use a vector whose components correspond to the terms in the document universe. By a document, they refer to the set of pages (instances) linked to an index page. They also apply a further feature selection technique, eliminating those terms appearing in less than 3 documents or more than 40.

Within their approach, they proposed two algorithms, boolean G3P-MI and frequency G3P-MI, whose differences are in the form used to represent instances. In boolean G3P-MI, each component of the vector is a boolean and indicates the presence of a term in the document, while in frequency G3P-MI, each component of the vector represents the absolute frequency of a term in the document.

Both methods use a context free grammar to evolve an evolutionary program in which individuals are rules that codify the conditions under which a bag is positive. The methods adopt the standard MIL assumption, according to which (considering that class labels 1 and 0 are equivalent to the logical values true and false) the label of a bag is obtained by the disjunction of the labels of its instances. In boolean G3P-MI the generated rules refer to the presence or absence of a term in a given page, while in frequency G3P-MI they indicate whether a term appears in a given page more or less times than a predefined threshold. The fitness function used in both cases tries to maximize the accuracy, seeking a balance between precision and recall by the formula 
                           
                              fitness
                              =
                              accuracy
                              ×
                              recall
                              ×
                              precision
                           
                        .

However, the method was not able to obtain a good trade-off between recall and precision. Therefore, Zafra et al. [32] updated the former method and proposed MOG3P-MI, a multi-objective grammar-guided genetic programming method, based on SPEA2 [40], able to simultaneously optimize more than one performance measure, namely, sensitivity and specificity. Sensitivity (resp., specificity) measures the proportion of actual positives (resp., negatives) which are correctly classified. The experiments showed that MOG3P-MI indeed improves the classification performance [32].

Like SPEA2 [40], the complexity of MOG3P-MI in its training stage is on average 
                           
                              O
                              
                                 
                                    
                                       
                                          
                                             N
                                          
                                          
                                             2
                                          
                                       
                                       log
                                       N
                                    
                                 
                              
                           
                        , with N equal to number of index pages. On the other hand, MOG3P-MI represents hypotheses as rules, so the generalization time is 
                           
                              O
                              
                                 
                                    
                                       1
                                    
                                 
                              
                           
                        .

To solve the WIR problem, we present a new MIL classification method based on plugging a Rocchio classifier into a specific kind of MIL wrapper. The Rocchio classifier has been widely used in the field of information retrieval for its reasonable accuracy and low computational cost. The wrapper transforms the original MIL data into simple-instance data in a process called propositionalization, so that the Rocchio classifier can be applied. The integration between the Rocchio classifier and the proposed wrapper provides an efficient learning method well suited to the WIR problem. For convenience, from now on we will refer to this proposal as MI-Rocchio.

Section 4.1 describes the way in which we represent the WIR data, while Section 4.2 explains the simple-instance Rocchio classifier we use. The design of the new wrapper we introduce is presented in Section 4.3, and an evaluation of its run-time efficiency appears in Section 4.4.

Like Zafra et al.’s frequency GP3MI and MOG3P-MI, [32,33], we use a classical vector representation whose components are term frequencies, and apply the same feature selection preprocessing step. However, while their methods use absolute frequencies in the feature values, we calculate a weighted and normalized value. In particular, we use tfidf feature weighting (Term Frequency/Inverse Document Frequency) [25] by the formula
                           
                              (2)
                              
                                 tfidf
                                 
                                 
                                    
                                       
                                          t
                                          ,
                                          x
                                       
                                    
                                 
                                 =
                                 tf
                                 
                                 
                                    
                                       
                                          t
                                          ,
                                          x
                                       
                                    
                                 
                                 log
                                 
                                    
                                       
                                          
                                             
                                                P
                                             
                                             
                                                df
                                                
                                                   
                                                      
                                                         t
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              tf
                              
                              (
                              t
                              ,
                              x
                              )
                           
                         is the number of times term t occurs in linked page 
                           
                              x
                              ,
                              P
                           
                         is the total number of linked pages in the training set, and 
                           
                              df
                              (
                              t
                              )
                           
                         denotes the number of linked pages in the training set in which term t occurs. We applied cosine normalization using the formula:
                           
                              (3)
                              
                                 
                                    
                                       w
                                    
                                    
                                       xk
                                    
                                 
                                 =
                                 tfidf
                                 
                                 
                                    
                                       
                                          
                                             
                                                t
                                             
                                             
                                                k
                                             
                                          
                                          ,
                                          x
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      i
                                                      =
                                                      1
                                                   
                                                   
                                                      d
                                                   
                                                
                                                
                                                   
                                                      
                                                         
                                                            tfidf
                                                            
                                                            
                                                               
                                                                  
                                                                     
                                                                        
                                                                           t
                                                                        
                                                                        
                                                                           i
                                                                        
                                                                     
                                                                     ,
                                                                     x
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                   
                                                   
                                                      2
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    w
                                 
                                 
                                    xk
                                 
                              
                           
                         is the normalization weight of term 
                           
                              
                                 
                                    t
                                 
                                 
                                    k
                                 
                              
                           
                         in linked page x, and d is the dimension of the attribute vector, i.e., the total number of remaining terms after the feature selection process.

The Rocchio algorithm is a two-class classifier which belongs to the family of profile-based classifiers. In profile-based classifiers, the classification function f is based on the similarity between the instance to be classified and the synthetic representation of each class, i.e., the class prototype or class profile. Instance information from both classes is aggregated to build each profile. Then, a new instance is assigned to the class whose profile is more similar.

In the WIR problem, we have the class label set 
                           
                              L
                              =
                              
                                 
                                    
                                       +
                                       ,
                                       -
                                    
                                 
                              
                           
                        , where 
                           
                              +
                           
                         means a relevant index page, and 
                           
                              -
                           
                         means an irrelevant index page. Following [20], the weights of attribute i in the positive profile 
                           
                              
                                 
                                    
                                       
                                          W
                                       
                                       
                                          +
                                       
                                    
                                 
                                 
                                    →
                                 
                              
                           
                         and the negative profile 
                           
                              
                                 
                                    
                                       
                                          W
                                       
                                       
                                          -
                                       
                                    
                                 
                                 
                                    →
                                 
                              
                           
                         are calculated by the Rocchio formula as
                           
                              (4)
                              
                                 
                                    
                                       W
                                    
                                    
                                       i
                                    
                                    
                                       +
                                    
                                 
                                 =
                                 
                                    max
                                 
                                 
                                    
                                       
                                          0
                                          ,
                                          
                                             
                                                1
                                             
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               R
                                                            
                                                            
                                                               +
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   x
                                                   ∈
                                                   
                                                      
                                                         R
                                                      
                                                      
                                                         +
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             
                                                w
                                             
                                             
                                                xi
                                             
                                          
                                          -
                                          
                                             
                                                
                                                   
                                                      ρ
                                                   
                                                   
                                                      +
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               R
                                                            
                                                            
                                                               -
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   x
                                                   ∈
                                                   
                                                      
                                                         R
                                                      
                                                      
                                                         -
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             
                                                w
                                             
                                             
                                                xi
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (5)
                              
                                 
                                    
                                       W
                                    
                                    
                                       i
                                    
                                    
                                       -
                                    
                                 
                                 =
                                 
                                    max
                                 
                                 
                                    
                                       
                                          0
                                          ,
                                          
                                             
                                                1
                                             
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               R
                                                            
                                                            
                                                               -
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   x
                                                   ∈
                                                   
                                                      
                                                         R
                                                      
                                                      
                                                         -
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             
                                                w
                                             
                                             
                                                xi
                                             
                                          
                                          -
                                          
                                             
                                                
                                                   
                                                      ρ
                                                   
                                                   
                                                      -
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               R
                                                            
                                                            
                                                               +
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   x
                                                   ∈
                                                   
                                                      
                                                         R
                                                      
                                                      
                                                         +
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             
                                                w
                                             
                                             
                                                xi
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    w
                                 
                                 
                                    xi
                                 
                              
                           
                         represents the weight of feature i in instance 
                           
                              x
                              ,
                              
                                 
                                    R
                                 
                                 
                                    +
                                 
                              
                           
                         is the set of positive instances, and 
                           
                              
                                 
                                    R
                                 
                                 
                                    -
                                 
                              
                           
                         is the set of negative instances. The Rocchio algorithm implicitly applies feature selection, and parameters 
                           
                              
                                 
                                    ρ
                                 
                                 
                                    +
                                 
                              
                           
                         and 
                           
                              
                                 
                                    ρ
                                 
                                 
                                    -
                                 
                              
                           
                         control the extent of this selection [20]. The higher the parameter value, the more intense the feature selection.

From the class profiles 
                           
                              
                                 
                                    W
                                 
                                 
                                    c
                                 
                              
                              ,
                              c
                              ∈
                              L
                           
                        , the class label of a new example b is given by the following expression:
                           
                              (6)
                              
                                 f
                                 (
                                 b
                                 )
                                 =
                                 
                                    
                                       
                                          arg
                                          
                                          max
                                       
                                       
                                          c
                                          ∈
                                          L
                                       
                                    
                                 
                                 
                                    
                                       S
                                    
                                    
                                       cos
                                    
                                 
                                 
                                    
                                       
                                          b
                                          ,
                                          
                                             
                                                W
                                             
                                             
                                                c
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    S
                                 
                                 
                                    cos
                                 
                              
                              
                                 
                                    
                                       x
                                       ,
                                       y
                                    
                                 
                              
                           
                         represents the cosine similarity between vectors x and y, and is defined as
                           
                              (7)
                              
                                 
                                    
                                       S
                                    
                                    
                                       cos
                                    
                                 
                                 
                                    
                                       
                                          x
                                          ,
                                          y
                                       
                                    
                                 
                                 =
                                 
                                    
                                       xy
                                    
                                    
                                       
                                          
                                             
                                                x
                                             
                                          
                                       
                                       
                                          
                                             
                                                y
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    
                                       x
                                    
                                 
                              
                           
                         is the vectorial norm of x.

The function of the wrapper is to interface between the MIL problem and the simple-instance classifier. Instance-based wrappers, like the one we propose, heuristically impute class labels to instances during the propositionalization step of the training stage. In the classification stage, the wrapper computes the class label of a new bag once its instances have been classified by the simple-instance classifier. Before we proceed to explain the details of the imputation scheme and the classification procedure, we first describe the MIL assumption that our wrapper implements.

We introduce a new MIL assumption which is a variation of the threshold-based assumption [28] described in Section 2.2. We call it proportion-based assumption because it requires a minimum proportion (relative to the size of the bag) of instances of each concept. In general, let 
                              
                                 C
                                 ⊂
                                 C
                              
                            be the set of underlying concepts used, and 
                              
                                 Δ
                                 :
                                 
                                    
                                       N
                                    
                                    
                                       X
                                    
                                 
                                 ×
                                 C
                                 →
                                 N
                              
                            a counting function which counts the members of a given concept in a bag. A mapping function 
                              
                                 
                                    
                                       f
                                    
                                    
                                       P
                                    
                                 
                              
                            under the proportion-based assumptionis defined as 
                              
                                 
                                    
                                       f
                                    
                                    
                                       P
                                    
                                 
                                 (
                                 b
                                 )
                                 
                                 ⇔
                                 
                                 ∀
                                 
                                    
                                       c
                                    
                                    
                                       i
                                    
                                 
                                 ∈
                                 C
                                 :
                                 
                                    
                                       Δ
                                       
                                          
                                             
                                                b
                                                ,
                                                
                                                   
                                                      c
                                                   
                                                   
                                                      i
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             n
                                          
                                          
                                             b
                                          
                                       
                                    
                                 
                                 ⩾
                                 
                                    
                                       t
                                    
                                    
                                       i
                                    
                                 
                              
                           , where 
                              
                                 
                                    
                                       t
                                    
                                    
                                       i
                                    
                                 
                              
                            is the lower threshold of the proportion of concept 
                              
                                 
                                    
                                       c
                                    
                                    
                                       i
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       n
                                    
                                    
                                       b
                                    
                                 
                              
                            is the number of instances in bag b.

In the WIR problem, this assumption models the behavior of users who consider index pages relevant provided they have a certain minimum proportion of relevant links. In this case, we are only interested in the concept of relevant linked page, i.e., the positive class. Therefore, 
                              
                                 C
                                 =
                                 
                                    
                                       
                                          +
                                       
                                    
                                 
                              
                            and the mapping function can be simplified as
                              
                                 (8)
                                 
                                    
                                       
                                          f
                                       
                                       
                                          P
                                       
                                    
                                    (
                                    b
                                    )
                                    
                                    ⇔
                                    
                                    g
                                    (
                                    b
                                    )
                                    ⩾
                                    t
                                 
                              
                           where 
                              
                                 g
                                 (
                                 b
                                 )
                                 =
                                 
                                    
                                       Δ
                                       
                                          
                                             
                                                b
                                                ,
                                                +
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             n
                                          
                                          
                                             b
                                          
                                       
                                    
                                 
                              
                           .

The first step in the training stage is the propositionalization process. Since we have no a priori information about the class labels of instances in training bags, we rely on the standard MIL hypothesis “by default” to obtain an initial imputation: an instance is assigned to the negative class if it appears in any negative bag, otherwise it is assigned to the positive class.

Instances with imputed class labels are then used to feed the Rocchio formula in Eqs. (4) and (5) in order to compute the positive profile 
                              
                                 
                                    
                                       
                                          
                                             W
                                          
                                          
                                             +
                                          
                                       
                                    
                                    
                                       →
                                    
                                 
                              
                            and the negative profile 
                              
                                 
                                    
                                       
                                          
                                             W
                                          
                                          
                                             -
                                          
                                       
                                    
                                    
                                       →
                                    
                                 
                              
                           . Next, the proportion threshold t is calculated based on the values of 
                              
                                 g
                                 (
                                 b
                                 )
                              
                            for training bags b. In the computation of 
                              
                                 g
                                 (
                                 b
                                 )
                              
                           , a second imputation process is implicit. An instance x will be considered positive if its cosine similarity with the positive profile is greater than the cosine similarity with the negative profile. In the opposite case, it will be considered a negative instance. The calculation of the proportion threshold is described in the next subsection.

In the classification stage, Eq. (8) is used to determine the class label of a new bag. The bag b will be assigned to the positive class if 
                              
                                 g
                                 (
                                 b
                                 )
                                 ⩾
                                 t
                              
                           , otherwise it will be assigned to the negative class; 
                              
                                 g
                                 (
                                 b
                                 )
                              
                            is calculated in the same way as in the training stage.

The outline of the algorithm is shown graphically in Fig. 1
                           . In summary, the proportion-basedwrapper first performs the process of instance label imputation, then constructs the class profiles by the Rocchio formula and finally determines the threshold for positive bags, using a second imputation. The class profiles together with the threshold constitute the learning model.

Let 
                              
                                 
                                    
                                       μ
                                    
                                    
                                       p
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       μ
                                    
                                    
                                       n
                                    
                                 
                              
                            be the mean of 
                              
                                 g
                                 (
                                 b
                                 )
                              
                            over all the positive, respectively negative bags in the training set. Also, let 
                              
                                 
                                    
                                       V
                                    
                                    
                                       p
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       V
                                    
                                    
                                       n
                                    
                                 
                              
                            be the variance of 
                              
                                 g
                                 (
                                 b
                                 )
                              
                            over all the positive, respectively negative bags in the training set. We want the threshold t to adequately separate positive and negative bags. Given that positive and negative bags have different degrees of dispersion we want to find a middle ground between 
                              
                                 
                                    
                                       μ
                                    
                                    
                                       p
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       μ
                                    
                                    
                                       n
                                    
                                 
                              
                            to take into account the sample variance of each class. Therefore, one can use the average of the means weighted by the variances:
                              
                                 (9)
                                 
                                    
                                       
                                          t
                                       
                                       
                                          l
                                       
                                    
                                    =
                                    
                                       
                                          
                                             
                                                μ
                                             
                                             
                                                p
                                             
                                          
                                          
                                             
                                                V
                                             
                                             
                                                n
                                             
                                          
                                          +
                                          
                                             
                                                μ
                                             
                                             
                                                n
                                             
                                          
                                          
                                             
                                                V
                                             
                                             
                                                p
                                             
                                          
                                       
                                       
                                          
                                             
                                                V
                                             
                                             
                                                p
                                             
                                          
                                          +
                                          
                                             
                                                V
                                             
                                             
                                                n
                                             
                                          
                                       
                                    
                                 
                              
                           which we call the linear weighting model. This equation calculates a threshold 
                              
                                 
                                    
                                       t
                                    
                                    
                                       l
                                    
                                 
                              
                            situated between 
                              
                                 
                                    
                                       μ
                                    
                                    
                                       p
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       μ
                                    
                                    
                                       n
                                    
                                 
                              
                           , which is located closest to either mean with the lowest variance. However, in some cases this threshold is too close to the sample mean as the population variance is greater than the sample variance. Specifically, when 
                              
                                 
                                    
                                       μ
                                    
                                    
                                       n
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       V
                                    
                                    
                                       n
                                    
                                 
                              
                            approach zero then t gets too close to zero and the classifier will not generalize well.

To alleviate this problem we calculate the average of the means weighted by the variances’ exponentials, in what we call the exponential weighting model:
                              
                                 (10)
                                 
                                    
                                       
                                          t
                                       
                                       
                                          e
                                       
                                    
                                    =
                                    
                                       
                                          
                                             
                                                μ
                                             
                                             
                                                p
                                             
                                          
                                          
                                             
                                                e
                                             
                                             
                                                -
                                                η
                                                
                                                   
                                                      V
                                                   
                                                   
                                                      p
                                                   
                                                
                                             
                                          
                                          +
                                          
                                             
                                                μ
                                             
                                             
                                                n
                                             
                                          
                                          
                                             
                                                e
                                             
                                             
                                                -
                                                η
                                                
                                                   
                                                      V
                                                   
                                                   
                                                      n
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                e
                                             
                                             
                                                -
                                                η
                                                
                                                   
                                                      V
                                                   
                                                   
                                                      p
                                                   
                                                
                                             
                                          
                                          +
                                          
                                             
                                                e
                                             
                                             
                                                -
                                                η
                                                
                                                   
                                                      V
                                                   
                                                   
                                                      n
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where 
                              
                                 η
                              
                            is a parameter that controls the slope of the curve. Fig. 2
                            shows how the threshold varies with each weighting model, the linear and the exponential, as the variance 
                              
                                 
                                    
                                       V
                                    
                                    
                                       n
                                    
                                 
                              
                            moves from 0.1 to zero. Unlike the linear model, the exponential is a convex function, which slows the decrease of the threshold as the variance approaches zero.

Our intention is not only to obtain a competitive algorithm in terms of classification accuracy, but also in terms of efficiency, as required for this type of online applications. For this reason, in this section we analyze the run-time complexity of our proposal and compare it to that of the algorithms Fretcit-kNN and MOG3P-MI.

As noted above, the simple-instance Rocchio classifier is very efficient; its algorithmic complexity in training is 
                           
                              O
                              (
                              N
                              )
                           
                        , where N is the total number of training bags. Therefore, the efficiency of the MI-Rocchio algorithm is determined by the complexity of the initial imputation method which is computationally more expensive because for each instance we need to check if it is in another bag, and the complexity of this step is 
                           
                              O
                              
                                 
                                    
                                       
                                          
                                             N
                                          
                                          
                                             2
                                          
                                       
                                    
                                 
                              
                           
                        .

During classification, instances of the new bag just have to be compared with the class profiles, so the computational complexity of this part is 
                           
                              O
                              
                                 
                                    
                                       1
                                    
                                 
                              
                           
                        . Thus, the complexity analysis support the claim that MIRocchio is more efficient than the compared algorithms Fretcit-kNN and MOG3P-MI, which both have complexity 
                           
                              O
                              
                                 
                                    
                                       
                                          
                                             N
                                          
                                          
                                             2
                                          
                                       
                                       log
                                       (
                                       N
                                       )
                                    
                                 
                              
                           
                        .

In this section, we evaluate the performance of our approach and compare it with the state-of-the-art solutions to the WIR problem. Our experiments are aimed at three objectives:
                        
                           •
                           To compare the linear and the exponential weighting model introduced in Section 4.3.3 to establish the proportion threshold of MI-Rocchio, in order to determine the cases for which each of them are best suited, and to find the overall best variant.

To verify the benefit of MI-Rocchio’s use of the proportion-based MIL assumption introduced in Section 4.3.1 over the standard MIL assumption.

To compare the MI-Rocchio variants with the state of the art algorithms proposed for the WIR problem, notably Fretcit-kNN and MOG3P-MI (see Section 3), in order to determine the relevance of our proposal.

For this purpose, we first discuss the data sets, performance measures and algorithm parameters used in the experimental study.

For comparison purposes, we use the benchmark data.
                           3
                           The data sets are available at http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/annex/milweb-datafile.htm.
                        
                        
                           3
                         introduced by Zhou et al. in [37]. This data consists of nine sets, as a result of 113 web index pages that were labeled by nine volunteers according to their interests. The label of the bag is positive if the web index page interested the user. Otherwise the label is negative. For each data set, 75 web index pages are used as training bags and the remaining 38 as test bags. These partitions were proposed by [37] and subsequently used by [31,32]. The number of positive and negative bags in the data sets is shown in Table 2
                        .

Note that the balance between positive and negative classes varies considerably among the data sets; indeed, the appearance of unbalanced classes is a very common characteristic in recommendation problems. In particular, the first three volunteers have marked positive only about a third of the bags, the three following volunteers have marked positive about two-thirds of the bags while the last three volunteers have marked approximately equal numbers of positive and negative bags.

We evaluate the quality of the classification using a total of five performance measures: accuracy, precision, recall/sensitivity, specificity and Area Under the ROC Curve (AUC). The first three measures were proposed by [37] to analyze the behavior of the algorithm Fretcit-kNN, while the first, third and fourth were used by [32] to measure the performance of MOG3P-MI. We have added the AUC measure here to take into account the imbalance characteristic that occurs in 6 out of 9 data sets; indeed, a trivial classifier that assigns all instances to the majority class is able to reach high accuracy, but is not useful in practice. For instance, if all the examples of the test set corresponding to V2 were classified negative, 
                           
                              accuracy
                              =
                              35
                              /
                              38
                              =
                              0.921
                           
                         which looks good. However, the classifier would have missed the three positive instances, so it would be useless to make recommendations. By contrast, AUC reflects the trade-off between correctly classified instances in the minority class and a high classification accuracy of instances in the majority class.

The definitions of these measures are based on the different outcomes of a two-class classification problem; a single prediction has the four different possible outcomes shown in Table 3
                        . The measures themselves are defined according to Eqs. (11)–(15).
                           
                              (11)
                              
                                 Accuracy
                                 =
                                 
                                    
                                       TP
                                       +
                                       TN
                                    
                                    
                                       TP
                                       +
                                       FP
                                       +
                                       TN
                                       +
                                       FN
                                    
                                 
                              
                           
                        
                        
                           
                              (12)
                              
                                 Precision
                                 =
                                 
                                    
                                       TP
                                    
                                    
                                       TP
                                       +
                                       FP
                                    
                                 
                              
                           
                        
                        
                           
                              (13)
                              
                                 Recall
                                 =
                                 
                                    
                                       TP
                                    
                                    
                                       TP
                                       +
                                       FN
                                    
                                 
                              
                           
                        
                        
                           
                              (14)
                              
                                 Specificity
                                 =
                                 
                                    
                                       TN
                                    
                                    
                                       TN
                                       +
                                       FP
                                    
                                 
                              
                           
                        
                        
                           
                              (15)
                              
                                 AUC
                                 =
                                 
                                    
                                       1
                                       +
                                       
                                          
                                             TP
                                          
                                          
                                             TP
                                             +
                                             FN
                                          
                                       
                                       -
                                       
                                          
                                             FP
                                          
                                          
                                             FP
                                             +
                                             TN
                                          
                                       
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        
                     

The execution of MI-Rocchio requires setting the parameters 
                           
                              
                                 
                                    ρ
                                 
                                 
                                    +
                                 
                              
                           
                         and 
                           
                              
                                 
                                    ρ
                                 
                                 
                                    -
                                 
                              
                           
                         in Eqs. (4) and (5) of the simple-instance Rocchio classifier. In our implementation, the values of these parameters are learned in the training stage; in particular, a total of 30 different configurations of 
                           
                              
                                 
                                    ρ
                                 
                                 
                                    +
                                 
                              
                           
                         (
                           
                              
                                 
                                    ρ
                                 
                                 
                                    +
                                 
                              
                              =
                              15
                              ,
                              20
                              ,
                              25
                              ,
                              30
                              ,
                              40
                           
                        ) and 
                           
                              
                                 
                                    ρ
                                 
                                 
                                    -
                                 
                              
                           
                         (
                           
                              
                                 
                                    ρ
                                 
                                 
                                    -
                                 
                              
                              =
                              2
                              ,
                              3
                              ,
                              4
                              ,
                              8
                              ,
                              15
                              ,
                              20
                           
                        ) are compared, using training accuracy to guide the search. As a general rule, the value of 
                           
                              
                                 
                                    ρ
                                 
                                 
                                    +
                                 
                              
                           
                         should be higher than that of 
                           
                              
                                 
                                    ρ
                                 
                                 
                                    -
                                 
                              
                           
                        , for the following reason. According to the initial imputation of the proportion-based assumption, all instances in a negative bag are considered negative while only a certain percentage of the instances in a positive bag are considered positive, and the remainder negative; that is, positive bags involve more uncertainty than negative ones. Therefore, a more intense feature selection (greater 
                           
                              
                                 
                                    ρ
                                 
                                 
                                    +
                                 
                              
                           
                        ) in the positive instances benefits the classification accuracy.

On the other hand, we compare the linear weighting model for selecting the proportion threshold with the exponential one. The respective implementations are called MI-Rocchio-L and MI-Rocchio-E. The parameter 
                           
                              η
                           
                         for 
                           
                              
                                 
                                    t
                                 
                                 
                                    e
                                 
                              
                           
                         in Eq. (10) is set to 10.

As mentioned before, we also want to investigate the effect of using the proportion-based MIL assumption. To this aim, we compare MI-Rocchio-E and Mi-Rocchio-L with SAMI-Rocchio, a variant that uses the standard MIL assumption instead of the proportion-based assumption. Instead of basing the classification of a new bag on the proportion of positive instances in the bag (as MI-Rocchio does), SAMI-Rocchio assigns the bag to the positive class if at least one positive instance is present in the test bag. Therefore, there is no proportion threshold computation in the training stage, nor is a second imputation needed, and the learning model comprises just the class profiles.

We compare MI-Rocchio-E, Mi-Rocchio-L and SAMI-Rocchio with those algorithms specifically developed and applied to the WIR problem: Fretcit-kNN and MOG3P-MI. We do not compare with G3P-MI because from [32], it is clear that its results are lower than those of MOG3P-MI. These classifiers were described in Section 3. We use the results reported by their authors for the best configuration of each algorithm: 15 frequent terms to describe instances in Fretcit-kNN [37] and boolean features in MOG3P-MI [32].


                        Table 4
                         shows the accuracy, precision, recall/sensitivity, specificity and AUC of MI-Rocchio-L/MI-Rocchio-E and their rival classifiers over the nine test data sets V1–V9. The last column displays the performance measure average of each classifier. The best method is highlighted in bold for each data set.

Note that as there are only nine data sets, and ties frequently occur in the values of performance measures, it is not useful to apply statistical significance tests to these results. For the same reason, nor are nonparametric tests recommended by studies like [10,18] helpful. As an alternative way to compare the performance of classifiers, we may count the number of data sets for which an algorithm is the winner, which has been done in Tables 5–7
                        
                        
                        .

Comparing first the two proposed implementations of MI-Rocchio, it is clear that the version using an exponentially weighted proportion threshold performs much better than the one that assumes the linear weighting model. As stated also in Section 4.3.3, this has to do with the fact that when 
                              
                                 
                                    
                                       μ
                                    
                                    
                                       n
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       V
                                    
                                    
                                       n
                                    
                                 
                              
                            get too close to zero, the threshold 
                              
                                 
                                    
                                       t
                                    
                                    
                                       l
                                    
                                 
                              
                            also becomes very small. This is especially so in the case of V4-V6, for which 
                              
                                 
                                    
                                       V
                                    
                                    
                                       p
                                    
                                 
                              
                            is very high and the proportion threshold will often be 0, not allowing to separate well between positive and negative bags.

However, for data sets V1–V3, in which the majority of examples are negative, the linear threshold performs quite well, and for V1 and V3 it even achieves the highest AUC value. This behavior can be explained in the following way. The preponderance of negative examples in data sets V1–V3 causes that a large number of instances in positive bags will be more similar to the negative profile than to the positive one, producing lower 
                              
                                 g
                                 (
                                 b
                                 )
                              
                            values for positive bags. Hence, data sets V1–V3 are best fitted with lower thresholds such as those produced by the linear weighting model. Balanced data sets V7–V9, and data sets V4–V6, where the majority examples are positive, produce higher 
                              
                                 g
                                 (
                                 b
                                 )
                              
                            values for positive bags, so they are best fitted with higher thresholds like that of the exponentially weighted model. This suggests that a hybrid model can be used for the selection of the proportion threshold, using the linear threshold when the majority examples are negative and the exponential threshold in the other case.

When we compare MI-Rocchio-E with SAMI-Rocchio, it is evident that the first algorithm, which uses the proportion-based assumption, performs much better than the variant relying on the standard assumption. Table 5 shows the winners between MI-Rocchio-E and SAMI- Rocchio for each performance measure.

MI-Rocchio-E is better than SAMI-Rocchio with respect to accuracy, precision, specificity and AUC. Only for recall, SAMI-Rocchio obtains more wins than MI-Rocchio-E. This can be explained by the different MIL assumptions they use. The set of test bags in which the proportion of positive instance is above a given threshold is included in the set of test bags containing at least one positive instance. In other words, the standard assumption is more general than the proportion-based assumption. Hence, SAMI-Rocchio assigns an example to the positive class more often than MI-Rocchio-E do. However, the lower precision values of SAMI-Rocchio reveal that this approach results in more false positives. Conversely, MI-Rocchio-E generates fewer false positives and more false negatives, which explains its high precision, high specificity, and moderate recall.

In the context of WIR, the standard assumption models the kind of user who considers an index page relevant if it has at least one relevant link. But the results shown here support the model of users who consider an index page relevant if it has a certain minimum proportion of relevant links, as implemented by the proportion-based assumption.

When we compare MI-Rocchio-E with Fretcit-kNN and MOG3P-MI, it is clear that our method shows very good behavior, performing better, on average, for all the selected performance measures. Table 6 shows the winners between MI-Rocchio and Fretcit-kNN for each performance measure. In general, both algorithms have comparable performance, except for specificity for which MI-Rocchio-E obtains more wins, indicating that MI-Rocchio-E is better at avoiding false positives (irrelevant index pages recommended to the user) than Fretcit-kNN; and for AUC, where Fretcit-kNN is better in more cases, which appears to indicate that it is better at dealing with the data imbalance than MI-Rocchio-E. However, if only the data sets that are actually imbalanced (V1–V6) are taken into account, this difference is less pronounced, and if moreover for V1–V3 the linear proportion threshold is used, as was suggested in the previous paragraph, the advantage is actually for MI-Rocchio.


                           Table 7 shows the winners between MI-Rocchio-E and MOG3P-MI. In this case, the advantage of MI-Rocchio-E over MOG3P-MI regarding precision is very clear, meaning that the former makes much more accurate recommendations (more index pages recommended by MI-Rocchio-E are actually relevant). Similar observations hold for specificity and AUC. In terms of recall, MOG3P-MI and MI-Rocchio-E perform comparably, but the lower precision values of MOG3P-MI indicate that it has a certain predisposition to assign examples to the positive class, resulting in fewer false negatives but more false positives. This is also consistent with its modest specificity. MI-Rocchio-E, however, is more conservative for assigning an example to the positive class. Therefore it generates fewer false positives and more false negatives, which explains its high precision, high specificity, and moderate recall. This can be seen as an additional advantage of our method: in the WIR problem, due to the large number of web index pages on the Internet, the recommendation of an irrelevant page is more expensive than not to recommend a relevant page. In other words, the recommendation for a user of all potentially interesting pages (recall) is not as important as the recommendation of a really interesting page (precision).

In this paper, we have presented a new multi-instance learning algorithm to be applied to the web index recommendation problem. The algorithm is a wrapper that implements a new MIL assumption that we introduced with the name proportion-based assumption. This assumption assumes that a bag is positive if it contains a certain proportion of positive instances. The wrapper uses the simple-instance Rocchio algorithm as base classifier.

The proposed algorithm has low computational cost and the experimental study on benchmark data sets showed that it outperforms state-of-the-art algorithms applied to the problem, and that in particular its precision is high.

Since the Rocchio classifier is considered useful for text applications, the proposed algorithm may be advantageous in other applications such as text categorization. In addition, the new MIL assumption introduced and its implementation by the wrapper are general enough to be used with any base classifier to other application domains.

@&#ACKNOWLEDGMENT@&#

This work was partially supported by Projects TIN2011-28488 and P10-TIC-6858.

@&#REFERENCES@&#

