@&#MAIN-TITLE@&#Automatic compilation of language resources for named entity recognition in Turkish by utilizing Wikipedia article titles

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           An automated procedure to compile resources for named entity recognition from Wikipedia is proposed.


                        
                        
                           
                           The contribution of the language resources is demonstrated through experimentation.


                        
                        
                           
                           Evaluation results are discussed and analyses of significant errors are provided.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Information extraction

Named entity recognition

Language resources

k-nearest neighbor algorithm

Wikipedia

@&#ABSTRACT@&#


               
               
                  We present an automatic approach to compile language resources for named entity recognition (NER) in Turkish by utilizing Wikipedia article titles. First, a subset of the article titles is annotated with the basic named entity types. This subset is then utilized as training data to automatically classify the remaining titles by employing the k-nearest neighbor algorithm, leading to the construction of a significant lexical resource set for Turkish NER. Experiments on different text genres are conducted after extending an existing NER system with the resources and the results obtained confirm that the resources contribute to NER on different genres.
               
            

@&#INTRODUCTION@&#

Textual information extraction (IE) is an important natural language processing task which requires considerable attention due to the need to automatically extract semantic information from textual documents, such as Web news articles and user-generated content such as blogs, among others, that keep increasing in size daily. Named entity recognition (NER) is a well-defined IE subtask corresponding to the extraction of person, location, and organization names, along with some numeric and temporal expressions from free natural language texts [1–3].

The approaches to the NER task generally range from rule-based systems to machine learning statistical systems [2,4,5]. The rule-based systems do not need annotated corpora as they usually employ lexical resources and rule bases. However, they suffer from the so-called porting problem, i.e., they achieve low success rates when ported to other domains, particularly when tested on text genres different from the initial target domain of these systems [4]. This problem is not a consideration for the learning statistical systems since they can be trained on new domains provided that annotated training corpora are available for these domains [4]. Nevertheless, building annotated corpora manually is a highly demanding task and the learning statistical systems need an additional training phase on such annotated corpora for newer domains in order to be applicable to these domains as well. Decision trees [6], Bayesian learning [7], hidden Markov models (HMMs) [8], relational learning (such as inductive logic programming) [7], support vector machines (SVM) [9,10], and conditional random fields (CRF) [11] are among the most prominent learning statistical techniques used for IE tasks including NER.

Apart from the diverse set of approaches to the IE tasks, Wikipedia
                        1
                     
                     
                        1
                        
                           http://en.wikipedia.org/wiki/Main_Page.
                      is known to be an important information source and several text processing applications including IE can readily benefit from the Wikipedia article contents as well as Wikipedia's existing structure [12]. Considering particularly the NER task, there are several studies ranging from proposals for automatic generation of gazetteers or annotations from Wikipedia [13,14] to those that target at NER on Wikipedia itself [15].

The studies on NER from Turkish texts are rather insufficient compared to those for well-studied languages like English. Among the related work, [16] is one of the first studies in which a NER system is tested on Turkish texts in addition to four other languages including Romanian, English, Greek, and Hindi. In [17], an HMM based NER system targeting at the extraction of person, location, and organization names from Turkish texts is presented. A rule-based NER system employing sets of lexical resources and pattern bases is proposed in [18]. This latter system has been seamlessly integrated into video annotation and retrieval systems for Turkish news videos for semantic information extraction from the video texts [19,20]. A hybrid version of this rule-based NER system which automatically enriches its lexical resources with new entities from annotated texts is presented in [21]. NER approaches on Turkish texts based on the employment of CRFs with morphological features have been reported in [22–24]. An automatic rule learning system for the NER task on Turkish texts is presented in [25]. Finally, a Bayesian learning based approach and hybrid approaches combining this Bayesian approach with the rule-based approach in [18] are presented in [26].

In this study, we propose an automatic approach to compile a language resource set for the NER task from Turkish by utilizing all of the article titles in Turkish Wikipedia.
                        2
                     
                     
                        2
                        
                           http://tr.wikipedia.org/wiki/Ana_Sayfa.
                      In order to achieve this, first we have downloaded a dump of Turkish Wikipedia article titles and annotated a randomly selected subset of the dump corresponding to about one twentieth of all titles included. Utilizing this annotated subset as the training data set, k-nearest neighbor algorithm is employed on the remaining Wikipedia article titles to automatically classify these titles as person, location or organization names, as well. This procedure has lead to the automatic compilation of a significant named entity resource set to be utilized by any NER system for Turkish.

The contribution of the automatically compiled resource set is demonstrated by utilizing the set as an additional source of lexical resources of the rule-based NER system presented in [18]. To examine the contribution of these new language resources to the NER procedure on Turkish texts, first, the aforementioned training data set is added to the lexical resources of the existing rule-based NER system for Turkish [18] and the evaluation results of the enhanced system on diverse text types turn out to be promising.
                        3
                     
                     
                        3
                        This manual annotation of the training data set and the NER experimentation involving this training data set have been previously presented in [27].
                      Next, similar experimentation is carried out, this time extending the aforementioned NER system with the automatically learned resource set and its cleaned version where this cleaned version has been created to determine the precision rate of the learning procedure. The results of all of these experiments are quite promising and they serve to confirm that such automatically learned language resources can contribute considerably to the NER task in Turkish.

The presented approach and the ultimate compiled resource set are significant as the whole procedure is automatic, necessitating very limited manual intervention just to create the annotated training data set. The ultimate resource set stands as an important contribution to the research on the NER task on Turkish texts, as the lack of such resources and that of common annotated data sets are important problems against related research. Similar procedures can also be employed for other lesser studied languages on which text processing research and related language resources are rare.

Additionally, we provide the details of the manual named entity annotation scheme on the data sets used during the evaluation of the NER system. This scheme will help the standardization of the annotation of reference corpora in Turkish, thereby facilitating the comparison of different NER approaches.

The rest of the paper is organized as follows: An overview of the rule-based named entity recognizer [18] is provided in Section 2 as its resources are utilized during the resource compilation procedure and it is also employed to observe the effects of the compiled resources to the NER procedure. In Section 3, the stages of the automatic compilation of language resources for the NER task on Turkish texts are described in details. The NER experimentation results with the recognizer [18] utilizing these language resources (in addition to its initial resource sets) are presented together with discussion of these results in Section 4. Section 5 presents the annotation scheme employed when annotating the evaluation data sets in Turkish with named entities. Finally, Section 6 concludes the paper with a summary of the proposed procedure and compiled resources, along with pointers to future research directions.

The rule-based NER system [18] utilizes a set of lexical resources and pattern bases for the recognition of person, location, and organization names along with date time and money percentage expressions in Turkish texts. These resources are demonstrated in a taxonomic structure in Fig. 1
                      as excerpted from [18].

The lexical resources include a person name dictionary of about 8300 entries, a small list of well-known people of about 70 entries, a location list of about 5400 entries, and finally an organization list of 1000 entries. The pattern bases for locations and organizations include about 90 patterns each. Example location patterns are provided below where X is the immediately preceding named entity found using only the lexical resources if there is such an entity extracted, and the immediately preceding single token otherwise. The pattern base for organization names includes similar patterns using the common endings for organization names in Turkish [18].


                     X Sokak/Yolu/Kulesi/Stadyumu/Havaalanı …
                  

‘X Street/Road/Tower/Stadium/Airport …’

Provided below are some discussions about the peculiarities of the patterns/resources employed by the recognizer:
                        
                           1
                           Currently, the recognizer does not make use of syntactic information such as part-of-speech (POS) tags within its patterns. In order to have more generalized patterns, following similar rule-based approaches with decent performance which make use of POS information (like the work presented in [28]), the patterns of the recognizer can be improved extended similarly, as part of future work on the recognizer.

In order to avoid ambiguities during the NER task, the lexical resources and patterns of the recognizer had been revised several times. For instance, an entry which can both be a person name and a city name (like the token Aydın in Turkish), the entry is only listed in one of the related lexical lists, based on an assumed probability in news articles (hence Aydın is included only within the location list of recognizer). The different pattern bases of the recognizer are similarly designed not to have common patterns to avoid ambiguities. However, there are cases in which the sentence itself might be ambiguous, especially when the tokens are not properly capitalized, where the named entities involved change according to the reading and they can only be recognized with real-world knowledge or based on the other sentences in the discourse. As the recognizer does not resolve such ambiguities, it extracts the named entity with the largest span based on its resources and patterns, which may be the correct named entity or not. To illustrate, there are two readings of the sentence ‘Sabiha Gökçen havaalanına gitti’ when the word ‘havaalanı’ (meaning airport) is not capitalized properly and the named entity to be recognized differs according to the readings. These two readings and the corresponding correct named entity annotations for these readings are provided in Table 1
                              . The annotations are shown with the named entity tag of ENAMEX, as utilized in the Message Understanding Conference (MUC) series.
                                 4
                              
                              
                                 4
                                 
                                    http://www-nlpir.nist.gov/related_projects/muc/.
                               For this sentence, the recognizer extracts the location name (using the aforementioned X Havaalanı pattern), like the correct annotation for the second reading given in Table 1. Exploring plausible ways to deal with such ambiguities is also left as a line of future work.

In Turkish, the initial letters of the proper names are capitalized and any set of suffixes added to the end of these proper names is separated from the names with an apostrophe. Yet, these two rules are not applicable to the temporal and numeric expressions which are within the scope of the recognizer. Moreover, some of the free texts available on the Web may not conform to these rules and especially the former one is not applicable if the input text is all in uppercase (such as sliding texts over some news videos) or all in lowercase. Hence, the recognizer can be configured to execute considering the case information or without considering it. To address the issues related to the second rule, the recognizer comprises a morphological analyzer to check the suffixes added to the end of a candidate named entity for validity [18].

In this section, we describe the automatic procedure for the compilation of language resources from Wikipedia for the NER task in Turkish. As there is a high probability that the article titles in Wikipedia usually correspond to a named entity of type person, location, or organization names, within the course of this study we only consider article titles, leaving the utilization of complete article contents as a plausible future research direction to pursue.

At the beginning of the procedure, we have downloaded the Turkish Wikipedia dump of article titles dated 12 July 2012. This database dump contains about 388,500 titles after some data cleaning operations such as removing some empty and non alphanumeric titles. Next, these titles are divided into 20 subsets where each of the first 19 of them contains 20,000 titles and the last one contains about 18,500 titles. We have randomly selected one of the subsets with 20,000 titles and annotated this subset with person, location, and organization names with the ENAMEX named entity tag. This manual annotation process has taken about 11h for the author and led to the annotation of 7098 named entities of all 20,000 titles. It should be noted that during this procedure, annotation of previously annotated titles (duplicate titles) is avoided in addition to common titles such as continent, country, and continent names, as much as possible since they are already available within the lexical resources of the rule-based NER system presented in [18]. Yet, these annotated titles are checked against the entries in the lexical resources of the recognizer and after filtering out the already existing resources and removing the duplicates, this overall annotation procedure has resulted in the determination of 4631 new named entities among which 3069 of them are person, 974 of them are location, and finally 588 of them are organization names [27]. As presented in Section 4.2, the evaluation results of the aforementioned recognizer after being enriched with these new named entities (utilized by the recognizer as additional lexical resources) are quite promising.

With an intention to benefit from all applicable Turkish Wikipedia article titles as NER resources, we have employed an automated procedure based on k-nearest neighbor algorithm utilizing the manually created resource set of 4631 named entities as the training data set. This automated procedure to compile language resources for NER is provided in Algorithm 1.
                           Algorithm 1
                           Learning NER resources from Wikipedia


                           
                              
                                 
                                    
                                 
                              
                           

In this algorithm, lines 1–7 correspond to the application of the k-nearest neighbor algorithm to the Wikipedia article titles where k is taken as 5 and the classification is performed provided that there are at least 5 neighbors in the training data set each with a similarity value above the threshold of 0.2. If a title does not have at least 5 neighbors each with a similarity value larger than 0.2, then this title is discarded and not considered further for inclusion in the resource set. The similarity metric utilized is Jaccard similarity which considers two strings as sets of tokens and for two strings with token sets A and B, it is calculated as 
                           
                              
                                 
                                    A
                                    ∩
                                    B
                                 
                              
                              
                                 
                                    A
                                    ∪
                                    B
                                 
                              
                           
                         
                        [29]. The value of 5 for k and the threshold value of 0.2 are heuristically determined considering the trade-off between a large overall resource set and a highly precise one.

The last three postprocessing steps of the algorithm, outlined at lines 8–10, are employed to filter out those possibly incorrect classifications within the lexical resource (the ne_set) by utilizing the pattern bases of the rule-based NER system [18] for the extraction of location (location_patterns) and organization names (organization_patterns). To illustrate, considering the example phrase Sabiha Gökçen Havaalanı (meaning ‘Sabiha Gökçen Airport’ as previously used in Table 1 of Section 2) as a candidate Wikipedia title given as input to the algorithm, since the first two tokens (Sabiha Gökçen) of this phrase constitute a person name, at the end of lines 1–7, the whole phrase might be incorrectly classified as a person name (due to its similarity to the person names in the training set) and has been added to the ne_set with this incorrect type at the end of line 7. Yet, since the phrase conforms to one of the location_patterns, namely ‘X Havaalanı’, previously given in Section 2, of the NER system, its corresponding entry (incorrectly classified as a person name) is removed from the ne_set at line 8, thereby increasing the precision of the overall algorithm. To summarize, the lines 8–10 of the algorithm target at reducing the noise in the automatically compiled lexical resource via the removal of incorrectly classified named entities in the resource by utilizing the location and organization name detecting patterns of the NER system [18] as filters, where these patterns have been previously discussed in Section 2.

The automatic language resource learning procedure outlined above has resulted in the classification of a total of 21,538 named entities from the Turkish Wikipedia article titles (of 368,500 entries) where 14,851 of them are person, 4448 of them are location, and 2239 of them are organization names. In order to check the accuracy of the procedure, these 21,538 entities are examined to determine whether they are correctly classified or not and this examination procedure has taken about 6h for the author to complete. The precision values (the number of correct classifications over all classifications performed) for the learning procedure are presented in Table 2
                         together with the number of correctly and incorrectly classified entities for each named entity type considered. The overall evaluation results, given in the last row of Table 2, are quite promising considering the fact that the whole procedure is automatic. Hence, with a precision rate of 91.25%, this procedure has resulted in the compilation of a plausible language resource set of 21,538 entries for the NER task in Turkish. The following features of this data set make it particularly significant:
                           
                              •
                              Manual compilation of such a moderately sized language resource set would require considerable time and effort. With the proposed procedure, this resource is compiled, particularly for a considerably lesser studied language, with limited manual intervention just to create the training set.

Since Wikipedia includes articles in a wide range of topics, this resource set can help alleviate the effects of the porting problem (addressed in Section 1) for the NER system that utilizes it.

One of the important problems encountered by the recognizers for Turkish is the existence of foreign named entities in their input texts. As Turkish Wikipedia includes several articles on foreign people, locations, and organizations, the ultimate resource set may help the recognizers identify at least some of the foreign names occurring in the Turkish documents considered.

In this section, we present the evaluation results of the rule-based NER system for Turkish [18] after extending it to utilize the training set and then the ultimate resource set including both the training set and the automatically learned entities from Wikipedia. First, we present information about the evaluation data sets and evaluation metrics employed. In the next subsection, the performance evaluations of the initial recognizer and that of the extended recognizer with the training data set are presented. The following subsection presents the performance rates of the system first extended with the set of learned resources and then with the cleaned version of this set. We provide discussions of these evaluation results at the end of the corresponding subsections.

The evaluation text data sets are three distinct sets on which the original rule-based recognizer has been previously evaluated [21]. Statistical information on these data sets is presented in Table 3
                         in terms of the number of words, the number of types, and the number of named entities included. Here, the number of types corresponds to the number of distinct words and hence is quite lower than the value of the number of individual words in the corresponding set. The number of named entities (in the last column) denotes the number of (unknown) named entities of type person/location/organization names; date/time and money/percent expressions which are annotated within each of the corresponding sets. The data sets have been annotated with these seven named entity types with the MUC style named entity tags (ENAMEX, TIMEX, and NUMEX) by utilizing a proprietary annotator interface, within the course of the previous study presented in [21]. The details of the annotation scheme employed are summarized in Section 5 of the current study.

The news set corresponds to 50 news articles from METU Turkish corpus [30] each with about 2000 words, the articles in the financial news set are from an online Turkish news provider, and finally historical text data set comprises the first three chapters of a book about Turkish history. Only person and organization names have been annotated in the financial news set which accounts for the comparatively lower number of named entities in this data set. In Table 4
                        , we also provide the number of named entities belonging to different types in each of these data sets.

There is a lack of common evaluation corpora for information extraction tasks including NER on Turkish texts which is one of the main problems against the related research on Turkish. In our case, particularly the historical text set is of limited size, yet it is utilized to observe the performance of the recognizer (originally engineered for news texts) enhanced with new language resources on a data set distinct from the news texts.

The evaluation metrics utilized are Precision (P), Recall (R), and F-Measure (F), which are calculated as follows:
                           
                              
                                 
                                    
                                       
                                          Precision
                                          =
                                          
                                             
                                                Correct
                                                +
                                                0.5
                                                *
                                                Partial
                                             
                                          
                                          /
                                          
                                             
                                                Correct
                                                +
                                                Spurious
                                                +
                                                0.5
                                                *
                                                Partial
                                             
                                          
                                       
                                    
                                    
                                       
                                          Recall
                                          =
                                          
                                             
                                                Correct
                                                +
                                                0.5
                                                *
                                                Partial
                                             
                                          
                                          /
                                          
                                             
                                                Correct
                                                +
                                                Missing
                                                +
                                                0.5
                                                *
                                                Partial
                                             
                                          
                                       
                                    
                                    
                                       
                                          F
                                          ‐
                                          Measure
                                          =
                                          
                                             
                                                2
                                                
                                                *
                                                Precision
                                                *
                                                Recall
                                             
                                          
                                          /
                                          
                                             
                                                Precision
                                                +
                                                Recall
                                             
                                          
                                          .
                                       
                                    
                                 
                              
                           
                        
                     

The above metrics also give half credit to partial extractions where the type of a recognized named entity is correct but it either misses some of the required tokens or includes excessive tokens [21], mainly following the corresponding definitions in [31].

The evaluation results of the initial form of the recognizer [18] and that of its extended form (which we call Extended Recognizer-1) with the training data set only (obtained by annotating about one twentieth of Turkish Wikipedia article titles) are presented in Table 5
                         where the capitalization features of the recognizers are turned off. With this feature turned off, the recognizers do not make use of the capitalization clue when identifying named entities.

Corresponding evaluation results for each NE type included within the data sets are presented in Table 6
                        . It should be noted again that only person and organization names are annotated within the financial news set and the historical text set does not include any money or percent expressions. These detailed results demonstrate that (i) temporal and numeric expressions are recognized with decent performance rates, (ii) the lowest rates are observed for person name extraction in news articles, and (iii) in historical texts, the performance of Extended Recognizer-1 when recognizing organization names (42.07% in F-Measure) is dramatically higher than the corresponding performance of the Initial Recognizer (12.23%).

The evaluation results of the initial and extended recognizers when the capitalization feature is turned on are provided in Table 7
                        .

The corresponding results for each NE type in the data sets are presented in Table 8
                        . The evaluation results for temporal and numeric expressions are excluded as they are the same as the results given in Table 8.

The results in Tables 5 and 7 demonstrate that the extended recognizer performs better than its predecessor. The best improvement over the initial recognizer is achieved on the historical text set while the least improvement is observed on the news text set. Particularly the latter result is an expected one since the initial recognizer has already been manually engineered for the generic news text genre and it achieves lower performance rates on the financial and historical texts which are two domains different from its target domain. Yet, some of the foreign names in news texts previously unrecognized by the initial recognizer seem to be recognized after the extension, which is again an expected result. The common knowledge on history and financial news available in the training data set helps recognition of some named entities previously unrecognized. Overall, the improvements for all three text genres are quite promising and provides justification for the utilization of all applicable Wikipedia article titles during the NER procedure.

We have used the statistical test described in [32] and accordingly utilized in [33] for the NER task, to discuss the significance of the improvements in NER performance after the improvements.

In this section, we present the evaluation results of the two further extended versions of the recognizer, the first version is the extended one with the automatically learned resources (without filtering out the incorrect classifications) in addition to the training set, which we call Extended Recognizer-2, while the second one is the extended version with the clean version of these learned resources, again together with the training set, which is called Extended Recognizer-3. Following this notation, Table 9
                         demonstrates the performance rates achieved by these two versions when the capitalization clue is not utilized and Table 11
                        
                         presents those results when the capitalization clue is utilized during the recognition procedure.

The corresponding results of the Extended Recognizer-2 and Extended Recognizer-3 for each named entity (NE) type in the data sets are presented in Table 10 when the capitalization feature is turned on. Again the rates for temporal and numeric expressions are excluded to avoid duplication as the resources of the extended recognizers are only enriched with lexical entries for person/location/organization name extraction. Similarly, the corresponding results for each NE type in the data sets are presented in Table 12
                         when the capitalization feature is turned on.

Considering the results in Tables 9 and 11, it can be deduced that the automatically compiled resource set improves the NER procedure. When the cleaner version of this set is utilized (Extended Recognizer-3), the performance rates are slightly higher as expected due to slight increases particularly in precision. All of the presented results are quite higher than the results obtained by the initial recognizer given in Tables 5 and 7. These are favorable results as they help to confirm that the proposed automatic compilation of language resources for NER in Turkish is a plausible contribution to related research.

Yet, the performance improvements of Extended Recognizer-2 and Extended Recognizer-3 over Extended Recognizer-1 are not as dramatic as those achieved by Extended Recognizer-1 over the initial recognizer. For the news text genre, the very slight improvement can again be attributed to the fact that the initial resources of recognizer have already been engineered for this domain, and hence additional resources do not lead to dramatic increases in performance rates, as pointed out in the previous subsection. For this domain, a better approach may be to tune and improve the existing resources and pattern bases after intensive error analysis. Because, several superfluous recognitions are due to the overly generic nature of some patterns in the pattern bases of the recognizer while some missing entities can readily be covered by adding a small set of convenient new patterns. Such improvements on the existing resources of the recognizer to achieve better performance rates on the news texts are left as future work.

One of the main reasons for the slight improvements on financial news and historical texts is the nature of considerable proportion of named entities included in the corresponding data sets, where these cases are elaborated below:
                           
                              •
                              For the employed financial news texts, there are several organization names with surface forms similar to person names (usually the founders of the organizations) which are incorrectly recognized as person names by the recognizer. Such cases are hard to distinguish without considering the context of the related text and hence they cannot be helped by adding new, though plausible, lexical resources.

Considering the historical texts utilized, there are two different and important sources of erroneous cases:
                                    
                                       –
                                       Some important empire names within the text are specified in contracted forms which correspond to person or location names and accordingly classified by the recognizer yet they are actually referring to organizations and annotated in the answer key as organization names. For instance, throughout the text, the name Roma (the city of ‘Rome’) is recognized as a location name by the recognizer, however, it is actually used to refer to ‘the Roman Empire’ throughout the text, and hence is annotated as an organization name in the answer key. Similarly, the common male person name Selçuk is recognized as a person name by the recognizer yet it is used to refer to a dynasty, ‘the Seljuks’, which ruled in the Middle East between 11th and 13th centuries, in the text, and hence is annotated in the answer key as an organization name. Such cases correspond to a considerable proportion of the errors made by the recognizer which cannot be helped with additional lexical resources.

Though Wikipedia article titles on history seem to increase the performance rates on this data set, still domain specific patterns to extract those historical location and organization names are required in order to increase the recall of the recognizer on this text genre, as several proper names that can be extracted with convenient patterns are missed. For instance, historical organizations such as Karatay Medresesi which denotes a historical education institution which can be captured with the second token Medrese which denotes the common name given to such institutions.

Another important source of superfluous recognitions observed during the above experimentation, particularly for the case of person name recognition, is the homonymity of a considerable proportion of Turkish person names within the person name list of the recognizer with common names and this problem applies to all text genres. Similar to the cases specified above, this problem cannot be alleviated with the extension of the NER system with new lexical resources and instead it requires a tuning procedure to be taken on the existing person name list of the recognizer.

In order to test the statistical significance of the performance improvements obtained, we have employed the randomization test described in [32,34] which has been employed in the MUC series [34] and also used in studies such as [33] for the NER task. With this method, the increases in F-Measure obtained by Extended Recognizer-1 over Initial Recognizer and Extended Recognizer-2 over Extended Recognizer-1 have been found to be statistically significant for all three data sets with significance levels (p-values) less than 0.01. The increases obtained by Extended Recognizer-3 over Extended Recognizer-2 have been found to be significant for news and financial news sets only while the improvement obtained on the historical text set is not statistically significant. Overall, these findings denote that significant increases in NER performance can be achieved using the outputs of the automatic resource compilation scheme proposed and a final manual revision of these resources is also important whenever feasible.

For comparison purposes, we have evaluated the CRF-based NER system for Turkish [24] which is made available at http://tools.nlp.itu.edu.tr/ as presented in [35]. The evaluation results of this NER system on our three data sets are provided in Table 13
                        . This NER system has been trained on news texts, so it achieves the best results on the news set and performs poorer on the financial news and historical texts as it is not particularly trained on corpora of these latter two genres. Its precision rates are comparatively higher than its recall rates. Overall, the results are in line with the results of the rule-based system [18] on these data sets given in the previous tables and the corresponding results of the rule-based system are better on all sets and especially on financial and generic news sets. The evaluation results of this rule-based system's extended versions are in turn better than the CRF-based [24] and rule-based [18] systems, as demonstrated above. Yet, the performance of the learning systems like [24] can be improved as long as domain-specific annotated training data is made available while rule-based systems require manual customizations for rule refinements for different domains. But, as we have also emphasized in our concluding summary below, we can achieve drastic improvements in NER performance of the rule-based systems through automatic means to extend the lexical resources, as we have demonstrated with our automatic resource compilation procedure that is presented in the current paper.

To summarize, the language resources automatically obtained from Wikipedia article titles in Turkish are shown to help increase the performance of the rule-based NER system on different text genres, with the above experimentation. We provide a graphical summary of our presented approach in Fig. 2
                        , together with the corresponding experimentation stages in which the extended versions of the recognizer (making use of different versions of the compiled resources) are involved. The names of the tasks carried out and the language resources produced within the course of the current study are written in boldface and the related shapes are drawn with thick borders and gray background within the figure, while previous work (i.e., the rule-based named entity recognizer and its initial resource set) is written in plain text and the corresponding shapes have white background. This graphical summary provides a convenient overview of the automatic resource compilation approach which can also be practically used by related language technology approaches like NER. The experimental setup in Fig. 2 is similarly useful as a model for testing the viability of the considered language resource compilation approaches.

As the recognizer has already been implemented for news texts with resources targeting this particular domain, the performance increase for this genre has been limited. Yet, on the other text genres of financial news and historical texts, the ultimate extended recognizer achieves more dramatic increases in performance when compared with the initial version of the recognizer. Thereby, the automatically compiled set of resources alleviates the effects of the porting problem to a certain degree for these genres. Further improvements on the recognition performance call for revisions on the resources and pattern bases employed by the recognizer after in-depth error analysis as outlined above.

In this section, we provide some remarks on the NE annotation scheme that we have employed on the three distinct data sets used during our experimentation. To the best of our knowledge, there is no study reported in the literature on NER on Turkish texts that explicitly provides the annotation scheme employed on the data sets. Hence, we believe that these pointers are useful for standardizing the annotation scheme for named entities in Turkish corpora as well as in similar morphologically rich and agglutinative languages. The related remarks are provided below with examples for illustrative purposes:
                        
                           1
                           The person/location/organization names; date/time and money/percent expressions are annotated in an inline manner with the ENAMEX, TIMEX, and NUMEX tags, mostly following the NE task definition of the MUC series for English.
                                 5
                              
                              
                                 5
                                 
                                    http://www.itl.nist.gov/iaui/894.02/related_projects/muc/proceedings/ne_task.html.
                               Yet, there have been some differences like the annotation of şimdi (‘now’) as a temporal expression of date type as <TIMEX TYPE=“DATE”>şimdi</TIMEX>.

NEs with metonymic readings are annotated with the types that are intended in the text. A relevant example case is the annotation of Roma (the city of ‘Rome’) as an organization name (as <ENAMEX TYPE=“ORGANIZATION”>Roma</ENAMEX>) since it is used to refer to the Roman Empire in the historical text data set, as explained in the previous section.

There are no nested annotations, hence if the name of a person is given to a location name as in Adnan Menderes Bulvarı (‘Adnan Menderes Boulevard’ where Adnan Menderes is a person name), then only the overall location name is annotated as <ENAMEX TYPE=“LOCATION”>Adnan Menderes Bulvarı</ENAMEX> and the person name included is not annotated in a nested manner.

In well-formed Turkish texts, the possible suffixes attached at the end of a proper person/location/organization name should be separated from the name with an apostrophe. Yet, there are cases when these apostrophes are incorrectly missed. For the temporal and numerical expressions considered, if the last token in these expressions is a common name, then there is no apostrophe as a separator, just like the case of other common names. For all of these inflected NE instances, we annotate them by excluding the attached suffixes. This choice is due to two reasons: (1) in the related task definition of the MUC series for English [1], the possessive marker ('s) is not included in the NEs, as demonstrated in a sample annotation of the expression, Canada's Parliament, as <ENAMEX TYPE=“LOCATION”>Canada</ENAMEX>'s <ENAMEX TYPE=“ORGANIZATION”>Parliament</ENAMEX>, and (2) from a practical point of view, by consistently annotating only the NE itself excluding the attached suffixes, we avoid the problem of considering the same entity as distinct entities due to the fact that it has been inflected with different suffixes. Hence, the two expressions including a location name; Ankara'ya (meaning ‘to Ankara’) and Ankara'nın (meaning ‘of Ankara’) are annotated as <ENAMEX TYPE=“LOCATION”>Ankara</ENAMEX>'ya and <ENAMEX TYPE=“LOCATION”>Ankara</ENAMEX>'nın, respectively. Even when the apostrophe that should separate the NE from the attached suffixes is incorrectly missing, these suffixes are still excluded from the NEs, as exemplified in the annotation given in the second row of Table 1 in Section 2.

Standardizing the NE annotation scheme described above will be an important contribution to information extraction research on Turkish texts as it will promote comparable annotated corpora and will facilitate the comparisons of different approaches.

@&#CONCLUSION@&#

Named entity recognition is an important information extraction task with several application areas. In this study, we present an automated procedure to compile language resources for named entity recognition in Turkish from Wikipedia article titles. First, we have annotated a subset of Turkish Wikipedia article titles with person, location, and organization names, where the subset corresponds to about one-twentieth of all the available articles. Next, this subset is utilized as training data set for a classification approach based on k-nearest neighbor algorithm which is executed on the remaining Wikipedia article titles in Turkish. This automated procedure has led to the determination of a moderately sized language resource set for named entity recognition in Turkish where the overall precision of the classification scheme for the set is found to be 91.25%. Several named entity recognition experiments utilizing this resource set have been performed with an existing named entity recognizer for Turkish, the results of which confirm that the automatically learned resources can contribute to the named entity recognition process in Turkish.

The proposed approach and the ultimate language resource set are significant as related research and resources for Turkish are quite insufficient compared to other well-studied languages like English. They also stand to confirm that Wikipedia texts can readily be utilized to alleviate the effects of the porting problem for rule-based (or manually engineered) information extraction systems. We can carry out experiments using the ultimate resource set on larger corpora and on corpora of other genres to observe its contribution on other text genres. Another plausible future research direction is to utilize full Wikipedia article contents to automatically learn contextual patterns for named entity recognition and other information extraction tasks and then test the validity of the learned patterns. Finally, using both the existing annotated corpora and the output of the ultimate recognizer on raw texts as training data, supervised classification techniques can be employed for named entity recognition on Turkish texts.

@&#REFERENCES@&#

