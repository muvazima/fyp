@&#MAIN-TITLE@&#A fuzzy psycho-physiological approach to enable the understanding of an engineer’s affect status during CAD activities

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           To monitor the emotional changes of the participants as they carry out an engineering design task.


                        
                        
                           
                           To investigate and develop a repeatable methodology to capture these individual motions during CAD system operation.


                        
                        
                           
                           To validate the captured emotions using other methods.


                        
                        
                           
                           To determine if there is a correlation between an engineer’s emotions and associated CAD tasks.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Affective computing

Fuzzy logic

Psycho-physiological signals

CAD

@&#ABSTRACT@&#


               
               
                  Affective computing involves human–computer interaction (HCI) where an interface can detect and respond in context to a user’s emotions. Emotions play an essential role in the daily activities of a human during work and, as a consequence, can critically affect their decision-making. Recent research on affective computing opens the door to exploring the emotional aspects of computer aided tasks and HCI; however, there is little research in this area related to computer aided design (CAD). There is an established connection between emotional and cognitive processes and without these emotional markers decision-making in humans is almost non-existent.
                  The holy grail of engineering design is for a concept to be ‘right-first-time’ and only by delving into the affect-cognitive domain will insights in terms of the associability of critical decision-making and the underlying intent of engineering decisions and tasks be better understood. Work in this field aims to recognize, interpret and process human emotions in order to support and adapt computer-based systems and their associated interfaces to user behaviors and to elicit appropriate system responses. This is a very contemporary area in engineering design research. One key aspect of provisioning CAD systems with the capability to detect the emotional state of the engineer is to lessen the chance of distorted reasoning as a result of irrational judgements. Work in this area can potentially identify emotional markers that influence the coherence/incoherence of reasoning against knowledge as well as the potential quality and suitability of a specific engineering solution or approach.
                  This paper proposes and investigates a methodology to determine the emotional aspects attributed to a set of CAD design tasks by analyzing the CAD operators’ psycho-physiological signals. Two sets of experiments were conducted. A pilot case study focused on modeling tasks carried out in Solid Edge™ was run to prove the validity of the methodology, the tools applied and subsequent analysis. The main case study extended this to more wide-ranging user logging, incorporating a series of configuration and optimization tasks in Siemens NX™ for a real design task. Psycho-physiological signals of electroencephalography (EEG) and galvanic skin resistance (GSR)/electrocardiography (ECG) were recorded along with a log of CAD system user interactions. A fuzzy logic model was established to map the psycho-physiological signals to a set of key emotions, namely frustration, satisfaction, engagement and challenge and the results analyzed.
                  The methodology and subsequent analysis applied was found to be repeatable. Over the various task action-chains the elicitation and interpretation of each participant’s emotions were successfully carried out with significant correlations demonstrated between the associated engineers’ CAD activities and their reported emotional states.
               
            

@&#INTRODUCTION@&#

Conventional computer aided design (CAD) packages are widely used in industry and have evolved beyond 3D modeling and now facilitate detailed numerical analysis, e.g. stress, thermal, dynamic, through to manufacturing simulation and planning. Industry is always striving to improve the efficiency of the engineering design process and one way this has been accomplished so far is by the gradual revision and evolution of the CAD system user-interface (UI) by adding greater functionality and automation. However, all improvements come with a compromise between the usability of the package and broader, more complicated functionality.

The diversity of CAD packages makes it ever more difficult to evaluate which one is most appropriate for a specific purpose. To overcome this problem, there has been widespread research to compare different types of CAD package utilizing numerous methods including techniques such as GOMS (Goals, Operators, Methods and Selection)  [1] and model human processor (MHP) modelling  [2] which compare the time taken to design real-world drawings.

The GOMS modeling technique predicts how long it would take for a user to carry out a specific task using a user interface. Goals refer to the user’s design aim. Operators describe the functionality provided by the software to allow the user to achieve their aims. Methods is the sequence of goals and operators combined to complete a specific aim. Selection is rules which define the methods that are accessible by the user under each specific circumstance. By breaking down each CAD design task into its primitive actions, such as pressing a key or clicking a menu and assigning times to these then a task completion time (TCT) can be predicted for each task. A CAD package that takes the least time to complete a design task implies its user interface is more efficient and hence is potentially more productive despite the quality of the associated solutions. Several forms of GOMS modeling are in use today and the types of predictions gained from these vary  [3].

MHP involves predicting how long a user will take to complete a task and therefore gain an insight into how well a user interface is designed  [2]. It models a user from an engineering perspective using processor and storage components to describe how a human user carries out a specific engineering task. MHP processes can be run in parallel whereas GOMS processes are run serially.

Apart from evaluating CAD applications using task-time predictions, an alternative way to ascertain performance is to carry out real-world evaluations. Okudan et al.  [4] devised a methodology to compare various types of solid modeling CAD software. They compiled a list of solid modelers to test and then train a group of users to use a small selection of functions. Users were then tested by asking them to create a 3D object using various solid modelers and the completion time and the quality of the drawings statistically analyzed to evaluate which CAD package made the users more productive. Finally, other, less scientific, methods of evaluating CAD applications are based on factors such as their cost to end users, how easy they are to learn, how well supported is the software and also how the software performs when reviewed by experts in the community  [5].

In terms of computer interfaces, Human–Computer Interaction (HCI) has long investigated how better interactive systems can be designed by evaluating their usefulness via well designed user trials  [6,7]. A useable system is not a one dimensional property of a user interface; in fact a system is useable  [8] when it is:


                     
                        
                           •
                           easy to learn, i.e. users can go quickly from not knowing the system to doing some work efficiently in a short period of time allowing them to attain a high level of productivity;

easy to remember, so that infrequent users can return after a period of inactivity without having to learn everything again;

relatively error-free or error-forgiving, so that users do not make many errors, particularly catastrophic errors, and can be easily recovered;

pleasant to use, satisfying users subjectively, so that they like to use the system and can use it effectively.

Clearly, it can be seen that there is a diverse range of criteria against which CAD applications can be judged, especially regarding the first three aspects. However, one aspect which has not been researched until now relates to the emotive affects of CAD  [9]. There is an increasing demand for computer–human interfaces that emulate the natural way of human communication  [10] which makes them intuitive and pleasant to use. However, before this can be really addressed there is a need to study the emotional aspects of the user experience.

Regarding emotions, a physiological signal is inherently controlled by the human autonomous nervous system; a psycho-physical signal of this type is far more robust against the possible artefacts of human social masking, such as changing facial expression, gestures or voice. Physiology studies provide strong relationship evidence between physiological reactions and the emotional/affective states of the human  [11–14]. Mental states classification using signals, such as electroencephalography (EEG), electromyography (EMG) for facial response, electrocardiography (ECG) or heart rate (HR) and galvanic skin resistance (GSR) can be used to both understand human brain functions and provide feedback for human-in-the-loop systems. There is a plethora of research for recording continuous signals from EEG with or without the combination of autonomic variables (heart rate, respiration rate) to control adaptive automation [15–21].

Inspired by research in neuroimaging and affective computing detailed in later sections, a fuzzy model approach was proposed in this work to examine the relationship between psycho-physiological signals and user emotions detected during the operation of a CAD system. The intention was to validate and qualify the fuzzy model via subjective user feedback and objective statistical correlations.

Therefore, the aim of this research was to investigate if affective psycho-physiological signals acquired from CAD users can help determine their emotional state and be mapped onto specific design activities.

The aim was addressed by three key research questions:


                     
                        
                           1.
                           Can affective psycho-physiological signals be used to quantify an engineer’s emotions during the operation of CAD systems?

Can these be mapped onto actual CAD tasks/activities?

Can these emotions be triangulated using user interview feedback, including captured task video discussions?

These questions were addressed by the following research objectives:


                     
                        
                           1.
                           To monitor the emotional changes of the participants as they carry out an engineering design task.

To investigate and develop a repeatable methodology to capture these individual motions during CAD system operation.

To validate the captured emotions using other methods.

To determine if there is a correlation between an engineer’s emotions and associated CAD tasks.

The paper is organized as follows. Essential background information on psycho-physiological signals and emotion recognition is first presented to lay the foundation for the proposed emotion analytical model in Section  2, followed by a detailed description of established sets of fuzzy models in Section  3. An initial pilot study follows with a full validation using different engineering design tasks, detailed by two case studies in Section  4. A comparative evaluation of the results of the studies and the overall approach is presented before concluding in Section  5.

The psycho-physiological signals used to help determine the user state and their corresponding emotions are varied and require grounding in the literature to support the development of a methodology which can be used as a means of taking physical response signals and, after processing, produce some measure of emotional response to an activity or experience. Previous work on emotion recognition requires investigation to determine how best to approach the use of such data and associated approaches in a CAD user environment.

Relationships between the electroencephalography (EEG), electromyogram (EMG), electrocardiography (ECG), heart rate (HR), galvanic skin response (GSR) and neurophysiologic changes in states of consciousness have been established and analyzed over the last few years  [22–25]. The changes in brain electrical activity, muscles’ response and skin conductivity involve a state of consciousness in the human subject.

EEG in certain frequency bands indicates activities at different hierarchical levels  [22]. The work reported here focuses on one particular EEG signal that is associated with alpha waves. Alpha waves indicate a relaxed awareness in a human being without any attention or concentration (Fig. 1
                           ). Alpha waves lie between 8 and 13 Hz with an amplitude of 30–50 μV   [26]. They can indicate a mindless state and can also be reduced or eliminated by anxiety or mental concentration. The asymmetry between the right and left hemisphere of the brain can indicate the valence value.

EMG measures muscle tension, for example electrodes placed on the face can be used to measure muscle activity and distinguish facial and gestural expressions  [27]. EMG over the brow or frown muscle is lower than that over the cheek or smile muscle; muscle regions where emotions are mildly positive as opposed to mildly negative  [11].

ECG measures the electrical activity of the heart over a period of time and is recorded by electrodes attached to the outer surface of the skin. HR is extracted from the ECG signals and is related to psychological arousal and excitement  [24].

GSR measures the electrical resistance of the skin. Sweat located in the palms and feet can indicate psychological stimulation even when it does not reach the surface of the skin. GSR is also correlated to arousal and indicates various emotional responses, e.g. stress and excitement as well as cognitive activity  [23].

EOG records the eye movements both vertically and horizontally. It does not represent the response to individual visual stimuli but instead measures the resting potential of the retina.

In this work the 10–20 system (Fig. 2
                           ) is used, as recommended by the International Federation of Societies for Electroencephalography and Clinical Neurophysiology  [22]. The 10–20 system considers distance of specific anatomic landmarks and then uses 10% or 20% of the distance as electrode intervals on the head. The datum points are the nasion, the dent at the top of the nose, and the inion which is the bony bump at the back of the skull. From 10% above the nasion and inion, along an imaginary vertical line that connects them, a circle is drawn around the head. The other electrodes are positioned by keeping a distance of 20%. Each site has a letter/number to identify the lobe and the hemisphere location. Letters F, T, C, P and O are the frontal, temporal, central, parietal and occipital locations respectively. Even numbers refer to the right hemisphere and odd numbers to the left. The “z” is the electrode placed in the middle.

EOG signals are measured using two pairs of electrodes, one pair above and below the eye, and the other pair is the left and right as shown in Fig. 3
                           (a).

A GSR signal is normally recorded from two fingers of the non-dominant hand, i.e. in the case of this research the middle and little fingers whereas an alternative to GSR can also be used, namely heart rate which can be extracted from ECG. ECG can be captured from the chest or forearm, for example in this study the electrode placements are shown in Fig. 3(b).

Emotions are associated with feelings, thoughts and behaviors and are therefore involved in decision making, perception, learning and thinking  [12]. There is an increasing demand for computer–human interfaces that emulate the natural way of human communication  [28].

Many theorists define some of the emotions as being basic/primary and complex/secondary and assume that secondary emotions are derived from basic emotions. There is no consensus as to which emotions are simply basic or how to ground complex emotions into the basic emotion categories. Ekman  [13] defined six basic emotions: anger, disgust, fear, joy, sadness and surprise. Plutchik  [14] created a wheel of emotions, which consists of eight primary emotions, namely: anger, fear, sadness, disgust, surprise, anticipation, acceptance and joy.

Psychological researchers and psychologists typically define emotions according to one or more dimensions. Arousal and valence are the most commonly used dimensions of the 2D emotion model (Fig. 4
                        ). Russell et al.  [30] used an arousal–valence space to create the Affect Grid. Arousal refers to “active” and “passive”, whereas valence generally refers to “positive” and “negative”. Lang et al.  [31] mapped individual pictures to emotions as defined by the arousal–valence space. Bradley and Lang  [29] later developed a non-verbal pictorial self assessment mannequin called Manikin SAM, which is now widely used to report user affective experiences in the field of advertising, product design, etc. Both the Affect Grid and Manikin SAM employ the 2D emotion model called the arousal and valence space with 9 grid scales. Indeed, the 2D emotion model is commonly used in emotion definition and classification; for example, the International Affective Picture System (IAPS) is built based on the score that subjects reported on the Manikin SAM scale and most of the image stimuli for emotion recognition research are selected from IAPS  [32].

Despite considerable achievements in the area of emotion recognition, there are still many problems to overcome. Emotion recognition is one of the first key steps towards the evaluation and development of advanced affective human–computer interaction (HCI). Recent research on Affective Computing emerged from the field of Human–Computer Interaction (HCI) which focuses on recognizing, interpreting, processing and simulating humans  [33]. Studies have shown that various changes in physiological activity are integrally related to emotional responses  [34]. For example, smiling is associated with positive valence, whereas a frown is associated with negative valence. Thus, measurements of the active facial muscles can obtain information about the participant’s affective state  [29,30]. Scheirer et al.  [35] used pattern recognition to recognize user frustration from two physiological signals: galvanic skin response and blood volume pressure. Other techniques to assess emotional stimulation include Klein et al.’s [36] method that deliberately frustrated the user during game-play whilst they were given text-based assistance. Their results showed that the users chose to interact significantly longer with the computer when they were given textual support compared to the condition in which no support was given. There is evidence that different emotional reactions can be evoked using different stimuli and so evoke different responses and subjective experiences  [37]. However, these emotions are generated by looking at or listening to artificial stimuli and it is difficult to apply such procedures to the use of real-world applications.

Numerous studies have been carried out in evaluating emotion recognition from facial movement and voice with high accuracy; however, the experimental circumstances are well controlled and any accuracy will potentially be much lower in ordinary circumstances. Wenger  [38] and Schachter  [39] believe emotion is as a result of the occurrence of physiological arousal while Frijda  [40] views physiological arousal as being part of the emotional process. A physiological signal is controlled by the human autonomous nervous system so it is more robust against the possible influence of human social masking and many works from the field of physiology provide evidence that there is a strong relationship between physiological reactions and the emotional/affective states of the human  [12,23,41–46].

In recent gaming research, Mandryk et al.  [23] introduced a fuzzy approach to emotion recognition based on the facial expressions and associated skin conductance of the players playing NHL 2003®  on a Sony PS2®. In the experiment, four electrodes on the face are placed to capture the facial muscle movements. Frowning is assumed to be associated with negative emotions whereas smiling is associated with positive emotions. According to FACS, the facial expressions are generated with the effort of all the muscles on the face; however, these simple assumptions are not satisfactory for this research since it does not establish the understanding of emotions on the valence scale  [47].

Juma  [42] studied secondary emotions derived from a first-person shooter (FPS®) game. This game is developed with an affective component which can combine the primary emotions onto a secondary emotion with the non-playing character (NPC) acting according to the player’s secondary emotions. This research revealed that secondary emotions play a crucial role in action selection in a human–computer interactive environment.

Costa et al.  [43] used emotional films to stimulate the five primary emotions of subjects in a random order, namely happiness, sadness, anger, fear and disgust. A synchronization index is introduced to measure the valence scale of emotions. Li et al.  [45] employed pictures to trigger the feelings of happiness and sadness in subjects and reported an accurate classification that was more than 90% correct. However, Horlings et al.  [12] reported that the classification rate is less than half if the selected stimuli are not with the extreme values either on the arousal and valence scales. This is important since this work requires confidence in valence and corresponding emotion classifications across the whole scale. Nie et al.  [46] introduced a subject independent emotion recognition classifier; however, the four emotions in this research are again extreme emotions stimulated by selected movies.

Most of the studies on neuroimaging and brain–computer interactions (BCIs) are in strictly-constraint environments due to the nature of EEG signals being easily affected by muscle and connection artefacts  [48–51]. However, there is very little tolerance on motor movements, which are essential for operating current CAD systems. Recently, many researchers have adopted research findings from the field of psychology and employed psycho-physiological signal analysis for many real life situations, such as improving athletes’ performance, evaluating game playing environments and so on  [23,52–55]. Thompson et al.  [52] reviewed current research on biofeedback and peak performance in sport and emphasized that the EEG recording interference is exacerbated by motor movement. Practical and computational techniques are also discussed in order to obtain reliable EEG signals while motion is present.

Given that there is no unanimous opinion on the correlation between psycho-physiological signals and real emotions, this paper proposes combining a number of techniques from the literature including a fuzzy approach based on a general understanding of psycho-physiological signals, mainly on alpha waves from EEG, and other psycho-physiological valence signals such as GSR and HR. The experimental methodology is detailed in the following section.

The proposed set of fuzzy models takes the concept of the Affect Grid and maps the captured physiological signals to the arousal–valence space and then, subsequently, onto individual emotions. The intention is to monitor the emotion status in participants as they carry out engineering design tasks and map these onto CAD activities. The first stage of the fuzzy models takes the EEG and GSR/HR as inputs and maps them onto the arousal and valence axis respectively.

Substantial research on EEG frontal asymmetry and the various aspects of emotion has been carried out in the last couple of decades. Davidson’s is one of the most accepted and popular models which points to the fact that high levels of left frontal activity indicates the user experiences positive, approach-related emotions whereas high levels of right frontal activity are normally associated with the experience of negative, withdrawal-related emotions  [56,57]. He also specified that the frontal lobe specializes in processing positive and negative emotion experiences. Ohme et al.  [58] suggested the possibility of diagnosing the approach dimension of advertisements by employing frontal alpha activities on marketing research based on Davidson’s model.

Taking Davidson’s model and the correlation between physiological signals and emotion mapping, the following parameters are measured and mapped in a novel fashion onto the mental arousal–valence space as shown in Fig. 5
                     :


                     
                        
                           •
                           
                              Galvanic Skin Response (GSR): linear with the mental arousal. When GSR is high, arousal is high; on the other hand, arousal is low when GSR is low  [23].


                              Heart rate (HR): correlated to psychological arousal and excitement. When arousal level is high, HR increases whereas if arousal level is low, HR decreases  [24].


                              Alpha Waves: when the alpha wave moves towards an extreme high of 13 Hz and the valence is low, negative emotions are dominant. When the alpha waves stay at the medium frequency range of approximately around 9–10 Hz and the valence is high then positive emotions are dominant  [44].

According to these combined criteria, the GSR and HR signals are able to represent the arousal axis of the participants, the frequency feature of an alpha wave can then be mapped into valence axis and subsequently used to identify certain emotions using an Affect Grid  [30].

In order to address the objectives of this research, particularly regarding mapping these onto CAD activities, it was necessary to record not only user physiological responses but also CAD system inputs and outputs in a time synchronized manner. The data recorded during the experiments included the psycho-physiological data, synchronized with user activity log files, video captures of the user’s screen and a structured feedback questionnaire with interview notes.

This work focuses on monitoring the affect status of CAD users in order to understand the decision rationale made during design processes. A design process involves a series of decision making steps which are impacted by the designer’s affective state. Satisfactory decisions can induce positive emotions and potentially immerse the user more in the design task, whereas the opposite will cause frustration and the potential to withdraw attention. Alternatively, a positive affective status may encourage the designer to make better and correct decisions, whereas negative emotions may cause impairment in decision making. Hence this work selected four emotions that related to decision-making in CAD systems, namely: frustration, satisfaction, engagement and challenge.

The four chosen emotions, circled in Fig. 6
                        , are defined on a Likert-type scale, where yellow means a stronger emotion and blue indicates otherwise. The reported emotions relate to the strength of a chosen emotion in a particular time frame.

Neural networks and Hidden-Markov Models are useful methods for dealing with uncertainty; however, they require large training sets (precise inputs) to cover the entire range over the different variables. In contrast, a fuzzy system takes non-precise inputs and generates fuzzy outputs where the relationship between inputs and outputs are well defined rules. Fuzzy systems are widely used in machine control since they can easily mimic human control logic  [59].

A fuzzy inference system (FIS) consists of inputs, outputs, membership functions and IF/THEN rules as shown in Fig. 7
                           . All the inputs and outputs are continuous data. The membership functions transform the membership of a specific input/output element into a degree of membership in the set, generally a linguistic term; for example, Monday is transformed to weekday, whereas Saturday is to weekend. IF/THEN rules are employed to describe the desired system response in terms of the linguistic variables rather than complex precise mathematical expressions. Once the linguistic output responses are obtained a defuzzification method is employed to transform the outputs to a scalar output.

An FIS is best used with continuous signals, especially when there is no mathematical model of the process. It can deal with high ambient noise with expert theoretical linguistic descriptions of the process. According to fundamental psycho-physiological theories, a fuzzy approach is chosen for mapping continuous psycho-physiological signals to affective emotions.

The modeling procedure follows three steps (Fig. 8
                           ). Physiological data are first captured from the participants while carrying out design tasks in CAD environment. The apparatus and experiment procedure are detailed in Section  4 for each case study. The data are then processed to remove noise and normalized if required; the general pre-processing methods are introduced briefly in Section  3.2.2. The detailed pre-processing methods are listed in Section  4 in each case study. A comparison of pre-processing methods was carried out with the most suitable method recommended in the discussion section. In Section  3.2.3, the pre-processed physiological signals are mapped to an arousal and valence space and, finally, the arousal and valence values are employed to generate numerical values for the emotions as outlined in Section  3.2.4, i.e. frustration, satisfaction, engagement and challenge. The last two subsections detail how the fuzzy models were established with the two models applied to both case studies.

Given that the psycho-physiological signals captured are from individual participants and the number of participants is relatively small, the proposed fuzzy logic model needs to be fine tuned to each individual in order to generate meaningful results. Therefore, this is a task and subject-based emotion recognition model that requires post-processing of captured data before being mapped onto emotions. According to this proposed approach there are two types of input signal for the fuzzy logic model: EEG signals from frontal lobe which relate to the valence axis and GSR/HR related to the arousal axis.

The pre-processing extracts the GSR from middle and little finger of non-dominant hand in Case Study I and the heart rate (HR) from the chest transducer ECG signals in Case Study II. GSR and HR both vary among individuals and the distribution of the signals can be very different. Therefore the GSR/HR signals are normalized in order to make it easier to tune the membership of the fuzzy model and makes it possible to generate a more generic distribution (Z score) when the sample size is bigger. This extraction and normalization method is detailed in Sections  4.1.3 and 4.2.3.

EEG signal pre-processing is used to extract the frequency feature of the alpha band of the frontal lobe. These signals are vulnerable to motor artefacts, especially those generated by eye and neck movements. Given the nature of the user interaction with the computer and associated peripherals, e.g. a space mouse, it is difficult to constrain the user movements in the same way as in typical neuroscience setups or the psychology laboratory, e.g. using a head rest and/or no limb movement. The nature of CAD work is such that motor movements will be detected as a natural part of operating the interface; therefore it is essential that the analyses of EEG signals are performed with minimum impact on the real brain wave profiles detected while the removal of artefact noise, as well as this the high tolerance of the algorithm itself associated with the artefact noise must be compensated. Different approaches were explored in order to find a suitable noise removal method and are detailed in case study Sections  4.1.3 and 4.2.3.

The first stage for producing fuzzy models to generate emotions is to map each user’s pre-processed EEG and GSR/HR signals onto the arousal–valence space (AV space), called Psycho-Physiological signal-Arousal and Valence space (PP-AV) model. There are two continuous inputs to the PP-AV model, the peak alpha frequency and the normalized GSR/HR. The arousal and valence values are the two outputs of the PP-AV model and both the inputs and outputs are presented as a percentage of a possible maximum value.

The membership function and rules of the psycho-physiological signal-AV space model are generated linguistically according to the rules in Section  3.1. Fig. 9
                            shows a participant’s distribution and the membership functions of their inputs. The input signals are transformed into five levels, they are named: low, mid-low, average, mid-high and high. The five levels are tuned onto the data distribution with each one covering 60% of the captured data samples. “Average” is the average level of the participant obtained using signals from all their sessions. The other four membership functions are set according to the data distribution of the inputs. All the individual memberships of each input signal are set according to the distributions of their signals, the outputs having five evenly distributed gauss membership functions.

There are 21 rules generated to linguistically describe the relationships/rules between the psycho-physiological signals and the arousal–valence space; the surface graphs produced by these are shown in Fig. 10
                           .

The second stage of the fuzzy model emotion generation is to map the arousal and valence information onto the different emotions (AV-Emotion model). The entire AV space–time series is mapped to continuous metrics of the four chosen emotions, i.e. frustration, satisfaction, engagement and challenge (Fig. 6). There are two inputs (arousal and valence) into and four outputs (frustration, satisfaction, engagement and challenge) from the AV space-emotions model. These are presented up to a possible maximum on a scale of 0–10.

The membership functions and rules of the AV-Emotion model are established according to the Affect Grid set which was developed from a circumflex model of emotions  [19]. Given that the alpha band lies between 8 and 13 Hz, a 4 s time frame is employed to calculate the peak alpha frequency with a resolution of 0.25 Hz, the optimal choice of both the time and frequency domains. Since the resolution of peak alpha frequency is as selected, there are only 20 grids on the valence axis. In this case, the Affect Grid has been modified to incorporate five levels of arousal and valence instead of the original nine. Using the modified five-level Affect Grid, the arousal/valence space was mapped onto four linguistic emotion levels, i.e. low, mid-low, average, mid-high and high. Hence, five evenly distributed gauss membership functions are used for inputs to the arousal and valence. There are no established methods to describe the levels of emotions as they vary in AV space. Thus, in this work “low” is defined as having no appearance of a particular emotion with the others increasing steadily up to high.

Since there are no formal linguistic emotion definitions, the four chosen emotions with their dominance levels have been defined as in Fig. 11
                            with the output levels showing how dominant a particular emotion appears at a specific moment.

The arousal and valence contribute equally when generating the different emotions with their levels mapped onto the different emotions based on the previous understanding of where each one exists in the AV space. 25 rules for each emotion were generated according to Fig. 11 with the rules are summarized as in Table 1
                           . From these, surface graphs (plan views) of the rules with corresponding emotion affect grid representations are produced as shown in Fig. 12
                           .

The pilot study used to develop the methodology outlined previously involved 15 participants creating a 3D model of a car using Solid Edge™ version 20  [61,62]. Psycho-physiological signals were recorded via Mind Media NeXus-10 and NeXus-32 biofeedback devices (Fig. 13
                           ). The NeXus-10 was used to measure the participant’s skin (GSR) and facial (EMG) responses respectively. The GSR was monitored using 2 finger sensors while the EMG was monitored using 5 electrodes attached to the participant’s face. Brain activity (EEG) was measured by way of an instrumented cap with 21 electrodes using the 10–20 system as described in Fig. 2. All measurements were recorded using software called BioTrace+ which accompanies the NeXus devices.

All of the participants were engineering students with prior experience of using Solid Edge and/or other CAD packages. Their ages ranged from 20 to 29 years old.

Each experimental session lasted approximately 60 min and was split into 3 stages. The participant was initially presented with a 3D solid block in Solid Edge and by working through a 3-part tutorial they produced a 3-D model of a car as shown in Fig. 14
                           (a). The data acquisition device was set up as shown in Fig. 14(b).

The experimental steps were as follows:


                           
                              
                                 1.
                                 Each session starts with a questionnaire collecting the demography of the participant, i.e. work background and CAD experience.

An animated image of a waterfall provided by the BioTrace+ software is displayed to the participant while a piece of soothing music is played. This process lasts for 2 min; the aim is to relax the participant, giving a baseline for the subsequent psycho-physiological measurements.

Stage 1: the participant follows detailed tutorial instructions to shape a solid block and create the basic outline of a car. Since the tasks involved during this stage are straightforward, there is a time-limit of 5 min.

A questionnaire is filled in by the participant and an interview conducted to identify how the participant is feeling during the Stage 1 design session.

Step 2 is repeated to allow the participant’s psycho-physiological signals to fall back to their baseline.

Stage 2: the participant adds a color to the car and creates window cut-outs. The instructions provided are less detailed than in Stage 1. Participant requires some creativity to enable them to complete the task. Duration–10 min.

The participant then fills in the second part of the questionnaire and is then interviewed regarding the Stage 2 design session.

Step 2 is repeated to allow the participant’s psycho-physiological signals to fall back to their baseline.

In Stage 3 the participant adds wheels and customizes the car appearance. The task is designed to provoke the participant into showing creativity. Minimal instructions are given compared to Stages 1 and 2. Lack of information may cause the participant to feel frustrated. Duration–15 min.

The experiment ends with the participant filling out the final part of the questionnaire and the final interview regarding any changes in feelings and emotions during the Stage 3.

Most participants reported that the relaxation session in between of each stage did not help them; they were still in the mood of either the previous or forthcoming stage. Hence, the recording of the relaxation sessions was discarded and it was decided to use all of the design stages as an overall emotional experience for each participant. In this case, the analysis is subjective and task-based. The recorded files were separated into three parts for analysis according to the three main stages of the experiment. All the files were validated by visual inspection according to the notes taken during the experiment. 31 samples were valid for further analysis from 12 participants; other samples were removed due to corrupted video recordings. Fig. 15
                            illustrates the method and procedure taken to generate the inputs of the fuzzy models.

The EEG signals were acquired through two mono-polar electrodes, namely F3 and F4 (see Fig. 2). As the EEG signal-to-noise ratio was low, a Butterworth band pass filter was initially applied to remove DC noise. The alpha wave (8–13 Hz) was subsequently obtained through using band pass Chebyshev filter. Any eye movement artefacts which overlap with the alpha wave frequency band were difficult to remove. Normally blind source separation (BSS) can be used to remove eye movement and blinking artefacts; however, this requires a large amount of data from different channels and a fundamental understanding of the different mappings of expected components  [22]. In this case, a subtraction of alpha waves from F3 and F4 was applied to look at asymmetries on the frontal lobe. 
                              
                                 (1)
                                 
                                    A
                                    l
                                    p
                                    h
                                    a
                                    =
                                    
                                       
                                          A
                                          l
                                          p
                                          h
                                          a
                                       
                                       
                                          F
                                          3
                                       
                                    
                                    −
                                    
                                       
                                          A
                                          l
                                          p
                                          h
                                          a
                                       
                                       
                                          F
                                          4
                                       
                                    
                                    .
                                 
                              
                            The frequency difference implies more about the emotional changes than the power difference. Hence, a 2048-point Fast Fourier Transform (FFT) was performed on the subtracted alpha wave at a sample rate of 512 samples/s. Thus a frequency spectrum with 0.25 Hz resolution is obtained using a time frame of 4 s. After applying the FFT, the frequency corresponding to the peak power of every 4 s can be found. The peak frequency is used as an input to the fuzzy model. The membership functions of the peak frequency of alpha are generated according to the distribution of the data from all stages. 
                              
                                 (2)
                                 
                                    n
                                    o
                                    r
                                    m
                                    a
                                    l
                                    i
                                    z
                                    e
                                    d
                                    
                                    G
                                    S
                                    R
                                    =
                                    
                                       
                                          
                                             (
                                             G
                                             S
                                             R
                                             
                                                (
                                                i
                                                )
                                             
                                             −
                                             
                                             
                                                
                                                   G
                                                   S
                                                   R
                                                
                                                
                                                   m
                                                   i
                                                   n
                                                
                                             
                                             )
                                          
                                       
                                       
                                          
                                             (
                                             
                                                
                                                   G
                                                   S
                                                   R
                                                
                                                
                                                   m
                                                   a
                                                   x
                                                
                                             
                                             −
                                             
                                             
                                                
                                                   G
                                                   S
                                                   R
                                                
                                                
                                                   m
                                                   i
                                                   n
                                                
                                             
                                             )
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        

Due to the variances among the participants, the GSR signal from each participant during all the sessions were normalized onto a scale between 0 and 100. The minimum and maximum values of the GSR signal values for each participant from all three experiment stages, including the relaxation sessions, were normalized using Eq. (2). The membership functions were then tailored according to the normalized GSR distribution from all stages.

Since the baseline varies significantly amongst all the participants, it is difficult to obtain the general distribution of participants due to small database size. Hence, our psycho-physiological to AV space fuzzy model maps the membership functions of the inputs according to the overall data distribution of all three stages. The rules were the same for all the participants and these were generated according to the previously defined emotion mapping on the Affect Grid. Continuous emotions were modeled from the outputs of the fuzzy model using the centroid defuzzification method  [59]. The emotions were manually synchronized for each step against the video captured during the design process and the corresponding interview notes. The visualized emotion spikes generated from 83% of the emotion spikes were correlated to those observed in the videos and mentioned during the interviews. It was relatively straightforward to directly recognize the emotional experiences of the participants from the continuous time-phased modeling of emotions. Table 2
                            shows how the correlations were done for participant 12 during stage 3.


                           Fig. 16
                            shows the changes in emotions of participant 10 during the second design stage. During the first two minutes the participant was satisfied with the outcome of the first step and planned his own strategy to complete the second step. As the difficulty increased in the second step, the challenge the participant felt was rising and he began to look for the functions he planned to use. From 200 to 320 s, the participant was carrying out his plan; the frustration level was at its lowest. However, the participant failed to find the function to complete his plan, the frustration level rose significantly and he started to try different methods to finish the design. The engagement and challenge level increased again. The satisfaction level varied until the time was up and the participant was obviously not happy with the outcome of the second step. The continuous emotional output of the fuzzy model made it easy to identify the participant’s emotions while they were practising engineering design, most significantly with no interruption of the design process. This provides considerable potential as an analysis tool to help to improve the CAD design process as well as support the training of inexperienced designer engineers. Using this natural method to identify the participant’s emotions will become important in evaluating engineering design processes in CAD environments.

In this experiment, the emotions output by the fuzzy models were manually checked against the users’ activity videos. 26 out of 31 copies of the sample data showed that the emotions output correlated closely with the user activities (Table 2). However, the sample size was too small to statistically prove the correlation. Therefore, for the second case study, lessons learnt from the pilot study were applied. The authors improved the questionnaire used in Case Study 2 to quantify the emotions ratings making it possible to carry out a statistical analysis on the sample set.

The task involved in this experiment only focused on the preliminary stage of the design process with modeling tasks; therefore the second set of experiments were designed to include configuration and optimization tasks to explore the potential of establishing fuzzy models on a variety of engineering tasks associated with CAD environments. The case study on the second experiment is detailed in Section  4.2.

Since the recorded EEG signals were only acquired from the frontal lobe, F3 and F4 in Fig. 2, the noise removal method was limited. The authors employed the Chebyshev filter to take the advantage of the sharp edges it provides on the passing band; however, it compromised the data quality on the passing band with more ripples. Fig. 17
                            illustrates the magnitude, phase and frequency response of the filter that was used during the pre-processing of the EEG signals. It shows the compromise made and the potential affect on the quality of the information on the alpha band. Therefore it was decided to use a full 10–20 system during the next experiment to support and enhance a better noise removal capability.

In conclusion, this first experiment not only enabled the development and validation of the approach but also demonstrated the considerable potential of using an engineer’s psycho-physiological signals to evaluate their emotions while working in an engineering CAD environment with, albeit, subjective relationships identified between FIS objective and questionnaire/interview subjective emotional measures. This has also introduced the possibility of capturing an engineer’s emotions without interruption to their design activities, addressing Research Objectives 1 and 2 as well as answering Research Questions 1 and 2. The next case study employs the same setup as the established fuzzy models and focuses on configuration and optimization tasks with improved pre-processing methods, as well as quantified statistical analysis.

The main experiment involved 24 users configuring the 3D model of a welded bracket using Siemens NX 7.5  [61] with psycho physiological properties measured using NeXus-32 biofeedback device  [60]. The NeXus-32 was used to measure the user’s heart rate (HR), horizontal and vertical eye movements (EOG) and electroencephalography (EEG). The HR is monitored using 2 sensors in the area of the chest; the placements of the electrodes are shown in Fig. 3. Brain activity (EEG) is measured by way of an instrumented cap with 19 electrodes using the 10–20 system  [22]. The users’ psycho-physiological signals were automatically time-synchronized with their activity log files from Siemens NX and the corresponding mouse clicks produced when interacting with the CAD system as were user screen videos using a synchronous logging architecture developed specifically for the project  [63].

The experiment lasted for approximately 60 min and consisted of two sessions with two levels of difficulty in each session. The participants were asked to optimize the design of an existing bracket to meet the functional requirements detailed in a design specification. As part of this activity they used a parametric CAD design system. Each session comprises two different user interface (UI) settings as shown in Fig. 18
                           . Each UI consists of two levels with increasing difficulty. There is no time restriction for the user to complete the task. The tasks are detailed as follows:


                           
                              
                                 1.
                                 Level 1: participant optimizes the given bracket to meet the stress requirements using a provided stock list; this requires each participant to calculate the tensile and shear stress based on the bracket material and then configure–through the interface–the cross-section (breadth–thickness) of the bracket.

Level 2: this level involves designing a weld in addition to repeating the first level but with different bracket functional requirements. The participants must select the weld types and calculate their size.

The experimental steps are as follows:


                           
                              
                                 1.
                                 Each participant was given some general instruction and practice on the stress analysis of a bracket to familiarize themselves with the equations required for the experiment.

A general questionnaire was then filled in to collect the demography of the user, i.e. personal details, work background and CAD experience.

The participant starts with first of the UIs they are allocated and completes the bracket design at both levels.

The participant fills in an emotion-ranking questionnaire relating to the UI.

Step 3 is repeated for the remaining UI.

Step 4 is repeated for the remaining UI.

A questionnaire is filled in on the UIs’ designs and how they affected the solution as well as the design process and their design experience.


                           Fig. 19
                            illustrates the emotion questionnaire used in the reported work. It employs the Likert-type scale of 0 (low)–10 (high) for each of the selected emotions: frustration, satisfaction, engagement, and challenge. Reasons and explanations of the ratings were also required in the blank field under the ratings at the same time. The experiment ends with an interview involving the participant on the design process experienced and the emotion changes they experienced during the process.

To distinguish the affect experiences the experiment was conducted using the conventional NX interface and an NX interface embedded with game element/game mechanics. Essentially, the game-based interface used here is to act as a differing stimulus, potentially influencing the emotional state and cognitive processes of designers, for comparisons with traditional CAD parametric interfaces. However, the comparison of two UIs is not within this paper’s remit or intention, since it specifically focuses on validation of the emotion monitoring model.

The recorded files were separated into four parts with regard to a combination of both interfaces and both levels, namely CADT1, CADT2, CADG1 and CADG2 respectively, where CADT refers to the traditional interface (Fig. 18(a)) and CADG represents the game-like interface (Fig. 18(b)). All the files were validated by visual inspection according to the notes taken during the experiment. 64 samples were found to be valid for further analysis from 16 participants. Fig. 21 demonstrates the pre-processing method and procedure of the captured data.

The EEG signals were recorded through 19 mono-polar electrodes on the Nexus-32 (Fig. 13, right) with EOG and ECG signals. The signals were imported into EEGLAB  [64] and, firstly, the DC noise removed using the built-in function as follows: 
                              
                                 (3)
                                 
                                    E
                                    E
                                    G
                                    
                                       (
                                       :
                                       ,
                                       i
                                       )
                                    
                                    =
                                    E
                                    E
                                    G
                                    
                                       (
                                       :
                                       ,
                                       i
                                       )
                                    
                                    −
                                    
                                    m
                                    e
                                    a
                                    n
                                    
                                       (
                                       E
                                       E
                                       G
                                       
                                          (
                                          :
                                          ,
                                          i
                                          )
                                       
                                       )
                                    
                                    .
                                 
                              
                            Two different approaches for noise removal were then compared to determine their suitability for this application:


                           
                              
                                 1.
                                 Firstly, two IIR Butterworth filters were employed, an 8-order high pass filter and an 8-order low pass filter. Since the edge of Butterworth filter is not as sharp as that of Chebyshev filter, the EEG signals were filtered from 1 to 50 Hz. However, as shown in Fig. 20
                                    , Butterworth filters have fewer ripples in the passing band which reduced the EMG artefacts from muscles, especially from neck and facial movements. There was also less loss on the alpha band caused by the filters, although it does not remove any eye movement artefact that may lie in the same frequency band. This could be a key disadvantage.

The second approach was based on independent component analysis (ICA) of the EEG signals, the default ICA analysis from EEGLAB being used to generate 19 independent components from the EEG sample as shown in Fig. 22
                                    . The artefact related components were then identified by ADJUST and removed from EEG samples  [65]. However, this approach is time and resource consuming. Given the nature of ICA methods, the more channels involved in the analysis the better the spatial features extracted from the EEG samples.

After each noise removal approach was applied, the F3 and F4 from both approaches were taken forward to be further analyzed. The sample rate of the EEG signals was 2048 samples/s so that an 8192-point FFT was performed on the subtracted alpha wave. Thus a frequency spectrum with 0.25 Hz resolution was again obtained over a frame of 4 s. After the FFT the frequency corresponding to the peak power of every 4 s was found and this peak frequency fed into the fuzzy model.

In general, the ECG signal consists of the different amplitudes of waves and the corresponding intervals between them. Extracting the features of amplitudes and intervals, the condition of the heart can be determined. A normal ECG has the following amplitude features: P-wave 0.25 mV, R-wave 1.6 mV, Q-wave 25% of the R-wave, T-wave 0.1–0.5 mV; and time features: PR-interval 0.12−−0.2 s, QRS complex 0.04 −0.12 s, QT-interval <0.42 s and the heart rate of 60–100 beats/min. The QRS complex is the most significant wave of an ECG signal; the heart rate and irregularity of heartbeat can be determined by calculating the R–R interval of the ECG signal (Fig. 23
                           )  [65].

The heart rate was extracted from the ECG signal from the participant’s chest using a discrete wavelet transform.  [66]. The ECG signal was recorded at 2048 samples/s and then re-sampled at 512 Hz. The mother wavelet that was selected for decomposition is Dabachies6 (db6). The decomposed signal was reconstructed to 16 approximations and detail coefficients, such as A1, A2…A8 and D1, D2…D8 as shown in Fig. 24
                           .

A8 and D8 are the components with the lowest frequency and are removed for de-trending purposes as shown in Eq. (2). The de-noise threshold was selected using the global thresholding option, which is derived from Donoho–Johnstone fixed form thresholding strategy for an un-scaled white noise. The de-noised ECG signal is again decomposed and the highest frequency components D1, D2 are discarded. The R–R interval of de-noised ECG signal was detected only among D3, D4, and D5, which covers the possible heart rate range (Fig. 25
                            “De-noised ECG”).


                           
                              
                                 
                                    (4)
                                    
                                       D
                                       e
                                       -
                                       t
                                       r
                                       e
                                       n
                                       d
                                       i
                                       n
                                       g
                                       
                                       E
                                       C
                                       G
                                       =
                                       E
                                       C
                                       G
                                       −
                                       
                                          (
                                          A
                                          8
                                          +
                                          D
                                          8
                                          )
                                       
                                    
                                 
                                 
                                    (5)
                                    
                                       D
                                       e
                                       -
                                       n
                                       o
                                       i
                                       s
                                       i
                                       n
                                       g
                                       
                                       E
                                       C
                                       G
                                       =
                                       D
                                       3
                                       +
                                       D
                                       4
                                       +
                                       D
                                       5
                                       .
                                    
                                 
                              
                            The R wave is the highest amplitude amongst all the waves; hence, it can be made more noticeable by squaring the de-noised signal and applying a low limit to remove pseudo peaks. Subsequently, all the R-wave-like peaks can be checked again to combine all the peak points on the same R wave, which assumes that the normal heart rate is between 60 and 100 beats/min. The heart rate is calculated as the reciprocal for the R–R interval, which is the time between two R waves. The average of heart rate is then calculated for every 4 s and normalized as an input for the fuzzy model.

The membership functions of both inputs of the psycho-physiological signals onto the arousal–valence space model are adjusted for each individual participant as in the pilot study experiment. The overall distributions of the signals are taken as the baseline of each participant. The rules remain the same for all the participants, which are generated according to the previously developed emotion mapping onto the Affect Grid. Alpha waves from both pre-process approaches are used to generate two sets of emotions for each task for each participant. Continuous emotions are modeled from the outputs of this fuzzy model using the centroid defuzzification method and the outputs correlated to the subjective rating from each individual as an evaluation of the pre-process method on the EEG signal. A detailed discussion is given in the following section.

The emotions are plotted automatically along with the user activities during the design process, most of the emotions spikes are correlated to specific user activity on the temporal axis. An example of the correlations is shown in Fig. 26
                           .


                           Fig. 26 illustrates the emotion changes of participant 17 during the first task of game-like interface. For the first 70 s, the user familiarized himself with the interface and at about 80 to 100 s he randomly picked up one cross-section available on the stock list; this failed the design requirement. There are two medium high spikes that showed his frustration whereas, at the same time, and all the other three outputs showed a trough. At 110–120 s, the participant crashed the parametric interface and was instructed how to restore it; consequently, medium high spike showed his frustration. After guessing, the participant began to look for the equations and prepared to use calculations to find the solution. At 190–200 s, the participant was frustrated with the design of the interface, as requirements of the task and available configuration of the bracket are not on the same dialogue box, which only allows one to be seen at a time. He tried to memorize all the given parameters and began calculating at about 220 s. At 254 s, the participant selected the cross-section according to his calculation and applied the value to configure the bracket correctly. He received a tick on the interface to indicate that he had met and completed one of the two requirements. Two high peak spikes showed his satisfaction with a drop in the other three emotion outputs. At the end of the task, the participant was frustrated when he could not figure out how to meet the other requirement. However, he guessed and chose an available configuration from the dropdown box in the interface and his configuration which, luckily, met all the requirements. Hence a sharp high peak spike comes in the frustration and the lowest trough in challenge appeared. After the configuration was selected then the satisfaction was back to about average.

This example and the previous pilot study show how the emotion spikes can be obtained and are also correlated to user activities in their respective CAD environments. This again answers the first two research questions by demonstrating that psycho-physiological signals can be used to quantify an engineer’s emotions during the operation of CAD systems and can be mapped onto CAD activities.

The continuous emotion outputs of each participant from both UIs are averaged to be correlated to the subjective reported ratings. The averaged outputs and reported ratings are listed in Tables 3 and 4
                           
                           .

The average outputs of each interface were tested against the subjective emotion ratings (0–10) obtained from the participants using Wilcoxon signed rank test  [63]. The correlations between fuzzy model outputs and reported emotions are 84.18% (Frustration), 76.83% (Satisfaction), 97% (Engagement) and 97.99% (Challenge) respectively (shown in Table 5
                           ). This statistical mapping answers the third research question by triangulating the user feedback data with the experimentally evaluated emotions. The high correlations indicate that there is potential to employ psycho-physiological signals to capture user emotions in a CAD environment without interruptions during the design process. It provides continuous outputs of emotion status rather than a subjective report from users at discrete point during design process.

In the pre-processing section, two methods were presented and compared for the EEG signals and two sets of the emotion outputs were generated by feeding in the alpha wave features into the same fuzzy model. Both outputs were correlated to the subjective emotion ratings with the results shown in Table 5. On average, the two methods performed very similarly with a maximum 6% difference. Hence, both methods are good enough to provide reasonable outputs of the fuzzy model for a post-processing approach. However, the ICA method consumes much more time and computing resources whereas the simpler alternative IIR filtering provides very similar results. Therefore, a simple IIR Butterworth filtering method as outlined in Section  4.1.3 is recommended as the preferred pre-processing approach.

In terms of the first research objective, it has been shown that the emotional changes in CAD users can be monitored during engineering design tasks.

In this experiment, the authors explored the potential to apply the established fuzzy logic models to configuration and optimization engineering tasks in NXTM. The outcomes of the analysis have proven that it is possible to capture an engineer’s emotional state with the type of methodology and model proposed. This has also been shown to be repeatable, as per the second research objective.

With regard to the third and fourth research objectives, the emotions were validated against user feedback and correlated statistically.

@&#CONCLUSION@&#

Current literature and associated analysis relating to psycho-physiological aspects is clinic-based; there is no solid theoretical foundation of psycho-physiological analysis in the field of engineering design or engineering task analysis during actual CAD operation. In this study, the potential for employing an engineer’s psycho-physiological signals in understanding their emotion state when using CAD has been demonstrated and can be used for unobtrusively and continuously capturing an engineer’s emotions without interrupting the design process.

Emotion expression plays an essential role of human communications, perception and decision making. This work shows that the potential now exists for design processes and design packages to be evaluated in a more vivid, natural and repeatable way. Mapping the affective psycho-physiological attributes onto a series of decision making activities in CAD-based engineering design where engineering behavior can be more deeply understood is now feasible and, consequently, more natural and intuitive CAD user interfaces can potentially be developed and tested.

The case studies have also demonstrated the possibility of using psycho-physiological signals to capture emotions states from engineers carrying out engineering CAD modeling including configuration and optimization tasks. The continuous emotion outputs were triangulated using interview notes and also statistically to the subjective questionnaire ratings. This enabled the affective discrimination of how users interact with design rules and other formal properties such as goals, actions and strategies within a design state.

Individuals have preferences for different types of stimuli ranging from visual, auditory to tactile. The fuzzy approach neatly addresses the diversity and provides a structured comparison between human affect modes and the components integral to CAD. While intrusive at this stage for the purpose of human-factors and usability studies, the method provides a transformative technology towards non-intrusive intelligent systems.

Although at an early stage, these important studies demonstrate a first attempt at addressing important affective computing issues in engineering design domains with the psycho-physiological approach relating to problem solving either participating singularly or in collaboration.

The continuous outputs show the potential to recognize, interpret and process human emotions in order to adapt the user’s behaviors in computer-based systems to elicit appropriate responses. Moreover, the human emotional state can now play an essential role in evaluating the interaction experiences within a digital engineering tool environment, whether it is, say, CAD or virtual reality.

Overall, it can be concluded that the original aim of this research has been achieved, i.e. that the CAD user affective psycho-physiological signals can determine their emotional state and that these can be mapped onto specific CAD tasks.

With regard to engineering design and CAD, future research will now be carried out to investigate how these repeatable methods can be applied to providing a deeper understanding of design methods, processes and practice as well as a means of evaluating design quality, engineers’ confidence levels and solution integrity. Consequently, they will also be used in the domain of design knowledge/rationale capture and reuse with the hope of providing an early indication of how the engineer’s psychological state may point to a means of automatically capturing decision cause and effect relationships and key decision making rationale during design activities.

@&#ACKNOWLEDGEMENTS@&#

This work is supported by the Heriot-Watt University Innovative Manufacturing Research Centre which is funded by the UK Engineering and Physical Sciences Research Council (Grant Number: EP/F02553X/1). This project is also partially funded under the European Community Seventh Framework Programme (FP7/2007 2013), Grant Agreement nr. 258169.

@&#REFERENCES@&#

