@&#MAIN-TITLE@&#The feature selection bias problem in relation to high-dimensional gene data

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           We analyze seven gene datasets to show the feature selection bias effect on the accuracy measure.


                        
                        
                           
                           We examine its importance by an empirical study of four feature selection methods.


                        
                        
                           
                           For evaluating feature selection performance we use double cross-validation.


                        
                        
                           
                           By the way, we examine the stability of the feature selection methods.


                        
                        
                           
                           We recommend cross-validation for feature selection in order to reduce the selection bias.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Feature selection bias

Convex and piecewise linear classifier

Support vector machine

Gene selection

Microarray data

@&#ABSTRACT@&#


               
               
                  Objective
                  Feature selection is a technique widely used in data mining. The aim is to select the best subset of features relevant to the problem being considered. In this paper, we consider feature selection for the classification of gene datasets. Gene data is usually composed of just a few dozen objects described by thousands of features. For this kind of data, it is easy to find a model that fits the learning data. However, it is not easy to find one that will simultaneously evaluate new data equally well as learning data. This overfitting issue is well known as regards classification and regression, but it also applies to feature selection.
               
               
                  Methods and materials
                  We address this problem and investigate its importance in an empirical study of four feature selection methods applied to seven high-dimensional gene datasets. We chose datasets that are well studied in the literature—colon cancer, leukemia and breast cancer. All the datasets are characterized by a significant number of features and the presence of exactly two decision classes. The feature selection methods used are ReliefF, minimum redundancy maximum relevance, support vector machine-recursive feature elimination and relaxed linear separability.
               
               
                  Results
                  Our main result reveals the existence of positive feature selection bias in all 28 experiments (7 datasets and 4 feature selection methods). Bias was calculated as the difference between validation and test accuracies and ranges from 2.6% to as much as 41.67%. The validation accuracy (biased accuracy) was calculated on the same dataset on which the feature selection was performed. The test accuracy was calculated for data that was not used for feature selection (by so called external cross-validation).
               
               
                  Conclusions
                  This work provides evidence that using the same dataset for feature selection and learning is not appropriate. We recommend using cross-validation for feature selection in order to reduce selection bias.
               
            

@&#INTRODUCTION@&#

Microarray technology has been one of the most important technological developments in biology in recent years [1]. It aims at the measurement of mRNA levels in particular cells or tissues for many genes at once. The application of microarray technology is very wide, ranging from the understanding of underlying biological mechanisms [2], the examining of drug response [3], the discovering of novel subgroups of diseases [4,5], the classifying of patients into disease groups [4] to the predicting the outcomes of a disease [6,7] and many other potential application areas. One of the major problems with microarray datasets is their very high dimensions. Each object of the dataset is usually described by thousands of genes. The classification or any other exploration task of high-dimensional datasets can be difficult due to the “curse of dimensionality” [8]. The feature selection can help in these cases.

Feature selection is a technique widely used in data mining, and the aim is to select a subset of features from all the available features that are relevant to the problem being considered [9,10]. The best subset should contain the minimum number of features that have the greatest impact on the quality of the model relating to the problem. Other features should be rejected as being irrelevant. In the case of a classification problem, the feature selection algorithm should select a subset of features to obtain the best properties of the classifier, for example, allowing one to obtain the best separation of decision-making classes measured on the basis of the established criterion.

There are three major categories of feature selection methods—filter, wrapper and embedded. Filter methods use some of the internal properties of the data to select and score features, which is usually done on an individual basis. Examples of such properties are feature dependence, redundancy and the entropy of distances between data points. In wrapper methods, the feature selection is connected with another data analysis technique, usually classification. The accompanying technique helps in evaluating the quality of the selected features sets. Embedded methods of feature selection integrate the selection in model building. An example of such a method is the decision tree induction algorithm, where a feature has to be selected at each node [10]. When feature subspace is selected the next step is to build a classifier and evaluate its performance.

It is not appropriate to use the same data to build a model and to evaluate its performance. The performance measure obtained that way constitutes a positive bias. For new data, we would probably see a worsening in performance. In the case of feature selection, it is necessary to select features from the whole dataset and then build the decision model (e.g. classifier) and estimate its performance vis-à-vis the selected features. Even if cross-validation is used to measure model quality, it is too optimistic because we look at the data before building the model—in this case conducting feature selection. In the literature, it is sometimes called feature selection bias [11] and it also can be seen as an example of so called “data snooping” [12]. Data snooping occurs when a given set of data is used more than once for purposes of inference or model selection—in this case twice, first for feature selection and later for classification. Here, we analyse several commonly used high-dimensional gene datasets to show the feature selection bias effect for quality measure. We measure unbiased accuracy using external cross-validation where the whole process of feature selection is repeated.

The quality of the subset of selected features is usually measured based on the performance of classifiers [10]. A relatively often neglected issue is the stability of feature selection. Such stability could be described as the sensitivity to perturbations in the data. The interest in the stability of feature selection algorithms is because biomarker experts usually prefer small subsets of features. These subsets of features should be relatively non-sensitive to slight changes in the data. Sometimes, classifier performance might be of less interest to them.

The goal of this paper is to present an empirical study where the feature selection bias is shown in these datasets, which are related to medical applications. We propose a framework of calculation that would avoid feature selection bias and present a series of experiments in which we use the latest feature selection methods to analyse genetic datasets. In addition, because our proposed framework of calculation repeatedly uses the same feature selection method to analyse slightly different datasets, we have complemented the study of feature selection bias with a stability analysis in order to verify the quality of solutions: a solution that drastically changes in the presence of a very minor variation of data can be less reliable.

To summarize, in this paper:
                        
                           •
                           We empirically study the feature selection bias in high-dimensional datasets. We analyse seven gene datasets and four feature selection methods to show the feature selection bias effect on the accuracy measure.

We propose a calculation scheme that uses a double cross-validation in order to avoid feature selection bias.

We examine the stability of the feature selection methods.

The rest of the paper is organized as follows: In Section 2, a short overview of related works on feature selection, its stability and bias is given. In Section 3, we detail a feature selection technique based on the minimization of the convex and piecewise linear (CPL) criterion function, the smaller details of the other feature selection algorithms included in the study and also the feature selection stability measure. In Section 4, we describe the datasets used and the experimental setup. The proposed approach is experimentally evaluated on real gene expression data. In Section 5, we present the results of the experiments and an interpretation. Finally, the concluding remarks outline the main findings of this work.

@&#RELATED WORKS@&#

In our study, we use seven high-dimensional gene datasets. There has been extensive work done on those data just to find the few specific genes that could allow appropriate tissue classification. For example, in the colon cancer dataset [13], Zhang et al. [14] discovered only 3 genes that allow for 98.4% accuracy (only 1 wrong assignment out of 62 tissues) using tree-based classification. For the leukemia dataset [4], Guyon et al. [15] achieved 100% accuracy using only two genes. Genes were selected using support vector machine-recursive feature elimination (SVM-RFE) and then cross-validation was performed with 100% accuracy. In all these cases, the authors did not take into account the feature selection bias. Genes were selected from the whole dataset and then cross-validation was performed to estimate classification accuracy. The first authors to notice this problem were Ambroise and McLachlan [11]. They proposed two solutions to the selection bias. The first solution used external cross-validation, and the second used the bootstrap approach. They showed that the selection bias cannot be ignored when estimating true prediction accuracy. They used SVM-RFE as a feature selection method and a linear support vector machine (SVM) as a classifier. For the colon cancer dataset, the bias was very large (above 15%), and for the leukemia dataset, it was smaller (around 5%). Our results are very similar. Singhi and Liu [16] reached quite a different conclusion regarding the feature selection bias. They studied this problem from a Bayesian perspective using synthetic datasets and real text datasets. They concluded that selection bias is not a problem in the case of classification but is a problem in the case of regression. Nevertheless, they noticed that datasets with a small number of examples and a large number of features, such as microarray data, could have a large bias.

Woody et al. [17] studied synthetic datasets with microarray data characteristics. They used two-level external cross-validation, similar to the procedure used in our study. They found it is useful to reduce the bias, which they discovered to be in the range of 3–5%.

Using external cross-validation with feature selection may potentially lead to selecting different features in the external loop. A problem that may occur relates to the stability of the feature selection method. A popular measure of feature selection stability is that proposed by Kuncheva [18]. However, it does not work with subsets with different cardinality. Thus, we use the adjusted stability measure (ASM) proposed by Lustgarten et al. [19].

There are also many embedded feature selection methods that, similar to relaxed linear separability (RLS), treat feature selection as an integral part of learning. One is a method called grafting 
                     [20], which is, like RLS, based on regularization, not only of the L
                     1 norm but also of L
                     2 and L
                     0. In this framework, the authors used a loss function similar to logistic regression. Unlike in RLS, where in each step features are removed (backward elimination), in grafting features are added (forward selection). Two other Bayesian classification algorithms incorporating feature selection were proposed by Li et al. [21]. Experiments run on the colon cancer dataset show that, on average, 15.13 features were selected by the first algorithm with 83.33% accuracy, and 8.555 features were selected with 78.83% accuracy by the second one. Regarding the leukemia dataset, 9.6 features were selected with 86.07% accuracy by the first algorithm, and 6.5 features were selected with 87.15% accuracy by the second one. The split for testing and learning was done 100 times. A different approach to combine the classification and feature selection tasks was proposed by Krishnapuram et al. [22]. Their method, called joint classifier and feature optimization (JCFO), which could be characterized as a Bayesian approach to learning, had 96.8% accuracy for the colon cancer dataset. Tuv et al. [23] proposed an algorithm that uses tree-based ensembles for feature selection and classification. Peralta and Soto [24] proposed the mixture of experts, where each expert uses the L
                     1 norm, similar to RLS. Their accuracy results for the colon cancer and leukemia datasets were 82.0% and 93.1%, respectively.

@&#METHODS@&#

This section describes four feature selection methods used in the experiments. We start with RLS, which is based on a linear classifier obtained by the minimization of the CPL criterion function. We introduce this classifier before the RLS method. The CPL classifier and the RLS method, although already described in the literature (e.g. [25,26]), are presented here to promote techniques we helped develop. Next, we briefly introduce the three other feature selection methods. At the end, we present the feature selection stability measure ASM.

Let us consider data represented as m feature vectors x
                        
                           j
                        
                        =[x
                        
                           j1, …, x
                        
                           jn
                        ]
                           T
                         (j
                        =1, …, , m) of the same dimensionality n. The components x
                        
                           ji
                         of the vectors x
                        
                           j
                         are called features.

Let us take into consideration two disjoined sets, C
                        + and C
                        −, containing m feature vectors x
                        
                           j
                        :
                           
                              (1)
                              
                                 
                                    C
                                    +
                                 
                                 ∩
                                 
                                    C
                                    −
                                 
                                 =
                                 ∅
                                 .
                              
                           
                        For example, the m
                        + vectors from the set C
                        + describe patients suffering from a certain disease and the m
                        − vectors from the set C
                        − describe patients without the disease (m
                        =
                        m
                        +
                        +
                        m
                        −).

We consider the linear separation of the sets C
                        + and C
                        − by the hyperplane H(w, θ):
                           
                              (2)
                              
                                 H
                                 (
                                 
                                    
                                       w
                                    
                                 
                                 ,
                                 θ
                                 )
                                 =
                                 {
                                 
                                    
                                       x
                                    
                                 
                                 :
                                 
                                    
                                       
                                          w
                                       
                                    
                                    T
                                 
                                 
                                    
                                       x
                                    
                                 
                                 =
                                 θ
                                 }
                                 ,
                              
                           
                        where 
                           
                              
                                 w
                              
                           
                           =
                           
                              
                                 [
                                 
                                    w
                                    1
                                 
                                 ,
                                 
                                 …
                                 ,
                                 
                                 
                                    w
                                    n
                                 
                                 ]
                              
                              T
                           
                           ∈
                           
                              
                                 
                                    R
                                 
                              
                              n
                           
                         is the weight vector, and θ
                        ∈
                        R
                        1 is the threshold.

The sets C
                        + and C
                        − are linearly separable if and only if they can be fully separated by some hyperplane H(w, θ):
                           
                              (3)
                              
                                 (
                                 ∃
                                 
                                    
                                       w
                                    
                                 
                                 ,
                                 θ
                                 )
                                 
                                 (
                                 ∀
                                 
                                    
                                       
                                          x
                                       
                                    
                                    j
                                 
                                 ∈
                                 
                                    C
                                    +
                                 
                                 )
                                 
                                    
                                       
                                          w
                                       
                                    
                                    T
                                 
                                 
                                    
                                       
                                          x
                                       
                                    
                                    j
                                 
                                 >
                                 θ
                                 
                                 
                                    
                                       and
                                    
                                 
                                 
                                 (
                                 ∀
                                 
                                    
                                       
                                          x
                                       
                                    
                                    j
                                 
                                 ∈
                                 
                                    C
                                    −
                                 
                                 )
                                 
                                    
                                       
                                          w
                                       
                                    
                                    T
                                 
                                 
                                    
                                       
                                          x
                                       
                                    
                                    j
                                 
                                 <
                                 θ
                                 .
                              
                           
                        
                     

If the sets C
                        + and C
                        − are linearly separable, there are many hyperplanes H(w, θ) (2) dividing them [27]. In order to obtain the model with generalizability, the optimal hyperplane H(w*
                        , θ
                        *) should be found.

The hyperplane H(w*
                        , θ
                        *) could be calculated by the minimization of the criterion function Φ
                           λ
                        (w, θ) [27]:
                           
                              (4)
                              
                                 
                                    Φ
                                    λ
                                 
                                 (
                                 
                                    
                                       w
                                    
                                 
                                 ,
                                 θ
                                 )
                                 =
                                 
                                    ∑
                                    
                                       
                                          
                                             
                                                x
                                             
                                          
                                          j
                                       
                                       ∈
                                       
                                          C
                                          +
                                       
                                    
                                 
                                 
                                    φ
                                    j
                                    +
                                 
                                 (
                                 
                                    
                                       w
                                    
                                 
                                 ,
                                 θ
                                 )
                                 +
                                 
                                    ∑
                                    
                                       
                                          
                                             
                                                x
                                             
                                          
                                          j
                                       
                                       ∈
                                       
                                          C
                                          −
                                       
                                    
                                 
                                 
                                    φ
                                    j
                                    −
                                 
                                 (
                                 
                                    
                                       w
                                    
                                 
                                 ,
                                 θ
                                 )
                                 +
                                 λ
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    n
                                 
                                 |
                                 
                                    w
                                    i
                                 
                                 |
                                 ,
                              
                           
                        where λ
                        ≥0.

The function Φ
                           λ
                        (w, θ) is the sum of the penalty functions 
                           
                              φ
                              j
                              +
                           
                           (
                           
                              
                                 
                                    w
                                    ,
                                    θ
                                 
                              
                           
                           )
                         and 
                           
                              φ
                              j
                              −
                           
                           (
                           
                              
                                 w
                              
                           
                           ,
                           θ
                           )
                         and the L
                        1 regularization function 
                           λ
                           
                              ∑
                              
                                 i
                                 =
                                 1
                              
                              n
                           
                           |
                           
                              w
                              i
                           
                           |
                        . The functions 
                           
                              φ
                              j
                              +
                           
                           (
                           
                              
                                 w
                              
                           
                           ,
                           θ
                           )
                         are defined on the feature vectors x
                        
                           j
                         from the set C
                        +. Similarly, 
                           
                              φ
                              j
                              −
                           
                           (
                           
                              
                                 w
                              
                           
                           ,
                           θ
                           )
                         are based on the elements x
                        
                           j
                         of the set C
                        −.
                           
                              (5)
                              
                                 (
                                 ∀
                                 
                                    
                                       
                                          x
                                       
                                    
                                    j
                                 
                                 ∈
                                 
                                    C
                                    +
                                 
                                 )
                                 
                                 
                                    φ
                                    j
                                    +
                                 
                                 (
                                 
                                    
                                       
                                          w
                                          ,
                                          θ
                                       
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   1
                                                   +
                                                   θ
                                                   −
                                                   
                                                      
                                                         
                                                            w
                                                         
                                                      
                                                      T
                                                   
                                                   
                                                      
                                                         
                                                            x
                                                         
                                                      
                                                      j
                                                   
                                                
                                                
                                                   if
                                                
                                                
                                                   
                                                      
                                                         
                                                            w
                                                         
                                                      
                                                      T
                                                   
                                                   
                                                      
                                                         
                                                            x
                                                         
                                                      
                                                      j
                                                   
                                                   <
                                                   1
                                                   +
                                                   θ
                                                
                                             
                                             
                                                
                                                   0
                                                
                                                
                                                   if
                                                
                                                
                                                   
                                                      
                                                         
                                                            w
                                                         
                                                      
                                                      T
                                                   
                                                   
                                                      
                                                         
                                                            x
                                                         
                                                      
                                                      j
                                                   
                                                   ≥
                                                   1
                                                   +
                                                   θ
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        and
                           
                              (6)
                              
                                 (
                                 ∀
                                 
                                    
                                       
                                          x
                                       
                                    
                                    j
                                 
                                 ∈
                                 
                                    C
                                    −
                                 
                                 )
                                 
                                 
                                    φ
                                    j
                                    −
                                 
                                 (
                                 
                                    
                                       
                                          w
                                          ,
                                          θ
                                       
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   1
                                                   −
                                                   θ
                                                   +
                                                   
                                                      
                                                         
                                                            w
                                                         
                                                      
                                                      T
                                                   
                                                   
                                                      
                                                         
                                                            x
                                                         
                                                      
                                                      j
                                                   
                                                
                                                
                                                   if
                                                
                                                
                                                   
                                                      
                                                         
                                                            w
                                                         
                                                      
                                                      T
                                                   
                                                   
                                                      
                                                         
                                                            x
                                                         
                                                      
                                                      j
                                                   
                                                   >
                                                   −
                                                   1
                                                   +
                                                   θ
                                                
                                             
                                             
                                                
                                                   0
                                                
                                                
                                                   if
                                                
                                                
                                                   
                                                      
                                                         
                                                            w
                                                         
                                                      
                                                      T
                                                   
                                                   
                                                      
                                                         
                                                            x
                                                         
                                                      
                                                      j
                                                   
                                                   ≤
                                                   −
                                                   1
                                                   +
                                                   θ
                                                
                                             
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

The criterion function Φ
                           λ
                        (w, θ) (4), as the sum of the CPL penalty functions 
                           
                              φ
                              j
                              +
                           
                           (
                           
                              
                                 w
                              
                           
                           ,
                           θ
                           )
                         
                        (5), 
                           
                              φ
                              j
                              −
                           
                           (
                           
                              
                                 w
                              
                           
                           ,
                           θ
                           )
                         
                        (6) and 
                           
                              ∑
                              
                                 i
                                 =
                                 1
                              
                              n
                           
                           |
                           
                              w
                              i
                           
                           |
                        , is also the CPL function. The basis exchange algorithm allows us to find the minimum efficiently, even in the case of large multidimensional datasets C
                        + and C
                        − 
                        [28]:
                           
                              (7)
                              
                                 
                                    Φ
                                    λ
                                    *
                                 
                                 =
                                 
                                    Φ
                                    λ
                                 
                                 (
                                 
                                    
                                       
                                          
                                             w
                                             *
                                          
                                       
                                    
                                 
                                 ,
                                 
                                    θ
                                    *
                                 
                                 )
                                 =
                                 min
                                 
                                 
                                    Φ
                                    λ
                                 
                                 (
                                 
                                    
                                       w
                                    
                                 
                                 ,
                                 θ
                                 )
                                 ≥
                                 0
                                 .
                              
                           
                        
                     

The vector of the parameter 
                           
                              w
                              *
                           
                         and the parameter θ
                        * define the optimal hyperplane H(w*
                        , θ
                        *).

Let us introduce the class labels y
                        
                           j
                        , defined as follows:
                           
                              (8)
                              
                                 (
                                 ∀
                                 
                                    
                                       
                                          x
                                       
                                    
                                    j
                                 
                                 ∈
                                 
                                    C
                                    +
                                 
                                 )
                                 
                                    y
                                    j
                                 
                                 =
                                 1
                                    
                                 and
                                    
                                 (
                                 ∀
                                 
                                    
                                       
                                          x
                                       
                                    
                                    j
                                 
                                 ∈
                                 
                                    C
                                    −
                                 
                                 )
                                 
                                    y
                                    j
                                 
                                 =
                                 −
                                 1
                                 .
                              
                           
                        
                     

This allows us to present the penalty functions 
                           
                              φ
                              j
                              +
                           
                         
                        (5) and 
                           
                              φ
                              j
                              −
                           
                         
                        (6) in a form similar to the SVM formula:
                           
                              (9)
                              
                                 (
                                 ∀
                                 
                                    
                                       
                                          x
                                       
                                    
                                    j
                                 
                                 ∈
                                 
                                    C
                                    +
                                 
                                 ∪
                                 
                                    C
                                    −
                                 
                                 )
                                 
                                 
                                    φ
                                    j
                                 
                                 (
                                 
                                    
                                       
                                          w
                                          ,
                                          θ
                                       
                                    
                                 
                                 )
                                 =
                                 max
                                 (
                                 0
                                 ;
                                 1
                                 −
                                 
                                    y
                                    j
                                 
                                 (
                                 
                                    
                                       
                                          w
                                       
                                    
                                    T
                                 
                                 
                                    
                                       
                                          x
                                       
                                    
                                    j
                                 
                                 +
                                 θ
                                 )
                                 )
                                 ,
                              
                           
                        
                        
                           
                              (10)
                              
                                 
                                    Φ
                                    λ
                                 
                                 (
                                 
                                    
                                       w
                                    
                                 
                                 ,
                                 θ
                                 )
                                 =
                                 
                                    ∑
                                    
                                       j
                                       =
                                       1
                                    
                                    m
                                 
                                 
                                    
                                       φ
                                       j
                                    
                                    (
                                    
                                       
                                          
                                             w
                                             ,
                                             θ
                                          
                                       
                                    
                                    )
                                 
                                 +
                                 λ
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    n
                                 
                                 |
                                 
                                    w
                                    i
                                 
                                 |
                                 .
                              
                           
                        
                     

The linear SVM criterion function has the form:
                           
                              (11)
                              
                                 
                                    Φ
                                    λ
                                    SVM
                                 
                                 (
                                 
                                    
                                       w
                                    
                                 
                                 ,
                                 θ
                                 )
                                 =
                                 
                                    ∑
                                    
                                       j
                                       =
                                       1
                                    
                                    m
                                 
                                 
                                    
                                       φ
                                       j
                                    
                                    (
                                    
                                       
                                          
                                             w
                                             ,
                                             θ
                                          
                                       
                                    
                                    )
                                 
                                 +
                                 λ
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    n
                                 
                                 
                                    w
                                    i
                                    2
                                 
                                 .
                              
                           
                        
                     

In the case of the SVM criterion function, the regularization part is the quadratic function, so the whole function is not a function of the CPL type. Quadratic programming techniques are used in its minimization process. There are modifications of the form of the SVM criterion function (11). Some are very similar to the CPL formula (10). 1-norm SVM [29] is even equivalent to the CPL formula (10), despite the fact that their genesis are different [30,28].

The RLS approach to the feature selection problem refers to the concept of the linear separability of the learning sets (3) 
                        [31]. The term “relaxation” here means the deterioration of the linear separability due to the gradual neglecting of selected features. The considered approach to feature selection is based on repetitive minimization of the CPL criterion functions Φ
                           λ
                        (w, θ) (4).

The RLS feature selection method consists of three stages [25,26]. The first stage is to determine an optimal hyperplane H(w
                        *, θ
                        *) separating objects x
                        
                           j
                         from the learning sets C
                        + and C
                        −. This stage results in the optimal hyperplane H(w
                        *, θ
                        *) and an initial feature set F
                        
                           k
                         composed of k features. The feature set F
                        
                           k
                         includes the features corresponding to non-zero weights 
                           
                              w
                              i
                           
                        . The rest of the n
                        −
                        k features correspond to weights 
                           
                              w
                              i
                           
                         equal to zero. In the case of high-dimensional data, there are a lot of features with 
                           
                              w
                              i
                           
                         equal to zero.

In the second stage, the value of the cost level λ in the criterion function Φ
                           λ
                        (w, θ) (4) is successively increased. This causes the reduction of some features x
                        
                           i
                         as a result of the zeroing of corresponding weights 
                           
                              w
                              i
                           
                        . It is possible to determine the value of Δ
                           k
                         (Δ
                           k
                        
                        >0), which will result in the reduction of only one feature from the feature set F
                        
                           k
                        . As a result of the second stage, we obtain the descended sequence of feature subsets F
                        
                           k
                         with decreased dimensionality (F
                        
                           k
                        
                        ⊃
                        F
                        
                           k−1):
                           
                              (12)
                              
                                 
                                    F
                                    k
                                 
                                 →
                                 
                                    F
                                    
                                       k
                                       −
                                       1
                                    
                                 
                                 →
                                 …
                                 →
                                 
                                    F
                                    1
                                 
                              
                           
                        
                     

The last step is the calculation of the classifier accuracy in each reduced dataset corresponding to the subsets of features in sequence (12). The accuracy is determined by the CPL classifier [27] in a cross-validation procedure. As a result of the third stage and the whole RLS method, we obtain the feature set F
                        *. This is the feature set characterized by the greatest accuracy of classifier.

The RLS method and three other feature selection algorithms were used in the experiments. The first of the other algorithms, ReliefF, is based on a feature ranking procedure proposed by Kononenko [32] as an extension of the Relief algorithm [33]. The ReliefF searches for the nearest objects from different classes and weight features according to how well they differentiate these objects.

The second one is SVM-RFE. This is a relatively new idea. The SVM-RFE is an iterative procedure that works backwards from an initial set of features. In each round, it fits a simple linear SVM, ranks the features based on their weights in the SVM solution and eliminates the feature with the lowest weight [15].

The third algorithm, minimum redundancy maximum relevance (MRMR) [34], is also a relatively new idea. It is based on a feature-ranking procedure with a special ranking criterion. The position of a single feature on the list depends on both its correlation with the class and its dissimilarity to each feature above it in the ranking.

A feature selection algorithm could be called stable when it is run on many samples from the same dataset and produces similar feature subsets. In our case, we took samples from the leave-one-out procedure so they differ only by one object, meaning they are very similar. We expect very similar feature subspaces from the stable feature selection algorithm. To define the stability measure including all subspaces, one common approach is to define the similarity measure between two feature sets F
                        
                           k
                           
                              i
                           
                         and F
                        
                           k
                           
                              j
                           
                         and then define the stability as the average similarity of all pairs of feature sets [19]. Let S
                        
                           A
                        (F
                        
                           k
                           
                              i
                           
                        , F
                        
                           k
                           
                              j
                           
                        ) be the similarity measure between two features sets, and then average for all pairs, ASM, could be calculated as:


                        
                           
                              (13)
                              
                                 ASM
                                 =
                                 
                                    2
                                    
                                       c
                                       (
                                       c
                                       −
                                       1
                                       )
                                    
                                 
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       c
                                       −
                                       1
                                    
                                 
                                 
                                    ∑
                                    
                                       j
                                       =
                                       i
                                       +
                                       1
                                    
                                    c
                                 
                                 
                                    S
                                    A
                                 
                                 (
                                 
                                    F
                                    
                                       
                                          k
                                          i
                                       
                                    
                                 
                                 ,
                                 
                                    F
                                    
                                       
                                          k
                                          j
                                       
                                    
                                 
                                 )
                                 ,
                              
                           
                        where c is the number of obtained features sets. In our case, c is equal to the number of observations because we use the leave-one-out procedure to cross-validate the features selection method (we call it external cross-validation, which is described later in the paper; the result of each step is the best feature set).

As the similarity measure S
                        
                           A
                         between the two subspaces, we used the measure proposed by Lustgarten et al. [19] as an extension to the Kuncheva [18] similarity measure. It can be applied to two subsets F
                        
                           k
                           
                              i
                           
                         and F
                        
                           k
                           
                              j
                           
                         with a different cardinality k
                        
                           i
                         and k
                        
                           j
                        .


                        
                           
                              (14)
                              
                                 
                                    S
                                    A
                                 
                                 (
                                 
                                    F
                                    
                                       
                                          k
                                          i
                                       
                                    
                                 
                                 ,
                                 
                                    F
                                    
                                       
                                          k
                                          j
                                       
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       r
                                       −
                                       (
                                       
                                          
                                             k
                                             i
                                          
                                          
                                             k
                                             j
                                          
                                       
                                       /
                                       n
                                       )
                                    
                                    
                                       min
                                       (
                                       
                                          k
                                          i
                                       
                                       ,
                                       
                                          k
                                          j
                                       
                                       )
                                       −
                                       max
                                       (
                                       0
                                       ,
                                       
                                          k
                                          i
                                       
                                       +
                                       
                                          k
                                          j
                                       
                                       −
                                       n
                                       )
                                    
                                 
                                 .
                              
                           
                        
                     

The total number of features is n and the number of the same features in two subspaces is r. This measure has a range (−1,1]. To better understand this measure, let us simplify its numerator and denominator to the first parts (the second parts are corrections), 
                           
                              S
                              A
                           
                           =
                           
                              r
                              
                                 min
                                 (
                                 
                                    k
                                    i
                                 
                                 ,
                                 
                                    k
                                    j
                                 
                                 )
                              
                           
                        . When there are none of the same features (r
                        =0), this simplified version has a value of 0. When all features in the smaller subset are in the bigger subset r
                        =
                        min(k
                        
                           i
                        , k
                        
                           j
                        ) then the value is 1, which is the highest possible value. The values are between 0 and 1 when they share only some features. But it can happen just by chance (even in case of random feature selection) that some features are shared. The second part of numerator and denominator accounts for this; they are the so called ‘correction for chance’. The need for such correction can be clearly seen in the case of a very big cardinality k
                        
                           j
                         of the subspace F
                        
                           k
                           
                              j
                           
                         close to, or even equal to all features n. In such case, the second subspace F
                        
                           k
                           
                              i
                           
                        , just by chance, needs to share almost all features with the subspace F
                        
                           k
                           
                              j
                           
                        . Therefore the expected number of same features k
                        
                           i
                        
                        k
                        
                           j
                        /n is subtracted from r in the numerator. For example, when k
                        
                           j
                        
                        =
                        n, we would have r-ki
                        , so the measure would be equal to 0 even if all features in subset F
                        
                           k
                           
                              i
                           
                         were in subset F
                        
                           k
                           
                              j
                           
                        . Values close to 0 mean that such overlaps between subsets are expected, even if features where selected randomly. The S
                        
                           A
                         measure has another property, which is that it could have values less than 0 in cases when even randomly we expect some features to overlap, but we do not observe it, so r is less than the expected k
                        
                           i
                        
                        k
                        
                           j
                        /n.

Regarding the denominator, substraction of max(0, k
                        
                           i
                        
                        +
                        k
                        
                           j
                        
                        −
                        n) is done for situations when k
                        
                           i
                        
                        +
                        k
                        
                           j
                        
                        >
                        n, which means that the minimal number of r is higher than zero. In the case of gene expression data when only a few genes are selected from the thousands, the sum k
                        
                           i
                        
                        +
                        k
                        
                           j
                         is much lower than n.

To summarize, values close to 1 indicate very similar subsets, lower values mean less similarity and values close to 0 indicate no similarity, but they could share some features just by chance. In cases when they share fewer features than expected, the values of this measure could be negative but higher than −1.

We have selected seven microarray datasets: colon cancer 
                        [13], leukemia 
                        [4], lung cancer 
                        [35], breast cancer 
                        [6], central nervous system 
                        [36], diffuse large B-cell lymphoma (DLBCL) [37] and prostate cancer 
                        [38]. All are characterized by a significant number of features, in this case describing the expression of specific genes, and the presence of two decision classes. The selected data are often used by different researchers in an attempt to use feature selection algorithms (see e.g. [15,39]). Apart from the colon cancer dataset, the original sets are, for a variety of reasons, divided into training and testing parts. In our experiment, both parts are combined into a single entity in connection with the applied cross-validation method [40]. Apart from the colon cancer dataset, all the features of the other sets have also been standardized. The features of the colon cancer dataset were originally standardized. Table 1
                         shows the most important parameters of the datasets.

To evaluate the feature selection performance, we use double cross-validation. External cross-validation (Fig. 1
                        ) is used to evaluate the performance of the feature selection method and internal cross-validation (Fig. 3) is used to evaluate the feature selection subspaces and to choose the best one (Fig. 2
                        ). In both cases, we use the leave-one-out method as a cross-validation method.
                     

External leave-one-out means that for each dataset, we select as many different best feature subsets 
                           
                              F
                              j
                              *
                           
                         (j
                        =1, …, m) as the number of objects m in the dataset. Each best feature subset 
                           
                              F
                              j
                              *
                           
                         is obtained from the set of objects C
                        
                           j
                         composed of m
                        −1 objects (all objects beside the O
                        
                           j
                         object). The procedure is repeated m times according to the leave-one-out method. Any two sets C
                        
                           k
                         and C
                        
                           l
                         (k
                        ≠
                        l) differ only by one object (m
                        −2 objects are the same). It should be expected that for very similar datasets C
                        
                           k
                         and C
                        
                           l
                        , we obtain very similar best feature subsets 
                           
                              F
                              k
                              *
                           
                         and 
                           
                              F
                              l
                              *
                           
                        . We measure the similarity between the best feature subspaces using the S
                        
                           A
                         
                        (14) measure.

The internal leave-one-out procedure is used to select the best feature subspace 
                           
                              F
                              j
                              *
                           
                         from many subspaces 
                           
                              F
                              j
                              p
                           
                         (p
                        =1, …, m
                        −1). The subspace 
                           
                              F
                              j
                              p
                           
                         is composed of p top features from a ranking returned by the feature selection algorithm. For p, we choose values from 1 to m
                        −1. We use only the top m
                        −1 features for two reasons. First, it is proven that m
                        −1 objects are linearly separable in m
                        −1 or less dimensional feature space [41]. The second reason is that the RLS procedure returns the maximum feature space with a dimension not greater than the number of objects. This reason is related to the previous one as the RLS uses the linear procedure. Fig. 4
                         is an example of the results for all internal leave-one-out calculations to find the best subspace during each of the external leave-one-out iterations.

The computational complexity of double cross-validation is quite high. It could be measured by the number of built classifiers. In each step of the m
                        −1 internal leave-one-out procedure, we build a linear classifier. In the case of the RLS, it is a classifier based on the CPL criterion function; in other cases, we use a linear SVM criterion function. Internal cross-validation is repeated for each m
                        −1 subspace and all those calculations are repeated m times in an external cross-validation loop. In addition, in the external cross-validation for each step, we build one classifier for all m
                        −1 objects in the best feature subspace 
                           
                              F
                              j
                              *
                           
                        . The number of built classifiers S can be calculated as follows:


                        
                           
                              (15)
                              
                                 S
                                 =
                                 m
                                 (
                                 m
                                 −
                                 1
                                 )
                                 
                                    
                                       n
                                       ˆ
                                    
                                 
                                 +
                                 m
                                 =
                                 O
                                 (
                                 
                                    m
                                    2
                                 
                                 
                                    
                                       n
                                       ˆ
                                    
                                 
                                 )
                                 ,
                              
                           
                        where 
                           
                              n
                              ˆ
                           
                         is the number of used top features from the ranking. In our study, it is always equal to the number of objects for feature evaluation, which is one less than all available objects 
                           
                              
                                 n
                                 ˆ
                              
                           
                           =
                           m
                           −
                           1
                        .


                        
                           
                              (16)
                              
                                 S
                                 =
                                 m
                                 (
                                 m
                                 −
                                 1
                                 )
                                 (
                                 m
                                 −
                                 1
                                 )
                                 +
                                 m
                                 =
                                 O
                                 (
                                 
                                    m
                                    3
                                 
                                 )
                              
                           
                        
                     

@&#RESULTS@&#

The performance of four feature selection methods with linear classifiers was compared for seven high-dimensional gene datasets using an accuracy measure (the percentage of objects correctly classified by the classifier). The most important result shows the test accuracy, which was calculated using an external leave-one-out procedure for the whole feature selection process. As described in Section 4, this measure is not biased by feature selection. On the other hand, validation accuracy, also reported in our results, is biased by feature selection. Validation accuracy is the average of the accuracies of the best subspaces 
                        
                           F
                           j
                           *
                        
                      (j
                     =1, …, m). The difference between test accuracy and validation accuracy could be called the feature selection bias [16]. In our experiments, it is always positive, as expected. For example, for the colon cancer dataset and the SVM-RFE feature selection, the test accuracy is 85.48%, but the validation accuracy is 100%, which gives an almost 15% feature selection bias. This, and our other results (see Tables 2–8
                     
                     
                     
                     
                     
                     
                     ) achieved for microarray datasets, provides evidence that the feature selection bias degraded the classification performance significantly.

What does this mean? When features are selected, and we say that cross-validation accuracy for selected features is equal to a specific value, this value is biased and will be lower for new objects. To calculate expected accuracy, cross-validation for the whole feature selection process needs to be used, not just for selected features. For example, for the breast cancer dataset (see Table 5), two feature selection methods, SVM-RFE and RLS, were able to find low dimension spaces (from 13 to about 30) where cross-validation accuracy was equal to 100%. However, the feature selection method worked on the same data as the cross-validation calculations, resulting in a biased accuracy measure. Unbiased accuracy was obtained by assessing objects excluded from the feature selection process using external cross-validation. In the case of the breast cancer dataset unbiased accuracy was 67.01% for SVM-RFE and 73.20% for RLS. Unbiased accuracy is 30% lower than biased accuracy; thus, feature selection bias equals 30% in this case.

In almost all cases, the accuracy determined with feature selection was lower or similar to the accuracy determined without feature selection. Among the four methods used, there is no single leader. In short, we noticed that ReliefF was the worst in five cases, MRMR and SVM-RFE were the best in three cases and RLS was the best in two cases. Sometimes, the differences were very low, for example for the lung cancer dataset (see Table 4).

The external cross-validation used for validating the feature selection methods produces many feature subspaces. In Tables 2–8, we report the minimum, maximum and average numbers of selected features. The number of selected features is one of the quality measures of the feature selection method. Of two methods with similar accuracy, the one with fewer features is considered the better one. In our experiment, the ReliefF method produces the biggest feature subsets in almost all cases (except the central nervous system dataset). The other methods produce feature subsets with a comparable number of features, with two exceptions: MRMR for the central nervous system dataset and DLBCL and SVM-RFE for the prostate cancer dataset have more features.

We also measure the feature selection stability using ASM (13) and its standard deviation (see Table 9
                     ). In the case of the ReliefF and MRMR methods, stability was great for all seven datasets, close to 0.90. The standard deviation for both methods was small, except for ReliefF on the breast cancer dataset and MRMR on the lung cancer dataset, where it was much higher. Of the two other feature selection methods, RLS was much more stable than SVM-RFE. In the case of the leukemia and lung cancer datasets, RLS was almost as stable as ReliefF and MRMR. The results show that the least stable method is SVM-RFE. Its value of ASM for all datasets equals about 0.4. Regarding stability, the RLS method is in the middle, with the lowest stability for the central nervous system dataset of 0.50 and the highest stability for the lung cancer dataset of 0.89. Fig. 5
                      presents the histograms of stability measure S
                     
                        A
                      
                     (14) between pairs of the best feature subspaces of particular feature selection methods for particular datasets. If many results are close to 1, it means that a lot of pairs of feature subsets are similar or almost the same. Results close to 0 mean that pairs are different, as they would be randomly selected.

We also reported the time of calculations for all methods, strictly for information purposes (see Table 10
                     ). The most time-consuming method was SVM-RFE. We need to refrain from using the leave-one-out for validation because of a very long calculation time in two datasets—breast cancer and lung cancer. The RLS was the fastest, but we used our own parallel implementation written in C. Calculations were run on a 4-core processor that favours RLS but not the other methods, as they were single thread implementations written in Java.

The feature selection methods and their application to high-dimensional gene data were considered in this paper. In our comparison, we took into account the classification accuracy, number of selected features, stability of the selected features and the calculation time of the investigated methods. However, the main aim of the paper was to measure the feature selection bias using cross-validation feature selection (external cross-validation).

Based on the results, we can state that we have found positive selection bias in all cases. The value of the bias was different for different methods and datasets. The smallest was 2.6% for ReliefF (validation accuracy 65.93% vs. 63.33% test) and the highest was 41.67% for SVM-RFE (validation accuracy 100% vs. 58.33% test), both for the central nervous system dataset.

This work provides evidence that using the same dataset for feature selection and learning is not appropriate. We recommend using cross-validation for feature selection in order to reduce selection bias.

@&#ACKNOWLEDGEMENT@&#

This work was supported by Bialystok University of Technology [S/WI/2/2013].

@&#REFERENCES@&#

