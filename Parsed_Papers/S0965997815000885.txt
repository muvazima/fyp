@&#MAIN-TITLE@&#Real-time identification of disaster areas by an open-access vision-based tool

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Use of a Border Detection Tool to identify the damaged systems after a catastrophic event.


                        
                        
                           
                           Application and detailed analysis for a real case study.


                        
                        
                           
                           Creation of pre-event and post-event layers related to structures and infrastructures network.


                        
                        
                           
                           Comparison between the layers of the two images in order to define the various scenarios.


                        
                        
                           
                           Integration of data that comes from multi-view image/video fusion applications.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

GIS

Catastrophic event

Selection tool

Open source

Damage detection

Risk assessment

@&#ABSTRACT@&#


               
               
                  Structural damages caused by natural catastrophic events cover a wide area and it is convenient to supervise the event consequences by vision tools. The aim of this paper is to supply a rapid damage detector designed as a way to aid in risk assessment, damage control and disaster prevention as well as a way to speed the examination of catastrophic effects for emergency studies. The satellite pictures covering the area of interest represent the required bits of information to manage the developed telematics tool. A case study is discussed in order to provide experimental evidence of the proposed procedure potential. Moreover, a multi-view image/video fusion system is integrated in the image process to detect the damage levels of structures to overcome the limitations on the vertical information provided by a satellite. In synthesis, this study shows how a GIS-based real time monitoring system can be effectively used for a rapid evaluation of structural damage and disaster management.
               
            

@&#INTRODUCTION@&#

There is not the time to forget the consequences of the last one that a new catastrophic event reminds, to the actors involved in natural risk management, the importance of a rapid damage assessment for providing a suitable emergency response. Indeed the entity damages caused by an earthquake also depends on the delay and inadequacy of the rescue interventions. For this reason, a post-disaster damage assessment can be achieved by integrating, within a single user interface environment, existing database information, GIS technology and image processing techniques. In literature there are several contributions along this line [1–6]. The software adopted by the authors for this purpose is QuantumGIS (QGIS). QGIS is an Open Source Geographic Information System that is developed using the Qt toolkit and C language. It has a pleasant, easy-to-use graphical user interface (GUI). QGIS is also released under the GNU General Public License: this means that everybody can inspect and modify the source code. In conclusion, QGIS guarantees access to a GIS program that is free of cost and can be freely modified [7].

This paper outlines the main steps of a procedure to locate quickly those structures, which have been affected by damage as a consequence of a natural disaster. First, a wide overview related to the state of art is provided. A detailed documentation on the architecture of the informatics tool set up for damage and loss estimation in emergency follows. Finally, the proposed procedure is applied to a case study.

Remote sensing is often used to obtain information. It is founded on the collection of data with the devices used to collect the data, which are not in direct contact with the object of the study. Satellites, which are the main platforms utilized in remote sensing, have a wide range of sensors and can also study the weather and the landscapes of natural disasters, with the possibility of acquiring images also in the night light. In addition to dedicated sensors, satellites also mount devices able to acquire images, which are quite different from a simple camera picture only able to provide the same information that can be acquired by the eyes. Finally, the satellite pictures are digital images that are composed by several squares blocks (called pixels) and, when a picture is analyzed, each pixel is associated with a value corresponding to the intensity of radiation reflected from the observed object within the range of wavelength where the sensor is active [8,9]. Fig. 1
                      summarized the process of image capturing by a satellite.

For the purpose of the subsequent analysis of the collected information, geographic information systems (GIS) are used. They integrate hardware, software and geographical data in order to capture, manage and analyze all forms of geographically referenced information. These systems allow the operator to view, to ask, to understand and to interpret data in many ways in order to reveal relationships, patterns and trends in the form of maps, globes, reports and charts. A GIS stores geographic information as a collection of theme layers that can be related each to the other through connection and geographical overlap. Moreover, these layers can be acquired using the vector mode or the raster mode. In the vector mode information on points, lines and polygons is encoded and stored as series of coordinates (x, y). By contrast, in the raster mode, it consists of a grid of pixels, which represents a specific value, with each pixel characterized by its own grid position (row number and column).

The implemented system architecture, provided pre-event and post-event satellite images are available, can be summarized in six steps as follows:
                        
                           –
                           load the pre-event image;

create the pre-event layers;

load the post-event image;

create the post-event layers;

compare the layers relating the two images;

integrate data that comes from multi-view image/video fusion applications.

The first step is realized using a plugin, based on Google Maps, that permits to automatically georeference the satellite image. After loading the addresses, the creation of layers over the pre-event satellite image follows. These layers are made by general geometric entities as points, lines and polygons that are gathered as shape file. An operation of this kind in QGIS results very simple through the “Border Detection Tool” that is designed to select areas of the image based on color similarity. It is based on ideas already implemented in different image managed software, but imported and modified in the QGIS environment. This tool is called “Magic Wand” and is good for selecting every object with sharp edges but it is also useful for selecting an area within a contour. It works very well for selecting an area with a solid color like could be buildings and roads [10]. The same process is made for the post-event image and the final step of the proposed procedure is the comparison between the layers of two satellite images. Several authors and software houses are working in this field in order to develop a tool able to compare images for specific application fields. Almost all of them are under copyright and do not offer user-friendly facilities. The Geoprocessing Tool, called “Intersect” as adopted in this paper, belongs to the class of open source software and is included in the QGIS environment. Given a photo of the area after a catastrophic event, the damage, i.e. the changes detected from a pre-event image, are deducted as the variation of the pixel intensity on the photos. The “Border Detection Tool” feels this pixel difference and so allows you to easily select the objects in the area, which were affected by damage to build new layers of different shape than those made before. Hence, the quick estimate of the extent on the damage area is obtained, by a simple difference by layers tracked within the pre- and post-event photos. It is worth noticing that the identification by satellite images may present some difficulties due to image distortion that may compromise the optimal resolution, and it must be corrected. A scheme of proposed methodology is shown in Fig. 2
                     .

The conceptual architecture of Fig. 2 is built using pre- and post-event satellite images of the same scene, but a remark needs to be given: immediately after a catastrophic event occurred, the system must be able to receive near-real-time imagery of the damaged area. In other words, the present work is carried out on the hypothesis of a fast delivery service linking satellite image providers with the operative civil protection units. Similar procedures for damage detection by image comparison had already been tested in other case study, but these preliminary works revealed some problems and suggested further improvements that are the topics of the present paper. In particular, simple approaches to image comparison are generally not sufficiently robust to cope with data corrupted by noise, registered with different light and day conditions, i.e. all the imprecisions that affect each pair of images to be simultaneously analyzed. For this reason, the difficulties found during the extraction of pixels in the images brought to the consideration that one needs to directly perform the difference between the layers created in the image through a tool capable of optimizing the trade-off between accuracy of analysis and the need to provide a rapid damage assessment.

The joint use of image data and data that come from webcams or simple videos from different view or remote sensing control devices has the potential to provide significant information to determine the reliable damage information including the level of damages. Generally, multi-view systems, which are capturing overlapped images from the same scene with different positions, are used. While satellite pictures are obtained in the same angle for the comparison before and after disaster images. On the other hand, numerous network cameras in buildings, street, archaeological monuments, etc. capture the same scene with ideal quality from multiple viewpoints. Furthermore, countless webcams that are placed globally in at least 17,000 places all over the world is a powerful, yet underutilized, source of imagery which in combination with satellite imagery is an area of new exploration especially in environmental monitoring applications.

The Advanced Data Fusion System tool comes with a very simple but complete Web GUI that gives you the ability to “run” over the globe through time and dates and aim on specific locations on which you can track information from multiple sources like cameras, smartphones, etc. The main technical features are [9]:
                           
                              –
                              integrated total solution for immediate fusion of near real-time tracking of terrestrial images all over the globe;

fuses information and images of third party devices other than satellites;

an algorithm of pixel classification according to their coordinates is used.

Before trying to implement a system that will fuse image and video data, it is highly important to define all the possible input of our system in order to develop a system, which can be applied to all “real” data.

Most of the earth observation satellites such as Spot, Ikonos, Quickbird, Formosat or Orbview, and also some digital airborne sensors like DMC or UltraCam, record image data in two different modes, a low-resolution multispectral and high-resolution panchromatic mode. A common feature for these sensors is the fact that the highest spatial resolution is recorded in their panchromatic mode whereas the multispectral recording mode produces images of reduced spatial resolution. The difference in spatial resolution between the panchromatic and the multispectral mode can be measured by the ratio of their respective ground sampling distances (GSD) and may vary between 1:2 and 1:5. This ratio can get worse if data from different satellites are used. For example, the resolution ratio between Ikonos (pan mode) and SPOT 5 (multispectral mode) is 1:10. The objective of iconic image fusion is to combine the panchromatic and the multispectral information to form a fused multispectral image that retains the spatial information from the high-resolution panchromatic image and the spectral characteristics of the lower resolution multispectral image. Many fusion methods are used in research for satellite fusion including wavelet transform, color normalization, IHS transform, etc. Hence, if one wants to fuse a satellite image with a frame from a video that was capture by an airborne system, one should have in mind:
                           
                              –
                              spatial resolution, and

fusion method that we are going to use.

The provided image is loaded to the system. During this step, it is very important to check image’s definition. For this reason, a function, which applies to the image de-noising techniques in order to have the best possible quality of the picture provided, is created. The output of this step is a cleaner image.

The case study reported in this section covers the area of the Philippines and in Fig. 3
                     a and b is shown the scale of devastation that Typhoon Haiyan caused in the city of Tacloban (November 2013).

For the region under study, it is chosen to not georeference the image, since a satellite image for the post-event (i.e., after a catastrophic event) is not available and it is necessary to simulate such a picture. Anyway, the georeferencing operation consists of associating all the pixels of the satellite image to their spatial coordinates relative to a given reference system, for instance the Datum World Geodetic System WGS84. For carrying out this operation, one must necessarily know the coordinates of n-points, the control ground points (CGPs), where n
                     ⩾3 on both reference systems. Many software are able “to connect” the coordinates “image” with the coordinates “ground” on a georeferenced satellite image from the knowledge of the n CGPs, but recently a plugin that permits to georeference automatically the satellite image based on Google Maps has been developed.

For the area under investigation, a layer (buildings) of the structures to be analyzed is created using the “Border Detection Tool”. This tool allows one to obtain quickly the intended effect because has an easy setup for the users. Indeed clicking on a point in the image, the tool finds all the pixels around that point that are similar in color, given a tolerance level, and selects them. The developed tool is conceptually attractive because it automatically makes the selection by grouping pixels that are similar in color and that are spatially connected, being grown from a seed pixel. A system like this one allows one to pose the following question: how much lower or higher can the pixel colors be to still be included in the selection? This is specified by the selection threshold that can be set in the “Tool Options” dialog. It can also be set interactively with the mouse by keeping the left mouse button pressed when selecting the seed pixel. As shown in Fig. 4
                        , clicking in the middle of the buildings, the tool selects all pixels connected and draw marching ants around them. From here, the user can select the Edit>Add Feature menu item to copy the selected pixels to the created layer [10].

This operation is performed by a Geoprocessing Tool called “Intersect” within the QGIS environment and it is also important clarify that higher the image resolution is, better the accuracy of the estimation will be. In addition, Fig. 5
                        a and b show the layer of interested buildings for the pre- (blue)
                           1
                           For interpretation of color in Fig. 5, the reader is referred to the web version of this article.
                        
                        
                           1
                         and post-event (orange) satellite images.

After the comparison between the pre- and post-event images, various scenarios are defined and it is possible to manage rescue teams in order to reach, in the quickest way, the damaged buildings (e.g., private houses, hospitals, schools, hospices, etc.). Moreover, could be formulated a first guess for the cost of reconstruction in order to estimate if it is more convenient to repair or demolishing them, which would facilitate the operations for future reconstruction. Frequently the amount of money to be spent in order to repair or re-build a building is very high and such a cost could result not affordable by a citizen in the absence of an insurance coverage [11]. In Fig. 6
                        , the result of comparison between the layers related to pre- and post-event satellite images is shown. The damaged building are colored in red.

After the comparison between the pre- and post-event images, the various scenarios are defined and an important aspect in emergency is to manage rescue teams in order to reach, in the quickest way, the damaged buildings. If a street is interrupted or at worst destroyed, after a catastrophic event, within the QGIS environment one can use a plugin, which gives the best way to connect two different points. This specific tool is called “Road graph” and is linked with Google Maps, therefore it is constantly updated. Indeed, in Fig. 7
                         is shown as in the case study and when a route is inaccessible, it can help to identify the available alternative routes to get the specified destination.

@&#CONCLUSIONS@&#

Recent experiences in disaster areas emphasized that the availability of a telematics tool able to compare in a GIS environment satellite images of the area, taken before and after a catastrophic event, respectively, is desirable. This paper provides details of the information technology process with reference to a case study. The goals are both to improve the disaster prevention using GIS technology and to reduce the recovery time by a sort of damage estimate. Indeed a damage assessment can be pursued more easily by using pre- and post-event satellite images which offer a wide coverage of the area.

Nevertheless, in some cases the area of the failed structure is not changed so much. This occurs, for instance when the building collapses due to heavily damaged columns in the building first floor. A solution could be the joint use of image data and data that comes from multi-view image/video fusion applications. This approach has the potential to provide a most refined damage information. In other words, since satellite information comes from a vertical viewpoint, it has limitations in detecting damage levels. Thus, additional images coming from remote sensing control devices located on the Earth surface can be used.

This data fusion can also be adopted in implementing a real-time monitoring system for atransportation network infrastructure to avoid global and local environmental risks. Indeed an effective security system requires an automatic, smart and vigilant system that integrates various methodologies [12].

@&#ACKNOWLEDGMENTS@&#

The authors gratefully acknowledge the financial support provided by the University of Pavia FAR (Fondo Agevolazioni Ricerca) grant. The Leonardo da Vinci grant – “LdV Transfer Innovation Project VET in Rapid Earthquake Damage Assessment of Buildings to Avoid the Demolishing (2011-1-TR1-LEO05-27938)” is also acknowledged.

@&#REFERENCES@&#

