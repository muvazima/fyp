@&#MAIN-TITLE@&#Multi-view facial landmark detection by using a 3D shape model

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Accurate facial landmark detector coupled with a 6 DoF head pose estimator


                        
                        
                           
                           A reliable global method followed by a precise local optimization


                        
                        
                           
                           Novel learning method based on structured output SVM


                        
                        
                           
                           Multi-view detector working on a wide range of viewing angles (frontal-profile). Self-occlusions handled.


                        
                        
                           
                           Comparison with recent state-of-the-art methods on standard “in the wild” datasets with quantitatively favourable results.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Face

Landmarks

Detection

Localization

3D model

Shape

Occlusions

@&#ABSTRACT@&#


               
               
                  An algorithm for accurate localization of facial landmarks coupled with a head pose estimation from a single monocular image is proposed. The algorithm is formulated as an optimization problem where the sum of individual landmark scoring functions is maximized with respect to the camera pose by fitting a parametric 3D shape model. The landmark scoring functions are trained by a structured output SVM classifier that takes a distance to the true landmark position into account when learning. The optimization criterion is non-convex and we propose a robust initialization scheme which employs a global method to detect a raw but reliable initial landmark position. Self-occlusions causing landmarks invisibility are handled explicitly by excluding the corresponding contributions from the data term. This allows the algorithm to operate correctly for a large range of viewing angles. Experiments on standard “in-the-wild” datasets demonstrate that the proposed algorithm outperforms several state-of-the-art landmark detectors especially for non-frontal face images. The algorithm achieves the average relative landmark localization error below 10% of the interocular distance in 98.3% of the 300W dataset test images.
               
            

@&#INTRODUCTION@&#

Facial landmarks refer to points on the face like the corners of the mouth, the corners of the eyes or the tip of the nose that can be annotated by a human. Detection of facial landmarks in images has many potential applications as e.g., animation, morphing, and it is an important step in most face image interpretation tasks. Face images usually need to be aligned and normalized with the help of landmarks prior to recognition of e.g., identity, age, gender, expression.

Most facial landmark detectors simultaneously model local appearance around the landmarks and their geometrical configuration. The local appearance is represented either by generative models (e.g. [1]) or by discriminatively trained detectors (e.g. [2]). The geometrical structure of the landmarks is usually modelled by a Point Distribution Model (PDM) [3], which describes the landmark positions on a face in canonical frame, and by a subsequent deviation from the canonical pose in 2D image coordinates. Both PDM of 2D shapes (e.g. [1,4]) and 3D shapes have been proposed (e.g. [5]).

Fitting the shape models into the image requires optimization of a highly non-convex fitness function typically carried out as gradient search sensitive to the initial estimate or by regression. The problem with local optima is mitigated either by re-initializing the optimization, by using global but expensive optimization methods (e.g. [6]), or by simplifying the shape model. A prominent example of a simplified 2D shape prior is the Deformable Part Model (DPM) [7] representing the shape by a pair-wise energy function whose global optimum can be found efficiently by dynamic programming. Excellent results of DPM based facial landmark detectors have been demonstrated e.g. in [8,9]. On the other hand, DPM detectors can describe only a limited range of face poses and thus a multi-view detector must be composed of several DPMs (e.g. [8]).

Recently, regression methods have been very successful. The methods avoid explicit local or global optimization and learn a cascade of regression functions that map the input image to the target output, which is either directly the landmark locations in the image [10,11] or indirectly a set of parameters of 2D [12] or 3D shape models [13]. Typically, pose-indexed (also known as shape-indexed) features are leveraged. In each stage of the cascade, a regressor extracts features from image locations relative to the current estimate of landmarks to predict a model update. Regression-based methods are usually much faster than optimization-based methods and often run faster than real-time [14,15]. However, like the local optimization methods, these methods require initialization. Another potential difficulty with the regression methods using pose-indexed features has to be overcome for training. Annotation of all landmarks is necessary for every image in the training set. Manual annotation is not always complete because of: (1) self-occlusions due to various pose changes – separate regressors need to be trained for poses with shared subsets of visible landmarks, or (2) occlusion by hairs, hands, or other objects – these images cannot be used, or simply (3) a different set of landmarks is often annotated for datasets – it is then typically not possible to combine multiple datasets for training.

Currently, the difference in performance of DPMs fitted by a global method, of the genuine 3D shape models fitted by local methods, and of the regression-based methods is not fully understood.

Besides using the intensity image alone, there are approaches that work with RGB-D data (image + depth) to detect landmark and to align 3D faces, e.g. [16,17,18].

In this paper we show that a robust and precise landmark detector is obtained by fitting a simple 3D shape model into the image using a full perspective projection. The method jointly fits M shape parameters and the 6 DoF pose (position and orientation) of the 3D face model with respect to the camera, see Fig. 1
                     .

In addition, we propose a novel method for discriminative learning of the local detectors that are used to guide the fitting of the model. We learn scoring functions whose value decreases approximately linearly with the Euclidean distance from the true landmark position. The method often produces unimodal peaks around the true landmark positions which helps to make the basin of attraction sufficiently large. The approach differs from the commonly used two-class classification methods, like the Support Vector Machines or AdaBoost, whose learning objective does not take the distance from the true landmark position into account.

Related to our approach is the work of [2], a local optimization-based method employing a 3D model, that has a single degree of freedom to capture the shape. However, a simpler camera model is used, weak perspective in [2] vs. full perspective, and a substantially different learning of the local landmark scoring functions is employed, a standard AdaBoost [2] vs. the novel learning method.

Another related work is a regression-based method [13] which uses a parametrization similar to ours, but again in conjunction with the weak perspective camera. Due to the nature of the regression that takes the entire face image as an input, this method is sensitive to self-occlusions. If the head is turned so that certain landmarks are not visible in the camera, the occluded landmarks cannot be easily disregarded. In the proposed method, a contribution of the occluded landmarks is easily switched off in the optimization data term. The property leads to accurate results on face images captured from arbitrary view-point and not only on near frontal images. To the best of our knowledge we are not aware of any landmark detector functioning reliably in a multi-view setup.

To summarize, the contributions of the paper are:
                        
                           1.
                           A novel precise local optimization-based algorithm coupled with a robust initialization scheme based on a global method is proposed. The initialization gives a raw estimate. Thanks to its global optimality it is likely to be free of outliers.

A novel method for learning local landmark detectors is introduced. It produces smooth unimodal score functions with a large basin of attraction.

Self-occlusions are explicitly taken into account, which results in an algorithm operating for a broad set of viewing angles, e.g. semi-profile and profile views.

A thorough comparison of the proposed method with state-of-the-art implementations of two different DPM based detectors [9,8], and with two recent regression-based method [13] and a multiview implementation of [11] is performed. The proposed method and [9] use the same local detectors, but differ in the used shape prior. The proposed method and [13] have a similar parametric model

This paper is an extension of [19] with improved 3D model parametrization to capture a wide range of subjects and facial expressions. The optimization scheme is robustified by introducing a reliable initialization strategy. A range of applicable angles is extended by modelling the landmark visibility. We are now estimating up to 49 landmarks as opposed to 7 landmarks in [19].

The rest of the paper is structured as follows: The algorithm is presented in Section 2, the justification of the design choices is discussed in Section 3, and its implementation details are given in Section 4. Experimental validation, including both the introspection and comparison with several state-of-the-art methods, is presented in Section 5. Finally, Section 6 concludes the paper.

The estimation problem addressed entails: (1) localization of landmarks in the image, (2) estimation of the head pose, i.e., a position and orientation with respect to the camera coordinate system, (3) reconstruction of the landmark points in 3D. Quantities (1–3) are calculated from a single image.

The architecture of the proposed algorithm is depicted in Fig. 2
                     . The pipeline consists of two stages: the initialization and the optimization. The three problems (the landmark detection, head pose estimation, and 3D reconstruction) are coupled and are solved as a single optimization problem, see Section 2.2. A parametric 3D shape model is fit to maximize the values of landmark scoring functions. Each landmark scoring function takes the image and for a query pixel returns a score proportional to how likely the landmark occurrence centred at the pixel is, see Section 2.1. The proposed criterion is non-convex, therefore a robust initialization procedure is needed, see Section 2.3. The initialization integrates a multi-view face detector and an implementation of a state-of-the-art DPM. We choose a simpler 2D landmark model working in a low image resolution, which provides a globally optimal solution.

Let us define the landmark score function c
                        
                           i
                        (x,
                        I) which estimates the likelihood of the i-th landmark being at position x in the image I. The most likely position is 
                           
                              
                                 x
                                 ̂
                              
                              i
                           
                           =
                           
                              argmax
                              
                                 x
                                 ∈
                                 
                                    X
                                    i
                                 
                              
                           
                           
                              c
                              i
                           
                           
                              x
                              I
                           
                         where 
                           
                              X
                              i
                           
                         denotes the searched positions. We consider a linearly parametrized score.
                           
                              (1)
                              
                                 
                                    c
                                    i
                                 
                                 
                                    x
                                    I
                                    
                                       w
                                       i
                                    
                                 
                                 =
                                 
                                    
                                       Ψ
                                       
                                          x
                                          I
                                       
                                    
                                    
                                       w
                                       i
                                    
                                 
                                 ,
                              
                           
                        where 〈⋅,⋅〉 denotes a dot product, Ψ(x,
                        I)∈ℜ
                           n
                         denotes a feature descriptor extracted from a patch cropped from the image I around the position x and w
                        
                           i
                        
                        ∈ℜ
                           n
                         is a weight vector associated with the i-th landmark. We construct the descriptor Ψ(x,
                        I) by concatenating the Local Binary Patterns (256 valued code assigned to a 3×3 patch) computed at all positions of the cropped patch rescaled to size 20×20, 10×10 and 5×5 pixels, respectively. By this process we obtain 256(182
                        +82
                        +32)-dimensional sparse (182
                        +82
                        +32 non-zero elements) binary feature descriptor whose values are to some extent robust against scale and lighting conditions. The side of the cropped squared patch is 0.3 of the bounding box side returned by the face detector.

The proposed method uses the score functions to guide the search for the most likely configuration of the 3D face pose. It is common to learn the score functions by two-class classification methods, like the Support Vector Machines or AdaBoost, learning the score that best separates example patches collected at the true positions from the patches sampled around the true position. These methods do not take the distance from the true landmark position explicitly into account. In turn, there is no guarantee that the learned score will form unimodal peaks around the true positions.

We propose a different approach which learns the score function such that its value decreases at least linearly with the Euclidean distance measured from the truth landmark position. To this end, we define the loss
                           
                              
                                 
                                    ℓ
                                    i
                                 
                                 
                                    x
                                    I
                                    
                                       w
                                       i
                                    
                                 
                                 =
                                 
                                    max
                                    
                                       x
                                       '
                                       ∈
                                       
                                          X
                                          i
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                x
                                                ′
                                             
                                             −
                                             x
                                          
                                       
                                       +
                                       
                                          
                                             Ψ
                                             
                                                
                                                   x
                                                   ′
                                                
                                                I
                                             
                                          
                                          
                                             w
                                             i
                                          
                                       
                                       −
                                       
                                          
                                             Ψ
                                             
                                                x
                                                I
                                             
                                          
                                          
                                             w
                                             i
                                          
                                       
                                    
                                 
                                 ,
                              
                           
                        where x denotes the true position of the i-th landmark in the image I. We require the score of the correct position to be higher than scores of incorrect positions by a margin. The margin is equal to the distance from the correct position. Any violation of this condition is penalized by the loss. Note that the value of 
                           ℓ
                        
                        
                           i
                        (x,
                        I,
                        w
                        
                           i
                        ) upper bounds the Euclidean distance between the true position x and the position with maximal score, i.e. 
                           
                              
                                 x
                                 ̂
                              
                              i
                           
                           =
                           a
                           rgma
                           
                              x
                              
                                 x
                                 ∈
                                 
                                    X
                                    i
                                 
                              
                           
                           
                              c
                              i
                           
                           
                              x
                              I
                           
                        .

Given a training set {(I
                        1,
                        x
                        
                           i
                        
                        1),…,(I
                        
                           m
                        ,
                        x
                        
                           i
                        
                        
                           m
                        )} containing pairs (I
                        
                           j
                        ,
                        x
                        
                           i
                        
                        
                           j
                        ) of images I
                        
                           j
                         and the ground-truth positions x
                        
                           i
                        
                        
                           j
                         of the i-th landmark, we learn the parameters w
                        
                           i
                         of the score function c
                        
                           i
                        (x;
                        w
                        
                           i
                        ) by solving
                           
                              (2)
                              
                                 
                                    w
                                    i
                                 
                                 =
                                 
                                    
                                       a
                                       rgmin
                                    
                                    
                                       w
                                       ∈
                                       
                                          ℜ
                                          n
                                       
                                    
                                 
                                 
                                    
                                       
                                          λ
                                          2
                                       
                                       
                                          
                                             w
                                          
                                          2
                                       
                                       +
                                       
                                          1
                                          m
                                       
                                       
                                          
                                             ∑
                                             
                                                j
                                                =
                                                1
                                             
                                             m
                                          
                                       
                                       
                                          ℓ
                                          i
                                       
                                       
                                          
                                             x
                                             i
                                             j
                                          
                                          
                                             I
                                             j
                                          
                                          w
                                       
                                    
                                 
                                 ,
                              
                           
                        where λ
                        >0 is a positive constant penalizing large weighs in order to prevent over-fitting and its optimal value is tuned on a separate validation set. The problem (2) can be seen as an instance of the Structured Output Support Vector Machines with margin-rescaling loss [20]. The formulation (2) translates learning of the i-th local detector to an unconstrained convex optimization problem which we solve efficiently by the cutting plane algorithm [21].

Before presenting the problem formulation, let us introduce the notation: we denote a vector v
                        =(v
                        1,
                        v
                        2,
                        v
                        3)
                           T
                        , its homogeneous extension as 
                           
                              v
                              ˜
                           
                           =
                           
                              
                                 
                                    v
                                    1
                                 
                                 
                                    v
                                    2
                                 
                                 
                                    v
                                    3
                                 
                                 1
                              
                              T
                           
                        , and the operator which transforms a vector to Euclidean representation as 
                           
                              
                                 v
                              
                              E
                           
                           =
                           
                              1
                              
                                 v
                                 3
                              
                           
                           
                              
                                 
                                    v
                                    1
                                 
                                 
                                    v
                                    2
                                 
                              
                              T
                           
                        .

A perspective camera K[R(Φ)|
                        t], with intrinsic matrix K, rotation matrix R parametrized by Φ
                        =(α,
                        β,
                        γ)
                           T
                        , which are roll, pitch, yaw angles respectively, and the translation t
                        =(t
                        
                           x
                        ,
                        t
                        
                           y
                        ,
                        t
                        
                           z
                        )
                           T
                         projects a 3D landmark point X
                        
                           i
                        
                        =(X
                        
                           i
                        ,
                        Y
                        
                           i
                        ,
                        Z
                        
                           i
                        )
                           T
                         into a 2D image point
                           
                              (3)
                              
                                 
                                    x
                                    i
                                 
                                 =
                                 
                                    
                                       
                                          x
                                          i
                                       
                                       
                                          y
                                          i
                                       
                                    
                                    T
                                 
                                 =
                                 
                                    
                                       
                                          K
                                          
                                             
                                                R
                                                
                                                   Φ
                                                
                                             
                                             t
                                          
                                          
                                             
                                                X
                                                ˜
                                             
                                             i
                                          
                                       
                                    
                                    E
                                 
                                 .
                              
                           
                        
                     

A 3D model of a person/expression specific landmark configuration can be expressed as a PCA model. The i-th landmark point
                           
                              (4)
                              
                                 
                                    X
                                    i
                                 
                                 
                                    η
                                 
                                 =
                                 
                                    X
                                    i
                                    0
                                 
                                 +
                                 
                                    
                                       ∑
                                       
                                          j
                                          =
                                          1
                                       
                                       M
                                    
                                    
                                       η
                                       j
                                    
                                    
                                       S
                                       i
                                       j
                                    
                                    ,
                                 
                              
                           
                        where X
                        0 is the mean model, S
                        
                           j
                         are length normalized eigenvectors 
                           
                              
                                 S
                                 j
                              
                           
                           =
                           
                              
                                 λ
                                 j
                              
                           
                           ,
                         where λ
                        
                           j
                         is an eigenvalue corresponding to eigenvector S
                        
                           j
                         of the shape covariance matrix. Then 
                           η
                        
                        =(η
                        1,…,
                        η
                        
                           M
                        ) is a vector of M parameters encoding the shape.

The pose and shape estimation is formulated as a single optimization problem
                           
                              (5)
                              
                                 
                                    
                                       Φ
                                       *
                                    
                                    
                                       t
                                       *
                                    
                                    
                                       η
                                       *
                                    
                                 
                                 =
                                 arg
                                 
                                    max
                                    
                                       Φ
                                       ,
                                       t
                                       ,
                                       η
                                    
                                 
                                 F
                                 
                                    Φ
                                    t
                                    η
                                 
                                 ,
                              
                           
                        where the criterion
                           
                              (6)
                              
                                 F
                                 
                                    Φ
                                    t
                                    η
                                 
                                 =
                                 D
                                 
                                    Φ
                                    t
                                    η
                                 
                                 +
                                 ρR
                                 
                                    η
                                 
                              
                           
                        is a sum of data term D and regularization term R. A small positive constant ρ is the weight of the regularizer. Data term
                           
                              (7)
                              
                                 D
                                 
                                    Φ
                                    t
                                    η
                                 
                                 =
                                 
                                    
                                       ∑
                                       
                                          t
                                          =
                                          1
                                       
                                       N
                                    
                                    
                                       V
                                       i
                                    
                                    
                                       c
                                       i
                                    
                                    
                                       
                                          
                                             K
                                             
                                                
                                                   R
                                                   
                                                      Φ
                                                   
                                                
                                                t
                                             
                                             
                                                
                                                   X
                                                   ˜
                                                
                                                i
                                             
                                             
                                                
                                                   η
                                                
                                                E
                                             
                                          
                                       
                                    
                                    ,
                                 
                              
                           
                        measures the fit of a model instance to the image. Scores c
                        
                           i
                        (x
                        
                           i
                        ) stands for a value of a local scoring function of landmark i located at image position x
                        
                           i
                        , see Eq. (1). An example of the score maps is shown in Fig. 3
                        . Variables V
                        
                           i
                         represents a visibility, V
                        
                           i
                        
                        =1 if landmark i is visible and V
                        
                           i
                        
                        =0 otherwise. Thus contributions of occluded landmarks are switched off. The data term is the sum of values of individual scoring functions for all visible landmarks given the camera pose and shape parameters.

Regularization term penalizes a discrepancy between the reconstructed and the mean shape,
                           
                              (8)
                              
                                 R
                                 
                                    η
                                 
                                 =
                                 −
                                 
                                    η
                                    2
                                    2
                                 
                                 =
                                 −
                                 
                                    
                                       ∑
                                       
                                          j
                                          =
                                          1
                                       
                                       M
                                    
                                 
                                 
                                    η
                                    j
                                    2
                                 
                                 .
                              
                           
                        
                     

It can be easily shown that this term is approximately the squared Mahalanobis distance between the reconstructed shape and the mean shape R(
                           η
                        )≈(X(
                           η
                        )−
                        X
                        0)
                           T
                        
                        C
                        −1(X(
                           η
                        )−
                        X
                        0). Where C is the shape covariance matrix. This immediately follows from (4). Recall that the eigenvectors are normalized to the length of corresponding eigenvalues. Note that the equality holds precisely when all the eigenvectors are considered.

After solving problem (5), the reconstructed 3D model of landmarks X
                        
                           i
                        
                        ⁎ is computed by substituting 
                           η
                        
                        ⁎ into (4), and the landmarks in the image x
                        
                           i
                        
                        ⁎ are found by projecting the 3D model X
                        
                           i
                        
                        ⁎ by the camera at the optimum rotation and translation {Φ
                        ⁎,
                        t
                        ⁎} according to (3).

The maximum of (6) can be found by any method for smooth optimization. In particular, we use the LBFGS algorithm of [22] which requires only a routine computing the value and the gradient of the objective function. The gradient of the regularization term is zero with respect to the pose parameters and 
                           
                              
                                 ∂
                                 R
                              
                              
                                 ∂
                                 
                                    η
                                    j
                                 
                              
                           
                           =
                           −
                           2
                           
                              η
                              j
                           
                        . The gradient of the data term has a special structure. To simplify the notation, let us collect all parameters into a tuple Θ
                        =(Φ,
                        t,
                        
                           η
                        ) where the first six entries represent the pose while the remaining M entries are the shape parameters. Let us denote the projection of the i-th model point into the image as 
                           
                              p
                              i
                           
                           
                              Θ
                           
                           =
                           
                              
                                 
                                    K
                                    
                                       
                                          R
                                          
                                             Φ
                                          
                                       
                                       t
                                    
                                    
                                       
                                          X
                                          ˜
                                       
                                       i
                                    
                                    
                                       η
                                    
                                 
                              
                              E
                           
                           =
                           
                              
                                 
                                    x
                                    i
                                 
                                 
                                    y
                                    i
                                 
                              
                              T
                           
                        . Then the data term in (6) becomes.
                           
                              (9)
                              
                                 D
                                 
                                    Θ
                                 
                                 =
                                 
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       N
                                    
                                    
                                       V
                                       i
                                    
                                    
                                       c
                                       i
                                    
                                    
                                       
                                          
                                             p
                                             i
                                          
                                          
                                             Θ
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

The gradient with respect to the parameters is.
                           
                              (10)
                              
                                 
                                    
                                       d
                                       D
                                       
                                          Θ
                                       
                                    
                                    
                                       d
                                       Θ
                                    
                                 
                                 =
                                 
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       N
                                    
                                    
                                       V
                                       i
                                    
                                    
                                       
                                          d
                                          
                                          
                                             c
                                             i
                                          
                                          
                                             
                                                
                                                
                                                   p
                                                   i
                                                
                                                
                                                   Θ
                                                
                                             
                                          
                                       
                                       
                                          d
                                          Θ
                                       
                                    
                                 
                                 =
                                 
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       N
                                    
                                    
                                       V
                                       i
                                    
                                    
                                       
                                          d
                                          
                                          
                                             c
                                             i
                                          
                                          
                                             
                                                
                                                
                                                   p
                                                   i
                                                
                                                
                                                   Θ
                                                
                                             
                                          
                                       
                                       
                                          d
                                          
                                          
                                             p
                                             i
                                          
                                          
                                             Θ
                                          
                                       
                                    
                                    
                                       
                                          d
                                          
                                          
                                             p
                                             i
                                          
                                          
                                             Θ
                                          
                                       
                                       
                                          d
                                          Θ
                                       
                                    
                                    ,
                                 
                              
                           
                        where
                           
                              (11)
                              
                                 
                                    
                                       d
                                       
                                          c
                                          i
                                       
                                       
                                          
                                             
                                                p
                                                i
                                             
                                             
                                                Θ
                                             
                                          
                                       
                                    
                                    
                                       d
                                       
                                          p
                                          i
                                       
                                       
                                          Θ
                                       
                                    
                                 
                                 =
                                 
                                    
                                       
                                          ∂
                                          
                                             c
                                             i
                                          
                                          
                                             
                                                x
                                                i
                                             
                                             
                                                y
                                                i
                                             
                                          
                                       
                                       
                                          ∂
                                          
                                             x
                                             i
                                          
                                       
                                    
                                    
                                       
                                       
                                          
                                             ∂
                                             
                                                c
                                                i
                                             
                                             
                                                
                                                   x
                                                   i
                                                
                                                
                                                   y
                                                   i
                                                
                                             
                                          
                                          
                                             ∂
                                             
                                                y
                                                i
                                             
                                          
                                       
                                    
                                 
                                 =
                                 
                                    
                                       J
                                       c
                                    
                                    i
                                 
                                 
                                    
                                       
                                          p
                                          i
                                       
                                       
                                          Θ
                                       
                                    
                                 
                                 ,
                              
                           
                        
                        
                           
                              (12)
                              
                                 
                                    
                                       
                                          dp
                                          i
                                       
                                       
                                          Θ
                                       
                                    
                                    
                                       d
                                       Θ
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   ∂
                                                   
                                                      x
                                                      i
                                                   
                                                   
                                                      Θ
                                                   
                                                
                                                
                                                   ∂
                                                   
                                                      θ
                                                      1
                                                   
                                                
                                             
                                             ,
                                          
                                          
                                             …
                                             ,
                                          
                                          
                                             
                                                
                                                   ∂
                                                   
                                                      x
                                                      i
                                                   
                                                   
                                                      Θ
                                                   
                                                
                                                
                                                   ∂
                                                   
                                                      θ
                                                      
                                                         6
                                                         +
                                                         M
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   ∂
                                                   
                                                      y
                                                      i
                                                   
                                                   
                                                      Θ
                                                   
                                                
                                                
                                                   ∂
                                                   
                                                      θ
                                                      1
                                                   
                                                
                                             
                                             ,
                                          
                                          
                                             …
                                             ,
                                          
                                          
                                             
                                                
                                                   ∂
                                                   
                                                      y
                                                      i
                                                   
                                                   
                                                      Θ
                                                   
                                                
                                                
                                                   ∂
                                                   
                                                      θ
                                                      
                                                         6
                                                         +
                                                         M
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                                 =
                                 
                                    J
                                    p
                                    i
                                 
                                 
                                    Θ
                                 
                                 .
                              
                           
                        
                     

The gradient is a sum of products of two matrices over the visible landmarks. Matrix J
                        
                           p
                        
                        
                           i
                        (Θ) is the 2×(6+
                           M) Jacobian matrix. The derivatives are rather complex due to a non-linearity of the mapping p
                        
                           i
                        (
                           Θ
                        ), but they are computed analytically. Matrix 
                           
                              
                                 J
                                 c
                              
                              i
                           
                           
                              
                                 x
                                 i
                              
                              
                                 y
                                 i
                              
                           
                         of size 1×2 is the spatial gradient of the landmark scoring function. The derivatives are computed using a symmetric Gaussian kernel of scale σ and finite differences. The choice of σ has to be carried out carefully since it influences the optimization outcome. A landmark scoring function is evaluated in a window of 4σ
                        ×4σ pixels around the target pixel, which can be expensive for large σ. Empirically we found that σ
                        =3 works well and this value is used in all our experiments.

The optimization problem formulated in (5) is non-convex. The gradient descent method finds a local optimum only and therefore it needs to be initialized carefully. We propose the following initialization strategy, which is depicted in Fig. 2.

First, a scanning window face detector is executed on the input image. We use a Waldboost-based commercial detector
                           1
                        
                        
                           1
                           By Eyedea recognition, Ltd. http://www.eyedea.com.
                         
                        [23], that successfully operates in a large range of viewing angles. Besides the bounding box, it also outputs a rough estimate of roll and yaw angles. The roll is the angle of the in-plane rotation while the yaw is the out-of-plane rotation of the head, i.e. from left to right. The pitch angle, i.e. the elevation of the head, is not provided.

For each detection, the image is cropped according to the bounding box, rotated so that the faces were upright, and finally normalized into a low resolution 80×60 pixel image I
                        
                           n
                        . The original s and normalized s
                        
                           n
                         image coordinates are related by similarity transformation 
                           
                              s
                              n
                           
                           =
                           
                              
                                 
                                    
                                       A
                                       n
                                    
                                    
                                       s
                                       ˜
                                    
                                 
                              
                              E
                           
                        .

The normalized image I
                        
                           n
                         is used as in input to the 2D DPM landmark detector [9]. We trained a collection of four DPM models for quantized yaw angles: near frontal (−15∘,15∘), semi-profiles (15∘,45∘) and (45∘,75∘) and profile (75∘,90∘). The remaining three models for yaw angles (−90∘,−75∘), (−75∘,−45∘) and (−45∘,−15∘) are not trained but the solution is obtained by mirroring the image. Each model is independently trained for a particular subset of landmarks which are typically visible in the respective yaw interval, see Fig. 4
                        . The estimate of the yaw angle from the face detector is used to select the model. Finally the 2D DPM algorithm provides 2D landmark positions s
                        
                           n
                         and the corresponding visibility vector V
                        =(V
                        1,…,
                        V
                        
                           N
                        ) with V
                        
                           i
                        
                        =1 for landmarks i that were found by the model and zero otherwise. An important advantage of 2D DPM is that it finds a globally optimal solution. Nevertheless, the model has a very simple structure, i.e. MRF on a tree, and since it needs to evaluate all possible configurations, it works in the low resolution image with pixel precision. This causes the final 2D landmark positions s, obtained by back-projecting s
                        
                           n
                         to the original high resolution image by A
                        
                           n
                        
                        −1, not to be very precise. Due to the global optimality they are likely to be free of gross errors.

The final optimization procedure described in Section 2.2 requires an initialization of the pose of the 3D model of landmark configuration. Once landmark points s
                        
                           i
                        
                        =(s
                        
                           x
                        
                        
                           i
                        ,
                        s
                        
                           y
                        
                        
                           i
                        )
                           T
                         are detected in the image, the pose can be estimated by the PnP algorithm [24,25], which additionally requires intrinsic camera matrix K and a 3D model. We use here a generic (not a person-specific) 3D model X
                        
                           i
                        
                        0, see (4). Using the correspondences between the 3D model points and 2D image points X
                        
                           i
                        
                        ↔
                        s
                        
                           i
                        , it estimates the 6 DOF pose by
                           
                              (13)
                              
                                 
                                    
                                       Φ
                                       0
                                    
                                    
                                       t
                                       0
                                    
                                 
                                 =
                                 arg
                                 
                                    min
                                    
                                       Φ
                                       ,
                                       t
                                    
                                 
                                 
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       N
                                    
                                    
                                       V
                                       i
                                    
                                    
                                       
                                          
                                             
                                                s
                                                i
                                             
                                             −
                                             [
                                             K
                                             [
                                             R
                                             
                                                Φ
                                             
                                          
                                          
                                             t
                                             ]
                                             
                                                
                                                   X
                                                   ˜
                                                
                                                i
                                                0
                                             
                                             ]
                                             
                                                
                                                E
                                             
                                          
                                       
                                       2
                                       2
                                    
                                 
                                 ,
                              
                           
                        i.e. minimizing the sum of squared geometric re-projection errors ‖s
                        
                           i
                        
                        −
                        x
                        
                           i
                        
                        0‖2
                        2 over the visible landmarks: It has an algebraic solution for minimal set of points N
                        =3, which leads to a set of polynomial equations with several real solutions. The minimal solution is repeatedly used in a RANSAC scheme [26]. For the consensus set, the problem (13) is solved by iterative Levenberg–Marquardt optimization starting from the solution having the maximum support in RANSAC.

Finally the optimization procedure solving the problem (5) is initialized by the pose of a generic model estimated by the PnP solver 
                           Φ
                        
                        0
                        ,
                        t
                        0. The initial shape parameters are set 
                           η
                        
                        =0, which represents the mean shape X(0)=
                        X
                        0. Visibility vector V is the same as for the PnP solver. This way we provide a tight initialization as will be demonstrated by experiments in Section 5.

@&#DISCUSSION@&#

In this section, we discuss and justify the engineering choices made in the algorithm design.

The initialization of the algorithm uses 3D positions of landmarks X
                     
                        i
                     
                     0. Similarly to [27], we use an average model of the 3D configuration of the landmarks computed over a dataset of subjects. We showed in [19] that fitting a fixed generic 3D model results in a surprisingly high pose precision and landmark accuracy.

Both the PnP algorithm (13) and the 3D optimization (5) assume the knowledge of camera calibration K which is usually not available. In all experiments, we placed the principal point in the centre of the image and the focal length equal to the maximum of the image width and image height (sensor width in pixels). The same guess on the camera calibration is made in the popular Bundler structure from motion package [28] when the calibration is not available. The focal length is not critical, we observe in many experiments that the choice we made leads to a good precision in landmark detection. Of course, the focal length influences the 3D translation. The estimated face position is either closer or farther then the estimated value if the true camera has a different focal length.

We use a full perspective camera. In [27,2], an affine camera model is used, which has the advantage of a simple solution of the model parameters. We aim for situations when the affine camera is not a suitable model, as e.g. a laptop webcam. A person may be very close to the camera, with a wide field of view. The differences of depth of the facial landmarks are no longer negligible to the distance to the face centre. This is exactly the situation when the affine camera approximation is poor as explained in [29], p. 169.

@&#IMPLEMENTATION DETAILS@&#

The shape model (4) was built using data from the Multi
                     −
                     PIE dataset [30]. This is a multi-view dataset consisting of images of about 300 subjects captured by synchronized cameras placed around each subject. Each subject showed various facial expressions. Using a standard structure from motion [28], the full calibration of all cameras was reconstructed first. Then having a manual annotation of facial landmarks in the images, the 3D positions were triangulated. Each such 3D landmark model is normalized to canonical coordinates: The model is scaled so that the distance between the eye centres is equal to 1. The model is translated, so that the zero point was at the centre of gravity of all reconstructed landmarks. Finally the model is rotated to align the horizontal direction with the direction from the left to the right eye centre and to align the vertical direction with the direction from the centre of the mouth corners to the to the centre between the eyes. The 3D models in canonical coordinates of all subjects are thus registered. Finally, the standard PCA model is constructed using about 270 reconstructed faces of about 65 subjects. The other subjects do not have the landmarks annotated across multiple views. This 3D model is used in all our experiments.

@&#EXPERIMENTS@&#

We use two publicly available benchmarks in experiments: Annotated Facial Landmarks in the Wild (AFLW) [27] and the 300W datasets [31,32].


                        The Annotated Facial Landmarks in the Wild (AFLW) is a large dataset of various images downloaded from Flickr. A large range of viewing angles in roll, pitch and yaw, the level of varying facial expressions, the amount of occlusions, varying illumination, diverse quality of the images caused by bad focus or motion blur, certain level of post-processing and artistic effects sometimes present make this dataset particularly challenging. The faces are manually annotated by a subset of 21 landmarks depending on the face pose: All 21 landmarks are visible only on frontal faces. The precision of the manual annotation varies and it is generally not good for small faces. For this reason we omit faces smaller than 60 pixels. We created three random splits of the dataset into training, validation and test parts which had 7010, 2337 and 2337 faces on average respectively. The training faces were used to learn parameters of the landmark scoring functions and the validation faces to select the best regularization constant λ. The reported results are averages and standard deviations computed on the test parts of the three splits.


                        The 300W dataset was created by organizers of a recently running challenge on landmark detection. The dataset comprises six benchmarks: AFW, HELEN, IBUG, LPFW and XM2VTS. The images are annotated with 68 landmarks. The annotation is created by a semi-automatic method combining a manual annotation with a shape model of a face which makes the annotation precise and consistent. Unlike AFLW, the 300W dataset contains only near frontal faces with all 68 landmarks always visible. The dateset comes with a split to training and test parts which we use in our experiments. In particular, there are 518 test faces and 5639 training faces. We further split the training part to 5075 faces for learning the parameters of the landmark scoring functions and 564 faces used to select the best λ.

For the AFLW dataset, locations of 15 landmarks are estimated, in the case of the 300W dataset, 49 landmarks. The 15 and 49 landmark configurations are shown in Fig. 2. The 15 and 49 landmark sets correspond to the intersection of the landmarks annotated in the respective dataset and the 49 landmarks estimated by the Chehra 
                        [13] as a reference method in the evaluation.

Two sets are associated to each test image: a set of N
                        ∈{15,49} ground-truth landmark positions {x
                        1,…,
                        x
                        
                           N
                        }, and a set of landmark positions 
                           
                              
                                 
                                    x
                                    ̂
                                 
                                 1
                              
                              …
                              
                                 
                                    x
                                    ̂
                                 
                                 N
                              
                           
                         estimated by a detector under evaluation. The average relative localization error is computed as.
                           
                              (14)
                              
                                 ε
                                 =
                                 
                                    100
                                    zN
                                 
                                 
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       N
                                    
                                    
                                       
                                          
                                             x
                                             i
                                          
                                          −
                                          
                                             
                                                x
                                                ̂
                                             
                                             i
                                          
                                       
                                    
                                 
                              
                           
                        where z is a normalization factor. The interocular distance is the commonly used normalization factor. We use the interocular distance for the evaluation on the 300W dataset. For the AFLW dataset, however, the interocular distance is inappropriate because its value approaches zero for profile faces. For this reason we use the distance between the eyes center and the mouth center as the normalization factor for the AFLW dataset. Note that the eye-mouth distance is much less variable because it depends on the pitch angle which varies much less than the yaw, the angle influencing the interocular distance.

We evaluate the mean localization error for each test image and report its cumulative histogram. In order to characterize the detector performance by a single number we use A5 and A10 scores defined as a percentage of images with the average localization error at most 5% and 10% of the normalization factor, respectively. More formally: let T
                        ={ε
                        1,…,
                        ε
                        
                           m
                        } be a set of average localization errors (14) computed for all m images in the test set, and let T
                        5
                        ={ε
                        
                           i
                        
                        ∈
                        T
                        :
                        ε
                        
                           i
                        
                        ≤5}, and T
                        10
                        ={ε
                        
                           i
                        
                        ∈
                        T
                        :
                        ε
                        
                           i
                        
                        ≤10}. Then we define A-scores 
                           A
                           5
                           =
                           
                              100
                              m
                           
                           
                           card
                           
                           
                              
                                 T
                                 5
                              
                           
                        , and 
                           A
                           10
                           =
                           
                              100
                              m
                           
                           
                           card
                           
                           
                              
                                 T
                                 10
                              
                           
                        .

We compare the proposed method with the following approaches:
                           
                              1.
                              
                                 Chehra 
                                 [13] is an implementation of the recent state-of-the-art facial landmark detector provided by the authors. The detector uses a discriminative 3D facial deformable model which is fitted to the 2D image by a cascade of linear regressors. The detector was trained on the 300W dataset. It estimates a set of 49 landmarks defined on the contours of eyebrows, eyes, mouth and the nose, see Fig. 5(b). Since we use a commercial face detector and Chehra works with Matlab built-in detector in the release code, we apply the following strategy to ensure a fair comparison. The Matlab detector bounding box is used if it is available. If it fails, we predict the Matlab bounding box (position and scale) from our face detector bounding box using least squares. It seems that our detector is systematically displaced by about 5% of the face box side.


                                 Regression. We reimplemented a supervised descent method similar to [13]. It is a cascade of linear regressors trained on shape-indexed (32×32) SIFT features. The method estimates 2D landmark positions directly avoiding a parametric model [13]. More importantly, unlike [13], we trained regressors in a multi-view setup: independent view-specific regressors were trained for yaw intervals (−15∘,15∘), ±(15∘,45∘), ±(45∘,75∘), ±(75∘,90∘). A specialized regressor is selected based on the face detector yaw angle estimate.


                                 CLandmark 
                                 [9] is based on an open-source library [33] implementing the deformable part model landmark detector trained by the structured output support vector machines. We used the library to train independent view-specific DPM landmark detectors, for the same yaw angle ranges as above, i.e. (−15∘,15∘), ±(15∘,45∘), ±(45∘,75∘), ±(75∘,90∘).


                                 Zhu-Ramanan 
                                 [8] is a multi-view landmark detector using a mixture of deformable part models trained by the support vector machines. In contrast to CLandmark, the Zhu-Ramanan detector simultaneously estimates the yaw angle and the corresponding landmark configuration and hence does not require the initial yaw estimate

The proposed detector and Chehra use 3D shape model of the face which is fitted by a local optimization method. On the other hand, CLandmark and Zhu-Ramanan use a simplified 2D shape model fitted by a global optimization method.

The proposed detector is composed of multiple components. We evaluated several variants of the components in order to measure their impact on the final performance of the detector. The evaluated variants are described below.

We consider two different ways of obtaining the initial pose estimate {Φ,
                           t}. First, we use a commercial face detector based on Waldboost [23] which is able to detect non-frontal faces. Besides the bounding box, the face detector also returns a raw estimate of yaw angle γ. This gives an estimate of head orientation Φ
                           =(0,0,
                           γ). A position (and size) of the bounding box gives an initial estimate of head position t. Second, we use the DPM based multi-view landmark detector implemented by CLandmark (see description above). The DPM landmark returns a set of 2D landmarks which are used to estimate the heads pose {Φ,
                           t} by PnP method defined in Section 2.3. In the sequel we denote the first method as the face detector initialization and the second as the DPM initialization.

We consider two variants of the shape model. First, a rigid model given by a single shape configuration, namely by the mean shape. The rigid model corresponds to the PCA model (4) with M
                           =0 eigenvectors. Second, a flexible PCA model (4) representing the face shape with M
                           =10 eigenvectors. In the sequel we denote the first option as the rigid shape model and the second as the PCA shape model.

We consider two ways to deal with self-occlusions. First, we use a baseline approach which ignores the self-occlusions in which case the detector always fits all landmarks. Second, the detector uses the output of the CLandmark to determine a subset of invisible landmarks whose contribution is then removed from the data term (7) by setting the values of V
                           
                              i
                           .

The proposed initialization procedure relies on a face detector that outputs an estimate of the yaw angle of the head besides the bounding box, see Fig. 2. In the following experiment we measured the yaw estimation accuracy. The experiments was conducted on a subset of the Multi
                        −
                        PIE
                         dataset, described in Sec. 4, where the truth head yaw is known. The results are shown in Fig. 6
                        . The plot shows the mean absolute error of the yaw estimate in the range from −90° to 90°. The results are averaged over 100 subjects. The highest error is for angles around 60 degrees. The reason is probably the insufficient number of training examples for this yaw. The table below the plot is a confusion matrix whose off-diagonal elements show the frequency of wrong selection of the DPM model (calculated with 10° tolerance) due to the face detector error in yaw. The highest confusion occurs for the second semi-profile model (45°,75°) that is often mixed up with the full-profile model (75°,90°). Nevertheless, this confusion is not critical, since the corresponding DPMs have exactly the same landmarks, they only differ by inner potentials. Moreover, we will show in the next subsection that the proposed method can often recover from a very distant initialization.

In this experiment, the proposed algorithm was initialized from the perturbed ground-truth landmark annotation. On the 300W dataset, a zero-mean Gaussian noise with increasing standard deviation was added to a subset of up to 15 landmarks simulating the imperfect output of the DPM.

The rest of the pipeline was then executed. We measured the A5 and A10 scores after the PnP solution (A5-init, A10-init) and after the final 3D-optimization (A5-final, A10-final), see Fig. 7
                        . Standard deviation σ of the noise is a fraction of the interocular distance (iod), e.g. the maximum noise in the experiment is equal to the iod. Notice the proposed algorithm is fairly robust to the noisy initialization — very little performance drop occurs up to (extreme) noise of 40% of the iod.

The effect of using different pose initialization methods and different shape models was evaluated. Namely, we consider the face detector versus the DPM initialization and the rigid versus the PCA shape model. We evaluated the detector on the 300W dataset containing near frontal faces and also on a subset of non-frontal faces of the AFLW dataset. The non-frontal faces are those which have at least one rotation angle bigger than 45∘. The results are summarized in Fig. 8
                         and Table 1
                        .

The results show that a raw pose estimate provided by the face detector works equally well as the more precise DPM initialization on the 300W dataset containing the near frontal faces. However, more precise DPM detector significantly improves accuracy on the non-frontal faces of the AFLW dataset. The influence of the initialization method is more significant for the PCA shape model in which case the A10 score is 53.4% for the DPM initialization and only 39.4% for the face detector.

The PCA shape model gives substantial improvement over the rigid one when a dense set of 49 landmarks is estimated on the 300W dataset, namely, the A5 score is 12% for the rigid and 53% for the PCA shape model regardless the initialization method. On the other hand, the rigid model and the PCA model work equally well on the AFLW where only a smaller set of 15 landmarks is estimated which can be attributed to less precise annotation in the dataset.

The effect of modelling the self-occlusions compared to the baseline approach which always fits the full set of landmarks was evaluated. The performance of the two strategies was measured for the rigid and the PCA shape models using the non-frontal faces (at least one rotation angle bigger than 45°) of the AFLW dataset. Note that modelling occlusions on the 300W dataset makes little sense because in this dataset all landmarks are always visible. The localization error is summarized in Fig. 9
                         and Table 2
                        . It is seen that modelling visibility decreases the localization error regardless which shape model is used. Namely, modelling the self-occlusions increases the A10 score by 7.2% and by 11.0% for the rigid and the PCA shape model, respectively, if compared to the baseline always fitting all landmarks.

The proposed detector was compared with Chehra, Regression, Zhu-Ramanan and CLandmark. We evaluated the setup of the proposed detector which uses the DPM initialization, the PCA shape model and the self-occlusion modelling. The self-occlusion is not used on the 300W dataset where all landmarks are visible. The localization errors for the compared methods are summarized in Fig. 10
                         and Table 3
                        .

The proposed detector, Regression, and Chehra significantly outperform the DPM based methods Zhu-Ramanan and CLandmark on both datasets. The performance of Regression and Chehra is very similar. The comparison on 300W shows that the proposed detector has slightly smaller A5 score than Regression and Chehra, however, it has higher A10 score. In other words, the proposed detector is less prone to make significant errors compared to both Regression, Chehra. The dominance of the proposed detector is clearly seen on the more challenging AFLW dataset.

On the frontal subset, the A10 score is again the highest for the proposed method, followed by Regression and Chehra. Since Chehra is not trained on non-frontal images, the performance naturally drops on the entire AFLW dataset. The algorithm simply does not generalize to unseen views. The multi-view Regression is inferior to the proposed method in A10 score by about 5% on frontal and 3% on all image set, see Table 3.

The dominance of the proposed detector can be attributed to a better initialization and modelling self-occlusions which are components both missing in Chehra detector. The Regression handles self-occlusion by switching the view-specific regressors. However, the major problem of this approach we see is that training regressors (with shape-indexed features [11]) require images having complete annotation of landmarks in the view-specific subset. Images having only partial annotation with missing landmarks due to occlusions (by e.g. glasses, hairs, hands, food) cannot be used. This results that much smaller set of images is usable for training — the original training split comprised about 7k images, however only about 600–700 images per each of the four view-specific regressors could have been used for training. While the landmark scoring functions, employed in the proposed method, are trained per landmark independently and therefore partial annotation is acceptable. The proposed methods were trained using all 7k images, which could be a factor responsible for consistently higher A10 score.

The test images were ranked by the difference of the average localization errors, ε
                        Chehra
                        
                        −
                        ε
                        proposed
                         and ε
                        Regression
                        
                        −
                        ε
                        proposed
                         on 300W and AFLW respectively. The difference is high if the proposed detector performs better and small if the other detector is better. In Figs. 11 and 12
                        
                         we show detector outputs on examples from the 300W and the AFLW dataset with the highest and the lowest value of the average localization error difference respectively.

The average time required by the competing detectors to process a single face was compared. The reported numbers correspond to the wall-clock time measured on a notebook with dual core Intel CPU/2.60GHz/16GB RAM. The results are summarized in Table 4
                        . CLandmark is the fastest among the tested detectors followed by Regression. Chehra is around 2 and 8 times faster than the proposed detector on the AFLW and the 300W dataset, respectively. CLandmark being a representative of the DPM based detectors requires a constant time to evaluate all admissible landmark positions by the dynamic programming procedure. Chehra detector applies a fixed number of pose updates. The cost of each update is dominated by computation of the feature descriptors for the fixed set of 49 landmarks. The authors of Chehra in [13] did not reveal the particular feature descriptor used in their implementation. The proposed detector runs the L-BFGS algorithm to find a local optimum of the problem (5). The number of the L-BFGS iterations varies from 1 to the maximal number set to 50. The iteration cost is dominated by computation of the LBP features defining the score of the landmark scoring functions. It is seen that the variant estimating 49 landmarks on the 300W dataset is about 4 times slower than the variant estimating at most 15 landmarks (exact number varies based on the estimated yaw angle) on the AFLW dataset.

We see several possibilities to further speed the implementation up without losing on the accuracy, e.g. by caching the features of the landmark scoring functions. There is a natural trade-off between the accuracy and the efficiency. For instance, we can limit maximum number of L-BFGS iterations or shrink the size of the smoothing filter to compute the score derivatives, as briefly discussed in the end of Section 2.2.

The proposed algorithm participated in the 300W 2015 challenge.
                           2
                        
                        
                           2
                           
                              http://ibug.doc.ic.ac.uk/resources/300-W_IMAVIS/
                           
                         We submitted a binary code of the complete algorithm, Fig. 2, (including the face detector) and it was then executed by the challenge organizers on non-public test set images. The results are presented in Fig. 13
                        . For the challenge we trained the algorithm for all 68 landmark points (including the face outline). The 3D shape includes points attached to the 3D body of the head, thus invariant to a relative camera-head pose. The 2D landmarks are projections of the 3D shape into the image. However, a part of the outline landmarks annotated in the image that form the occluding contour violates this assumption and probably causes a lower performance of the 68-point over the 51-point model. The results of the challenge have not been released.

@&#CONCLUSION@&#

We have presented an efficient landmark detector which operates for a large set of viewing angles, from near frontal to profile views. The detection is precise which we quantitatively tested on two standard datasets (300W, AFLW) against four state-of-the-art methods: our multi-view implementation of [11,8,9,13]. The proposed detector has equal or better performance for near frontal images and performs consistently better for non-frontal images against all tested algorithms.

This success is achieved by a local optimization procedure coupled with a robust initialization scheme with a simple self-occlusion modelling. The local optimization jointly fits the 3D model pose and shape parameters and uses landmark scoring functions trained by a structured output SVM in a way that a distance to the true landmark position is taken into account. The initialization procedure starts by the multi-view face detector, then runs a 2D DPM model specific for a range of head yaw angles. The DPM works with low resolution images, so the initial landmark detector is not very precise, it finds the global optimum using a simple tree-based shape prior. The global optimum is likely to prevent the initial solution to contain outliers. Subsequently, the initial pose, which is passed to the final local optimization, is estimated by a PnP solver considering a mean 3D shape model. The visibility of landmarks is handled simply by switching off a contribution of occluded landmarks from the data term based on an estimate of the yaw angle, which turns out to be effective and helps significantly for non-frontal face images.

The state-of-the-art results were achieved despite shortcomings we are aware of. The current 3D PCA shape model is under-trained. It was trained using the Multi
                     −
                     PIE
                      dataset from a set of about 65 subjects only. The multi-view detector was trained on the AFLW dataset, however, we observed some limitations in the manual annotation which probably lead to a sub-optimal training of the landmark scoring functions.

@&#ACKNOWLEDGEMENTS@&#

The first and the last author were supported by the Czech Science Foundation Project GACR 
                  P103/12/G084. The second author was supported by the project ERC-CZ 
                  LL1303.

@&#REFERENCES@&#

