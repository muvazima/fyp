@&#MAIN-TITLE@&#Functional grouping of similar genes using eigenanalysis on minimum spanning tree based neighborhood graph

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           Eigenanalysis on spanning tree neighborhood graph(E-MST) for clustering is introduced.


                        
                        
                           
                           Diameter based criteria to determine the sufficiency of neighborhood is proposed.


                        
                        
                           
                           The objective criterion reveals spectral properties of neighborhood graph.


                        
                        
                           
                           Experimental analysis shows the impressive results of E-MST.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Gene expression analysis

Minimum Spanning Tree

Spectral clustering

Similarity graph,

Microarray analysis

@&#ABSTRACT@&#


               
               
                  Gene expression data clustering is an important biological process in DNA microarray analysis. Although there have been many clustering algorithms for gene expression analysis, finding a suitable and effective clustering algorithm is always a challenging problem due to the heterogeneous nature of gene profiles. Minimum Spanning Tree (MST) based clustering algorithms have been successfully employed to detect clusters of varying shapes and sizes. This paper proposes a novel clustering algorithm using Eigenanalysis on Minimum Spanning Tree based neighborhood graph (E-MST). As MST of a set of points reflects the similarity of the points with their neighborhood, the proposed algorithm employs a similarity graph obtained from 
                        
                           
                              k
                           
                           
                              ′
                           
                        
                      rounds of MST (
                        
                           
                              k
                           
                           
                              ′
                           
                        
                     -MST neighborhood graph). By studying the spectral properties of the similarity matrix obtained from 
                        
                           
                              k
                           
                           
                              ′
                           
                        
                     -MST graph, the proposed algorithm achieves improved clustering results. We demonstrate the efficacy of the proposed algorithm on 12 gene expression datasets. Experimental results show that the proposed algorithm performs better than the standard clustering algorithms.
               
            

@&#INTRODUCTION@&#

Microarray technology is a boon for biological researchers to monitor the expression of thousands of genes simultaneously. The results of microarray experiments are often organized as gene expression matrices whose rows represent genes and columns represent various environmental conditions or samples such as tissues. Set of genes with similar expression patterns are called as co-expression genes and extracting such genes from microarray experiments is an important task as it helps in identifying the group of genes involved in the same cellular processes [1,2].

Clustering is an unsupervised learning technique that is used extensively in various exploratory analyses to discover natural grouping in a given set of objects. The objective of gene-based clustering is to partition the microarray genes into k distinct groups, where k is the number of clusters which may or may not be known in advance. Genes with similar expression profiles should be put into a single cluster which corresponds to a particular macroscopic phenotype, such as clinical syndromes or cancer types [1]. Many clustering algorithms have been proposed in the literature for gene expression analysis [5,9,10,14,18,26,28].

Various approaches to clustering can be broadly categorized into hierarchical and partitional methods [3,4]. While hierarchical clustering generates a nested sequence of partitions in the form of dendrogram tree, partitional clustering directly divides the dataset into k clusters [1]. The traditional approaches like hierarchical and partitional methods lack in ability to detect the intrinsic clusters because of the heterogeneous nature of gene profiles [6].

Recently more new techniques using graph theory have been proposed for gene expression analysis. Graph models are widely studied in various problem domains due to their ability to represent complex data. In graph-based clustering, the gene profiles are represented as a weighted undirected graph 
                        G
                        =
                        (
                        V
                        ,
                        E
                        )
                      called as gene network, where each node 
                        v
                        ϵ
                        V
                      corresponds to a gene and the edgeset E represents the dissimilarity (distance) between the genes [8]. Gene expression data are often highly connected and clustering algorithms based on graph connectivity are becoming popular in identifying the co-expression genes with highly intersecting profiles. Refs. [9,10,14,18] witness the effectiveness of graph-based clustering algorithms for the identification of similar genes based on their connectivity.

Given a connected, undirected graph, a spanning tree of the graph is an acyclic subgraph that spans all the vertices in the graph. Minimum Spanning tree (MST) is the spanning tree with minimum weight. MST is a well-known combinatorial optimization problem that is successfully applied for various tasks such as image segmentation and cluster analysis [14–18]. The key idea of MST based clustering is to identify and remove the inconsistent edges in order to obtain a set of clusters. Besides having a number of clustering algorithms using MST, an unified approach to detect and remove the inconsistent edges of MST that results in desired cluster structure is always being a challenging one.

Another popular clustering method that is predominantly used in several fields of data analysis is spectral clustering. Spectral methods find a wide range of practical applications including bio-informatics [26,28]. The core of spectral clustering is the Laplacian matrix which is the difference between degree and adjacency (weight) matrices of the similarity graph. The spectrum (eigenvalues) of the Laplacian matrix is used to partition the given dataset [24].

Although spectral methods are widely discussed, there has been a little attention on how the similarity graph is constructed from the given set of points. It is worthwhile to note that the results of spectral clustering depend on the choice of similarity graph used to encode the given dataset [20]. A similarity graph should depict the underlying structure of the dataset in order to achieve better clustering results. The most commonly used similarity graphs in the literature are ε-neighborhood graph, K-nearest neighbor graph and fully connected graph [19]. To the best of our knowledge, no theoretical study has been done about which similarity graph suits a particular dataset and how to choose the parameters e.g., ε, K, σ in case of ε-neighborhood graph, K-nearest neighbor graph and fully connected graph respectively.

Most of the algorithms become inefficient when applied on heterogeneous datasets which are diverse in shape, size and densities. This is a common problem in microarray analysis. Moreover, microarray datasets often contain noise and outliers and thus finding a robust clustering algorithm is always a challenging problem [1]. As MST-based and spectral clustering methods have shown better clustering performance in identifying arbitrary shaped clusters, the proposed algorithm E-MST aims to integrate these two methods. Our proposed algorithm is free from any user-defined parameters.

The rest of the paper is organized as follows. In Section 2, related work in gene data clustering is presented with particular attention given to the graph based cluster analysis. Section 3 highlights our contribution. Spectral clustering method is introduced in Section 4. The proposed algorithm is described in Section 5. The results of experimental validation are reported and discussed in Section 6, and finally our research is concluded in Section 7.

@&#RELATED WORK@&#

Hierarchical clustering algorithms have been used extensively for microarray analysis due to their ability to describe the clustering results through multi-level visual structure. Eisen et al. applied hierarchical agglomerative clustering for gene expression analysis and also developed a graphical display called as Eisen plot to aid visual illustration of group of genes that share similar expression patterns [5]. The hierarchical algorithms work on distance matrix of 
                        O
                        (
                        
                           
                              n
                           
                           
                              2
                           
                        
                        )
                      computations, which is expensive for large datasets.

As opposed to hierarchical approach, partitional algorithms directly divide the data points into number of clusters without imposing the hierarchical structure. K-means is one of the most popular partitional clustering algorithms used for gene expression analysis due to its ease of implementation and linear complexity [1]. However, K-means is highly sensitive to the choice of initial centers. If the initial centers are improperly chosen, then the algorithm may converge to a local optimum [1,7].

Graph based clustering algorithms are becoming popular in recent years as they seek partition based on connectivity rather than on centroid. CLuster Identification via Connectivity Kernels (CLICK) [9] models the gene expression clustering as a graph partitioning problem. It iteratively determines the min-cut to partition the graph into highly connected components according to certain homogeneity threshold. The results were further refined using two post-pruning steps such as adoption and merging. The results of CLICK demonstrated its improved cluster quality in terms of cluster separation and homogeneity. However it may not perform well while the gene expression data contains highly intersecting profiles, which is a commonly occurring scenario in microarray experiments [1].

Clustering gene network by exploiting neighborhood properties of each gene in the network is also widely studied in the microarray analysis [10–12]. Nearest Neighbor Network (NNN) algorithm makes use of mutual nearest neighborhood principle to extract more complex and biologically relevant clusters [10]. The NNN algorithm achieves higher precision in detecting functionally related genes. However the results of the algorithm depend on the size of the neighborhood [8].

Ruan et al. applied an efficient graph partitioning algorithm QCut to identify dense subgraphs from the gene co-expression network [11]. First a rank-based gene network is constructed and then QCut algorithm with an objective function called modularity is used to automatically determine the optimal partitioning and the number of partitions. The authors of the paper believe that the rank-based co-expression networks (those that utilize the rank-transformed similarities) can better capture the global topology of the network, identifying both strongly and weakly co-expressed modules, whereas the conventional value-based methods (those that utilize similarity values) can only exhibit the strongly co-expressed modules.

Baya et al. proposed penalized K-Nearest Neighbor graph based clustering for gene expression analysis (PKNNG) [12]. They first construct KNN graph of the given gene expression data for low value of K in the range 3–7. If the KNNG is disconnected, then they connect the subgraphs by adding edges with an exponentially penalized weight 
                        W
                        =
                        d
                        (
                        i
                        ,
                        j
                        )
                        ⁎
                        
                        
                           
                              exp
                           
                           
                              d
                              (
                              i
                              ,
                              j
                              )
                           
                        
                     . Different connecting schemes discussed in [12] are: MinSpan (uses edges of the minimum spanning set to connect the subgraphs), AllSubgraphs (connects each subgraph to all other subgraphs using minimum length edges), AllEdges (similar to fully connected graph, but the edges across different subgraphs have penalized weight) and Medoid (connects medoids of each subgraph to medoid of remaining subgraphs). Once a connected graph known as Penalized KNNG (PKNNG) is obtained, the underlying neighborhood relation is used as a metric for measuring the pairwise similarity of the points and any traditional clustering approaches such as K-means or hierarchical methods can utilize this metric for clustering. They reported that PKNNG with MinSpan connecting scheme and applying PAM as a final clustering method have been shown to provide the better clustering results.

MST of a set of points can be used to reflect the similarity of the points with their neighborhood and simply removing 
                        k
                        −
                        1
                      inconsistent edges (for eg. longest edges) from the MST results in k disjoint subsets such that each subset would represent a cluster [13]. The key advantage of the MST-based clustering algorithm is that the points of a cluster are neither grouped around cluster center nor separated by a geometric boundary. Thus the performance of these clustering algorithms do not depend on the shape of the cluster and they can detect clusters of irregular boundaries [14,15].

Identifying a suitable measure of inconsistent edges to partition the MST is a crucial step in MST-based clustering algorithms. In most of the cases, longest edges of the MST may not actually represent the inconsistent edges and this is illustrated in Fig. 1
                     . Many researchers proposed different partitioning criteria [14–18]. Xu et al. proposed a MST based clustering algorithm for gene expression analysis using three objective functions namely “removing long MST edges”, “center based iterative clustering algorithm” and “a globally optimal clustering algorithm” to address various clustering problems [14].

More recently, Pirim et al. proposed a new MST based heuristic B-MST which tries to identify and remove the high-betweenness edges from the MST to identify more biological clusters [18]. The initial clustering results from B-MST were refined using an objective function called as Tightness and Separation Index (TSI) to simultaneously maximize the intra-cluster similarity and minimize the inter-cluster similarity. Given a set of genes, first the gene co-expression network is constructed where the edges between genes are weighed using Pearson-correlation measure. During the first phase of the algorithm, the MST of the co-expression network is used to find an initial clustering solution. Betweenness values of the edges are computed and the edge with the highest betweenness score is removed from the MST. Edge removal continues as long as 
                        k
                        −
                        1
                      edges are removed from the MST to obtain k clusters. During the second phase, the co-expression network is transformed to a binary network retaining only the strongest edges whose similarity is higher than the given threshold. Then TSI values are computed for the binary graph. The initial solution is refined by applying a local search procedure which minimizes the TSI.

Although there have been a number of MST-based clustering algorithms, no standard method exists to identify and remove the inconsistent edges from the MST to obtain correct cluster structure. Moreover, some algorithms require certain hidden parameters to get optimal clustering results. To name a few, (i) 2MSTClus algorithm proposed in [15] makes use of hidden parameters such as validity of the graph-cut (λ), number of inconsistent vertices β; (ii) CLICK algorithm uses similarity threshold for merging subgraphs during adoption phase [9], (iii) Nearest Neighbor Network (NNN) algorithm identifies clusters based on neighborhood size parameter [10]. Small changes in these parameters may lead to different clustering structure and thus choosing the suitable values of these parameters demands domain expertise [15,17].

Spectral clustering has been widely applied in bio-informatics. It clusters the points using eigenvectors of the graph Laplacian matrix L which is defined as 
                        L
                        =
                        D
                        −
                        A
                     , where D and A are the degree and adjacency matrices of the data similarity graph. Once eigenvectors of the matrix L are obtained, spectral clustering is carried out either as recursive bi-partitioning [21] or as multi-way partitioning [22]. In spectral bi-partitioning, the information contained in the second eigenvector corresponding to second smallest eigenvalue is used to partition the data points and the bi-partitioning continues recursively until the desired number of clusters is obtained [21]. In spectral multi-way partitioning, the information contained in the first k eigenvectors corresponding to first k smallest eigenvalues are used to form a 
                        n
                        ×
                        k
                      matrix U, where each of the k eigenvectors is stacked as a column in U. This new representation is called as the transformed space and a clustering algorithm such as K-means is applied on the matrix U, treating each row as a data point. Clusters are easily detected in the new representation [19,22].

Tritchler et al. [26] presented a spectral clustering algorithm for microarray data using recursive bi-section of the constrained co-variance matrix along the principal direction. The added constraints make the partitions of the co-variance matrix with high values in the diagonal blocks and low off-diagonal elements. The diagonal blocks represent the clusters with high correlation [26]. Higham et al. [28] proved the effect of normalized and unnormalized Laplacian matrix on the results of spectral clustering, both theoretically and experimentally. They demonstrated the performance of the spectral algorithms on three different microarray data sets and shown that the normalized version worked superior to unnormalized version in revealing the biologically relevant clusters [28].

Although spectral clustering has emerged as a powerful tool for data analysis, the impact of graph construction technique on the performance of spectral methods has not been studied in depth [19,20]. Moreover, the standard graph construction methods heavily depend on the choice of parameters, which is not always easy to code.

The objective of this paper is to obtain a robust and parameter-free similarity graph using multiple rounds of MSTs and applying eigenanalysis on the graph to identify similar groups of genes. Our contribution lies here in integrating the advantages of both MST and spectral clustering. One common feature that both these algorithms have is that they detect clusters by seeking continuity of the data rather than by partitioning based on centroid. While the former algorithm infers the similarity of the points directly on the given feature space, the later algorithm infers the similarity in the transformed space. With this notion, this paper presents an improved clustering algorithm by carrying out eigenanalysis on the spanning subgraph obtained from multiple rounds of MSTs (
                        
                           
                              k
                           
                           
                              ′
                           
                        
                     -MST neighborhood graph). The key highlights of the paper is summarized as follows:
                        
                           (i)
                           We propose a similarity graph for spectral clustering by exploiting the neighborhood principle of multiple rounds of MSTs (
                                 
                                    
                                       k
                                    
                                    
                                       ′
                                    
                                 
                              -MST neighborhood graph).

We suggest a diameter-based criteria for determining the number of rounds of MST 
                                 
                                    
                                       k
                                    
                                    
                                       ′
                                    
                                 
                              . As the diameter of the graph (the maximum distance between any two vertices in the graph) is closely related with graph spectrum [29], we derive a terminating condition to fix 
                                 
                                    
                                       k
                                    
                                    
                                       ′
                                    
                                 
                               using diameter-based function.

We consider 12 gene expression datasets for testing the proposed algorithm. As the 
                                 
                                    
                                       k
                                    
                                    
                                       ′
                                    
                                 
                              -MST graph contains most discriminating edges, applying spectral clustering on such graph leads to improved clustering results.

Through experimental results, we show that the proposed algorithm is invariant to (a) the choice of distance measure, (b) the choice of post-clustering phase used in multi-way spectral partitioning, and (c) the normalization of the Laplacian matrix.

We also demonstrate the use of eigengap statistics for estimating the number of clusters in a dataset.

Spectral clustering is a popular clustering method which makes use of the eigenvectors of the data similarity graph to partition the dataset. Lots of work have been carried out on spectral clustering and the notable references can be seen in [21,22,27]. Given a dataset of random points, first the similarity graph which model the underlying structure of the dataset is constructed and then the Laplacian matrix of the graph is computed as follows [19]:
                        
                           
                              Unnormalizedform
                              :
                              
                              L
                              =
                              D
                              −
                              A
                              .
                              Normalizedform
                              :
                              
                              
                                 
                                    L
                                 
                                 
                                    sym
                                 
                              
                              =
                              
                                 
                                    D
                                 
                                 
                                    −
                                    1
                                    /
                                    2
                                 
                              
                              
                                 
                                    LD
                                 
                                 
                                    −
                                    1
                                    /
                                    2
                                 
                              
                           
                        
                     where A is the adjacency matrix (weight matrix) of the similarity graph where each 
                        A
                        (
                        i
                        ,
                        j
                        )
                      denotes weight of the edge 
                        (
                        
                           
                              x
                           
                           
                              i
                           
                        
                        ,
                        
                           
                              x
                           
                           
                              j
                           
                        
                        )
                      and D is the degree matrix which is a diagonal matrix where each 
                        D
                        (
                        i
                        ,
                        i
                        )
                      represents the total weight of edges incident on vertex x
                     
                        i
                     .

Spectral clustering may be carried out as either recursive bi-partitioning [21] or multi-way partitioning [22]. In spectral bi-partitioning, the information contained in the second eigenvector corresponding to second smallest eigenvalue is used to partition the data points and the bi-partitioning continues recursively until the desired number of clusters is obtained. In spectral multi-way partitioning, the information contained in the first k eigenvectors corresponding to first k smallest eigenvalues are used to form an 
                        n
                        ×
                        k
                      matrix U, where each eigenvector is stacked as a column in U 
                     [19,22]. This new representation is called as the transformed space and a clustering algorithm such as K-means is applied on the matrix U, treating each row as a data point. The eigenvectors can be determined using the well known algorithms such as QR algorithm, power iteration and Lanczos methods [23]. The details of the spectral clustering with multi-way partitioning are given in Algorithm 1.

The performance of the spectral clustering depends on the choice of similarity graph that is used to model the given dataset [20]. Different similarity graphs have been studied in the literature and the most commonly used graph construction methods are as follows [19]: 
                        Definition 1
                        
                           
                              ε
                           -Neighborhood graph
                        


                        Given a set of points (vertices), ε-neighborhood graph connects the points whose pairwise distance is smaller than ε, where ε is an user-specified parameter.

A pair of points x
                           
                              i
                            and x
                           
                              j
                            are said to be connected in KNN- graph, if x
                           
                              j
                            belongs to the set of K-nearest neighbors of x
                           
                              i
                           .

All those points with non-zero similarity are connected with each other, where similarity is defined using the Gaussian similarity function 
                              s
                              (
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    x
                                 
                                 
                                    j
                                 
                              
                              )
                              =
                              exp
                              (
                              −
                              ∥
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              −
                              
                                 
                                    x
                                 
                                 
                                    j
                                 
                              
                              
                                 
                                    ∥
                                 
                                 
                                    2
                                 
                              
                              /
                              (
                              2
                              
                                 
                                    σ
                                 
                                 
                                    2
                                 
                              
                              )
                              )
                           . Here the parameter σ is used to control the width of the neighborhood.

Spectral clustering [19]. 
                              
                                 
                                    
                                    
                                       
                                          
                                             Input: Dataset 
                                                X
                                                =
                                                {
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      1
                                                   
                                                
                                                ,
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      2
                                                   
                                                
                                                ,
                                                …
                                                ,
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      n
                                                   
                                                
                                                }
                                             , Number of clusters k.
                                       
                                       
                                          
                                             Output: 
                                             k clusters.
                                       
                                       
                                          
                                             1. Construct similarity graph 
                                                G
                                                =
                                                (
                                                V
                                                ,
                                                E
                                                )
                                              of the dataset X.
                                       
                                       
                                          
                                             2. Compute the Laplacian matrix 
                                                L
                                                =
                                                D
                                                −
                                                A
                                             .
                                       
                                       
                                          
                                             3. Compute the first k eigenvectors 
                                                
                                                   
                                                      v
                                                   
                                                   
                                                      1
                                                   
                                                
                                                ,
                                                
                                                   
                                                      v
                                                   
                                                   
                                                      2
                                                   
                                                
                                                ,
                                                …
                                                ,
                                                
                                                   
                                                      v
                                                   
                                                   
                                                      k
                                                   
                                                
                                             .
                                       
                                       
                                          
                                             4. Form a 
                                                n
                                                ×
                                                k
                                              matrix U, where each eigenvector 
                                                
                                                   
                                                      v
                                                   
                                                   
                                                      i
                                                   
                                                
                                                ,
                                                1
                                                ≤
                                                i
                                                ≤
                                                k
                                              is stacked as a column in U.
                                       
                                       
                                          
                                             5. Treat each row 
                                                
                                                   
                                                      u
                                                   
                                                   
                                                      i
                                                   
                                                
                                                ,
                                                1
                                                ≤
                                                i
                                                ≤
                                                n
                                              of U as a data point.
                                       
                                       
                                          
                                             6. Apply a clustering algorithm to partition the points 
                                                
                                                   
                                                      (
                                                      
                                                         
                                                            u
                                                         
                                                         
                                                            i
                                                         
                                                      
                                                      )
                                                   
                                                   
                                                      i
                                                      =
                                                      1
                                                      ,
                                                      2
                                                      ,
                                                      …
                                                      ,
                                                      n
                                                   
                                                
                                              into k clusters.
                                       
                                    
                                 
                              
                           
                        

One of the major problems in constructing the similarity graph is how to tune the graph parameters. For example, the choice of ε should be chosen such that the resulting ε-neighborhood graph must portray the similarity of the points with their neighbors. Fig. 2
                      demonstrates spectral clustering with ε-neighborhood graph on the dataset shown in Fig. 1a. Similarly, Figs. 3 and 4
                     
                      demonstrate spectral clustering with KNN graph and fully-connected graph on the same dataset respectively. It is clearly understood that results of spectral clustering depend on the choice of similarity graph.

This section describes the details of the proposed algorithm named as Eigenanalysis on Minimum Spanning Tree based neighborhood graph (E-MST). The reason for employing eigenanalysis on a subgraph obtained from multiple rounds of MSTs is as follows. The MST of a set of points reflects the similarity of the points with their neighborhood. In order to collect more information on neighborhood of a point, multiple rounds of MSTs are used [16]. Thus the graph obtained from multiple rounds of MSTs can effectively model the local neighborhood of the points and so can be used as a similarity graph in spectral clustering.

Let 
                        
                           
                              k
                           
                           
                              ′
                           
                        
                      denote the number of rounds of MSTs. The 
                        
                           
                              k
                           
                           
                              ′
                           
                        
                     -MST graph is defined as follows. 
                        Definition 4
                        
                           
                              
                                 
                                    k
                                 
                                 
                                    ′
                                 
                              
                           -MST neighborhood graph
                        


                        Let 
                              G
                              =
                              (
                              V
                              ,
                              E
                              )
                            be the weighted undirected graph of the dataset X such that the vertex set 
                              V
                              =
                              {
                              X
                              }
                            and the edge set 
                              E
                              =
                              {
                              (
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    x
                                 
                                 
                                    j
                                 
                              
                              )
                              |
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    x
                                 
                                 
                                    j
                                 
                              
                              ϵ
                              V
                              }
                           . The edges are weighted using a dissimilarity measure 
                              d
                              (
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    x
                                 
                                 
                                    j
                                 
                              
                              )
                            (e.g., Euclidean distance). The first round of MST T
                           1 is constructed from G. The subsequent MSTs are constructed by excluding the edges of the previous round of MST from G, i.e., T
                           
                              i
                            is obtained from 
                              G
                              =
                              (
                              
                                 V
                                 ,
                                 E
                                 ⧹
                                 
                                    
                                       
                                          ∪
                                       
                                    
                                    
                                       j
                                       =
                                       1
                                    
                                    
                                       j
                                       =
                                       i
                                       −
                                       1
                                    
                                 
                                 
                                    
                                       T
                                    
                                    
                                       j
                                    
                                 
                              
                              )
                           , where 
                              2
                              ≤
                              i
                              ≤
                              
                                 
                                    k
                                 
                                 
                                    ′
                                 
                              
                           . Then 
                              
                                 
                                    k
                                 
                                 
                                    ′
                                 
                              
                           -MST graph is defined as 
                              
                                 
                                    G
                                 
                                 
                                    ′
                                 
                              
                              =
                              (
                              
                                 V
                                 ,
                                 
                                    
                                       
                                          ∪
                                       
                                    
                                    
                                       j
                                       =
                                       1
                                    
                                    
                                       j
                                       =
                                       
                                          
                                             k
                                          
                                          
                                             ′
                                          
                                       
                                    
                                 
                                 
                                    
                                       T
                                    
                                    
                                       j
                                    
                                 
                              
                              )
                           .


                     Fig. 5
                      illustrates 2-MST graph of the dataset shown in Fig. 1a.

The proposed algorithm uses 
                        
                           
                              k
                           
                           
                              ′
                           
                        
                     -MST graph 
                        
                           
                              G
                           
                           
                              ′
                           
                        
                        =
                        (
                        
                           V
                           ,
                           
                              
                                 
                                    ∪
                                 
                              
                              
                                 j
                                 =
                                 1
                              
                              
                                 j
                                 =
                                 
                                    
                                       k
                                    
                                    
                                       ′
                                    
                                 
                              
                           
                           
                              
                                 T
                              
                              
                                 j
                              
                           
                        
                        )
                      as the similarity graph. Let 
                        
                           
                              A
                           
                           
                              ′
                           
                        
                      and 
                        
                           
                              D
                           
                           
                              ′
                           
                        
                      denote the adjacency matrix and degree matrix of 
                        
                           
                              G
                           
                           
                              ′
                           
                        
                      respectively. The Laplacian matrix is computed as 
                        L
                        =
                        
                           
                              D
                           
                           
                              ′
                           
                        
                        −
                        
                           
                              A
                           
                           
                              ′
                           
                        
                     . Once matrix L is defined in the above manner, next step is to determine the eigenvalues and eigenvectors of L. The proposed algorithm is implemented as multi-way partitioning using first k eigen vectors. Let 
                        
                           
                              λ
                           
                           
                              1
                           
                        
                        ,
                        
                           
                              λ
                           
                           
                              2
                           
                        
                        ,
                        …
                        ,
                        
                           
                              λ
                           
                           
                              k
                           
                        
                      be the first k smallest eigenvalues and let 
                        
                           
                              v
                           
                           
                              1
                           
                        
                        ,
                        
                           
                              v
                           
                           
                              2
                           
                        
                        ,
                        …
                        ,
                        
                           
                              v
                           
                           
                              k
                           
                        
                      be their corresponding eigenvectors. These eigenvectors are stacked as columns in a 
                        n
                        ×
                        k
                      matrix U which denote the new representation of the data points in the transformed space. Each row u
                     
                        i
                      in the matrix U represents a k-dimensional vector that corresponds to the ith data point. Finally a clustering algorithm is applied on the rows of U to get k clusters.

In order to figure out the value of the parameter number of rounds of MST (
                           
                              
                                 k
                              
                              
                                 ′
                              
                           
                        ) automatically, we make use of an objective function involving diameter of the graph (D), which is defined as the maximum shortest-path distance between any two vertices in the graph. The motivation behind diameter-based objective function is that diameter of the graph is closely related to spectrum of the graph. One of the relations connecting the graph׳s diameter and second smallest eigenvalue of the Laplacian matrix λ
                        2 is as follows [29]:
                           
                              (1)
                              
                                 D
                                 ≥
                                 
                                    
                                       4
                                    
                                    
                                       n
                                       
                                          
                                             λ
                                          
                                          
                                             2
                                          
                                       
                                    
                                 
                              
                           
                        
                     

From graph theory point of view, well-connected graphs have more number of shortest paths between the vertices and hence the diameter D is smaller. The connectivity of the graph is also defined using the second smallest eigenvalue λ
                        2 which is called as the algebraic connectivity of the graph. The multiplicity of λ
                        2 provides the number of connected components in the graph. λ
                        2 is monotone increasing function with respect to edgeset [30]. Let 
                           G
                           =
                           (
                           V
                           ,
                           E
                           )
                         be a graph and let G
                        1 and G
                        2 be two induced subgraphs of G with the same set of vertices V such that 
                           
                              
                                 E
                              
                              
                                 1
                              
                           
                           ⊆
                           
                              
                                 E
                              
                              
                                 2
                              
                           
                        . Then 
                           
                              
                                 λ
                              
                              
                                 2
                              
                           
                           (
                           
                              
                                 G
                              
                              
                                 1
                              
                           
                           )
                           ≤
                           
                              
                                 λ
                              
                              
                                 2
                              
                           
                           (
                           
                              
                                 G
                              
                              
                                 2
                              
                           
                           )
                         
                        [30]. The more connected graphs have larger second-smallest eigenvalue λ
                        2 and conversely a smaller λ
                        2 provides optimal bi-section of the graph as compared to graphs with larger λ
                        2. Hence the objective of graph construction phase in the proposed algorithm is to keep the λ
                        2 as smaller as possible.

Let 
                           G
                           =
                           (
                           V
                           ,
                           E
                           )
                         be the complete graph obtained from the given dataset X. Let T
                        1 be the MST of G. Let 
                           
                              
                                 G
                              
                              
                                 ′
                              
                           
                           =
                           (
                           V
                           ,
                           
                              
                                 E
                              
                              
                                 ′
                              
                           
                           )
                         be the similarity graph to be constructed, where the edge set 
                           
                              
                                 E
                              
                              
                                 ′
                              
                           
                         is initialized to T
                        1. As the proposed algorithm starts from the MST T
                        1 of the dataset and progressively adds edge-disjoint MSTs to the similarity graph, λ
                        2 of the graph increases with every round. To keep λ
                        2 as smaller as possible, the number of rounds of MSTs 
                           
                              
                                 k
                              
                              
                                 ′
                              
                           
                         should also be smaller. In order to save the computation of λ
                        2 at every round, we use diameter based criteria as follows from Eq. (1). So the optimal value of 
                           
                              
                                 k
                              
                              
                                 ′
                              
                           
                         corresponds to smaller value of λ
                        2 which in turn corresponds to larger value of diameter D. As we commence by adding edge-disjoint MSTs T
                        
                           i
                         to 
                           
                              
                                 G
                              
                              
                                 ′
                              
                           
                        , where 
                           i
                           ≥
                           2
                        , the vertices are connected by more shortest paths. This indicates that the vertices are structurally closer to each other and hence the diameter D of the graph either decreases or maintains the same value as the previous round i.e. 
                           
                              
                                 D
                              
                              
                                 j
                              
                           
                           ≤
                           
                              
                                 D
                              
                              
                                 i
                              
                           
                        , 
                           i
                           <
                           j
                        . Accordingly, λ
                        2 increases as D decreases in every round. At certain point, adding more MST edges may not introduce any change in the diameter, and this is the point where graphs in subsequent iterations are structurally similar. With the intuition that structurally similar graphs results in similar partitioning, we stop at this round.

Let 
                           Δ
                           D
                         denote the change in diameter of the graphs in round 
                           i
                           −
                           1
                         and i i.e. 
                           
                              
                                 Δ
                                 D
                                 =
                                 
                                    
                                       D
                                    
                                    
                                       i
                                       −
                                       1
                                    
                                 
                                 −
                                 
                                    
                                       D
                                    
                                    
                                       i
                                    
                                 
                              
                           
                        
                     

Let 
                           Y
                           =
                           
                              
                                 4
                              
                              
                                 n
                                 
                                    
                                       λ
                                    
                                    
                                       2
                                    
                                 
                              
                           
                         then, 
                           
                              
                                 Δ
                                 Y
                                 =
                                 
                                    
                                       Y
                                    
                                    
                                       i
                                       −
                                       1
                                    
                                 
                                 −
                                 
                                    
                                       Y
                                    
                                    
                                       i
                                    
                                 
                                 =
                                 
                                    
                                       4
                                    
                                    
                                       n
                                    
                                 
                                 (
                                 
                                    
                                       
                                          1
                                       
                                       
                                          
                                             
                                                λ
                                             
                                             
                                                2
                                             
                                             
                                                i
                                                −
                                                1
                                             
                                          
                                       
                                    
                                    −
                                    
                                       
                                          1
                                       
                                       
                                          
                                             
                                                λ
                                             
                                             
                                                2
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                 
                                 )
                              
                           
                        
                     

From Eq. (1), 
                           
                              
                                 D
                                 −
                                 Y
                                 ≥
                                 0
                                 ⇒
                                 
                                    
                                       D
                                    
                                    
                                       i
                                       −
                                       1
                                    
                                 
                                 −
                                 
                                    
                                       D
                                    
                                    
                                       i
                                    
                                 
                                 ≥
                                 
                                    
                                       Y
                                    
                                    
                                       i
                                       −
                                       1
                                    
                                 
                                 −
                                 
                                    
                                       Y
                                    
                                    
                                       i
                                    
                                 
                                 ⇒
                                 Δ
                                 D
                                 ≥
                                 Δ
                                 Y
                                 .
                              
                           
                        
                     

The first place where we get 
                           Δ
                           D
                           =
                           0
                         is an indication of stopping criteria. We can stop at this round of 
                           
                              
                                 k
                              
                              
                                 ′
                              
                           
                        . Hence during 
                           
                              
                                 k
                              
                              
                                 ′
                              
                           
                        -MST graph construction, just by comparing 
                           Δ
                           D
                        , we can determine 
                           
                              
                                 k
                              
                              
                                 ′
                              
                           
                        . The parameter 
                           
                              
                                 k
                              
                              
                                 ′
                              
                           
                         is a small constant and can be set in the range 
                           2
                           ≤
                           
                              
                                 k
                              
                              
                                 ′
                              
                           
                           ≤
                           10
                        . We empirically observed that the best clustering results are achieved within 10 rounds and for 
                           
                              
                                 k
                              
                              
                                 ′
                              
                           
                           >
                           10
                        , the results are not improved further. The proposed algorithm is summarized in Algorithm 2.

The complexity of the proposed algorithm is analyzed as follows. Time required to construct edge-disjoint 
                           
                              
                                 k
                              
                              
                                 ′
                              
                           
                         rounds of MSTs is 
                           O
                           (
                           
                              
                                 n
                              
                              
                                 2
                              
                           
                           )
                        , considering 
                           
                              k
                              ′
                           
                           <
                           <
                           n
                        . Diameter of the graph can be determined by performing a Breadth-First Search (BFS) from every node of the graph and the maximum shortest path distance is the diameter of the graph. This operation can be accomplished in 
                           O
                           (
                           
                              
                                 n
                              
                              
                                 2
                              
                           
                           )
                         time. Hence the total time to construct the similarity graph by the proposed algorithm is 
                           O
                           (
                           
                              
                                 n
                              
                              
                                 2
                              
                           
                           )
                        . The complexity of the spectral clustering 
                           O
                           (
                           
                              
                                 n
                              
                              
                                 3
                              
                           
                           )
                         in general. We need to compute only first k eigenvectors for clustering the data points. There are efficient algorithms to compute the first k eigenvectors of the sparse matrix such as Lanczos and Arnoldi method [19,31]. As the 
                           
                              
                                 k
                              
                              
                                 ′
                              
                           
                        -MST neighborhood graph is sparse graph containing only 
                           O
                           (
                           
                              
                                 k
                              
                              
                                 ′
                              
                           
                           n
                           )
                         edges, where 
                           
                              
                                 k
                              
                              
                                 ′
                              
                           
                         is a smaller constant in the range 
                           2
                           ≤
                           
                              
                                 k
                              
                              
                                 ′
                              
                           
                           ≤
                           10
                        , sparse eigen solvers can be utilized to reduce the run time.

Our proposed algorithm is generic and can be applied for analyzing both genes and samples. The performance of the proposed algorithm is evaluated on 12 gene expression datasets, the details of them are given in Table 1
                        . The BreastA, BreastB, DLBCLA, Novartis, ALB, Leukemia, cGCM and LungA datasets are available at [34]. The datasets BreastA, BreastB, DLBCLA and Novartis are addressed in [38]. ALB, Leukemia, cGCM and LungA datasets are addressed in [39]. The Yeast cell cycle (Yeast1) and Rat CNS datasets can be downloaded from [35]. We use the class label given in [35] as the ground truth for Yeast1 dataset. For Rat CNS dataset, we take the cluster solution obtained by Wen et al. 1998 [37] as the gold standard. Yeast Sporulation (Yeast2) and Arabidopsis are taken from [36]. 
                           Algorithm 2
                           Eigenanalysis on Minimum Spanning Tree based neighborhood graph (E-MST).
                                 
                                    
                                       
                                       
                                          
                                             
                                                Input: Dataset X, Number of clusters k.
                                          
                                          
                                             
                                                Output: Partition Set 
                                                   S
                                                   =
                                                   {
                                                   
                                                      
                                                         S
                                                      
                                                      
                                                         1
                                                      
                                                   
                                                   ,
                                                   
                                                      
                                                         S
                                                      
                                                      
                                                         2
                                                      
                                                   
                                                   ,
                                                   ⋯
                                                   
                                                      
                                                         S
                                                      
                                                      
                                                         k
                                                      
                                                   
                                                   }
                                                .
                                          
                                          
                                             
                                                1. Let 
                                                   G
                                                   =
                                                   (
                                                   V
                                                   ,
                                                   E
                                                   )
                                                 be the complete graph obtained from the given dataset X.
                                          
                                          
                                             
                                                2. Construct first round of MST T
                                                1 from 
                                                   G
                                                   =
                                                   (
                                                   V
                                                   ,
                                                   E
                                                   )
                                                 and set 
                                                   
                                                      
                                                         D
                                                      
                                                      
                                                         1
                                                      
                                                   
                                                   =
                                                   Diameter
                                                   (
                                                   
                                                      
                                                         T
                                                      
                                                      
                                                         1
                                                      
                                                   
                                                   )
                                                .
                                          
                                          
                                             
                                                3. Set i=2 and the change in diameter 
                                                   Δ
                                                   D
                                                   =
                                                   ∞
                                                .
                                          
                                          
                                             
                                                4. While 
                                                   (
                                                   Δ
                                                   D
                                                   ≠
                                                   0
                                                   )
                                                
                                             
                                          
                                          
                                             
                                                
                                                4.1 Obtain T
                                                
                                                   i
                                                 from 
                                                   G
                                                   =
                                                   (
                                                   V
                                                   ,
                                                   E
                                                   ⧹
                                                   
                                                      
                                                         
                                                            ∪
                                                         
                                                      
                                                      
                                                         j
                                                         =
                                                         1
                                                      
                                                      
                                                         j
                                                         =
                                                         i
                                                         −
                                                         1
                                                      
                                                   
                                                   
                                                      
                                                         T
                                                      
                                                      
                                                         j
                                                      
                                                   
                                                   )
                                                .
                                          
                                          
                                             
                                                
                                                4.2 Obtain 
                                                   
                                                      
                                                         k
                                                      
                                                      
                                                         ′
                                                      
                                                   
                                                -MST neighborhood graph, 
                                                   
                                                      
                                                         G
                                                      
                                                      
                                                         ′
                                                      
                                                   
                                                   =
                                                   (
                                                   V
                                                   ,
                                                   
                                                      
                                                         
                                                            ∪
                                                         
                                                      
                                                      
                                                         j
                                                         =
                                                         i
                                                      
                                                      
                                                         j
                                                         =
                                                         1
                                                      
                                                   
                                                   
                                                      
                                                         T
                                                      
                                                      
                                                         j
                                                      
                                                   
                                                   )
                                                .
                                          
                                          
                                             
                                                
                                                4.3 Compute diameter D
                                                
                                                   i
                                                 of the graph 
                                                   
                                                      
                                                         G
                                                      
                                                      
                                                         ′
                                                      
                                                   
                                                . 
                                                4.4 Compute 
                                                   Δ
                                                   D
                                                   =
                                                   
                                                      
                                                         D
                                                      
                                                      
                                                         i
                                                         −
                                                         1
                                                      
                                                   
                                                   −
                                                   
                                                      
                                                         D
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                .
                                          
                                          
                                             
                                                
                                                4.5 
                                                   i
                                                   =
                                                   i
                                                   +
                                                   1
                                                .
                                          
                                          
                                             
                                                5. Let 
                                                   
                                                      
                                                         A
                                                      
                                                      
                                                         ′
                                                      
                                                   
                                                 be the adjacency matrix of 
                                                   
                                                      
                                                         G
                                                      
                                                      
                                                         ′
                                                      
                                                   
                                                .
                                          
                                          
                                             
                                                6. Find the degree matrix 
                                                   
                                                      
                                                         D
                                                      
                                                      
                                                         ′
                                                      
                                                   
                                                 of 
                                                   
                                                      
                                                         G
                                                      
                                                      
                                                         ′
                                                      
                                                   
                                                , where each 
                                                   D
                                                   (
                                                   i
                                                   ,
                                                   j
                                                   )
                                                   =
                                                   0
                                                   ,
                                                   i
                                                   ≠
                                                   j
                                                 and 
                                                   D
                                                   (
                                                   i
                                                   ,
                                                   i
                                                   )
                                                   =
                                                   
                                                      
                                                         ∑
                                                      
                                                      
                                                         j
                                                         =
                                                         1
                                                      
                                                      
                                                         j
                                                         =
                                                         n
                                                      
                                                   
                                                   
                                                      
                                                         A
                                                      
                                                      
                                                         ′
                                                      
                                                   
                                                   (
                                                   i
                                                   ,
                                                   j
                                                   )
                                                .
                                          
                                          
                                             
                                                7. Compute the Laplacian Matrix 
                                                   L
                                                   =
                                                   
                                                      
                                                         D
                                                      
                                                      
                                                         ′
                                                      
                                                   
                                                   −
                                                   
                                                      
                                                         A
                                                      
                                                      
                                                         ′
                                                      
                                                   
                                                .
                                          
                                          
                                             
                                                8. Compute first k eigenvectors 
                                                   
                                                      
                                                         v
                                                      
                                                      
                                                         1
                                                      
                                                   
                                                   ,
                                                   
                                                      
                                                         v
                                                      
                                                      
                                                         2
                                                      
                                                   
                                                   ,
                                                   …
                                                   ,
                                                   
                                                      
                                                         v
                                                      
                                                      
                                                         k
                                                      
                                                   
                                                .
                                          
                                          
                                             
                                                9. Form a 
                                                   n
                                                   ×
                                                   k
                                                 matrix U, where each eigenvector 
                                                   
                                                      
                                                         v
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   ,
                                                   1
                                                   ≤
                                                   i
                                                   ≤
                                                   k
                                                 is stacked as a column in U.
                                          
                                          
                                             
                                                10. Treat each row 
                                                   
                                                      
                                                         u
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   ,
                                                   1
                                                   ≤
                                                   i
                                                   ≤
                                                   n
                                                 of U as a data point.
                                          
                                          
                                             
                                                11. Apply a clustering algorithm to partition the points 
                                                   
                                                      
                                                         (
                                                         
                                                            
                                                               u
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                         )
                                                      
                                                      
                                                         i
                                                         =
                                                         1
                                                         ,
                                                         2
                                                         ,
                                                         …
                                                         ,
                                                         n
                                                      
                                                   
                                                 into k clusters.
                                          
                                       
                                    
                                 
                              
                           

We compare the results of the proposed algorithm with the traditional methods such as K-means and average linkage. We also compare our proposed algorithm with few recent graph theoretical methods such as spectral clustering [22], MST-based clustering algorithm SMST [14]. The experimental setup for the algorithms are as follows. We have implemented SMST algorithm with the objective function of removing long MST edges. For spectral clustering, we have used three most commonly used similarity graphs such as ε-neighborhood graph (SCεG), KNN-graph (SCKNNG) and fully connected graph (SCFCG). There is no specific means by which one can find optimum values of ε, K and σ in constructing the similarity graphs SCεG, SCKNNG and SCFCG respectively. However, as suggested in [19], following values of the parameters (ε and K), in general, give better performance: for ε-graph, the radius ε is set as the weight of longest edge in the MST T
                        1 of the dataset; for KNN-graph, K is set in the order of 
                           log
                           (
                           n
                           )
                        . Therefore for the present study, the above mentioned values of ε and K are chosen for comparison. For fully-connected graph, σ is set as the mean of all-pair distances.

In addition to the above said algorithms, we also compare the proposed algorithm with network based approaches such as Betweenness based heuristic on Minimum spanning tree of the correlation network (B-MST) [18], co-expression network based approach to gene expression analysis using QCut [11] and Penalized K-Nearest Neighbor graph based clustering for gene expression analysis (PKNNG) [12]. We have used the implementation of QCut algorithm available at http://cs.utsa.edu/ jruan/Software.html. For PKNNG algorithm, we have implemented MinSpan connecting scheme as PKNNG-MinSpan has been shown to provide better clustering results [12]. We use PKNNG with MinSpan scheme as a base metric, then we apply PAM on the resulting graph.

We use two cluster validity indices such as Adjusted Rand index (ARI) and Jaccard Index as comparison criteria [32]. These indices are most commonly used external validity measures that evaluate the degree of agreement between two partitions. Let P
                        1 be the ground truth partitions and let P
                        2 be the partitions predicted by a clustering algorithm. Let N
                        11 be the number of pairs that are clustered together in both P
                        1 and P
                        2. Let N
                        00 be the number of pairs that are in different clusters in both P
                        1 and P
                        2. Let N
                        10 be the number of pairs that are in the same cluster in P
                        1 and different clusters in P
                        2. Let N
                        01 be the number of pairs that are in different clusters in P
                        1 and the same cluster in P
                        2. ARI and Jaccard indices are calculated as follows [32]:
                           
                              
                                 ARI
                                 =
                                 
                                    
                                       2
                                       (
                                       
                                          
                                             N
                                          
                                          
                                             11
                                          
                                       
                                       
                                          
                                             N
                                          
                                          
                                             00
                                          
                                       
                                       −
                                       
                                          
                                             N
                                          
                                          
                                             10
                                          
                                       
                                       
                                          
                                             N
                                          
                                          
                                             01
                                          
                                       
                                       )
                                    
                                    
                                       (
                                       
                                          
                                             N
                                          
                                          
                                             11
                                          
                                       
                                       +
                                       
                                          
                                             N
                                          
                                          
                                             10
                                          
                                       
                                       )
                                       (
                                       
                                          
                                             N
                                          
                                          
                                             10
                                          
                                       
                                       +
                                       
                                          
                                             N
                                          
                                          
                                             01
                                          
                                       
                                       )
                                       +
                                       (
                                       
                                          
                                             N
                                          
                                          
                                             11
                                          
                                       
                                       +
                                       
                                          
                                             N
                                          
                                          
                                             01
                                          
                                       
                                       )
                                       (
                                       
                                          
                                             N
                                          
                                          
                                             01
                                          
                                       
                                       
                                          
                                             N
                                          
                                          
                                             00
                                          
                                       
                                       )
                                    
                                 
                                 Jaccard
                                 
                                 index
                                 =
                                 
                                    
                                       
                                          
                                             N
                                          
                                          
                                             11
                                          
                                       
                                    
                                    
                                       (
                                       
                                          
                                             N
                                          
                                          
                                             11
                                          
                                       
                                       +
                                       
                                          
                                             N
                                          
                                          
                                             10
                                          
                                       
                                       +
                                       
                                          
                                             N
                                          
                                          
                                             01
                                          
                                       
                                       )
                                    
                                 
                              
                           
                        While complete agreement between the two partitions P
                        1 and P
                        2 is indicated by ARI=1 and Jaccard index=1, the complete disagreement between the partitions is observed when ARI=−1 and Jaccard index=0.

The performance of the proposed algorithm depends on the number of rounds of MSTs (
                           
                              
                                 k
                              
                              
                                 ′
                              
                           
                        ). Starting from complete graph 
                           G
                           =
                           (
                           V
                           ,
                           E
                           )
                        , we obtain first round of MST T
                        1; second round of MST T
                        2 is obtained from 
                           G
                           =
                           (
                           V
                           ,
                           E
                           ⧹
                           
                              
                                 T
                              
                              
                                 1
                              
                           
                        ). In this way, the maximum number of rounds of edge-disjoint MSTs that can be obtained from G is 
                           n
                           −
                           1
                        , where n is the number of points. Although the parameter 
                           
                              
                                 k
                              
                              
                                 ′
                              
                           
                         takes the values in the range 1 to 
                           n
                           −
                           1
                        , the experimental analysis carried out starting from 
                           
                              
                                 k
                              
                              
                                 ′
                              
                           
                           =
                           1
                         to 
                           n
                           −
                           1
                         have shown that, the optimal clustering results with respect to quality measures such as ARI and Jaccard are obtained for smaller value of 
                           
                              
                                 k
                              
                              
                                 ′
                              
                           
                         in the range 2–10. The reason is that the most discriminating edges which are crucial for clustering are obtained in the first few rounds i.e. from 
                           
                              
                                 k
                              
                              
                                 ′
                              
                           
                           =
                           2
                         to 10. We have tested the proposed algorithm on all the datasets for various values of 
                           
                              
                                 k
                              
                              
                                 ′
                              
                           
                         ranging from 1 to 100 and the optimal partitioning with respect to the quality measures ARI and Jaccard index is achieved for 
                           2
                           ≤
                           
                              
                                 k
                              
                              
                                 ′
                              
                           
                           ≤
                           10
                         (see Fig. 6
                        ).

Two distance measures such as Euclidean distance and Pearson-correlation distance measures are considered for the analysis. The ARI and Jaccard index calculations of the various algorithms using Euclidean distance are shown in Tables 2
                         and 3
                         respectively. The proposed algorithm E-MST outperforms other algorithms on BreastA, BreastB, ALB, Leukemia, LungA, Yeast1 and Rat CNS datasets. While QCut outperforms other algorithms on DLBCLA dataset, B-MST algorithm attains highest score for ARI and Jaccard on Novartis and cGCM datasets.


                        Tables 4 and 5
                        
                         respectively report ARI and Jaccard index calculations of the various algorithms using correlation measure. Here also the proposed algorithm performs relatively better than other algorithms on BreastA, BreastB, DLBCLA, ALB, Leukemia, cGCM, LungA datasets. Other competent algorithms such as B-MST and QCut obtain better results on Novartis and Rat CNS datasets. On Yeast1 dataset, SCKNNG algorithm shows good performance.

In order to observe the effect of distance measure on the performance of the algorithms, we have also observed the results using two other measures such as cityblock and chebychev distances (refer to Appendix for the results). Comparing the results obtained using four distance measures, it is visible that the results of the clustering algorithm depend on the distance measure used for similarity calculations. We compute the variance of ARI values obtained using four different distance measures and take the mean value for all 10 datasets. Fig. 7
                         illustrates the mean value of the variance of ARI score obtained using Euclidean, correlation, cityblock and chebychev distances on all 10 datasets. While the other algorithms have a major variations with respect to the distance measure, the proposed algorithm E-MST relatively has little impact which leads to achieve better results as compared to other algorithms.

Although spectral methods are widely discussed, there has been a little attention on which post-clustering algorithm (for e.g. K-means) should be used in multi-way spectral partitioning. This motivated us to observe the impact of choice of post-clustering algorithm on the performance of spectral methods. We apply two algorithms namely K-means and average linkage. Consider the same dataset shown in Fig. 1a. While K-means and average linkage fails to separate the two overlapping clusters as shown in Fig. 8
                        a and b respectively, the proposed algorithm is able to identify the expected clusters even though they are not separated by the longest edge as shown in Fig. 8e (K-means based post-clustering) and Fig. 8f (average linkage based post-clustering).


                        Fig. 9
                         illustrates the comparison of proposed as well as the basic spectral methods using average linkage based post-clustering. It is observed from the results that the proposed algorithm outperforms on all the datasets except BreastB dataset on which SCKNNG algorithm performs better than the proposed algorithm. Comparing the results of average linkage based post-clustering shown in Fig. 9 with the results of K-means based post-clustering shown in Tables 2 and 3, it is observed that the performance of the spectral methods depend on the choice of post-clustering method. Although there is a variation in the results of the proposed algorithm with respect to the choice of post-clustering method, the proposed algorithm performs better than the other algorithms irrespective of the post-clustering method on most of the datasets.

Next, we show the performance of the proposed algorithm according to the type of Laplacian Matrix used. Laplacian matrix can be used in both normalized and unnormalized versions. The results that we have discussed so far employed unnormalized Laplacian matrix. Tables 6 and 7
                        
                         show the comparison of the clustering results obtained by the proposed algorithm using unnormalized and normalized Laplacian matrices. It is noted that the performance of the proposed algorithms with unnormalized and normalized versions of Laplacian matrix is approximately similar. The only exception is observed on the BreastB dataset where normalized Laplacian matrix seems to perform better than unnormalized one. Hence the proposed algorithm is invariant to the normalization of the Laplacian matrix.

We have also tested the internal cluster quality obtained by the proposed algorithm. For this purpose we use the Tightness and Separation Index (TSI) defined in [18] as an internal quality measure to compare the results. TSI calculations are performed on a binary correlation network which contains only the strongest correlation edges according to some threshold. TSI is computed as follows [18]: 
                           
                              
                                 TSI
                                 =
                                 
                                    
                                       D
                                    
                                    
                                       max
                                    
                                 
                                 +
                                 
                                    
                                       K
                                    
                                    
                                       max
                                    
                                    
                                       out
                                    
                                 
                                 ,
                              
                           
                        where D
                        
                           max
                         is the maximum shortest path distance between any two genes in the network and K
                        
                           out
                        
                        max is the fraction of inter-cluster edges in the correlation network. Lower value of TSI indicates well-separated as well as more-compact clusters. The TSI values of proposed algorithm are compared against B-MST algorithm [18] and the results are shown in Table 8
                        . It can be seen from the table that the proposed algorithm achieves relatively lower value of TSI on all the datasets except Novartis and cGCM datasets.

In order to evaluate the robustness of our proposed algorithm against noise, we have carried out experiments on synthetic datasets such as two-moons and two-spirals. We have added different levels of noise to the original datasets as shown in Fig. 10
                        . The noise level “Low” indicates that 25% additional noise added to the original data. Similarly “Medium” and “High” levels respectively denote the addition of 50% and 75% noise to the original data.

The proposed algorithm E-MST has shown a consistent performance against varying levels of noise on two-moons dataset as shown in Fig. 11
                        a. The other algorithms such as SCKNNG, PKNNG and K-means have also attained minimal variance even though their ARI values are lower than that of E-MST. Two-spirals dataset is little bit complex dataset as each spiral contains sparsely connected points. The addition of noise may disturb the within-cluster neighborhood properties of the points, thus it becomes difficult for the clustering algorithms to separate the two-spirals. Fig. 11b illustrates the performance comparison of different algorithms on two-spirals dataset. Although the proposed algorithm E-MST exhibits a varying performance across different levels of noise, it has achieved relatively higher value of ARI as compared to other algorithms.

The performance of the proposed algorithm has been compared against other existing algorithms in terms of speed. All the algorithms are run for 10 times and mean value of run time (in milliseconds) is observed and the same is shown in Table 9
                        . Due to the repeated betweenness calculations, B-MST performs much slower as compared to other algorithms. PKNNG-PAM implementation also takes quite longer time due to the iterative overhead caused by PAM. Similarly K-means also incurs more runtime due to the number of distance computations involved with high dimensional datasets over multiple iterations. The proposed algorithm E-MST takes relatively lesser time as compared to all other algorithms except QCut. Multiple passes over adjacency matrix during 
                           
                              
                                 k
                              
                              
                                 ′
                              
                           
                        -MST graph construction makes E-MST a bit slower. This can be improved by sparsifying the adjacency matrix through certain rank-transformation as is used in [11]. Although QCut outperforms all the other algorithms including E-MST, the proposed algorithm E-MST provides a qualitative improvement over QCut method, without much compromise in the speed.

In order to estimate the number of clusters in the datasets for which class distribution is unknown, we use gap statistics of the eigenvalues [19]. The maximum difference between any two consecutive eigenvalues gives us an intuition about the number of clusters. We consider only the first 
                           
                              
                                 n
                              
                           
                         eigenvalues, as the number of clusters 
                           k
                           <
                           
                              
                                 n
                              
                           
                        . We apply this heuristics to determine the number of clusters k. For the sake of illustration, we consider the BreastA dataset. The number of genes in BreastA dataset 
                           n
                           =
                           98
                         and first 
                           
                              
                                 n
                              
                           
                           ≃
                           10
                         eigenvalues of the dataset is shown in Fig. 12
                        a. The maximum gap between two successive eigenvalues lies between clusters 2 and 3. Hence, the number of clusters is taken as 3. Similar eigengap analysis is carried out on all other datasets and the predicted value of k is compared against QCut algorithm. The results are shown in Table 10
                        .

In order to observe the eigengap analysis on unlabeled datasets, we consider two datasets namely Yeast Sporulation (Yeast2) and Arabidopsis which were addressed in [36]. Fig. 12b and c respectively demonstrate the estimation of number of clusters in Yeast2 and Arabidopsis datasets. In both the datasets, the maximum eigengap lies between clusters 5 and 6 and the number of clusters is taken to be 6, which conforms to the finding in [33]. Hence the eigengap statistics can be effectively employed to determine the number of functional groups in the gene profiles where class distribution is unknown.

We have used Gene Ontology (GO) Term finder tool (http://www.yeastgenome.org/cgi-bin/GO/goTermFinder.pl) to analyze the biological significance of clusters obtained by the proposed algorithm E-MST. The GO Term Finder searches for significant shared GO terms used to establish the biological relevance of a cluster. The functional enrichment of a group of genes is represented by three independent structured, controlled vocabularies: molecular function, biological process and cellular component. The degree of functional enrichment is measured by the cumulative hypergeometric distribution. For a particular GO category, the probability p of producing k or more genes within a cluster of size x is calculated as [36] 
                        
                           
                              
                                 p
                                 =
                                 1
                                 −
                                 
                                    
                                       ∑
                                    
                                    
                                       j
                                       =
                                       1
                                    
                                    
                                       k
                                       −
                                       1
                                    
                                 
                                 
                                    
                                       (
                                       
                                          
                                             
                                                M
                                             
                                             
                                                j
                                             
                                          
                                       
                                       )
                                       (
                                       
                                          
                                             
                                                N
                                                −
                                                M
                                             
                                             
                                                x
                                                −
                                                j
                                             
                                          
                                       
                                       )
                                    
                                    
                                       (
                                       
                                          
                                             
                                                N
                                             
                                             
                                                x
                                             
                                          
                                       
                                       )
                                    
                                 
                                 ,
                              
                           
                        where M and N denote the total number of genes within a category and that within the genome respectively. By computing the p-values, we can analyze the statistical significance of the genes in a cluster. If majority of genes in a cluster possess the same biological function, then p-value of the obtained cluster will be equal to 0 [33]. For illustration purpose, we consider Yeast2 dataset. E-MST algorithm has obtained 6 clusters for this dataset, which was explained in Section 6.9. We test the biological relevance of each of these 6 clusters using GoTermFinder at 1% significant level. Table 11
                         reports three most significant GO terms along with their p-values. All the clusters identified by E-MST have obtained p-values less than 0.01, revealing that the clusters are biologically significant.

@&#CONCLUSION@&#

Grouping functionally similar genes is a fundamental task in microarray analysis. This paper presented an improved clustering algorithm by employing eigenanalysis on the similarity graph obtained from multiple rounds of MSTs. The number of rounds of MSTs (
                        
                           
                              k
                           
                           
                              ′
                           
                        
                     ) that is sufficient to construct the similarity graph has been determined by studying the relationship between the graph׳s diameter and the second eigenvalue of the Laplacian matrix. The proposed algorithm is compared with traditional algorithms such as K-means and average linkage and recent algorithms such as spectral and MST-based clustering methods. Experimental results on 10 gene expression datasets demonstrated that the proposed algorithm E-MST outperforms the traditional and recent algorithms on 8 of the 10 datasets considered.

We have also observed the effect of normalizing the graph Laplacian and the results showed that the proposed algorithm is performed invariably to the effect of normalization. Moreover, we have tested our algorithm by applying two post-clustering methods as K-means and average linkage and the results conveyed that the results of spectral clustering not only depend on the type of similarity graph employed but also the choice of post-clustering algorithm utilized in multi-way partitioning. We have employed the eigengap statistics for estimating the number of clusters. We tested this on both labeled and unlabeled datasets and the results are promising. Biological significance of the clusters have also been investigated.

As a future work we carry out a rigorous analysis of the proposed algorithm E-MST on more gene expression datasets.

None declared.


                     Table 12 and 13
                     
                      show the ARI and Jaccard index obtained by different algorithms using cityblock distance measure. Similarly, Tables 14 and 15
                     
                      show the ARI and Jaccard index obtained by different algorithms using the results using Chebychev measure.

@&#REFERENCES@&#

