@&#MAIN-TITLE@&#Detecting affective states from text based on a multi-component emotion model

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A multi-component emotion describing model is proposed.


                        
                        
                           
                           A text-based emotion detecting method based on the describing model is proposed.


                        
                        
                           
                           Both the interactions among affective components and contextual effect are involved.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Multi-component emotion model

Deep stacking network

Visible intermediate layers

Contextual impact

Expressive speech synthesis

@&#ABSTRACT@&#


               
               
                  A multi-component emotion model is proposed to describe the affective states comprehensively and provide more details about emotion for the application of expressive speech synthesis. Four types of components from different perspectives – cognitive appraisal, psychological feeling, physical response and utterance manner are involved. And the interactions among them are also considered, by which the four components constitute a multi-layered structure. Based on the describing model, a detecting method is proposed to extract the affective states from text, as it is the requisite first step for an automatic generation of expressive synthetic speech. The deep stacking network is adopted and integrated with the hypothetic producing process of the four components, by which the intermediate layers of the network become visible and explicable. In addition, the affective states at document level and paragraph level are regarded as contextual features to extend available information for the emotion detection at sentence level. The effectiveness of the proposed method is validated through experiments. At sentence level, a 0.59 F-value of the predictions of utterance manner is achieved.
               
            

@&#INTRODUCTION@&#

Natural generation of expressive synthetic speech presents the researchers with two main requirements: one is an automatic detection of affective states from the given text, including a proper description of emotion; the other is the effective transformations on speech signal according to the delivered affective messages (Bellegarda, 2011; Roach, 2000; Trilla and Alías, 2013). Most of the studies in expressive speech synthesis (ESS) area are devoted to address the second issue, see Govind and Prasanna (2013) for a comprehensive review, in which the desired emotion is given directly as an additional input along with text. And the emotion is described by the ready-made models borrowed from other disciplines like psychology, which is suggested to be fairly inadequate in the ESS scenario. This work focuses on describing affective states more comprehensively and searching an available text-based detecting method based on the newly proposed description model, as they are the requisite first step for an automatic generation of expressive synthetic speech.

Speech researchers are increasingly paying attention on identifying the affective states from text to direct the subsequent expressive rendering. For example, Trilla and Alías (2013) and Alm et al. (2005) explored the Sentiment Analysis (SA) problem at sentence level to inform a Text-to-Speech (TTS) system about a proper sentiment (negative, positive or neutral). Wu et al. (2009) adapted the Pleasure-Arousal-Dominance (PAD) model to describe the affective information of particular words and then to guide the acoustic variation at different levels. Tao (2003) and Bellegarda (2011) placed emphasis on the generation of a number of typical discrete emotions (happiness, anger, sadness, fear and disgust). It is implied that the affective states are often simplified to a number of categories or several dimensional values in the previous work. The categorical model supplies an intuitive description of emotion for humans, but it is quite confused for the TTS system to get further guidance about the expressive rendering. The dimensional model supplies two or three primary characteristics which are deemed to be more directly correlated with vocal variations, but it is argued that the projection from the possibly structured multi-dimensional space of emotion to the fairly reduced space with two or three homogeneous dimensions may cause a loss of information (Cowie and Cornelius, 2003).

There are plenty of ways to define or delimit emotion. In a narrow sense, it is restricted to subjective feelings; while in a broader definition, it includes both the emotional states and the emotion-related states (e.g., arousal, attitude), where the latter are suggested to be more pervasive in everyday conversations and probably more important to speech (Cowie and Cornelius, 2003; Scherer, 2003). Therefore, following the broader definition of emotion, this work proposes a multi-component model, in the hope to provide a relatively comprehensive description of emotion and supply more information for the expressive rendering. Four components: cognitive appraisal, psychological feeling, physical response and utterance manner are included in the model and distributed in a multi-layered structure. Emotions are represented by different patterns of the four components, and utterance manner will be used as an interface between the affective description and the acoustic signal of emotional speech in the ESS system.

To detect these affective states from textual content, the Deep Stacking Network (DSN) (Deng et al., 2012) is exploited, owning to the multi-layered stacking architecture which suits the structure of the proposed emotion model properly. The DSN is integrated with the hypothetic producing process of the affective components, forming a stacking network with visible intermediate layers. Besides the interactions among different components, the contextual impacts are also taken into account in this work. In the TTS scenario, the analyzing granularity of texts is usually determined as the sentence level, so that the natural expressive variations can be investigated inside the document. In this work, instead of extracting the affective states at the sentence level directly, the emotions of documents and paragraphs are analyzed first, and then sent to the lower levels as a contextual reference. Given that the information provided by a sentence is rather reduced, this method can be regarded as a tactic to extend extra available information. Experiments are implemented to validate the effectiveness of the detecting method and the rationality of the describing model.

The rest of the paper is organized as follows. Section II presents the existing emotion describing models and detecting methods from other related work. Section III describes the details of the proposed multi-component emotion model. Section IV elaborates the detecting method according to the multi-component model. Section V illustrates the verification experiments and discusses the results. The last section draws the conclusions and the future work.

@&#RELATED WORK@&#

This section outlines some existing emotion describing models in psychology, and the commonly used techniques to detect emotions from text.

There is no generally accepted methodology to describe emotion at present. Extensive theories and approaches have been proposed in psychology. Cowie and Cornelius (2003) divided the affective states into emotional states (subjective feelings) and emotion-related states (e.g., attitudes and physical arousal). Likewise, Ortony et al. (1987) divided these states into physical/bodily states and mental conditions, in which the former corresponds to physical arousal and the latter includes “a significant cognitive component or a significant affective component, some have a significant behavioral component”. Cornelius (2000) summarized the explanations of emotion into four major perspectives – Darwinian, Jamesian, Cognitive appraisal, and Social constructivist. Similarly, Scherer (2000) summarized these theories into four pairs of debates: Cognition-Emotion Debate, Mind-Body Debate, Biology-Culture Debate and Center-Periphery Debate. Each perspective of these theories has its unique way to interpret emotion, but overlapping may also exist. Four major aspects about emotion can be inferred from these interpretations: cognitive appraisal, psychological feeling, physical arousal and social behavior.

As mentioned in the introduction, the two major types of models to describe emotion are discrete models and dimensional models, which have dominated the research in this area for decades. Discrete models are established based on the Darwinian theories which insist on the existence of a small number of basic or fundamental emotions with specific eliciting conditions and response patterns in physiology and expression. The best known list of basic emotions is the “Big Six”: joy, sadness, anger, fear, disgust and surprise (Cowie and Cornelius, 2003; Ekman, 1992). Dimensional models map emotional states in a low-dimensional space (with two or three dimensions). The two widely accepted dimensions are activation (or arousal) dimension and valence (or pleasure) dimension (Cowie et al., 2000). And the third one often refers to control (or dominance) (Mehrabian, 1996). Recently, the appraisal theories (Moors et al., 2013) become popular in affective computing area (Calvo and D’Mello, 2010), which assume that emotions are elicited by a cognitive evaluation of antecedent situations, and reactions in different domains (physiology, expression, action tendency, and feeling) are determined by this evaluation. The Component Process Model (CPM) (Scherer, 2009) is a new kind of emotion model based on appraisal theories. Instead of restricting the description to several dimensions or a number of basic emotions, CPM supplies a more open solution that considers different aspects of emotional states as we expect. However, it emphasizes the link between the different patterns of appraisal elicitation and the changes of other emotional states. While in the ESS scenario, we are more concerned about the relationship between different types of emotional states and the variation of vocal expression. Therefore, following appraisal theories, this work proposes a multi-component model to meet the requirements of ESS, where the types of the components and their relations are all different with CPM.

The methods for detecting emotions from text can be roughly divided into three types: lexical-based, semantic-based and machine learning approaches (Agrawal and An, 2012; Calix et al., 2012). Lexical-based approaches tend to classify texts based on a dictionary of affect words, which are annotated with emotional categories as in the discrete model (Calix et al., 2010), or sentiment ratings as in the dimensional model (Tokuhisa et al., 2008). Other researchers attempted to analyze the affective states in text semantically. For instance, Liu et al. (2003) used knowledge bases to extract additional information about the meanings of words and their relations. Gill et al. (2008) calculated the semantic similarities between texts and emotional terms in a semantic space. Lexical-based and semantic-based approaches are all dependent on hand-crafted resources and rules that are not applicable for the newly proposed model. Comparatively, the machine learning approaches have the flexibility to adapt any emotion model and perform well in predicting tasks (Balahur et al., 2012; Trilla et al., 2010). Therefore, the machine learning method is selected to implement the detecting work.

The classifiers such as Support Vector Machine (SVM) and Maximum Entropy (ME) are the commonly used machine learning techniques in the emotion detection task. However, most of them are considered as “shallow” networks without a layered structure. Recently, the Deep Neural Network (DNN) has gained great successes in the applications of speech recognition (Deng et al., 2012) and image recognition (Hinton and Salakhutdinov, 2006). The multi-layered structure of DNN matches the hierarchy of the proposed emotion model. And the introducing of the Restricted Boltzmann Machine (RBM) makes it possible to take advantage of unlabeled samples. In this work, the producing process of the multiple components is given. To train the network based on the defined producing process, the stacking DNN (DSN) (Deng et al., 2012) is exploited and integrated with the producing process, in which the intermediate layers can be supervised separately. More details will be illustrated in Section 4.

Based on the analysis of the related theories in psychology, as well as the requirements toward the application of ESS, a multi-component emotion model is proposed which includes four components: cognitive appraisal, psychological feeling, physical response and utterance manner. Each component interprets emotions from a different perspective but none of them captures the whole picture. The four perspectives constitute a relatively comprehensive description for emotion. The producing of the four components is not an instantaneous phenomenon but a continuous process, just as the appraisal theories believed. Based on an imitation of the process that human beings transform a textual message into the equivalent message in spoken form, the producing process of the four emotional components is hypothesized as: (1) given the textual content, an overall cognitive appraisal is made; (2) some psychological feelings are generated based on the appraisal results; (3) physical responses are stimulated accordingly; and (4) utterance organs are adjusted finally.

Each component is represented by a number of dimensions which are obtained through psychological perception tests in the previous work (Gao and Zhu, 2012). The finalized representations of them are listed as follows:
                        
                           (1)
                           
                              Cognitive appraisal: an overall judgment about the sentiment tendency. Based on the research about attitude (Gu et al., 2011) and the studies in broadcasting science (Zhang, 2003), five dimensions are proposed: Denouncing–Commending, Indifferent–Passionate, Informal–Formal, Soft–Tough, and Euphemistic–Straightforward.
                           


                              Psychological feeling: subjective experience or “conscious reflection” (Lambie and Marcel, 2002; Moors et al., 2013; Scherer, 2000). The distribution of psychological feelings is represented by a hierarchical structure, from which the inheritance relationship among them can be observed, and the classification granularity can be relatively freely chosen. The hierarchical tree is gained from 43 Chinese affect terms which are commonly used to represent feelings. Firstly an artificial classification is implemented by 49 subjects, who are asked to divide the candidate terms into 4 to 10 classes. Then based on the results of artificial classification, a hierarchical classification is carried out. After a slight adjustment, the hierarchical tree is gained as shown in Table 1
                              . The categories in higher layers are compounded by the ones in sub-layers.


                              Physical response: the bodily changes and regulations. Two properties are inspected: Arousal – the activation of the organism; and Dominance – the control of the bodily changes, rather than the control over the surrounding situation as in the Valence-Arousal-Dominance (VAD) model (Mehrabian, 1996).


                              Utterance manner: estimation about the vocal expression. Seven characteristics either at the segmental level or at the super-segmental level are evaluated: (a) segmental level: Dark–Bright (the brightness of voice quality), Partial–Full (the completion of the utterance); (b) super-segmental level: Slow–Fast (the speech rate), Discrete–Adhesive (the binding tightness between adjacent syllables), Low–High (the height of tone), Dull–Tortuous (the change of intonation), and Steady–Changeable (the variation of rhythm).

The dimensions in psychological component and physical component are rated on a four-point scale: none (“0”), weak (“1”), medium (“2”), and strong (“3”). And the dimensions in cognitive component and utterance component are described on a seven-point scale (“−3” to “3”), take the informal-formal dimension as an example, “−3” indicates strongly informal, “0” indicates neither informal nor formal, and “3” indicates strongly formal.

As mentioned above, the producing process of the four emotional components is assumed, which is considered as continuous and recursive. Changes in one component will influence the generation of other components, and feedback may also exist. To simplify the computation, the feedback is not considered at present. Along with the producing process, the producing of one component will be influenced by all the other components generated before it. And the textual feature, gained through the linguistic analysis, will be delivered to all the components as there may be additional invisible information left. The framework of the producing process is shown in Fig. 1
                        . The estimation of utterance manner is set as the eventual output, which has relatively specific mapping relationship with the subsequent acoustic variation.

Based on the Framework shown in Fig. 1, the Deep Stacking Network (DSN) is exploited to detect the affective components in the proposed model. DSN is a variant of the multi-layered Deep Neural Network (DNN) (see Fig. 2
                        (a), the utterance manner is set as the target). DSN integrates the “stacking” architecture into DNN, by which the learning speed and the performance of the network are improved (Deng et al., 2012). The fundamental idea of “stacking” is that decomposing a complex structure into a series of simple modules, then connecting them together in the way that the output of one module feeds the input of the next. DSN uses the neural network as the basic module, which consists of one hidden layer, an input layer and an output layer. The output of a lower module is sent as a subset of the input in the adjacent higher module. Meanwhile, all the previous outputs and the raw input are also delivered (see Fig. 2(b)). In DNN, the intermediate layers are completely invisible, which are called “hidden layers”. Hence it is difficult to explain what we have learned, and the learning process is hard to track. DSN achieves the supervision of intermediate layers, but the definite meanings of them are still unexplainable, since each module in DSN estimates the same target as the eventual one. In the proposed model, the producing process of the four components is given, which means that the intermediate process is known partly if not fully. Therefore, the DSN in this work is trained along the producing process, as shown in Fig. 2(c). The utterance manner is treated as the ultimate target and the modules of the other three components are viewed as the intermediate layers. This network is still a deep one, with a stacking architecture, and the intermediate layers are partially visible and traceable. It is named as “Stacking Component Network” (SCN), in order to differ from the traditional DSN. The stacking architecture is able to model the interactions of different modules, and the visualization of intermediate process takes advantage of the related prior knowledge.

Besides the impacts of other components, the producing of the affective states may also be influenced by the contextual information. In this work, the affective states at higher levels (the document level and the paragraph level) are also analyzed, and sent to the lower levels as a contextual reference (see Fig. 3
                        ). Specifically, the affective states at document level are detected first, through the network shown in Fig. 2(c). Then the document-level components are used as a subset of the input to produce their counterparts at paragraph level, together with the former components and the textual features at paragraph level. Finally both the document messages and the paragraph messages are sent to the sentence level, along with the outcomes at its own level.

The learning process of the network is divided into two steps: pre-training and fine-tuning, just as the learning of other deep neural networks. But the learning is carried out within each module rather than crossing the entire network. In the pre-training stage, the RBM is used to provide a better initialization for the network, using both the labeled samples and the unlabeled samples. After the pre-training, a small amount of annotated data is used to fine-tune the network to match the desired task. The gradient decent algorithm is utilized during the fine-tuning, in a batch mode as in DSN. The primary formulae are listed below, see (Hinton, 2012; Yu and Deng, 2011) for more details.

(1) Pre-training

RBM is a two-layer network which contains a visible layer and a hidden layer, with full symmetric connections between the two layers and no connections inside each layer. The update of the weights between them is:
                           
                              (1)
                              
                                 
                                    
                                       Δ
                                    
                                    
                                       w
                                       
                                          i
                                          j
                                       
                                    
                                    =
                                    ε
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         v
                                                         i
                                                      
                                                      
                                                         h
                                                         j
                                                      
                                                   
                                                
                                             
                                             
                                                data
                                             
                                          
                                          −
                                          
                                             
                                                
                                                   
                                                      
                                                         v
                                                         i
                                                      
                                                      
                                                         h
                                                         j
                                                      
                                                   
                                                
                                             
                                             
                                                recon
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where ɛ is the learning rate, 
                           
                              
                                 
                                    
                                       
                                          
                                             v
                                             i
                                          
                                          
                                             h
                                             j
                                          
                                       
                                    
                                 
                                 
                                    data
                                 
                              
                           
                         denotes the expectation with respect to the data distribution and 
                           
                              
                                 
                                    
                                       
                                          
                                             v
                                             i
                                          
                                          
                                             h
                                             j
                                          
                                       
                                    
                                 
                                 
                                    recon
                                 
                              
                           
                         is the corresponding fraction for the reconstruction. The visible layer and the hidden layer are updated based on each other using the following two conditional probability formulae:
                           
                              (2)
                              
                                 
                                    p
                                    
                                       
                                          
                                             
                                                
                                                   h
                                                   j
                                                
                                                =
                                                1
                                             
                                          
                                          
                                             v
                                          
                                       
                                    
                                    =
                                    σ
                                    
                                       
                                          
                                             b
                                             j
                                          
                                          +
                                          
                                             ∑
                                             i
                                          
                                          
                                             
                                                v
                                                i
                                             
                                             
                                                w
                                                
                                                   i
                                                   j
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (3)
                              
                                 
                                    p
                                    
                                       
                                          
                                             
                                                
                                                   v
                                                   i
                                                
                                                =
                                                1
                                             
                                          
                                          
                                             h
                                          
                                       
                                    
                                    =
                                    σ
                                    
                                       
                                          
                                             a
                                             i
                                          
                                          +
                                          
                                             ∑
                                             j
                                          
                                          
                                             
                                                h
                                                j
                                             
                                             
                                                w
                                                
                                                   i
                                                   j
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              σ
                              (
                              x
                              )
                              =
                              1
                              /
                              
                                 
                                    1
                                    +
                                    exp
                                    (
                                    −
                                    x
                                    )
                                 
                              
                           
                         is the sigmoid function, v
                        
                           i
                         is the unit in visible layer, h
                        
                           j
                         is the unit in hidden layer, w
                        
                           ij
                         is the weight between them, b
                        
                           j
                         and a
                        
                           i
                         are the biases.

(2) Fine-tuning

The module of DSN or SCN consists of three layers (the input layer X, the hidden layer H and the output layer Y) and two trainable sets of weights (the weight matrix W between X and H, and the weight U between H and Y).


                        W is initialized based on RBM, 
                           
                              
                                 U
                              
                              =
                              
                                 
                                    
                                       
                                          
                                             H
                                          
                                          Λ
                                          
                                             H
                                          
                                       
                                    
                                 
                                 
                                    −
                                    1
                                 
                              
                              
                                 H
                              
                              Λ
                              
                                 
                                    T
                                 
                                 T
                              
                           
                        , 
                           
                              
                                 H
                              
                              =
                              σ
                              
                                 
                                    
                                       b
                                    
                                    +
                                    
                                       
                                          W
                                       
                                       T
                                    
                                    
                                       X
                                    
                                 
                              
                           
                        , and Y
                        =
                        U
                        
                           T
                        
                        H. The square error 
                           
                              E
                              =
                              T
                              r
                              
                                 
                                    
                                       
                                          
                                             Y
                                          
                                          −
                                          
                                             T
                                          
                                       
                                    
                                    Λ
                                    
                                       
                                          
                                             
                                                
                                                   Y
                                                
                                                −
                                                
                                                   T
                                                
                                             
                                          
                                       
                                       T
                                    
                                 
                              
                           
                        , T is the target data. The updating of W is based on Eq. (4):
                           
                              (4)
                              
                                 
                                    
                                       
                                          ∂
                                          E
                                       
                                       
                                          ∂
                                          
                                             W
                                          
                                       
                                    
                                    =
                                    2
                                    X
                                    
                                       
                                          
                                             
                                                H
                                             
                                             T
                                          
                                          ∘
                                          
                                             
                                                
                                                   
                                                      1
                                                      −
                                                      
                                                         H
                                                      
                                                   
                                                
                                             
                                             T
                                          
                                          ∘
                                          
                                             
                                                
                                                   
                                                      H
                                                   
                                                   +
                                                
                                                
                                                   
                                                      
                                                         H
                                                      
                                                      Λ
                                                      
                                                         
                                                            T
                                                         
                                                         T
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         T
                                                      
                                                      
                                                         
                                                            H
                                                         
                                                         +
                                                      
                                                   
                                                
                                                −
                                                Λ
                                                
                                                   
                                                      T
                                                   
                                                   T
                                                
                                                
                                                   
                                                      
                                                         T
                                                      
                                                      
                                                         
                                                            H
                                                         
                                                         +
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    H
                                 
                                 +
                              
                              =
                              Λ
                              
                                 
                                    H
                                 
                                 T
                              
                              
                                 
                                    
                                       
                                          
                                             H
                                          
                                          Λ
                                          
                                             
                                                H
                                             
                                             T
                                          
                                       
                                    
                                 
                                 
                                    −
                                    1
                                 
                              
                           
                        , 
                           
                              Λ
                              =
                              diag
                              
                                 
                                    
                                       λ
                                       
                                          11
                                       
                                    
                                    ⋯
                                    
                                       λ
                                       
                                          i
                                          i
                                       
                                    
                                    ⋯
                                    
                                       λ
                                       
                                          n
                                          n
                                       
                                    
                                 
                              
                           
                         is a diagonal weight matrix to improve the convergence speed by focusing on the samples with larger errors, and the weight for the i-th sample 
                           
                              
                                 λ
                                 
                                    i
                                    i
                                 
                              
                              =
                              
                                 
                                    N
                                    /
                                    E
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      y
                                                   
                                                   i
                                                
                                                −
                                                
                                                   
                                                      t
                                                   
                                                   i
                                                
                                             
                                          
                                       
                                       2
                                    
                                    +
                                    1
                                 
                              
                              /
                              2
                           
                        , N is the training set size,° is element-wise product.

The linguistic analysis prepares textual features for the affective computing by the above network. Three primary steps are included: lexical analysis, text segmentation and feature extraction.

(1) Lexical analysis

A Chinese lexical analysis system ICTCLAS (Zhang, 2014) is used to convert the plain texts into word streams. After that, the “stop words” such as function words are filtered out and the remaining words are all regarded as emotion containers.

(2) Text segmentation

This step is to split the documents into paragraphs and sentences. The boundaries of sentences are recognized from punctuation (full stop, exclamation mark, question mark, and ellipsis). The paragraphs are considered as “semantically coherent chunks” which are “similar in a topical sense” (Riedl and Biemann, 2012). Thereby the boundaries of them are detected through the topic-based method called “TopicTiling” (Riedl and Biemann, 2012). Briefly, the topic model of documents is estimated first then the documents are represented as a sequence of topic IDs instead of words. Each sentence is assumed as a basic unit and represented as a T-dimensional vector (T is the topic number), where the t-th element refers to the frequency of the topic t occurring in the current unit. Then the coherence scores (cosine similarities) of every two adjacent sentences are calculated through Eq. (5), where s
                        
                           i
                         refers to the coherence score, v
                        
                           i
                         and v
                        
                           i+1 represent the two sentence vectors, • is the inner product. With the coherence scores, the depths of each position between two adjacent sentences (called the sentence gaps) are computed through Eq. (6), where 
                           
                              h
                              l
                              
                                 i
                              
                           
                         and 
                           
                              h
                              r
                              
                                 i
                              
                           
                         are the highest coherence scores on the left side and the right side, see Fig. 4
                         for an example. If the depth is larger than a threshold (μ
                        +
                        σ/2 is used in this work, μ is the mean value of the depths and σ is the standard variation), then the current gap is judged as a paragraph boundary.
                           
                              (5)
                              
                                 
                                    
                                       s
                                       i
                                    
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      v
                                                   
                                                   i
                                                
                                                
                                                   
                                                
                                                
                                                   
                                                      v
                                                   
                                                   
                                                      i
                                                      +
                                                      1
                                                   
                                                
                                             
                                          
                                       
                                       /
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            v
                                                         
                                                         i
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         
                                                            v
                                                         
                                                         
                                                            i
                                                            +
                                                            1
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (6)
                              
                                 
                                    
                                       d
                                       i
                                    
                                    =
                                    
                                       
                                          
                                             
                                                h
                                                l
                                                
                                                   i
                                                
                                                +
                                                h
                                                r
                                                
                                                   i
                                                
                                                −
                                                2
                                                
                                                   s
                                                   i
                                                
                                             
                                          
                                       
                                       /
                                       2
                                    
                                 
                              
                           
                        
                     

(3) Feature extraction

The semantic-based method is used to extract the textual feature, in which the term frequency vectors are projected into a dimension-reduced semantic space using the Latent Dirichlet Allocation (LDA) algorithm (Blei et al., 2001, 2003). LDA is a generative probabilistic model developed on the basis of Latent Semantic Indexting (LSI) (Deerwester et al., 1990) and probabilistic Latent Semantic Indexting (pLSI) (Hofmann, 1999). The documents are represented as random mixtures over latent semantic units which are called “topics”. This document-topic distribution is used as the textual feature of documents. Meanwhile, a topic-word distribution is obtained, where each topic is characterized by a distribution over words. Based on the topic-word distribution, the words in a sentence are replaced by their corresponding topic IDs. By counting the frequencies of each topic occurring in the current sentence, the sentence-topic distribution is calculated, which is used as the textual feature of sentences. And the paragraph-topic distribution is obtained by the same approach and used as the textual feature of paragraphs. The Gibbs-sampling-based topic modeling toolbox (Steyvers and Griffiths, 2011) is applied to train the LDA model.

There are two imbalance problems which might influence the performance of the detecting method. Two measures are taken to adjust them.

(1) Adjustment on dimensionality imbalance

The dimensionality of each subset in the input layer is unevenly distributed. For instance, the physical component has only two dimensions while the psychological component has 43, and the textual feature even has hundreds of dimensions. Therefore, when they are stacked in one layer, the impacts of each part are unequal. To alleviate the effect of dimensionality imbalance, all the subsets in the input layer (the four components and the textual feature) are weighted before being sent to the hidden layer. The weights are inversely proportional to their dimensionalities and the reciprocals of the dimensionalities are applied in this work.

(2) Adjustment on sample imbalance

The imbalance between the amounts of different types of samples will also impact the training of the model. Similar to the adjustment on dimensionality imbalance, a weight matrix V is introduced, whose element v
                        
                           ij
                         is inversely proportional to the sample size. We set 
                           
                              
                                 v
                                 
                                    i
                                    j
                                 
                              
                              =
                              1
                              /
                              
                                 
                                    
                                       N
                                       
                                          j
                                          k
                                       
                                    
                                    +
                                    1
                                 
                              
                           
                        , where i is the sample ID, j is the dimension ID, and k is the intensity rating, N
                        
                           jk
                         means the amount of the samples whose j-th dimension is rated as k. The weight matrix V is added to the diagonal optimization matrix Λ during the learning of DSN (Section 4.3), by which the element in matrix Λ is changed into 
                           
                              
                                 λ
                                 
                                    i
                                    i
                                    _
                                    a
                                    d
                                 
                              
                              =
                              
                                 
                                    N
                                    /
                                    E
                                    
                                       ∑
                                       j
                                    
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      y
                                                      
                                                         i
                                                         j
                                                      
                                                   
                                                   −
                                                   
                                                      t
                                                      
                                                         i
                                                         j
                                                      
                                                   
                                                
                                             
                                          
                                          2
                                       
                                       
                                          v
                                          
                                             i
                                             j
                                          
                                       
                                       +
                                       1
                                    
                                 
                              
                              /
                              2
                           
                        , and the other steps remain the same.

Experiments are implemented to validate the effectiveness of the detecting method based on the multi-component emotion model. Before introducing the verification experiments, the database and the annotation work are illustrated first. Then some parameter tests are carried out to confirm the parameters during the feature extraction and the network training. Finally the verification experiments are illustrated and the results are analyzed.

The Root-Mean-Square-Error 
                        
                           (
                           R
                           M
                           S
                           E
                           =
                           
                              
                                 
                                    ∑
                                    i
                                    N
                                 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      y
                                                   
                                                   i
                                                
                                                −
                                                
                                                   
                                                      t
                                                   
                                                   i
                                                
                                             
                                          
                                       
                                       2
                                    
                                 
                                 /
                                 N
                              
                           
                           )
                        
                      is adopted as the measurement, as the affective states in the proposed model are represented by multiple dimensions and RMSE is able to measure the distance between the predictions and the tags in the multi-dimensional space. Additionally, the commonly used measurements in the affective detection task (recall, precision and F-value) are also appended to supply a general evaluation of the present work.

The verification experiments are implemented based on a Chinese news broadcast database. The materials are collected from a column of China National Radio – “News and Newspapers Summary”, containing both the textual form and the spoken form. The speaker is a Chinese female professional broadcaster. The database covers a wide range of topics (education, health, science, economy, culture, art, entertainment, military and politics) and a variety of reporting styles (broadcast, newsletter, announcement, commentary and obituary). Totally 8600 documents are included, which are further split into 27,431 paragraphs and 64,568 sentences.

To train and test the model, a part of the database is annotated, based on the proposed emotion model. 150 documents are picked out (considering the topic types, reporting styles and the intensities of emotional expression) and annotated by three annotators at document level, paragraph level and sentence level. They are suggested to annotate the document level first then only need to mark the obvious variations at lower levels with respect to the basis at document level. In order to further reduce the workload of annotation, only the bottom layer of the psychological component is annotated. Then the values of upper layers can be inferred from the members in sub-layers. There are different ways to combine the ratings in sub-layers to produce a single annotation for the higher layer, such as taking the average value or the maximum value. The present work adopts the latter.

The final agreement of the annotations is 78%, which is regarded as acceptable. To get a single annotation for each sample, the annotated data from different annotators is integrated together through a weighted average method. Compared with the arithmetic average method, which treats each annotation equally, the weighted method supplies a different weight for each annotation to weaken the contribution of the ratings that seem unreliable. The Gaussian weight is utilized in this work, which is calculated by Eq. (7). Eq. (8) presents the weighted average process. 
                           
                              
                                 W
                                 k
                              
                              
                                 
                                    i
                                    ,
                                    j
                                 
                              
                           
                         stands for the Gaussian weight, i is the index of samples, j is the index of dimensions, and k is the index of annotators. dist
                           euc
                         represents the Euclidean distance between the individual annotation 
                           
                              
                                 L
                                 k
                              
                              
                                 
                                    i
                                    ,
                                    j
                                 
                              
                           
                         and the mean value M(i, j). 
                           
                              L
                              
                                 
                                    i
                                    ,
                                    j
                                 
                              
                           
                         is the weighted average annotation which we want. σ is a scale parameter, which influences the shape of the Gaussian weight curves (see Fig. 5
                        ). It is set as 0.2 in this work.
                           
                              (7)
                              
                                 
                                    
                                       W
                                       k
                                    
                                    
                                       
                                          i
                                          ,
                                          j
                                       
                                    
                                    =
                                    exp
                                    
                                       
                                          −
                                          d
                                          i
                                          s
                                          
                                             t
                                             
                                                e
                                                u
                                                c
                                             
                                          
                                          
                                             
                                                
                                                   L
                                                   k
                                                
                                                
                                                   
                                                      i
                                                      ,
                                                      j
                                                   
                                                
                                                ,
                                                M
                                                (
                                                i
                                                ,
                                                j
                                                )
                                             
                                          
                                          /
                                          2
                                          
                                             σ
                                             2
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (8)
                              
                                 
                                    L
                                    
                                       
                                          i
                                          ,
                                          j
                                       
                                    
                                    =
                                    
                                       ∑
                                       k
                                    
                                    
                                       
                                          W
                                          k
                                       
                                       
                                          
                                             i
                                             ,
                                             j
                                          
                                       
                                       ∗
                                       
                                          L
                                          k
                                       
                                       
                                          
                                             i
                                             ,
                                             j
                                          
                                       
                                       /
                                       
                                          ∑
                                          k
                                       
                                       
                                          
                                             W
                                             k
                                          
                                          
                                             
                                                i
                                                ,
                                                j
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

The finalized annotated samples are statistically analyzed and the results are shown in Fig. 6
                        . Two dimensions (Euphemistic–Straightforward and Discrete–Adhesive) are excluded since they are not distinguished in the current corpus. The amounts of different types of samples are quite unbalanced, which therefore have to be adjusted by the measure described in Section 4.5.

To confirm the parameters during the extraction of textual feature and the network training, several parameter tests are carried out. In addition, the effect of the data size (especially the amount of unlabeled data) is also tested. The following tests are implemented on three datasets: 150 documents (all labeled), 600 documents (150 labeded and 450 unlabeled), and 8600 documents (150 labeded and 8450 unlabeled).

The LDA model is used to extract textual features. There are three parameters to be determined: two hyperparameters and the topic number. The two hyperparameters are introduced to control the sparsity of the two distributions: the document-topic distribution and the topic-word distribution. β controls the sparsity of the topic-word distribution and a smaller value of β means that the model prefers to assign fewer words to each topic. α controls the sparsity of the document-topic distribution and a smaller value means that the model prefers to characterize documents by fewer topics. The topic number corresponds to the dimensionality of textual feature, further influences the complexity of the neural network. A thorough estimation of the parameters is quite complex since they are interacted with each other. Two empirical values of the two hyperparameters are adopted, α
                           =50/T (T is the topic number) and β
                           =0.01, which have been tested to provide a good model quality (Griffths, 2002).

The model quality is evaluated by “perplexity”, which is calculated through Eq. (9). D is the number of documents, N
                           
                              d
                            is the number of words in each document, and 
                              
                                 p
                                 
                                    
                                       
                                          w
                                          d
                                       
                                    
                                 
                              
                            represents the likelihood of each word. The perplexity is inversely proportional to the mean per-word likelihood and a lower value of perplexity indicates a better model quality (Blei et al., 2003).
                              
                                 (9)
                                 
                                    
                                       perplexity
                                       =
                                       exp
                                       
                                          
                                             −
                                             
                                                ∑
                                                d
                                                D
                                             
                                             
                                                log
                                                
                                                   
                                                      p
                                                      
                                                         
                                                            
                                                               w
                                                               d
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                             /
                                             
                                                ∑
                                                d
                                                D
                                             
                                             
                                                
                                                   N
                                                   d
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        


                           Fig. 7
                           (a) shows the perplexity curves varying with topic number (10–400). The topic numbers of the three datasets are set as 100, 120 and 210 respectively, determined by the points that the tangent values are less than 1, which means further increasing the topic number, the perplexity declines slowly or no longer changes. In addition, fixed the topic numbers and one hyperparameter, the perplexity curves varying with the other hyperparameter are presented in Fig. 7(b) and (c). It is shown that the recommended hyperparameters provide relatively lower perplexities, which means that the current combination of the three parameters is reasonable and available.

Three parameters related with the training of neural networks are analyzed: the number of the units in hidden layer, the number of iterations in the fine-tuning stage and the epoch of RBM.

A 5-fold Cross Validation (CV) is implemented, in which the annotated samples are randomly partitioned into five equal size subsets, and each time (one fold) one subset is used as the test set and the other parts (along with the unlabelled data) are used as the training set. The results of the five folds are averaged to produce a single estimation. In addition, due to the random nature during the initialization of RBM, the 5-fold CV is repeated 10 times and the final results are the means of these 10 tests. The experiments below are all in the same way. The test is evaluated by the RMSE of utterance manner as it is the final output of the detecting model.


                           Fig. 8
                            shows the RMSEs of utterance manner varying with the three testing parameters. When one of them is being tested, the other two are set as 10. Fig. 8(a) shows that with the growth of the number of hidden units, the RMSEs of the three datasets all drop first then rise back. The number of hidden units is determined as 10, where the RMSE is not very large and the hidden layer will not be too thin with respect to the input layer. Fig. 8(b) represents that when the iteration number is less than 10, the RMSEs of the three datasets are relatively small. Thereby the iteration number is set as 10. Fig. 8(c) shows that with the enlargement of the epoch of RBM, the RMSEs of the smallest dataset are oscillating, and the RMSEs of the other two datasets keep steady after a slight decrease. The epoch of RBM is also set as 10.

In addition to the effects of the three parameters, we can also investigate the influence of the data size from Fig. 8. The RMSEs of the smallest dataset are higher than the other two datasets; the RMSEs of the medium-sized dataset are in the middle; and the largest dataset has the lowest RMSEs. This suggests that adding unlabeled data into the training set will lead a reduction of the RMSE; further enlarging the amount of unlabeled data will be more beneficial. In the following experiments, the largest dataset is adopted.

The effectiveness of proposed method is validated from the following experiments. Firstly, the effect of stacking other components is tested, to validate if the involvement of the interactions among the components is helpful for the emotion detection. Secondly, the adopted network (SCN) is compared with the other two networks shown in Fig. 2, to validate the efficiency of the stacking network integrated with the producing process. Thirdly, the contextual impact is tested, which is assumed to further improve the model quality. Finally, all the influential factors are combined together to supply an overall estimation of the present work.

There are different ways to predict the four components, with or without stacking other components, stacking a part of them or all of them. The exclusion of feedback leads fewer stacking patterns to be considered. We produce the four components in all the remaining ways to test the benefits of stacking other components, further to seek better stacking patterns. The four ways without stacking any other components are set as baselines for the producing of the four components respectively (the italic rows in Table 2
                           ). The adjustments on dimensionality imbalance and sample imbalance (as illustrated in Section 4.5) are also tested in this part.

The “Original” column in Table 2 lists the results without adjusting imbalances, from which a progressive decrease of RMSEs can be found along with the stacking of other components. After adjusting the imbalance of dimensionality, the progressive decrease of RMSEs becomes more obvious (see the “Adjust1” column in Table 2). Further adjusting the imbalance of sample size, the RMSEs become even smaller (see the “Adjust1+ Adjust2” column). T-test proves that the reductions between the columns are significant (p-value<0.05).

From the comparison between different stacking patterns and their respective baselines, the following phenomena can be found: stacking cognitive appraisal is beneficial to predict psychological feeling and the improvement is significant (p-value=0.047); appending either cognitive appraisal (p-value=0.092) or psychological feeling (p-value=0.21) onto the input of physical response is less helpful unless adding both of them (p-value=0.037); except adding cognitive appraisal (p-value=0.11) or physical response (p-value=0.092) individually, the other stacking patterns are all significantly useful to improve the prediction of utterance manner (p-value<0.05). These results show that stacking other components improves the results of affective computing, and stacking more components seems to be more helpful, which supports our proposal that all the previous components might influence the generating of latter components.

The adopted network SCN is a deep network with a stacking architecture and visible intermediate layers. It is compared with the other two networks shown in Fig. 2. DNN is a normal deep neural network with multiple hidden layers but without a stacking architecture; DSN is a deep stacking network but the intermediate layers are hidden; SCN is the proposed network with a stacking architecture and visible intermediate layers. The comparison between DNN and DSN can validate the utility of the stacking architecture; further compare DSN with SCN, the effect of the visible intermediate layers can be validated. All these three networks are pre-trained by RBM.

The comparison takes the utterance manner as the target. There are different ways to generate utterance manner when using SCN, see Table 2, corresponding to different depths of the network. The network without stacking other components (“text->utt”) contains only one module, and the depth of it is considered as 1. Along this way, the depth of the network with one other component stacked (“text->cog->utt”, “text->psy->utt”, and “text->phy->utt”) is 2, and the depth with two other components stacked (“text->cog->psy->utt”, “text->cog->phy->utt”, and “text->psy->phy->utt”) is 3, and the depth of the final full stacking pattern is 4. Table 3
                            lists the predicted errors through different networks with different depths. The results of SCN with two or three other components stacked are not single, so they are averaged to compare with others.


                           Table 3 shows that DSN has smaller RMSEs than DNN (p-value<0.01), which implies that the stacking network is helpful. SCN performs better than DSN (p-value<0.01), which indicates that visible intermediate layers are more beneficial than hidden ones. In addition, along with the increase of the network depth, the RMSEs decrease gradually especially in SCN. The results of the other two networks are not improved when the networks become deeper, which implies that the current dataset might be not large enough to train a very deep network. But the appending of other related components transcends this limitation to some extent, which again proves the benefits of introducing prior knowledge.

Besides the stacking of other components, the features from higher levels of textual units are also appended to supply contextual references. This experiment is to validate the impact of the contextual features. The results without stacking contextual features are set as baselines (the italic rows in Table 4
                           ). The components at the same level are stacked along the producing process.


                           Table 4 lists the prediction errors of the four components at paragraph level and sentence level. Through the comparison between their baselines respectively, we can investigate that most of the RMSEs with contextual features are obviously reduced. At paragraph level, except the results of physical response (p-value=0.12), all the other improvements are significant (p-value<0.01). At sentence level, the document feature seems more influential than the paragraph feature, but stacking both of them is most helpful. Except the result of utterance manner with only the paragraph feature stacked (p-value=0.20), all the other changes are significant (p-value<0.01). The reduction of the RMSEs demonstrates that stacking the contextual features is beneficial, and using the document feature and paragraph feature to supply contextual information is effective.


                           Tables 2 and 4 show that the RMSEs of the four components are not on the same level, where cognitive appraisal and utterance manner have relatively larger RMSEs (around 2.0) and physical response has much smaller RMSE than others (around 0.60). We analyze that there may be two reasons related with this situation. One reason is that RMSE is a distance measure in a multi-dimensional space. The value of this measure is related with the dimensionality of the space. For instance, the physical response is represented by a two-dimensional space, and the RMSE for a single sample is equivalent to the sum of the errors on the two dimensions, while the RMSE of utterance manner is a sum of the errors on six dimensions. In this way, the average error on each dimension of cognitive appraisal is 0.95, transformed by the value of RMSE (
                              
                                 R
                                 M
                                 S
                                 
                                    E
                                    
                                       dim
                                    
                                 
                                 =
                                 
                                    
                                       R
                                       M
                                       S
                                       
                                          E
                                          2
                                       
                                       /
                                       dimensionality
                                    
                                 
                              
                           ). Similarly, the average error on each dimension of psychological feeling is 0.23, the one for physical response is 0.40, and the one for utterance manner is 0.83. We can find that the mean deviation between the predicted value and the tagged value on each dimension is no more than one notch, which is suggested to be fairly acceptable.

The other reason for the uneven results might be related with the limitation of the textual feature, which is extracted based on the co-occurrence frequencies of lexical. Although the LDA model attempts to add semantic analysis to the feature, there is still much unknown information which may affect the estimation of cognitive appraisal and utterance manner, such as the literal style and the personal standpoint. These kinds of features are beyond the textual content, which are not able to be detected from the semantic-based textual feature. Therefore, the extraction of this kind of background knowledge needs further research.

In addition to RMSE, the recall, precision and F-value are also appended. Nevertheless, the present model is a fairly complex one that multiple dimensions are included and each dimension is divided into different intensities. We take the average of all the dimensions and intensities of the utterance manner to give an overall evaluation of the prediction. The result without any other components or contextual features stacked is set as the “Baseline”. The proposed method that considers both the impacts of other components and contextual features is labeled as “Stacking”. From Table 5
                            we can find that the recall, precision and F-value are all improved, where the recall grows 31.8% and the F-value increases 22.8% relatively.

In summary, both the components in the proposed model and the features from different textual levels are interacted with each other, and the stacking network is available to model the effects among them. Stacking each of them or both of them is beneficial for the detecting of affective states.

@&#CONCLUSION@&#

This work focuses on detecting affective states from text based on a newly proposed multi-component emotion model toward the application in expressive TTS synthesis. Four types of affective components are involved and organized in a multi-layered structure. The deep stacking network is introduced to detect these components from text, and the producing process of the four components is adopted to guide the learning of the network additionally, which is called “visualize” the intermediate layers of the deep network partially. Experiments validate the effectiveness of the proposed model and the stacking network for emotion detection. Moreover, the affective states from higher levels of texts are considered as contextual references to supply additional information, which is proved to be beneficial.

The multi-component emotion model is proposed based on Chinese terms. But the four components and their interrelations are obtained based on the analysis of psychological theories which are language independent. And each component is represented by several dimensions which are expected to describe the inherent natures. Therefore the proposed model and the detecting method are suggested to be suitable for other languages, but the definite performance remains to be tested in the future. In addition, due to the limitations of the textual features at present, as well as the adopted dataset with unbalanced samples, more related features will be considered and more annotated samples for each type of categories will be tested.

@&#REFERENCES@&#

