@&#MAIN-TITLE@&#A hybrid cost-sensitive ensemble for imbalanced breast thermogram classification

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           Novel cost-sensitive hybrid ensemble for efficient breast thermogram classification.


                        
                        
                           
                           Evolutionary algorithm for simultaneous feature selection and classifier weighting.


                        
                        
                           
                           A combined criterion for the ensemble training algorithm.


                        
                        
                           
                           Estimating cost parameter with the usage of ROC analysis.


                        
                        
                           
                           Embedded pruning procedure for discarding irrelevant classifiers.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Classifier ensemble

Imbalanced classification

Cost-sensitive classification

Ensemble pruning

Evolutionary algorithm

Breast cancer detection

Thermogram

@&#ABSTRACT@&#


               
               
                  Objectives
                  Early recognition of breast cancer, the most commonly diagnosed form of cancer in women, is of crucial importance, given that it leads to significantly improved chances of survival. Medical thermography, which uses an infrared camera for thermal imaging, has been demonstrated as a particularly useful technique for early diagnosis, because it detects smaller tumors than the standard modality of mammography.
               
               
                  Methods and material
                  In this paper, we analyse breast thermograms by extracting features describing bilateral symmetries between the two breast areas, and present a classification system for decision making. Clearly, the costs associated with missing a cancer case are much higher than those for mislabelling a benign case. At the same time, datasets contain significantly fewer malignant cases than benign ones. Standard classification approaches fail to consider either of these aspects. In this paper, we introduce a hybrid cost-sensitive classifier ensemble to address this challenging problem. Our approach entails a pool of cost-sensitive decision trees which assign a higher misclassification cost to the malignant class, thereby boosting its recognition rate. A genetic algorithm is employed for simultaneous feature selection and classifier fusion. As an optimisation criterion, we use a combination of misclassification cost and diversity to achieve both a high sensitivity and a heterogeneous ensemble. Furthermore, we prune our ensemble by discarding classifiers that contribute minimally to the decision making.
               
               
                  Results
                  For a challenging dataset of about 150 thermograms, our approach achieves an excellent sensitivity of 83.10%, while maintaining a high specificity of 89.44%. This not only signifies improved recognition of malignant cases, it also statistically outperforms other state-of-the-art algorithms designed for imbalanced classification, and hence provides an effective approach for analysing breast thermograms.
               
               
                  Conclusions
                  Our proposed hybrid cost-sensitive ensemble can facilitate a highly accurate early diagnostic of breast cancer based on thermogram features. It overcomes the difficulties posed by the imbalanced distribution of patients in the two analysed groups.
               
            

@&#INTRODUCTION@&#

Breast cancer is the most commonly diagnosed form of cancer in women, accounting for about 30% of all cases [1]. Medical thermography, which uses a thermal-imaging infrared camera to capture a temperature distribution of the human body, has been shown to be well-suited for the task of detecting breast cancer [2,3]. In contrast to other modalities such as mammography, thermography is a non-invasive, non-contact, passive, and radiation-free technique. The radiance from human skin is a function of the surface temperature which in turn is influenced by the level of blood perfusion in the skin. Thermal imaging is hence capable of detecting changes in blood perfusion which may be the result of inflammation, angiogenesis, or other causes [4]. Asymmetrical temperature distributions as well as hot or cold spots are known to be strong indicators of an underlying dysfunction [5].

It has been shown that thermography has advantages over the standard modality of mammography for breast cancer diagnosis, in particular when the tumor is in its early stages or in dense tissue. Early detection is crucial as it provides significantly higher chances of survival [6], and in this respect infrared imaging can outperform mammography. While mammography can detect tumors only after they exceed a certain size, even small tumors can be identified using thermography, because the high metabolic activity of cancer cells leads to an increase in local temperature detectable by infrared imaging. Published analyses report that the average tumor size undetected by mammography is 1.66 cm compared to only 1.28 cm by thermography [7].

In this paper, we present an effective approach for analysing thermographic breast images to detect breast cancer. The basis of our approach is the characterisation of image features describing the bilateral symmetry between the two breast regions, because tumors lead to asymmetries between the temperature distributions of the two sides. The image features we employ include basic statistical features, histogram features, image moments, and various texture features.

The derived image features are then used during a pattern classification stage. While numerous classification algorithms have been published [8], it is well known that no method is superior for all possible decision problems [9]. Consequently, it is common to train several predictors and then select a model that displays desirable properties for the problem at hand. An alternative is to employ a multiple classifier system, or ensemble classifier, considers the decisions of a committee of individual classifiers [10].

There are several factors that may strongly affect the performance of classification algorithms. An important aspect, in particular in the medical domain [11], is the underlying class distribution. Most learning models assume that the distribution of samples is roughly equal between the classes. An imbalanced binary classification problem [12] occurs when the number of samples in one class (the majority class) significantly outnumbers that in the other class (the minority class). Imbalanced datasets frequently appear in a variety of applications including medical data analysis [13], anomaly detection [14], handwritten symbol recognition [15], or object detection [16]. While classifier performance is typically evaluated based on predictive accuracy, this is not appropriate for imbalanced data as it would favour a bias towards the majority class.

Furthermore, and again particularly in medical decision making, it is often the minority class that is of higher importance. Clearly, the costs associated with missing a cancer case (false negative) are much higher than those for mislabelling a benign one (false positive). Therefore, standard classifiers aimed only at maximising overall classification accuracy have no means of considering this, inevitably resulting in an inferior decision making system [17].

In this paper, we address these challenges and propose, based on our earlier work [18], a hybrid cost-sensitive classifier ensemble algorithm for effective breast thermogram feature analysis. We use cost-sensitive decision trees as base classifiers, because they are readily improved with the ensemble approach. Instead of using a pre-defined cost matrix, we derive its parameters through receiver operating characteristic (ROC) analysis. To generate our ensemble, we employ an evolutionary algorithm to simultaneously perform feature selection and classifier fusion. In that context, we propose a combination of cost and diversity criteria to form a pool of mutually complementary classifiers which provides excellent sensitivity. We furthermore prune the ensemble by removing predictors that contribute minimally to the overall decision, thereby reducing the computational complexity of the ensemble. Extensive experimental results demonstrate that our approach achieves improved recognition of cancer cases, statistically outperforming several state-of-the-art algorithms designed for imbalanced classification. The main contributions of this paper are as follows:
                        
                           •
                           A new hybrid ensemble approach based on combining cost-sensitive decision trees for efficient breast thermogram classification.

Use of an evolutionary algorithm for simultaneous feature selection and weight assignment for classifier fusion. This method explores diverse subsets of features and promotes the best individual classifiers to boost the recognition rate of the malignant class.

A combined criterion for the training algorithm which simultaneously minimises the overall misclassification cost and assures diversity among the pool of classifiers.

Analysis of the influence of the cost matrix parameters (derived based on ROC analysis) on the performance of the proposed ensemble.

A pruning procedure for discarding classifiers with low influence on the final decision, decreasing the computational complexity of the ensemble.

The remainder of the paper is organised as follows. In Section 2 we describe the analysed breast thermogram data, while Section 3 discusses the problem of imbalanced classification. Our new algorithm is introduced in detail in Section 4. Experimental results are reported and discussed in Section 5, while Section 6 concludes the paper.

While both frontal and/or lateral-view thermograms can be imaged for breast cancer diagnosis, in this paper, we restrict out attention to frontal-view images. As has been shown [19], analyzing the symmetry between the thermal images of the left and right breast is an effective approach to automatically detecting cancer cases. A malignant tumor recruits blood vessels resulting in hot spots and a change in the vascular pattern. This causes an asymmetry between the temperature distributions of the two breasts [20]. In contrast, symmetrical thermal images typically signify the absence of breast cancer.

We segment the areas corresponding to the left and right breast from the thermograms. Once segmented, we convert the breast regions to a polar coordinate representation to simplify the calculation of several of our features. A series of statistical features is then calculated to provide indications of symmetry between the regions of interest (i.e., the two breast areas) [21].

The simplest feature describing the temperature distribution captured in thermograms is the statistical mean. Since we are interested in symmetry features, we calculate the mean for each breast and use the absolute difference between the two. Similarly, we calculate the standard temperature deviation and use the absolute difference as a feature. Furthermore, we employ the absolute differences of the median temperature and the 90th percentile temperature.

Image moments [22] are defined as
                        
                           (1)
                           
                              
                                 m
                                 pq
                              
                              =
                              
                                 ∑
                                 
                                    y
                                    =
                                    0
                                 
                                 
                                    M
                                    −
                                    1
                                 
                              
                              
                                 ∑
                                 
                                    x
                                    =
                                    0
                                 
                                 
                                    N
                                    −
                                    1
                                 
                              
                              
                                 x
                                 p
                              
                              
                                 y
                                 q
                              
                              g
                              (
                              x
                              ,
                              y
                              )
                              ,
                           
                        
                     where x and y denote the pixel location and N and M the image size. We utilise moments m
                     01 and m
                     10 which essentially describe the centre of gravity of the breast regions, as well as the distance (in both the x and y directions) of the centre of gravity from the geometrical centre of the breast. For all four features, we calculate the absolute differences of the values between the left and right breast.

Histograms record the frequencies of certain temperature ranges in the thermograms. In our work, we construct normalised histograms for each breast and use the cross-correlation between the two histograms as a feature. From the difference histogram (i.e., the difference between the two histograms), we compute the absolute value of its maximum, the number of bins exceeding a certain threshold (0.01 in our experiments), the number of zero crossings, energy and the difference of the positive and negative parts of the histogram.

Co-occurrence matrices have been widely used in texture recognition tasks [23] and can be defined as
                        
                           (2)
                           
                              
                                 γ
                                 
                                    
                                       T
                                       i
                                    
                                    ,
                                    
                                       T
                                       j
                                    
                                 
                                 
                                    (
                                    k
                                    )
                                 
                              
                              (
                              I
                              )
                              =
                              
                                 PR
                                 
                                    
                                       p
                                       1
                                    
                                    ∈
                                    
                                       I
                                       
                                          
                                             T
                                             i
                                          
                                       
                                    
                                    ,
                                    
                                       p
                                       2
                                    
                                    ∈
                                    I
                                 
                              
                              [
                              
                                 p
                                 2
                              
                              ∈
                              
                                 I
                                 
                                    
                                       T
                                       j
                                    
                                 
                              
                              ,
                              |
                              
                                 p
                                 1
                              
                              −
                              
                                 p
                                 2
                              
                              |
                              =
                              k
                              ]
                              ,
                           
                        
                     with
                        
                           (3)
                           
                              |
                              
                                 p
                                 1
                              
                              −
                              
                                 p
                                 2
                              
                              |
                              =
                              max
                              {
                              |
                              
                                 x
                                 1
                              
                              −
                              
                                 x
                                 2
                              
                              |
                              ,
                              |
                              
                                 y
                                 1
                              
                              −
                              
                                 y
                                 2
                              
                              |
                              }
                              ,
                           
                        
                     where T
                     
                        i
                      and T
                     
                        j
                      denote two temperature values and (x
                     
                        k
                     , y
                     
                        k
                     ) denote pixel locations. In essence, given a temperature T
                     
                        i
                      in thermogram I, γ is the probability that a pixel at distance k away has a temperature of T
                     
                        j
                     . To arrive at an indication of asymmetry between the two sides, we adopt this concept and use the cross co-occurrence matrix [24] defined as
                        
                           (4)
                           
                              
                                 γ
                                 
                                    
                                       T
                                       i
                                    
                                    ,
                                    
                                       T
                                       j
                                    
                                 
                                 
                                    (
                                    k
                                    )
                                 
                              
                              (
                              I
                              (
                              1
                              )
                              ,
                              I
                              (
                              2
                              )
                              )
                              =
                              
                                 PR
                                 
                                    
                                       p
                                       1
                                    
                                    ∈
                                    I
                                    
                                       
                                          (
                                          1
                                          )
                                       
                                       
                                          
                                             T
                                             i
                                          
                                       
                                    
                                    ,
                                    
                                       p
                                       2
                                    
                                    ∈
                                    I
                                    (
                                    2
                                    )
                                 
                              
                              [
                              
                                 p
                                 2
                              
                              ∈
                              I
                              
                                 
                                    (
                                    2
                                    )
                                 
                                 
                                    
                                       T
                                       j
                                    
                                 
                              
                              ,
                              |
                              
                                 p
                                 1
                              
                              −
                              
                                 p
                                 2
                              
                              |
                              =
                              k
                              ]
                              ,
                           
                        
                     i.e., temperature values from one breast are related to those of the other. From this matrix, we can extract the following features [23]:
                        
                           (5)
                           
                              Homogeneity
                              
                              G
                              =
                              
                                 ∑
                                 k
                              
                              
                                 ∑
                                 l
                              
                              
                                 
                                    
                                       γ
                                       
                                          k
                                          ,
                                          l
                                       
                                    
                                 
                                 
                                    1
                                    +
                                    |
                                    k
                                    −
                                    l
                                    |
                                 
                              
                           
                        
                     
                     
                        
                           (6)
                           
                              Energy
                              
                              E
                              =
                              
                                 ∑
                                 k
                              
                              
                                 ∑
                                 l
                              
                              
                                 γ
                                 
                                    k
                                    ,
                                    l
                                 
                                 2
                              
                           
                        
                     
                     
                        
                           (7)
                           
                              Contrast
                              
                              C
                              =
                              
                                 ∑
                                 k
                              
                              
                                 ∑
                                 l
                              
                              |
                              k
                              −
                              l
                              |
                              
                                 γ
                                 
                                    k
                                    ,
                                    l
                                 
                              
                           
                        
                     and
                        
                           (8)
                           
                              Symmetry
                              
                              S
                              =
                              1
                              −
                              
                                 ∑
                                 k
                              
                              
                                 ∑
                                 l
                              
                              |
                              
                                 γ
                                 
                                    k
                                    ,
                                    l
                                 
                              
                              −
                              
                                 γ
                                 
                                    l
                                    ,
                                    k
                                 
                              
                              |
                              .
                           
                        
                     
                  

We further calculate the first four moments, m
                     1 to m
                     4, of the matrix
                        
                           (9)
                           
                              
                                 m
                                 p
                              
                              =
                              
                                 ∑
                                 k
                              
                              
                                 ∑
                                 l
                              
                              
                                 
                                    (
                                    k
                                    −
                                    l
                                    )
                                 
                                 p
                              
                              
                                 γ
                                 
                                    k
                                    ,
                                    l
                                 
                              
                              .
                           
                        
                     
                  

The mutual information MI between two distributions is calculated from their joint entropy H and is defined as
                        
                           (10)
                           
                              MI
                              =
                              
                                 H
                                 L
                              
                              +
                              
                                 H
                                 R
                              
                              +
                              H
                           
                        
                     with
                        
                           (11)
                           
                              
                                 
                                    
                                       
                                          H
                                          L
                                       
                                    
                                    
                                       =
                                    
                                    
                                       −
                                       
                                          ∑
                                          k
                                       
                                       
                                          P
                                          L
                                       
                                       (
                                       k
                                       )
                                       
                                          log
                                          2
                                       
                                       
                                          p
                                          L
                                       
                                       (
                                       k
                                       )
                                    
                                 
                                 
                                    
                                       
                                          H
                                          R
                                       
                                    
                                    
                                       =
                                    
                                    
                                       −
                                       
                                          ∑
                                          l
                                       
                                       
                                          P
                                          R
                                       
                                       (
                                       l
                                       )
                                       
                                          log
                                          2
                                       
                                       
                                          p
                                          R
                                       
                                       (
                                       l
                                       )
                                    
                                 
                                 
                                    
                                       H
                                    
                                    
                                       =
                                    
                                    
                                       
                                          ∑
                                          k
                                       
                                       
                                          ∑
                                          l
                                       
                                       
                                          P
                                          LR
                                       
                                       (
                                       k
                                       ,
                                       l
                                       )
                                       
                                          log
                                          2
                                       
                                       
                                          p
                                          
                                             L
                                             ,
                                             R
                                          
                                       
                                       (
                                       k
                                       ,
                                       l
                                       )
                                    
                                 
                                 
                                    
                                    
                                    
                                 
                              
                           
                        
                     and
                        
                           (12)
                           
                              
                                 
                                    
                                       
                                          p
                                          LR
                                       
                                       (
                                       k
                                       ,
                                       l
                                       )
                                    
                                    
                                       =
                                    
                                    
                                       
                                          x
                                          
                                             k
                                             ,
                                             l
                                          
                                       
                                       /
                                       
                                          ∑
                                          
                                             k
                                             ,
                                             l
                                          
                                       
                                       x
                                       (
                                       k
                                       ,
                                       l
                                       )
                                    
                                 
                                 
                                    
                                       
                                          p
                                          L
                                       
                                       (
                                       k
                                       )
                                    
                                    
                                       =
                                    
                                    
                                       
                                          ∑
                                          l
                                       
                                       
                                          p
                                          LR
                                       
                                       (
                                       k
                                       ,
                                       l
                                       )
                                    
                                 
                                 
                                    
                                       
                                          p
                                          R
                                       
                                       (
                                       k
                                       )
                                    
                                    
                                       =
                                    
                                    
                                       
                                          ∑
                                          k
                                       
                                       
                                          p
                                          LR
                                       
                                       (
                                       k
                                       ,
                                       l
                                       )
                                    
                                 
                                 
                                    
                                    
                                    
                                 
                              
                           
                        
                     
                  

We thus use MI as a further descriptor.

Finally, we calculate the Fourier spectra of the two regions of interest (ROIs) and use the difference of their absolute values. The features we adopt are the maximum of the difference and the distance of this maximum from the centre.

In summary, we characterise each breast thermogram using the following set of features: four basic statistical features, four moment features, eight histogram features, eight cross co-occurrence features, mutual information, and two Fourier descriptors. We further apply a Laplacian filter to enhance the contrast and calculate an additional subset of features (the eight cross co-occurrence features together with mutual information and the two Fourier descriptors) from the resulting images. In total, we compile a set of 38 descriptors designed to describe the thermal asymmetry between the two breasts.

A dataset is imbalanced if the classification categories are not (approximately) equally represented [25]. Classifier performance is typically evaluated based on predictive accuracy. However, this simple and intuitive measure is not appropriate for imbalanced data, because it typically leads to a bias towards the majority class. Consequently, a classifier can display a poor recognition rate for the minority class while still achieving a high overall accuracy, as illustrated in Fig. 1
                     .

The disproportion in terms of the number of samples in different classes in the training set is not the sole source of complications [26]. It has been shown that if the number of minority samples is sufficient, the uneven class distribution itself does not cause a significant drop in recognition rate [27]. However, the uneven class distribution is usually accompanied by other difficulties such as:
                        
                           •
                           Class overlap: A decision hyperplane constructed to minimise the number of incorrect classifications may lead to poor performance with respect to the minority class in the overlap region [28].

Small sample size: In many cases, the number of minority class samples is insufficient to properly train a classifier leading to poor generalisation and model overfitting.

Small disjuncts: It is possible that the minority class does not have an atomic distribution and is represented by a number of sub-concepts such that its samples are spread among several “chunks” of data.

Various approaches have been suggested to address class imbalance. Multiple classifier systems (MCSs), which are based on the principle of combining the decisions of several base classifiers [10], typically include one of the techniques dedicated to dealing with imbalanced data [29–31]. SMOTEBagging [29] and SMOTEBoosting [32] are the most popular examples of combinations of oversampling and ensemble classifiers. These methods are based on introducing new objects into each of the bagging or boosting iterations separately using SMOTE (Synthetic Minority Over-sampling Technique) [25]. IIVotes [33] fuses a rule-based ensemble with a SPIDER (a Matlab machine learning library) pre-processing scheme to achieve a more robust classifier for atypical data distributions in minority classes. Cost-sensitive MCSs typically revolve around sample weight adjustments in a boosting scheme [34]. EasyEnsemble [35] is a hierarchical MCS, as it utilises bagging as the primary learning scheme, but AdaBoost is the base model for each of the bags.

In this paper, we propose an effective hybrid cost-sensitive classifier ensemble (HCSE) to address class imbalance and misclassification costs in the context of breast thermogram analysis. Our aim is to create an ensemble with minimal classification error P within the cost bounds of a cost matrix C.

Our hybrid method consists of a pool of cost-sensitive decision trees. A genetic algorithm is used to select features for each of the base classifiers and to simultaneously assign their degree of importance in a weighted voting fusion process. We use a combined fitness function to guarantee a boosted recognition rate of the minority class while maintaining diversity among the base predictors. Finally, to reduce the overall computational complexity of our composite model, we prune it by discarding classifiers with weights below a given threshold.

The high-level overview of our approach is illustrated in Fig. 2
                     ; in the following subsections, we describe in detail each component.

To create an MCS, we require a pool of base classifiers at our disposal. To this end, we select a cost-sensitive classification tree rooted in the idea of the EG2 algorithm [36] and whose induction technique is based on the misclassification cost rate proposed in [37]. We associate a larger cost with the malignant class, because it is both the more important class and the minority class. To achieve this, a local sequential search is performed at each node [38]. Our classifier uses the cost-matrix structure presented in Table 1
                        , where C
                        
                           minority
                         represents the cost penalty associated with misclassifying a malignant sample.

For a given object x, each of N classifiers 
                           
                              Ψ
                              
                                 
                                    1
                                 
                              
                           
                           ,
                              
                           
                              Ψ
                              
                                 
                                    2
                                 
                              
                           
                           ,
                              
                           .
                           .
                           .
                           ,
                              
                           
                              Ψ
                              
                                 
                                    N
                                 
                              
                           
                         makes a decision regarding class 
                           i
                           ∈
                           M
                           =
                           
                              
                                 
                                    1
                                    ,
                                       
                                    .
                                    .
                                    .
                                    ,
                                    M
                                 
                              
                           
                        . The combined classifier 
                           
                              Ψ
                              ¯
                           
                         then makes a decision according to a weighted voting rule
                           
                              (13)
                              
                                 
                                    Ψ
                                    ¯
                                 
                                 
                                    
                                       
                                          
                                             Ψ
                                             
                                                
                                                   1
                                                
                                             
                                          
                                          (
                                          x
                                          )
                                          ,
                                             
                                          
                                             Ψ
                                             
                                                
                                                   2
                                                
                                             
                                          
                                          (
                                          x
                                          )
                                          ,
                                             
                                          .
                                          .
                                          .
                                          ,
                                             
                                          
                                             Ψ
                                             
                                                
                                                   N
                                                
                                             
                                          
                                          (
                                          x
                                          )
                                       
                                    
                                 
                                 =
                                 arg
                                 
                                    max
                                    
                                       j
                                       ∈
                                       M
                                    
                                 
                                 
                                    ∑
                                    
                                       l
                                       =
                                       1
                                    
                                    N
                                 
                                 δ
                                 
                                    
                                       
                                          j
                                          ,
                                             
                                          
                                             Ψ
                                             
                                                
                                                   l
                                                
                                             
                                          
                                          (
                                          x
                                          )
                                       
                                    
                                 
                                 
                                    w
                                    
                                       
                                          l
                                       
                                    
                                 
                                 
                                    Ψ
                                    
                                       
                                          l
                                       
                                    
                                 
                                 (
                                 x
                                 )
                                 ,
                              
                           
                        where
                           
                              (14)
                              
                                 δ
                                 
                                    
                                       
                                          j
                                          ,
                                          i
                                       
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      0
                                                   
                                                   
                                                      if
                                                   
                                                   
                                                      i
                                                      ≠
                                                      j
                                                   
                                                
                                                
                                                   
                                                      1
                                                   
                                                   
                                                      if
                                                   
                                                   
                                                      i
                                                      =
                                                      j
                                                   
                                                
                                             
                                          
                                          ,
                                       
                                    
                                 
                              
                           
                        and 
                           
                              w
                              
                                 
                                    l
                                 
                              
                           
                         is the weight assigned to the l-th classifier. The weights used in Eq. (13) play a key-role in establishing the quality of 
                           
                              Ψ
                              ¯
                           
                         
                        [39]. The proposed ensemble fusion is a weighted voting approach that has demonstrated superior performance compared to canonical voting methods [40].

To train an effective MCS, we employ a genetic algorithm (GA). An individual in the GA population represents a classifier ensemble
                           
                              (15)
                              
                                 Ch
                                 =
                                 [
                                 A
                                 ,
                                 W
                                 ]
                                 .
                              
                           
                        
                        A represents the D features used by the individual classifiers in the pool
                           
                              (16)
                              
                                 A
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      a
                                                      
                                                         (
                                                         1
                                                         )
                                                      
                                                   
                                                   (
                                                   
                                                      x
                                                      
                                                         (
                                                         1
                                                         )
                                                      
                                                   
                                                   )
                                                
                                                
                                                   ⋯
                                                
                                                
                                                   
                                                      a
                                                      
                                                         (
                                                         1
                                                         )
                                                      
                                                   
                                                   (
                                                   
                                                      x
                                                      
                                                         (
                                                         D
                                                         )
                                                      
                                                   
                                                   )
                                                
                                             
                                             
                                                
                                                   ⋮
                                                
                                                
                                                   ⋱
                                                
                                                
                                                   ⋮
                                                
                                             
                                             
                                                
                                                   
                                                      a
                                                      
                                                         (
                                                         N
                                                         )
                                                      
                                                   
                                                   (
                                                   
                                                      x
                                                      
                                                         (
                                                         1
                                                         )
                                                      
                                                   
                                                   )
                                                
                                                
                                                   ⋯
                                                
                                                
                                                   
                                                      a
                                                      
                                                         (
                                                         N
                                                         )
                                                      
                                                   
                                                   (
                                                   
                                                      x
                                                      
                                                         (
                                                         D
                                                         )
                                                      
                                                   
                                                   )
                                                   ,
                                                
                                             
                                          
                                       
                                    
                                 
                                 ,
                              
                           
                        where a
                        (l)(x
                        (q))=1 if the q-th feature is used by the l-th individual classifier used by Ψ
                           h
                        (x), otherwise a
                        (l)(x
                        (q))=0. This enables the direct embedding of the feature selection step into our proposed ensemble training procedure. To ensure that the starting pool of classifiers contains heterogeneous rather than homogeneous predictors, we initialise A using the random subspace approach [41].


                        W represents the weights assigned to each of the base classifiers in the form
                           
                              (17)
                              
                                 W
                                 =
                                 [
                                 
                                    W
                                    1
                                 
                                 ,
                                 
                                    W
                                    2
                                 
                                 ,
                                 .
                                 .
                                 .
                                 ,
                                 
                                    W
                                    N
                                 
                                 ]
                                 ,
                              
                           
                        and is a real-valued vector with values in [0;1]. The initial weights are calculated according to
                           
                              (18)
                              
                                 
                                    w
                                    
                                       
                                          l
                                       
                                    
                                 
                                 ∝
                                 log
                                 
                                    
                                       
                                          p
                                          
                                             
                                                l
                                             
                                          
                                       
                                    
                                    
                                       1
                                       −
                                       
                                          p
                                          
                                             
                                                l
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              w
                              
                                 (
                                 l
                                 )
                              
                           
                         is the weight assigned to the l-th classifier according to its individual accuracy 
                           
                              p
                              
                                 
                                    l
                                 
                              
                           
                        .

Our GA chromosome thus consists of two separate parts: one binary-encoded (representing selected features) and one real-encoded (representing weights assigned to classifiers in the pool). Given that these two constituents are innately different, we prevent any information exchange between them. For the real-number encoding, we use the efficient scheme given in [42].

To reduce the computational complexity of the developed ensemble, we prune it to remove non-contributing classifiers. As classifiers with very low weights have little impact on the final output, we discard those classifiers from the pool having weights lower than a threshold α.

The pseudocode of our proposed algorithm is given in Algorithm 1.


                        
                           Algorithm 1
                           Hybrid cost-sensitive ensemble algorithm.
                                 
                                    
                                       Input:
                                    


                                       U → number of classifiers in the ensemble


                                       C → cost-matrix


                                       α → threshold for ensemble pruning


                                       Output:
                                    


                                       Q → final ensemble


                                       A → set of features used by classifiers in the pool


                                       W → set of weights assigned to classifiers


                                       P → ensemble error


                                       B → best solution


                                       P
                                       =1.0


                                       B
                                       = empty

Create initial population

Initialise feature components

Initialise weight components

Train initial pool of classifiers on features from A with respect to C
                                    

Prune ensemble with respect to α
                                    

Select individuals for evaluation


                                       for all selected individuals do
                                    

  Evaluate individual's fitness functions with weights from W
                                    

  
                                       if fitness <P 
                                       then
                                    


                                       replace P
                                    


                                       replace B
                                    

  
                                       end if
                                    


                                       end for
                                    


                                       while termination conditions not satisfied do
                                    

  Select pairs for crossover from best-ranked individuals

  Apply crossover operator

  Apply mutation operator

  Select new individuals

  Train classifiers on features from A with respect to C
                                    

  Assign weights from W to classifiers

  Prune ensemble with respect to α
                                    

  Perform weighted voting fusion

  
                                       for all selected individuals do
                                    


                                       Evaluate new individual's fitness functions with weights from W
                                    


                                       
                                       if fitness <P 
                                       then
                                    


                                         replace P
                                    


                                         replace B
                                    


                                       
                                       end if
                                    

  
                                       end for
                                    

  Create new population


                                       end while
                                    

Return Q

The control parameters of the GA are as follows:
                           
                              •
                              
                                 N
                                 
                                    c
                                  - the upper limit of algorithm cycles,


                                 N
                                 
                                    p
                                  - the population quantity,


                                 β - the mutation probability,


                                 γ - the crossover probability,

Δ
                                    m
                                  - the mutation range factor,


                                 V - the upper limit of algorithm iterations without quality improvement.

In the following, we detail each of the steps of the algorithm:


                        Population generation: Individuals forming an initial population are generated such that all constraints and implications resulting from the logic of the model as well as the values of the input parameters are preserved.


                        Population assessment: At the beginning, for each individual in the population, a value of the fitness function is calculated.


                        Choosing elite members: Members with the highest fitness values are carried over to the descendant population without any modification in their structure. In our implementation, we use a single-member elite.


                        Mutation: The mutation operator applies random changes to the chromosome to maintain diversity among the population. For the binary-encoded part of the chromosome, we use a random bit-value change, according to the given probability. For the real-encoded part, mutation involves adding a vector of numbers randomly generated according to a normal density distribution (with a mean of 0 and a standard deviation of Δ
                           m
                        ).


                        Crossover: The crossover operator generates one offspring member from two parents. For the binary-encoded part of our chromosome, offspring are obtained according to the two-point rule. For the real-encoded part, we apply uniform arithmetic crossover.


                        Selection of new population: A selection of individuals from the population is formed by merging the descendant population and a set of individuals. A tournament selection scheme is applied whereby the probability P
                        
                           s
                         of selecting a particular individual is proportional to its fitness value.

Clearly, our GA training algorithm requires an appropriate fitness function to return a high quality classifier ensemble. For this, we propose the use of a multi-criterion optimisation derived as a linear combination of the misclassification cost and a diversity measure. That is,
                           
                              (19)
                              
                                 minimise
                                 
                                 g
                                 (
                                 
                                    Π
                                    l
                                 
                                 )
                                 =
                                 (
                                 COST
                                 (
                                 
                                    Π
                                    l
                                 
                                 )
                                 +
                                 DIV
                                 (
                                 
                                    Π
                                    l
                                 
                                 )
                                 )
                                 ,
                              
                           
                        where Π
                           l
                         is the given pool of classifiers, COST(Π
                           l
                        ) denotes the overall misclassification penalty for a given ensemble normalised to the interval [0;1], and DIV(Π
                           l
                        ) is the diversity of the considered ensemble.

In our case, we have more than a single classifier at our disposal. Different predictors typically display different properties, have different areas of competence, and may provide different contributions to the committee. Careful classifier selection is crucial in choosing the most valuable individual models. There are several ways to perform such an ensemble pruning process. One of the most popular criteria is ensemble diversity [43], which aims at choosing predictors that are as different from each other as possible. This is motivated by the fact that adding similar classifiers to the committee increases its complexity without improving its quality. These diverse models are expected to be mutually complementary and to exploit different areas of competence [43]. Evolutionary algorithms are a promising tool for enforcing diversity in ensembles because of their ability to efficiently check various solutions and find several feasible possibilities (e.g., by using different initialisations for the search procedure) [44].

To measure the diversity of the ensemble, we use a pairwise double-fault diversity measure [45] based on the idea that it is more important to know when simultaneous misclassifications occur than when both classifiers are correct. This is also well aligned with the problem of imbalanced classification where the main priority is to minimise the number of misclassifications of the minority class.

Given two base classifiers h
                        
                           i
                         and h
                        
                           j
                        , let n(a, b) denote the number of training samples for which the output of these classifiers is a and b, respectively. The double-fault diversity measure can then be calculated as
                           
                              (20)
                              
                                 DIV
                                 (
                                 
                                    h
                                    i
                                 
                                 ,
                                 
                                    h
                                    j
                                 
                                 )
                                 =
                                 
                                    
                                       n
                                       (
                                       −
                                       1
                                       ,
                                       −
                                       1
                                       )
                                    
                                    
                                       n
                                       (
                                       1
                                       ,
                                       1
                                       )
                                       +
                                       n
                                       (
                                       −
                                       1
                                       ,
                                       1
                                       )
                                       +
                                       n
                                       (
                                       1
                                       ,
                                       −
                                       1
                                       )
                                       +
                                       n
                                       (
                                       −
                                       1
                                       ,
                                       1
                                       )
                                    
                                 
                                 .
                              
                           
                        
                     

Diversity for an ensemble of L base classifiers is then obtained by averaging the measure over all classifier pairs in the ensemble
                           
                              (21)
                              
                                 DIV
                                 (
                                 Ψ
                                 )
                                 =
                                 
                                    2
                                    
                                       NL
                                       (
                                       L
                                       −
                                       1
                                       )
                                    
                                 
                                 
                                    ∑
                                    
                                       j
                                       =
                                       1
                                    
                                    L
                                 
                                 
                                    ∑
                                    
                                       k
                                       =
                                       j
                                       +
                                       1
                                    
                                    L
                                 
                                 
                                    n
                                    
                                       j
                                       ,
                                       k
                                    
                                 
                                 (
                                 −
                                 1
                                 ,
                                 −
                                 1
                                 )
                                 ,
                              
                           
                        where N is the number of training samples. The established diversity measure is in the interval [0, 1], where 1 corresponds to a set of identical classifiers and 0 to the highest possible diversity, respectively.

Applying the proposed combined criterion is expected to ensure that our ensemble simultaneously provides good performance on the malignant/minority class (by minimising the misclassification cost) and consists of heterogeneous, complementary individual classifiers (by enforcing diversity).

For our experiments, we used a dataset of 146 thermograms of which 29 cases had been confirmed as malignant and the remaining 117 cases were benign. This is the same dataset that was used in earlier work [24,21], and is significantly larger than those used in other studies (e.g. [19]). For all thermograms, the 38 features from Section 2 were extracted.

Our experiments were designed with respect to the following objectives:
                        
                           •
                           To examine the influence of the cost matrix settings on the overall classification accuracy and the recognition rate of the minority class. For this purpose, we apply ROC analysis [46] with cut-off point being the misclassification cost of the malignant class denoted as C
                              
                                 minority
                               in Table 1. This approach allows us to search for correlations between the cost parameter and the overall classification accuracy.

To find an optimal ensemble size for the considered problem.

To investigate the maximum value of the ensemble pruning threshold α such that the complexity of the ensemble is reduced without compromising its accuracy.

To compare our proposed algorithm with several state-of-the-art ensemble methods dedicated to imbalanced classification.

The parameters for the optimization procedures were selected based on a grid-search procedure. The detailed ranges of tested values and best selected parameters are given in Table 2
                        . The initial range values were selected on the basis of our previous experience with optimisation-based ensembles [47]. In the remaining part of this paper, we only present results for the best obtained settings.


                        A was initialised using the random subspace approach, consisting of 40% of the original feature space.

For comparison, we implemented a single C4.5 decision tree, a single cost-sensitive tree (using the same cost matrix as derived for the committee), and several state-of-the-art ensemble methods for imbalanced classification, namely SMOTEBagging [29], SMOTEBoost [32], IIvotes [33], EasyEnsemble [35], and AdaCost [34], all with C4.5 decision trees as base classifiers. We used the standard settings recommended in [48] for the best performance results. In addition, we compared our ensemble with its simpler versions, which utilise only a single criterion in the fitness function (i.e., either cost or diversity).

For a statistical pairwise comparison, we used a 5x2 combined cross-validation (CV) F-test [49], which repeats two-fold cross-validation five times such that the size of the training and testing sets is equal in each of the folds. This test is conducted based on all-versus-all comparison. We use the probability of rejecting the null hypothesis as the test score (i.e., assume by default that classifiers have the same error rates). The alternative hypothesis is that the classifiers have different error rates. A small difference in the error rate implies that the different algorithms construct two similar classifiers with similar error rates; thus, the hypothesis should not be rejected. For a large difference, the classifiers have different error rates and the hypothesis should be rejected.

A classifier is assumed as statistically significantly better compared to another one if one or both of the following is true:
                           
                              •
                              its sensitivity is statistically significantly better and its overall accuracy is not statistically significantly worse;

its overall accuracy is statistically significantly better and its sensitivity is not statistically significantly worse.

@&#EXPERIMENTAL RESULTS@&#

In this subsection, we present the detailed results of our experiments; ranging from a discussion of the parameter-tuning to a detailed comparison with state-of-the-art reference methods.

In our first experiment, we searched for the optimal settings for our approach, starting with the C
                           
                              minority
                            parameter from Table 1. This involved testing values within the interval [5;100] with a step-size of five and plotting the resulting ROC curve (averaged over all CV folds) in Fig. 3
                           . The optimal cut-off parameter is then determined by measuring the distance from points on the ROC curve to the point representing optimal performance (i.e., 100% sensitivity and specificity - the top left corner of Fig. 3). The point with minimal distance is then selected as the optimal setting for the cost matrix, which we determined as C
                           
                              minority
                           
                           =60.

Next, we examined the correlation between the number of classifiers in the committee and the achieved sensitivity. Each member of the committee was trained using the cost matrix derived using the above procedure. The results for ensembles comprising 2 to 50 classifiers, averaged over all CV folds, are shown in Fig. 4
                           . The best initial size of the ensemble is thus 20 classifiers.

Finally, we examined the behaviour of the pruning threshold α which is the cut-off point for discarding classifiers with weights below its value to reduce the complexity of the ensemble. Results (again averaged over all CV folds) are presented in Fig. 5
                           , which provides plots of sensitivity and ensemble size as a function of the threshold. From this, we determine that α
                           =0.1 did not decrease the sensitivity of the ensemble, while on average it led to the elimination of 6 classifiers from the pool, thus yielding a hybrid ensemble of 14 classifiers on average.

Following the identification of suitable parameters for cost, ensemble size, and pruning threshold, we compared our HCSE against the other classifiers which we implemented. The results of this experimental comparison are given in Table 3
                            which lists sensitivity (i.e., the probability that a case identified as malignant is indeed malignant), specificity (i.e., the probability that a case identified as benign is indeed benign), and overall classification accuracy (i.e., the percentage of correctly classified patterns) for each approach. In addition, we provide the results of the statistical significance test in Table 4
                           .

@&#DISCUSSION@&#

From the results, we can see that a canonical C4.5 classifier performed rather poorly on the breast thermogram data, resulting in a very low sensitivity. This confirms that considering the class imbalance is crucial to achieving reasonable classification performance.

When applying a cost-sensitive decision tree, we can indeed observe a significant increase in terms of recognition performance compared to the canonical C4.5 classifier. The tested ensemble methods further improved the results, statistically outperforming a single cost-sensitive classifier. This confirms their usefulness for imbalanced datasets.

Our proposed hybrid cost-sensitive ensemble provides superior performance to all other comparison algorithms. While it did not provide the best overall accuracy, it statistically outperformed all other approaches, providing significantly better sensitivity compared to all other ensembles while maintaining a similar overall classification accuracy.

The comparison with simplified versions of our HCSE demonstrates that superior performance can only be achieved through the integration of both cost and diversity measures. The use of only the cost criterion led to an ensemble with excellent sensitivity but rather low specificity. The nature of the learning algorithm promotes only the combinations of features and weights leading to a high specificity. Without assuring diversity, we obtained a fairly homogeneous ensemble that was not sufficiently competent to cope with the entire recognition process. In contrast, employing only the diversity criterion led to a heterogeneous ensemble at the cost of inadequate sensitivity. In this case, the learning algorithm promoted the most diverse predictors without considering their impact on the classification process. Clearly, both drawbacks were eliminated by applying the combined criterion, leading to a heterogeneous cost-sensitive ensemble.

The proposed ensemble worked best when coupled with a relatively high misclassification cost of the malignant/minority class. Lower values failed to provide a satisfactory level of sensitivity, while higher ones led to a classifier overfitting towards the malignant class. Ensemble size also plays a crucial role in the quality of the algorithm. The optimal number of individual classifiers oscillated around 20, signifying a medium sized committee. A smaller pool of predictors failed to capture all of the properties of the data, while larger ensembles only increased the computational complexity of the recognition model without providing improved performance.

The presented approach also clearly outperforms previous approaches in the literature of breast thermogram analysis. In [21], the same features and dataset were employed together with a cost-sensitive fuzzy if-then rule based classifier which was optimised by a genetic algorithm. The best 10CV results reported for this approach were a sensitivity of 79.86% with a specificity of 79.49%. Similar results, on the same dataset and using the same features, were achieved in [24] based on a (single) neural network classifier coupled with a feature selection stage, while an overall classification accuracy of 79.52% was achieved using an ant colony optimisation classifier in [50]. Thus, our proposed method results in markedly higher performance than all of these existing approaches.

@&#CONCLUSIONS@&#

In this paper, we have proposed a hybrid cost-sensitive classifier ensemble for analysing breast thermogram features in the context of cancer diagnosis. We extract a set of image features describing bilateral asymmetry between the two breast areas. These features then form the basis of a classification task for which we employ our dedicated ensemble. We utilise cost-sensitive decision trees as base classifiers. These classifiers are combined using an evolutionary algorithm that simultaneously performs feature selection and classifier weight assignment. Ensembles are pruned to remove non-contributing classifiers and to reduce the computational complexity. An optimisation criterion considering both misclassification costs and ensemble diversity is shown to be crucial for successful classification. The resulting classifier ensemble was demonstrated to provide excellent sensitivity (83.10%) while maintaining good overall classification accuracy (88.17%), statistically outperforming several state-of-the-art classifier committees dedicated to imbalanced classification.

@&#ACKNOWLEDGMENTS@&#

Bartosz Krawczyk and MichałWoźniak were supported by the Polish National Science Center under grant no. DEC-2013/09/B/ST6/02264, as well as statutory funds of Department of Systems and Computer Networks, Wrocław University of Technology, Poland.

@&#REFERENCES@&#

