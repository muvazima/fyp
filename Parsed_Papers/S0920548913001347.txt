@&#MAIN-TITLE@&#RankCNN: When learning to rank encounters the pseudo preference feedback

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Our approach integrates deep learning with pseudo preference feedback to rerank the initial list.


                        
                        
                           
                           The optimal set of pseudo preference pairs is detected by a modified graph-based method.


                        
                        
                           
                           Ranking is then reduced to unsupervised pairwise classification in the architecture of CNN.


                        
                        
                           
                           Accelerated Mini-Batch Stochastic Dual Coordinate Ascent (ASDCA) is introduced to the framework to accelerate the training.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Pseudo preference feedback

Convolutional neural networks

Learning to rank

RankCNN

@&#ABSTRACT@&#


               
               
                  Learning to rank has received great attentions in the field of text retrieval for several years. However, a few researchers introduce the topic into visual reranking due to the special nature of image presentation. In this paper, a novel unsupervised visual reranking is proposed, termed rank via the convolutional neural networks (RankCNN). This approach integrates deep learning with pseudo preference feedback. The optimal set of pseudo preference pairs is first detected from initial list by a modified graph-based method. Ranking is then reduced to pairwise classification in the architecture of CNN. In addition, Accelerated Mini-Batch Stochastic Dual Coordinate Ascent (ASDCA) is introduced to the framework to accelerate the training. The experiments indicate the competitive performance on the LETOR 4.0, the Paris and the Francelandmark dataset.
               
            

@&#INTRODUCTION@&#

The content-based visual retrieval [1] has been extensively investigated and applied in a number of applications. While successful, the traditional content-based search sometimes fails with some irrelevant retrievals (Fig. 1
                     ). As a result, many methods are proposed to rerank the initial results, known as learning to rank. Learning to rank has been applied to document retrieval, collaborative filtering, expert finding, sentiment analysis, and product rating [2]. In conducting learning to rank, the major methods fall into three categories: (1) the pointwise approach, (2) the pairwise approach, and (3) the listwise approach. In the pointwise approaches, each training instance is associated with a rating. The learning is to find a model that can map instances into ratings that are close to their true ones. The listwise approaches use a list of ranked objects as training instances and learn to predict the list of objects. The pairwise approaches take document pairs as instances in learning, and formalize the problem of learning to rank as that of classification. Specifically, document pairs are collected from the ranking lists, and each pair is assigned a label representing the relative relevance of the two documents. A classification model is then developed based on the labeled data to rerank the list. Based on the Support Vector Machines (SVM), Boosting, and Neural Network classifiers, some known methods are developed such as RankSVM [3], RankBoost [4], and RankNet [5]. Considering the most existing methods are pairwise approaches, we choose the pairwise approaches as the basic conception in this paper.

It is interesting to note “learn to rank” has attracted great attention in text retrieval not for the visual ranking. This may be due to the major difference between the document representation and the image representation. For example, the dimension of visual feature is generally much greater than that of document feature; each dimension of document feature has real physical meaning but visual feature does not. Let image be presented as a (Bag of the Word) BoW vector, it will be up to million dimensions which truly challenges the classifiers such as SVM, Boosting or Neural Network.

Fortunately, convolutional neural networks (CNN) [5], one of variants of multilayer perception (MLP), have been proposed to handle the natural image classification problem effectively. According to [7], the current best error rate of CNN on the MNIST digit recognition task is less than 0.3% which is comparable to human performance [8]. CNN thus has the promise to be applied in the visual reranking if it is offered the correct preference pairs. During the reranking process, there is a need to detect the maximum likelihood preference pairs. A method proposed by [9], termed pseudo preference feedback (PPF), is demonstrated to be successful to automatically discover an optimal set of pseudo preference pairs. This method could potentially integrate with CNN as an unsupervised learning pipeline to handle visual reranking problem. While promising, one challenge in the field is the computation expenses. It is known that the efficient gradient descent method is able to accelerate the convergence, and obtain better local minimum. Yet, implementing the method in the parallel computing system is less touched.

In this paper, we propose an unsupervised learning-based visual reranking. Before training, a graph-based method [10] is improved to detect the pseudo preference pairs. During the learning, a simple probabilistic cost function is proposed, which reduces the ranking to the pairwise classification. This approach called RankCNN, motivated by RankNet, is implemented in the architecture of Convolutional Neural Networks to model the underlying ranking function. Fig. 2
                      outlines the overall framework for visual reranking.

The rest of the paper is organized as follows. Section 2 introduces related work. The detection of pseudo preference pairs is introduced in Section 3 and the RankCNN model is described in Section 4. Section 5 reports our experimental results followed by conclusion in Section 6.

@&#RELATED WORK@&#

In this section, several existing methods are presented. These methods were chosen due to their close relation with our approach. The spirit of RankNet is the key of our reranking method, and the convolutional neural networks are the basic architecture. Pseudo preference feedback and gradient descent are the focus for optimization. These methods will be discussed as follows.

In the RankNet, the ranking problem is transformed into the classification of two categories, namely, high-rank and low-rank. This method is simple to train and gives good performance on a real world ranking problem with large amounts of data. It is known that RankNet outperforms RankSVM and RankBoost for large dimension dataset [5]. In the RankNet, the cross entropy is employed as loss function as follows:
                           
                              
                                 
                                    
                                       C
                                       ij
                                    
                                    =
                                    −
                                    
                                       
                                          P
                                          ij
                                       
                                       ¯
                                    
                                    
                                    log
                                    
                                    
                                       P
                                       ij
                                    
                                    −
                                    
                                       
                                          1
                                          −
                                          
                                             P
                                             ij
                                          
                                       
                                    
                                    log
                                    
                                       
                                          1
                                          −
                                          
                                             
                                                P
                                                ij
                                             
                                             ¯
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 P
                                 ij
                              
                              ¯
                           
                         is the desired target value for the posteriors probability that x
                        
                           i
                         ranks higher than x
                        
                           j
                        , and P
                        
                           ij
                         is the modeled posterior probability. Several existing methods could be used to solve the minimization problem such as gradient descent. While successful, to the best of our knowledge, there is little exploration on applying RankNet for visual ranking.

The convolutional neural networks (CNNs) have strong capability to present the image because of its controlled depth and breadth, and they could make strong and mostly correct assumptions about the nature of images [6]. Compared to standard feedforward neural network with similarly-sized layers (such as DBN), CNNs have fewer connections and parameters thus they are easier to train, though their theoretically-best performance may be slightly worse [11]. However, the CNNs still suffer from computation expense which limits its application to high-resolution images. Fortunately, current GPUs, paired with a highly-optimized implementation of 2D convolution, are powerful enough to facilitate the training of interestingly-large CNNs. The most recent literature indicates that the CNNs have been successfully applied in some large datasets such as ImageNet, which contain 1.2million high-resolution images [11].

Labeled data is precondition for training. There are several methods proposed to detect an optimal set of pseudo preference pairs as the labeled data. [10] presents a learning-based approach to video search reranking by investigating the ranking order information. This is called pseudo preference feedback (PPF) as it does not rely on any user interaction. In this approach, considering that even relevant search results often have distinct appearances, all the samples in the initial ranked list are clustered into several categories in terms of visual appearance. These categories indicate the level of relevance. Thus the idea of PPF is to learn the preference relation in each category. In [10], the pairs of training samples include one pseudo-positive and one pseudo-negative sample (PP–PN), or two pseudo-positive samples (PP–PP). The Ranking Support Vector Machines (Ranking SVM) is used to learn a reranking model based on the selected pairs. The PPF-based method is fully automatic, without any auxiliary knowledge. It has been proven that this approach can effectively improve the performance of initial search result.

Gradient descent is the most common method on solving minimization problem as follows.
                           
                              
                                 
                                    P
                                    
                                       w
                                    
                                    =
                                    
                                       1
                                       n
                                    
                                    
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          n
                                       
                                       
                                          
                                             Φ
                                             i
                                          
                                          
                                             w
                                          
                                          +
                                          g
                                          
                                             w
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Where ridge regression is obtained by setting 
                           
                              g
                              
                                 w
                              
                              =
                              
                                 λ
                                 2
                              
                              
                                 
                                    w
                                 
                                 2
                              
                           
                         and ϕ
                        
                           i
                        (w)=(w
                        
                           T
                        
                        u
                        
                           i
                        
                        −
                        y
                        
                           i
                        )2. At the same time, the closely related methods are proposed such as stochastic gradient descent (SGD) [12–18], which has become popular for solving large scale supervised machine learning optimization problems such as SVM, due to their strong theoretical guarantees. Dual Coordinate Ascent (DCA) [19–21] solves the dual problem of the above equation. Specifically, for each i let ϕ
                        
                           i
                        
                        ⁎
                        :
                        R
                        →
                        R be convex conjugate of ϕ
                        
                           i
                        , namely, ϕ
                        
                           i
                        
                        ⁎
                        =
                        max
                        
                           z
                        (z
                        
                           T
                        
                        u
                        ‐
                        ϕ
                        
                           i
                        (z)). The dual problem is max
                        
                           α
                        
                        D(α)
                           
                              
                                 
                                    where
                                    
                                    D
                                    
                                       α
                                    
                                    =
                                    
                                       1
                                       n
                                    
                                    
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          n
                                       
                                       
                                          −
                                          
                                             Φ
                                             i
                                             *
                                          
                                          
                                             
                                                −
                                                
                                                   α
                                                   i
                                                
                                             
                                          
                                          −
                                          
                                             g
                                             *
                                          
                                          
                                             
                                                
                                                   1
                                                   n
                                                
                                                
                                                   
                                                      ∑
                                                      
                                                         i
                                                         =
                                                         1
                                                      
                                                      n
                                                   
                                                   
                                                      
                                                         α
                                                         i
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

Based on Dual Coordinate Ascent (DCA), a new technique called Stochastic Dual Coordinate Ascent (SDCA) [22] is presented to perform more efficient than SGD while keeping the strong theoretical guarantees. [23] considers an extension of SDCA under the minibatch setting that is often used in practice. This method not only provides a fast convergence rate for solving regularized loss minimization problems in machine, but also could be easily implemented over a parallel computing system.

In this research, we propose a novel unsupervised visual reranking method, termed rank via the convolutional neural networks (RankCNN). We integrate deep learning with pseudo preference feedback (See Section 3) into the initial ranking. CNN is then applied for the reranking. Accelerated Mini-Batch Stochastic Dual Coordinate Ascent is applied to accelerate the training for computationally affordable process (See Section 4).

In the framework, the pseudo preference pairs are detected using graph-based method instead of cluster-based. The pseudo preference pair includes query and a pseudo-negative sample (Q–PN), or query and a pseudo-positive sample (Q–PP). Let us first consider the detection of pseudo-positive sample.

Given that the perfect pseudo-positive samples are relevant to the query image, the detected pseudo-positive samples should be as precise as possible. The architecture of the previous work [9] is slightly modified to detect the pseudo-positive samples. Before detection, the database is translated into a weighted undirected graph G
                     =(W,V,E). It is reasonable to assume that the images containing the same view of the object being connected, thus the reciprocal neighbor relation [24,25] are used to weight the edge between images as follows:
                        
                           (1)
                           
                              
                                 w
                                 
                                    
                                       i
                                       ,
                                       i
                                       '
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      N
                                                      k
                                                   
                                                   
                                                      i
                                                   
                                                   ∩
                                                   
                                                      N
                                                      k
                                                   
                                                   
                                                      
                                                         i
                                                         '
                                                      
                                                   
                                                
                                                k
                                             
                                          
                                          
                                             if
                                             
                                             
                                                
                                                   i
                                                   ,
                                                   i
                                                   '
                                                
                                             
                                             ∈
                                             
                                                R
                                                k
                                             
                                             
                                                
                                                   i
                                                   ,
                                                   i
                                                   '
                                                
                                             
                                          
                                       
                                       
                                          
                                             0
                                          
                                          
                                             else
                                          
                                       
                                    
                                 
                              
                           
                        
                     where N
                     
                        k
                     (i) is the set of the k nearest neighbors of image i, and R
                     
                        k
                     (i, i') is described as:
                        
                           
                              
                                 
                                    R
                                    k
                                 
                                 
                                    
                                       i
                                       ,
                                       i
                                       '
                                    
                                 
                                 □
                                 i
                                 '
                                 ∈
                                 
                                    N
                                    k
                                 
                                 
                                    i
                                 
                                 ∧
                                 i
                                 ∈
                                 
                                    N
                                    k
                                 
                                 
                                    
                                       i
                                       '
                                    
                                 
                                 .
                              
                           
                        
                     
                  

The object is to search a subgraph G' from G maintaining the maximum density in Eq. (2). Since there exists the direct or indirect reciprocal neighbor relation between query and other images in subgraph G', we conclude that these images are pseudo-positive samples.
                        
                           (2)
                           
                              
                                 G
                                 '
                                 =
                                 
                                    
                                       argmax
                                       
                                          G
                                          '
                                          =
                                          
                                             
                                                W
                                                ,
                                                V
                                                '
                                                ,
                                                E
                                                '
                                             
                                          
                                          ⊂
                                          G
                                          ,
                                          q
                                          ∈
                                          V
                                          '
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             ∑
                                             
                                                
                                                   i
                                                   ,
                                                   i
                                                   '
                                                
                                             
                                          
                                          
                                             w
                                             
                                                
                                                   i
                                                   ,
                                                   i
                                                   '
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          V
                                          '
                                       
                                    
                                 
                                 .
                              
                           
                        
                     
                  

The subgraph starts with query and other pseudo-positive samples are inserted into the subgraph successively. An approximate solution is adopted to solve the Eq. (2) as Algorithm 1.
                        Algorithm 1
                        Pseudo-positive samples detection


                           
                              
                           
                        

In experiment, the images from the complementary set of subgraph G' are sampled randomly as pseudo-negative samples.

Assumed that the query has the same rank as the pseudo-positive sample (which here and below is written as Q≈P), query would be higher than pseudo-negative sample (Q
                     >
                     P). Given the set of Q–PP and Q–PN, they can be described as P(Q
                     ≈
                     P) and P(Q
                     >
                     P). The training pairs need not to be complete, but all the pseudo preference pairs are supposed to maximize the above probability as far as possible.

We first consider the model f
                     :(R
                     
                        d
                     ,R
                     
                        d
                     )→
                     R
                     ∈(0,1). More concretely, the model can be indicated as follows:
                        
                           (3)
                           
                              
                                 f
                                 
                                    A
                                    B
                                 
                                 =
                                 
                                    
                                       
                                          1
                                       
                                       
                                          if
                                       
                                       
                                          A
                                          ∈
                                          Q
                                          ,
                                          B
                                          ∈
                                          PN
                                       
                                    
                                    
                                       
                                          0
                                       
                                       
                                          if
                                       
                                       
                                          A
                                          ∈
                                          Q
                                          ,
                                          B
                                          ∈
                                          PP
                                       
                                    
                                 
                                 .
                              
                           
                        
                     
                  

In this model, there exists an implicit function h: Rd
                     
                     →
                     R, which means that h(A)>
                     h(B) when A
                     >
                     B. In order to meet the above form, Eq. (3) is rewritten as follows:
                        
                           (4)
                           
                              
                                 f
                                 
                                    A
                                    B
                                 
                                 =
                                 
                                    
                                       exp
                                       
                                          
                                             h
                                             
                                                A
                                             
                                             ‐
                                             h
                                             
                                                B
                                             
                                          
                                       
                                    
                                    
                                       1
                                       +
                                       exp
                                       
                                          
                                             h
                                             
                                                A
                                             
                                             ‐
                                             h
                                             
                                                B
                                             
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        
                     
                  

In order to train the model maintaining maximal probability, the cost function is formulated as follows:
                        
                           (5)
                           
                              
                                 
                                    C
                                    q
                                 
                                 =
                                 α
                                 
                                    
                                       ∑
                                       
                                          d
                                          ∈
                                          PN
                                       
                                    
                                    
                                       
                                          
                                             
                                                f
                                                
                                                   q
                                                   d
                                                
                                                ‐
                                                1
                                             
                                          
                                          2
                                       
                                    
                                 
                                 +
                                 
                                    
                                       1
                                       ‐
                                       α
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                          
                                             d
                                             ∈
                                             PP
                                          
                                       
                                       
                                          
                                             
                                                f
                                                
                                                   q
                                                   d
                                                
                                             
                                          
                                       
                                    
                                    2
                                 
                              
                           
                        
                     where α is a coefficient weighting the pseudo preference pairs.

Considering that the convolutional neural networks (CNN) have been proven successfully in visual classification, the CNN could be integrated with RankNet as the presentation of the function f. Similar to CNN, the system starts with image pixels as input, followed by three alternative convolutional layers and max pooling layers. The difference is that the input is an image pair and the last layer is only one neuron which serves as the output of C
                     
                        q
                     .

In our experiments, the image is transformed into grayscale image size of 128×128. The first layer (Conv1) has 32 kernel size of 5×5. The next layer (Pool1) has pooling size of 3×3. The third layer (Conv2) has 32 kernel size of 5×5. The fourth layer (Pool2) has pooling size of 3×3. The fifth layer (Conv3) has 64 kernel size of 5×5. The sixth layer (Pool3) has pooling size of 3×3. The architecture of the RankCNN is shown in Fig. 3
                     .

According to Eq. (5), cost function is convex function over g and h. In addition, the CNN reminds us of the existing methodologies on classification which can be directly applied, such as back propagation. In general, the cost function can be solved by using gradient descent method. Denote all the parameters in the model by θ
                     ={w
                     
                        i
                     , b
                     
                        j
                     ; i
                     =1 ⋯ m, j
                     =1 ⋯ n}, we take the derivatives of cost function with respect to the parameters as:
                        
                           (6)
                           
                              
                                 
                                    
                                       ∂
                                       
                                          C
                                          q
                                       
                                    
                                    
                                       ∂
                                       θ
                                    
                                 
                                 =
                                 2
                                 α
                                 
                                    
                                       ∑
                                       
                                          d
                                          ∈
                                          PN
                                       
                                    
                                    
                                       
                                          
                                             f
                                             
                                                q
                                                d
                                             
                                             ‐
                                             1
                                          
                                       
                                    
                                 
                                 
                                    
                                       ∂
                                       f
                                       
                                          q
                                          d
                                       
                                    
                                    
                                       ∂
                                       θ
                                    
                                 
                                 +
                                 2
                                 
                                    
                                       1
                                       ‐
                                       α
                                    
                                 
                                 
                                    
                                       ∑
                                       
                                          d
                                          ∈
                                          PP
                                       
                                    
                                    
                                       f
                                       
                                          q
                                          d
                                       
                                    
                                 
                                 
                                    
                                       ∂
                                       f
                                       
                                          q
                                          d
                                       
                                    
                                    
                                       ∂
                                       θ
                                    
                                 
                              
                           
                        
                     
                     
                        
                           (7)
                           
                              
                                 
                                    
                                       ∂
                                       f
                                       
                                          q
                                          d
                                       
                                    
                                    
                                       ∂
                                       θ
                                    
                                 
                                 =
                                 
                                    
                                       exp
                                       
                                          
                                             h
                                             
                                                q
                                             
                                             ‐
                                             h
                                             
                                                d
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   ∂
                                                   h
                                                   
                                                      q
                                                   
                                                
                                                
                                                   ∂
                                                   θ
                                                
                                             
                                             ‐
                                             
                                                
                                                   ∂
                                                   h
                                                   
                                                      d
                                                   
                                                
                                                
                                                   ∂
                                                   θ
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             1
                                             +
                                             exp
                                             
                                                
                                                   h
                                                   
                                                      q
                                                   
                                                   ‐
                                                   h
                                                   
                                                      d
                                                   
                                                
                                             
                                          
                                       
                                       2
                                    
                                 
                                 .
                              
                           
                        
                     
                  

Because the derivatives of h are similar to back propagation, interested readers please refer to CNN for details [5]. In order to avoid trained model being biased toward queries with more document pairs, the pseudo positive samples are divided into several batches. At each iteration, one batch is selected randomly as pseudo positive input and the pseudo negative samples are sampled as 5 times as that of pseudo positive input. Pseudo code for training the RankCNN model is given in Algorithm 2.
                        Algorithm 2
                        Training the RankCNN


                           
                              
                           
                        

Though the above algorithm could be solved successfully by the Stochastic Gradient Descent (SGD), the promise of a recently proposed Accelerated Mini-Batch Stochastic Dual Coordinate Ascent (ASDCA) motivates us to adopt ASDCA into our framework. Specifically, our goal is to solve 
                        
                           mi
                           
                              n
                              
                                 x
                                 ∈
                                 
                                    R
                                    d
                                 
                              
                           
                           P
                           
                              θ
                           
                        
                      where
                        
                           (8)
                           
                              
                                 
                                    
                                       
                                          P
                                          
                                             θ
                                          
                                          =
                                          
                                             1
                                             N
                                          
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                N
                                             
                                             
                                                
                                                   ϕ
                                                   i
                                                
                                                
                                                   θ
                                                
                                             
                                          
                                          +
                                          
                                             1
                                             M
                                          
                                          
                                             
                                                ∑
                                                
                                                   j
                                                   =
                                                   1
                                                
                                                M
                                             
                                             
                                                
                                                   φ
                                                   j
                                                
                                                
                                                   θ
                                                
                                             
                                          
                                          +
                                          g
                                          
                                             θ
                                          
                                       
                                    
                                    
                                       
                                          
                                          
                                             ϕ
                                             i
                                          
                                          
                                             θ
                                          
                                          =
                                          
                                             
                                                
                                                   f
                                                   
                                                      q
                                                      
                                                         d
                                                         i
                                                      
                                                      θ
                                                   
                                                   ‐
                                                   1
                                                
                                             
                                             2
                                          
                                          
                                          
                                             d
                                             i
                                          
                                          ∈
                                          PN
                                       
                                    
                                    
                                       
                                          
                                          
                                             φ
                                             j
                                          
                                          
                                             θ
                                          
                                          =
                                          
                                             
                                                
                                                   f
                                                   
                                                      q
                                                      
                                                         d
                                                         j
                                                      
                                                      θ
                                                   
                                                
                                             
                                             2
                                          
                                          
                                          
                                             d
                                             j
                                          
                                          ∈
                                          PP
                                       
                                    
                                 
                                 .
                              
                           
                        
                     
                  

Similarly, the dual problem is to max
                     
                        α,β
                     
                     D(α,β)
                        
                           
                              
                                 where
                                 
                                 D
                                 
                                    α
                                    β
                                 
                                 =
                                 
                                    1
                                    N
                                 
                                 
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       N
                                    
                                    
                                       ‐
                                       
                                          Φ
                                          i
                                          *
                                       
                                       
                                          
                                             ‐
                                             
                                                α
                                                i
                                             
                                          
                                       
                                    
                                 
                                 +
                                 
                                    1
                                    M
                                 
                                 
                                    
                                       ∑
                                       
                                          j
                                          =
                                          1
                                       
                                       M
                                    
                                    
                                       ‐
                                       
                                          φ
                                          j
                                          *
                                       
                                       
                                          
                                             ‐
                                             
                                                β
                                                j
                                             
                                          
                                       
                                    
                                 
                                 ‐
                                 
                                    g
                                    *
                                 
                                 
                                    
                                       
                                          1
                                          N
                                       
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             N
                                          
                                          
                                             
                                                α
                                                i
                                             
                                          
                                       
                                       +
                                       
                                          1
                                          M
                                       
                                       
                                          
                                             ∑
                                             
                                                j
                                                =
                                                1
                                             
                                             M
                                          
                                          
                                             
                                                β
                                                j
                                             
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        
                     
                  


                     Algorithm 3 lists the pseudo code of the ASCDA on the proposed RankCNN model.
                        Algorithm 3
                        Accelerated training the RankCNN


                           
                              
                           
                        

After learning the model, the parameters of the RankCNN model are obtained. Each image in the initial list is taken as input of h, and then is ranked according to the output in descending order.

Before introducing the system in visual field, we first test Algorithm 2 in text retrieval. LETOR 4.0 dataset is chosen as the benchmark.

Following the same experimental setting as in [26], we evaluate the ability of visual reranking on the two databases — only the Paris database, and the Paris
                        +
                        Francelandmark. Francelandmark include some images crawled from Flickr, Bing and Google using queries of famous 78 France landmarks and 24 artworks.

The LETOR 4.0 
                        [27] contains 8 datasets for four ranking settings derived from the two query sets and the Gov2 web page collection. The 5-fold cross validation strategy is adopted and the 5-fold partitions are included in the package. In each fold, there are three subsets for learning: training set, validation set and testing set. The precision at top n (p@n) is taken as measurement.

The Paris 
                        [28] includes 6391 images collected from Flickr by searching for particular Paris landmarks. There are 55 images extracted from dataset as the query. The retrieval performance is measured by mAP(mean Average Precision). In addition, p@n is also taken as the measurement of ranking ability.

The Francelandmark contains 86,717 images in total, which get closer to the authentic application. The performance is evaluated by the precision at top n candidates. We choose two groups of queries to simulate the real conditions:
                           
                              •
                              Low Precision (LP): 25 queries where the precision at top 25 candidates is lower than 30%.

High Precision (HP): 25 queries where the precision at top 25 candidates is high than 70%.

In text retrieval, our algorithm is slightly modified. The documents with the largest relevance label are taken as queries. The pseudo positive set and pseudo negative set come from the documents with the different relevance label. Because our learning algorithm is based on pairwise model, we choose the RankSVM, RankNet, and Rankboost as the comparative test.

In visual retrieval, the initial result is generated as follows: the Harris Laplace and SIFT is taken as the detector and descriptor respectively. Next, 40M descriptors are selected randomly as training set of 1M codebook, which is generated using Approximate K-means (AKM). All images are represented by the BoW vector, and are ranked according to the negative Euclidean distance between these images and query in descending order. In order to evaluate the reranking performance, several existing reranking methods are compared as follows.

This method [29] assumes the high-rank images in the initial list to be relevant. The images are ranked according to their average similarities with the top candidates.

This method [10] reranks the images according to their time of insertion into subgraph while maximizing weighted density.

This method is introduced generally in the above sections. The RankCNN is built based on a Python Library Theano [30].

@&#EVALUATION@&#

In this dataset, the performance is shown as follows:

As shown in Fig. 4
                           , compared with other text retrieval methods, Algorithm 2 constructed randomly pseudo preference pairs to minimize the cost function, which could efficiently solve the problem that the number of generated document pairs varies largely form query to query. Thus the introduction of (Q–PN) contributes to obtaining the more robust model than other pairwise approaches which only concern about positive–negative pairs.


                           Table 1
                            and Fig. 5
                            show the results for the Paris dataset, the performances of all methods are better against the initial rank.

The performance of several methods on different initial list is shown in Fig. 6
                           .


                           Figs. 5 and 6 demonstrate that RankCNN has been successfully proven in visual reranking. The QEB could provide better reranking results when the initial list has high precision on the top candidates, but it suffers when the high-ranking images include much false positive. MWD outperforms QEB, however it encounters low recalling problem for certain query. RankCNN is the best performers on Francelandmark dataset.

In Fig. 7
                            32 convolutional kernel size of 5×5 learned by the first convolutional layers are shown. From the pattern of these images, they are similar with the kernels of the CNN. They both indicate the depiction of the texture of images.

We note in RankCNN, parameters k and Nc would impact the performance of the algorithm. The size of k controls is the strict degree about choosing the reciprocal neighbors. If the reciprocal neighbors are selected too rigorously, the image and its reciprocal neighbors look pretty much the same leading to lose the augmentability. On the other hand, if k is set to be too loose, the algorithm may render large number of noisy neighbor. In terms of Nc, its role is similar with k, and the difference is that Nc is related to sufficiency and precision of the confident samples. To fully explore the applicability of RankCNN on visual reranking, additional experiments are conducted to access the impact of the parameter setting on the performance of RankCNN. Fig. 8(a) shows that the parameters k and Nc have great influence on the performance of proposed RankCNN.


                           Fig. 8(b) illustrates that when solving the loss minimization problems, the ASDCA outperforms the SGD in our RankCNN model. At the same time, it is found that ASDCA has more smooth convergence curve than SGD. This improvement could be explained in the following aspects. First, the ASDCA absorbs the benefit of SGD by using random minibatch. In this way, the processing time of a mini-batch of size m is much smaller than m times the processing time of one example (mini-batch of size 1). In the practical training of neural networks with SGD or ASDCA, it is more efficient to perform matrix–matrix multiplications over a mini-batch than an equivalent amount of matrix–vector multiplication operations. This benefit is notable especially when GPU is used. Second, as stated in [22] said, the convergence rate of ASDCA is significantly better than that of the SGD because the ASDCA has a clear stopping criterion and it does not tend to be too aggressive at the beginning of the optimization process, especially when λ is very small. Generally, the convergence of ASDCA becomes faster when we are interested in more accurate solutions while SGD reaches a moderate accuracy quite fast. According to Fig. 6, when the number of pseudo positive set and pseudo negative set is 20 and 50 respectively, the solution is most accurate.


                           Fig. 9
                            shows the initial result and refined result for different query. The top 20 candidates are only given in score order. As for the ‘Louvre (a)’ and ‘Louvre (b)’, they produce two totally different initial results. In the query ‘Louvre (a)’, the precision of top 20 candidates is less than 25% mainly because of the noisy descriptors from query. Our reranking method could model the latent target. As a result, the refined outcome is accurate in the top 20 candidates while the top one is not the most similar to the query. In the query ‘Louvre (b)’, the precision of top 20 candidates are higher than 75%. According to the refined result, the rerank method still work. In the query ‘Eiffel Tower’, due to the cluster of text, the initial result includes several false positives. In the query ‘triumphal arch’, the affined deformation makes it difficult for image retrieval. Still, we observe that the search target could be detected accurately.

@&#CONCLUSION@&#

In this paper, an unsupervised learning-based visual reranking approach is proposed, where the training pairs are obtained by pseudo preference feedback. From the experiment, pseudo preference feedback lays the groundwork for our unsupervised learning, and RankCNN model has been proven successful for different datasets. Moreover, it broadens the outlook of visual reranking by integrating the pseudo preference feedback with class-based approach. In future work, more sophisticated model would be explored to implement efficient online learning.

@&#ACKNOWLEDGMENTS@&#

This work is sponsored by collaborative Research Project (SEV01100474) between Beijing University of Posts and Telecommunications and France Telecom R&D, and the National High Technology Research and Development Program of China (863 Program, No.2012AA012505).

@&#REFERENCES@&#

