@&#MAIN-TITLE@&#Screening drug target proteins based on sequence information

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Two strategies enhanced the drug target (DT) proteins prediction power of SVM classifier.


                        
                        
                           
                           39 DT protein chemo-physical features were collected based on sequence information.


                        
                        
                           
                           1797 and 227 potential DT proteins were identified.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Machine learning

Drug target

SVM

@&#ABSTRACT@&#


               
               
                  Identifying new drug target (DT) proteins is important in pharmaceutical and biomedical research. General machine learning method (GMLM) classifiers perform fairly well at prediction if the training dataset is well prepared. However, a common problem in preparing the training dataset is the lack of a negative dataset. To address this problem, we proposed two methods that can help GMLM better select the negative training dataset from the test dataset. The prediction accuracy was improved with the training dataset from the proposed strategies. The classifier identified 1797 and 227 potential DT proteins, some of which were mentioned in previous research, which added correlative weight to the new method. Practically, these two sets of potential DT proteins or their homologues are worth considering.
               
            

@&#INTRODUCTION@&#

At the molecular level, the main targets for drugs are proteins (mainly enzymes, receptors and transport proteins) and nucleic acids (DNA and RNA). In recent years, novel drug identification research has been widely conducted. For example, Hughes et al. [1] summarized the key preclinical stages of the drug discovery process and demonstrated that data mining of available biomedical data has led to a significant increase in target identification. Yang et al. [2] introduced tentative strategies of integrating different sources for target discovery. Imming et al. [3] considered the natural properties of DT proteins and conducted a comprehensive analysis of the DT proteins to estimate the number of known targets. Many papers have been published to address the screening drug target protein based on sequence information (see, e.g., [4–8]).

Because of the uncertainty of the drug action mechanism for each specific drug-protein target, there is no wide consensus on the optimal computational identification method. Several prediction methods have recently been developed. They performed fairly well given their specific biological hypothesis (i.e., based on side-effect similarity [9], based on ligands [10], or based on chemical structure and genomic sequence information [11]) and were aimed at integrating as much information as possible. Current knowledge is limited about what makes a protein a drug target, and there are also limits on the reliability of the biological assumptions. The latter can restrict the corresponding biological hypothesis. Therefore, it is important to develop a method that does not depend on the DT protein properties and can balance the training dataset to avoid the overfit of the prediction results. To address this problem, Li and Lai [12] developed a drug target prediction method based solely on protein sequence information without the knowledge of family/domain annotation or the protein 3D structure.

Once the training dataset is prepared, traditional machine learning strategies perform fairly well to predict new DT proteins. However, the process inherently suffers from an overfit problem [13]. This is because the training datasets have two classes. One is called the positive dataset (proteins that are known as DT proteins), and the other is called the negative dataset (proteins that are not DT proteins). The accuracy and completeness of the predictions are limited by our inability to be certain that proteins in the negative dataset are not DT proteins. To provide negative training datasets to the classifier for training, it is necessary to use some of the test data (proteins that will be predicted) for the negative dataset. The commonly used strategy is to randomly extract the negative training dataset from the putative non-DT dataset [12,14].

In this study, two strategies for selecting the negative dataset for a classifier are presented. The diagram of our workflow is shown in Fig. 1
                     . The first strategy is aimed at increasing prediction accuracy in cross-validation, and the second strategy is aimed at filtering out as many non-DT proteins as possible. The proposed training dataset helps enhance the classifier’s cross-validation accuracy. Based on the combination of SVM and different kernel functions, the classifier was trained by two datasets. The SVM with a radial basis function kernel classifier predicted 1797 and 227 DT proteins, respectively.

As demonstrated by a series of recent publications [4,5,15–17] and summarized in a comprehensive review [18], to develop a really useful predictor for a protein or peptide system, one needs to go through the following five steps: (a) select or construct a valid benchmark dataset to train and test the predictor; (b) represent the samples with an effective formulation that can truly reflect their intrinsic correlation with the target to be predicted; (c) introduce or develop a powerful algorithm to conduct the prediction; (d) properly perform cross-validation tests to objectively evaluate the anticipated prediction accuracy; (e) establish a user-friendly web-server for the predictor that is accessible to the public. Below, let us elaborate how to deal with these five steps.

The DT protein information was extracted from the DrugBank database Version 3.0 [13], in which the approved DT proteins set contains approximately 1273 proteins. Moreover, 262 DT proteins were used as approved carriers, 819 DT proteins as approved transporters and 984 DT proteins as approved enzymes. All of the sequence data were extracted from the UniProtKB/SwissProt data file (October, 2010). The non-human proteins were removed from both the DT and non-DT protein datasets. The known DT proteins that we collected from the DrugBank database were removed from the non-DT protein dataset. We applied the redundancy method [14] to obtain our DT and non-DT protein datasets. Then, PISCES [19] was used to remove the sequences with identity larger than 20% for both the DT and the non-DT sequences. We obtained 517 DT proteins for a positive dataset from the 1604 known DT proteins and 3834 proteins as a test dataset for later use by the machine learning algorithm.

Since the final goal of drug target prediction is drug development, the classification of drug target should base on the drug property. Therefore, the classification should be standard classification. If the target is membrane or exterior, antibody can be used as drug. Otherwise, the drug must be small molecule, since the antibody cannot penetrate the membrane. If the target is membrane, the required affinity could be weak rather than the drug for cytoplasm or nucleus considering the distribution of drug molecule. If the target is GPCR, the drug should be mono amine and the drug could be agonist and inverse agonist. If the target is enzyme, the drug should be inhibitor. Enhancing the enzyme activity is difficult. There have been many articles and reviews about the drug target [20,21]. The target proteins are classified into membrane, cytoplasm, exterior, nucleus, or, GPCR, nuclear receptor, channel, enzyme, transcription factor, etc.

The DrugBank database categorized DT proteins into three groups: Enzyme, Carrier and Transporter. To determine whether we can consider these three groups of proteins as a whole set of DT proteins for the latter training data of SVM, we analyzed their Gene Ontology (GO) [22] term. GO provides standardized terms to describe the biological properties of gene products. The ontology covers three domains:cellular components (the parts of a cell or its extracellular environment);molecular function (the elemental activities of a gene product at the molecular level, such as binding or catalysis); andbiological process (operations or sets of molecular events with a defined beginning and end relevant to the functioning of integrated living units: cells, tissues, organs, and organisms) [23,24]. We used GOstat [25] to obtain the significant GO terms. The minimal length of considered GO paths was set at level 3, and the three GO hierarchies were biological process, molecular function and cellular component. The maximal p-value in the GO output list was set as 0.1, and the maximum number of GOs display was set at 50; Other default settings were used, and Benjamini False Discovery Rate was chosen for multiple testing. Despite the different group numbers of the target datasets, Lauss et al. [26] and Bakheet and Doig [14] reported similar results.

Recently, there is a discussion regarding the justification of the GO approach [18]. They believe protein samples defined in a GO database space would be clustered in a way better reflecting some of their important attributes, such as subcellular localization and biological function. We conducted our GO terms analysis according to their suggestion. A detailed analysis of the GO terms and protein properties are presented in the additional file. Here, we try to see whether it is reasonable to pool the three sub groups (carries, enzyme and transporter) training dataset together.

GO distributions of carrier, enzyme, transporter, and the test dataset are shown in Table 1
                        : for molecular function, the carrier, enzyme and transporter proteins are all mainly related to catalytic activity, e.g., binding and transporter activity; for biological process, they all primarily participate in biological regulation, e.g., cellular process and metabolic process; and for the cellular component, they are mainly cells, macromolecular complexes or organelles.

The GO distributions are similar within the three categories of DT proteins and between the DT proteins and the non-DT proteins. This implies that the carrier, enzyme, and transporter drug proteins can actually be pooled together for the classifier’s training dataset.

Amino acids play very important roles both as building blocks of proteins and as intermediates in metabolism. The chemical properties of the amino acids that make up a protein determine the biological activity of that protein and can therefore help determine whether a protein is suitable to be a drug target protein. Here we used PEPSTATS (an online software program from EMBOSS [27]) to calculate the statistics of protein properties. We also identified other properties such as single peptide cleavage [28], transmembrane helices [29], low complexity region [30], N-glycosylation [31], and O-glycosylation [32]. We identified 39 properties in total. They are showed in the Supplementary file.

We only have the known DT proteins as a positive training dataset (known DT proteins) and a test dataset (all the other proteins). However, a classifier needs both a positive training dataset and a negative training dataset. We chose two strategies to select a negative dataset from our test dataset and combined three different kernel functions with SVM to develop the training data and to classify the testing data.

Given that there is much more data in the test dataset than the positive dataset, using the entire test dataset as the negative dataset might cause an overfit problem. SVM is the most robust and accurate of the well known data mining algorithms [33]. The basic challenge would be to separate the multi-dimensional data perfectly into its two classes. Suppose a binary classification problem, an input vector x
                           
                              i
                            contains m attributes. yi
                           ,
                           yi
                           
                           ∊{−1,1},is a binary label corresponding to x
                           
                              i
                            classification. Let 
                              D
                           
                           ={(xi
                           ,
                           yi
                           )|xi
                           
                           ∊
                           
                              R
                              m
                           ,
                           yi
                           
                           ∊{−1,1}} denote the training dataset which contains n points. The hyperplane can be written as the set of points x satisfying w
                           ⋅
                           x
                           −
                           b
                           =0. w is the normal vector to the hyperplane. The SVM can be expressed via the optimization problem: minimize (in w, b) ‖w‖ subject to yi
                           (w
                           ⋅
                           x
                           
                              i
                           
                           −
                           b)⩾1 (for any i
                           =1,⋯,
                           n). Using the fact that ‖w‖2
                           =
                           w
                           ⋅
                           w and substituting 
                              
                                 w
                                 =
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       n
                                    
                                 
                                 
                                    
                                       α
                                    
                                    
                                       i
                                    
                                 
                                 
                                    
                                       y
                                    
                                    
                                       i
                                    
                                 
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                              
                           , the dual form SVM can also be written as the following optimization problem: Maximize (in αi
                           ).
                              
                                 (1)
                                 
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             i
                                             =
                                             1
                                          
                                          
                                             n
                                          
                                       
                                    
                                    
                                       
                                          α
                                       
                                       
                                          i
                                       
                                    
                                    -
                                    
                                       
                                          1
                                       
                                       
                                          2
                                       
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             i
                                             =
                                             1
                                          
                                          
                                             n
                                          
                                       
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             i
                                             =
                                             1
                                          
                                          
                                             n
                                          
                                       
                                    
                                    
                                       
                                          y
                                       
                                       
                                          i
                                       
                                    
                                    
                                       
                                          y
                                       
                                       
                                          j
                                       
                                    
                                    k
                                    (
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    
                                       
                                          x
                                       
                                       
                                          j
                                       
                                    
                                    )
                                    
                                       
                                          α
                                       
                                       
                                          i
                                       
                                    
                                    
                                       
                                          α
                                       
                                       
                                          j
                                       
                                    
                                 
                              
                           
                           
                              
                                 
                                    subject to
                                    
                                    0
                                    ⩽
                                    
                                       
                                          α
                                       
                                       
                                          i
                                       
                                    
                                    ⩽
                                    C
                                    
                                    (
                                    for any
                                    
                                    i
                                    =
                                    1
                                    ,
                                    2
                                    ,
                                    …
                                    ,
                                    n
                                    )
                                 
                              
                           
                           
                              
                                 
                                    and to the constraint
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             i
                                             =
                                             1
                                          
                                          
                                             n
                                          
                                       
                                    
                                    
                                       
                                          α
                                       
                                       
                                          i
                                       
                                    
                                    
                                       
                                          y
                                       
                                       
                                          i
                                       
                                    
                                    =
                                    0
                                 
                              
                           where C
                           is a parameter that trades off wide margin with a small number of margin failures. αi i
                           =1,2,⋯,
                           n are parameters of the linear combination 
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       n
                                    
                                 
                                 
                                    
                                       α
                                    
                                    
                                       i
                                    
                                 
                                 
                                    
                                       y
                                    
                                    
                                       i
                                    
                                 
                                 =
                                 0
                              
                           , and k(xi
                           ,
                           xj
                           ) is thekernel function. The theoretical foundation guarantees that the classifier can be established by maximizing the margin between these different dimensional data.

We then turned our attention to the fact that we initially only had known DT proteins. We could not identify a priori with any certainty which proteins were not DT proteins. However, the percentage of the human genome that is ‘druggable’ is 10% [34]. Therefore, it is reasonable that there are significantly fewer DT proteins than non-DT proteins, leading us to infer that the significant majority of the proteins in our test dataset are non-DT proteins. Based on this hypothesis, it is necessary to design criteria that can differentiate the negative training dataset from the testing dataset. We applied the deviation as a classification criterion and designed two strategies to generate the training data. The first strategy focused on prediction accuracy by generating a smaller training dataset from our test dataset. The second strategy combined a fitness proportionate selection method giving a probability for each protein belonging to the negative dataset. This method produces a smaller prediction scope by generating a larger training dataset from our test dataset. The implementation of support vector machine used here is LIBSVM [35].

Suppose there were n drug proteins (instances) and each protein had m properties (attributes). Each X
                           
                              i
                           (i
                           =1,⋯,
                           n) is a m dimensional vector X
                           
                              i
                           
                           =(xi
                           
                           1,
                           xi
                           
                           2,⋯xim
                           ). We use x
                           
                              j
                           
                           =(x
                           1
                           
                              j
                           ,
                           x
                           2
                           
                              j
                           ,⋯,
                           xnj
                           ) to denote the vector of jth attribute. xij
                            denoted the ith drug proteins jth property’s value.

We then define the normalized weight as below:
                              
                                 (2)
                                 
                                    
                                       
                                          W
                                       
                                       
                                          j
                                       
                                    
                                    =
                                    
                                       
                                          mean
                                          
                                             
                                                (
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      j
                                                   
                                                
                                                )
                                             
                                             
                                                2
                                             
                                          
                                          /
                                          var
                                          (
                                          
                                             
                                                x
                                             
                                             
                                                j
                                             
                                          
                                          )
                                       
                                       
                                          
                                             ∑
                                          
                                          (
                                          mean
                                          
                                             
                                                (
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      j
                                                   
                                                
                                                )
                                             
                                             
                                                2
                                             
                                          
                                          /
                                          var
                                          (
                                          
                                             
                                                x
                                             
                                             
                                                j
                                             
                                          
                                          )
                                          )
                                       
                                    
                                 
                              
                           So the drug protein’s deviation is the following:
                              
                                 (3)
                                 
                                    ξ
                                    (
                                    
                                       
                                          X
                                       
                                       
                                          i
                                       
                                    
                                    )
                                    =
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             j
                                          
                                       
                                    
                                    
                                       
                                          W
                                       
                                       
                                          j
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         x
                                                      
                                                      
                                                         ij
                                                      
                                                   
                                                   -
                                                   mean
                                                   (
                                                   
                                                      
                                                         x
                                                      
                                                      
                                                         j
                                                      
                                                   
                                                   )
                                                
                                                
                                                   mean
                                                   (
                                                   
                                                      
                                                         x
                                                      
                                                      
                                                         j
                                                      
                                                   
                                                   )
                                                
                                             
                                          
                                       
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             j
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   mean
                                                   (
                                                   
                                                      
                                                         x
                                                      
                                                      
                                                         j
                                                      
                                                   
                                                   )
                                                   (
                                                   
                                                      
                                                         x
                                                      
                                                      
                                                         ij
                                                      
                                                   
                                                   -
                                                   mean
                                                   (
                                                   
                                                      
                                                         x
                                                      
                                                      
                                                         j
                                                      
                                                   
                                                   )
                                                   )
                                                
                                                
                                                   var
                                                   (
                                                   
                                                      
                                                         x
                                                      
                                                      
                                                         j
                                                      
                                                   
                                                   )
                                                   
                                                      ∑
                                                   
                                                   (
                                                   mean
                                                   
                                                      
                                                         (
                                                         
                                                            
                                                               x
                                                            
                                                            
                                                               j
                                                            
                                                         
                                                         )
                                                      
                                                      
                                                         2
                                                      
                                                   
                                                   /
                                                   var
                                                   (
                                                   
                                                      
                                                         x
                                                      
                                                      
                                                         j
                                                      
                                                   
                                                   )
                                                   )
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           We first obtained 517 drug proteins as the positive dataset and 3834 proteins as the test dataset. Fig. 2
                           (A) shows the distributions of the drug and test protein deviation. The shape of the test dataset is wider than the shape of the DT protein dataset. This reveals the existence of new DT proteins in the test dataset. Therefore, building a classifier is practical.

The cumulative distribution in Fig. 2(B) shows that when P(ξ(Xi
                           )⩽0.42)>0.95, we chose those 823 proteins with ξ(Xi
                           )>0.42 as the negative dataset. The SVM kernel function was set as a linear, polynomial and radial basis function in Eq. (1). Based on Fig. 2(C), the distribution of the predicted data is fairly close to the distribution of the DT proteins but was not well fitted around the peak area (0.1, 0.2).

The negative dataset (non-DT proteins) was chosen from the proteins whose mean values of protein sequence properties have a larger difference from the positive data. Therefore, for each unknown protein i, we evaluated the probability of its inclusion in the non-DT protein dataset:
                              
                                 (4)
                                 
                                    P
                                    (
                                    
                                       
                                          X
                                       
                                       
                                          i
                                       
                                    
                                    ∈
                                    NT
                                    )
                                    =
                                    
                                       
                                          
                                             
                                                (
                                                ξ
                                                (
                                                
                                                   
                                                      X
                                                   
                                                   
                                                      i
                                                   
                                                
                                                )
                                                -
                                                mean
                                                (
                                                
                                                   
                                                      ξ
                                                   
                                                   
                                                      positive
                                                   
                                                
                                                )
                                                )
                                             
                                             
                                                2
                                             
                                          
                                       
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                i
                                             
                                          
                                          
                                             
                                                (
                                                ξ
                                                (
                                                
                                                   
                                                      X
                                                   
                                                   
                                                      i
                                                   
                                                
                                                )
                                                -
                                                mean
                                                (
                                                
                                                   
                                                      ξ
                                                   
                                                   
                                                      positive
                                                   
                                                
                                                )
                                                )
                                             
                                             
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

In this way, each testing protein is given a probability of being a non-DT (NT) protein. Given the total number of proteins in the test dataset was 3834, we considered each protein with a probability of 0.5 in the negative dataset. Using the probability Eq. (4) to segregate the target and negative elements of the datasets, we obtained 1917 proteins from the test dataset to function as a negative dataset (non-DT proteins). Based on Fig. 2(D), the distribution of the predicted data more accurately fitted the distribution of the drug target data.

@&#RESULTS@&#

We chose the same SVM classifier that was used in [12,14]. The classifier was trained with two proposed training datasets and with randomly chosen datasets. The comparisons of the receiver operating characteristic curves (ROC) are shown in Fig. 3
                     . Using the same kernel function, the SVM classifier that was trained with the proposed training dataset outperformed the classifier that was trained with the randomly chosen training dataset.

In statistical prediction, the following three cross-validation methods are often used to examine a predictor for its effectiveness in practical application: independent dataset test, subsampling test, and jackknife test [36]. However, of the three test methods, the jackknife test is deemed the least arbitrary that can always yield a unique result for a given benchmark dataset as elaborated in [37] and demonstrated by Eqs. (28–30) in [18]. Accordingly, the jackknife test has been increasingly used and widely recognized by investigators to examine the quality of various predictors (see, e.g., [38–42]). However, to reduce the computational time, we adopted the 10-fold cross-validation in this study as done by many investigators with SVM as the prediction engine. The comparisons of sensitivity, specificity, and accuracy results are shown in Table 2
                     . The training datasets from the two proposed strategies improved the classifiers’ accuracies. Among the three kernel functions, the accuracies of the SVM classifier with the RBF kernel function are higher than the accuracies of the SVM classifier with the other two kernel functions. Therefore, we chose the SVM-RBF classifier to predict the potential DT proteins. The classifier from the first strategy produced more prediction results than the classifier from the second strategy. The numbers of the DT protein prediction results from the two strategies are 1797 and 227. This means that the first strategy was intended to train the classifier to not reject a protein as a drug target. Compared to the first strategy, the second strategy was aimed at shrinking the prediction scope by rejecting more proteins as DT proteins. Therefore, its cross-validation accuracy was lower than the accuracy of the first strategy.

We chose some of the prediction results and manually searched them online and found some results that have already been reported as DT proteins, such as CPSF1 [43], MYCB2 [44], MIPEP [45], DSRAD [46], DGLA [12] and ALKB1 [47].

@&#DISCUSSION@&#

The advantages of our data preparation with strategies via SVM for the prediction of DT proteins can be outlined as follows: (1) The GO analysis reveals the existence of potential DT proteins. (2) With the two suggested strategies, the SVM classifier helps in selecting and prioritizing potential disease targets by allocating a smaller group of DT proteins. (3) More drug target protein properties. Both strategies are suitable for preprocessing drug target protein prediction from different data sources, as long as features of the drug target proteins are used. (4) The two selection strategies can satisfy different needs. The first strategy provides a rough screening with lower misleading risk, and the second one produces smaller prediction scope with higher misleading risk. Either of these two strategies relieves the overfit problem by balancing the training dataset. The first strategy can identify a larger number of predicted results. However, it leaves the burden of time and resources to the subsequent procedures because the prediction is rough. The classifier from the second strategy can produce a smaller scope of prediction results, which means that it is more efficient for future wet experiments.

Since user-friendly and publicly accessible web-servers represent the future direction for developing practically more useful models, simulated methods, or predictors [48,49], we shall make efforts in our future work to provide a web-server for the method presented in this paper.

@&#ACKNOWLEDGMENTS@&#

This work was funded by the Chinese National Key Program of Basic Research (31000591, 31000587, 31171266).

Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.jbi.2014.03.009.


                     
                        
                           Supplementary material 1
                           
                              GO and drug protein properties analysis.
                           
                           
                        
                     
                     
                        
                           Supplementary material 2
                           
                              Prediction result.
                           
                           
                        
                     
                     
                        
                           Supplementary material 3
                           
                        
                     
                  

@&#REFERENCES@&#

