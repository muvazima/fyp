@&#MAIN-TITLE@&#Social and Q&A interfaces for app download

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           The paper deals with interfaces that prompt users to download or not a product.


                        
                        
                           
                           We compare 3 different designs of such software download interfaces.


                        
                        
                           
                           Question & Answer (Q&A) interfaces result the most effective in minimizing users incoherent behaviors.


                        
                        
                           
                           Differences in reputation rankings significantly influence users.


                        
                        
                           
                           Results suggest guidelines to design the best interface depending on the context.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Web interfaces

Software download

Usability

Q

&

A

Reputation system

Social web

@&#ABSTRACT@&#


               
               
                  Downloading software via Web is a major solution for publishers to deliver their software products. In this context, user interfaces for software downloading play a key role. Actually, they have to allow usable interactions as well as support users in taking conscious and coherent decisions about whether to accept to download a software product or not. This paper presents different design alternatives for software download interfaces, i.e. the interface that prompts the user if he wishes to actually complete its download, and evaluates their ability to improve the quality of user interactions while reducing errors in user decisions. More precisely, we compare Authenticode, the leading software download interface for Internet Explorer, to Question-&-Answer, a software download interface previously proposed by the authors Dini, Foglia, Prete, & Zanda (2007). Furthermore, we evaluate the effect of extending both interfaces by means of a reputation system similar to the eBay Feedback Forum. The results of the usability studies show that (i) the pure Question-&-Answer interface is the most effective in minimizing users incoherent behaviors, and (ii) the differences in reputation rankings significantly influence users. Overall results suggest guidelines to design the best interface depending on the context (brand reputation and product features).
               
            

@&#INTRODUCTION@&#

User interfaces for software download on the Internet are the front end of online software publishers for remote users. One important component of such interfaces is the software download interface, namely the interface that prompts the user if he wishes to actually complete its download, after that he/she has browsed on a web site and selected the desired product to download. The design of such an interface is critical, because it usually shows product related information that should guide the user in its final choice. Software download interfaces are critical for users who may take unconscious or misguided decisions if the interfaces have not been properly designed (Allen, Currie, Bakken, Patel, & Cimino, 2006; Maxion & Reeder, 2005). Software download interfaces are critical for publishers too, if the interfaces reduce user trust and satisfaction and system usability at large (Flavian, Guinaliu, & Gurrea, 2006; Hoffman, Blum, & Lawson-Jenkins, 2006; Shin, 2010). Finally, software downloading has security implications too. Brustoloni & Villamarin-Salomon have shown that a poorly designed software download interface may cause unjustified risks in terms of breaches in a per-user derived policy adopted to classify risks (Brustoloni & Villamarin-Salomon, 2007).

The Security Warning Dialog Box in the Microsoft Authenticode interface is a reference, largely-used software download interface for Internet Explorer. Prior works have highlighted weaknesses of such an interface (Dini, Foglia, Prete, & Zanda, 2006; Cranor, 2007), as well as the attitude of the users to seldom act appropriately when presented with such a download interface (Wood2010). In particular, in line with Brustoloni and Villamarin-Salomon (2007), incoherent behaviors in users interacting with the Microsoft Authenticode interface were observed in Dini et al. (2006), where an incoherent behavior consists in a download decision that is incoherent with the motivation given a posteriori. According to Shin (2010), the rate of online visitors who buy a product or download software is influenced, among other factors, by trust and attitude, with trust also depending on user interface design quality (Flavian et al., 2006; Hoffman et al., 2006). As a consequence, software download interfaces should have a good design that minimizes incoherent behaviors, increases user trust and, then, supports the user intention to purchase.

In prior works we have proposed a Question-&-Answer (Q&A) interface, i.e. a software download interface aimed at reducing incoherent behaviors by asking the user questions about specific product download issues, e.g., the product cost (Dini et al., 2007). Furthermore, we have suggested the use of a Reputation System
                     (RS)–a service like the eBay Feedback Forum–to increase the user trust on a given software product (Dini, Foglia, Prete, & Zanda, 2013).

In this paper we experimentally compare the Authenticode and Q&A software download interfaces and evaluate the effects of their extension by means of a reputation system. The overall result is that any solution that increases users attention to the interface contents also significantly reduces incoherent behaviors. Nonetheless, the three design choices are not equivalent as they make users focus on different aspects of the proposed products. For example, RS make users to focus on the aggregated social feedback from previous users whereas Q&A makes users to focus on specific software download issue, such as the cost. Even if the minimization of incoherent behaviors through a Q&A-enhanced interface has positive effects on users trust, it causes a strong reduction in all software downloads because users tend only to accept free software, refusing software that entails a charge. So, if having a large market penetration is needed, publishers should not adopt a Q&A interface if the software offered entails a charge. However, embedding a reputation mechanism side by side to a Q&A dialog reduces the relevance of cost, and reduces incoherent behaviors at an intermediate level. Showing aggregated feedback from previous users has a mitigating role between the common interface and the Q&A interface. From these findings we indicate how designers can better tackle the design tradeoffs in these user interfaces.

The paper is structured as follows. Section 2 presents the background of the problem, and shows how the macro-hypotheses under test have been developed. Sections 3 and 4 show the graphical interfaces that have been tested in the experiments and present the experimental settings. Then, the experimental results are given with a concluding discussion on the observed trends.

Online software delivery, like common e-commerce, must solve concerns especially regarding authentication and trust, in order to make users confident that they are not being cheated (CommerceNet., 2000; Corritore, Kracher, & Wiedenbeck, 2003). Complex Web interfaces and fraudulent software houses have caused serious problems such as spreading dialers, spywares and other threats (Shukla & Nah, 2005).

In this environment users cannot relate with a merchant and understand whether the software is in line with their expectations. In an ordinary shop, clients relate with a merchant and establish a trust relationship, and a similar relationship should be established also online (Corritore et al., 2003; Cheskin research, 1999). Easy-to-use interfaces and complete information are highly correlated to user trust (Flavian et al., 2006; Hoffman et al., 2006; Nilsson, Adams, & Herd, 2005; Laberge & Caird, 2000; Lanford, 2006), and a trustworthy interface is more likely to make users trust a vendor on the Internet (Fogg et al., 2001).

A trustworthy, usable interface would not solve two relevant questions for users dealing with software download from the Internet: first, the need to know who really published the file offered, second, supporting the user in understanding what to expect from the executable file. As a solution to the first problem (authentication), major software vendors have developed frameworks for code signing (Jansen, 2000; Thawte., 2007), such as the Microsoft Authenticode that has been taken as reference in the current study due to its widespread diffusion on the Internet: 53% of the market share (BMS, 2012). By signing code, publishers seek to build a relationship of trust with users, satisfying the matter of accountability at the same time. Software publishers sign the piece of software they are releasing. The publisher’s certificate, the certification authority certificate, and the signed code are then packaged together.

At the client side, the browser verifies the publisher’s signature on the code, and, if verifications are successful, the Security Warning dialog box (SWDB) is presented to the user (Fig. 1
                        ). If some problems arise, the user is notified with a different dialog box. The SWDB presents the certification authority name, the software publisher name and the software name, as well as other details such as the cost and other optional information pieces. This information is shared by means of the different Authenticode versions that have been released over the years.

The further issue to be faced in software downloading concerns the user’s expectations of the file. The Authenticode framework, like other code signing frameworks, ties a publisher to a file via its certification authority, but it does not help the user understand what to expect from the file. Predictability in this scenario would be important as it influences trust (Corritore et al., 2003). Current software download frameworks do not give any safe information about what and how the executable file will behave once run on a computer system. Given that predictability cannot be fully ensured with the above approaches, in recent years Reputation Systems (RS) have emerged as a method for fostering trust among strangers cooperating online by gathering, distributing, and aggregating the feedback of previous consumers (Resnick, Kuwabara, Zeckhauser, & Friedman, 2000).

Deciding whether to accept or refuse a software download is a context-dependent decision: “In such a situation, an application usually needs user input, because the application cannot determine automatically all the context relevant to the decision” (Brustoloni & Villamarin-Salomon, 2007). For instance, receiving a software should be allowed if the sender is a friend, and should be denied if the sender is a malicious user. Based on Brustoloni and Villamarin-Salomon (2007) classification of interaction paradigms that support such decisions, in this paper we analyze the warn-and-continue and the context sensitive guidance. A context sensitive guidance interface asks the user to provide context information necessary for a security decision, while in the warn-and-continue approach the application warns the user of the risk and asks whether the user wants to accept it or not. Interfaces for software download (e.g.: the Authenticode Security Warning Dialog Box) follow the warn-and-continue approach.

In Dini et al. (2006) users claimed that they wished to download free software only, but they largely accepted software from Well-Known (WK) publishers, showing that the brand effect was more important than cost. Thus, users did not focus their attention on the whole interface, but they accepted or refused software downloads according to the publisher name, not the item cost. In addition, the Authenticode Security Warning Dialog Box prevented Common Name (CN) publishers with little or null brand name from having their software downloaded. Similar considerations were done by Brustoloni and Villamarin-Salomon (2007) and , Wood2010 highlighting the problems deriving from the warn-and-continue approach: users click “continue” automatically, click through warnings due to habit (Chia, Heiner, & Asokan, 2010), do not pay attention to the whole information in the interface, and run into incoherent behaviors (Dini et al., 2007) or unjustified risks (breaches in the policy adopted to classify risks) (Brustoloni & Villamarin-Salomon, 2007). As a countermeasure to these problems, (Brustoloni and Villamarin-Salomon2007) propose the adoption of polymorphic dialogs that change the form of the required user inputs, forcing users to pay attention to security decisions. While Dini et al. (2007) tested a Question-&-Answer (Q&A) interface designed to increase user attention on the contents of the interface. Both the designs proved effective in reducing unjustified risks or incoherent behaviors compared to other interaction paradigms.

Trust for Mayer, Davis, & Schoorman (1995) is the willingness of a trustor to be vulnerable to the actions of a trustee based on the expectation that the trustee will perform a particular action important to the trustor, irrespective of the ability to monitor or control the trustee. Trust is driven by many variables, and upon deciding whether to accept or refuse a software download, the user should be able to predict if the software meets with his/her expectations. Predictability influences trust: “predictability is a trustor’s expectation that an object of trust will act consistently based on past experience” (Corritore et al., 2003). To better understand and solve trust issues, Egger (2000) proposed a model of trust for electronic commerce, which includes interface properties such as usability and familiarity, and cooperation between consumers. Cooperation is relevant also for Olson & Olson (2000), who states that trust deals with behavior: “people learn to trust others by noting their behavior”. In the online domain, the end user has limited possibilities to fully try a software product or a service before buying, while the software publisher knows what he gets as long as he receives money. “The inefficiencies resulting from this information asymmetry can be mitigated through trust and reputation” (Josang, Ismail, & Boyd, 2007). Reputation Systems help end users in predicting whether a product can be trusted or not. Enforcing a trust belief in the end users towards a remote software provider is a subjective phenomenon influenced by a set of factors, and in absence of personal experience, trust must be based on third party reviews. For Josang et al. (2007), reputation is a collective measure of trustworthiness based on reviews from members in a community.


                        Lin, Rong, & Thatcher (2009) validate a research model wherein purchase intention and swift trust are influenced by a set of parameters, including perceived social presence, awareness of 3rd party assurance, and perceived security (Fig. 2
                        ). Swift trust refers to trust formed quickly in new or transitory relationships (Meyerson et al., 1996). According to Lin, “social presence refers to consumers’ perception that there is personal, sociable, and sensitive human contact and/or peer community on the Web site”. This type of reassurance for online entities is similar to the reassurance provided by brick and mortar businesses (Gefen & Straub, 2004). The model by Lin et al. infers users opinions and intentions by looking at the answers from a survey. The users answers are analyzed to derive how “Purchase intention” is influenced by a set of factors. The problem with survey-only studies is that relevant aspects of the user interaction are not differentiated qualitatively. In an everyday Internet scenario users can make incoherent decisions, which happen to differ significantly from the desired behaviors. For instance, it is clear that asking a user “Do you feel safe to provide your login and password to a phishing Website?” will cause a negative answer. Though in real life the user could behave differently because of badly designed interactions, fraudulent designs, lack of attention.

The results of this paper help to understand how interfaces can be better designed to meet the users needs, in a safe way. Thus, our study is orthogonal to the above research model: we investigate what happens in some building blocks of the above model, when real interfaces must be designed. The block “Social presence” suggests the adoption of a Reputation System as aggregator of reputation scores from a community, also to improve the efficiency in the process of software downloading. Resnick et al. (2000) state that these mechanisms can help people make decisions about who to trust and provide incentive for honest behavior, while deterring dishonest parties from participating. Despite many problems in theory (Resnick et al., 2000), in practice RSs proved to work rather well as a means to provide relevant information on the quality of merchants and products in auction sites. Successful examples are represented by iTunes and its reputation system for both desktop and mobile users, by the eBay auction site, or online book sellers like Amazon (Chia et al., 2010; Dellarocas C., 2003; Chen and Liu, 2011). RSs have been proposed as a means to identify inauthentic content in file sharing applications (Walsh & Sirer, 2006), and similarly they could be effective in helping users to identify deceptive software publishers.

The model by Lin also includes “Perceived security” which, as shown in Section 2.1, can be managed by designing a context sensitive interface like the Q&A. The Q&A proved effective in minimizing incoherent decisions, thus increasing perceived security in the interaction paradigm, as also observed in questionnaire answers (Dini et al., 2007). In our design, the Q&A interface asks for an explicit feedback on cost, and this results in minimizing the incoherent behaviors, but also in focusing users attention on cost.

The Reputation System makes users focus on aggregated scores from previous users, and by doing so it also increases users overall attention in the interface. With the RS, the answer given in the Q&A interface becomes one of the possible driving factors together with the scores shown in the RS box. This is tested with the hypothesis H1:
                           H1
                           The RS reduces the relevance of cost aspects when compared to the Q&A interface.

The RS does not ask for an explicit feedback in order to let the user go further with the download, so it should be expected an increase in incoherent behaviors when compared with a Q&A interaction (hypothesis H2).
                           H2
                           The Q&A interface minimizes incoherent behaviors when compared to the other interfaces.

From these considerations, combining the Q&A with the RS should increase incoherent behaviors when compared to plain Q&A (hypothesis H3), and reduce them when compared with the common Authenticode interface (hypothesis H4).
                           H3
                           The RS with Q&A increases incoherent behaviors when compared to the Q&A.

The RS with Q&A reduces incoherent behaviors when compared to the SWDB.

And we also test H5, H6 and H7:
                           H5
                           The RS with Q&A reduces the relevance of cost aspects when compared to the Q&A interface.

The RS with Q&A increases the relevance of trust when compared to the Q&A interface.

The RS with Q&A increases the relevance of trust when compared to the SWDB interface.

The Question-&-Answer interface (Q&A) (Dini et al., 2007) utilized in our study is a wizard with three steps. The first step replicates the information of the common SWDB, while the second step asks the cost of the software under download, and the third steps allows to download the software if the given answer is correct (Fig. 3
                     ). The reason why we choose to focus on the cost information is the following. In a previous work (Dini et al., 2007) we found incoherent behaviors of users with respect to cost, namely users stated they wanted to download free software whereas in practice they actually downloaded expensive one (or vice versa). We also proved that the Q&A interface mitigates such a behavior. Therefore, in this work, we decided to start from such consolidated results. Conceptually, the Q&A interface may present questions on any other information that is considered relevant, such as name of the producers, name of the software, et cetera.


                     Fig. 4
                      shows the other interfaces used in our experiment. They are, respectively, the Security Warning Dialog Box (left) and the Q&A interface (right), both augmented by a reputation systems which display the aggregation of users rankings regarding the software to be downloaded (Dini et al., 2013). As shown in Fig. 4, the RS is placed at the bottom of the dialog box, showing the total number of feedback reviews, as well as the rate of positive, neutral, and negative reviews. According to the classification of RS ranking aggregation mechanisms proposed by Josang et al. (2007), the interface, like that of the eBay Feedback Forum, is a simple summation system. In a simple summation RS, the system accumulates all given ratings to get the overall reputation. The RS is simple and readable at a glance, providing a concise view of the distribution of users’ feedback (Dini et al., 2013). The implementation aspects of the RS are out the scope of this work. Anyway, the RS can be realized folling the approaches proposed in Josang et al. (2007), by utilizing object oriented technology (Bechini, Foglia, & Prete, 2002) in CMP systems (Foglia & Solinas, 2014).

@&#METHOD@&#

The Reputation System was inspired by the eBay Feedback Forum also described by Resnick et al. (2000), with the purpose to influence user trust beliefs about software publishers while dealing with software download. The RS effect is assessed by looking at the software download acceptances rates and by considering the motivations given for each acceptance or refusal. The RS must be populated meaningfully, in order to simulate an every day scenario for all three classes of publishers tested in the experiment. Looking at the eBay Feedback Forum, often well-known and established merchants have a high and positive reputation ranking, unless they decide to have no ranking at all as their brand effect is strong enough. On the other hand, deceptive merchants who may provide an RS present few comments mostly negative. Finally, common name merchants tend to support an RS as they want to achieve visibility and, contextually, be trusted by end users. Thus, in our experiments we do not test patterns in the population of the RS, but instead we choose to assume that the RS is already populated in line with the above observations. In particular: well-known publishers have a high and positive RS, or do not support it; deceptive publishers have a low and negative RS, or do not support it; publishers with a common name have either high and positive or low and negative RS ranking. The reason is twofold: reducing the number of test cases for each user, and simulating real life scenarios by associating appropriate reputation ranks to the three classes of publishers, while optimizing the time to complete the experiment.

In detail, the rankings present in the experiments are shown in Table 1
                        : a high ranking means that the number of reviews is greater than 400, with 80% positives, 10% neutrals, 10% negatives; a low ranking means that the number of feedback is smaller than 40, with 30% positives, 30% neutrals, 40% negatives; and finally the publisher can decide to not support the RS.

The study included the following independent variables.
                           
                              –
                              Software publisher name. Software publishers were varied according to three classes: (i)well-known software publishers (WK); (ii)common name publishers (CN), and (iii)companies whose name was deceptive (DN).

Software cost. It was varied between free and charge entailed (above 20 €).

Reputation System ranking. The RS was varied with the following scheme:
                                    
                                       •
                                       High and positive rankings for WK publishers and CN publishers;

Low and negative rankings for CN publishers and DN publishers;

Not supported: the publisher decides not to support a Reputation System, for WK publishers and DN publishers.

Interface for software downloading. Participants interacted with four interfaces to perform differential analysis: the original SWDB, the SWDB enriched with the RS, the Q&A enriched with the RS and the plain Q&A.

Each participant was shown a complete combination of the independent variables. More in details, each participant completed 36 test cases (Fig. 5
                        ), obtained from combining the independent variables with: (i) the SWDB interface, publisher name (3 values), and cost details (2 values); (ii) the SWDB+RS interface, publisher name, cost details, and RS ranking (2 values); (iii)the Q&A+RS interface, publisher name, cost details, and RS ranking; and finally, (iv)the Q&A interface, publisher name and cost details. In particular, we defined a pool of 36 pages divided in 4 groups as described in Fig. 5. Such pages are then presented each participant in a random order.

The test consists of a sequence of test cases, where each test case consists of three stages. In the first stage we display a web page that proposes a software download. In the second stage, the participant decides whether to accept or not the software product by using one of the displayed interfaces. In the third stage, the participant is required to motivate his/her download choice with a motivation radio-button. The given motivations, together with the acceptance or refusal of a piece of software, are dependent variables in our experiments. The motivation radio-button presents a set of options for participants to choose. The content of this window is described in Table 2
                         (column Motivation). If the participant accepts to download, then he/she is presented a list of motivations mainly related to interest (I), trust (T), free cost (F) and other minor motivations (O). In contrast, if the participant refuses to download, he/she is shown another list of motivations related to no interest (NI), distrust (DT), high cost (C) and other minor ones (O).

At the end of the 36 test cases, participants answered a multiple-choice questionnaire containing personal questions and questions about the used interface. The full questionnaire is reported in Appendix B. As each participant had to complete 36 test cases, we minimized learning effects by not giving participants feedback on their actions during the experiment.

In a previous work (Dini et al., 2006), it was observed that user intentions and proper behaviors for each single test case could not be derived by just looking at the final questionnaire answers, as each test case needed a precise motivation. For this reason, we let participants choose the most suitable motivation out of a set (see Table 2), after either accepting or refusing a downloading. By doing this, participants can motivate their decisions with precision. In addition, we used motivations to clearly identify participants’ coherent and incoherent behaviors. In fact, in the cited previous works, it was observed that users interacted with interfaces incoherently at times: a participant behaves incoherently if his/her motivation does not match with the features of a piece of software downloaded or refused. In Table 2, column “Incoherent, if in combination with” shows when motivations not matching actual software features identify incoherent behaviors. An example of an incoherent behavior is if a participant refuses a download picking the motivation “I could be interested, but it was expensive”, while the software is free.

At the beginning of a test session each participant was asked to “either accept or refuse the proposed software as if you were using your own computer”, since we want to simulate the scenario of a typical Web user who browses the Internet and downloads software according to its own interests. Participants were not rewarded for their time, but they were told: “you are participating to the design of a new user interface for e-government web applications”. A gender balanced group of 40 participants was recruited, with ages ranging from 18 to 40 (mean age 23). 34 participants were high school students or undergraduate students, while 6 participants were employed in public administration. Participants attended the tests in person in our labs or in their office (rather than remotely via web). The recruited group had good Internet experience, as shown in Table 3
                        . All the experiment were performed on a windows based platform.

All participants actions were logged on a database for subsequent analyses. Results presented in the paper are given reporting raw percentages, or Chi-Square χ
                        2 statistics. Chi-square was adopted to compare groups with different cardinalities, and we also considered the Yates correction for one degree of freedom. It provides a p-value that indicates the probability that the observed difference is due to chance, where p
                        <0.05 is commonly accepted to indicate a statistically significant difference in the sample (Glantz, 2005).

We minimized learning effects by not giving the users any feedback on each single test cases (e.g., users performing free acceptances or acceptances entailing a charge without highlighting, during the test, incoherent motivations). We observed that the actions taken by the users with tested interfaces were very conservative: they tried to minimize possible risks (even if they were aware that the experiment could not harm their terminal). Thus, we believe that the test environment had little impact on our results. In addition, we did not observe a lack of motivation in the participants, also because after initial tests, we minimized the duration of a single test at less than 30min on average.

There were three ranking categories shown in the Reputation System in our test: high and positive rankings, low and negative rankings, and rankings not supported. Although we observed that rankings tend to be classified mainly in line with these three categories, other studies should investigate how users behave while dealing with other reputation patterns.

@&#RESULTS@&#

In this Section we verify the hypotheses under test specified in Section 2.4 and give further significant findings. Tables with full results in details are reported in Appendix A.

Presenting aggregated social-feedback with an RS influences user decisions even in presence of a Q&A interface asking for cost details: users’ focus shifts from cost to rankings given from previous users (H1, H6). Such a shift of focus can cause an effect that we identify with a soft information overload, since incoherent behaviors are minimized only with the plain Q&A interface (H2). The presence of more information provided by the RS in the Q&A interface makes users to focus on aspects not related to cost but also causes an increase in incoherent behaviors (H3).
                           H1
                           The RS reduces the relevance of cost aspects when compared to the Q&A interface. – Confirmed (χ
                              2 (1,40): p<
                              0.05, power>0.8).

The Q&A minimizes incoherent behaviors when compared to the other interfaces. – Confirmed (χ
                              2 (1,40): p<0.05, power>0.8).

The RS with Q&A increases incoherent behaviors when compared to the Q&A. – Confirmed (χ
                              2 (1,40): p
                              <0.05, power>0.8).

The RS not only shifts users’ focus from cost, but it also reduces incoherent behaviors observed in the SWDB (H4). “Flow” is a concept that characterizes the human–computer experience as playful and exploratory (Trevino & Webster, 1992). We believe that the RS presence increases users “flow”, making them more active in exploring the information shown in the dialog box, and by doing so they make fewer incoherent behaviors.

The RS presence in the Q&A manages to increase the relevance of trust, reducing the focus on cost, when compared with the plain Q&A interface (H6). This means that showing a social feedback from previous peer users is considered a valid source of information and influences users.
                           H4
                           The RS with Q&A reduces incoherent behaviors when compared to the SWDB. – Confirmed (χ
                              2 (1,40): p
                              <0.05, power>0.8).

The RS with Q&A reduces the relevance of cost aspects when compared to the Q&A interface. – Not confirmed (χ
                              2 (1,40): p
                              >0.05, power>0.6).

The RS with Q&A increases the relevance of trust when compared to the Q&A interface – Confirmed (χ
                              2 (1,40): p
                              <0.05, power>0.8).

The RS with Q&A increases the relevance of trust when compared to the SWDB interface – Not confirmed (χ
                              2 (1,40): p
                              >0.05, power>0.8).

The RS in the Q&A did not show statistically significant differences (H5 not confirmed) when compared with the plain Q&A even if raw percentages indicate a difference, but looking at the statistical power, the non-significance can be due to the limited number of participants. No statistical significance on trust motivations was observed when the Q&A+RS is compared with the SWDB interface (H7 not confirmed). This is due to the fact that with the SWDB users gather mainly the publisher name, and base their decisions on the trust that they have in it. Thus, trust motivations with SWDB are high, but they are high because users do not find the relevant information and as a consequence base their decisions on the publisher name.

The macro-effect of the RS on motivations (Figs. 8 and 9) is reduced as results by the aggregation of positive, negative or null rankings in the classes of publishers, but different ranking influence downloads acceptances and refusals as shown on Table 6 in Appendix A.

The Q&A has few graphical effects, and asks for explicit feedback on cost, to ensure that the user gathers it. The results show that when users are pushed to think about cost details, cost becomes the driving factor for software acceptance/refusal. In fact, with the plain Q&A interface users prefer to download free software, and download software entailing a charge only if they are interested in it (Table 4
                        ).

We investigated the rates of incoherent behaviors for all the tested interfaces. Minimizing the rate of the observed incoherent behaviors can be considered as an indicator of interface quality. By observing the decreasing tendency line of incoherent behaviors in Fig. 6
                        , we derive that the plain Q&A performs better than the other proposed interfaces, ensuring the safest interaction paradigm. An SWDB-based design results more prone to incoherent behaviors, causing higher acceptances of costly pieces of software (often not desired by the users). Adding graphical elements, like the RS, causes a sort of information overload (Farhoomand & Drury, 2002), as incoherent behaviors with the plain Q&A are lower than with the Q&A+RS.

The analyses of the three novel interfaces (SWDB+RS, Q&A+RS, and Q&A) compared with the SWDB show a global reduction of accepted costly software, in particular concerning software by WK publishers. Fig. 7
                         shows the acceptance trends of costly pieces of software (data are taken from Tables 5 and 6 in Appendix A). The effect of introducing the Q&A and the RS is clear compared to the basic SWDB: the Q&A reduces significantly the acceptances of costly code, while the RS reduces the relevance of cost providing higher costly acceptances than the basic Q&A.

The motivations given by the users for either accepting or refusing software download are shown in Figs. 8 and 9
                        
                        , respectively (data are taken from Tables 7 and 8 in Appendix A). Both figures show the effect of the Q&A on the SWDB, as well as the mitigating role played by the RS between the SWDB and the Q&A. Aligning acceptance motivations from the four interfaces, the Q&A pushes free motivations reducing interest (I) and trust (T) motivations. Dually, as for refusal motivations, the Q&A emphasizes the influence of motivations related to high Cost (Fig. 9), still reducing the other motivations, namely non-interest (NI) and distrust (DT), from the average rows. Looking back at the histograms about the incoherent behaviors (Fig. 7), the Q&A related acceptances and motivations match more closely with actual users intentions compared to the other tested interfaces, as the Q&A nullifies incoherent behaviors.

The usability studies as a whole have shown stable trends in user behaviors while interacting with the different graphical interfaces. The Question-&-Answer interface focuses very significantly the user attention on cost aspects, driving users to largely accept free software. Adding a Reputation System in the Q&A interface causes a reduced relevance of cost, while in the classic SWDB interface it reduces a little incoherent user behaviors and influences a little their trust beliefs.

The overall results, as shown in the General Trends section, suggest guidelines to be considered while choosing the best interface depending on the brand effect of the software publisher and/or the need to increase the software market penetration. From our design, where the Q&A asked for cost information, it derives that a Q&A-like interface is to choose if software is free, while, if software entails a charge, such an interface should be proposed if the publisher wants to ensure that users fully understand cost aspects of the product offered. In detail, our Q&A interface prevents from having users who in the future give negative feedback about a publisher because the interface is very robust in not causing incoherent behaviors. However, the Q&A can increase the attention level on any content of the interface, so if the publisher wants to increase the attention on aspects other than cost, like brand name, this can be an option, though not investigated in our tests.

The classic SWDB interface results now somehow out-of-date because of the spreading of social aspects in a plethora of web based activities. Providing a feedback aggregation mechanism (such as a reputation system) upon software download is a feature that is not unexpected by the users, as also shown in our questionnaire answers. For this reason the publisher should not provide an RS only if negative feedback is likely to be received. This might be the case of software out as alpha- or beta- version, that still has not the desired quality. When the publisher has not a strong brand effect, but the product is valid, providing an RS has a positive effect on software downloads. When the RS is embedded in the Q&A we designed, it increases trust-based motivations, reducing those related to cost aspects. However, enriching the Q&A with the RS shows incoherent behaviors similar to the common SWDB interface with RS, and an acceptance rate similar to the plain Q&A for well-known publishers, so its adoption has little advantages. Conversely, the Q&A with RS should be preferred compared to the plain Q&A by common-name publishers as it provides higher acceptances for software entailing a charge, keeping incoherent behaviors low.

As for future works, we plan to extend our analysis to different application domains (Foglia, Giuntoli, Prete, & Zanda, 2007), and to investigate the opportunity of assessing usability measures via physiological signals (Foglia, Prete, & Zanda, 2008).

See Appendix Tables 5–8
                     
                     
                     
                     
                  


                     
                        
                           (1)
                           How often do you use the Internet? (Rarely, Once a month, once a week, Every day)

Why do you use the Internet? (Work, Study, Entertainment, Other)

Do you usually download software? (Never, At times, Frequently)

As a Internet user, have you ever seen the “Security Warning” dialog box? (Yes, No, Don’t remember)

How do you usually behave when confronted with a “Security Warning” dialog box? (Click Yes, Click No, It depends)

Do you usually pay attention to what is written in the “Security Warning” dialog box? (Yes, No, At times)

While deciding whether to accept or refuse a download, the publisher name matters to me. (Agree completely, Agree somewhat, Yes/no, Disagree somewhat, Disagree completely)

Do you trust more well-known publishers? (A lot, Not much, Yes/no, Little, Very little)

Did you notice publisher names similar to famous ones? (Yes, No)

How did you behave when dealing with publishers with a name similar to those of well-known ones? (Always accepted, Always refused, It depends)

Do you know what a premium rate connection is? (Yes, No)

Is it a risk for you that software download entails a charge? (A lot, Not much, Yes/no, Little, Very little)

Is it a risk for you that software usage implies a premium rate connection? (A lot, Not much, Yes/no, Little, Very little)

How much are you willing to spend to use a piece of software? (0 - I don’t want to spend, 3€, 10€, 30€ for full download, more than 30€ for full download)

Would you feel safer if software which entails a charge were guaranteed by well-known entities? (A lot, Not much, Yes/no, Little, Very little)

Before this test, had you already dealt with a Reputation System and feedback systems? (Yes, No)

How much were you influenced by the extra information provided by the reputation system? (A lot, Not much, Yes/no, Little, Very little)

How much do you trust a publisher with no reputation system? (A lot, Not much, Yes/no, Little, Very little)

Do you consider it useful to know the precise number of users who gave feedback comments? (A lot, Not much, Yes/no, Little, Very little)

Do you feel safer having a reputation system in the dialog box? (A lot, Not much, Yes/no, Little, Very little)

@&#REFERENCES@&#

