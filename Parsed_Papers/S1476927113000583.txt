@&#MAIN-TITLE@&#Computational intelligence techniques in bioinformatics

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           Present an overview of the computational intelligence (CI) techniques in bioinformatics.


                        
                        
                           
                           Shows how Computational intelligence techniques could be successfully employed to tackle various problems in bioinformatics


                        
                        
                           
                           Discuss some representative methods to provide inspiring examples to illustrate how CI can be utilized to address these problems and how bioinformatics data can be characterized by CI.


                        
                        
                           
                           Challenges to be addressed and future directions of research are also presented and an extensive bibliography is included.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

@&#ABSTRACT@&#


               
               
                  Computational intelligence (CI) is a well-established paradigm with current systems having many of the characteristics of biological computers and capable of performing a variety of tasks that are difficult to do using conventional techniques. It is a methodology involving adaptive mechanisms and/or an ability to learn that facilitate intelligent behavior in complex and changing environments, such that the system is perceived to possess one or more attributes of reason, such as generalization, discovery, association and abstraction. The objective of this article is to present to the CI and bioinformatics research communities some of the state-of-the-art in CI applications to bioinformatics and motivate research in new trend-setting directions. In this article, we present an overview of the CI techniques in bioinformatics. We will show how CI techniques including neural networks, restricted Boltzmann machine, deep belief network, fuzzy logic, rough sets, evolutionary algorithms (EA), genetic algorithms (GA), swarm intelligence, artificial immune systems and support vector machines, could be successfully employed to tackle various problems such as gene expression clustering and classification, protein sequence classification, gene selection, DNA fragment assembly, multiple sequence alignment, and protein function prediction and its structure. We discuss some representative methods to provide inspiring examples to illustrate how CI can be utilized to address these problems and how bioinformatics data can be characterized by CI. Challenges to be addressed and future directions of research are also presented and an extensive bibliography is included.
               
            

@&#INTRODUCTION@&#

During the past few decades we have experienced a massive growth in biological information gathered by the related scientific communities. A deluge of such information coming in the form of genomes, protein sequences, gene expression data and so on have led to the need for effective and efficient computational tools to store, analyze and interpret these data. Bioinformatics (Cios et al., 2005; Kelemen et al., 2008; Gusfield, 2004; Valentini et al., 2009; Smolinski et al., 2008a,b) involves the use of techniques including applied mathematics, informatics, statistics, computer science, artificial intelligence, chemistry, and biochemistry to solve biological problems, often on the molecular level. Major research efforts in the field include sequence alignment, gene finding, genome assembly, protein structure alignment, protein structure prediction, prediction of gene expressions and protein–protein interactions, and the modeling of evolution (Anon., 2013; Smolinski et al., 2008a,b). Hence, in other words, bioinformatics can be described as the application of computational methods to make biological discoveries (Baldi and Brunak, 1998; Mitra and Hayashi, 2006).

The ultimate attempt of the field is to develop new insights into the science of life as well as creating a global perspective, from which the unifying principles of biology can be derived (Altman et al., 2001). Cohen (2004) explained the needs of biologists to utilize and help interpret the vast amounts of data that are constantly being gathered in genomic research (e.g. there are at least 26 billion base pairs representing the various genomes available in the server of the National Center for Biotechnology Information (NCBI) (Zoheir Ezziane, 2006)). Cohen (2004) also pointed out the basic concepts in molecular cell biology, and outlined the nature of the existing data, and illustrated the algorithms needed to understand cell behavior. Methods from bioinformatics and computational biology are increasingly used to augment or leverage traditional laboratory and observation-based biology. These methods have become critical in biology due to recent changes in our ability and determination to acquire massive biological datasets, and due to the ubiquitous, successful biological insights that have come from the exploitation of those data. This transformation from a data-poor to a data-rich field began with DNA sequence data, but is now occurring in many other areas of biology such as mining hypermethylated genes in breast cancer tumor subtypes (Amin et al., 2012; Zoheir Ezziane, 2006; Smolinski et al., 2008a,b).

Recently, we have seen a new era of computational intelligence emerging that is focussing on the principles, theoretical aspects, and design methodology of algorithms gleaned from nature. Examples include artificial neural networks inspired by mammalian neural systems, evolutionary computation inspired by natural selection in biology, simulated annealing inspired by thermodynamics principles, and swarm intelligence inspired by collective behavior of insects or micro-organisms interacting locally with their environment causing coherent functional global patterns to emerge. Arenas et al. (2011) reviewed the adaptations of graphical processing unit (GPU) in scientific computing applications, particularly in the fields of computational biology and bioinformatics.

CI is a well-established paradigm with current systems having many of the characteristics of biological computers and capable of performing a variety of tasks that are difficult to do using conventional techniques. It is a methodology involving adaptive mechanisms and/or an ability to learn that facilitate intelligent behavior in complex and changing environments, such that the system is perceived to possess one or more attributes of reason, such as generalization, discovery, association and abstraction. These methodologies including technologies such as neural networks, fuzzy systems, rough sets, evolutionary computation, swarm intelligence, probabilistic reasoning and multi-agent systems. Recent trends aim to integration of different components to take advantage of complementary features and to develop synergistic systems leading to hybrid architectures such as neuro-fuzzy systems, genetic-fuzzy systems, evolutionary-fuzzy systems, evolutionary-neural networks, genetic programming neural networks or rough-neural, fuzzy-rough approaches for problem solving (Ritchie et al., 2007; Weyde and Dalinghaus, 2004; Hassanien and Slezak, 2006; Hassanien, 2007; Papageorgiou and Froelich, 2012).

The objective of this article is to present to the CI and bioinformatics research communities some of the state-of-the-art in CI applications to bioinformatics and motivate research in new trend-setting directions. Hence, we review and discuss some representative methods to provide inspiring examples to illustrate how CI techniques can be applied to solve bioinformatics problems and how bioinformatics data can be analyzed, processed, and characterized by CI. We want to stress that the literature in this domain is of course huge and that therefore the work that we include here is only exemplatory and focussing on recent advances, while many other interesting research approaches had to be omitted due to space limitations such as k-means clustering, decision trees, and case-based reasoning (Horng et al., 2009).

The rest of the article is organized as follows. Section 2 introduces the fundamental aspects of the key components of modern computational intelligence including including neural networks, restricted Boltzmann machine, deep belief network, fuzzy logic, rough sets, evolutionary algorithms (EA), genetic algorithms (GA), swarm intelligence, artificial immune systems and support vector machines. Sections 3 and 4 review some of the work that utilize computational intelligence for gene expression data clustering and classification respectively. An overview of work in computational intelligence based approaches in protein sequence classification is given in Section 5. Section 6 reviews and discusses some successful approaches to illustrate how CI can be applied to gene selection problems. CI in DNA fragment assembly and in multiple sequence alignment is discussed in Sections 7 and 8 respectively. Protein structure prediction is reviewed in Section 9. Challenges and future trends are presented in Section 10 and an extensive bibliography is provided.

In this section, we present an overview of modern CI techniques including artificial immune systems, neural networks, restricted Boltzmann machine, deep belief network, fuzzy logic, rough sets, EAs, GAs, swarm intelligence, and SVMs,

Artificial neural networks (ANN) have been developed as generalizations of mathematical models of biological nervous systems. In a simplified mathematical model of a neuron, the effects of the synapses are represented by connection weights that modulate the effect of the associated input signals, while the nonlinear characteristic exhibited by neurons is represented by a transfer function. Each neuron is characterized by an activity level (representing the state of polarization of a neuron), an output value (representing the firing rate of the neuron), a set of input connections (representing synapses on the cell and its dendrite), a bias value (representing an internal resting level of the neuron), and a set of output connections (representing a neuron's axonal projections). Each of these aspects of the unit is represented mathematically by real numbers. Thus, each connection has an associated weight (synaptic strength), which determines the effect of the incoming input on the activation level of the unit. The neuron impulse is hence computed as the weighted sum of the input signals, transformed by the transfer function. The learning capability of an artificial neuron is achieved by adjusting the weights in accordance with a chosen learning algorithm. The reader may refer to Haykin (1999), Bishop (1995), Motsinger et al. (2006) for an extensive overview of the artificial neural networks.

Restricted Boltzmann machine (RBM) is an energy-based undirected generative model that uses a layer of hidden variables to model a distribution over visible variables (Noulas and Kröse, 2008; Larochelle and Bengio, 2008). The undirected model for the interactions between the hidden and visible variables is used to ensure that the contribution of the likelihood term to the posterior over the hidden variables is approximately factorial which greatly facilitates inference. Energy-based model means that the probability distribution over the variables of interest is defined through an energy function. It is composed from a set of observable variables 
                           V
                           =
                           {
                           
                              v
                              i
                           
                           }
                         and a set of hidden variables H
                        ={h
                        
                           j
                        }, i node in the visible layer, j node in the hidden layer. It is restricted in the sense that there are no visible-visible or hidden-hidden connections.

The key idea behind training a deep belief network (DBN) by training a sequence of RBMs is that the model parameters θ, learned by an RBM define both 
                           p
                           (
                           v
                           ∣
                           h
                           ,
                           θ
                           )
                         and the prior distribution over hidden vectors, p(h
                        ∣
                        θ), so the probability of generating a visible vector, 
                           v
                        , can be written as:


                        
                           
                              (1)
                              
                                 p
                                 (
                                 v
                                 )
                                 =
                                 
                                    Σ
                                    h
                                 
                                 p
                                 (
                                 h
                                 ∣
                                 θ
                                 )
                                 .
                                 p
                                 (
                                 v
                                 ∣
                                 h
                                 ,
                                 θ
                                 )
                              
                           
                        After learning θ, 
                           p
                           (
                           v
                           ∣
                           h
                           ,
                           θ
                           )
                         is kept while p(h
                        ∣
                        θ) can be replaced by a better model that is learned by treating the hidden activity vectors H
                        =
                        h as the training data (visible layer) for another RBM. This replacement improves a variation lower bound on the probability of the training data under the composite model.

In case of not using class labels and back-propagation in the DBN architecture (unsupervised training) (Noulas and Kröse, 2008), DBN could be used as a feature extraction method for dimensionality reduction. On the other hand, when associating class labels with feature vectors, DBN is used for classification. There are two general types of DBN classifier architectures which are the back-propagation DBN (BP-DBN) and the associate memory DBN (AM-DBN) (Marchette, 1999). For both architectures, when the number of possible classes is very large and the distribution of frequencies for different classes is far from uniform, it may sometimes be advantageous to use a different encoding for the class targets than the standard one-of-K softmax encoding.

Kernel-based techniques (such as SVM, Bayes point machines, kernel principal component analysis, and Gaussian processes) represent a major development in machine learning and computational intelligence algorithms. SVMs were first suggested by Vapnik in the 1960s for classification and have recently become an area of intense research owing to developments in the techniques and theory coupled with extensions to regression and density estimation (Vapnik, 1998; Burges, 1998).

SVM are a group of supervised learning methods that can be applied to classification or regression. Classification is achieved by a linear or nonlinear separating surface in the input space of the dataset. SVM delivers state-of-the-art performance in real-world applications such as text categorization, hand-written character recognition, image classification, bio-sequences analysis, etc., and is now established as one of the standard tools for CI and data mining. It employs kernels to map the input data into some much higher dimensional feature space implicitly in which data becomes linear separable. The linear decision boundary is drawn in a manner that the margin, minimum distance between training examples to the boundary, is maximized. In case that the mapped data points are linearly inseparable, a cost is included to account for the wrongly classified examples and the margin is maximized together with minimizing the cost.


                        Zadeh (1965) introduced the concept of fuzzy logic to present vagueness in linguistics, and to implement and express human knowledge and inference capability in a natural way. Fuzzy logic starts with the concept of fuzzy sets. A fuzzy set is a set without a crisp, clearly defined boundary and can contain elements with only a partial degree of membership. A membership function defines how each point in the input space is mapped to a membership value (or degree of membership) between 0 and 1. Let X be the input space and x be a generic element of X. A classical set A is defined as a collection of elements or objects x
                        ∈
                        X, such that each x can either belong or not belong to the set A, A
                        ⊑
                        X. By defining a membership function on each element x in X, a classical set A can be represented by a set of ordered pairs (x, 0) or (x, 1), where 1 indicates membership and 0 non-membership. Unlike conventional sets, fuzzy set express the degree to which an element belongs to a set. Hence, the membership function of a fuzzy set can take on a value between 0 and 1, denoting the degree of membership of an element in a given set. The reader may refer to Nguyen and Walker (1999) for an extensive overview of the fuzzy logic.

Rough set theory (Pawlak, 1991, 1982; Pawlak et al., 1995; Polkowski, 2003) is a fairly new intelligent technique for managing uncertainty that has can be used for the discovery of data dependencies, evaluation of the importance of attributes, discovery of patterns in data, reduction of attributes, and the extraction of rules from databases. Such rules have the potential to reveal new patterns in the data and can also collectively function as a classifier for unseen datasets. Unlike other computational intelligence techniques, rough set analysis requires no external parameters and uses only the information present in the given data. One of the interesting features of rough sets theory is that it can tell whether the data is complete or not based on the data itself. If the data is incomplete, it suggests more information about the objects to be collected in order to build a good classification model. On the other hand, if the data is complete, rough sets can determine the minimum data needed for classification. This property of rough sets is important for applications where domain knowledge is limited or data collection is very expensive/laborious because it makes sure the data collected is just good enough to build a good classification model without sacrificing the accuracy of the classification model or wasting time and effort to gather extra information about the objects (Pawlak, 1991, 1982; Pawlak et al., 1995; Polkowski, 2003).

Evolutionary algorithms (EAs) are adaptive methods, which can be used to solve search and optimization problems, based on genetic processes of biological organisms. Over many generations, natural populations evolve according to the principles of natural selection and “survival of the fittest”. By mimicking this process, evolutionary algorithms are able to “evolve” solutions to real world problems, if they have been suitably encoded (Fogel, 1999). Usually grouped under the term evolutionary algorithms or evolutionary computation, we find the domains of genetic algorithms (Holland, 1975), evolution strategies (Back, 1996), evolutionary programming (Fogel et al., 1967), genetic programming (Koza, 1992), and learning classifier systems (Smolinski et al., 2008a). They all share a common conceptual base of simulating the evolution of individual structures via processes of selection, mutation, and reproduction.

These processes depend on the perceived performance of the individual structures as defined by the environment. EAs deal with parameters of finite length, which are coded using a finite alphabet, rather than directly manipulating the parameters themselves. This means that the search is unconstrained neither by the continuity of the function under investigation, nor the existence of a derivative function.

Genetic algorithm (GA) is an adaptive search technique initially introduced by Holland (Mohamed et al., 2009). It is a stochastic search method for solving optimal solutions within large and complicated search spaces. Genetic algorithm operates on a set of individuals called population, where each individual is an encoding of the problem's input data and are called chromosomes. Each chromosome is composed of genes, each of them has a binary value that indicates the presence or not of a specific element of the set. The search for the best solution is guided by an objective function called fitness function. The selected solutions of higher fitness function are more ability to produce new solutions than the less of fitness value while those of weak fitness function will be eliminated gradually. Fitness function controls the selection of best solution and provides criteria to evaluate the candidate individuals. In general, GA includes three fundamental operators: selection, crossover and mutation within chromosomes. A population is created with a group of randomly individuals. The individuals in the population are then evaluated by fitness function. Two individuals (off-spring) are selected for the next generation based on their fitness.

Crossover is a process yielding recombination of bit strings via an exchange of segments between pairs of chromosomes to create the new individuals. Finally, mutation has the effect of ensuring that all possible chromosomes are reachable or a certain number of generations have passed. We have to note that GAs is computational model designed to simulate the evolutionary processes in the nature Hinton (2006). The reader may refer to Imade et al. (2003), Jiao and Li (2008), Glen and Payne (1995), Venkatasubramanian et al. (1995), Goldberg (1989) for an extensive overview of the GAs.

The artificial immune systems (AISs) were inspired by the human immune system (HIS) which is robust, decentralized, error tolerant, and adaptive. The HIS has different cells with so many different tasks, so the resultant mimic algorithms give differing levels of complexity and can accomplish a range of tasks. Its emerged in the 1990s as a new computational research area and are linking several emerging computational fields inspired by biological behavior such as artificial neural networks and artificial life (Perelson and Oster, 1979). There are a number of AIS models used in assembly sequence planning exploration, prediction of protein cellular localization and a variety of other applications in the field of bioinformatics. The immunity system is a system of biological structures and processes within an organism to resist infection from micro organisms or viruses, especially as a result of antibody formation. The viruses or organisms that trigger the immune system to react are defined as an antigen. There exists no single algorithm from which all immune algorithms are derived, as AISs are designed using a number of algorithms. The negative selection approach (NSA) explains how T-cells are being selected and their maturation in the system. T-cells are blood cells that belong to a group of white blood cells called lymphocytes. In the NSA, whenever the T-Cells are produced, they undergo an immaturely period to learn which antigen recognition results in their death. The T-cells need activation to develop the ability to remove pathogens. They are exposed to a comprehensive sample of self antigens, then they are tested against self and non-self antigens to match the non-self ones. If a T-Cell matched a self antigen, it is then removed until they are mature and released to the system, the reader may refer to Perelson and Oster (1979), Zhou (2009), Timmis et al. (2000), Wang et al. (2008) for an extensive overview of the artificial immune system.

Swarm intelligence is aimed at collective behavior of intelligent agents in decentralized systems. Although there is typically no centralized control dictating the behavior of the agents, local interactions among agents often cause a global pattern to merge. Most of the basic ideas are derived from real swarms in nature, which includes ant colonies, bird flocking, honeybees, bacteria and microorganisms. Swarm models are population-based and the population is initialized with a population of potential solutions. These individuals are then optimized over many iterations using several heuristics inspired from the social behavior of insects in an effort to find the optimal solution (Kennedy et al., 2001; Kennedy and Eberhart, 1948).

Ant colony optimization (ACO) has been successfully applied to solve various engineering optimization problems. Ant colony algorithms are inspired by the behavior of natural ant colonies, in the sense that they solve their problems by multi-agent cooperation using indirect communication through modifications in the environment. Ants release a certain amount of pheromone while walking, and each ant prefers (probabilistically) to follow a direction, which is rich of pheromone. This simple behavior explains why ants are able to adjust to changes in the environment, such as optimizing the shortest path to a food source or a nest. In ACO, ants use information collected during past simulations to direct their search and this information is available and modified through the environment. ACO algorithms can also be used for clustering datasets and to optimism rule bases. The reader may refer to Blum (2005), Parpinelli and Lopes (2011), Kentzoglanakis and Poole (2011), Das et al. (2008), Kennedy et al. (2001), Kennedy and Eberhart (1948).

The concept of particle swarms, although initially introduced for simulating human social behaviors, has become very popular these days as an efficient search and optimization technique. Particle swarm optimization (PSO) (Kennedy and Eberhart, 1948), does not require any gradient information of the function to be optimized, uses only primitive mathematical operators and is conceptually very simple. PSO has attracted the attention of a lot of researchers resulting into a large number of variants of the basic algorithm as well as many parameter automation strategies. We have to note that PSO is mainly used for continuous optimization while ACO is mainly used for combinatorial optimization.

Gene expressions involve a process through which the coded information of a gene is converted into structures operating in the cell. It provides the physical evidence that a gene has been turned on or activated. Expressed genes include those that are transcribed into m-RNA and then translated into protein and those that are transcribed into RNA but not translated into protein (Luscombe et al., 2001). The expression levels of thousands of genes can be measured at the same time using the modern microarray technology (Quackenbush, 2001; Anon., 2002; Aaron and James, 2010; Hwang et al., 2000; Gruzdz et al., 2006).

Recently developed CI approaches for gene expressions clustering include neural networks (Yuhui et al., 2002), fuzzy sets (Futschik and Kasabov, 2002; Arima et al., 2008), rough sets (Midelfart et al., 2002; Slezak and Wroblewski, 2007, 2006), self-organising tree (Herrero et al., 2001) and various techniques (Roberto and Giorgio, 2009; Okada et al., 2005; Mitra, 2004).

Du et al., in Zhihua and Zhen (2008) used particle-pair optimizer (PPO) as a variation on the traditional PSO algorithm, which is stochastic particle-pair based optimization technique. In this article authors bridges PPO and k-means within the algorithm k-means for the first time. K-means performs a PPO-based search reduces the probability of becoming trapped in a local minimum, in particular when the local minimum is close to the initial centroid. Results of the proposed method are compared to k-means and fuzzy k-means.

Yuhui et al. (Yuhui et al., 2002; Arima et al., 2008) developed an associative clustering neural network to the analysis of gene expression data. The developed approach evaluates similarities between any two gene samples through the interactions of a group of gene samples. It provides more robust performance compared with the similarities evaluated by direct distances. The performance of the developed approach has been tested on the Leukemia dataset and results demonstrate that the associative clustering neural networks can achieve good performance in high dimensional data. They also reported that performance can be further enhanced when some useful feature selection methodologies are incorporated. The reader may refer to Mahanta et al. (2012) for feature extraction techniques.


                     Arima et al. (2008) introduced a modified fuzzy gap statistic algorithm for estimating the number of clusters. Two gene expression datasets with noise were used to test their algorithm. The fuzzy gap statistic algorithms showed a higher performance in terms of robustness against noise for estimating the optimal number of clusters. It was reported that the proposed fuzzy gap statistic algorithm is a useful algorithm for estimating the number of clusters for microarray datasets.


                     Xiao et al. (2003) further proposed a hybrid approach based on the synergism between particle swarm optimization and self-organizing maps for clustering and achieved promising results over the gene expression data of yeast and rat hepatocytes. They also used particle swarm optimization to evolve the weights for self-organizing maps. Self-organizing maps are first used to cluster the dataset. Conscience is added to self-organizing maps to obtain a better approximation of the pattern distribution. Then, particle swarm optimization is initialized with the weights produced by self-organizing maps in the first stage. Then, a particle swarm optimization algorithm was used to refine the clustering process and to improve the clustering result by evolving the population of particles. The proposed hybrid approach has been shown to be highly successful on a range of simulated data.

One application of microarray technology is cancer studies where supervised learning may be used for predicting tumor subtypes and clinical parameters. Midelfart et al. (2002) presented a general rough set approach for classification of tumor samples analysed with microarrays. This approach is tested on a dataset of gastric tumors, and authors developed classifiers for six clinical parameters. Their results show that it is possible to develop classifiers with a small number of tumor samples. They reported that rough set based learning combined with feature selection such as principle component analysis (PCA) may become an important tool for microarray analysis.

In bioinformatics, several ensemble clustering approaches have been proposed for the analysis of gene expression data (Hu and Yoo, 2004; Yu et al., 2007; Roberto and Giorgio, 2009). Roberto and Giorgio (2009) proposed new strategies and development clustering methods that combines the accuracy and the effectiveness of the ensemble clustering techniques based on random projections, with the expressive capacity of fuzzy sets, to obtain clustering algorithms both reliable and able to express the uncertainty underlying the assignment of examples to clusters in the context of gene expression data analysis. The experimental results show that the proposed fuzzy ensemble approach is competitive with other ensemble methods and it may be successfully applied to the analysis of gene expression data, even when considering datasets with a single crisp label for each example.


                     Okada et al. (2005) point out that although hierarchical clustering has been extensively used in analysing expression patterns in microarray gene expression data, their biological interpretations are not easy to determine by analysing the resulting clusters. The authors proposed a novel algorithm that automatically finds biologically interpretable cluster boundaries in hierarchical clustering by referring to gene annotations stored in public genome databases. In addition, the proposed algorithm has a new function of generating a set of clusters that are independent of each other with respect to the distributions of gene functions. The authors claim that this function would enable investigators to efficiently identify non-redundant and biologically-independent clusters.

Biclustering techniques (Nepomuceno et al., 2011) have been widely adapted for analyzing microarray gene expression data due to its ability to extract local patterns with a subset of genes that are similarly expressed over a subset of samples. Abohamad et al. (2010) presented a clonal selection algorithm for biclustering that incorporates these greedy searching procedures as local search heuristics in an immune-inspired algorithm. The quality of biclusters has been demonstrated by experimentation on a well known benchmark dataset. A comparative analysis between other related local search-based methods and immune inspired algorithms has been done and the obtained results showed that the proposed algorithm outperforms other algorithms in terms of bicluster size and mean-squared residue.

Microarray classification has a broad variety of biomedical applications. Tasoulis et al. (2008) study and compare various computational intelligence approaches such as neural networks, EAs, and clustering algorithms, then demonstrate their applicability as well as their weaknesses and shortcomings to efficient DNA microarray data analysis. SVMs have emerged as a powerful and popular classifier for microarray data. At the same time, there is increasing interest in the development of methods for identifying important features in microarray data. Many of these methods use SVM classifiers either directly in the search for good features or indirectly as a measure of dissociating classes of microarray samples. Shan-Wen et al. (2010) proposed a novel approach for tumor classification based on wavelet packet transforms for feature extraction and neighborhood rough sets for feature reduction. the experimental results are done on three gene expression datasets. A study is done in Bo et al. (2010) to project the gene expression data into a subspace with high intra-class compactness and inter-class separability. Experimental results were done using several DNA microarray datasets.

Thaut (Peterson and Thaut, 2004) presents a study that describes empirical results in model selection for SVM classification of DNA microarray data. The authors demonstrate that classifier performance is very sensitive to the SVM's kernel and model parameters, and also demonstrate that the optimal model parameters depend on the cardinality of feature subsets and can influence the evolution of a genetic search for good feature subsets. Their results suggest that application of SVM classifiers to microarray data should include careful consideration of the space of possible SVM parameters. The results also suggest that feature selection search and model selection should be conducted jointly rather than independently.

Heterogeneous types of gene expressions may provide a better insight into the biological role of gene interaction with the environment, disease development and drug effect at the molecular level. Liang and Kelemen (2008) proposed a time lagged recurrent neural network with trajectory learning for identifying and classifying the gene functional patterns from the heterogeneous nonlinear time series microarray experiments. The proposed procedures identify gene functional patterns from the dynamics of a state-trajectory learned in the heterogeneous time series and the gradient information over time. Also, the trajectory learning with back-propagation through time algorithm can recognize gene expression patterns vary over time. This may reveal much more information about the regulatory network underlying gene expressions. The analyzed data were extracted from spotted DNA microarrays in the budding yeast expression measurements, produced by Eisen et al. (1998). The gene matrix contained 79 experiments over a variety of heterogeneous experiment conditions. The number of recognized gene patterns in their study ranged from two to ten were divided into three cases. Optimal network architectures with different memory structures were selected based on Akaike and Bayesian information criteria using two-way factorial design. The optimal model performance was compared to other popular gene classification algorithms, such as nearest neighbor search, support vector machines, and self-organizing maps. The reliability of the performance was verified with multiple iterated runs.

Efficient and reliable methods that can find a small sample of informative genes amongst thousands are of great importance. In this area, much research is investigating the combination of advanced search strategies (to find subsets of features), and classification methods. Juliusdottir et al. (2005) investigate a simple evolutionary algorithm/classifier combination on two microarray cancer datasets, where this combination is applied twice: once for feature selection, and once for further selection and classification. They demonstrate that a simple EA/classifier combination is capable of good feature discovery and classification performance with no initial dimensionality reduction, and that a simple repeated EA/K-NN (k-nearest neighbor) approach is capable of competitive or better performance than methods using more sophisticated preprocessing and classifier methods.


                     Lin et al. (2006) proposed a genetic algorithm with silhouette statistics as discriminant function (GASS) for gene selection and pattern recognition. The proposed method evaluates gene expression patterns for discriminating heterogeneous cancers. Distance metrics and classification rules have also been analyzed to design a GASS with high classification accuracy. Moreover, the proposed method is compared to previously published methods. Various experimental results show that the method is effective for classifying the NCI60, the GCM and the SRBCTs datasets. Moreover, GASS outperforms other existing methods in both the leave-one-out cross validations and the independent test for novel data.

Identification of short DNA sequence motifs that serve as binding targets for transcription factors is an important challenge in bioinformatics. Unsupervised techniques from the statistical learning theory literature have often been applied to motif discovery, but effective solutions for large genomic datasets have yet to be found. Mahonya et al. (2006) present three self-organizing neural networks that have applicability to the motif-finding problem. The core system in this study is a previously described SOM-based motif-finder named SOMBRERO. The motif-finder is integrated in this work with a SOM-based method that automatically constructs generalized models for structurally related motifs and initializes SOMBRERO with relevant biological knowledge. A self-organizing tree method that displays the relationships between various motifs is also presented, and it is shown that such a method can act as an effective structural classifier of novel motifs. The performance of the three self-organizing neural networks is evaluated and analyzed using various datasets.

Identifying and pruning redundant genes and samples simultaneously can improve the performance of classification and circumvent the local optima problem. Shen et al. (2009) proposed a modified particle swarm optimization to select optimal genes and samples simultaneously and SVM was used as an objective function to determine the optimum set of genes and samples.

The problem of protein sequence classification is one of the crucial tasks in the interpretation of genomic data. Many high-throughput systems were developed with the aim of categorizing the proteins based only on their sequences. However, modeling how the proteins have evolved can also help in the classification task of sequenced data. Hence the phylogenetic analysis has gained importance in the field of protein classifications (Blanco et al., 2002).


                     Busa-Fekete et al. (2008) provided an overview about the problem of protein sequence classification area, and propose two algorithms well suited for this which are based on a weighted binary tree representation of protein similarity data. The first one is called TreeInsert which assigns the class label to the query by determining a minimum cost necessary to insert the query in the (precomputed) trees representing the various classes. Then the TreNN algorithm assigns the label to the query based on an analysis of the query's neighborhood within a binary tree containing members of the known classes. The two algorithms were tested in combination with various sequence similarity scoring methods such as Smith–Waterman and local alignment kernel.

Mapping the pathways that give rise to metastasis is one of the key challenges of breast cancer research. Recently, several large-scale studies have shed light on this problem through analysis of gene expression profiles to identify markers correlated with metastasis. Chuang et al. (2007) applied a protein-network-based approach that identifies markers as subnetworks extracted from protein interaction databases. The resulting subnetworks provide novel hypotheses for pathways involved in tumor progression. They also find that the subnetwork markers are more reproducible compared with individual marker genes selected without network information. The proposed protein-network-based approach has been shown to be highly successful in the classification of metastatic versus non-metastatic tumors.


                     Erten et al. (2011) proposed a topological similarity based disease gene prioritization scheme, through the developed measure of topological similarity among pairs to assess the topological similarity of proteins in a protein–protein interactions (PPI) network and use the network similarity between seed and candidate proteins to infer the likelihood of disease association for candidates. They employed this measure to develop algorithms that prioritize candidate disease genes based on the topological similarity of their products and the products of known disease genes. Also, they integrated PPI network and the Online Mendelian Inheritance (OMIM) database to verify the performance the proposed algorithm.

Selecting informative and discriminative genes from huge microarray gene expression data is an important and challenging bioinformatics research topic. Focussing on the gene selection and extraction problem, many successful works have been addressed and discussed (Banerjee et al., 2007; Li and Zhang, 2006; Feng et al., 2004). In addition, Fernando et al. (2006) demonstrate how a supervised fuzzy pattern algorithm can be used to perform DNA microarray data reduction over real data. The benefits of their method can be employed to find biologically significant insights relating to meaningful genes in order to improve previous successful techniques. Experimental results on acute myeloid leukemia diagnosis show the effectiveness of the proposed approach.

In Huawen et al. (2010) an ensemble gene selection method is proposed to choose multiple gene subsets for classification purposes, where the significant degree of gene is measured by conditional mutual information or its normalized form. After different gene subsets have been obtained by setting different starting points of the search procedure, they are used to train multiple base classifiers and then aggregated into a consensus classifier by the manner of majority voting. The proposed method is compared with many popular gene selection methods on several public microarray datasets.

Based on rough set theory, Pradipta and Sushmita (2011) proposed a feature selection algorithm. It selects a set of genes from microarray data by maximizing the relevance and significance of the selected genes. A theoretical analysis is presented to justify the use of both relevance and significance criteria for selecting a reduced gene set with high predictive accuracy.

A new method combining correlation based clustering and rough sets attribute reduction together for gene selection from gene expression data is proposed by Sun et al. (2007). Three different classification algorithms has been employed to evaluate the performance of the proposed method. The algorithm has demonstrated using two public gene expression datasets and showed that high classification accuracies achieved and the introduced method is successful for selecting high discriminative genes for classification task. The generic approach to cancer classification based on gene expression data is important for accurate cancer diagnosis, instead of using all genes in the dataset.


                     He et al. (2006) proposed a fuzzy-granular method for the gene selection task. The genes are grouped into different function granules using a fuzzy c-means algorithm, then informative genes in each cluster are selected with based on the signal to noise metric. As a result, more informative genes for cancer classification are selected and more accurate classifiers can be modeled. The simulation results on two publicly available microarray expression datasets show that the proposed hybrid method is more accurate than traditional algorithms for cancer classification.


                     Khan et al. (2001) developed a artificial neural networks method for classifying cancers to specific diagnostic categories based on their gene expression signatures. Then, they trained the networks using small, round blue-cell tumors as a model. These cancers belong to four distinct diagnostic categories and often present diagnostic dilemmas in clinical practice. The neural network correctly classified all samples and identified the genes most relevant to the classification. This study demonstrates the potential applications of the developed method for tumor diagnosis and the identification of candidate targets for therapy.


                     Ruffino et al. (2007) proposed a new gene selection method for analyzing microarray experiments. The new technique is based on switching neural networks (WNN), learning machines that assign a relevance value to each input variable, and adopts recursive feature addition (RFA) for performing gene selection. The performances of proposed algorithm are evaluated by considering its application on two real and two artificial gene expression datasets generated according to a proper mathematical model that possesses biological and statistical plausibility. A a comparison with other two widely used gene selection methods, namely the signal to noise ratio (S2N) and support vector machines with recursive feature elimination (SVM-RFE), has been performed. SNN-RFA achieves the best performances and arriving to determine the whole collection of relevant genes in one of the three artificial datasets. The S2N method exhibits a quality similar to that of SNN-RFA, whereas SVMRFE shows the worst behavior.

DNA fragment assembly is a problem to be solved in the early phases of the genome project and thus is very important since the other steps depend on its accuracy. It is an NP-hard combinatorial optimization problem which is growing in importance and complexity as more research centers become involved on sequencing new genomes. Various heuristics have been designed, but as it represents a crucial part of any sequencing project, better assemblers are needed. For example, Enrique and Gabriel (2007) proposed and study and discuss a new features (characteristics) of a new heuristic algorithm for the DNA fragment assembly problem.

Authors in Shahla et al. (2009) proposed a novel hybridization feature selection algorithm that combines genetic algorithms and ACO. Experimentation is carried out using two challenging biological datasets, involving the hierarchical functional classification of GPCRs and enzymes. The criteria used for comparison are maximizing predictive accuracy, and finding the smallest subset of features.


                     Wannasak et al. (2006) presented the use of a combined ant colony system (ACS) and nearest neighbor heuristic (NNH) algorithm in DNA fragment assembly. The aim is to find the right order of each fragment in the ordering sequence that leads to the formation of a consensus sequence that truly reflects the original DNA strands. The assembly procedure proposed is composed of two stages: fragment assembly and contiguous sequence (contig) assembly. In the fragment assembly stage, a possible alignment between fragments is determined where the fragment ordering sequence is created using the ACS algorithm. The resulting contigs are then assembled together using the NNH rule. Their results indicate that in overall the performance of the combined ACS/NNH technique is superior to that of a standard sequence assembly program (CAP3), which is widely used by many genomic institutions.


                     Angeleri et al. (1999) described an alternative approach to the fragment assembly problem. The key idea is to train a recurrent neural network to tracking the sequence of bases constituting a given fragment and to assign to a same cluster all the sequences which are well tracked by this network. Authors make use of a 3-layer recurrent perceptron and examine both edited sequences and artificial fragments from a common simulation soft- ware: the clusters they obtain exhibit interesting properties in terms of error filtering, stability and self-consistency; they define as well, with a certain degree of approximation, a metric on the fragment set.

Sequence alignment refers to the process of arranging the primary sequences of DNA, RNA, or protein to identify regions of similarity that may be a consequence of functional, structural, or evolutionary relationships between the sequences.

Given two sequences X and Y, a pair-wise alignment indicates positions of each sequence that are considered to be functionally or evolutionarily related. From a family S
                     =(S
                     0, S
                     1, …, S
                     
                        N−1) of N sequences, we would like to find out the common patterns of this family. Since aligning each pair of sequences from S separately often does not reveal the common information, it is necessary to perform multiple sequence alignment (MSA). In general, the input set of query sequences are assumed to have an evolutionary relationship by which they share a linkage and are descended from a common ancestor (Chen et al., 2006).

To evaluate the quality of an alignment, a popular choice is to use the sum of pairs (SP) score method (Carrillo and David, 1988). The SP score basically sums the substitution scores of all possible pair-wise combinations of sequence characters in one column of a multiple sequence alignment. Assuming c
                     
                        i
                      representing the i-th character of a given column in the sequence matrix and match (c
                     
                        i
                     , c
                     
                        j
                     ) denoting the comparing score between characters c
                     
                        i
                      and c
                     
                        j
                     .

Progressive alignment is a widely used heuristic MSA method that does not guarantee any level of optimality (Feng and Doolittle, 1987). Clustal W in Thompson et al. (1994) is another widely popular program that improved the algorithm presented by citetfeng. The main shortcoming of this method is that once a sequence has been aligned, that alignment can never be modified even if it conflicts with sequences added later.


                     Fasheng and Yuehui (2009) designed an improved PSO to solve MSA. In the algorithm, each particle represents an alignment and flies to the particle which has the best solution by some rules. Moreover, in order to expand the diversity of the algorithm and enhance the possibility of finding the optimal solution. The proposed approach was tested using some protein families and compared with the alignments generated by the Clustal X 2.0 algorithm. Authors in Hai-Xia et al. (2009) present a novel algorithm of binary PSO for MSA solving. The approach is examined by using a set of standard instances taken from the benchmark alignment database, BALIBASE. Numerical simulation results are compared with four leading multiple alignment systems and show the superiority of the proposed algorithm.

Genetic algorithms have been shown to be successful in multiple sequences alignment problems. Zhang and Huang (2004) proposed an improved GA method, multiple small-popsize initialization strategy (MSPIS) and hybrid one-point crossover scheme (HOPCS) based GA, which can search the solution space in a very efficient manner. The experimental results show that their improved approach can obtain a better result compared with traditional GA approach in aligning multiple protein sequences problem.

DNA matching is a crucial step in sequence alignment. Since sequence alignment is an approximate matching process there is a need for good approximate algorithms. The process of matching in sequence alignment is generally finding longest common subsequences. However, finding longest common subsequence may not be the best solution for either a database match or an assembly. An optimal alignment of subsequences is based on several factors, such as quality of bases, length of overlap, etc. (Smolinski et al., 2008a) and (Smolinski et al., 2008b).


                     Nasser et al. (2007) proposed a fuzzy logic for approximate matching of subsequences. First, fuzzy characteristic functions are derived for parameters that influence a match. then developed fuzzy assembler to work with low quality data which is generally rejected by most of the existing techniques. Experimental results show the effectiveness of the proposed approach.

In multiple DNA sequence alignment, some researchers used divide-and-conquer techniques to cut the sequences for decreasing the space complexity for sequence alignment. Because the cutting points of sequences of the existing methods are fixed at the middle or the near-middle points, the performance of sequence alignment of the existing methods is not good enough  (Smolinski et al., 2008a,b).


                     Chen et al. (2005) presented genetic algorithms and divide-and-conquer techniques for multiple DNA sequence alignment to choose optimal cut points of multiple DNA sequences. The experimental results show that the presented technique is better when dealing with multiple DNA sequence alignment.


                     Layeb and Deneche (2007) proposed a new iterative approach for multiple sequence alignment problem. The multiple sequence alignment problem is viewed as an optimization problem for which a new framework relying on natural computing The used the BAUBASE benchmark database to test and evaluate their proposed algorithm and the experimental results showed the effectiveness of the proposed framework and its ability to achieve good quality solutions comparing to other existing approaches.

Protein structure prediction is one of the most important goals pursued by bioinformatics and theoretical chemistry. Its aim the prediction of the three-dimensional structure of proteins from their amino acid sequences, sometimes including additional relevant information such as the structures of related proteins  (Anon., 2013). In other words, it deals with the prediction of a protein's tertiary structure from its primary structure. Protein structure prediction is of high importance in medicine (for example, in drug design) and biotechnology (for example, in the design of novel enzymes). Focussing on this problem, many successful works have been addressed and discussed in Marcio and Osmar (2010).


                     Tang et al. (2005) address the problem of predicting protein homology between given two proteins. They propose a learning method that combines the idea of association rules with their previous method called granular support vector machines (GSVM), which systematically combines SVM with granular computing. Their method, called GSVM-AR, uses association rules with high enough confidence and significant support to find suitable granules to build a GSVM with good performance. The authors compared their method with SVM by KDDCUP04 protein homology prediction data. From the experimental results, GSVM-AR showed significant improvement compared to building a single SVM.


                     Unger (2004) presents a general framework of how genetic algorithms can be used for structure prediction problem. Using this framework, the significant studies that were published in recent years are discussed and compared. Applications of genetic algorithms to the related question of protein alignments are also mentioned. The rationale of why genetic algorithms are suitable for protein structure prediction is presented, and future improvements that are still needed are discussed.


                     Pan (2005) shows how to use machine learning methods with various advanced encoding schemes and classifiers improve the accuracy of protein structure prediction. The explanation of how a decision is made is also important for improving protein structure prediction. The reasonable interpretation is not only useful to guide the “wet experiments”, but also the extracted rules are helpful to integrate computational intelligence with symbolic AI systems for advanced deduction. Results using support vector machines and decision trees for rule extraction and prediction interpretation were presented.


                     Yang et al. (2011) proposed a cellular sorting structure for better prediction of protein subcellular location, They integrated the interdependencies between subcellular locations with support vector machines for prediction of protein subcellular localization. The authors highlighted the drawbacks of traditional prediction systems which utilizes a flat structure of classifiers, such as the one-versus-all and one-versus-one schemes, with amino acid compositions to perform the prediction. Most of these methods ignore the interdependencies between subcellular locations. Yang et al. (2011) also utilized advantages of a hierarchical structure to organize the subcellular locations and model their relationships, through proposing four types of hierarchical prediction methods. Experimental results show that three of the hierarchical models outperformed the traditional flat model in terms of tree loss values for all evaluation measures. Some valuable insights into the sorting process by using hierarchical structures were gained with the proposed scheme.


                     Morgado et al. (2011) proposed a machine learning architecture based on SVM binary models and a neural network (NN) to handle the large multiclass problem of protein superfamily prediction. They include a parallelism mechanism through a multi-agent strategy to reduce the total processing time when getting a prediction for a new query protein. The efficiency of the algorithm and the advantages of the parallelization were verified.


                     Nguyen et al. (2011) introduced a novel algorithm for annotating protein function comprising Naïve Bayes and association rules. They employed advantage of the underlying topology in protein–protein interaction (PPI) networks and the structure of graphs in the Gene Ontology. Predicted functions were analyzed using association rules to discover relationships among the assigned functions, i.e., when one set of functions occurs in a protein, then the protein may be annotated with an additional set of other specific functions at some confidence level. The proposed algorithm was tested on the Human Protein Reference Database (HPRD), and compared with the majority and χ
                     2 statistics algorithms. Obtained results showed that the proposed algorithm predicts protein functions with significantly higher recall with no loss of precision.

Computational intelligence (CI) has increasingly gained attention in bioinformatics research. classify and mine their databases. At present, with various intelligent methods available in the literature, researchers are facing difficulties in choosing the best method that could be applied to a specific dataset. Researchers need tools, which present the data in a comprehensible fashion, annotated with context, estimates of accuracy and explanation. The main purpose of this article was to present to the computational intelligence and bioinformatics research communities some of the state-of-the-art and recent advances in CI applications to bioinformatics, and to inspire further research and development on new applications and new concepts in new trend-setting directions.

Rough set theory encompasses an extensive group of methods that have been applied in the bioinformatics domain and that are used for the discovery of data dependencies, importance of features, patterns in sample data, and feature space dimensionality reduction. Most of the current literature on rough-set-based methods for bioinformatics data focuses on classification and dimensionality reduction issues. A number of papers also deal with bioinformatics problems such as protein structure prediction, gene selection, and protein sequence classification. From what has been presented in the literature, it is obvious that the rough set approach provides a promising means of solving a number of bioinformatics problems. It should be observed that rough set by itself or in combination with other computational intelligence technologies work remarkably well in many bioinformatics problems. The challenge now is to develop CI-based methods that offer an approach to classifying perceptual objects by means of features. It is fairly apparent that CI methods can be useful in biomedical object recognition, especially in solving bioinformatics problems.

A combination of various computational intelligence technologies in bioinformatics has become one of the most promising avenues in bioinformatics research. From the perspective of CI, further explorations into possible hybridizations of CI technologies are necessary to build a more complete picture of CI-set-based applications in bioinformatics such as neural-fuzzy, neural-rough, EAs-GAs and other hybridizations of different CI technologies. What can be said at this point is that the CI approaches pave the way for new and interesting avenues of research in bioinformatics and represent an important challenge for researchers.

@&#REFERENCES@&#

