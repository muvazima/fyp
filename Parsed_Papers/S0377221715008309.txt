@&#MAIN-TITLE@&#A mean-shift algorithm for large-scale planar maximal covering location problems

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           The CIPS method is computational expensive in removing potential locations.


                        
                        
                           
                           The mean-shift (MS) algorithm is introduced to optimize coverage maximization.


                        
                        
                           
                           The MS has been revised for large-scale PMCLP with point-based demand.


                        
                        
                           
                           We test the performance of MSMC against the CIPS approach on many data sets.


                        
                        
                           
                           Results illustrate MSMC’s outstanding performance in tackling large-scale PMCLPs.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Location

Large scale optimization

Planar maximal covering location problem

Mean shift

@&#ABSTRACT@&#


               
               
                  The planar maximal covering location problem (PMCLP) concerns the placement of a given number of facilities anywhere on a plane to maximize coverage. Solving PMCLP requires identifying a candidate locations set (CLS) on the plane before reducing it to the relatively simple maximal covering location problem (MCLP). The techniques for identifying the CLS have been mostly dominated by the well-known circle intersect points set (CIPS) method. In this paper we first review PMCLP, and then discuss the advantages and weaknesses of the CIPS approach. We then present a mean-shift based algorithm for treating large-scale PMCLPs, i.e., MSMC. We test the performance of MSMC against the CIPS approach on randomly generated data sets that vary in size and distribution pattern. The experimental results illustrate MSMC’s outstanding performance in tackling large-scale PMCLPs.
               
            

@&#INTRODUCTION@&#

Covering problems in facility location have received considerable research interest due to its applicability in the real world (Farahani, Asgari, Heidari, Hosseininia, & Goh, 2012). Each facility is able to provide services within a given critical distance, i.e., the coverage radius. A customer is considered served from a facility if the distance between them is less than or equal to the facility’s coverage radius. In reality, however, budget limits often constrain the number of service facilities to be located. This gives rise to the maximal covering location problem (MCLP) (Church & Velle, 1974), which seeks to maximize the coverage of customer demands by siting a given number of new facilities. In the last 40 years, MCLP and its extensions have been widely applied to study various location issues such as planning emergency facilities (e.g., Schilling, Revelle, Cohon, & Elzinga, 1980; Eaton, Daskin, Simmons, Bulloch, & Jansma, 1985; Murray & Tong, 2007), siting telecommunications equipment (e.g., Akella, Delmelle, Batta, Rogerson, & Blatt, 2010; Oztekin, Pajouh, Delen, & Swim, 2010; Shillington & Tong, 2011), location for business (e.g., Jones & Simmons, 1993; Pastor, 1994), and public services (e.g., Hougland & Stephens, 1976; Otto & Boysen, 2014).

Most MCLP models have been built under the assumption that the candidate locations of the new facilities are known in advance. In other words, facilities can only be installed in discrete nodes. Some researchers (e.g., Mehrez, 1983; Mehrez & Stulman, 1982, 1984; Church, 1984) have relaxed this constraint and extended the discrete version of MCLP to consider facility location in a continuous space, i.e., facilities are allowed to be placed anywhere on a plane. This problem is known as the planar maximal covering location problem (PMCLP), originally defined in Church (1984). For PMCLP, it is possible to attain a greater demand coverage because many more desired locations are available for selection when making strategic facility location decisions such as infrastructure investment (Murray & Tong, 2007; Wei, 2008). Murray and Tong (2007) suggested that more general representations (points, lines or polygons) of demand can also be optimally served in a region. They introduced the extended planar maximal covering location problem-Euclidean (EPMCE) and applied it to emergency warning sirens siting in Dublin. Matisziw and Murray (2009) formulated the 1-facility continuous maximal covering problem (CMCP-1), where demand is considered as continuously distributed within a whole region (convex or non-convex). They addressed CMCP-1 by generating the medial axis, which can be viewed as a geometrical representation of the region. Later in Wei (2008), the multi-facility case of CMCP was solved based on Voronoi diagrams and the geometric properties of the region. Recently, several applications of PMCLP have been reported by geographical researchers (see for example Liu & Hodgson, 2013; Wei, Murray, & Batta, 2014; Wei & Murray, 2014b).

To solve PMCLP, it is natural to reduce it to MCLP by finding a finite number of potential sites on the plane. With this set of discrete sites, MCLP can be solved by either exact or heuristic approaches. In other words, PMCLP can be addressed in two phases: I – identify a candidate locations set (CLS) and II – use exact or non-exact methods to address the degraded PMCLP for coverage maximization. Therefore, it is critical to identify a good CLS for solving PMCLP. We can analyze a CLS from various angles: (1) Coverage. The objective of the MCLP is to find a solution with maximal coverage. Hence, the coverage of the candidates in a CLS is an important consideration. (2) Size of the CLS. MCLP is NP-hard (Downs & Camm, 1996), whose size is determined by the numbers of demand nodes, potential locations, and facilities to be located. For a very large-scale MCLP, we have to apply heuristic algorithms to search for the optimal solution. However, using such non-exact methods may cause a loss of coverage (obtaining a local optimal solution only), or even fail to produce a feasible solution. Therefore, given a large number of demand points, it is highly desirable to reduce the number of potential sites in Phase I before solving MCLP. In addition to the above two essential principles, decision-makers in reality may also be concerned with the following objectives. (3) Time to generate the CLS. Is there an efficient way to generate the CLS in Phase I? This research question has largely been neglected in the literature, yet it is a significant issue to address in real life. For example, when a severe disaster (e.g., a storm, an earthquake etc.) takes place, it is an urgent task for the government to decide where to site search and rescue (SAR) stations in order to provide medical and rehabilitation services for as many victims as possible in a large area. The scale of such a problem can be remarkably large as the numbers of SAR stations and victims may be numerous. In the context of such emergency situations, the decision-maker must consider not only the above features of the candidate locations, but also the efficiency of solving such large-scale PMCLPs in real time. (4) Average distance to demand. Given that serving faraway demands will incur a high cost, planners may also be concerned about the distances between the covered demand points to the closest facility.

A three-step procedure, often referred to as the circle intersect points set (CIPS) method in the literature, has been the dominating technique for creating the CLS since it was introduced by Church (1984). First, this method produces a demand and intersection points set (DIPS
                        1
                     
                     
                        1
                        Note that this set was also called CIPS in Church (1984) and thus the term CIPS had two meanings: the whole method and the points set. To avoid confusion, hereafter in this paper we use the term CIPS to denote the method, and DIPS to call the demand and intersection points set generated by the CIPS method at the first step. We thank an anonymous referee for this helpful suggestion.
                     ) by exploiting the geometric properties of coverage. The circles are centered to cover demand locations with predefined coverage radii under the Euclidean distance measure. The second step in Phase I, which is optional if the DIPS only has few members, is to remove all the dominated points from the DIPS in order to reduce the size of the CLS. It has been shown that the reduced DIPS, i.e., the final CLS, contains at least one optimal solution to the PMCLP (Church, 1984). Given this CLS, the last step is to solve MCLP in Phase II. The CIPS method greatly facilitates coverage maximization and contributes significantly to size reduction. Consequently, it has been widely applied and extended in many studies (e.g., Younies & Wesolowsky, 2004; Murray & Tong, 2007; Canbolat & Massow, 2009; Yildiz, Akkaya, Sisikoglu, & Sir, 2011) as a standard approach to address PMCLP. However, the CIPS method has high time complexity and is generally unable to handle large-scale PMCLPs (see Section 2.2 for the details).

In this study we propose a mean-shift based algorithm for treating large-scale PMCLPs, i.e., MSMC. In MSMC, we introduce a revised mean-shift procedure that is less time consuming, and hence more suitable for solving large PMCLPs, than the traditional CIPS method. The mean-shift procedure has been successfully adapted to various application domains, such as cluster analysis in computer vision and image processing (Comaniciu & Meer, 2002). To the best of our knowledge, this work is the first attempt to solve location problems using the mean-shift procedure. The advantages for choosing the mean-shift procedure to identify the CLS for PMCLP are discussed in detail in Section 3.2.

The remainder of the paper is organized as follows: in Section 2 we review PMCLP, and discuss the pros and cons of the traditional CIPS approach. In Section 3 we briefly introduce the mean-shift procedure and the core of the proposed MSMC algorithm. We present each step of MSMC in detail. In Section 4 we compare the performance of MSMC against the CIPS method on randomly generated data sets that vary in size and distribution pattern, and discuss the experimental results to reveal the various performance aspects of the MSMC and CIPS approaches. In Section 5 we conclude the paper, discuss the research limitations, and suggest topics for future research.

PMCLP seeks to maximize the demand coverage on a plane. Unlike MCLP which locates facilities on a network or discrete locations, PMCLP concerns the siting of a given number of facilities anywhere on a plane; in other words, the number of potential sites for location is infinite in PMCLP. To address this issue, there is a need to generate a set of discrete potential locations from the continuous space, essentially reducing PMCLP to MCLP. To recap, solving PMCLP can be executed in two phases, namely I – identify a CLS and II – solve the MCLP given the CLS.

In this section we first present the MCLP formulation, from which PMCLP naturally arises. We then detail Phase I of the CIPS approach to solve PMCLP proposed by Church (1984). Finally, we discuss the advantages and shortcomings of the CIPS method.

MCLP has been mathematically formulated by Church and Velle (1974) as follows:

                           
                              (1)
                              
                                 
                                    Maximize
                                    
                                    Z
                                    =
                                    
                                       ∑
                                       
                                          i
                                          ∈
                                          I
                                       
                                    
                                    
                                       w
                                       i
                                    
                                    
                                       Y
                                       i
                                    
                                    .
                                 
                              
                           
                        subject to

                           
                              (2)
                              
                                 
                                    
                                       ∑
                                       
                                          j
                                          ∈
                                          
                                             Θ
                                             i
                                          
                                       
                                    
                                    
                                       X
                                       j
                                    
                                    ≥
                                    
                                       Y
                                       i
                                    
                                    ,
                                    
                                    for
                                    
                                    all
                                    
                                    i
                                    ∈
                                    I
                                 
                              
                           
                        
                        
                           
                              (3)
                              
                                 
                                    
                                       ∑
                                       j
                                    
                                    
                                       X
                                       j
                                    
                                    =
                                    p
                                    ,
                                 
                              
                           
                        
                        
                           
                              (4)
                              
                                 
                                    
                                       X
                                       j
                                    
                                    =
                                    
                                       {
                                       0
                                       ,
                                       1
                                       }
                                    
                                    ,
                                    
                                    for
                                    
                                    all
                                    
                                    j
                                    ∈
                                    J
                                 
                              
                           
                        
                        
                           
                              (5)
                              
                                 
                                    
                                       Y
                                       i
                                    
                                    =
                                    
                                       {
                                       0
                                       ,
                                       1
                                       }
                                    
                                    ,
                                    
                                    for
                                    
                                    all
                                    
                                    i
                                    ∈
                                    I
                                 
                              
                           
                        where

                           
                              
                                 
                                    
                                       
                                          i
                                       
                                       
                                          
                                             =
                                             index
                                             
                                             of
                                             
                                             demand
                                             
                                             points
                                             
                                             (entire
                                             
                                             set
                                             
                                             I
                                             )
                                             ;
                                          
                                       
                                       
                                    
                                    
                                       
                                          j
                                       
                                       
                                          
                                             =
                                             index
                                             
                                             of
                                             
                                             facility
                                             
                                             locations
                                             
                                             (entire
                                             
                                             set
                                             
                                             J
                                             )
                                             ;
                                          
                                       
                                       
                                    
                                    
                                       
                                          
                                             w
                                             i
                                          
                                       
                                       
                                          
                                             =
                                             weight
                                             
                                             of
                                             
                                             demand
                                             
                                             node
                                             
                                             i
                                             ;
                                          
                                       
                                       
                                    
                                    
                                       
                                          
                                             Θ
                                             i
                                          
                                       
                                       
                                          
                                             =
                                             {
                                             j
                                             ∈
                                             J
                                             
                                             |
                                             
                                             
                                                d
                                                
                                                   i
                                                   j
                                                
                                             
                                             ≤
                                             R
                                             }
                                             ;
                                          
                                       
                                       
                                    
                                    
                                       
                                          
                                             d
                                             
                                                i
                                                j
                                             
                                          
                                       
                                       
                                          
                                             =
                                             the
                                             
                                             shortest
                                             
                                             distance
                                             
                                             from
                                             
                                             i
                                             
                                             to
                                             
                                             j
                                             ;
                                          
                                       
                                       
                                    
                                    
                                       
                                          R
                                       
                                       
                                          
                                             =
                                             predefined
                                             
                                             coverage
                                             
                                             radius
                                             
                                             of
                                             
                                             facility;
                                          
                                       
                                       
                                    
                                    
                                       
                                          p
                                       
                                       
                                          
                                             =
                                             number
                                             
                                             of
                                             
                                             facilities
                                             
                                             to
                                             
                                             be
                                             
                                             located;
                                          
                                       
                                       
                                    
                                    
                                       
                                          
                                             X
                                             j
                                          
                                       
                                       
                                          
                                             =
                                             
                                                {
                                                
                                                   
                                                      
                                                         
                                                            1
                                                            ,
                                                         
                                                      
                                                      
                                                         
                                                            if
                                                            
                                                            a
                                                            
                                                            facility
                                                            
                                                            is
                                                            
                                                            located
                                                            
                                                            at
                                                            
                                                            j
                                                            ,
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            0
                                                            ,
                                                         
                                                      
                                                      
                                                         otherwise.
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             Y
                                             i
                                          
                                       
                                       
                                          
                                             =
                                             
                                                {
                                                
                                                   
                                                      
                                                         
                                                            1
                                                            ,
                                                         
                                                      
                                                      
                                                         
                                                            if
                                                            
                                                            demand
                                                            
                                                            i
                                                            
                                                            is
                                                            
                                                            covered
                                                            
                                                            by
                                                            
                                                            any
                                                            
                                                            sited
                                                            
                                                            facility,
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            0
                                                            ,
                                                         
                                                      
                                                      
                                                         otherwise.
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

The objective function (1) maximizes the total weighted demand served by the facilities. Constraint (2) allows Yi
                         to equal 1 only when demand point i is served by at least one facility. The number of facilities to be sited is restricted to equal p in constraint (3). Constraints (4) and (5) impose the binary integer restriction on the decision variables.

Given a CLS with size 
                           
                              f
                              =
                              
                                 |
                                 CLS
                                 |
                              
                              ,
                           
                         the number of possibilities of facility location is 
                           
                              
                                 (
                                 
                                    f
                                    p
                                 
                                 )
                              
                              =
                              f
                              !
                              /
                              
                                 (
                                 p
                                 !
                                 
                                    (
                                    f
                                    −
                                    p
                                    )
                                 
                                 !
                                 )
                              
                           
                        . OR-based methods like mixed integer programming can be applied to find the best solution for small-sized MCLPs. However, when the size of the CLS is sufficiently large, exploring the huge solution space is quite time consuming and may be prohibitive, so heuristic algorithms are required to search for acceptable solutions within a reasonable time. There are both exact and non-exact approaches for various types of MCLPs in the literature. However, given that MCLP is NP-hard, it is desirable to find ways to reduce the size of the CLS in Phase I as a smaller f can save considerable computing time in Phase II.

According to Church (1984), the CIPS approach can be itemized as follows
                           2
                        
                        
                           2
                           
                              Appendix A provides the pseudo codes of the key procedures discussed in this paper, including both the CIPS and MSMC approaches. Comprehensive discussion of the time complexity of each procedure is also given.
                        :

                           
                              1.
                              Generating DIPS. Draw the covering boundary for each demand point (DP) and identify the intersection points (IPs) of the boundaries, and combine the DPs and the IPs as the DIPS.

Refining DIPS (optional for small-sized DIPS). Drop all the dominated points from the DIPS as final CLS.

Solving MCLP given the CLS.

Suppose that there are n demand points to be covered on the plane. Our interest is to generate a CLS in an efficient way to help address a large-scale PMCLP. Fig. 1
                         gives a simple example to show the basic process of CLS generation using the CIPS method. There are four DPs placed on the plane according to their coordinates and the coverage radius 
                           
                              R
                              =
                              1
                           
                         in this instance. In Step 1, four circles centered at the blue DPs are drawn as shown in Fig. 1(a). As a result, five red IPs are recognized and their coverage boundaries are also plotted in Fig. 1(b). The next step is to recognize the dominating points from the nine nodes. According to Church (1984), point A is considered to dominate point B if point A covers the same, or more than, all the demand points that point B can serve. According to this rule, all the four DPs are considered to be dominated by the four corner IPs since a facility located at any one of the DPs can only serve one DP, i.e., itself, while each corner IP covers two DPs. At the end, however, the only member of the DIPS, or the CLS, is IP
                        5 located at the origin because it has all the four DPs covered, dominating the other IPs.

Admittedly, the CIPS is excellent in creating a CLS in terms of coverage maximization since the generated DIPS is guaranteed to contain at least one optimal solution to the PMCLP (Church, 1984). However, finding the dominating points can be time consuming, especially for large-sized PMCLPs. The total amount of time required to refine the DIPS depends on not only the number of DPs, but also the pattern of the demand distribution. Given n demand points on the plane, the DIPS may have at least n members when the DPs are too far away from one another, so no IP can be created. In this case, it is optional to recognize the dominating points. In the worst case where all the DPs gather inside a circle of radius (2 * R), the DIPS has 
                           
                              2
                              *
                              
                                 (
                                 
                                    n
                                    2
                                 
                                 )
                              
                              +
                              n
                              =
                              
                                 n
                                 2
                              
                           
                         members, making it necessary to remove the dominated candidate locations from the DIPS. In other words, the size of the DIPS is c ∈ [n, n
                        2]. Before incorporating the reduction technique in Step 2, we know that the final size f of the CLS will be less than or equal to c. However, it is difficult to predict the precise f since we need to compute regardless of whether these c sets containing the covered DPs are subsets of one another. In the worst case, the complexity will tend towards 
                           
                              O
                              
                                 (
                                 
                                    (
                                    
                                       c
                                       2
                                    
                                    )
                                 
                                 *
                                 n
                                 )
                              
                              =
                              O
                              
                                 (
                                 
                                    n
                                    5
                                 
                                 )
                              
                              ,
                           
                         as detailed in Appendix A. Therefore, the CIPS method may fail to produce the CLS for a large-sized PMCLP with point-based demands in a reasonable time, as suggested by Murray and Tong (2007) and Wei (2008).

Motivated by the above observations, we set out to propose an algorithm that is more efficient in creating the CLS for solving a large-scale PMCLP.

The mean-shift algorithm is an iterative mode-seeking method introduced by Fukunaga and Hostetler (1975). It presents a simple but efficient way to converge to the local density maxima (LDM) in any probability distribution. During the last decade, the mean-shift algorithm has been extensively studied and successfully applied to many applications in the computer vision domain, such as real-time object tracking (Comaniciu, Ramesh, & Meer, 2000), image segmentation (Tao, Jin, & Zhang, 2007), cluster analysis (Wu & Yang, 2007) etc.

In this section we present a brief review of the original mean-shift procedure. We then propose a new algorithm MSMC based on a revised mean-shift procedure to generate the CLS for solving PMCLPs.

Suppose that there are n data points 
                           
                              
                                 {
                                 
                                    y
                                    i
                                 
                                 }
                              
                              
                                 i
                                 =
                                 1
                              
                              n
                           
                         in a d-dimensional Euclidean space Rd
                        . The mean-shift algorithm assumes that they are independent, identically distributed samples drawn from a population with an unknown density function f(y). The multivariate kernel density estimator obtained with kernel K(y) and bandwidth h is defined as Silverman (1986):

                           
                              (6)
                              
                                 
                                    
                                       f
                                       ^
                                    
                                    
                                       (
                                       y
                                       )
                                    
                                    =
                                    
                                       
                                          1
                                          
                                             n
                                             
                                                h
                                                d
                                             
                                          
                                       
                                    
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       n
                                    
                                    K
                                    
                                       (
                                       
                                          
                                             
                                                y
                                                −
                                                
                                                   y
                                                   i
                                                
                                             
                                             h
                                          
                                       
                                       )
                                    
                                    ,
                                 
                              
                           
                        where the kernel K(y) is a scalar function under the conditions given in Fukunaga and Hostetler (1975). Among the various valid kernels, the radially symmetric kernel is often selected since it is more suited for the mean-shift algorithm (Comaniciu & Meer, 2002), satisfying:

                           
                              (7)
                              
                                 
                                    K
                                    
                                       (
                                       y
                                       )
                                    
                                    =
                                    
                                       c
                                       
                                          k
                                          ,
                                          d
                                       
                                    
                                    
                                       
                                          k
                                          (
                                          ∥
                                          y
                                          ∥
                                       
                                       2
                                    
                                    
                                       )
                                       ,
                                    
                                 
                              
                           
                        where c
                        
                           k,d
                         is a positive normalization constant that makes K(y) integrate to one. The function k(x), only for x ≥ 0, is called the profile of the kernel (Comaniciu & Meer, 2002).

In order to locate the modes of the density function, the gradient of the density estimator is formulated by exploiting the linearity of Eq. (6) as follows:

                           
                              (8)
                              
                                 
                                    
                                       
                                          
                                             
                                                ∇
                                                ^
                                             
                                             
                                                f
                                                
                                                   h
                                                   ,
                                                   K
                                                
                                             
                                             
                                                (
                                                y
                                                )
                                             
                                          
                                       
                                       
                                          ≡
                                       
                                       
                                          
                                             ∇
                                             
                                                
                                                   f
                                                   ^
                                                
                                                
                                                   h
                                                   ,
                                                   K
                                                
                                             
                                             
                                                (
                                                y
                                                )
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      2
                                                      
                                                         c
                                                         
                                                            k
                                                            ,
                                                            d
                                                         
                                                      
                                                   
                                                   
                                                      n
                                                      
                                                         h
                                                         
                                                            d
                                                            +
                                                            2
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                n
                                             
                                             
                                                (
                                                
                                                   y
                                                   i
                                                
                                                −
                                                y
                                                )
                                             
                                             g
                                             
                                                (
                                                
                                                   
                                                      ∥
                                                      
                                                         
                                                            
                                                               y
                                                               −
                                                               
                                                                  y
                                                                  i
                                                               
                                                            
                                                            h
                                                         
                                                      
                                                      ∥
                                                   
                                                   2
                                                
                                                )
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      2
                                                      
                                                         c
                                                         
                                                            k
                                                            ,
                                                            d
                                                         
                                                      
                                                   
                                                   
                                                      n
                                                      
                                                         h
                                                         
                                                            d
                                                            +
                                                            2
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                [
                                                
                                                   ∑
                                                   
                                                      i
                                                      =
                                                      1
                                                   
                                                   n
                                                
                                                g
                                                
                                                   (
                                                   
                                                      
                                                         ∥
                                                         
                                                            
                                                               
                                                                  y
                                                                  −
                                                                  
                                                                     y
                                                                     i
                                                                  
                                                               
                                                               h
                                                            
                                                         
                                                         ∥
                                                      
                                                      2
                                                   
                                                   )
                                                
                                                ]
                                             
                                             
                                                m
                                                h
                                             
                                             
                                                (
                                                y
                                                )
                                             
                                             ,
                                          
                                       
                                    
                                 
                              
                           
                        where

                           
                              (9)
                              
                                 
                                    g
                                    
                                       (
                                       x
                                       )
                                    
                                    =
                                    −
                                    
                                       k
                                       ′
                                    
                                    
                                       (
                                       x
                                       )
                                    
                                    ,
                                 
                              
                           
                        
                     


                        
                           
                              (10)
                              
                                 
                                    
                                       m
                                       h
                                    
                                    
                                       (
                                       y
                                       )
                                    
                                    =
                                    
                                       
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                n
                                             
                                             
                                                y
                                                i
                                             
                                             g
                                             
                                                (
                                                
                                                   
                                                      ∥
                                                      
                                                         
                                                            
                                                               y
                                                               −
                                                               
                                                                  y
                                                                  i
                                                               
                                                            
                                                            h
                                                         
                                                      
                                                      ∥
                                                   
                                                   2
                                                
                                                )
                                             
                                          
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                n
                                             
                                             g
                                             
                                                (
                                                
                                                   
                                                      ∥
                                                      
                                                         
                                                            
                                                               y
                                                               −
                                                               
                                                                  y
                                                                  i
                                                               
                                                            
                                                            h
                                                         
                                                      
                                                      ∥
                                                   
                                                   2
                                                
                                                )
                                             
                                          
                                       
                                    
                                    −
                                    y
                                    .
                                 
                              
                           
                        
                     

The first term of the product in (8) is proportional to the density estimate at y computed with the kernel G(y):

                           
                              (11)
                              
                                 
                                    G
                                    
                                       (
                                       y
                                       )
                                    
                                    =
                                    
                                       c
                                       
                                          g
                                          ,
                                          d
                                       
                                    
                                    
                                       
                                          g
                                          (
                                          ∥
                                          y
                                          ∥
                                       
                                       2
                                    
                                    
                                       )
                                       .
                                    
                                 
                              
                           
                        
                     


                        Eq. (10), i.e., the second term in Eq. (8), is the mean shift, which denotes the vector from the centre of the kernel y to the weighted mean computed with the kernel G. In other words, it always points towards the direction of maximum increase in density (Comaniciu & Meer, 2002). In the event that 
                           
                              
                                 m
                                 h
                              
                              
                                 (
                                 y
                                 )
                              
                              =
                              0
                              ,
                           
                         the mean-shift algorithm converges to the LDM where the density estimator 
                           
                              
                                 
                                    f
                                    ^
                                 
                                 
                                    h
                                    ,
                                    K
                                 
                              
                              
                                 (
                                 y
                                 )
                              
                           
                         has a zero gradient. Hence, given the bandwidth h, also known as the window size, and the initial starting points (SPs), the mean-shift procedure comprises the following steps:

                           
                              1.
                              fix a kernel (window) around each SP;

compute the mean-shift vector m
                                 
                                    h
                                 (y
                                 
                                    t
                                 ) within the kernel (window);

shift the kernel (window) to the mean: 
                                    
                                       
                                          y
                                          
                                             t
                                             +
                                             1
                                          
                                       
                                       =
                                       
                                          y
                                          t
                                       
                                       +
                                       
                                          m
                                          h
                                       
                                       
                                          (
                                          
                                             y
                                             t
                                          
                                          )
                                       
                                    
                                 ;

go to Step 2 until convergence.

The above procedure has been shown to converge at the LDM where the multivariate kernel density estimator has zero gradient (see Comaniciu & Meer, 2002; Li, Hu, & Wu, 2007). Therefore, the mean-shift algorithm is actually a gradient ascent method which has good potential to search for LDM.


                        Fig. 2
                         is a demonstration of applying the mean-shift algorithm on the planar where the kernel 
                           
                              g
                              (
                              y
                              )
                              =
                              1
                           
                        . As a result, the mean-shift vector 
                           
                              
                                 m
                                 
                                    h
                                    ,
                                    G
                                 
                              
                              
                                 (
                                 y
                                 )
                              
                              =
                              
                                 
                                    1
                                    n
                                 
                              
                              
                                 ∑
                                 
                                    i
                                    =
                                    1
                                 
                                 n
                              
                              
                                 (
                                 
                                    y
                                    i
                                 
                                 −
                                 y
                                 )
                              
                              ,
                           
                         which means all the data points have identical weights. In Fig. 2(a), a window (circle) is centered at one data point and the mean-shift vector can be calculated after searching for the covered data points. Next, the centre of the disk keeps moving to the new mean location along the shift vector until reaching a LDM, as shown in Fig. 2(b), (c), and (d). These shift vectors compose a path leading to the stationary point, which is a desirable feature of our MSMC algorithm as we will see later in this section.

As this work is the first attempt to solve location problems using the mean-shift procedure, there is a need to explain the rationale for it.
                           3
                        
                        
                           3
                           We thank an anonymous referee for this helpful suggestion.
                         We summarize the reasons for choosing the mean-shift procedure to identify the CLS for PMCLP are as follows.

                           
                              •
                              The PMCLP model meets all the requirements of the mean-shift method. A plane is a two-dimensional space and the demand points on it can be viewed as samples from an unknown distribution. Furthermore, the mean-shift algorithm requires no prior knowledge of the distribution because the mean-shift vector always points in the direction of maximum increase in the density. Therefore, it can be directly applied to identify the CLS for PMCLP.

The aim of the mean-shift procedure is consistent with the first criterion of the CLS: coverage. Given a coverage region, a candidate location should cover as many demand points as possible. The identified LDM could be such ideal potential sites since they are located at the centres of the densest regions and should thus, cover the majority of demand points.

The number of LDM could be relatively small, which is the second principle of a CLS. Comaniciu and Meer (2002) defined the term basin of attraction of a mode as the set of all the locations that get sufficiently close to the mode and will definitely converge to it eventually. Fig. 2 is an example to illustrate how the starting point in Fig. 2(a) is captured and shifted from the initial location to the mode in Fig. 2(d). This feature allows modes to attract nearby mean locations, so reducing the size of the set of unique stationary points significantly.

The mean-shift procedure consumes less time to yield the CLS, which is another theoretical advantage. It requires multiple nearest neighbor searches during the execution of Step 2, so its time complexity is O(τ * s * n), where s and n are the numbers of starting points and data points, respectively, and τ stands for the number of iterations before convergence. For the original mean-shift procedure that choose the data points as initial starting points, it takes O(τ * n
                                 2) time. Admittedly, this algorithm is not highly scalable; however, it is still much more efficient than the CIPS method, which runs in O(n
                                 5) time in the worst case. Therefore, one would expect that the larger the n is, the faster is MSMC than the traditional CIPS method.

A LDM is the mean, i.e., the centre of covered demand locations. Therefore, the distances to these served nodes are minimized, which has a practical advantage over the CIPS method in terms of distance reduction, even though distance reduction is not an objective in the original PMCLP formulation. This is because siting facilities at intersection points produced by the CIPS method may not be optimal in practice if the distance factor is taken into account, as noted by Wei and Murray (2014a). For example, in Fig. 3
                                 , the four DPs are closer to one another than twice of the given radius. Therefore, 12 red IPs can be found in Fig. 3(a). After removing the dominated nodes, there are eight candidate locations left and each one is able to cover all the DPs. In other words, any one of them, i.e., four red IPs and the original DPs themselves as illustrated in Fig. 3(b), can be chosen as the only member of the CLS. If one of the blue DPs, e.g., DP
                                 3 is selected, there is an overlap between the locations of the demand nodes and facility locations; if one of the red IPs, e.g., IP
                                 1 is chosen, it is certain that two DPs will lie on their coverage boundaries. In real-world applications, such as distribution of medical supplies, the decision-maker may still prefer locating a facility on the origin of the plane since it is generally considered that a facility close to the demand points can provide a better quality of coverage (Dessouky, Ordez, Jia, & Shen, 2006; Jia, Ordez, & Dessouky, 2007). However, this kind of optimal locations in practice cannot be produced using the CIPS method.

In view of the above discussion, we consider that the mean-shift algorithm is a promising technique for identifying a good CLS for solving PMCLP.

Similarly, our MSMC comprises the following three steps:

                           
                              1.
                              Generating DIPS.

Run the revised mean-shift procedure to identify LDM as final CLS.

Solving MCLP given the CLS.

The original mean-shift procedure was not designed for the PMCLP. Therefore, we need to configure and revise it in order to improve its performance.

The original mean-shift approach has three main parameters: starting points (SPs), the bandwidth h and the kernel function g(y). The mean-shift procedure usually launches from the data points themselves. As mentioned in previous subsection, demand points (DPs) in the PMCLP are the data points for the mean-shift procedure, thus SPs = DPs. However, we suggest that both DPs and IPs should be used as the SPs to identify the CLS, i.e., SPs = DPs + IPs = DIPS. The reason for this modification is that starting only with the DPs makes the mean-shift procedure fail to discover the critical candidate locations such as the IPs. For example, in Fig. 1(a), if we simply choose the DPs as the initial SPs, then the algorithm will stop immediately and offer four DPs as the LDM. As shown in Fig. 4
                        , MSMC starting with the DIPS is able to recognize five more LDM, including the origin. However, the side effect of this adaptation is that it increases the time complexity. Although determining the DIPS is a computationally affordable operation with a time complexity of O(n
                        2), the size of the DIPS raises the time complexity of the mean-shift procedure from O(τ * n
                        2) to O(τ * n
                        3) in the worst case. However, on weighing the performance tradeoff, we suggest that the DIPS, rather than the DPs, be used as the initial starting points for MSMC, with a view to achieving maximal coverage.

Bandwidth selection is an important topic in the research community since it directly affects the performance of density estimation, especially for the tracking of size-changing objects. Many approaches have been developed to address this issue and the bandwidth of the mean-shift procedure can be automatically selected in various practical applications (see for example Comaniciu, Ramesh, & Meer, 2001; Ming, Ci, Cai, Li, Qiao, & Du, 2012). In the context of PMCLP, it is natural to set the facility’s covering distance as the bandwidth in MSMC, i.e., 
                           
                              h
                              =
                              R
                           
                        .

The kernel function in the mean-shift process essentially imposes additional weights on the data points according to their distances to the shifting mean (Comaniciu & Meer, 2002). Cheng (1995) summarized four types of kernel functions frequently used in mean-shift research, namely flat, Gaussian, Epanechnikov, and quartic kernel. The experimental results presented in Appendix B demonstrate that the flat kernel performs best in terms of coverage. Therefore, we suggest that a flat kernel be chosen where all the data points are assumed to have the same distance weight when identifying LDM, i.e., 
                           
                              g
                              (
                              y
                              )
                              =
                              1
                           
                         as in Fig. 2. Moreover, if the demand points are associated with different weights wi
                         in the PMCLP model, they should be taken into consideration when solving MCLP in Phase II, rather than in Phase I. The reason for forcing weights to be equal in computing the means and shift vectors is that we are able to recognize some saddle-like LDM without considering the weights. For instance, the origin found in Fig. 4 can be viewed as a saddle point. Suppose that one of data points, say, DP
                        1 has a larger weight, it will pull the centered LDM to the right. In that case, we will lose this candidate location on the origin.

The sharp-eyed readers may find in Fig. 4 that the origin dominates the other LDM, raising the question about the necessity of removing the dominated LDM using the same reduction technique employed by the second step of the CIPS method. The main motivation to add this step after identifying the LDM is that dropping the dominated LDM will reduce the size of the CLS, consequently saving time for solving MCLP. However, this operation is actually quite computationally demanding, as analyzed in Appendix A. In other words, using the reduction procedure requires much more time than what it saves because running the solver for MCLP is usually not the bottleneck of the algorithm’s whole performance, as we will see later in the experimental results.

In conclusion, the MSMC algorithm incorporates and modifies the original mean-shift procedure in order to generate CLS more efficiently for large-sized PMCLPs. Furthermore, it also borrows helpful ideas from the traditional CIPS method, such as creating the DIPS as the SPs for MSMC.

In the next section we generate a host of data sets to test the performance of the MSMC algorithm against the CIPS method.

For comparison purposes, we re-organize the steps of the two algorithms as two time-lines illustrated in Fig. 5
                        . We coded all the stages in Python 2.7, combining with the CPLEX® API
                           4
                        
                        
                           4
                           The Python API of CPLEX is part of IBM’s ILOG CPLEX optimization studio (Version: 12.5).
                         for solving MCLP in Stage 3. We conducted all the experiments on an Intel Xeon computer with two processors (2.30 gigahertz) running 64-bit Windows 7 with 32 gigabyte of RAM.

At the beginning, we generated four types of test data set using the scikit-learn module, which is a well-known Python package for data mining and data analysis. Although we added the standard deviation of the Gaussian noise to the data, the generated data sets were repeated exactly for the two algorithms since the random seed was fixed. We also standardized all the data sets by removing the mean and scaling to unit variance in order to specify their x and y coordinates over the range of (
                           −
                        2, 2). Most of their parameters are listed in Table 1
                        , whose values are chosen arbitrarily. Random weights between 0 and 10 were assigned to the data points of the two data sets, namely Circles and Blobs. Therefore, the only parameter we needed to control was the number of data points, which was assigned as 
                           
                              n
                              =
                              {
                              20
                              ,
                              60
                              ,
                              100
                              ,
                              200
                              ,
                              300
                              ,
                              400
                              ,
                              500
                              }
                           
                        . Additional large-scale instances where 
                           
                              n
                              =
                              {
                              1000
                              ,
                              2000
                              ,
                              3000
                              }
                           
                         are created to test the MSMC method only, since the CIPS is unable to solve them in a reasonable time. In other words, we generated 28 data sets to evaluate and compare the performance of two algorithms, and 12 larger-scale instances to assess the performance of MSMC in solving large-sized PMCLPs. Note that as n grows, a dramatically increasing number (up to 
                           
                              n
                              (
                              n
                              −
                              1
                              )
                           
                        ) of the IPs will be produced in the cramped region with a higher density, causing stress-growing tests for both algorithms
                        .

In order to obtain a wealth of information about the experimental results, we define the following metrics based on the four criteria discussed above, where the subscript α is used to denote the two algorithms (CIPS: 
                           
                              α
                              =
                              1
                           
                        ; MSMC: 
                           
                              α
                              =
                              2
                           
                        ):

                           
                              1.
                              The ratio of covered demands to total demand: zα
                                  ∈ [0, 1], and the gap between them: 
                                    
                                       Δ
                                       z
                                       =
                                       
                                          z
                                          1
                                       
                                       −
                                       
                                          z
                                          2
                                       
                                    
                                 . Since it has been shown that the CIPS method is capable of yielding the optimal solution, the coverage ratio of the CIPS z
                                 1 is always larger than or equal to z
                                 2.

The average weighted distance between all the covered data points to the nearest facility: 
                                    
                                       w
                                       
                                          d
                                          α
                                       
                                       =
                                       
                                          
                                             
                                                
                                                   ∑
                                                   
                                                      k
                                                      ∈
                                                      Ω
                                                   
                                                
                                                
                                                   w
                                                   k
                                                
                                                
                                                   d
                                                   k
                                                   ′
                                                
                                             
                                             
                                                
                                                   ∑
                                                   
                                                      k
                                                      ∈
                                                      Ω
                                                   
                                                
                                                
                                                   w
                                                   k
                                                
                                             
                                          
                                       
                                       >
                                       0
                                       ,
                                    
                                  where 
                                    
                                       Ω
                                       =
                                       
                                          
                                             {
                                             
                                                y
                                                k
                                             
                                             }
                                          
                                          
                                             k
                                             =
                                             1
                                          
                                          K
                                       
                                    
                                  is the set of the K covered demand points and 
                                    
                                       d
                                       k
                                       ′
                                    
                                  denotes the distance from the covered demand point k to the nearest facility located. This indicator can be viewed as the average distance to unit demand, which is essential for the facility owner to minimize the transportation cost or time in practice. For example, in locating emergency services, it is paramount to achieve coverage as fast as possible. In the business context, a smaller wd usually means greater cost saving under the condition of same coverage.

The size of the generated locations c is the number of candidate locations after Stage 1, i.e., the number of elements in the DIPS. fα
                                  is that in the CLS after Stage 2. A smaller fα
                                  stands for better performance in reducing the size of the DIPS as it directly reduces the possible solution space for solving MCLP in Phase II.

The total time consumed: 
                                    
                                       
                                          T
                                          α
                                       
                                       =
                                       
                                          t
                                          α
                                          1
                                       
                                       +
                                       
                                          t
                                          α
                                          2
                                       
                                       +
                                       
                                          t
                                          α
                                          3
                                       
                                    
                                 . The three terms on the right hand side of this equation represent the time cost in the three stages, respectively. Note that the unit of these metrics is in seconds.

In sum, the four groups of indicators for evaluating these two approaches can be divided into two categories, namely solution quality and solution efficiency.

@&#RESULTS AND DISCUSSION@&#

We present all the results of the computational experiments in Tables 2–4
                        . We also plot the plane and experimental results when 
                           
                              n
                              =
                              100
                           
                         in Fig. 6
                        , including the demand points, the final selected sites, and the coverage. The Random data set distributes the DPs uniformly, while the Blobs locates the DPs in three clusters. We generate the other two data sets in order to simulate demand regions that can be represented as an arc or a non-convex polygon.

The first group of metrics in the result tables measures the proportion of covered demands, where z
                           1 ≥ z
                           2 as expected. Sometimes 
                              
                                 Δ
                                 z
                                 =
                                 
                                    z
                                    1
                                 
                                 −
                                 
                                    z
                                    2
                                 
                                 =
                                 0
                                 ,
                              
                            showing that MSMC has good potential to reach the optimal solution as the CIPS method. However, we also see that 
                              
                                 Δ
                                 z
                                 =
                                 0.15
                              
                            in the worst case of the Circles data set with 
                              
                                 n
                                 =
                                 20
                              
                           . This result exposes the main weakness of the mean-shift procedure in covering the DPs: it could ignore some points located at the low density regions, especially relatively orphaned ones. For our objective function seeking to maximize coverage, the reader may argue that a 15 percent loss of demand coverage is unacceptable. However, as n increases, the average coverage gap between MSMC and the CIPS method (i.e., 
                              
                                 
                                    Δ
                                    z
                                 
                                 ¯
                              
                           ) is diminishing as illustrated in Fig. 7
                           , due to the higher density that promotes the advantage of the mean-shift procedure. Moreover, MSMC always outperforms the CIPS method in terms of service cost reduction, i.e., 
                              
                                 Δ
                                 w
                                 d
                                 =
                                 w
                                 
                                    d
                                    1
                                 
                                 −
                                 w
                                 
                                    d
                                    2
                                 
                                 ≥
                                 0
                              
                           . In fact, a linear regression equation with 
                              
                                 
                                    R
                                    2
                                 
                                 =
                                 0.88
                              
                            can be fitted to the obtained results in order to describe the significant positive relationship between 
                              
                                 
                                    Δ
                                    z
                                 
                                 ¯
                              
                            and 
                              
                                 
                                    
                                       Δ
                                       w
                                       d
                                    
                                    ¯
                                 
                                 ,
                              
                            as follows:

                              
                                 (12)
                                 
                                    
                                       
                                          
                                             Δ
                                             z
                                          
                                          ¯
                                       
                                       =
                                       0.8975
                                       ×
                                       
                                          
                                             Δ
                                             w
                                             d
                                          
                                          ¯
                                       
                                       −
                                       0.0166
                                       .
                                    
                                 
                              
                           
                        

The gist of Eq. (12) is the second shortcoming of the CIPS method discussed in Section 2.2, i.e., at least two served DPs will lie on the facility’s coverage boundary if an IP is chosen, yielding the farthest distances between them. In other words, maximizing coverage is usually in conflict with shortening the service distance. Therefore, it is a challenge to simultaneously optimize these two conflicting objectives. Facing this issue, the CIPS method completely neglects the distance factor, while MSMC achieves a reasonable trade-off between coverage maximization and transportation cost saving. Based on the above findings, we believe that the MSMC method can be a promising approach to address small- and medium-sized PMCLPs.

For large-scale PMCLPs, however, MSMC may be the only viable approach to generate candidate locations. The fourth group of metrics indicates that the CIPS method experiences an exponential growth in computational time, e.g., 2.25 hours for the Blobs data set with 
                              
                                 n
                                 =
                                 500
                              
                           . For identical data sets, MSMC only takes tens of seconds to produce the CLS, which can be one hundred times less than that of the CIPS method as n increases. It is seen that Step 1 takes less than two seconds to create the DIPS for both algorithms, i.e. 
                              
                                 
                                    t
                                    1
                                    1
                                 
                                 ≅
                                 
                                    t
                                    2
                                    1
                                 
                              
                           . The second step is the most computationally demanding operation whose time complexity is discussed in Appendix A. The final step widens the gap between the two algorithms’ performance since MSMC drops many more of the dominated candidate locations from the DIPS than the CIPS approach does as n increases. Both Tables 2 and 3 show that f
                           1 > f
                           2 when n ≥ 100, which is the main reason for 
                              
                                 
                                    t
                                    1
                                    3
                                 
                                 >
                                 
                                    t
                                    2
                                    3
                                 
                              
                           . Based on the above analysis, it is not surprising that MSMC is much more efficient than the CIPS method to address large-scale PMCLPs.

It is also worth noting that as the DPs gather into clusters, most of the performance metrics of MSMC improve. The main reason, as mentioned before, is that the increase in the local density helps MSMC cover more DPs within a fixed region. This factor also has a major impact on the size of the DIPS since more intersection points will be calculated if the radius of a cluster is relatively small. As demonstrated in Fig. 3(a), if the radius is less than 2 * R, k DPs will generate 
                              
                                 k
                                 (
                                 k
                                 −
                                 1
                                 )
                              
                            IPs. This is the reason why the Blobs data set produces a DIPS of size 137,012 given 
                              
                                 n
                                 =
                                 500
                                 ,
                              
                            much more than the other data sets. Compared with the CIPS method, which has to perform the computationally demanding isSubset check one hundred thousand times, it is easy and fast for MSMC to locate the density mode just nearby. In other words, the iteration time before convergence is small, reducing the complexity from O(τ * c * n) to O(c * n).


                           Table 4 reports experimental results of MSMC when solving large-scale PMCLPs. Comparing with Tables 2 and 3, the MSMC still performs well in terms of solution quality. Meanwhile, the increased execution time (T
                           2) is also acceptable given the large number of demand nodes, which could produce millions of intersection points in DIPS. There is an interesting finding that, although the Blobs data set is always able to create the largest DIPS, the most time-consuming instance is the Moons. This is because the mean-shift process, which is the bottleneck of MSMC, has to find more temporary means along the semi circumferences before locating the LDM, leading to higher time complexity. Yet despite the large size of data sets and increased time complexity, our proposed MSMC approach has demonstrated to be very successful in solving large-scale PMCLPs.

@&#SUMMARY@&#

The CIPS algorithm has high time complexity and may not be appropriate for solving large-scale PMCLPs in practice. In contrast, the proposed MSMC, without the random factors, is a highly efficient tool for dealing with large-scale PMCLPs. The limitation of the MSMC algorithm is that it may not contain an optimal solution to the PMCLP. However, our experiments suggest that the optimality gap of the solution obtained using the CLS generated by the MSMC algorithm is likely to be small in practice. Furthermore, our experiments also suggest that the average distance to the demand points of the solution based on the MSMC algorithm is small. Table 5
                            summarizes the general performance of the two methods based on the four criteria discussed in the Introduction.

@&#CONCLUSION@&#

PMCLP can be applied to make strategic facility location decisions, such as infrastructure investment, because it allows a given number of facilities to be placed anywhere on a plane for coverage maximization, so many more desired locations are available for selection. However, due to the analytical difficulty of tackling geometrical computation in a continuous plane, there is a need to identify potential sites for evaluation, reducing PMCLP to the relatively simple MCLP.

The technique for determining the CLS has been mostly dominated by the CIPS approach. This approach exploits the geometric properties of coverage by identifying all the demand nodes plus the intersection points as DIPS, and then drops the dominated points from the DIPS in order to generate a reduced CLS. Although it has been shown that the CLS created by the CIPS method contains at least one optimal solution to the PMCLP, the CIPS method has high time complexity and is generally unable to handle large-scale PMCLPs.

This paper presents a deterministic algorithm MSMC, based on a revised mean-shift procedure, to address large-scale PMCLPs. We make three revisions to the original mean-shift procedure to adapt it for PMCLP, and generate a number of data sets that vary in size and distribution pattern to test MSMC against the CIPS method. Unlike the CIPS method, we are able to generate the CLS for large PMCLPs in a reasonable amount of time. Besides computational time, we also discuss the experimental results in terms of different performance aspects of the algorithms, e.g., coverage ratio, weighted distance to customers, and CLS size. The experimental findings indicate that MSMC strikes a reasonable trade-off between coverage maximization and transportation cost saving. Hence, the MSMC approach is also potentially promising for solving small- and medium-sized PMCLPs.

Although the examined PMCLP assumes Euclidean distance as the coverage standard, the proposed MSMC algorithm remains applicable if other distance metrics are used. For example, if the metropolitan metric (rectilinear) distance measure is taken into account, MSMC only requires a minor revision of changing the distance computing function for the mean-shift procedure. Another difference concerns the development of the intersection points whereby the coverage area changes from a circle to a diamond, which may create an infinite diamond intersect points set when the intersection set becomes a line segment. Church (1984) commented on this case and suggested the use of the two endpoints of the line segment as the IPs. Therefore, the attributes of the diamond intersect points set are analogous to those of the DIPS. In other words, all the key procedures of the two algorithms can still be applied to solve PMCLP under the rectilinear distance measure.

This study is an initial attempt to solve the location problem using the mean-shift algorithm. Further research is possible such as developing a suitable mechanism to reconcile the solution features. Moreover, in our model, demand on the plane is represented as discrete points, rather than lines or polygons. Applying MSMC or new algorithms to optimally cover these shapes of demands is not only challenging but also interesting. Finally, the maximal service distance R for each demand is a constant. In reality, it may vary due to the facility’s size, availability, or service standard. Therefore, relaxing this assumption makes PMCLP much more applicable in practice, but such a problem requires further in-depth research to address.

@&#ACKNOWLEDGMENTS@&#

We thank the anonymous referees for their helpful comments on earlier versions of our paper. This research was supported in part by the Singapore National Research Foundation (grant no. E2S2) under the Campus for Research Excellence and Technological Enterprise (CREATE) programme, the National Science Foundation of China (Nos. 91024007 and 71371122), and National Social Science Foundation of China (No. 14ZDB152). Cheng was also supported in part by The Hong Kong Polytechnic University under the Fung Yiu King – Wing Hang Bank Endowed Professorship in Business Administration.

Given the radii and centre coordinates of two circles, it is routine to programme the process of recognizing their intersection points. Algorithm 1 shows a piece of the pseudo code, whose main body consists of two nested loops. Each line of the codes within the inner loop is a constant-time operation, i.e., it runs in O(1) time. As each loop needs to execute n times, the time complexity of the whole procedure is O(n
                        2).
                     

As noted by Church (1984), point A is considered to dominate point B if point A covers the same, or more than, all the demand points that point B can serve. Therefore, examining the coverage information of each potential location is necessary before comparison. As presented in Algorithm 2,
                         we first construct an empty list with length c and fill it up with sublists of candidate node i’s covered data points. Note that this step runs in O(c * n) time since the inside codes are independent of n and c. The second part of Algorithm 2 is much more computationally demanding because the two nested loops take subquadratic time O(c
                        2), where c could be as large as O(n
                        2), as mentioned before. It is even worse that the function isSubset in line 13 also has a time complexity of O(n) as Ωi
                         could have n members. Therefore, this part is the bottleneck in running the DIPS reduction procedure, whose time complexity could be O(c
                        2 * n) or O(n
                        5) in the worst case.

The mean shift procedure embedded in the MSMC is modified to share the same input and output parameters with the DIPS reduction process, as detailed in Algorithm 3. There are three nested loops, similar to the DIPS reduction as well. After initializing an empty set for storing the identified unique means, the algorithm runs an outer loop to pick each starting point from the DIPS of size c. The middle loop is a do-while loop since the iteration time before convergence is unknown, denoted as τ. Inside the do-while loop are two functions. The first one is to compute the equal-weighted mean of the coverage window, requiring an inner loop to calculate the distances to all the n points. Therefore, the time complexity of this function is 
                           
                              O
                              
                                 (
                                 c
                                 *
                                 τ
                                 *
                                 n
                                 )
                              
                              =
                              O
                              
                                 (
                                 τ
                                 *
                                 
                                    n
                                    3
                                 
                                 )
                              
                           
                        . The second function is to check and break the do-while loop if the stop condition is met, without using a loop. To sum up, the time complexity of the mean shift procedure is O(τ * n
                        3) in the worst case, which is much less than that of the DIPS reduction procedure.

Given the same 28 data sets mentioned in Section 4.1, we have tested the MSMC using four different kernels as following:

                        
                           •
                           Flat kernel 
                                 
                                    g
                                    (
                                    y
                                    )
                                    =
                                    1
                                 
                              ;

Gaussian kernel 
                                 
                                    g
                                    
                                       (
                                       y
                                       )
                                    
                                    =
                                    
                                       e
                                       
                                          −
                                          
                                             
                                                ∥
                                                y
                                                ∥
                                             
                                             2
                                          
                                       
                                    
                                 
                              ;

Epanechnikov kernel 
                                 
                                    g
                                    
                                       (
                                       y
                                       )
                                    
                                    =
                                    1
                                    −
                                    
                                       
                                          ∥
                                          y
                                          ∥
                                       
                                       2
                                    
                                    ,
                                 
                               and

Quartic kernel 
                                 
                                    g
                                    
                                       (
                                       y
                                       )
                                    
                                    =
                                    
                                       
                                          (
                                          1
                                          −
                                          
                                             
                                                ∥
                                                y
                                                ∥
                                             
                                             2
                                          
                                          )
                                       
                                       2
                                    
                                 
                              .

The coverage ratios are presented in Table B.1, where the maximum ones are in bold.
                  

@&#REFERENCES@&#

