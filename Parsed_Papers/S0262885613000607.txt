@&#MAIN-TITLE@&#A review of motion analysis methods for human Nonverbal Communication Computing

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Introduce the concept of nonverbal communication computing and it use cases


                        
                        
                           
                           Review motion analysis techniques used for nonverbal communication computing


                        
                        
                           
                           Discuss future directions of this area


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Nonverbal Communication Computing

Motion analysis

Face tracking

Facial expression recognition

Gesture recognition

Group activity analysis

@&#ABSTRACT@&#


               
               
                  Human Nonverbal Communication Computing aims to investigate how people exploit nonverbal aspects of their communication to coordinate their activities and social relationships. Nonverbal behavior plays important roles in message production and processing, relational communication, social interaction and networks, deception and impression management, and emotional expression. This is a fundamental yet challenging research topic. To effectively analyze Nonverbal Communication Computing, motion analysis methods have been widely investigated and employed. In this paper, we introduce the concept and applications of Nonverbal Communication Computing and also review some of the motion analysis methods employed in this area. They include face tracking, expression recognition, body reconstruction, and group activity analysis. In addition, we also discuss some open problems and the future directions of this area.
               
            

@&#INTRODUCTION@&#

Understanding how people exploit nonverbal aspects of their communication to coordinate their activities and social relationships is a fundamental scientific challenge. Deeper insights into nonverbal communication can have a profound impact on how we link theories of perception, learning, cognition and action to models of interactions and groups at the social level. Models of nonverbal behaviors in interaction are essential for collaboration tools, human–computer and virtual interaction and other assistive technologies designed to support people in real-world activities. This knowledge is also useful to develop models of the deficits of specific populations, such as autistic children, and interventions that bring them into fuller participation in communities. In general, nonverbal communication research offers high-level principles that might explain how people organize, display, adapt and understand such behaviors for communicative purposes and social goals. However, the specifics are generally not fully understood, nor is the way to translate these principles into algorithms and computer-aided communication technologies such as intelligent agents.

To model such complex dynamic processes effectively, novel computer vision and learning algorithms are needed that take into account both the heterogeneity and the dynamicity intrinsic to behavior data. As one of the most active research areas in computer vision, human motion analysis has become a widely-used tool in this area. It uses image sequences to detect and track people, and also to interpret human activities. Emerging automated methods for analyzing motion [1] have been studied and developed to enable tracking diverse human movements precisely and robustly as well as correlating multiple people's movements in interaction. Some of the applications of using motion analysis methods for Nonverbal Communication Computing include deception detection, expression recognition, sign language recognition, behavior analysis, and group activity recognition. In the following we illustrate several examples of Nonverbal Communication Computing.


                     Fig. 1
                      shows an example of deception detection during interactions using an automated motion analysis system [2]. This work investigates how degree of the interactional synchrony can signal whether an interactant is truthful or deceptive. This automated, data-driven and unobtrusive framework consists of several motion analysis methods such as face tracking, gesture detection, facial expression recognition and interactional synchrony estimation. It is able to automatically track gestures and analyze expressions of both the target interviewee and the interviewer, extract normalized meaningful synchrony features and learn classification models for deception detection. The analysis results show that these features reliably capture simultaneous synchrony. The relationship between synchrony and deception is shown to be correlated and complex.

The second example is to use an automated motion analysis system to recognize facial expressions of emotions and fatigue from sleep loss in spaceflight [3]. Specifically, this research project aims to develop non-obtrusive objective means of detecting and mitigating cognitive performance deficits, stress, fatigue, anxiety and depression for the operational setting of spaceflight. To do so, a computational model-based tracker and an emotion recognizer of the human face have been developed to reliably identify when astronauts are displaying various negative emotional expressions and ocular signs of fatigue from sleep loss during space flight. Fig. 2
                      shows an illustration of using this system to recognize the facial expression of emotion. This subject had an emotion of sadness induced by guided recollection of negative memories. The system scored the video clip for a 2min period. Sad was the predominant selection for the frames in the clip. This agreed with the human ratings of sadness as the dominant emotional expression during this period, as well as with the emotion induced.

The third application is an automated detection of non-manual grammatical markings in American Sign Language (ASL) [4], as shown in Fig. 3
                     . Facial expressions and head gestures convey important linguistic information, including cues to the locations of word and phrase boundaries, emphasis on particular sentence parts, affective/emotional state, and attitude. They can offer backchannel information, regulate turn-taking, and provide indicators of speaker confidence, uncertainty, or deception. Using a robust face tracker and 3D warping [5] to extract and combine geometric and appearance features, this system can effectively recognize the eyebrows and the head gestures, as well as their temporal phases. After detecting the linguistically relevant portion of the eyebrow and periodic head gestures, it further leverages this information to improve the detection of non-manual grammatical markers in ASL.

Besides using the non-manual grammatical markings, the hand movement information can also be employed for the discrimination between fingerspelling and continuous signs in American Sign Language. In ASL, fingerspelled words are articulated using one hand (usually the dominant one) in a specific area of the signing space (in front of and slightly above the signer's shoulder). However, automatic identification of fingerspelling portions within a fluent stream of signing is non-trivial, as many of the same handshapes that are used as letters are also used in the formation of other types of signs. Fig. 4
                      shows examples of gesture recognition (Left) and hand tracking results (Right) by coupling dynamically the discrete and continuous trackers [7], using videos of native ASL signers collected and annotated at Boston University as part of the American Sign Language Linguistic Research Project (http://www.bu.edu/asllrp/), in conjunction with the National Center for Sign Language and Gesture Resources.
                        1
                     
                     
                        1
                        The video data and annotations associated with this project are available to the research community from http://www.bu.edu/asllrp/cslgr/.
                     Since this method robustly handles articulations, rotations, abrupt movements and cluttered background, it can accurately discriminate between fingerspelling and non-fingerspelled signs in ASL.

The above examples demonstrate that motion analysis methods such as face tracking are critical to Nonverbal Communication Computing. In this paper, we focus on reviewing the research in the area of human Nonverbal Communication Computing, and especially the motion analysis tools developed to address this problem. We discuss methods to analyze Nonverbal Communication Computing in multiple scales, including face, head, full body and group activities. The remainder of this paper is organized as follows. Section 2 reviews relevant work in human motion analysis and Nonverbal Communication Computing, and also introduces our recent achievements. Section 2.1 covers face tracking methods; Section 2.2 discusses expression recognition; body reconstruction is presented in Section 2.3; and human activity recognition is introduced in Section 2.4. Section 3 summarizes this paper and discusses future directions and open problems.

Research in the area of human Nonverbal Communication Computing can be categorized in two main categories: a) highly structured such as American Sign Language (ASL) and, b) less structured which includes application domains such as detection of deception, emotional expressions, stress, and impairments with respect to cognitive and social skills. Both of them rely on robust motion analysis methods such as tracking, reconstruction and recognition. In the following, we will present the motion analysis methods needed for this line of work and several examples to demonstrate the complexity of the problems.

One of the most important cues for Nonverbal Communication Computing comes from facial motions. Thus accurately tracking head movements and facial actions is very important and has attracted much attention in computer vision and graphics community. Early work typically focused on either rigid head tracking with no facial expression [8,9], or recognizing expressions of a roughly stationary head [10,11]. In contrast, contemporary face tracking systems need to track facial features (e.g. eye corners, nose-tip etc.) under both head motion and varying expressions. A series of face models and tracking algorithms have been developed in recent years. We will introduce face tracking methods based on parametric models, statistical models (Active Shape Models, Constrained Local Models and Active Appearance Models), as well as face tracking from range data.

Parametric face models were first explored to track facial features. Black and Yacoob [12,13] explored the use of local parameterized models and image motion for recovering and recognizing non-rigid and articulated motion of human faces. De Carlo and Metaxas [14,15] described the 3D shape of the face as a polygon mesh, and applied optical flow as a non-holonomic constraint solved by using the least square method. Pighin et al. [16] proposed to use a linear combination of 3D texture-mapped models, each corresponding to a particular basic facial expression. They used a scattered data interpolation technique to deform the face mesh to fit the subject's face from photographs.

The parametric face models have to be carefully designed beforehand, with a set of parameters controlling the deformations driven by elastic forces or image motion. Since the models cannot exactly represent anatomical structures of bones and muscles, unrealistic shapes may be generated. An alternative approach is to learn 3D morphable models from a group of face shapes and textures [17,18], which are usually acquired by high accuracy 3D scans. These 3D face models can represent a wide variety of faces and facial motions. On the other hand, it is computationally expensive and unable to run in real time.

The Active Shape Models (ASMs) [19] learn statistical distributions of 2D feature points, which allow shapes to vary only in ways seen in a training set. Kanaujia and Metaxas [20] built a real-time face tracking system based on ASM. They trained a mixture of ASMs for pre-aligned faces of different clusters, each corresponding to a different pose, as shown in Fig. 5
                           . The target shape is fitted by first searching the local features along the normal direction, followed by constraining the global shape using the most probable cluster.

2D ASM based methods are also combined with 3D face models, which govern the overall shape, orientation and location. Vogler et al. [21] developed a framework to integrate both 2D ASM and 3D deformable models, which allows robust tracking of faces and estimation of both rigid and non-rigid motions. The displacements between the actual projected model points and the identified correspondences are defined as image forces to update the deformation parameters. Yang et al. [22,23] built a face tracker by combining statistical models of both 2D and 3D faces. Shape fitting was performed by minimizing both feature displacement errors and subspace energy terms with temporal smoothness constraints.

Given the limited number of training samples, traditional statistical shape models may overfit and generalize poorly for new samples. Instead of building models on the entire face, Huang et al. [24] built separate ASM models for face components to preserve local shape deformations. They applied Markov Network to provide global geometry constraints. Some recent research work enhanced the ASM fitting by using sparse displacement errors [25–28]. These models are more robust to outliers and partial occlusions.

The constrained local models (CLMs) are extensions of ASM, and they use an independent set of local detectors for landmark detection [29]. CLM fitting is generally posed as the search for the point distribution model (PDM) parameters p, and it jointly minimizes the misalignment error over all landmarks: Q(p)=
                           R(p)+∑
                           
                              i
                              =1
                           
                              n
                           
                           
                           D
                           
                              i
                           (x
                           
                              i
                           ), where R(p) measures the distance of the current shape from the shape distribution, which is often modeled as Gaussian [30] and Gaussian mixture model (GMM) [31]. Di
                           (xi
                           ) measures the misalignment of the ith landmark at position xi
                           . Examples of the misalignment error functions include the Mahalanobis distance for local patch appearances [19], as well as the boosted Harr-like feature based classifiers [29].

As the local landmark detectors are learned from small image regions with limited structures, the maximum responses may not coincide with the correct landmark locations. Some recently proposed methods try to alleviate this problem. Wang et al. [32] proposed a convex quadratic function to fit to the negative log of the response map, from which the mean and covariance of the approximating density can be inferred. Zhou et al. [33] used the summed-squared-difference as a measure of landmark fit, and applied Laplace's approximation to find the covariance estimate. Saragih et al. [34,35] proposed an optimization strategy where a nonparametric representation of the landmark distributions is maximized within a hierarchy of smoothed estimates. The resulting update equations are reminiscent of mean-shift but with a subspace constraint placed on the shape's variability.

The ASM and CLM introduced above only model statistical distributions of the shapes. In contrast, AAM decouples the shape and texture of the deformable object, and is able to generate a variety of photo-realistic instances [36]. Fitting an AAM to an image consists of minimizing the error between the input image and the closest model instance, i.e. solving a nonlinear optimization problem. Matthews and Baker [37] suggested to reformulate AAM model fitting as an image alignment problem, which can be efficiently solved by Lucas–Kanade inverse compositional algorithm [38]. The proposed method avoids updating texture parameters and turns out to be the fastest fitting algorithm for AAM.

AAM has been successfully used for real time face tracking. To deal with pose variations, Cootes et al. [39] proposed view based AAM models, which are a combination of a few 2D models. Sung et al. [40] combined AAM with a cylinder head model, where the global head motion parameters obtained from the cylinder model are used as the cues of the AAM parameters for a good fitting or re-initialization. Xiao et al. [41,42] proposed a real time face tracking algorithm by combining 2D+3D AAM models. Zhou et al. [43] introduced temporal matching constraints to enforce inter-frame coherence in AAM fitting. A comprehensive review is provided by Gao et al. [44].

The face tracking systems using optical cameras suffer from bad lighting conditions, which significantly alter the appearance of feature and cast shadows on faces. In contrast, face tracking systems by using range data are more robust in such conditions. Structured light stereo methods have been applied for capturing depth maps of moving faces [45,46]. Zhang et al. [47] developed a 3D face tracking system by employing synchronized video cameras and structured light projectors to capture streams of images from multiple viewpoints, and the 3D shapes were matched to a template by using both depth error and shape regularization.

Following the recent development of inexpensive depth cameras, there is rapidly growing interest in exploring depth information in vision systems. Fanelli et al. [48] developed a random forest algorithm to estimate head orientations from the range data. Cai et al. [49] developed a maximum likelihood solution to track face shapes from the noisy input depth data. Weise et al. [50] developed a realtime system to reconstruct 3D head shapes from the range data, and used them to generate face animations. Baltrusaitis et al. [51] extended the Constrained Local Model approach to use depth information alongside intensity for facial feature point tracking. Microsoft also published its official Face Tracking SDK that is able to track facial landmark and detect head pose and face expressions in realtime by using a Kinect camera [52].

Based on the tracked face region, we are able to analyze facial expressions. Facial expression recognition has attracted much attention since as early as 1970s, and it has still been widely investigated in the past decade [53–59], for there remain a lot of opening issues due to the complexity and variety of facial expressions.

The previous works of automatic facial expression recognition can be categorized into two main categories: image based methods [60–62] and video-based methods [63,64,53]. The image based methods take only mug shots as observations which capture characteristic images at the apex of the expressions, and recognize expressions according to appearance features [61,65–67,60,53]. For examples, Gabor features were used in [60] and demonstrated to be more robust in low-resolution facial expression recognition [62]. However, it is computationally expensive to convolve face images with multi-banks of Gabor filters in order to extract multi-scale and orientational coefficients. Haar features were employed in [68] and the Haar+Adaboost method is proved to operate at least two orders of magnitude faster than Gabor+SVM method with a comparable recognition rate. Local Binary Pattern features are used to efficiently represent the facial images [69]. In some cases, it is sufficient to do expression recognition based on the information on a single static image. However, a natural facial expression is dynamic, which evolves over time from the onset, the apex, to the offset. The image based methods ignore such dynamic characteristics, so they cannot perform well in most real world settings. In [53], it states that spontaneous deliberately displayed facial behavior has differences both in utilized facial muscles and their dynamics. Psychological researches have also demonstrated that besides the categories of expression, facial expression dynamics is important to decipher its meaning [70]. Therefore, the video-based methods become much popular in recent years [13,71,53], which aim to analyze the dynamics of facial expression for recognition.

For the video-based methods, how to extract and represent the dynamics of facial expression is a key issue. The typical approaches track facial key points, and analyze their motion and geometric variation of facial appearance [72,73]. These approaches highly depend on the facial key point detection and tracking, which should be invariant to occlusions like glasses and facial hair as long as these do not entirely occlude facial key points. On the other hand, they are easily influenced by illumination. Some researchers assume that the dynamics of facial expression are embedded in a manifold subspace, and such manifold subspace can be learned for facial expression recognition [61,74,75]. However, how to decide the dimension of manifold is still an open problem. In [76], Zhao and Peitikainen proposed Volume Local Binary Pattern (VLBP) and LBP-TOP (LBP from three orthogonal planes) descriptors to capture the dynamics of facial expression, which take the video as a volumetric data in the spatio-temporal domain. The volume feature has the advantage of coupling temporal dynamics with spatial appearance tightly. Similar volume features have also been introduced to action recognition [77], video-based face recognition [78], and pedestrian detection [79]. The performance of volume features suffers from the varying speed at which the facial expressions or actions performed by different people at various situations. One solution is to employ a dynamic time warping preprocessing step, but in literature few work discussed this. The other approach is to make the volume feature detectors robust to such variations. For instance, in [77] the sequences are aligned at the start of the motion but diverge at the end of the sequences, and their work automatically learns to ignore the noisy tail ends of the sequences.

Most of the existing 2D intensity image or video feature-based methods are suitable for the analysis of facial expressions under a small range of head motions. Some attempts have been made to produce pose invariant facial expression classifiers. However, most of these attempts have only considered yaw variations of up to 45°, where the whole face is still visible [80]. They do not consider views greater than 45°, when part of the face is occluded. In order to deal with the inherent pose and illumination variations, 3D and 4D (dynamic 3D) recordings are increasingly used in expression analysis research [81]. Zafeiriou and Yin [82] also present a brief overview on this direction. The first systematic effort to collect 3D facial data for facial expression recognition resulted in the creation of BU-3DFE dataset [83], and they also collected a high-resolution 3D dynamic facial expression database BU-4DFE (3D+time) [84] two years later. The majority of systems developed have attempted the recognition of expressions from static 3D facial expression data [85–89], however, more recent methods employ dynamic 3D facial expression data for this purpose [90,91,88,92]. Most methods in the field of facial expressions in 3D are all based on databases of acted, exaggerated expressions of the six basic emotions, although these are significantly different from natural facial expressions occurring in everyday life.

Furthermore, focus is now shifting towards the recognition of spontaneous facial micro-expressions very recently [56,93–97]. Facial micro-expressions are rapid involuntary facial expressions which reveal suppressed affect. In contrast to the large number of facial expression recognition publications, only a few studies have been done on recognizing micro-expressions. Michael et al. [93] proposed a method for automated deception detection using body movement and extracted motion profiles to capture micro-expressions. In [56], they show how temporal interpolation model together with the first comprehensive spontaneous micro-expression corpus enable them to accurately recognize these very short expressions. Shreve et al. [94] used strain patterns as a feature descriptor for spotting micro-expressions in videos. While more recently Wu et al. [95] used GentleSVM, which is a combination of Gentleboost algorithm and SVM classifier, for spotting and recognizing micro-expressions. However, the biggest obstacle of micro-expression recognition to date has been the lack of a suitable database. In [97], they present a novel Spontaneous Micro-expression Database, which is available online to foster the research in this branch.

We have begun our work on facial expression from synthesizing the 3D facial expressions [98]. In this work, the deformable mesh was used to track the facial motions, and the novel expressions can be synthesized after the facial motion was mapped into low dimensional space. The synthesis work was later extended in [99] for visual interactions. We also perform expression classification in the real data [20], based on our facial tracker. Fig. 6
                         shows an example of estimating facial expression. The facial motion is estimated by tracking the landmarks on the faces, and the shape information is also integrated into expression analysis. In order to further analyze the facial expression in the video, the encoded dynamic features, which contain both spatial and temporal information, were developed, and boosting method was applied to handle the large dimension problem [100,101]. In order to handle the time resolution problem, the dynamic binary pattern was further proposed [102,103]. Besides the expression classification, the continuous change of expression also plays a key role in lots of applications. Therefore, we further proposed the ranking model to estimate the expression intensity [104]. Comparing with the previous methods, it was the first time that the intensity order was exploited into a learning phase, and this method achieved state-of-the-art performance.

In addition to face modeling and analysis, whole body motions and gestures are also important factors for Nonverbal Communication Computing. Many applications of Nonverbal Communication Computing, such as the recognition of ASL, need to combine the nonmanual markers (e.g., facial expression) with body movements and gestures to improve the recognition accuracy. Therefore, we have included the discussion of full body reconstruction and 3D pose estimation.

A series of methods have been developed to reconstruct 3D body gesture from monocular video sequences [105–110]. The general framework for 3D pose recovery from monocular sequence has been inspired by the gaining popularity of part-based methods for the problem of 2D human pose alignment in the images. There exists extensive literature on part-based models for the detection and localization of 2D human body parts in images. A few recent works on 2D human pose estimation are [111–117]. Most of these approaches focus on either improving feature extraction to improve part detection confidence or learning efficient priors to model plausible spatial configurations of parts in 2D. Prominent among them is Yang and Ramanan [112] which further enhanced the pictorial structure framework by modeling contextual co-occurrence relations between different part configurations. Poselet based approach [114] uses 3D human pose dataset to improve part detectors and uses Hough transform to vote for the 2D pose configuration. However, all the above approaches focus on estimating 2D pose which is significantly difficult to constrain using standard anthropometric priors.

A much richer literature is in the domain of 3D human pose recovery from monocular images. Several generative [118–121] as well as discriminative [122–125] methods have been proposed for 3D human pose prediction. One major challenge of resolving 3D-pose is that the inverse mapping from observations to (3D pose) states is multi-valued and cannot be functionally or globally approximated [123]. Therefore, these methods primarily focus on resolving ambiguities due to multi-valuedness [122,123,125], and are based on coarse, global feature encodings [124,123,125] such as silhouettes that are often noisy and cannot resolve depth ambiguities in images. Lee and Cohen [126] used local likelihood distribution for body parts to locally refine 3D pose. However, their part appearance modeling is based on skin pixel extraction that has limited application to detecting body parts. More recently the work by Serra et al. [127] investigates 3D pose estimation using 2D part-based models. Off-the-shelf articulated 2D pose detector is used to generate initial hypotheses for the 3D pose estimation framework. Recent studies in feature extraction and depth estimation from a single image [128,129] have also shown that image encodings at multiple scale space can be used to estimate depth.

We have developed a discriminative approach — Bayesian Mixture of Experts (BME) [123]. BME models multivalued image-to-pose relations using several experts. Predictions from these experts are combined in a probabilistic Gaussian mixture, with centers at predicted values. However, the predicted 3D pose is sensitive to image ambiguities and lacks true geometric or kinematic constraints. Therefore, we can use the BME model as an efficient (but approximate) prior to a search space in the greedy optimization framework that we have developed. Fig. 7
                         shows examples of BME predictions. The new framework involves using both global shape cues and local alignment features to search for the optimal 3D pose that best matches a given 2D observation, as shown in Fig. 8
                        . Our 3D search framework introduces a new method of 2D to 3D pose reconstruction that is flexible and can be applied to cluttered real-world images. We combine discriminative and generative approaches that include both local/part-based fitting, as well as global shape, into a single framework. The discriminative initialization can provide efficient robust approximation(s) while the generative 3D model will enable us to resolve ambiguities due to depth and occlusion.

Besides analyzing Nonverbal Communication Computing at the level of individual persons, many researchers have also investigated the group activities employing motion analysis and/or machine learning methods [131–139]. Modeling group activities plays an important role in video surveillance and smart camera systems, and there are many promising applications. For examples, automated recognition and classification of videos enable more efficient video searching, e.g. finding tackles in soccer matches, handshakes in news footage or typical dance moves in music videos. It is also important for automatic surveillance, e.g. monitoring shopping malls. Another example is to support aging in places for the elderly in smart homes. Interaction applications like human–computer interactions also benefit from the advances in automatic human action recognition. Various abnormal activities have been studied, including restricted-area access detection [140], car counting [141], detection of people carrying cases [142], abandoned objects [143], group activity detection [144,145], social network modeling [146], monitoring vehicles [147], scene analysis [148] and so on. Fig. 9
                         shows two sample frames from the BEHAVE dataset [130].

In recent years, a lot of algorithms have been proposed to improve the performance of action/activity analysis. Many of them focus on finding better image representation and features extracted from the image sequences. Ideally, these should generalize over small variations in person appearance, background, viewpoint and action types. At the same time, the representations must be sufficiently rich for robust action classification. Using local descriptors or patches is a popular way to represent human actions. A video sequence is then represented by a collection of independent patches. Accurate localization and background subtraction are not required. The local representations are invariant to changes in viewpoint, person appearance and partial occlusions. Space–time interest points are the locations in space and time where sudden changes of movement occur in the video. Laptev and Lindeberg [149] extended the Harris corner detector [150] to 3D. Space–time interest points are those points where the local neighborhood has a significant variation in both the spatial and the temporal domains. Dollár et al. [151] used dense sampling instead of sparse interest points for feature representation. This method applies Gabor filtering on the spatial and temporal dimensions individually. In addition to intensity and motion cues, Rapantzikos et al. [152] also incorporated color information.

After local interest point detection, local descriptors are applied to summarize an image/video patch. The spatial and temporal size of a patch is usually determined by the scale of the interest point. Schuldt et al. [153] calculated patches of normalized derivatives in space and time. Niebles et al. [154] took the same approach but apply smoothing before reducing the dimensionality using PCA. Dollar et al. [151] tested with both image gradients and optical flow (refer to Mikolajczyk et al. [155] for a detailed survey on features). How to model the relationship among local features is also very important. One solution is to build grids over spatial/temporal domain. Ikizler and Duygulu [156] sampled oriented rectangular patches and bin them into a grid. Zhao and Elgammal [157] used local descriptors around interest points in a histogram with different levels of granularity. Nowozin et al. [158] used a temporal instead of a spatial grid. Another way is to exploit correlations between local descriptors to construct higher-level descriptors. Scovanner et al. [159] constructed a word co-occurrence matrix for a reduced codebook size. Liu et al. [160] used a combination of the space–time features and spin images to represent the correlations of features.

These algorithms have been successfully applied to action recognition problems. They focus on single action with one person (hand-waving, running… [153]) or pair-wise action recognition (answer phone 
                        [161], horse riding 
                        [162]). However, they do not consider interactions among multiple people. For most of the surveillance systems in public area, it is also important to identify group activities. Events like fighting or escaping often involve multiple people and their interactions. Several algorithms for group activity modeling have been proposed in recent years. Different features are used for group activity: human body/body parts [163,164], optical flow [165] and detecting moving regions [166]. Recently, Zhou et al. [144] and Ni et al. [145] used trajectory analysis to describe different group activities.

Modeling social behaviors of people is an important branch to represent group activity, and it has been widely used in evacuation dynamics, traffic analysis and graphics. Pedestrian behaviors have been studied from a crowd perspective, with macroscopic models for crowd density and velocity. On the other end, microscopic models deal with individual pedestrians. A popular model is the Social Force Model [167]. In the Social Force Model, pedestrians react to energy potentials caused by other pedestrians and static obstacles through a repulsive force, while trying to keep a desired speed and motion direction. Helbing and Molnar [167] originally introduced this concept to investigate people movement dynamics. It is also applied to the simulation of crowd behavior [168], virtual reality and studies in computer graphics for creating realistic animations of the crowd [169].

Social behavior analysis has also attracted much attention in the computer vision community. Ali and Shah [170] used the cellular automaton model to track in extremely crowded situations. Antonini et al. [171] proposed a variant of Discrete Choice Model to build a probability distribution over pedestrian positions in next time step. Scovanner and Tappen [172] modeled pedestrians' dynamics and motions as a continuous optimization problem. Pellegrini et al. [173] proposed a Linear Trajectory Avoidance (LTA) method to track multiple targets. Predictions of velocities are computed by the minimization of energy potentials. Recently, Mehran et al. [174] proposed a method to model behaviors among a group of people. It represents the abnormal patterns in a local region based on moving particles. Wu et al. [175] used chaotic invariants of Lagrangian Particle Trajectories to model abnormal patterns in crowded scenes. They have been successfully used in crowded scene modeling. We have also proposed a method named as Interaction Energy Potential to model such interactions [176]. It is based on the relationship between the current state of a person and his/her reactions. Specifically, the relationship between the current state of a subject and the corresponding reaction is explored to model the normal/abnormal patterns. The framework will learn and recognize abnormal events in different environmental contexts.

Although a significant amount of progress on activity recognition has been achieved, there are still many open problems. First, accurate segmentation and tracking are still a challenging task, which are caused by the poor lighting, crowded environments, noisy images, and camera movements. Therefore, developing robust segmentation and tracking methods is always important. Furthermore, most public databases are still based on experimental settings. There is still a big gap between the research and practice. Thus it is necessary to validate current approaches on real-world applications. New applications are also encouraged, such as monitoring doctors and patients in the hospital environment or other health-care facilities.

Research in Nonverbal Communication Computing and motion analysis is maturing, and there are many exciting methods and applications that need to be addressed. Most current systems suffer from the lack of accuracy and robustness, which is a major obstacle when dealing with large data and complex motions. Analytic methods should employ robust tracking and statistical learning methods to identify the important motion parameters that describe behavior. In this section, we discuss the future work and open problems in two main directions: 1) robust motion analysis methods using 3D deformable models, and 2) fusion of domain knowledge and multiple cues.

Most of the above-mentioned motion analysis methods are based on 2D models. Traditional 2D methods are not able to handle large off plane pose changes (e.g., rotations or head tilts) and occlusions. The reason is that objects they track have a 3D shape and a 2D solution tracks the projection on the plane only. Therefore, when the rotation is large (e.g., head shaking), the 2D shape of the face may be degenerated to a thin region.

To deal with such problems, one can train multiple 2D models at different rotations, and switch among multiple models during motion analysis. However, it significantly increases the computational complexity, and the rotation space is actually infinite (it is a continuous variable). Therefore the solution is not as accurate as that from a continuous 3D model. A 2D tracking solution has significant problems with large occlusions since it only tracks 2D projections; for example a hand in front of a face is ambiguous in terms of how far it is from the face in 3D, touching or not. Using 3D models (e.g., a 3D face mask) [178,5,179,178], we can parametrically represent 3D rotations and relative depth, which we can estimate during tracking. Therefore a 3D model can deal seamlessly with occlusions and non-planar movement, as opposed to 2D approaches. In addition, in the case of deformations, it can also deal with 3D deformations that a 2D solution cannot. Fig. 10
                         shows an example of using a 3D deformable model. It is able to handle occlusion and large rotations. Fig. 11
                         shows an example of tracking hand in 3D. It is able to robustly track a sequence of hand rotation and finger movements (such accurate hand tracking results can be employed for deception detection [6]).

In addition, traditional 2D approaches provide the shape (in the form of 2D contours) of the face, eyes, eyebrows, nose, and other subparts, which are often used for recognition. They work well when people face the camera. However, they are challenged with non-frontal facial poses and often are not robust to head shaking, head tilting and large rotations. By contrast, with a 3D model based tracker, one can track inherently 3D parameters which are continuous and not discrete, and can also obtain improved recognition results for both the head pose and related facial deformations. An additional significant benefit of a 3D approach is that it can naturally normalize the tracking pose and facial estimation parameters, which is a requirement for the recognition of pose and expression. In a 2D approach the normalization process is as accurate as in a 3D approach since the perspective distortions are nonlinear [180,181].

Although 3D approaches can achieve promising performance, its main challenge is the computational efficiency. 3D approaches usually have significantly more degrees of freedom and thus estimating 3D parameters is more complex. Therefore, developing real-time 3D solutions is a very important research topic.

To further improve the performance of Nonverbal Communication Computing, researchers have proposed solutions such as the incorporation of nonverbal coding systems and domain knowledge in motion analysis and behavior interpretation. They include kinesics, proxemics, and linguistic knowledge (e.g., in ASL). Possible future contexts will range from highly structured (e.g., interviews and ASL) to little structured (e.g., casual conversations), from face-to-face to mediated contexts, social interactions and social network-based interactions, and will include both normal and impaired communication. These extensions will allow the research of Nonverbal Communication Computing to evolve beyond initial foundational science and proof of concept, to include and solve problems in applied contexts. For example, from linguistic knowledge, we know that changes in eyebrow configuration, in combination with head gestures and other facial expressions, are used to signal essential grammatical information in signed languages. Therefore, we propose methods to recognize the components of eyebrow and periodic head gestures, and successfully improve the detection of non-manual grammatical markings in ASL [4]. The main challenge of these approaches is how to effectively incorporate domain knowledge into traditional models, and how to efficiently solve them. The composite prior models are promising solutions because of their flexibility in modeling prior knowledge and their computational efficiency [182,183].

Finally, some of the clusterings of nonverbal behaviors observed to correlate with specific constructions can be decomposed into components with their own semantic contributions, physical realizations, and linguistic distributions. Therefore, combining multiple nonverbal behaviors can potentially advance the performance of Nonverbal Communication Computing. In addition, combining nonverbal behavior analytics with other behavioral cues will allow eventually the comprehensive study of human behavior and human–computer interaction. A major challenge of these approaches is to effectively fuse these cues, or features. Since these features may have redundant information, sparse methods, especially group sparsity, are potential solutions to fuse them [184–186]. These fusion methods can automatically discover a sparse subset of multiple features and improve the recognition accuracy and efficiency.

@&#SUMMARY@&#

To summarize, although a large amount of work has been done in the area of Nonverbal Communication Computing, there are still many open problems and new promising applications to explore. Other interesting topics and open problems for future research include large-scale data analysis, physics-based modeling, and robust learning. For example, most datasets used for ASL recognition [187,188] are not large-scale. However, to be able to process massive data in real world application, it is necessary to provide ready access to large-scale, high-quality multimodal corpora for several signed and spoken languages, with linguistic annotation, fine-grained computational analysis, and tools for data visualization. This has been constrained by difficulties inherent in collecting, annotating, and analyzing large quantities of video data. Therefore, new protocols should be developed for collection, analysis, storage, and dissemination of high-quality audio/video language corpora larger in scale and more diverse in content than ASL datasets now available. New promising applications are also encouraged. For example, researchers have started to analyze neurological diseases, such as Parkinson's disease and schizophrenia, by coupling motion analysis methods and brain activity. The computational methods developed in Nonverbal Communication Computing can be combined with brain activity analysis to transform the screening and treatments of these neurological spectral disorders. Such applications will have significant scientific and societal impact.

@&#ACKNOWLEDGMENTS@&#

The authors would like to thank all the reviewers for their constructive suggestions. We also would like to thank our long time collaborators Judee Burgoon (UA), David Dinges (UPENN), and Carol Neidle (BU). Metaxas would like to thank his previous PhD students Ioannis Kakadiaris (Univ of Houston), Atul Kanaujia (Object Video), Siome Goldenstein (UNICAMP, Brazil), Dimitris Samaras (SUNY Stony Brook), Christian Vogler (Gallaudet) and Gabriel Tsechpenakis (IUPUI) for their seminal work in this area. Finally, we would like to thank our current CBIM students and colleagues Fei Yang, Xinyi Cui, Mark Dilsizian, Christophe Restif and Qiong Hu for their help and valuable suggestions. This work is supported in part by NSF (award numbers 0964597, 1064965, 1059281), NASA-NSBRI-NBTS01601, NASA-NSBRI-NBTS004, and ONR-N000140910104.

@&#REFERENCES@&#

