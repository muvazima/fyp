@&#MAIN-TITLE@&#A novel monochromatic cue for detecting regions of visual interest

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We study monochromatic cues in the modeling of bottom-up attention.


                        
                        
                           
                           We propose a novel monochromatic cue for ROI detection.


                        
                        
                           
                           Experimental results demonstrate the effectiveness of our method.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Regions of interest (ROIs)

Monochromatic cues

Visual attention

Taxonomy

Performance comparison of algorithms and systems

@&#ABSTRACT@&#


               
               
                  Finding regions of interest (ROIs) is a fundamentally important problem in the area of computer vision and image processing. Previous studies addressing this issue have mainly focused on investigating chromatic cues to characterize visually salient image regions, while less attention has been devoted to monochromatic cues. The purpose of this paper is the study of monochromatic cues, which have the potential to complement chromatic cues, for the detection of ROIs in an image. This paper first presents a taxonomy of existing ROI detection approaches using monochromatic cues, ranging from well-known algorithms to the most recently published techniques. We then propose a novel monochromatic cue for ROI detection. Finally, a comparative evaluation has been conducted on large scale challenging test sets of real-world natural scenes. Experimental results demonstrate that the use of our proposed monochromatic cue yields a more accurate identification of ROIs. This paper serves as a benchmark for future research on this particular topic and a steppingstone for developers and practitioners interested in adopting monochromatic cues to ROI detection systems and methodologies.
               
            

@&#INTRODUCTION@&#

Upon seeing images, humans have a natural tendency to pay more attention to some parts of the images. Over the last few decades, computational modeling of such a mechanism (i.e., selective visual attention) has attracted much research interest in computer vision because of its potential usefulness: many computer vision tasks (e.g., image compression [1,2], image quality assessment [3,4], image segmentation [5–7], image watermarking [8], image classification [9], image retrieval [10], surveillance [11], object detection and recognition [12], robot control, navigational assistance, etc.) can greatly benefit from the ability to identify regions of interest (ROIs), which attract the observer's attention, in an image. A large number of methods for detecting such visually conspicuous image regions have been reported in the literature so far, and they mainly belong to either of two broad categories: top-down and bottom-up. The top-down approaches are directed by prior expectations, i.e., they are task-specific or goal-driven. More specifically, with the proliferation of (pretrained) object recognition systems, they may attempt to estimate the likelihood of existence of interesting objects (e.g., face [13]) at each location and scale in an image. The vast majority of ROI detection methods focus on the behavior of bottom-up attention. The bottom-up approaches often use findings from psychophysics and physiology. Treisman and Gelade [14] suggest the feature integration theory (which is one of the most influential psychological models of bottom-up attention). They claim that a visual stimulus is composed of separable features (e.g., intensity, orientation, and color). Koch and Ullman [15] introduce the concept of saliency map, which quantifies visual attractiveness at each pixel in an image. Building upon their achievement, Itti and colleagues [16] suggest a biologically motivated computational model, and many variants of it are proposed (e.g., [17]).

The central aim of this paper is to study “monochromatic cues” in the modeling of bottom-up attention. Due to their importance in such a modeling [18,19], chromatic cues have been extensively studied in the literature [20–23], whereas investigating monochromatic cues has received less attention. Note here that the monochromatic cues are an important and beneficial complement to the chromatic cues, and this is the motivation of this study: for that reason, to detect salient regions, several authors have employed the monochromatic cues as well as the chromatic cues (e.g., [11,16,24,25]). We first present a taxonomy of existing methods to find ROIs from monochromatic cues, and then introduce a new validated monochromatic cue for the detection of ROIs in an image. We note that the proposed approach is targeted towards “natural scenes” (see examples in Figs. 1 and 7, 8, and 9
                     ). Our proposed method is inspired by the fact that human visual perception is highly adaptive and sensitive to structural information in images [26]: we show how such an attribute of human visual system (HVS) can be effectively extended to solve the problem of ROI detection and prove that, unlike existing biologically inspired methods, the proposed method has great ability of providing powerful contextual information regarding ROIs. We finally conducted an empirical comparative evaluation of the algorithms.

The rest of this paper is organized as follows. In Section 2, a taxonomy of previous techniques to detect ROIs from monochromatic cues is provided. Section 3 describes the details of our proposed method. Experimental results are reported in Section 4, followed by concluding remarks in Section 5.

In this section, we present a taxonomy of existing bottom-up methods to find ROIs from monochromatic cues. One important advantage of the bottom-up approaches is that they do not need any priors. Based on the nature of approach and features used, these methods may be broadly divided into two categories: 1) biologically inspired methods and 2) purely computational methods. ROI detection methodologies in each category may be further classified, i.e., spatial-domain and frequency-domain approaches or local and global approaches. Table 1
                      summarizes the taxonomy of 16 different ROI identification techniques using monochromatic cues.

The biologically inspired methods attempt to simulate mechanisms of preattentive vision: specifically, they focus on the neurobiological hypothesis that human vision may preferentially respond to “high contrast stimuli” [27]. These approaches first extract low-level visual features such as intensity, orientation, and texture, then identify ROIs with local or global contrast analysis (e.g., [16]). Ma and Zhang [28] compute the local spatial contrast of image intensity at each location. The authors argue that the locations with high feature contrast also often have rich information. Liu et al. [24] propose to calculate such a visual feature in a multi-scale manner. Seo and Milanfar [29] suggest a bottom-up method motivated by the center-surround contrast mechanism [16,30] of preattentive biological vision. Rather than taking the classical image features, they propose the use of nonparametric kernel density, which is designed to capture local data structure, as a feature. Kim et al. [11] also exploit the center-surround paradigm based on an ordinal signature of feature distribution. The rationale behind this approach is the fact that the ordinal measure of edge orientation histogram indicates the location of interesting regions. Mancas et al. [31] propose an information-theoretic global approach for ROI detection. The underlying hypothesis of this method is that sparse features over the whole scene attract the observer's attention or interest. In their implementation, the saliency (at each pixel) is defined as self-information of the local mean and variance of image intensity. Gopalakrishnan et al. [25] endeavor to take advantages of both local and global contrast analysis schemes. In particular, the authors suggest using the feature entropy, instead of the naive use of low-level feature.

As alternatives to biologically inspired solutions, numerous purely computational methods have been proposed. These approaches essentially attempt to find ROIs without mimicking attention mechanisms of human vision. Rosin [32] suggests an edge-based ROI detection method. It assumes that dense regions in an edge map correspond to interesting locations. Given a grayscale image, the distance transform is applied to each of the edge maps obtained from threshold decomposition. The transformed signals are summed and then binarized to detect ROIs. Deng and Luo [33] also employ edge information. They formulate the ROI identification problem as finding closed curves in an edge map. Cohen and Basri [34] introduce a computational approach to locate ROIs from not only grayscale images but also binary images. The authors claim that most of ROIs are circle-shaped. This method computes saliency values based on contour criteria such as closure, convexity, and size. Caron et al. [35] suggest the use of power law distribution, which originates from natural language processing, in detecting ROIs. Unlike the above spatial-domain computational approaches, Hou and Zhang [36] show that the frequency-domain approach is a promising solution for ROI detection. They find ROIs from the spectral residual between the log amplitude spectrum of an image and its blurred counterpart. Guo et al. [2,37] argue that the phase spectrum of an image plays an important role rather than the spectral residual in ROI detection. Bian and Zhang [38] demonstrate that the flattened amplitude spectrum is a good indicator to identify ROIs. Recently, Xu et al. [39] propose to use the spatial-frequency information (i.e., one-dimensional or two-dimensional pseudo-Wigner–Ville distribution) due to its excellence in localizing ROIs. In the method, the statistical property of Rényi entropy for the distribution is exploited. The principle of sparse coding is employed to detect ROIs. In such an approach, a set of sparse coding basis functions is pre-computed and to this end the independent component analysis is usually used. For example, Wang et al. [40] estimate the site entropy rate of the random walk on the graph (which is constructed for each feature map) to compute a saliency map.

In this section, we propose a simple yet powerful biologically inspired method for finding ROIs. As discussed in the previous section, most of prior biologically motivated techniques definitely depend on the “contrast-sensitivity” attribute of human vision, but they often fail to discriminate between ROIs and irrelevant clutter (see Fig. 1(b)–(f)). Unlike the traditional (biologically based) approaches, our method does not rely on any contrast analysis models. The method makes use of the fact that human visual perception is highly adaptive and sensitive to structural information in images [26]. Indeed, it allows robust identification of ROIs even in cluttered natural scenes (see Figs. 1(j) and 7(h), 8(h), and 9(h)). Note that our method does not require any training bases and is targeted towards “natural scenes” (see examples in Figs. 1 and 7, 8, and 9).

The motivation of our new approach is to develop a quantitative measure of structural information. From our study, we find that such an image structure metric (ISM) has great ability of providing powerful contextual information regarding ROIs: we will demonstrate that this framework outperforms existing ROI detection techniques using monochromatic cues (refer to Section 4). This new paradigm can be best understood through comparison with the contrast-sensitivity paradigm. Note that, for this demonstration, we have chosen the generalized contrast (GC) analysis model [28,24] (which is the simplest and most widely used contrast measure). A motivating example is shown in Fig. 2(a)–(d), where some different image patches exhibit nearly identical GC. However, their visual attractiveness is drastically different. With the contrast-sensitivity paradigm, it is difficult to explain why Fig. 2(a) and (b) are much more visually appealing than Fig. 2(c) and (d), whereas it is easily discerned with our new paradigm, i.e., the attractiveness scores of Fig. 2(a) and (b) are much higher than those of Fig. 2(c) and (d).

Now, we construct a specific example of an ISM with a series of well-motivated steps. We define the image structure measure building upon image patches, assuming that the distribution of pixel intensities in an image patch is either structured or random. Here, we take advantage of the fact that the presence of structured image components (e.g., edges, ridges, corners, junctions, etc.) is indicated by dominant gradient directions, whereas the absence of dominant gradient directions indicates that there is no structured component. Let ψ denote an image patch of size N
                     ×
                     N. In this paper, a fixed size square image patch (i.e., N
                     =64) is used: it is demonstrably observed that the best results are obtained with the size of 64×64 patch (refer to section 4). Let ψ
                     
                        x
                      and ψ
                     
                        y
                      denote the first-order intensity derivatives of ψ, i.e., ψ
                     
                        x
                     
                     =∂ψ/∂x and ψ
                     
                        y
                     
                     =∂ψ/∂y. We start with transforming an image patch into a gradient vector flow as follows (see Fig. 3(a)):
                        
                           (1)
                           
                              
                                 
                                    F
                                    ψ
                                 
                                 =
                                 
                                    
                                       
                                          
                                             Ψ
                                             
                                                1
                                                1
                                             
                                          
                                          
                                             ⋯
                                          
                                          
                                             Ψ
                                             
                                                N
                                                1
                                             
                                          
                                       
                                       
                                          
                                             ⋮
                                          
                                          
                                             ⋱
                                          
                                          
                                             ⋮
                                          
                                       
                                       
                                          
                                             Ψ
                                             
                                                1
                                                N
                                             
                                          
                                          
                                             ⋯
                                          
                                          
                                             Ψ
                                             
                                                N
                                                N
                                             
                                          
                                       
                                    
                                 
                                 ,
                              
                           
                        
                     where Ψ denotes [ψ
                     
                        x
                      
                     ψ
                     
                        y
                     ]⊤. The next step is to identify dominant (vector) directions (see superimposed arrows in Fig. 3(a)). Let 
                        H
                     
                     
                        ψ
                      denote an oriented gradient histogram of F
                     
                        ψ
                     . For each gradient vector Ψ(m, n), a bin (of the histogram) is increased in value: the orientation (θ
                     =arctan(ψ
                     
                        y
                     /ψ
                     
                        x
                     )) of the gradient determines which bin, and the magnitude (|Ψ|=[ψ
                     
                        x
                     
                     2
                     +
                     ψ
                     
                        y
                     
                     2]1/2) how much is added to it. In this paper, we formulate the identification of dominant orientations as a problem of finding modes in an oriented gradient histogram and solve it using the mean shift algorithm [41] (see Fig. 3(b) and (c)), which is useful to seek modes of data represented as arbitrary-dimensional vector [42]. This allows us to reliably detect dominant gradient directions with a simple thresholding as follows (see Fig. 3(c)):
                        
                           (2)
                           
                              
                                 
                                    Ω
                                    ψ
                                 
                                 =
                                 
                                    
                                       
                                          ω
                                       
                                       
                                          
                                             H
                                             ˜
                                          
                                          ψ
                                       
                                       
                                          ω
                                       
                                       >
                                       ε
                                       ,
                                       −
                                       
                                          180
                                          ∘
                                       
                                       ≤
                                       ω
                                       <
                                       
                                          180
                                          ∘
                                       
                                       ,
                                       0
                                       ≤
                                       ε
                                       ≤
                                       1
                                    
                                 
                                 ,
                              
                           
                        
                     where 
                        
                           
                              
                                 H
                                 ˜
                              
                              ψ
                           
                           
                              ω
                           
                        
                      represents the normalized mean shift filtered histogram for ω
                     =−180∘, ⋯, 179∘. Ω
                     
                        ψ
                      and ε represent a set of dominant directions and a predefined threshold, respectively. In this paper, we set the threshold to be 0.6: it is empirically verified that the best detection performance is achieved when ε
                     =0.6 (refer to Section 4). We then compute the directional residual between the gradient orientation and the nearest dominant orientation, θ(m, n)−
                     ϕ(m, n), for each gradient vector Ψ(m, n), where
                        
                           (3)
                           
                              
                                 ϕ
                                 
                                    m
                                    n
                                 
                                 =
                                 arg
                                 
                                    min
                                    
                                       ω
                                       ∈
                                       
                                          Ω
                                          ψ
                                       
                                    
                                 
                                 
                                    
                                       θ
                                       
                                          m
                                          n
                                       
                                       −
                                       ω
                                    
                                 
                                 .
                              
                           
                        
                     
                  

It is important to note that the directional residual indicates the consistency of gradient orientation: the smaller the directional residuals are, the higher the consistency is (i.e., the more likely the vector flow has dominant orientations). Finally, we define the image structure metric as follows:
                        
                           (4)
                           
                              
                                 
                                    S
                                    ψ
                                 
                                 =
                                 
                                    
                                       ∑
                                       
                                          m
                                          =
                                          1
                                       
                                       N
                                    
                                    
                                       
                                          
                                             ∑
                                             
                                                n
                                                =
                                                1
                                             
                                             N
                                          
                                          
                                             
                                                
                                                   Ψ
                                                   
                                                      m
                                                      n
                                                   
                                                
                                             
                                             cos
                                             
                                                
                                                   θ
                                                   
                                                      m
                                                      n
                                                   
                                                   −
                                                   ϕ
                                                   
                                                      m
                                                      n
                                                   
                                                
                                             
                                             .
                                          
                                       
                                    
                                 
                              
                           
                        
                     
                  

Note that our metric exhibits higher values in structured image components (such as edges, ridges, corners, junctions), allowing discrimination between structured and random components (see examples in Fig. 2). Building upon the proposed measure, the likelihood of each pixel being ROI is given by
                        
                           (5)
                           
                              
                                 R
                                 
                                    x
                                    y
                                 
                                 =
                                 
                                    
                                       S
                                       ¯
                                    
                                    
                                       ψ
                                       
                                          x
                                          y
                                       
                                    
                                 
                                 ,
                              
                           
                        
                     where 
                        
                           
                              S
                              ¯
                           
                           ψ
                        
                      and ψ(x, y) represent the normalized ISM and the image patch centered at pixel (x, y), respectively. Given an image, our proposed method can be summarized as follows:
                        Algorithm 1
                        ROI detection from gradient vector flow


                        For each local image patch ψ(x, y),
                              
                                 1.
                                 Compute the first-order intensity derivatives (∂ψ/∂x and ∂ψ/∂y)

Build gradient vector flow F
                                    
                                       ψ
                                    
                                 

Find dominant orientations of F
                                    
                                       ψ
                                    
                                    
                                       
                                          1
                                          create oriented gradient histogram 
                                                H
                                             
                                             
                                                ψ
                                              of F
                                             
                                                ψ
                                             
                                          

adjust 
                                                H
                                             
                                             
                                                ψ
                                              using the mean shift algorithm

detect modes from the adjusted histogram 
                                                
                                                   
                                                      H
                                                      ˜
                                                   
                                                   ψ
                                                
                                             
                                          

For each gradient vector Ψ(m, n),
                                       
                                          1
                                          compute directional residual θ
                                             −
                                             ϕ
                                          

Calculate ISM 
                                       
                                          
                                             S
                                             ψ
                                          
                                          =
                                          
                                             ∑
                                             
                                                
                                                   ∑
                                                   
                                                      
                                                         
                                                            Ψ
                                                            
                                                               m
                                                               n
                                                            
                                                         
                                                      
                                                      cos
                                                      
                                                         
                                                            θ
                                                            
                                                               m
                                                               n
                                                            
                                                            −
                                                            ϕ
                                                            
                                                               m
                                                               n
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 

In the following section, to confirm the effectiveness of the proposed method, extensive experimental results on large scale challenging test sets of natural scenes are reported. It will be shown that the ISM provides great contextual information for ROI detection.

@&#EXPERIMENTS@&#

@&#RESULTS@&#

In this subsection, we present an empirical performance comparison of 17 different ROI detection methods from both quantitative and qualitative perspectives. We have conducted extensive experiments on a large scale challenging test set of 1897 real-world natural images
                           1
                        
                        
                           1
                           Note that all the color images are converted to grayscale images.
                         randomly selected from the MSRA database [24] and the PASCAL VOC database [43] (examples in Figs. 1 and 7, 8 and 9). For the sake of completeness, we have calculated the receiver operating characteristic (ROC)-area under the curve (AUC) from binary (thresholded) computational ROI maps and human labeled ground truth maps (supplied with the databases). We also have evaluated the precision, recall, and F-measure. To this end, we have employed the Tsai's moment preserving algorithm [44] to compute binary computational ROI maps as recommended in [32] (see examples in Fig. 4
                        ).

We first report the sensitivity of the proposed algorithm to the parameters (N and ε) as shown in Fig. 5
                        . The performance in terms of ROC–AUC is measured by varying both N and ε (0≤
                        qε
                        ≤1): we have varied N (where N
                        =16, 32, 64, 128) and ε (where ε
                        =0.5, 0.6, 0.7, 0.8). From Fig. 5, we can see that the proposed method is not much sensitive to the parameters. It is also empirically demonstrated that the best detection performance is achieved when N
                        =64 and ε
                        =0.6. The resulting ROC curves of 17 different ROI detection methods are shown in Fig. 6
                        . These curves show relative trade-offs between true positive rate (benefits) and false positive rate (costs). To quantitatively evaluate the performance, the ROC–AUCs are calculated from the ROC curves as shown in Table 2
                        . Note that our proposed method has the highest ROC–AUC performance. In Table 2, we also report the precision, recall, and F-measure (={(1+
                        β
                        2) Precision×Recall}/{β
                        2
                        ×Precision+Recall}) of each method. We have set β
                        2
                        =0.3 in this paper. The F-measure is an overall performance measurement. As shown in the table, the proposed method has slightly lower recall (than some state-of-the-art methods) but has the highest precision. In summary, the proposed ROI detection method has the best overall performance (74% on F-measure with N
                        =64 and ε
                        =0.6) among all methods. Figs. 7, 8, and 9
                        
                        
                         illustrate some ROI detection results. Fig. 10
                         shows the resulting precision-recall curves. Now, we present a comparative evaluation of the algorithms on a test set of 531 real-world monochrome (i.e., grayscale) natural images randomly selected from the databases [24,43]. To such end, a total of 565 images are randomly chosen, and then the color images are converted to grayscale images. Six observers were asked to label the images. We have employed the saliency probability map [24] to remove inconsistently labeled images. As a result, 34 images are discarded among the 565 images. In this demonstration, ground truth maps determined by a single observer are used because different observers could generate slightly different ground truth maps. Fig. 11
                         shows the precision-recall curves for the test set. As shown in the figure, the best performance is achieved with the proposed method.

The purpose of this subsection is to empirically demonstrate that our proposed monochromatic cue for the detection of ROIs could be actually a valuable complement to the color cues, enhancing the performance of ROI detection as a whole. In this paper, to that end, we have employed the color cue building upon the center-surround histogram [24]. Let 
                           R
                        
                        
                           m
                         and 
                           R
                        
                        
                           c
                         denote the saliency maps generated by using the proposed monochromatic cue and the above-mentioned color cue [24], respectively. Note that, in our demonstration, we have used not only the weighted summation scheme [24] (i.e., λ
                        
                           m
                        
                        
                           R
                        
                        
                           m
                        
                        +
                        λ
                        
                           c
                        
                        
                           R
                        
                        
                           c
                        , where λ
                        
                           m
                         and λ
                        
                           c
                         denote the weighting factors) but also the multiplication scheme (i.e., 
                           R
                        
                        
                           m
                        
                        ×
                        
                           R
                        
                        
                           c
                        ) to combine the saliency maps. To verify the robustness and effectiveness of our method with the color information, we have compared our approach with some state-of-the-art methods, i.e., Cheng et al. [20], Liu et al. [24], Zhang et al. [45], Achanta et al. [46], Goferman et al. [47], Yan et al. [48], Zhang and Sclaroff [49], Cheng et al. [50], and Cheng et al. [51]. The precision-recall curves of the algorithms on the MSRA database [46] are shown in Fig. 12
                        . From the figure, we can see that our method with the color cue [24] is comparable with some recent published work (Cheng et al. [20], Yan et al. [48], Zhang and Sclaroff [49], Cheng et al. [50], and Cheng et al. [51]) and significantly outperforms other previous methods, and the multiplication-based combination scheme performs better than the weighted summation-based combination scheme. Fig. 13
                         shows some examples of saliency maps. In summary, it is experimentally demonstrated that our proposed monochromatic cue is a promising complement to the color information, and thus their combination can achieve higher detection accuracy.

@&#CONCLUSIONS@&#

In this paper, we have studied “monochromatic cues” in the modeling of bottom-up attention. We have presented a taxonomy of existing bottom-up approaches to identify ROIs from monochromatic cues. We specifically have proposed a new validated monochromatic cue for the detection of ROIs in an image. Extensive comparative evaluation results reported in the previous section demonstrate that the use of our proposed monochromatic cue yields a more accurate identification of ROIs. Currently, our implementation on a 3.0-GHz Intel processor has taken 4.15s for an image of size 400×300 on average. The proposed algorithm can be parallelized on multiple CPUs for significant speedup.

@&#REFERENCES@&#

