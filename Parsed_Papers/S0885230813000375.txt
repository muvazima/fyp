@&#MAIN-TITLE@&#Automatically annotating a five-billion-word corpus of Japanese blogs for sentiment and affect analysis

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           We perform automatic annotation of a large blog corpus with affective information.


                        
                        
                           
                           A survey in emotion corpora shows there is no large emotion corpus for Japanese.


                        
                        
                           
                           The annotations contain emotion classes, emoticons, valence/activation, etc.


                        
                        
                           
                           The annotations are evaluated on a random 1000 sentence sample.


                        
                        
                           
                           The statistics of annotations are compared to other existing emotion corpora.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Emotion corpora

Corpus annotation

Sentiment analysis

Affect analysis

@&#ABSTRACT@&#


               
               
                  This paper presents our research on automatic annotation of a five-billion-word corpus of Japanese blogs with information on affect and sentiment. We first perform a study in emotion blog corpora to discover that there has been no large scale emotion corpus available for the Japanese language. We choose the largest blog corpus for the language and annotate it with the use of two systems for affect analysis: ML-Ask for word- and sentence-level affect analysis and CAO for detailed analysis of emoticons. The annotated information includes affective features like sentence subjectivity (emotive/non-emotive) or emotion classes (joy, sadness, etc.), useful in affect analysis. The annotations are also generalized on a two-dimensional model of affect to obtain information on sentence valence (positive/negative), useful in sentiment analysis. The annotations are evaluated in several ways. Firstly, on a test set of a thousand sentences extracted randomly and evaluated by over forty respondents. Secondly, the statistics of annotations are compared to other existing emotion blog corpora. Finally, the corpus is applied in several tasks, such as generation of emotion object ontology or retrieval of emotional and moral consequences of actions.
               
            

@&#INTRODUCTION@&#

There is a lack of large corpora for Japanese applicable in sentiment and affect analysis. Although there are large corpora of newspaper articles, like Mainichi Shinbun Corpus
                        1
                     
                     
                        1
                        
                           http://www.nichigai.co.jp/sales/mainichi/mainichi-data.html.
                     , or corpora of classic literature, like Aozora Bunko
                        2
                     
                     
                        2
                        
                           http://www.aozora.gr.jp/.
                     , they are usually unsuitable for research on emotions since spontaneous emotive expressions either appear rarely in these kinds of texts (newspapers), or the vocabulary is not up to date (classic literature). Although there exist speech corpora, such as Corpus of Spontaneous Japanese
                        3
                     
                     
                        3
                        
                           http://www.ninjal.ac.jp/products-k/katsudo/seika/corpus/public/.
                     , which could become suitable for this kind of research, due to the difficulties with compilation of such corpora they are relatively small. In research such as the one by Abbasi and Chen (2007) it was proved that public Internet services, such as forums or blogs, are a good material for affect analysis because of their richness in evaluative and emotive information. One kind of these services are blogs, open diaries in which people encapsulate their own experiences, opinions and feelings to be read and commented by other people. Recently blogs have come into the focus of opinion mining or sentiment and affect analysis (Aman and Szpakowicz, 2007; Quan and Ren, 2010). Therefore creating a large blog-based emotion corpus could help overcome both problems: the lack in quantity of corpora and their applicability in sentiment and affect analysis. There have been only a few small Japanese emotion corpora developed so far (Hashimoto et al., 2011). On the other hand, although there exist large Web-based corpora (Erjavec et al., 2008; Baroni and Ueyama, 2006), access to them is usually allowed only from the Web interface, which makes additional annotations with affective information difficult. In this paper we present the first attempt to automatically annotate affect on YACIS, a large scale corpus of Japanese blogs. To do that we use two systems for affect analysis of Japanese, one for word- and sentence-level affect analysis and another especially for detailed analysis of emoticons, to annotate on the corpus different kinds of affective information (emotive expressions, emotion classes, etc.).

The outline of the paper is as follows. Section 2 describes the related research in emotion corpora. Section 3 presents our choice of the corpus for annotation of affect- and sentiment-related information. Section 4 describes tools used in annotation. Section 5 presents detailed data and evaluation of the annotations. Section 6 presents tasks in which the corpus has already been applied. Finally the paper is concluded and future applications are discussed.

Research on affect analysis has resulted in a number of systems developed within several years (Aman and Szpakowicz, 2007; Ptaszynski et al., 2009c; Matsumoto et al., 2011). Unfortunately, most of such research ends in proposing and evaluating a system. The real world application that would be desirable, such as annotating affective information on linguistic data is limited to processing a usually small test sample in the evaluation. The small number of annotated emotion corpora that exist are mostly of limited scale and are annotated manually. Below we describe and compare some of the most notable emotion corpora. Interestingly, six out of eight emotion corpora described below are created from blogs. The comparison is summarized in Table 1
                     . We also included information on the work described in this paper for better comparison (YACIS).


                     Quan and Ren (2010) created a Chinese emotion blog corpus Ren-CECps1.0. They collected 500 blog articles from various Chinese blog services, such as sina blog (http://blog.sina.com.cn/), qq blog (http://blog.qq.com/), etc., and annotated them with a large variety of information, such as emotion class, emotive expressions or polarity level. Although syntactic annotations were simplified to tokenization and POS tagging, this corpus can be considered a state-of-the-art emotion blog corpus. The motivation for Quan and Ren is also similar to ours – dealing with the lack of large corpora for sentiment analysis in Chinese (in our case – Japanese).


                     Wiebe et al. (2005) report on creating the MPQA corpus of news articles. The corpus contains 10,657 sentences in 535 documents.
                        4
                     
                     
                        4
                        The new MPQA Opinion Corpus version 2.0 contains additional 157 documents, 692 documents in total.
                      The annotation schema includes a variety of emotion-related information, such as emotive expressions, emotion valence, intensity, etc. However, Wiebe et al. focused on detecting subjective (emotive) sentences, which do not necessarily convey emotions, and classifying them into positive and negative. Thus their annotation schema, although one of the richest, does not include emotion classes.

A corpus of Japanese blogs, called KNB, rich in the amount and diversification of annotated information was developed by Hashimoto et al. (2011). It contains 67 thousand words in 249 blog articles. Although it is a rather small scale corpus, it developed a certain standard for preparing corpora, especially blog corpora for sentiment and affect-related studies in Japan. The corpus contains all relevant grammatical annotations, including POS tagging, dependency parsing or Named Entity Recognition. It also contains sentiment-related information. Words and phrases expressing emotional attitude were annotated by laypeople as either positive or negative. One disadvantage of the corpus, apart from its small scale, is the way it was created. Eighty-one students were employed to write blogs about different topics especially for the need of this research. It could be argued that since the students knew their blogs will be read mostly by their teachers, they selected their words more carefully than they would in private.


                     Aman and Szpakowicz (2007) constructed a small-scale English blog corpus. They did not include any grammatical information, but focused on affect-related annotations. As an interesting remark, they were some of the first to recognize the task of distinguishing between emotive and non-emotive sentences. This problem is usually one of the most difficult in text-based affect analysis and is therefore often omitted in such research. In our research we applied a system proved to deal with this task with high accuracy for Japanese.


                     Das and Bandyopadhyay (2010) constructed an emotion annotated corpus of blogs in Bengali. The corpus contains 12,149 sentences within 123 blog posts extracted from Bengali web blog archive (http://www.amarblog.com/). It is annotated with face recognition annotation standard (Ekman, 1992).


                     Matsumoto et al. (2011) created 
                        Wakamono Kotoba
                      (Slang of the Youth) corpus. It contains unrelated sentences extracted manually from Yahoo! blogs (http://blog-search.yahoo.co.jp/). Each sentence contains at least one word from a slang lexicon and one word from an emotion lexicon, with additional emotion class tags added per sentence. The emotion class set used for annotation was chosen subjectively, by applying the 6 class face recognition standard and adding 3 classes of their choice.


                     Mishne (2005) collected a corpus of English blogs from LiveJournal
                        5
                     
                     
                        5
                        
                           http://www.livejournal.com/.
                      blogs. The corpus contains 815,494 blog posts, from which many are annotated with emotions (moods) by the blog authors themselves. The LiveJournal service offers an option for its users to annotate their mood while writing the blog. The list of 132 moods include words like “amused”, or “angry”. The LiveJournal mood annotation standard offers a rich vocabulary to describe the writer's mood. However, this richness has been considered troublesome to generalize the data in a meaningful manner (Quan and Ren, 2010).

Finally, Minato et al. (2006) collected a 14,195 word, 1191 sentence corpus. The corpus is a collection of sentence examples from a dictionary of emotional expressions (Hiejima, 1995). The dictionary was created for the need of Japanese language learners. Differently to the dictionary applied in our research (Nakamura, 1993), in Hiejima (1995) sentence examples were mostly written by the author of the dictionary himself. The dictionary also does not propose any coherent emotion class list, but rather the emotion concepts are chosen subjectively. Although the corpus by Minato et al. is the smallest of all mentioned above, its statistics is described in detail. Therefore in this paper we use it as one of the Japanese emotion corpora to compare our work to.

All of the above corpora were annotated manually or semi-automatically. In this research we performed the first attempt to annotate a large scale blog corpus (YACIS) with affective information fully automatically. We did this with systems based on positively evaluated affect annotation schema, performance, and standardized emotion class typology.

Although Japanese is a well recognized and described world language, there have been only few large corpora for this language. For example, Erjavec et al. (2008) gathered a 400-million-word scale Web corpus JpWaC, or Baroni and Ueyama (2006) developed a medium-sized corpus of Japanese blogs jBlogs containing 62 million words. However, both research faced several problems, such as character encoding, or web page metadata extraction, such as the page title or author which differ between domains. Apart from the above mentioned medium sized corpora at present the largest Web based blog corpus available for Japanese is YACIS or Yet Another Corpus of Internet Sentences. We chose this corpus for the annotation of affective information for several reasons. It was collected automatically by Maciejewski et al. (2010) from the pages of Ameba blog service. It contains 5.6 billion words within 350 million sentences. Maciejewski et al. were able to extract only pages containing Japanese posts (pages with legal disclaimers or written in languages other than Japanese were omitted). In the initial phase they provided their crawler, optimized to crawl only Ameba blog service, with 1000 links taken from Google (response to one simple query: ‘site:ameblo.jp’). They saved all pages to disk as raw HTML files (each page in a separate file) and afterward extracted all the posts and comments and divided them into sentences. The original structure (blog post and comments) was preserved, thanks to which semantic relations between posts and comments were retained. The blog service from which the corpus was extracted (Ameba) is encoded by default in Unicode, thus there was no problem with character encoding. It also has a clear and stable HTML meta-structure, thanks to which they managed to extract metadata such as blog title and author. The corpus was first presented as an unannotated corpus. Recently Ptaszynski et al. (2012b) annotated it with syntactic information, such as POS, dependency structure or Named Entity Recognition. An example of the original blog structure in XML is represented in Fig. 1
                     . Some statistics about the corpus are represented in Table 2
                     .


                     Emotive Expression Dictionary (Nakamura, 1993) is a dictionary developed by Akira Nakamura in a period of over 20-year time. It is a collection of over two thousand expressions describing emotional states collected manually from a wide range of literature. It is not a tool per se, but was converted into an emotive expression database by Ptaszynski et al. (2009c,a) in their research on affect analysis of utterances in Japanese. Since YACIS is a Japanese language corpus, for the annotation of expressions of emotions on the corpus we needed to choose a lexicon proved to be the most appropriate for the Japanese language. Nakamura's dictionary is a state-of-the art example of a hand-crafted lexicon of emotive expressions. It also proposes a classification of emotions that reflects the Japanese language and culture the most appropriately. This classification is also applied in the lexicon itself. All expressions are classified as representing a specific emotion type, one or more if applicable. In particular, Nakamura proposes ten emotion types the most appropriate for the Japanese language and culture. These are: 
                        
                      
                     ki/yorokobi
                     
                        6
                     
                     
                        6
                        Separation by “/” represents two possible readings of the character.
                      (joy, delight; later referred to as joy), 
                        
                      
                     dō/ikari (anger), 
                        
                      
                     ai/aware (sorrow, sadness, gloom; later referred to as gloom), 
                        
                      
                     fu/kowagari (fear), 
                        
                      
                     chi/haji (shame, shyness, bashfulness; later referred to as shame), 
                        
                      
                     kō/suki (liking, fondness; later referred to as fondness), 
                        
                      
                     en/iya (dislike, detestation; later referred to as dislike), 
                        
                      
                     kō/takaburi (excitement), 
                        
                      
                     an/yasuragi (relief), and 
                        
                      
                     kyō/odoroki (surprise, amazement; later referred to as surprise). The distribution of separate expressions across all emotion classes is represented in Table 4
                     .

A frequent manner in text-based affect analysis research is applying a list of emotion classes based on other modalities than linguistic, such as face recognition, or simply creating a new class list for the need of a particular research (see Table 1 for details). In our research we aimed at contributing to the standardization of emotion class list in the language based research on emotions in Japanese. Therefore we selected Nakamura's emotion type list, developed for over 20 years, as the most appropriate for our research.


                     ML-Ask, or eMotive eLement and Expression Analysis system is a keyword-based language-dependent system for automatic affect annotation on utterances in Japanese constructed by Ptaszynski et al. (2009c,a). It uses a two-step procedure:
                        
                           1.
                           Specifying whether an utterance is emotive, and

Recognizing the particular emotion types in utterances described as emotive.


                              Emotive elements or emotemes, which indicate that a sentence is emotive, but do not detail what specific emotions have been expressed. For example, interjections such as “whoa!” or “Oh!” indicate that the speaker (producer of the utterance) have conveyed some emotions. However, it is not possible, basing only on the analysis of those words, to estimate precisely what kind of emotion the speaker conveyed. Ptaszynski et al. include in emotemes such groups as interjections, mimetic expressions, vulgar language and emotive markers. The examples in Japanese are respectively: sugee (great! - interjection), wakuwaku (heart pounding - mimetic), -yagaru (syntactic morpheme used in verb vulgarization), (∧_∧) (emoticon expressing joy) and ‘!’, or ‘??’ (sentence markers indicating emotiveness). Ptaszynski et al. collected and hand-crafted a database of 907 emotemes. A set of features similar to what is defined by Ptaszynski et al. as emotemes has been also applied in other research on discrimination between emotive (emotional/subjective) and non-emotive (neutral/objective) sentences (Wiebe et al., 2005; Aman and Szpakowicz, 2007; Wilson and Wiebe, 2005).


                              Emotive expressions are words or phrases that directly describe emotional states, but could be used to both express one's emotions and describe the emotion without emotional engagement. This group could be realized by such words as aijou (love - noun), kanashimu (feel sad, grieve - verb), ureshii (happy - adjective), or phrases such as: mushizu ga hashiru (to give one the creeps [of hate]) or ashi ga chi ni tsukanai (walk on air [of happiness]). As the collection of emotive expressions ML-Ask uses the database created on the basis of Nakamura's Emotive Expressions Dictionary. Examples from the affect database are represented in Table 3
                              . A common approach in text-based affect analysis is either to construct an emotion class list ad hoc convenient for the research, or to apply the names of emotion classes from other fields, such as face or voice recognition. This comes from a lack of firm standardization of emotion class naming. In our research we aimed at promoting standardized approach. Therefore the fact that ML-Ask incorporates the reliable emotion lexicon based on a firm and long-time study, was one of the reasons to choose this system for annotation of YACIS.

With these settings ML-Ask was proved to distinguish emotive sentences from non-emotive with a very high accuracy (over 90%) and to annotate affect on utterances with a sufficiently high Precision (85.7% compared to human annotators), but low Recall (54.7%) (Ptaszynski et al., 2009a,c). The low Recall is a disadvantage of the system, but we assumed that in a corpus as big as YACIS there should still be millions of annotations. Another reason to choose ML-Ask was that it is the only present system recognized to fully implement the idea of Contextual Valence Shifters in Japanese.

The idea of Contextual Valence Shifters (CVS) was first proposed by Polanyi and Zaenen (2006). They distinguished two kinds of CVS: negations and intensifiers. The group of negations contains words and phrases like “not”, “never”, and “not quite”, which change the valence polarity of the semantic orientation of an evaluative word they are attached to. The group of intensifiers contains words like “very”, “very much”, and “deeply”, which intensify the semantic orientation of an evaluative word. ML-Ask fully incorporates the negation type of CVS with a 108 syntactic negation structures. Examples of CVS negations in Japanese are structures such as: amari -nai (not quite-), -to wa ienai (cannot say it is-), or -te wa ikenai (cannot+[verb]-). As for intensifiers, although ML-Ask does not include them as a separate database, most Japanese intensifiers are included in the emoteme database. The system also calculates emotive value, or emotional intensity of a sentence, on the basis of the number of emotemes in the sentence. The performance of setting the emotive value was evaluated on 84% of accuracy (Ptaszynski et al., 2009a). Finally, the last distinguishable feature of ML-Ask is implementation of Russell's two dimensional affect space (Russell, 1980). It assumes that all emotions can be represented in two dimensions: the emotion's valence or polarity (positive/negative) and activation (activated/deactivated). An example of negative-activated emotion could be “anger”; a positive-deactivated emotion is, e.g., “relief”. The mapping of Nakamura's emotion types on Russell's two dimensions proposed by Ptaszynski et al. (2009c) was proved reliable in several research (Ptaszynski et al., 2010, 2009c,b). An example of ML-Ask output is represented in Fig. 3. The mapping of Nakamura's emotion types on Russell's space is represented in Fig. 2
                     .

We also compiled a simpler version of ML-Ask, not using emotemes, only the emotive expression dictionary with CVS and Russell's emotion space (called further ML-Ask-simple) and performed additional annotations without distinguishing between emotive and non-emotive sentences. This was done for three reasons. Firstly, to check how many of non-emotive sentences contain emotive expressions, which information is often provided in research on emotion corpora (Minato et al., 2006), and therefore is useful in comparison of corpora. Secondly, although ML-Ask distinguishes between emotive and non-emotive sentences with high accuracy, there could be made a system that does that better in the future. Finally, in research on opinion mining and sentiment analysis it has been shown that it is possible to predict sentiment without considering emotiveness of sentences (Turney and Littman, 2002). Therefore annotations performed with ML-Ask-simple could be applied in such research.


                     CAO is a system for estimation of emotions conveyed through emoticons
                        7
                     
                     
                        7
                        In particular Japanese emoticons called kaomoji.
                      developed by Ptaszynski et al. (2010). Emoticons are sets of symbols widely used to convey emotions in text-based online communication, such as blogs. CAO, or emotiCon Analysis and decOding of affective information system extracts an emoticon from an input (a sentence) and determines specific emotion types expressed by it using a three-step procedure. Firstly, it matches the input to a predetermined raw emoticon database containing over ten thousand emoticons. The emoticons, which could not be estimated using only the database are automatically divided into semantic areas, such as representations of “mouth” or “eyes”, based on the idea of kinemes, or minimal meaningful body movements, from the theory of kinesics (Birdwhistell, 1952). The areas are automatically annotated according to their co-occurrence in database. The annotation is firstly based on eye-mouth-eye triplet. If no triplet was found, all semantic areas are estimated separately and summarized. This provides information about potential groups of expressed emotions giving the system coverage of over 3 million possibilities. The performance of CAO was evaluated as nearly ideal (Ptaszynski et al., 2010) (exceeding 97% of F-score, with P
                     =100% and R
                     =94.3%) which proved CAO as a reliable tool for analysis of Japanese emoticons. In the annotation process CAO was used as a supporting procedure in ML-Ask to improve the performance of the affect annotation system and add detailed information about emoticons appearing in the text. An example of CAO output is represented in Fig. 3
                     .

It is physically impossible to manually evaluate all annotations on the corpus.
                        8
                     
                     
                        8
                        Having 1s to evaluate one sentence, one evaluator would need 11.2 years to verify the whole corpus (354mil.s.).
                      Therefore we applied three different types of evaluation. First was based on a sample of 1000 sentences randomly extracted from the corpus and annotated by laypeople. In second we compared YACIS annotations to other emotion corpora. The third evaluation was application based and is described in Section 6.


                     Evaluation of affective annotations: Since the performance of affect analysis systems differs depending on the type of test set applied in evaluation, we needed to verify actual performance of ML-Ask and CAO on YACIS. ML-Ask was previously positively evaluated on separate sentences and on an online forum. It was not yet evaluated on blogs (neither on Ameblo nor on any other). Moreover, the version of ML-Ask supported by CAO has not been evaluated thoroughly as well.

In the evaluation we used a test set created by Ptaszynski et al. (2010) for the evaluation of CAO. It consists of thousand sentences randomly extracted from YACIS. They were manually annotated with emotion classes by 42 layperson annotators in an anonymous survey. There are 418 emotive and 582 non-emotive sentences. We compared the results on those sentences for ML-Ask, CAO (described in detail in Ptaszynski et al., 2010), and ML-Ask supported with CAO. We calculated the results for Precision, Recall, F1-measure, Accuracy (separately for all sentences and only for emotive sentences), Error rate, and agreement coefficient of the system with annotators as Cohen's kappa. The results are summarized in Table 5
                     .

Firstly we verified the performance of discrimination between emotive and non-emotive sentences. The result of ML-Ask baseline was a high 98.8% of Accuracy, which was much higher than in original evaluation of ML-Ask (around 90%). This could indicate that sentences with which the system was not able to deal with appear less frequently on Ameblo. As for CAO, although it does not include such procedure per se, it is capable of detecting the presence of emoticons in a sentence, which is partially equivalent to detecting emotive sentences in ML-Ask, since emoticons are one kind of expressions distinguishable in emotive sentences. The performance of CAO was also high, 97.6% of Accuracy. This was due to the fact that grand majority of emotive sentences contained emoticons. Finally, ML-Ask supported with CAO achieved remarkable 100% accuracy. This was a surprisingly good result, although it must be remembered that the test sample contained only 1000 sentences (less than 0.0003% of the whole corpus).

Next we verified emotion class annotations on sentences. The baseline of ML-Ask achieved better results (73.4% of Accuracy, 84.7% of balanced F1-score with P=100% and R=73.4%) than in its primary evaluation in Ptaszynski et al. (2009c) (F1=67%, P=85.7% and R=54.7%). For the record, we also checked the results of ML-Ask-simple, although it was predictable the performance will be lower, as some amount of non-emotive sentences contain emotive expressions, which negatively influence the results, and ML-Ask-simple does not perform emotive/non-emotive sentence filtering. The result of ML-Ask-simple was 65.4% of Accuracy. CAO achieved 80.1% of Accuracy. Interestingly, this makes CAO a better affect analysis system than ML-Ask. 
                        9
                     
                     
                        9
                        This could mean that it is easier to detect emotions basing only on emoticons than on language, or that emoticons express emotions in a more straightforward and unambiguous way than language.
                      However, the condition is that a sentence must contain an emoticon (which was the case in the test set). The best result 90%, was achieved by ML-Ask supported with CAO. We also checked the results when only the dimensions of valence and activation were taken into account. ML-Ask achieved 88.5% of Accuracy, CAO 94.5%. Support of CAO to ML-Ask again resulted in the best score, 97.4%.


                     Statistics of affective annotations: There were nearly twice as many emotive sentences than non-emotive (ratio 1.94). Moreover, when all sentences are considered, there was approximately one emoteme class per sentence in general (ratio 0.92). This suggests that the corpus is biased in favor of emotive contents, which could be considered as a proof for the assumption that blogs make a good base for emotion related research. In previous research it has been often assumed that blogs make a good base for creating emotion corpora as they contain many emotive contents. The statistics we provide here (Table 6
                     ) could be considered as the first attempt to provide a quantitative confirmation for those claims. When it comes to statistics of each emotive feature (emoteme), the most frequent class were interjections. Second frequent was the exclamative marks class, which includes punctuation marks suggesting emotive engagement (such as “!”, or “??”). Third frequent emoteme class was emoticons, followed by endearments. As an interesting remark, emoteme class that was the least frequent were vulgarities. As one possible interpretation of this result we propose the following. Blogs are social space, where people describe their experiences to be read and commented by other people (friends, colleagues). The use of vulgar language could discourage potential readers from further reading, making the blog less popular. Next, we checked the statistics of emotion classes annotated on emotive sentences. The results are represented in Table 7
                      and in Fig. 4
                     . The most frequent emotions were joy (31%), dislike (20%) and fondness (19%). These emotions are the ones related the most to appraising events and experiences in a straightforward manner. Only these three emotion classes covered over 70% of all annotations. The result is reasonable, since it is normal in casual talk in Japanese to describe and evaluate one”s experiences in a simple straightforward manner. The below example is a typical way of or appraising (as unpleasant) an experience (hangover).
                        
                           
                              
                           
                        
                        
                           Futsukayoi ha iya da na ∼
                        Hangover TOP unpleasant COP SFP VowPr
                              10
                           
                           
                              10
                              TOP=topic marker, COP=copula, SFP=sentence final particle, VowPr=vowel prolongation. Grammatical annotations according to the Leipzig Glossing Rules standard (Comrie et al., 2004).
                           
                        
                        Translation: Oh, I don’t like having a hangover...
                     
                  

However, it could happen that the number of expressions included in each emotion class database influenced the number of annotations (database containing many expressions has higher probability to gather more annotations). Therefore we verified if there was a correlation between the number of annotations and the number of emotive expressions in each emotion class database. The verification was based on Spearman's rank correlation test between the two sets of numbers. The test revealed no statistically significant correlation between the two types of data, with ρ
                     =0.38. Next, we analyzed the generalizations of emotive annotations on Russell's two-dimensional scale (valence and activation). Annotations of the two dimensions are represented in Table 8
                      and Figs. 5 and 6
                     
                     . We also compared the two sets of results provided by ML-Ask and ML-Ask-simple. ML-Ask first separates emotive sentences from non-emotive and annotates emotion classes only on emotive sentences. ML-Ask-simple does not diversify between emotive and non-emotive sentences and annotates emotion classes on all sentences. This means that the latter annotates an emotion class on a sentence even if the sentence is non-emotive and was not written to express any emotion. Sentences like these are usually used to, for example, deliberate about a particular emotion, or express a piece of non-emotive opinion. Examples of such sentences are represented below.


                     
                        
                           
                              
                              
                                 
                                    Deliberation about emotion
                                 
                              
                              
                                 
                                    “All parents love their children.”
                                 
                                 
                                    “Fear is an emotion people experience in dangerous situations.”
                                 
                                 
                                    “There is a difference between feeling relieved and feeling sad.”
                                 
                                 
                                    
                                 
                              
                           
                           
                              
                              
                                 
                                    Non-emotive opinion
                                 
                              
                              
                                 
                                    “The movie was interesting.”
                                 
                                 
                                    “Its a good piece of literature, but it doesn’t move me.”
                                 
                              
                           
                        
                     
                  

It was predictable that ML-Ask-simple would provide more annotations. However, it could happen that for some emotion types there were larger differences, which would suggest that emotive sentence discrimination in ML-Ask is worse for sentences with some emotion classes. For example, if the differences between the two annotation sets were similar for most emotion classes, but differ greatly for one or two emotion classes, it could be assumed that some of those sentences are in fact emotive, but ML-Ask was unable to discover that. This phenomenon was noticed by Ptaszynski et al. (2009c) in their analysis of Japanese online forum 2channel. In particular, they report that in the test set (containing about two thousand sentences from a single forum thread) ML-Ask annotations were statistically similar to a gold standard (human annotations) for eight out of ten emotion classes. Detailed analysis of the two problematic outliers revealed that the cause lied in enormous number of emoticons used to express excitement and dislike, which their version of ML-Ask could not process. In our research we supported ML-Ask with CAO (emoticon analysis system), which solves the problem for most cases. We compared the percentage of emotion class annotations on emotive sentences and on all sentences to find potential outliers. Although we assumed there would be some differences, surprisingly for all emotion classes the results were similar, with emotive sentences being approximately 75% of all annotated sentences (25% were non-emotive sentences containing emotive expressions). Even after excluding the lowest and the highest results, the average still remained the same (74.589% emotive and 25.411% non-emotive). The annotations were well balanced, which proves that ML-Ask, although not ideal, is a reliable and stable affect annotation system. Although ML-Ask was able to specify emotion class only for 22% of all emotive sentences. The annotations of ML-Ask-simple, except providing statistical proof for ML-Ask performance have another potential applicability. As stated above, among non-emotive sentences containing emotive expressions some sentences are representations of opinions. If the opinion sentences could be separated from purely neutral ones, they could become useful as additional training data in Opinion Mining.


                     Comparison with other emotion corpora: Firstly, we compared YACIS with KNB. The KNB corpus was annotated mostly for the need of sentiment analysis and therefore does not contain any information on specific emotion classes. However, it is annotated with emotion valence for different categories valence is expressed in Japanese, such as emotional attitude (e.g., “to feel sad about X” [NEG], “to like X” [POS]), opinion (e.g., “X is wonderful” [POS]), or positive/negative event (e.g., “X broke down” [NEG], “X was awarded” [POS]). We compared the ratios of sentences expressing positive to negative valence. The comparison was made for all KNB valence categories separately and as a sum. In our research we do not make additional sub-categorization of valence types, but used in the comparison ratios of sentences in which the expressed emotions were of only positive/negative valence and including the sentences which were mostly (in majority) positive/negative. The comparison is presented in Table 9
                     . In KNB for all valence categories except one the ratio of positive to negative sentences was biased in favor of positive sentences. Moreover, for most cases, including the ratio taken from the sums of sentences, the ratio was similar to the one in YACIS (around 1.7). Although the numbers of compared sentences differ greatly, the fact that the ratio remains similar across the two different corpora suggests that the Japanese express in blogs more positive than negative emotions.

Next, we compared the corpus created by Minato et al. (2006). This corpus was prepared on the basis of an emotive expression dictionary. Therefore we compared its statistics not only to YACIS, but also to the emotive lexicon used in our research (see Section 4 for details). Emotion classes used in Minato et al. differ slightly to those used in our research (YACIS and Nakamura's dictionary). For example, they use class name “hate” to describe what in YACIS is called “dislike”. Moreover, they have no classes such as excitement, relief or shame. To make the comparison possible we used only the emotion classes appearing in both cases and unified all class names. The results are summarized in Table 10
                     . There was no correlation between YACIS and Nakamura (ρ
                     =0.25), which confirms the results calculated in previous paragraph. A medium correlation was observed between YACIS and Minato et al. (ρ
                     =0.63). Finally, a strong correlation was observed between Minato et al. and Nakamura (ρ
                     =0.88), which is the most interesting observation. Both Minato et al. and Nakamura are in fact dictionaries of emotive expressions. However, the dictionaries were collected in different times (difference of about 20 years), by people with different background (lexicographer vs. language teacher), based on different data (literature vs. conversation) assumptions and goals (creating a lexicon vs. Japanese language teaching). The only similarity is in the methodology. In both cases the dictionary authors collected expressions considered to be emotion-related. The fact that they correlate so strongly suggests that for the compared emotion classes there could be a tendency in language to create more expressions to describe some emotions rather than the others (dislike, joy and fondness are often some of the most frequent emotion classes). This phenomenon needs to be verified more thoroughly in the future.

In evaluation of sentiment and affect analysis systems it is very important to provide a statistically reliable random sample of sentences or documents as a test set (to be further annotated by laypeople). The larger is the source, the more statistically reliable is the test set. Since YACIS contains 354mil. sentences in 13mil. documents, it can be considered sufficiently reliable for the task of test set extraction, as probability of extracting twice the same sentence is close to zero. Ptaszynski et al. (2010) already used YACIS to randomly extract a 1000 sentence sample and used it in their evaluation of emoticon analysis system. The sample was also used in this research and is described in more detail in Section 5.

One of the applications of large corpora is to extract from them smaller sub-corpora for specified tasks. Ptaszynski et al. (2012a) applied YACIS for their task of generating a robust emotion object ontology. They used cross-reference of annotations of emotional information described in this paper and syntactic annotations done by Ptaszynski et al. (2012b) to extract only sentences in which expression of emotion was proceeded by its cause, like in the example below.


                        
                           
                              
                                 
                                 
                                    
                                       
                                          
                                             
                                          
                                       
                                    
                                    
                                       
                                          Kanojo ni furareta 
                                          
                                             kara
                                           
                                          
                                             kanashii
                                          ...
                                    
                                    
                                       Girlfriend DAT dump PAS CAUS sad...
                                    
                                    
                                       I’m sad 
                                          because my girlfriend dumped me...
                                    
                                 
                              
                           
                        
                     

The example can be analyzed in the following way. Emotive expression (
                           kanashii
                        , “sad”) is related with the sentence contents (Kanojo ni furareta, “my girlfriend dumped me”) with a causality morpheme (
                           kara
                        , “because”). In such situation the sentence contents represent the object of emotion. This can be generalized to the following meta-structure,


                        
                           
                              
                                 
                                    O
                                    E
                                 
                                 
                                 CAUS
                                 
                                 
                                    X
                                    E
                                 
                                 ,
                              
                           
                        where O
                        
                           E
                        
                        =[emotion object], CAUS
                        =[causal form], and X
                        
                           E
                        
                        =[expression of 
                        emotion].

The cause phrases were cleaned of irrelevant words like stop words to leave only the object phrases. The evaluation showed they were able to extract nearly 20mil. object phrases, from which 80% was extracted correctly with a reliable significance. Thanks to rich annotations on YACIS corpus the ontology included such features as emotion class (joy, anger, etc.), dimensions (valence/activation), POS or semantic categories (hypernyms, etc.).

Third application of the YACIS corpus annotated with affect- and sentiment-related information has been in a novel research on retrieval of moral consequences of actions, first proposed by Rzepka and Araki (2005) and recently developed by Komuda et al. (2010).
                           11
                        
                        
                           11
                           See also a mention in Scientific American, by Anderson and Anderson (2010).
                         The moral consequence retrieval agent was based on the idea of Wisdom of Crowd. In particular Komuda et al. (2010) used a Web-mining technique to gather consequences of actions applying causality relations, like in the research described in Section 6.2, but with a reversed algorithm and lexicon containing not only emotional but also ethical notions. They cross-referenced emotional and ethical information about a certain phrase (such as “To kill a person.”) to obtain statistical probability for emotional (“feeling sad”, “being in joy”, etc.) and ethical consequences (“being punished”, “being praised”, etc.). Initially, the moral agent was based on the whole Internet contents. However, multiple queries to search engine APIs made by the agent caused constant blocking of IP address an in effect hindered the development of the agent.

The agent was tested on over 100 ethically significant real world problems, such as “killing a man”, “stealing money”, “bribing someone”, “helping people” or “saving environment”. The problems in a form of sentences, or statements were first annotated with probable moral consequences by laypeople. When compared to these annotations, the agent's results were correct in approximately 86% (accuracy). Some examples of the results are presented in Appendix 1 on the end of this paper.

@&#CONCLUSIONS@&#

We performed automatic annotation of a five-billion-word corpus of Japanese blogs with information on affect and sentiment. A survey in emotion blog corpora showed there has been no large scale emotion corpus available for the Japanese language. We chose YACIS, a large-scale blog corpus and annotated it using two systems for affect analysis for word- and sentence-level affect analysis and for analysis of emoticons. The annotated information included affective features like sentence subjectivity (emotive/non-emotive) or emotion classes (joy, sadness, etc.), useful in affect analysis and information on sentence valence/polarity (positive/negative) useful in sentiment analysis obtained as generalizations of those features on a two-dimensional model of affect.

We evaluated the annotations in several ways. Firstly, on a test set of thousand sentences extracted and evaluated by over forty respondents. Secondly, we compared the statistics of annotations to other existing emotion corpora. Finally, we showed several tasks the corpus has already been applied in, such as generation of emotion object ontology or retrieval of emotional and moral consequences of actions.

As for other evaluation methods, we consider crowd-sourcing on larger and more differentiated datasets extracted from the corpus. Crowd-sourcing (such as Amazon Mechanical Turk for English language) allows relatively inexpensive evaluation of large datasets. We have begun to investigate crowd-sourcing services available for Japanese language. We have also proposed previously a method for more objective evaluation of subjective contents Ptaszynski et al. (2008, 2009a), which we also plan to apply to the evaluation of the corpus.

YACIS corpus is meant to be used for pure scientific purposes and is not planned to be available on sale. However, we wish to contribute with it to other research. Therefore we are open to make the corpus available to other researchers after contacting us and specifying applicable legal conditions by obtaining full usage agreement. In the near future we also plan to release an n-gram version of the corpus, which would be available online without any restrictions from the project homepage.

@&#ACKNOWLEDGMENTS@&#

This research was supported by (JSPS) KAKENHI Grant-in-Aid for JSPS Fellows (Project Number: 22-00358).

YACIS project page, including recent news, online demo, etc. can be found at: http://arakilab.media.eng.hokudai.ac.jp/~ptaszynski/repository/yacis.htm.

Examples of emotional and ethical consequence retrieval.


                     
                        
                           
                              
                              
                              
                              
                              
                              
                              
                                 
                                    Success cases
                                 
                                 
                                    Emotional conseq.
                                    Results
                                    Score
                                    Ethical conseq.
                                    Results
                                    Score
                                 
                              
                              
                                 
                                    “To hurt somebody.”
                                 
                                 
                                    
                                         Anger
                                    13.01/54.1
                                    0.24
                                    Penalty/punishment
                                    4.01/7.1
                                    0.565
                                 
                                 
                                    
                                         Fear
                                    12.01/54.1
                                    0.22
                                    
                                    
                                    
                                 
                                 
                                    
                                         Sadness
                                    11.01/54.1
                                    0.2
                                    
                                    
                                    
                                 
                                 
                                    “To kill one's own mother.”
                                 
                                 
                                    
                                         Sadness
                                    9.01/35.1
                                    0.26
                                    Penalty/punishment
                                    5.01/5.1
                                    0.982
                                 
                                 
                                    
                                         Surprise
                                    6.01/35.1
                                    0.17
                                    
                                    
                                    
                                 
                                 
                                    
                                         Anger
                                    5.01/35.1
                                    0.14
                                    
                                    
                                    
                                 
                                 
                                    “To steal an apple.”
                                 
                                 
                                    
                                         Surprise
                                    2.01/6.1
                                    0.33
                                    Reprimand/scold
                                    3.01/3.1
                                    0.971
                                 
                                 
                                    
                                         Anger
                                    2.01/6.1
                                    0.33
                                    
                                    
                                    
                                 
                                 
                                    “To steal money.”
                                 
                                 
                                    
                                         Anger
                                    3.01/9.1
                                    0.33
                                    Penalty/punish.
                                    3.01/6.1
                                    0.493
                                 
                                 
                                    
                                         sadness
                                    2.01/9.1
                                    0.22
                                    Reprimand/sco.
                                    2.01/6.1
                                    0.330
                                 
                                 
                                    “To kill an animal.”
                                 
                                 
                                    
                                         Dislike
                                    7.01/23.1
                                    0.3
                                    Penalty/punishment
                                    36.01/45.1
                                    0.798
                                 
                                 
                                    
                                         Sadness
                                    5.01/23.1
                                    0.22
                                    
                                    
                                    
                                 
                                 
                                    “To drive after drinking.”
                                 
                                 
                                    
                                         Fear
                                    6.01/19.1
                                    0.31
                                    Penalty/punish.
                                    24.01/36.1
                                    0.665
                                 
                                 
                                    “To cause a war.”
                                 
                                 
                                    
                                         Dislike
                                    7.01/15.1
                                    0.46
                                    Illegal
                                    2.01/3.1
                                    0.648
                                 
                                 
                                    
                                         Fear
                                    3.01/15.1
                                    0.2
                                    
                                    
                                    
                                 
                                 
                                    “To stop a war.”
                                 
                                 
                                    
                                         Joy
                                    6.01/13.1
                                    0.46
                                    Forgiven
                                    1.01/1.1
                                    0.918
                                 
                                 
                                    
                                         Surprise
                                    2.01/13.1
                                    0.15
                                    
                                    
                                    
                                 
                                 
                                    “To prostitute oneself.”
                                 
                                 
                                    
                                         Anger
                                    6.01/19.1
                                    0.31
                                    Illegal
                                    12.01/19.1
                                    0.629
                                 
                                 
                                    
                                         Sadness
                                    5.01/19.1
                                    0.26
                                    
                                    
                                    
                                 
                                 
                                    “To have an affair.”
                                 
                                 
                                    
                                         Sadness
                                    10,01/35.1
                                    0.29
                                    Penalty/punish.
                                    8.01/11.1
                                    0.722
                                 
                                 
                                    
                                         Anger
                                    9.01/35.1
                                    0.26
                                    
                                    
                                    
                                 
                                 
                                    
                                       Inconsistency between emotions and ethics
                                    
                                 
                                 
                                    “To kill a president.”
                                 
                                 
                                    
                                         Joy
                                    2.01/4.1
                                    0.49
                                    Penalty/punishment
                                    2.01/2.1
                                    0.957
                                 
                                 
                                    
                                         Likeness
                                    1.01/4.1
                                    0.25
                                    
                                    
                                    
                                 
                                 
                                    “To kill a criminal.”
                                 
                                 
                                    
                                         Joy
                                    8.01/39.1
                                    0.2
                                    Penalty/punishment
                                    556/561
                                    0.991
                                 
                                 
                                    
                                         Excite
                                    8.01/39.1
                                    0.2
                                    
                                    
                                    
                                 
                                 
                                    
                                         Anger
                                    7.01/39.1
                                    0.18
                                    
                                    
                                    
                                 
                                 
                                    
                                       Context dependent
                                    
                                 
                                 
                                    “To act violently.”
                                 
                                 
                                    
                                         Anger
                                    4.01/11.1
                                    0.36
                                    Penalty/punish.
                                    1.01/2.1
                                    0.481
                                 
                                 
                                    
                                         Fear
                                    2.01/11.1
                                    0.18
                                    Agreement
                                    1.01/2.1
                                    0.481
                                 
                                 
                                    
                                       No ethical consequences
                                    
                                 
                                 
                                    “Sky is blue.”
                                 
                                 
                                    
                                         Joy
                                    51.01/110.1
                                    0.46
                                    None
                                    0
                                    0
                                 
                                 
                                    
                                         Sadness
                                    21.01/110.1
                                    0.19
                                    
                                    
                                    
                                 
                              
                           
                        
                     
                  

@&#REFERENCES@&#

