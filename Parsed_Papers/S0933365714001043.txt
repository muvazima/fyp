@&#MAIN-TITLE@&#NICeSim: An open-source simulator based on machine learning techniques to support medical research on prenatal and perinatal care decision making

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           NICeSim is flexible: It allows inclusion/deletion of any variable of interest.


                        
                        
                           
                           NICeSim is dynamic: It creates a just-in-time ML model from the loaded dataset.


                        
                        
                           
                           NICeSim provides a graphical user interface for simulation after model building.


                        
                        
                           
                           NICeSim provides qualitative and quantitative analyses.


                        
                        
                           
                           NICeSim is open-source allowing a cooperative development.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Machine learning in medicine

Artificial neural networks

Support vector machine

Clinical decision making

Prenatal care

Perinatal care

@&#ABSTRACT@&#


               
               
                  Objective
                  This paper describes NICeSim, an open-source simulator that uses machine learning (ML) techniques to aid health professionals to better understand the treatment and prognosis of premature newborns.
               
               
                  Methods
                  The application was developed and tested using data collected in a Brazilian hospital. The available data were used to feed an ML pipeline that was designed to create a simulator capable of predicting the outcome (death probability) for newborns admitted to neonatal intensive care units. However, unlike previous scoring systems, our computational tool is not intended to be used at the patients bedside, although it is possible. Our primary goal is to deliver a computational system to aid medical research in understanding the correlation of key variables with the studied outcome so that new standards can be established for future clinical decisions. In the implemented simulation environment, the values of key attributes can be changed using a user-friendly interface, where the impact of each change on the outcome is immediately reported, allowing a quantitative analysis, in addition to a qualitative investigation, and delivering a totally interactive computational tool that facilitates hypothesis construction and testing.
               
               
                  Results
                  Our statistical experiments showed that the resulting model for death prediction could achieve an accuracy of 86.7% and an area under the receiver operating characteristic curve of 0.84 for the positive class. Using this model, three physicians and a neonatal nutritionist performed simulations with key variables correlated with chance of death. The results indicated important tendencies for the effect of each variable and the combination of variables on prognosis. We could also observe values of gestational age and birth weight for which a low Apgar score and the occurrence of respiratory distress syndrome (RDS) could be less or more severe. For instance, we have noticed that for a newborn with 2000 g or more the occurrence of RDS is far less problematic than for neonates weighing less.
               
               
                  Conclusions
                  The significant accuracy demonstrated by our predictive model shows that NICeSim might be used for hypothesis testing to minimize in vivo experiments. We observed that the model delivers predictions that are in very good agreement with the literature, demonstrating that NICeSim might be an important tool for supporting decision making in medical practice. Other very important characteristics of NICeSim are its flexibility and dynamism. NICeSim is flexible because it allows the inclusion and deletion of variables according to the requirements of a particular study. It is also dynamic because it trains a just-in-time model. Therefore, the system is improved as data from new patients become available. Finally, NICeSim can be extended in a cooperative manner because it is an open-source system.
               
            

@&#INTRODUCTION@&#

Machine learning (ML) algorithms have been the basis for data mining frameworks for detecting patterns in large datasets. These frameworks comprise computational solutions for many complex scenarios such as decision making in business and weather forecasting [1,2]. In the medical field, ML approaches had a great impact, being applied to a myriad of complex problems, such as providing a more precise diagnosis and prognosis for patients with severe diseases, simulating drug reactions, and predicting the probability of transplant rejection [3–5].

Particularly in the clinical field, early attempts of health professionals to predict medical treatment outcomes were made exclusively through subjective procedures conceived according to the clinicians’ knowledge. However, the necessity for a more precise assessment of the outcome of clinical practices, especially in intensive care units (ICUs), led to the development of computational/mathematical methods, resulting in methods termed scoring systems [6]. As a consequence, health professionals can apply such computational tools to combine their own knowledge with mathematical measurements to make more accurate decisions.

One known example of such a scoring system is APACHE II. It provides a score to assess disease severity based on 12 physiological measurements that are taken during the first 24 h that the patient is in the ICU, in addition to age and previous health status [7]. The score is an integer varying from 0 to 71, where greater values correspond to a higher risk of death. Additionally, the system provides probabilities by means of logistic regression [8]. However, according to Frize and Walker [9], APACHE II is recommended more to predict outcomes of a whole medical unit rather than an individual patient.

Some years later, another well-known scoring system termed SAPS II was proposed by Le gall et al. [10]. Initially, SAPS II used 12 physiological variables, age, type of admission, and three disease-related variables. Its score values ranged from 0 to 163, and probabilities were also calculated with logistic regression. Later on, to improve the model's generality, the authors proposed an extension including other features: Sex, length of stay, patient location before ICU, clinical category, and the occurrence of drug overdose [11].

MODS was proposed to measure the severity of multiple organ dysfunction as an outcome of an illness at the critical stage [12]. The score is based on six organ failures, varying from 0 to 4 for each type of failure, i.e., the maximum overall score value is 24.

Vincent et al. created the SOFA system [13]. Similar to MODS, the SOFA score is based on a scale ranging from 0 to 4 to measure the degree of dysfunction for each of six organs to determine the gravity of a patient's illness. The variation in the patient's condition during the ICU stay is also taken into consideration.

Although the above-mentioned works have been demonstrated to be very important in the evolution of methods to predict outcomes in critical illness, they were not designed for newborn patients. Inspecting the literature, we could find only two efforts directed to neonates admitted to ICUs.

Clinical risk index for babies (CRIB) is a scoring system that takes into account birth weight, gestational age, the maximum and minimum fraction of inspired oxygen and the maximum base excess during the first 12 h, and the presence of congenital malformations [14]. CRIB has demonstrated a much higher predictive power using these attributes versus using birth weight alone, although the system has been validated only for neonates of <1500 g birth weight.

Frize and Walker proposed a case-based reasoning approach for the estimation of medical outcomes and resource utilization [9]. The system is designed to assess patient status, to assist in making a diagnosis, and to facilitate the selection of a course of therapy. Their program targeted adults initially, but it was later redesigned for neonatal ICUs.

It is important to note that all methods cited above are intended to be used at the patient's bedside. Our proposal here is different. Although our computational system can be easily adapted to bedside use, our main goal is to provide an approach to support medical research. In particular, we propose an open-source simulator called NICeSim that is grounded on ML techniques to facilitate studies for understanding how critical features of premature newborns affect their risk of death, to influence future medical decision-making concerning prenatal and perinatal care. Therefore, our primary objective is not delivering a real-time system or even a computational tool to assist a health professional when the illness is already in progress, but instead, to provide support for studies that clarify how new medical conduct would lead to the minimization of critical cases that result in newborn admission to ICUs.

In this vein, it is crucial to design a flexible system that allows the deletion and the inclusion of attributes, so that a health professional can choose the most suitable variables to study. According to Afessa et al. [15], prediction models should periodically be updated to reflect the changes in medical practice. However, the previously proposed scoring systems have rigid variables, i.e., once the features putatively related to the outcome are chosen and implemented in the system, the user cannot change them, and is limited to work that way. In our case, the data needed in our computational system are in a specific file format called ARFF [16], which can be easily generated from the comma-separated values (CSV) format. The CSV format is available in all well-known spreadsheet applications such as OpenOffice and Excel. Therefore, the data could be edited in a spreadsheet, including all variables of interest (even some of the variables already proposed in previous methods), to be used later in our computational tool. Our programs recognize all attributes, build a just-in-time ML model, and automatically create user interface controls (sliders for numeric values, and combo boxes for nominal values) that allow a very easy manipulation of the attribute values, showing the immediate effects of the alterations in the outcome variable. These characteristics deliver a user-friendly environment that permits the efficient study of the relationship between the variables and the outcome. To our knowledge, no graphical user interface for current computational tools for ML algorithms includes the possibility of interactive simulation of attribute values to study their effects on the outcome. Hence, our system allows not only qualitative analyses, but also studies of quantitative aspects of the problem, as demonstrated in our experiments. This type of simulation might be of great utility for governments, hospitals, and related institutions that need to better understand the circumstances that lead to death in neonatal ICUs. For instance, after using the simulator to understand important correlations with newborn deaths, these results might help the government personnel in the decision to launch a large-scale national campaign to decrease the number of neonate deaths. Although other ML algorithms can be easily incorporated into NICeSim, the user can currently choose one of two algorithms to build a learning model: Support vector machine (SVM) or artificial neural network (ANN). Note that the health professional user does not need to have any ML knowledge to use the system. When choosing an algorithm, the learning process is automatic. The user can continue with the algorithm resulting in the highest accuracy, i.e., the highest number of correctly classified instances. This is shown on the screen after training. Alternatively, the user can choose a default algorithm, with which the training process is automatically started after loading the data, and begin simulation with the resulting model without worrying about how it actually works.

In this work, we applied the proposed method to a dataset constructed using information on premature newborns admitted to an ICU in Brazil to understand how the attributes: Apgar scale, respiratory distress syndrome, gestational age, and birth weight affect the newborns’ risk of death. For our dataset, ANN showed a better result and is the default learning algorithm for NICeSim. Our statistical experiments using 10-fold cross validation showed an accuracy of 86.7% and an area under the receiver operating characteristic (ROC) curve of 0.84 for the positive class. Using the resulting model, the physicians that collaborated in this work, who have significant experience in intensive care, as well as a neonatal nutritionist, simulated different scenarios for the above-mentioned attributes, verifying that the predicted outcomes in these experiments showed a very good agreement with expected results described in the literature. We have demonstrated that once the simulator is fed with appropriate data, it can be a powerful tool to comprehend quantitative and qualitative characteristics of the attributes and the outcome variable. We believe that the fact that NICeSim is open source is also of great importance, because other groups might suggest improvements and extensions to this research tool, i.e., NICeSim might be improved in a cooperative manner all over the world, similarly to other open-source systems.

@&#MATERIAL AND METHODS@&#

Even though this work focuses on the specific case of newborns in the ICU, it is important to emphasize the general aspects presented here, i.e., as long as appropriate data are available, the machine learning steps presented in this work could be applied to any domain of interest, serving as a powerful tool for hypothesis testing and, consequently, decision making. Everything for which details are presented in the following sections can be summarized by the following machine learning framework:
                           
                              1.
                              Conversion of raw data into an appropriate format: Here we chose the ARFF format;

Data preprocessing: Dimensionality reduction by attribute selection. The participation of specialists is important because a mix of knowledge and mathematical methods can be used to extract the most important variables. If convenient, sampling, discretization, and binarization may also be useful;

Experiments with many ML algorithms: After the two steps above, the final TS is ready, allowing the ML experiments to begin. The statistical performance of the obtained models must be analyzed to select the ML approaches that perform better for the available data. The use of techniques to counteract the dataset imbalance issue might be important. Here, we suggest the use of cost matrices;

Coding of a graphical user interface (GUI) that facilitates hypothesis testing: Program a GUI that embeds the learning algorithms chosen in the previous step and provides interface controls after model construction to ease attribute value manipulations in an interactive manner, i.e., for each attribute value change, the effect on the outcome is immediately displayed to facilitate a quick simulation of several scenarios of interest. Here, an attribute ranking method, such as information gain, might be of great interest to provide some optimized directions to the user.

The three first steps above are traditional stages for many ML approaches in several fields where such algorithms have been shown to be useful. Our main contribution is step 4. Although it seems obvious, we did not find any previous work describing such a task. Graphical user interfaces in ML software are, generally, concerned with providing a suitable tool for performing the three first steps. However, there is no environment to execute simulations, as we describe here. The best the user can do is to construct a test set and run the obtained model in batch mode – not interactively as with our system – thus limiting the usability to simulate different scenarios and excluding users that are not familiar with machine learning techniques.

We built a training set (TS) for the learning algorithms from a database of data collected in the Neonatal Intensive Care Unit of São Sebastião Hospital, at Viçosa, Minas Gerais State, Brazil, during the years 2008–2010 [17]. In 2009, this hospital became a health service reference unit for high risk pregnancies. Its neonatal ICU was inaugurated in March of 2004 and totaled 1059 calls in December of 2010, out of which 70% were cases of premature newborns [18].

The first concern during the design and implementation of NICeSim was to preprocess the available data to achieve our goals. The initial dataset had 114 attributes and 293 instances. This dataset was not balanced: 39 instances were positive cases (regarding newborn deaths in the neonatal ICU), while the remaining consisted of negative instances. The problem with imbalanced data is that it often causes machine learning algorithms to perform poorly on the minority class, i.e., the rare instances might be treated as noise [19]. It turns out that in binary classification the minority class is normally the class of interest, as in our case. To counteract the imbalance problem, we use a cost sensitive algorithm [19], where a cost matrix is applied to impose that the penalty for a false negative is higher than the penalty for a false positive, so that the model is built favoring more the positive class.

The next step was to find relevant and non-redundant attributes among the 114 initial attributes. For this task, we submitted the dataset to an attribute selection process. Attribute selection, also known as feature selection, is a common step in the knowledge discovery process [20,21]. Its purpose is to diminish the complexity of a dataset by reducing its dimensionality, without losing important information. It can be accomplished by computational/mathematical methods, by human input, or by both. In our case, this task was divided into three stages – the first and last stages were performed by our partner physicians. Each stage is described below.

Many attributes in the initial set were identified as out of the scope of our simulations (e.g.: ID, date of birth, date of discharge, etc.). Hence, based on a pre-analysis by our partner physicians as well as a deep study of previous work that informed our dataset [17], we reduced the initial set of attributes to only the 15 presented in Table 1
                        , which are considered very important with regard to the outcome variable (probability of death) according to the current medical knowledge. Therefore, the first stage of attribute selection was accomplished entirely via human input.

After the initial selection, the dimensionality of our dataset was greatly reduced. Next, an additional attribute selection stage was performed, but this time using ML algorithms. Notice that this drastic reduction allowed us to run our learning algorithms for all possible subsets of the remaining attributes. By doing so, we could assign a score to each subset based on the sensitivity and specificity provided by the model built from the given attribute subset. Attribute selection methods that train a new model for each candidate subset and analyze the resulting model performance are also known as wrapper methods [21]. We used a 10-fold cross validation for each subset evaluation to minimize the possibility of overfitting. The subset that achieved the best score value was composed of the following attributes: apgarLess7, occurrence of intrauterine growth restriction (IUGR), gestational age (GA), occurrence of respiratory distress syndrome (RDS) – also known as hyaline membrane disease, occurrence of patent ductus arteriosus (PDA), ageParentalNutrition, and agePlainDiet. Therefore, the other 8 attributes showed no evidence, for the given dataset, that they were important for predicting the outcome variable, i.e., the probability of death.

The seven selected features went through an additional analysis, this time also by human intervention. At this stage, we mainly analyzed how each attribute would relate to our goals. Considering that the available database was originally built for a different purpose [17,18], our partner physicians concluded that three variables, namely, PDA, ageParentalNutrition, and agePlainDiet, would not make sense for the objectives of this work because of the way they were collected. As a consequence, the final dataset is composed by the four remaining attributes: apgarLess7, IUGR, GA, RDS. Thus, our learning models are focused on detecting how these attributes correlate with the probability of death in premature newborns. An additional experiment was also performed replacing GA by birth weight, which is described later.

This study involves humans as research participants (data collected in neonatal ICU). The project was submitted for analysis and approval by UFV Ethical Committee for Research with Human Subjects (CEP), in accordance with Resolution 196/1996 and Resolution 466/2012 of the Brazilian National Health Council (Conselho Nacional de Saúde do Brasil).

NICeSim provides the user with two optional learning algorithms. We decided to do this because ML algorithms can show different performance for different training sets with distinct characteristics. Both learning algorithms used here are part of the Weka library, a well-known and extensible tool widely adopted for machine learning tasks [16].

Artificial neural networks are an approach inspired by biological neural systems aiming at creating a powerful learning technique [22,23,16,24]. Similar to the human brain, ANNs are composed of a set of nodes (neurons) connected by directed links (synaptic connections). The so-called perceptron was the first proposed model [25]. In this simplified architecture, just two types of nodes are present: Input nodes, representing attributes, and a single output node, representing the ANN output. Every input node is connected to the output node by a weighted link. The weights indicate the strength of the synaptic connections between neurons. In a perceptron, the output node produces a linear combination of the inputs, leading to a decision boundary represented by a hyperplane that is given by:


                           
                              
                                 (1)
                                 
                                    
                                       
                                          w
                                       
                                    
                                    ·
                                    
                                       
                                          x
                                       
                                    
                                    +
                                    b
                                    =
                                    0
                                    ,
                                 
                              
                           where w is the vector for real-valued weights, x is the vector for input values, and b is the bias, which is a constant value that does not depend on any input. The bias term provides more flexibility to the hyperplane. Without this term, any hyperplane would be constrained to pass through the origin of the space defined by the attributes (inputs). Next, the result of the sum (the left side of Equation 1) is applied to a function termed activation function to generate the predicted output. The perceptron uses the signum function [24], i.e., if the sum value is positive, then it outputs +1, if it is negative, then the output is −1, and if it is zero, the result is also zero and can be interpreted according to convenience. This is frequently considered as +1. Thus, the perceptron training process is the adaptation of the weights until a satisfactory relationship between input and output regarding observations in the training dataset is obtained. Initially, random weights are produced. Next, they are updated by the following equation:


                           
                              
                                 (2)
                                 
                                    
                                       w
                                       j
                                       
                                          k
                                          +
                                          1
                                       
                                    
                                    =
                                    
                                       w
                                       j
                                       k
                                    
                                    +
                                    λ
                                    (
                                    
                                       y
                                       i
                                    
                                    −
                                    
                                       
                                          
                                             y
                                             ˆ
                                          
                                       
                                       i
                                       
                                          (
                                          k
                                          )
                                       
                                    
                                    )
                                    
                                       x
                                       ij
                                    
                                    ,
                                 
                              
                           where variable x
                           
                              ij
                            keeps the value of the jth attribute of the training instance x
                           
                              i
                           , y
                           
                              i
                            represents the instance class, 
                              
                                 
                                    
                                       y
                                       ˆ
                                    
                                 
                                 i
                              
                            is the predicted class, and λ is known as the learning rate. The value λ is used to control the weight adjustment in each iteration k.

The perceptron concepts have quickly evolved to a more complete approach called multilayer artificial neural network. In this case, the network might contain several intermediary layers defined as hidden layers. Furthermore, more sophisticated activation functions are used, such as sigmoid (logistic) and hyperbolic tangent functions. These improvements, including an output layer with one or more nodes, provide more complex and useful decision boundaries. Additionally, the learning process generally applies a method called backpropagation, where the deviation between observed and expected outputs is used in an update formula for the weights in the reverse direction, i.e., the weights at level d
                           +1 are updated before the weights at level d 
                           [24].

In NICeSim, we use the multilayer with backpropagation approach. In our ANN architecture, the input layer nodes correspond to the four features previously mentioned, and there is one hidden layer with three nodes (⌈5/2⌉, where 5 is the number of attributes including the class). We use the sigmoid as the activation function (so probabilities can be generated), epochs =500, learning rate =0.3, and momentum =0.2. Sigmoid functions are very frequently used to map real values to the range [0, 1] that can be interpreted as probabilities [24].

The second option of ML algorithm available to the user is the support vector machine method that constitutes a very powerful classification approach [26,27,24].

Assuming a binary classification and that the data are linearly separable, a linear SVM approach aims at finding a hyperplane (a straight line in dimension 2) to distinguish elements of different classes as such, i.e., elements having different labels lie in opposite sides of the hyperplane. However, there is an infinity of possible hyperplanes that perfectly separate the data. In SVM, the maximum margin hyperplane is preferred because the fundamental principle is delineated from the fact that decision boundaries with large margins tend to minimize overfitting, whereas those with small margins, even presenting low training errors, generalize poorly with unknown data [24]. Notice that the margin of a hyperplane H
                           1 is the distance between two other associated hyperplanes H
                           11 and H
                           12. In short, a hyperplane H
                           11 is obtained by moving a parallel copy of H
                           1 toward the elements of one of the classes until reaching the closest instances that are the support vectors of that class. Hyperplane H
                           12, in turn, is generated by moving a parallel copy of H
                           1 to the opposite direction until touching the closest instances of the contrary class, which are the support vectors of this class.

The SVM method is formulated as a quadratic optimization problem for which well-established algorithms can be employed to find global optima for the objective function. The N instances in a TS with d attributes can be represented by tuples of the form (x
                           
                              i
                           , y
                           
                              i
                           ), where x
                           
                              i
                           
                           =(x
                           
                              i1, x
                           
                              i2, …, x
                           
                              id
                           )
                              T
                            is a vector containing the attribute values of the ith instance, and y
                           
                              i
                            indicates the class to which x
                           
                              i
                            belongs. For convenience, let y
                           
                              i
                           
                           ∈{−1, 1}. Any hyperplane can be described by the set of points x satisfying Eq. (1), where w is a normal vector to the hyperplane and b/∥w∥ denotes the hyperplane offset from the origin along w. Therefore, considering that we found w and b that describe hyperplane H
                           1 and using −1 and +1 as class labels, the class of a given instance x
                           
                              i
                            can be predicted as follows:


                           
                              
                                 (3)
                                 
                                    
                                       y
                                       =
                                    
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      −
                                                      1
                                                      ,
                                                   
                                                   
                                                      if
                                                      
                                                      
                                                         
                                                            w
                                                         
                                                      
                                                      ·
                                                      
                                                         
                                                            
                                                               x
                                                            
                                                         
                                                         i
                                                      
                                                      +
                                                      b
                                                      <
                                                      0
                                                      ;
                                                   
                                                
                                                
                                                   
                                                      +
                                                      1
                                                      ,
                                                   
                                                   
                                                      if
                                                      
                                                      
                                                         
                                                            w
                                                         
                                                      
                                                      ·
                                                      
                                                         
                                                            
                                                               x
                                                            
                                                         
                                                         i
                                                      
                                                      +
                                                      b
                                                      >
                                                      0
                                                      .
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

Furthermore, it is possible to rescale w and b such that H
                           11 and H
                           12 can be represented as follows:


                           
                              
                                 (4)
                                 
                                    
                                       
                                          
                                             
                                                H
                                                11
                                             
                                             :
                                          
                                          
                                             
                                                
                                                   w
                                                
                                             
                                             ·
                                             
                                                
                                                   x
                                                
                                             
                                             +
                                             b
                                             =
                                             −
                                             1
                                             ,
                                          
                                       
                                       
                                          
                                             
                                                H
                                                12
                                             
                                             :
                                          
                                          
                                             
                                                
                                                   w
                                                
                                             
                                             ·
                                             
                                                
                                                   x
                                                
                                             
                                             +
                                             b
                                             =
                                             1
                                             .
                                          
                                       
                                    
                                 
                              
                           
                        

By using geometry, the distance between H
                           11 and H
                           12, i.e., the margin of the decision boundary, turns out to be 2/∥w∥. Consequently, we want to find values for w and b that minimize f(w)=∥w∥. For convenience in the derivative calculations, we can replace ∥w∥ with 1/2∥w∥2, as the solution does not change (the same values for w and b are found in both cases), changing this task to a quadratic optimization problem. As a result, the learning procedure in linear SVM can be seen as the following constrained optimization problem:


                           
                              
                                 (5)
                                 
                                    
                                       
                                          
                                          
                                             
                                                min
                                                
                                                   
                                                      1
                                                      2
                                                   
                                                
                                                
                                                   
                                                      
                                                         ∥
                                                         
                                                            w
                                                         
                                                         ∥
                                                      
                                                      2
                                                   
                                                
                                             
                                          
                                          
                                       
                                       
                                          
                                             subject to
                                             
                                          
                                          
                                             
                                                y
                                                i
                                             
                                             (
                                             
                                                
                                                   w
                                                
                                             
                                             ·
                                             
                                                
                                                   
                                                      x
                                                   
                                                
                                                i
                                             
                                             +
                                             b
                                             )
                                             ≥
                                             1
                                             ,
                                          
                                          
                                             i
                                             =
                                             1
                                             ,
                                             2
                                             ,
                                             …
                                             ,
                                             N
                                             .
                                          
                                       
                                    
                                 
                              
                           
                        

This convex optimization problem with inequality constraints can be solved by the Lagrange multiplier method along with the Karush–Kuhn–Tucker (KKT) conditions [26,28]. These techniques along with the use of kernel functions for nonlinearly-separable data, and some other convenient transformations, lead to the more familiar formulation of SVM contained in the principal literature regarding this subject [26,27,24]. In short, a kernel function represents a mapping of nonlinear data to a new higher-dimensional space by a transformation Φ(x), so that the instances in the obtained space can then be separated by a linear decision boundary, i.e., the techniques presented above can be applied.

The execution of an SVM procedure in this work uses the complexity parameter C =1 and buildLogisticModels =
                           true to produce probabilities. The inclusion of kernel functions did not present any improvement to the model's predictive power, which indicates that our data are linearly separable. Therefore, we proceeded with the linear SVM because the training time is shorter.

The simulator is composed of two screens. The initial one is for the user to provide all necessary information before starting the simulation: The training set, percentage of instances for training, the entries for the cost matrix, and the ML algorithm.

The TS is a text file in attribute-relation file format (ARFF) [16]. It is important to clarify that it is not necessary for the user to be a specialist in ML algorithms to build the model. Once the TS is loaded, the model is automatically built according to the given cost matrix and the chosen ML algorithm filled in by the system. When the model is ready, some statistical parameters related to a 10-fold cross validation are presented in the initial screen, summing up the learning performance. Although many parameters are given, the user might use “Correctly Classified Instances” to obtain a good picture of the model's predictive power. Additionally, this is the only screen needed before the simulation per se, which indicates that very little user training is required to create a model that can be used for simulation.

The entries of the cost matrix are automatically filled in with values that compensate for the imbalance problem. The default values are as follows. In entries [1, 1] and [2, 2] that represent the costs of getting a true positive and a true negative, respectively, the values are zero, because we do not want to penalize correct classifications. In entry [1, 2] that represents the cost of getting a false negative, the value is set to the percentage of negative instances. Finally, the entry [2, 1] used for penalizing a false positive is filled with the percentage of positive values. In our dataset, there are 39 positive (death) instances and 254 negative instances. As a consequence, the penalization for a false negative is more than six times higher than for a false positive to overcome the imbalance issue. However, the cost matrix values can be changed if it is convenient.

Once the model is built, the simulations can be run. This procedure is executed on the second screen of NICeSim. When getting to this screen, a random instance of the TS is selected and has its attribute values and the outcome displayed. The outcome, or class, is shown as a binary value (true or false – death or not) as well as a percentage representing the chance of death. It is also possible to randomly obtain a new positive or negative instance from the TS by pressing the appropriate buttons.

A very important feature to note is that the user interface controls (sliders for numeric values, and combo boxes for nominal values) are automatically built according to the specification of each attribute in the ARFF file. These controls allow easy manipulation of the attribute values. As a result, a health professional specialist might formulate many hypotheses corresponding to several distinct scenarios and check their likelihood by performing the simulations of interest, i.e., by changing the attribute (GA, apgarLess7, etc.) values using the controls and analyzing how these changes affect the outcome, so that the relationship variables-outcome is more well understood and, as a consequence, the number of hypotheses to test in vivo is narrowed to a feasible number. For each attribute value alteration, the class is automatically predicted by the model and displayed on the screen, allowing a quick analysis of the impact that each change might cause.

One important point to notice is that the attribute merit rank is also provided on the simulation screen. Each attribute has a quality, represented by a score, assigned by the information gain (IG) method [24]. The information gain of an attribute expresses how much information this attribute yields for the class of the instances. The IG of an attribute a regarding to a class variable c is given by:


                        
                           
                              (6)
                              
                                 IG
                                 (
                                 c
                                 ,
                                 a
                                 )
                                 =
                                 ent
                                 (
                                 c
                                 )
                                 −
                                 ent
                                 (
                                 c
                                 |
                                 a
                                 )
                                 ,
                              
                           
                        where ent(c) is the entropy of the variable class c, and ent(c|a) is the conditional entropy. The entropy of a class variable c is defined as:


                        
                           
                              (7)
                              
                                 ent
                                 (
                                 c
                                 )
                                 =
                                 −
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    k
                                 
                                 p
                                 (
                                 i
                                 )
                                 
                                    log
                                    2
                                 
                                 p
                                 (
                                 i
                                 )
                                 ,
                              
                           
                        where k is the number of class labels and p
                        
                           i
                         is the proportion of instances belonging to class i.

IG scores highlight the attributes that might yield the most significant effects in the class. Therefore, a specialist can combine knowledge with IG scores to perform simulations in an optimized manner. Once again, we emphasize that the user has no need to understand the mathematical basis stated above. It is enough to know that a score is assigned to each attribute, providing a ranking where the variable with the most impact is on top. This might be helpful to the user in hypothesis formulation, but might be completely ignored if the user feels that this ranking is not important for his/her purposes.

@&#RESULTS@&#

To evaluate the SVM and the ANN models on the ICU dataset, we used two different statistical approaches largely adopted in the absence of testing data for measuring how general the models are: Data split and cross-validation [24].

In the data split procedure, the TS is divided into two parts, one for training and another for testing. The most popular proportion is 66% for training and the remainder for testing.

In cross validation, the dataset is split into k disjoint parts of roughly the same size. One part is taken as test data, while the remaining k
                        −1 parts are used to build the model. This process is repeated k times, each time using a different partition for testing. At the end, the validation results are averaged over the k executions. The most popular value for k is 10 that is the value we used in our experiments. This value is widely used because ten different experiments using ten different partitions is considered statistically significant to obtain a reliable measurement of the model's performance, i.e., there is a very low risk of model overfitting, while not overloading the evaluation process with a high number of partitions.


                        Table 2
                         summarizes the performance of each learning algorithm on ICU dataset in terms of sensitivity, specificity, and accuracy, measured by both validation methods described above. It is noteworthy that ANN achieved better values of accuracy as well as specificity, whereas SVM presented better sensitivities. It demonstrates that it is important to provide both options in the simulator so that the user can choose the most suitable one.

We also analyzed the area under the ROC curve. This curve is a plot of the true positive rate versus the false positive rate for several probability threshold values. The greater the area under this curve, the greater the power of prediction of the ML model. Table 2 shows the area under the ROC curve for all the tests. Fig. 1
                         depicts the ROC curve of the ANN model, concerning the positive class, for a 10-fold cross-validation. As can be noticed in the figure and in Table 2, the predictive power demonstrated by the learning procedures is very good. As a consequence, we can assign a high confidence to the simulation process. This is demonstrated in the next section.

This section describes some of the practical experiments we performed with NICeSim to evaluate the importance of the key attributes and the coherence of the model to determine the simulator validity. Three physicians and a neonatal nutritionist executed simulations independently, and the results from each individual's work were compared with each other. In each experiment, the ANN approach was used to build an ML model on the neonatal ICU dataset. With the resulting model, simulations targeting the risk of death of newborns could be performed with chosen attributes.

To make the results comparable, a protocol was established in which the attributes Apgar (less than 7), RDS, birth weight (BW), and gestational age were chosen for the simulations due to their clear importance for characterizing a neonate's health condition and to support clinical decisions. Some values of these attributes were defined a priori, and all possible combinations of these values were taken into account to perform simulations. For apgarLess7 and RDS, we considered values 0 and 1, indicating occurrence or not. For GA, we used values 24, 28, and 32 in one experiment (Table 3
                        ) and integer values from 24 to 32 in another experiment (Fig. 2
                        ). Finally, for BW, we have taken into account a value of 520 – that is the minimum in our DB – and values from 600 to 3500, with a step of 100 (i.e., 600, 700, 800,…).

Even though the attribute IUGR has been demonstrated (through information gain) to be important to the predictive power of the resulting model, it was not considered in the simulations, i.e., it was fixed with a value of 0 (no occurrence). Notice that IUGR might occur in both premature as well as non-premature neonates and might have several etiologies. As a consequence, the use of IUGR might lead to a very difficult interpretation of the results.

After each individual's work, the health professionals discussed their results, and realized that the tendencies and the values obtained from their experiments were quite similar and agreed with current medical knowledge. Table 3, Figs. 2 and 3
                        , as well as Section 4 sum up the outcome of their experiments.

The Apgar scale represents the clinical evaluation of newborns measured in the first 5min of life. This scale measures five main characteristics: Appearance (complexion), pulse rate, reflex irritability, activity, and respiratory effort. Each one of these measurements can have a score ranging from 0 to 2, resulting in a final score ranging from 0 to 10. Scores above 7 are generally regarded as normal, while scores of 7 or less mean that the newborn possibly needs medical attention. At the moment of birth, the Apgar score is useful to evaluate the newborn's health condition and to guide the necessary actions to improve the neonate's survival chances, e.g., whether there is a necessity for mechanical ventilation.

Respiratory distress syndrome is related to the pulmonary maturity of newborns. This is associated with insufficient surfactant production. The main characteristic of RDS is an increased effort necessary for the newborn to breathe.

Gestational age is usually defined by an estimate that combines the obstetric ultrasound, the first day of the woman's last normal menstrual period, and the New Ballard score [29]. Here, we separated the newborns as follows [30]:
                           
                              •
                              Extreme preterm: GA less than 28 weeks,

High preterm: GA ranging from 28 to 31 weeks, and

Moderate preterm: GA ranging from 32 to 36 weeks.

The predictions of death chance calculated by NICeSim based on the above-mentioned attributes are presented in Table 3 for three different critical values of GA. We can see that the outcomes provided by NICeSim are consistent with the clinical information available in the literature. For instance, the scenario where the Apgar score is low, RDS is positive, and the gestational age is low demonstrates that all these factors clearly correlate with poor prognostics for newborns, which has been previously shown by many authors [31,32,18]. Each of the attributes Apgar score and RDS, individually, shows a strong correlation with the outcome as well. Either the Apgar score below 7 or the occurrence of RDS leads to a higher chance of death, mainly in extreme preterms and high preterms. Furthermore, when the Apgar score is 7 or more, and RDS is negative, the chances of death are very low, even for extreme preterms.

In Fig. 2, a plot of death risk versus predicted GA illustrates how gestational age, combined with Apgar and RDS, affects the probability of death. The plot corroborates the information in Table 3. It is clear that the higher the GA value the lower the death risk. It also can be seen how each condition affects the outcome. The occurrence either of RDS alone or an Apgar ≤ 7 alone implies much more severe conditions than the case of no RDS and high Apgar score when we consider GA < 31. It is even more problematic when both complications (RDS and Apgar ≤ 7) are present.

We also analyzed how birth weight along with RDS and Apgar score affect death risk. Notice that BW is highly correlated with GA, as expected. For this reason, the former was not included in the predictive model described so far. In fact, the ANN classifier accuracy including GA and BW is 83.62%. When the latter is removed, the accuracy reaches 86.69%, as reported in Table 2. On the other hand, repeating the analysis made above, but replacing GA by BW might be interesting because many methods in the past used this attribute as the main variable to assess the risk of mortality [14]. Additionally, it is a demonstration of the flexibility of our system to include/exclude new attributes, according to the researcher's purposes. Fig. 3 shows a plot of BW versus death risk. We can see some similar patterns when it is compared to Fig. 2. As BW increases the death chance decreases because all curves tend toward lower values of death risk, and stabilize for some value of BW. Still, the influence of RDS and Apgar for the BW analysis is similar to their influence observed for GA in the sense that each condition individually implies a higher probability of death. Furthermore, the occurrence of both conditions (positive RDS and low Apgar values) result in a much worse prognosis.

@&#DISCUSSION@&#

We emphasize here the possibility that our system can provide a quantitative analysis in addition to a qualitative study. As seen in the analyses above, we could measure the probability of death for several precise values of GA and BW. Considering that the predictive power of our model has proven to be good (Table 2), the output probabilities can be taken as good estimators of the real patient risk. Such analyses can be very useful, especially for numerical variables, as for GA and BW, for which the distinct levels of these variables might influence the result in completely different ways. Therefore, our approach can provide not only tendencies (qualitative analysis), but also experiments with precise values (quantitative analysis) that might guide new clinical practices in the future. For instance, one might conclude, based on Fig. 3, that a low Apgar is less critical when BW is greater than or equal to 1200g. The same is true for RDS, i.e., for BW values of 2100g and higher, the occurrence of RDS is not as problematic compared to cases of lower weights. Attention to these facts may guide future decisions in prenatal (e.g., administration of corticosteroids toward the end of the pregnancy) and neonatal (e.g., administration of surfactant) care in the clinical and public health scope, allowing still a clear comprehension of the effectiveness of treatment to optimize the use of hospital resources, aiding in the development of treatment standards.

Another important aspect involves ethical issues. In developing countries it is a common circumstance that a hospital has a very low budget. Therefore, in a possible scenario where two premature neonates with RDS need surfactant therapy, but the hospital has available resources for only one, a previous analysis such as the one we show in Figs. 2 and 3 might support this difficult ethical decision, i.e., the baby for which the preliminary study points to a greater death risk would be given higher priority. The same reasoning can also be applied concerning the need of mechanical ventilation for babies with RDS or a low Apgar score, i.e., standards established with studies such as the one we present here would assist in situations where an insufficient number of mechanical ventilators is available.

@&#CONCLUSIONS@&#

In this work, we propose a powerful ML framework to support medical research on prenatal and perinatal care. We validated our approach by constructing a computational tool based on the proposed method and applying it to a dataset built from a database of neonatal ICU patients in a hospital in Brazil. Although our NICeSim simulator could be easily adapted to be used as a scoring system at the patient's bedside, our primary goal was to demonstrate the great potential of our system for medical research given its great flexibility and dynamism.

NICeSim is flexible because it accepts alterations of the chosen variables. Hence, each research group interested in testing different types of attributes and outcomes could build a proper dataset according to the targeted purposes. This is possible because NICeSim automatically detects the ARFF specification for each attribute and creates interface controls accordingly. Therefore, once an ARFF file is provided to our programs for any type of data, a model, user interface controls, and an attribute ranking are automatically produced.

NICeSim is dynamic because it uses cutting-edge learning methods to build a just-in-time ML model. Notice that this is an important feature as well, because new instances can be added to the dataset as new patients are admitted to the ICU. Hence, future models become increasingly accurate.

A current limitation of the simulation environment built in this study is that only binary classification is allowed. However, it is clear that a plethora of interesting studies concerning binary classification that are very common in several domains of science could be approached by the computational method presented here.

In particular, we demonstrated the usefulness of NICeSim for studies on prenatal and perinatal care by constructing a model with a very good predictive power to simulate the effects that the variables Apgar scale, respiratory distress syndrome, gestational age, and birth weight have on the death risk of preterm neonates. Through experiments performed by three physicians with experience in intensive care as well as a neonatal nutritionist, we showed qualitative and quantitative aspects of this study, and discussed how such an analysis could be useful for investigations where the objective is to establish new medical standards, especially for public health planning.

Unfortunately, we could not compare NICeSim with other scoring systems such as CRIB, because our dataset does not contain all attributes that are necessary for these systems. For instance, CRIB uses the maximum and minimum fraction of inspired oxygen and the maximum base excess during the first 12 h. Additionally, as exhaustively mentioned in the text, our goals are different from the goals of these scoring systems. We are more concerned with providing a computational environment for medical research aimed at supporting decision making in terms of prevention and collective health care.

In a future effort, we intend to work with health professionals from other hospitals to further assess the reliability of our system by comparing the results of distinct groups of clinicians. In addition, although our system has only two simple screens, we intend to assess the usability of NICeSim by using metrics such as the system usability scale (SUS) [33].

By including other professionals and other populations of neonates from several hospitals in the future, we will have also the possibility of building more comprehensive datasets with the specific goal of performing simulations as presented in this study. Notice that the database that originated the data used here was not intended for the purposes presented in this work. We strongly believe that constructing databases with the original objective of running the described computational simulations would maximize the results that such in silico experiments could produce. Furthermore, comprehensive databases result in a lower chance of obtaining an ML model with distorted responses, because the presence of more instances also indicates greater representativeness of distinct scenarios.

Finally, considering the generality of our approach, we plan to apply our method to other medical problems.

@&#ACKNOWLEDGEMENTS@&#

We would like to thank the funding agencies Capes, FAPEMIG, and CNPq for the financial support for this project.

@&#REFERENCES@&#

