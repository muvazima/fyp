@&#MAIN-TITLE@&#Manycore challenge in particle-in-cell simulation: How to exploit 1 TFlops peak performance for simulation codes with irregular computation

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Toughness of inter-node, intra-node and intra-core parallelism is discussed.


                        
                        
                           
                           Story change by manycore processors is exemplified by PIC simulation code.


                        
                        
                           
                           Manycore- and SIMD-aware implementation improves the performance 10-fold.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Manycore processors

SIMD-vectorization

Multithreading

Particle-in-cell simulation

High-performance computing

@&#ABSTRACT@&#


               
               
                  This paper discusses the challenge in post-Peta and Exascale era especially that brought by manycore processors of ordinary (i.e., non-GPU type) CPU cores. Though such a processor like Intel Xeon Phi gives us TFlops-class computational power and may lead us to Exascale computing, full exploitation of its potential is far from an easy job due to its source of high performance, namely a large scale multithreading and a wide SIMD mechanism. In fact, in the three-tier parallelism namely inter-node, intra-node and intra-core ones, we found their order does not represent the toughness in HPC programming but the order should be reversed to do that. Our case study with a particle-in-cell plasma simulation code supports our observation revealing that a simple porting of an existing code to Xeon Phi is infeasible from the viewpoint of performance and we have to make a significant change of the code structure so that it conforms with the features of the processor. However the study also confirms that the recoding effort is well rewarded achieving a good single-node performance higher than that obtained from an execution on four dual-socket nodes of Cray XE6.
               
            

@&#INTRODUCTION@&#

In the recent post-Peta supercomputers, we have seen a significant change in their architecture especially of processors. At first, we had a new type of processor namely GPGPU by which a certain kind of programs for high-performance computing (HPC) is significantly accelerated when it is rewritten with, for example, CUDA in order to exploit many but simple cores of the processor. Then more recently another type of manycore processor has emerged to give us an alternative way to enjoy 1 TFlops-class peak performance with conventional OpenMP programming.

That is, in 2012 Intel started the delivery of their new processor family based on Knights Corner architecture as their first commercial products of Xeon Phi series [1]. Since a CPU core of Xeon Phi has an ordinary type of x86 architecture though old-fashioned and up to 61 cores share a single memory as done in ordinary multicore processors, we can easily port a HPC program running on a multicore processor or a shared memory node of them onto Xeon Phi, providing the relatively small memory of up to 16GB can accommodate the large data set of the program or a process. Moreover, a core is capable of 16 double-precision floating-point operations in every machine cycle by a SIMD (single-instruction multiple-data) fused multiply–add mechanism to make its per-core performance comparable to that of multicore processors of same generation though its clock frequency is less than the half of theirs. Therefore, thanks to the large number of cores, Xeon Phi’s peak performance is about 5–6 times as high as that of these multicore processors and is comparable to a GPGPU’s.

These features of compatibility and high performance have attracted HPC community resulting in a significant number of recent installations of high-end supercomputers with Xeon Phi. In fact, the latest Top500 list of November, 2014 [2] has 25 systems including 11 in the top 100 and, more impressively, the world fastest Tianhe-2 [3] of 55 PFlops peak and 34 PFlops LINPACK performance. Moreover, the attractivity has grown by the recent announcement from Intel that they continue the development of Xeon Phi series to deliver its next version based on Knights Landing architecture in a few years with performance enhancement in both of computation and memory access. The other good news is that the successor will work stand-alone unnecessitating the host processor which the current coprocessor version requires.

As for the application performance, it is well known that Xeon Phi exerts almost full of its potential with applications whose kernel has dense matrix multiplication as evidenced by the fairly good, if not impressive, LINPACK efficiency [4,5]. It is also predictable that stencil computations on regular structured meshes are favorites of Xeon Phi because of a large scale parallelism and regular memory accesses [6–8]. On the other hand, it is reported that applications having irregular computation structure in terms of control flow and/or data access are not benefited well even when they have sufficiently large parallelism [9,10]. In fact, sometimes we see that Xeon Phi shows a good speed-up relative to its own single core execution, but fails to achieve a satisfactory level of absolute performance, occasionally being inferior to its host of an ordinary Xeon.

In this paper the author discusses one of the last type applications, namely particle-in-cell (PIC) simulation code [11] for plasma physics, for which a significant recoding effort is required to achieve good absolute performance. Before describing the effort and its toughness, however, Section 2 summarizes the architectural features of Xeon Phi emphasizing the source of its high performance, which at the same time is the cause of toughness. Then Section 3 discusses another important and general issue about the hierarchy of parallelism and the toughness to exploit each tier of it, as the basis of the discussion specific to PIC simulation given in Section 4 emphasizing the importance of lower two tiers of the hierarchy namely intra-node and intra-core parallelisms. After summarizing related work on PIC and other particle simulations and their SIMD-aware implementations in Section 5, Section 6 gives the conclusion of this paper mentioning to the future work.

Intel is delivering three series of Xeon Phi processors of Knights Corner (KNC) architecture, 3100, 5100 and 7100 of different core counts, frequencies, memory capacities and bandwidths as shown in Table 1
                      
                     [12]. The micro-architecture of them is, however, perfectly common and is based on the in-order dual-issue Pentium core having 32KB L1 instruction and data caches and 512KB L2 unified cache [13]. A processor chip consists of up to 61 cores connected by a ring interconnect to make their caches coherent and GDDR5 memory uniformly shared by them.

The most important enhancement to the old-fashioned core is the 512-bit wide SIMD vector unit to enable us to perform eight double-precision floating-point (DPFP) operations by a single instruction in the instruction set extended only for KNC architecture
                        1
                        Therefore the extension is incompatible with SSE or AVX.
                     
                     
                        1
                      
                     [14]. Since the extension has fused multiply–add (FMA) instructions to perform multiplication and addition such as 
                        
                           (
                           a
                           ×
                           b
                           )
                           +
                           c
                        
                      of eight sets of DPFP operands, per-core and per-instruction peak throughput of DPFP operations is 
                        
                           16
                           =
                           8
                           ×
                           2
                        
                     . Note that a vector operand of a SIMD instruction may come from memory but, if so, its elements must be contiguous and aligned in memory.

Another important feature of the core is 4-way Hyper Threading (HT) to pick a pair of instructions from up to four threads in a round-robin manner. This mechanism is essential to let Xeon Phi exert its full potential because it cannot issue an instruction pair from a thread back-to-back and SIMD instructions cannot be paired. That is, in order to have the throughput of 16 DPFP operations per cycle, we need to have not only a tight loop issuing FMA instructions continuously but also at least two threads to feed the FMA to the vector unit every cycle. In addition, since the in-order mechanism fully relies on HT for instruction latency hiding, it may require to have three or four threads for better performance.

In summary, for the exertion of 1 TFlops potential power of Xeon Phi, the innermost kernel loop must be SIMD-vectorized and thus its body preferably consists of conditional- and recurrence-free operations with contiguous memory accesses. The other requirement is to place two or more threads onto a core resulting in a deep multithreading of 120 or more threads if we adopt a simple and natural one-process/one-node configuration in our, for example, MPI-OpenMP hybrid parallel programming. In addition, it should be remembered that Xeon Phi is a scalar processor whose memory access performance heavily relies on caches and thus access locality. The other remark on memory access is that its large bandwidth up to 352GB/s is not really large in terms of relative value to its peak performance. In fact, the byte-per-flop ratio 0.35 of the model 5100D is not much superior to Xeon E5-2670’s 0.31.

This section discusses the role of manycore processors like Xeon Phi in the hierarchy of parallelism in large scale HPC systems and programs. The hierarchy is formed by three tiers; inter-node parallelism usually programmed by MPI; intra-node parallelism corresponding to shared memory multithreading; and intra-core parallelism to exploit SIMD mechanism.

In a large scale system, the dominating tier is of course inter-node and we have already had 
                        
                           
                              
                                 10
                              
                              
                                 5
                              
                           
                        
                     -scale parallelism in Blue Gene/Q [15] and K-computer [16], in which the lower two tiers are much smaller than the inter-node, 
                        
                           16
                           ×
                           8
                        
                      and 
                        
                           8
                           ×
                           8
                        
                      respectively. However, the dominance is now declining due to the emergence of manycore processors by which the lower two tiers become significantly fat. That is, Tianhe-2 has about a half number of processors of the former Top-1 rankers but its 
                        
                           
                              
                                 10
                              
                              
                                 8
                              
                           
                        
                     -scale total parallelism of three tiers is more than one order of magnitude as large as them because Xeon Phi has 
                        
                           120
                           ×
                           16
                        
                      parallelism in the lower two tiers.
                        2
                        For intra-node parallelism, the core count is doubled because the inevitable 2-way HT on a core can be considered as the pair of cores of half clock frequency.
                     
                     
                        2
                      This growth of the lower two tiers will continue to Exascale era in which we should have 
                        
                           
                              
                                 10
                              
                              
                                 9
                              
                           
                        
                      or more parallelism in total while the inter-node one is expected to stay around 
                        
                           
                              
                                 10
                              
                              
                                 5
                              
                           
                        
                     -scale.

From the current and future trend described above, it is obvious that we have to pay more attention to the lower two tiers in our HPC programming. An important question is how large effort we have to make for it or, in other words, which tier requires the hardest effort. The answer would seem simple; inter-node is toughest because it requires cumbersome MPI programming in which even getting a small piece of information needs an explicit messaging incurring a long latency and a small throughput; intra-node is somewhat tough but not toughest because OpenMP provides us of a convenient means for easy parallelization; and intra-core is definitely easiest because our compiler takes care of everything about SIMD-vectorization. Unfortunately, however, this answer is totally incorrect, or at least has become incorrect since the manycore processors emerged, as discussed in the following three subsections.

Nobody cannot claim that inter-node MPI programming is easy. Even for a problem having much and obvious parallelism, it is not always an easy job to decompose the problem for many processes avoiding random, frequent and/or a large amount of data transfer to cope with limited bandwidth and relatively long latency of inter-node communication mechanism.

However, at least we have played this game for many years writing a wide spectrum of MPI programs to enjoy a huge speed-up which scalable programs show in their run on large-scale parallel systems. More importantly, we have already learned scalability issues from the programming experience, especially some golden rules about what we must not do. For example, we have already known that a simple one-dimensional problem decomposition cannot be scaled and thus are decomposing a problem space multi-dimensionally and sometimes multi-paradigmatically to give each process a sufficient amount of computation minimizing the communication amount between them. We have also learned that global collective communications can be a severe bottleneck when they are too frequently performed and/or data amount transferred by them is too large. As for other potential bottlenecks such as the cost of initialization and file I/O, we have been taught that they must not be neglected. Then when a given formulation, algorithm and/or baseline implementation for a problem did not conform to the golden rules, we have dared to redesign them throwing away those old schemes, such as FFT.

Therefore, for an experienced HPC programmer, playing the inter-node game again in a familiar playfield of 
                           
                              
                                 
                                    10
                                 
                                 
                                    5
                                 
                              
                           
                        -scale parallelism will not be extremely tough. Even for those who are still at 
                           
                              
                                 
                                    10
                                 
                                 
                                    3
                                 
                              
                           
                         or less scale, at least they can follow forerunners working on Blue Gene/Q, K-computer, etc. From these observations, we may conclude that we may not fear the toughness of inter-node 
                           
                              
                                 
                                    10
                                 
                                 
                                    5
                                 
                              
                           
                        -scale MPI programming even in Exascale era, providing we will exploit intra-node parallelism by a scheme other than flat-MPI unless it is still feasible with respect to the MPI implementation and the algorithm of our application.

One exceptional emerging problem in MPI programming might be communication latency hiding which, by non-blocking collectives of MPI 3.0 [17], becomes applicable more widely. Though it often requires a drastic change of the structure of existing MPI program codes, we may abandon it if too costly because its acceleration factor cannot exceed 2.0, which we can achieve only in an ideal case of perfectly balanced computation and communication costs. Of course some latency hiding algorithms will emerge to cope with inevitable growth of communication latency relative to computation time in a node, but they may not be a part of the story of MPI programming and its toughness.

It is a straight fact that intra-node parallel programming was easier than the inter-node counterpart in good-old 
                           
                              
                                 
                                    10
                                 
                                 
                                    1
                                 
                              
                           
                        -scale multithreading. That is, a simple one-dimensional problem decomposition implemented with “#pragma omp for” was sufficient for most of HPC programs and, even if such a simple and easy coding brought an overhead proportional to the number of threads, an additional 
                           
                              
                                 
                                    10
                                 
                                 
                                    1
                                 
                              
                           
                        -order cost for parallelization was acceptable.

However, the 
                           
                              
                                 
                                    10
                                 
                                 
                                    2
                                 
                              
                           
                        -scale multithreading in manycore with HT changed the story significantly. For example, the full employment of 
                           
                              60
                              ×
                              4
                              =
                              240
                           
                         threads for Xeon Phi could make the one-dimensional decomposition impossible. That is, if you decompose your whole three-dimensional problem into cubes for MPI processes and then decompose a cube for OpenMP threads one-dimensionally, the cube must be at least 
                        
                           
                              
                                 
                                    240
                                 
                                 
                                    3
                                 
                              
                           
                         even if you accept the inevitable cache coherence overhead caused by the fact that every plane of 
                           
                              
                                 
                                    240
                                 
                                 
                                    2
                                 
                              
                           
                         is shared by three threads in the simplest 7-point stencil computation. On the other hand, this minimum cube of 
                           
                              
                                 
                                    240
                                 
                                 
                                    3
                                 
                              
                              ≈
                              1.4
                              ×
                              
                                 
                                    10
                                 
                                 
                                    7
                                 
                              
                           
                         elements might be too large to be accommodated in the relatively small 8GB memory of Xeon Phi 5100, if an element consumes 640-byte for 80 DPFP data or more. In other words, you will face a very critical memory capacity problem when you try to double the size of the cube in all of three dimensions to decrease the number of sharer of a plane to two, even if a cube element has only a few three-dimensional vectors.

Therefore, we will have to decompose the problem multi-dimensionally giving up the easiest implementation with “omp for”. Such implementation itself is not very tough because we have already done similar work for MPI parallelization, though narrating one story twice in different dialect should be cumbersome. However, we have to be aware that this MPI-like OpenMP programming must be tougher than MPI programming because it easily sneaks bugs, especially those degrade performance, into the program. That is, accessing shared variables is too easy to keep us from doing stupid operations such as those causing racing or, more likely, severe false sharing, which we cannot do in MPI programming.

In addition, multithreading with HT has a subtle issue in problem decomposition because threads running on a core shares everything in the core. For example, in ordinary multithreading, we try to decompose a data set into subsets so that they are separated as much as possible to minimize coherence traffic between caches of cores each of which accommodates only one thread. However in two-way HT case, this decomposition simply halves the effective size of a cache for a thread and thus, if the total working set size of two threads is larger than the cache size, doubles the cache miss rate. Therefore, it might be better to mix two subsets by, for example, cyclic decomposition especially for stencil computation. If we have to perform this non-uniform decomposition, the programming should become much tougher.

Readers who are familiar with vector supercomputers should be aware of the resemblance between the classic vector pipeline and SIMD vector mechanism. Both mechanisms share favorite loops in which one-dimensional arrays, or vectors, are scanned to perform operations onto their elements in a do-all manner. Another resemblance is that the loops are automatically vectorized by compilers by which we are almost free from explicit parallelization of the loops.

The list of resemblance between two mechanisms, however, terminates here because SIMD’s capability is much more limited than vector pipeline’s. For example, SIMD load/store does not have the capability of striding access or the powerful list-vector mechanism resulting in the degradation of applicability and/or efficiency of SIMD-vectorization for loops with non-contiguous accesses and a high byte-per-flop requirement. Even for a loop only with contiguous accesses, SIMD’s efficiency should be significantly inferior to vector pipeline’s when the loop has many long vector streams because, as discussed in Section 2, the byte-per-flop ratio is at the level of ordinary scalar processors and thus much lower than vector machines. Another architectural factor limiting the applicability of SIMD-vectorization is the size of vector registers, 
                           
                              32
                              ×
                              8
                              ×
                              8
                           
                         B which looks impressive but is much smaller than 
                           
                              576
                              ×
                              64
                              ×
                              8
                           
                         B of the modern vector machine NEC SX-9 [18].

Though these limitations are inevitable to make manycore processors much more cost effective and power efficient than classic vector processors, they definitely and significantly reduce the chance of SIMD-vectorization by a compiler for a loop fairly more complicated than a simple dense matrix arithmetic or stencil computation. A bad news is that we must completely rely on our compiler about whether and how our loop is vectorized,
                           3
                           Unless we use assembly-style intrinsic functions to degrade the productivity, maintainability and portability of our codes.
                        
                        
                           3
                         unlike intra-node parallelization for which OpenMP have made us free from parallelizing compilers having annoyed us by their mysterious and uncontrollable behavior.

Therefore, we have to make try-and-errors of rewriting the innermost kernel loop to present it to our compiler in the form conformable to it examining the result by compilation messages and, to have more confidence, execution time and profile.
                           4
                           And compilations with “-S” option as the author does for the codes discussed in Section 4.
                        
                        
                           4
                         Such rewriting requires some deep knowledge not only about the SIMD instruction set and micro-architecture but also about classic loop optimization techniques such as loop-splitting, loop-interchange, and so on, whose application must be cache aware. For example, loop-splitting is a powerful means to reduce the number of vector streams and register usage so that each of split loop is SIMD-vectorized more easily than the original. However, a blind application of it to a loop for long vectors or a multi-dimensional loop nest should drastically degrade the memory access locality and thus the performance. Therefore, the long vector loop must be tiled before the application to the resulting inner loop, while the application to the multi-dimensional loop nest should be restricted to the innermost one [7].

In addition to the code restructuring, the rewriting may also involve the redesign of data structures. For example, an array-of-structure may have to be converted to a structure-of-array to change the access of structure elements from striding to contiguous. Coping with the lack of list-vector will require more complicated redesign to convert random and indirect accesses to contiguous ones even if it is possible.

In summary, we have to conclude that the intra-core parallelism is toughest among the parallelism hierarchy for HPC programming. This means that the exploitation of Xeon Phi’s full potential is tough but, at the same time, we cannot escape from challenging this toughness because the toughness is not unique to Xeon Phi but will be common in any high-performance processors in foreseeable future toward Exascale. A good news is the toughness can be conquered by combining deep knowledge about architecture, compilation techniques and, most importantly, an HPC application itself as discussed in the next Section 4.

This section discusses a recent ongoing work of the author to implement a particle-in-cell (PIC) space plasma simulation code on Xeon Phi, in order to exemplify manycore challenges discussed in the previous Section 3. The PIC method is to simulate the motion of plasma particles being ions and electrons by modeling them as a huge set of super-particles moving in a large scale discretized electromagnetic field [11]. In order to have a result conforming with the real physical behavior with a sufficient level of accuracy, such as that obtained from a simulation of plasma around a spacecraft shown in Fig. 1
                      
                     [19], it is common that the number of simulated particles is 
                        
                           
                              
                                 10
                              
                              
                                 11
                              
                           
                        
                     -order and the grid of electromagnetic field is 
                        
                           
                              
                                 (
                                 
                                    
                                       10
                                    
                                    
                                       3
                                    
                                 
                                 )
                              
                              
                                 3
                              
                           
                        
                     -scale.

A PIC simulation is to track the time evolution of the system by repeating the following three operations for each discrete time step.
                        
                           •
                           
                              Particle-push is to accelerate each particle by electric and Lorentz force laws referring to the electric and magnetic filed 
                                 
                                    E
                                 
                               and 
                                 
                                    B
                                 
                               surrounding the particle. More specifically, with the first-order shape function, the acceleration acting on a particle p is determined by its position 
                                 
                                    
                                       
                                          x
                                       
                                       
                                          p
                                       
                                    
                                 
                              , velocity 
                                 
                                    
                                       
                                          v
                                       
                                       
                                          p
                                       
                                    
                                 
                              , mass 
                                 
                                    
                                       
                                          m
                                       
                                       
                                          p
                                       
                                    
                                 
                              , charge 
                                 
                                    
                                       
                                          q
                                       
                                       
                                          p
                                       
                                    
                                 
                               and the electromagnetic field vectors 
                                 
                                    E
                                 
                               and 
                                 
                                    B
                                 
                               defined at the set of grid points 
                                 
                                    δ
                                    
                                       
                                          x
                                       
                                       
                                          p
                                       
                                    
                                 
                               being the eight vertex of the cubic cell in which the particle resides, as shown in Fig. 2
                              . Then the particle moves to a new position 
                                 
                                    
                                       
                                          x
                                       
                                       
                                          p
                                       
                                    
                                 
                               according to its updated velocity 
                                 
                                    
                                       
                                          v
                                       
                                       
                                          p
                                       
                                    
                                 
                              .


                              Current-scatter is to calculate current density 
                                 
                                    J
                                 
                               summing up the small currents caused by the motion of charged particles. From the viewpoint of a particle p, this calculation is to add the contribution of its motion to 
                                 
                                    J
                                 
                               at 
                                 
                                    δ
                                    (
                                    
                                       
                                          x
                                       
                                       
                                          p
                                       
                                    
                                    -
                                    
                                       
                                          v
                                       
                                       
                                          p
                                       
                                    
                                    )
                                 
                               and 
                                 
                                    δ
                                    
                                       
                                          x
                                       
                                       
                                          p
                                       
                                    
                                 
                               corresponding to the cells in which the particle reside before and after the motion respectively, according to the first-order shape function again, as shown in Fig. 2.


                              Field-solve is to update 
                                 
                                    E
                                 
                               and 
                                 
                                    B
                                 
                               with 
                                 
                                    J
                                 
                               using the leapfrog method to solve Maxwell’s equations.

Since the particle-push for a particle is perfectly independent to others and current-scatter and field-solve for a grid point depend only on particles surrounding it and other grid points adjacent to it respectively, the PIC method inherently has plenty of parallelism. At the same time, the hugeness of the particle population and the largeness of the grid space make it inevitable to parallelize PIC simulations not only for reduce the execution time of a simulation but also for its feasibility with respect to the limited amount of node memory of distributed memory supercomputers. Therefore there have been many researches of parallelized PIC simulations in which particle decomposition (e.g., [20]) and/or space decomposition (e.g., [21]) are adopted as exemplified in the following three subsections.

A serious problem in parallelizing PIC simulation is that particles are not distributed in the simulated space domain uniformly. Therefore a simple space decomposition should face a severe load imbalance because a process can have too many particles in the subdomain given to it while another process can be given an almost empty subdomain. More seriously, this non-uniform distribution is not stable but dynamically changes in the progress of the simulation to make a dense subdomain sparse and vice versa. On the other hand, a simple particle decomposition is not scalable because the current-scatter needs to sum up the contribution from particles surrounding each grid point resulting in a global reduction of prohibitively large three-dimensional array for 
                           
                              J
                           
                        , whose size is about 24GB in a typical space domain size of 
                           
                              
                                 
                                    (
                                    
                                       
                                          10
                                       
                                       
                                          3
                                       
                                    
                                    )
                                 
                                 
                                    3
                                 
                              
                           
                        , in every time step.

The load balancing library OhHelp 
                        [22] developed by the author is a powerful means to solve the both problems in MPI parallelization by the combination of space and particle decomposition. As shown in Fig. 3
                        , OhHelp simply and equally decomposes the space domain into subdomains to give each of them to each process as its primary subdomain. Then in addition, a subdomain whose particle population is above the average is also given to other processes as their secondary subdomains. An important feature is that every process but one root has one single secondary subdomain regardless the particle density in its primary subdomain to achieve perfect balancing of particle population as shown in Fig. 4
                        . Moreover, OhHelp examines if particle transfers across subdomain boundaries make the load imbalance intolerable. Then if so, OhHelp tries to make it tolerable by exchanging particles among processes responsible for a subdomain as primary or secondary ones. If this local balancing is infeasible, the global balancing is reexamined to modify secondary subdomain assignments.

The effectiveness and efficiency of OhHelp are proved in various production-level simulations. For example, Fig. 5
                         shows the weak-scale speed-up rates of a OhHelp-ed flat-MPI simulator named EMSES [19,23] run on Cray XE6 supercomputer in Kyoto University, whose node has two 16-core Opteron Abu Dhabi processors of 320 GFlops aggregate peak performance. The evaluation was carried out with two extreme artificial settings of particle distribution; uniform to let all subdomains of 
                           
                              
                                 
                                    32
                                 
                                 
                                    3
                                 
                              
                           
                         grid points always have the same number of particles 
                           
                              
                                 
                                    2
                                 
                                 
                                    23
                                 
                              
                           
                        ; and congested to make all particles in the uniform case, i.e., 
                           
                              
                                 
                                    2
                                 
                                 
                                    23
                                 
                              
                           
                         times the number of processes, condensed in a small region of 
                           
                              
                                 
                                    32
                                 
                                 
                                    3
                                 
                              
                           
                        . The figure clearly exhibits good scalability in both of uniform and congested cases to achieve 2865-fold and 2720-fold speed-up rates with 4096 processes and cores. Note that since two-process speed-up rates are 1.54 and 1.53 respectively due to a take-off cost, the parallel efficiencies of any number of processes are almost ideal.

Since there are no visible problems in scaling OhHelp-ed simulation with more processes or replacing the one-core execution of a process with multithreaded one, we may confidently expect that this application efficiently runs on a manycore-based large-scale supercomputer, providing its manycore processor executes the multithreaded code efficiently.
                           5
                           And MPI on it is reasonably efficient. Though this proposition is questionable mainly due to the current hosted configuration of Xeon Phi equipped nodes in which an inter-node communication involves a data transfer between Xeon Phi and the host processor, this issue is beyond the scope of this paper.
                        
                        
                           5
                         Therefore, the inter-node parallel programming of PIC exemplifies the claim in Section 3.1 that we may rely on our savings produced from our hard effort made in the past. This also lets us concentrate on the implementations of lower two tiers, intra-node and intra-core, as discussed in the following two subsections together with preliminary performance evaluations.

Though the EMSES simulator shown in Section 4.1 is a flat-MPI type parallel program, we have already experienced hybrid implementations with MPI and OpenMP for other plasma simulators. Since our target was a cluster of dual Xeon E5-2670 (Sandy Bridge) nodes having 16 cores in total, a simple multithreading discussed below worked well showing better performance than the flat-MPI implementation especially when we allocate two 8-thread processes to a node, i.e., one process to a processor.

In the simple multithreading we adopted almost only the particle decomposition because it is easy and natural to let all threads in a process share the whole of 
                           
                              E
                           
                         and 
                           
                              B
                           
                         of the subdomain for the process. As for 
                           
                              J
                           
                        , we let each thread t have its own copy namely 
                           
                              
                                 
                                    J
                                 
                                 
                                    t
                                 
                              
                           
                         in which the contributions of particles allocated to t are accumulated, and sum up 
                           
                              
                                 
                                    J
                                 
                                 
                                    t
                                 
                              
                           
                         for all t to have 
                           
                              J
                           
                        . Since the array for particles is one-dimensional, particle-push and current-scatter are easily implemented by “omp for”, while field-solve and 
                           
                              J
                              =
                              ∑
                              
                                 
                                    J
                                 
                                 
                                    t
                                 
                              
                           
                         are also done with “omp for” for the outermost of three-dimensional loop nest to scan the subdomain.

Though this implementation was good for 
                           
                              
                                 
                                    10
                                 
                                 
                                    1
                                 
                              
                           
                        -scale multithreading, manycore’s 
                           
                              
                                 
                                    10
                                 
                                 
                                    2
                                 
                              
                           
                        -scale one changed the story as discussed in Section 3.2. In particular, the spatial overhead of 
                           
                              
                                 
                                    J
                                 
                                 
                                    t
                                 
                              
                           
                         proportional to the number of threads is now prohibitively large because, in 240-thread execution for a subdomain of 
                           
                              
                                 
                                    N
                                 
                                 
                                    3
                                 
                              
                           
                        , it consumes a memory space of 
                           
                              
                                 
                                    N
                                 
                                 
                                    3
                                 
                              
                              ×
                              3
                              ×
                              240
                           
                         DPFP elements while the array for particles has 
                           
                              
                                 
                                    N
                                 
                                 
                                    3
                                 
                              
                              ×
                              6
                              ×
                              ρ
                           
                         elements
                           6
                           Six per-particle elements are only for 
                                 
                                    
                                       
                                          x
                                       
                                       
                                          p
                                       
                                    
                                 
                               and 
                                 
                                    
                                       
                                          v
                                       
                                       
                                          p
                                       
                                    
                                 
                               because the mass 
                                 
                                    
                                       
                                          m
                                       
                                       
                                          p
                                       
                                    
                                 
                               and charge 
                                 
                                    
                                       
                                          q
                                       
                                       
                                          p
                                       
                                    
                                 
                               are common in all particles of a particular species and thus separated from the particle array by a trivial implementation technique.
                        
                        
                           6
                         where 
                           
                              ρ
                              ≈
                              100
                           
                         is the average per-cell density of particles. This means the total of 
                           
                              
                                 
                                    J
                                 
                                 
                                    t
                                 
                              
                           
                         is larger than the particle array having been the biggest memory eater, and required memory space exceeds the double of the baseline.

The overhead caused by 
                           
                              
                                 
                                    J
                                 
                                 
                                    t
                                 
                              
                           
                         is not only spatial but also temporal because the large array must be fully scanned twice in a time step, once for zero-clearing and then for summing up. This means the total of per-step required memory bandwidth is at least tripled
                           7
                           If particles for a thread are well localized. Otherwise, it can be quadrupled in the worst case of random distribution.
                        
                        
                           7
                         to limit the performance due to memory bandwidth bottleneck even with GDDR5 memory.

In fact, we observed the multithread performance is capped by the bottleneck in our experimental implementation of this simple multithreading as shown in Fig. 6
                        . In this experiment and others shown later in this and next subsections, we run a single-process PIC simulation program, written in C99 and OpenMP 3.0, and compiled by Intel Composer version 14.0.3 with “-ipo -O3” option of optimization level, on a node of Cray XC30 having a Xeon Phi 5120D hosted by a Xeon E5-2670v2 (Ivy Bridge). We measured strong-scale multithread performance in terms of the number of particles processed in one second (particles/s), varying the number of cores and the number of threads run on each core. That is, we run the program using c cores each of which has h threads of HT to have 
                           
                              c
                              ×
                              h
                           
                         threads for all 28 combinations of 
                           
                              c
                              ∈
                              {
                              1
                              ,
                              2
                              ,
                              4
                              ,
                              8
                              ,
                              16
                              ,
                              32
                              ,
                              60
                              }
                           
                         and 
                           
                              h
                              ∈
                              {
                              1
                              ,
                              2
                              ,
                              3
                              ,
                              4
                              }
                           
                        . Note that Fig. 6 and similar plots shown later have horizontal axes of core counts rather than thread counts because having many threads by HT does not always give us good performance as exemplified by Fig. 6. Also note that we only used the native-mode in which the whole computation is done only by Xeon Phi without any help from the host Xeon, because the whole of the single-process
                           8
                           Therefore the symmetric-mode is out of scope.
                        
                        
                           8
                         simulation kernel is well-parallelized unnecessitating the offloading for host/accelerator work sharing and the execution mode will definitely be the sole choice in the stand-alone host-free configuration of Knights Landing.

The size of space domain 
                           
                              
                                 
                                    N
                                 
                                 
                                    3
                                 
                              
                           
                         in this experiment is 
                           
                              
                                 
                                    64
                                 
                                 
                                    3
                                 
                              
                           
                        , less than one third of the other experiments due to the memory pressure by 
                           
                              
                                 
                                    J
                                 
                                 
                                    t
                                 
                              
                           
                        , and particles are uniformly distributed in the space domain with the density 
                           
                              ρ
                              =
                              100
                           
                        . The space decomposition for field-solve and the summation of 
                           
                              
                                 
                                    J
                                 
                                 
                                    t
                                 
                              
                           
                         is two-dimensional applied to each of yz-planes of the space domain which C’s three-dimensional arrays of shape “[z][y][x]” correspond to, because we could not parallelize them by “omp for” due to the large number of threads as discussed in Section 3.2. The speed-up curve in Fig. 6 clearly shows that the performance is capped by the memory bandwidth bottleneck especially with 2-way or larger degree of HT, to make 60-core but 1-way case fastest.

The discussion and the experimental result above clearly state the necessity of the replacement of 
                           
                              
                                 
                                    J
                                 
                                 
                                    t
                                 
                              
                           
                         with another efficient method to solve the conflicting update of 
                           
                              J
                           
                         in current-scatter. One strong candidate for the replacement is a well-known technique particle sorting 
                        [24] according to position which many simulators including EMSES incorporate in their codes mainly for improving access locality of 
                           
                              E
                              ,
                              B
                           
                         and 
                           
                              J
                           
                        . The sorting also enables us to hold all particles in a small set of cells. For example, sorting particles by the z-coordinate component of their position gives us disjoint sets of particles residing in each xy-slab. The other type sorting to gather particles in each of cuboid block of cells is done by assuming that the values of coordinate components in a certain range are equal.

An important feature of sorting of a particle set P is that it can be implemented with a 
                           
                              O
                              (
                              |
                              P
                              |
                              )
                           
                         algorithm (e.g., bucket sort) rather than 
                           
                              O
                              (
                              |
                              P
                              |
                              log
                              |
                              P
                              |
                              )
                           
                         because we know the range of coordinate component values. Therefore, the overhead for sorting is acceptable especially when a simulator is multiprocessed by space decomposition for which we need to scan all particles to pick those crossing subdomain boundaries.
                           9
                           For example, OhHelp has an option to sort particles on its balancing-aware particle transfer.
                        
                        
                           9
                         Moreover, sorting can be done occasionally, e.g., every some time steps, if we allow a small amount of mixture of particles among adjacent sets of cells.

With the sorting, we can make the update of 
                           
                              J
                           
                         
                        conflict-free by coloring cell-blocks as shown in Fig. 7
                        . More specifically, we decompose the space domain k-dimensionally into cell-blocks whose size of each axis is 3 or larger and give them one of 
                           
                              
                                 
                                    2
                                 
                                 
                                    k
                                 
                              
                           
                         colors so that adjacent cell-blocks are colored differently. Then we let each thread has its own set of 
                           
                              
                                 
                                    2
                                 
                                 
                                    k
                                 
                              
                           
                         cell-blocks adjacent to each other and having different colors, so that threads perform particle-push and current-scatter on particles in cell-blocks of a particular color in parallel with barrier synchronization on the completion of the procedure for the color and thus the start of that for the next color. The reason why this multithreading is conflict-free is illustrated in the figure. That is, since a particle can travel only to one of the cells adjacent to the cell where it resided
                           10
                           This proposition is true in almost all PIC simulations.
                        
                        
                           10
                         and the edge size of a colored cell-block is at least three, all particles in cell-blocks of a particular color are separated by at least one cell after the particle-push operations on them. Note that this coloring technique does not require strict sorting on each time step. For example, if the shortest edge of a cell-block is 
                           
                              2
                              τ
                              +
                              1
                           
                         or longer, we may perform the sorting in every τ steps. In addition, the period can be a little bit longer than τ if we update 
                           
                              J
                           
                         with “omp atomic” after the expiration of τ steps providing the atomic update is fast enough for eventually non-conflict operations.


                        Fig. 8
                         shows the performance of the colored multithreading together with the best result of the simple multithreading. The environment and settings of this experiment are same as the previous ones except for the space domain size 
                           
                              
                                 
                                    N
                                 
                                 
                                    3
                                 
                              
                              =
                              
                                 
                                    96
                                 
                                 
                                    3
                                 
                              
                           
                         because it is now possible. In this program we sort all particles strictly according to their resident cell (not cell-block) and in every time step because its cost must be paid in multiprocess execution anyway. The effectiveness of the coloring to remove the large 
                           
                              
                                 
                                    J
                                 
                                 
                                    t
                                 
                              
                           
                         is clear because the best 60-core performance with 2-way HT is 1.8 times as high as the best of simple multithreading. This 2-way HT case also shows an almost ideal 57.3-fold speed-up relative to 1-core 2-thread execution.

Though the result above looks very satisfactory, we cannot conclude the story here because Fig. 8 only shows the relative performance inside Xeon Phi. Therefore, we measured the performance of this version of the program on two Xeon processors, E5-2670v2 (Ivy Bridge) of 10 cores being the host of Xeon Phi, and E5-2695v3 (Haswell) of 14 cores being the basic component of another XC30 in Kyoto. The full-core performance comparison shown in Fig. 9
                         is disappointing unfortunately, because it says Xeon Phi’s performance is less than half of Haswell’s though its peak performance is about twice as high as Haswell’s. More disappointingly Xeon Phi is inferior to its host Ivy Bridge whose peak performance is less than one fifth of Xeon Phi’s.

Therefore, we have to continue the challenge descending into the lowest tier of parallelism, intra-core, in the next Section 4.3.

The reason why we had the disappointing result in Section 4.2 is that the compiler cannot perform SIMD-vectorization onto the innermost loop to scan particles in a cell-block for particle-push and current-scatter, due to indirect accesses to 
                           
                              E
                           
                        , 
                           
                              B
                           
                         and 
                           
                              J
                           
                         in its body. As shown in Fig. 10
                        , in each iteration 
                           
                              24
                              =
                              3
                              ×
                              8
                           
                         elements of 
                           
                              E
                              (
                              δ
                              
                                 
                                    x
                                 
                                 
                                    p
                                 
                              
                              )
                           
                         and 
                           
                              B
                              (
                              δ
                              
                                 
                                    x
                                 
                                 
                                    p
                                 
                              
                              )
                           
                         are referred to for updating 
                           
                              
                                 
                                    v
                                 
                                 
                                    p
                                 
                              
                           
                        . Though the accesses are not fully random because all particles scanned in the loop are in a cell-block, the compiler cannot know that because the array indices are calculated from DPFP data representing 
                           
                              
                                 
                                    x
                                 
                                 
                                    p
                                 
                              
                           
                        . As for 
                           
                              J
                           
                        , the situation is much worse because the read-modify-writes of 
                           
                              J
                              (
                              δ
                              (
                              
                                 
                                    x
                                 
                                 
                                    p
                                 
                              
                              -
                              
                                 
                                    v
                                 
                                 
                                    p
                                 
                              
                              )
                              )
                           
                         and 
                           
                              J
                              (
                              δ
                              
                                 
                                    x
                                 
                                 
                                    p
                                 
                              
                              )
                           
                         look random and thus they should be serialized for each iteration. Therefore, if the compiler tries to vectorize the loop with 8-way unrolling, the resulting code should be as follows.
                           
                              1.
                              Load 
                                    
                                       24
                                       ×
                                       2
                                       ×
                                       8
                                       =
                                       384
                                    
                                  elements of 
                                    
                                       E
                                       (
                                       δ
                                       
                                          
                                             x
                                          
                                          
                                             p
                                          
                                       
                                       )
                                    
                                  and 
                                    
                                       B
                                       (
                                       δ
                                       
                                          
                                             x
                                          
                                          
                                             p
                                          
                                       
                                       )
                                    
                                  for eight particles one by one and pack them into 
                                    
                                       24
                                       ×
                                       2
                                       =
                                       48
                                    
                                  vector registers.

Perform 8-way vectorized operations to update 
                                    
                                       
                                          
                                             v
                                          
                                          
                                             p
                                          
                                       
                                    
                                  and then 
                                    
                                       
                                          
                                             x
                                          
                                          
                                             p
                                          
                                       
                                    
                                 .

Perform 8-way vectorized operations to calculate the increments of 
                                    
                                       12
                                       ×
                                       2
                                       ×
                                       8
                                       =
                                       192
                                    
                                  elements of 
                                    
                                       J
                                       (
                                       δ
                                       (
                                       
                                          
                                             x
                                          
                                          
                                             p
                                          
                                       
                                       -
                                       v
                                       )
                                       )
                                    
                                  and 
                                    
                                       J
                                       (
                                       δ
                                       
                                          
                                             x
                                          
                                          
                                             p
                                          
                                       
                                       )
                                    
                                  for eight particles to store the result into 
                                    
                                       12
                                       ×
                                       2
                                       =
                                       24
                                    
                                  vector registers. Note that the motion of a particle p in a cell does not contribute to all 
                                    
                                       3
                                       ×
                                       8
                                       =
                                       24
                                    
                                  elements at 8 vertices of the cell but only to 12 ones in our formulation of PIC method.

Perform read-add-write for each of 192 elements of 
                                    
                                       J
                                    
                                  one by one.

Finding so many one-by-one operations in the procedure above and knowing Xeon Phi has only 32 vector registers, we cannot complain to the compiler of its abandonment of vectorization. Note that the story cannot be changed even if we scan particles in each cell rather than cell-block, because the compiler cannot know the elements of 
                           
                              E
                              ,
                              B
                           
                         and 
                           
                              J
                           
                         accessed in the whole iterations of the loop are common.

Therefore, we have to let the compiler know that 
                           
                              E
                              (
                              δ
                              
                                 
                                    x
                                 
                                 
                                    p
                                 
                              
                              )
                           
                        , 
                           
                              B
                              (
                              δ
                              
                                 
                                    x
                                 
                                 
                                    p
                                 
                              
                              )
                           
                         and 
                           
                              J
                              (
                              δ
                              
                                 
                                    x
                                 
                                 
                                    p
                                 
                              
                              )
                           
                         are common by placing them in a set of scalar variables. That is, in addition to the strict sorting in every time step which we have already done, the particle scanning loop is transformed into the following three cell-wise loops as shown in Fig. 11
                        .
                           
                              1.
                              Update 
                                    
                                       
                                          
                                             v
                                          
                                          
                                             p
                                          
                                       
                                    
                                  for all p in a cell referring to the common 
                                    
                                       E
                                       (
                                       δ
                                       
                                          
                                             x
                                          
                                          
                                             p
                                          
                                       
                                       )
                                    
                                  and 
                                    
                                       B
                                       (
                                       δ
                                       
                                          
                                             x
                                          
                                          
                                             p
                                          
                                       
                                       )
                                    
                                  which have been scalarized prior to the loop.

Update scalarized and common 
                                    
                                       J
                                       (
                                       δ
                                       
                                          
                                             x
                                          
                                          
                                             p
                                          
                                       
                                       )
                                    
                                  and then update 
                                    
                                       
                                          
                                             x
                                          
                                          
                                             p
                                          
                                       
                                    
                                  for all p in a cell. This loop is split from the first one to reduce the number of scalar variables referred to in each loop. After the completion of the loop, the scalarized 
                                    
                                       J
                                       (
                                       δ
                                       
                                          
                                             x
                                          
                                          
                                             p
                                          
                                       
                                       )
                                    
                                  are written back to the array for 
                                    
                                       J
                                    
                                 .

After particle sorting, scan all cells and all particles in each of them to update 
                                    
                                       J
                                       (
                                       δ
                                       
                                          
                                             x
                                          
                                          
                                             p
                                          
                                       
                                       )
                                    
                                  according to their new residence. Elements of 
                                    
                                       J
                                       (
                                       δ
                                       
                                          
                                             x
                                          
                                          
                                             p
                                          
                                       
                                       )
                                    
                                  are scalarized prior to the particle scanning loop and then written back to the array of 
                                    
                                       J
                                    
                                  after the loop completion.

Note that 
                           
                              24
                              ×
                              2
                              =
                              48
                           
                         scalar variables for 
                           
                              E
                           
                         and 
                           
                              B
                           
                         commonly referred to in each iteration of the loop-1 are not necessary to occupy 48 vector registers because a vector register element can be broadcasted in SIMD vector instructions. As for 12 scalar variables for 
                           
                              J
                           
                         in the loop-2 and 3, however, it requires 12 vector registers to keep the local sum of all i-th iterations such that 
                           
                              i
                              
                              mod
                              
                              8
                              =
                              k
                           
                         in the k-th element of each vector register and to sum up them after the loop completion to have the value to be written back to the array for 
                           
                              J
                           
                        . Therefore we split the loop-1 and 2 to minimize the register pressure. Also note that since this split is done at the deepest level of the loop nest to scan all particles in the cell-blocks for a thread, array elements commonly accessed in the two loops, namely 
                           
                              
                                 
                                    x
                                 
                                 
                                    p
                                 
                              
                           
                         and 
                           
                              
                                 
                                    v
                                 
                                 
                                    p
                                 
                              
                           
                         for all p in a cell, are strongly expected to reside in L1 data cache at the beginning of the loop-2.

This drastic code transformation combined with the cell-block coloring is well rewarded as shown in Fig. 12
                         obtained from the experiment with the same environment and settings as Section 4.2. The best case 60-core execution with 3-way HT is 6.9 times as fast as the best of colored multithreading and its 51.5-fold speed-up relative to its one-core execution is sufficiently large. More importantly, the absolute performance is satisfactory in this implementation, because it runs on Xeon Phi as 1.57 times and 2.15 times as fast as on Haswell and Ivy Bridge respectively as shown in Fig. 13
                        . Though these performance ratios do not directly reflect those of peak performance, this reduction can be explained by the less-powerful micro-architecture of Xeon Phi especially for integer instructions. The other remark is that the SIMD-aware implementation is effective to these Xeons as well, because their DPFP computation powers are also obtained from their SIMD mechanisms, AVX-2. The other proof of the satisfaction is found in the comparison between the Xeon Phi’s performance of this SIMD-aware implementation and Cray XE6’s one of EMSES. That is, Xeon Phi’s performance 
                           
                              581
                              ×
                              
                                 
                                    10
                                 
                                 
                                    6
                                 
                              
                           
                         particles/s is superior to that of the four-node 128-core performance of 2.5GHz Opteron processor, 
                           
                              494
                              ×
                              
                                 
                                    10
                                 
                                 
                                    6
                                 
                              
                           
                         particles/s.

In summary, the cell-block coloring discussed in Section 4.2 and SIMD-aware implementation given in this Section 4.3 contributed to the improvement of 3-way HT, 60-core, and thus 180-thread performance, the best one in Fig. 12, from the baseline simple multithreading with 
                           
                              
                                 
                                    J
                                 
                                 
                                    t
                                 
                              
                           
                         as follows. The cell-coloring improved the performance by a factor 2.2 eliminating the accesses to 
                           
                              
                                 
                                    J
                                 
                                 
                                    t
                                 
                              
                           
                         by which we suffer a large amount of physical memory accesses, about 2.1GB per time-step. Then the SIMD-aware implementation gave us further improvement of 8.0-fold resulting in 17.6-fold total improvement from the baseline. Note that though this 8.0-fold improvement looks consistent with the per-cycle DPFP operation counts of the SIMD mechanism, it includes the effect of eliminating per-particle indirect accesses to the elements of 
                           
                              E
                              ,
                              B
                           
                         and 
                           
                              J
                           
                         by letting them loop-invariant scalar variables.

@&#RELATED WORK@&#

Efficient implementations of PIC plasma simulation have been investigated since early 1980s (e.g., [25]) to give us plenty literature including those discussing the parallelization from late 1980s (e.g., [26]). Though most of such work targeted inter-node multiprocess parallelization (e.g., [20,21]), some of them exploited lower-tier parallelism of vector machines [27] and large-scale shared-memory machines [28] straightforwardly like our baseline simple implementation with 
                        
                           
                              
                                 J
                              
                              
                                 t
                              
                           
                        
                     .

Then the recent progress of multicore processors and emergence of manycore processors stimulated PIC research community to report a few but more sophisticated implementations. Among such reports, Ibrahim et al. [24] thoroughly investigated the accesses to field-related arrays from each thread working on its own set of particles so as to determine the optimal size of thread-private replica of the array for charge density, which the thread updates according to the position of each particle as our thread does for 
                        
                           J
                        
                     , and the optimal frequency of particle binning to sort particles according to their positions for access locality improvement. Their optimizations were rewarded with up to 77% performance improvement on various single/dual-socket multicore nodes. They also applied the optimizations to their GPGPU implementation to achieve up to 2.1-fold acceleration from host processor execution, but the acceleration is not very impressive due to irregular and possibly conflicting accesses to the field-related arrays.

More recently, Bastrakov et al. [29] presented performance numbers of their implementations for multicore processors, Xeon Phi and two types of GPGPU. By their locality- and SIMD-aware optimizations, they achieved 10-fold single-core performance improvement for Xeon E5-2690 processor. This optimization should have been effective in the execution on Xeon Phi, but the reported performance of Xeon Phi 7110X of 1.24 TFlops peak performance is merely about 1.25 times as fast as that of Xeon E5-2690 of 185.6 GFlops, supposedly due to limited utilization of SIMD mechanism.

In a broader sense of particle simulation, we can find more reports on Xeon Phi implementations and related issues of molecular dynamics (MD), hydrodynamics (HD), cosmology, and so on. In these applications, the hot kernel is to calculate interactions between particles (atoms, virtual fluid particles, cosmological matters) proximate to each other. Pennycook et al. [9] reported this kind of calculation, short-range force calculation in MD simulation, can be SIMD-vectorized exploiting that a particle data has a few components such as those of its three-dimensional position vector while particles are scanned through the neighboring list of them by indirect accesses. They presented that this SIMD-vectorization is effective in executions on dual Xeon E5-2660 of 282 GFlops and a pre-production Xeon Phi of 1.25 TFlops
                        11
                        Significantly faster than production chips due to higher frequency of 1.3GHz.
                     
                     
                        11
                      but the superiority of the latter over the former is limited to up to about 1.4-fold. Therefore, it could be necessary for the full exploitation of SIMD performance to redesign the neighbor list and the calculation on it so that particle data components are accessed directly and contiguously. For example, Eckhardt et al. [30] proposed this kind of SIMD-aware implementation to have a good performance on a cluster of Xeon E5-2680, and mentioned its application to Xeon Phi as their future work.

Smoothed-particle hydrodynamics (SPH) is a well-known method for HD simulation and has similar calculation of short-range particle interactions. Dokulil et al. [31] evaluated the performance of a SPH program together with their hybrid execution library to find adding one or two pre-production Xeon Phis of 1.06 TFlops to the host Xeon X5680 of 83 GFlops gave them 2.1- and 3.4-fold speed-up respectively though the peak performance boost factors are 13.8- and 26.6-fold respectively. The other example of HD and MD is found in the work of Liu and Chow [32], in which they presented an optimized implementation of particle-mesh Ewald algorithm with cell coloring for Xeon Phi to have up to 3.8-fold speed-up by its acceleration over dual Xeon X5680.

Finally, it is worth to show a deep SIMD-aware optimization presented by Chhuganiy et al. [33]. They SIMD-vectorized a particle scanning loop for two-point correlations among cosmological matters to have a histogram of the distance of particle pairs by using AVX intrinsic functions to obtain 7.7-fold speedup exploiting 8-way single-precision SIMD mechanism of Xeon E5-2670. This work proves the importance of SIMD-aware optimization as well as of better support from architecture, compilers and/or libraries to make us free from assembly-like coding with intrinsic functions.

@&#CONCLUSIONS@&#

This paper discussed a new challenging issue in HPC programming brought by the emergence of scalar-type manycore processors like Xeon Phi. The remarkable features of Xeon Phi namely 
                        
                           
                              
                                 10
                              
                              
                                 2
                              
                           
                        
                     -scale multithreading and 512-bit SIMD mechanism give us a cost- and power-efficient solution for TFlops-class computation, but at the same time change the traditional story of the programming toughness in the three tiers, inter-node, intra-node and intra-core to reverse their order from tough-easy-free to tough-tougher-toughest. Our experimental implementation of a PIC plasma simulation code exemplified that this story change requires us a hard work, but at the same time that the work will be well rewarded achieving performance improvement of one order of magnitude.

The implementation of the PIC simulation code on Xeon Phi is still ongoing because there are rooms of improvement such as SIMD- and locality-aware bucket sorting and locality-aware splitting of two-phased current-scatter. It is also necessary to combine the intra-node implementation with OhHelp-ed inter-node parallelization though that should only require a coding effort to take care relatively few particles crossing subdomain boundaries as mentioned in Section 4.1.

More importantly, it is essentially required to make the drastic change of the code structure shown in Section 4.3 less drastic by expressing the baseline code in an implementation-free style and transforming it to platform-dependent optimized code. This is the ultimate goal of our research on our domain-specific language framework to integrate deep knowledge about the scientific theories, formulation and modeling, application program behavior, its compilation, and its execution on a micro-architecture, into an efficient simulation code.

@&#ACKNOWLEDGMENTS@&#

This work was partly supported by JSPS Grant-in-Aid for Scientific Research (KAKENHI) #20280044.

@&#REFERENCES@&#

