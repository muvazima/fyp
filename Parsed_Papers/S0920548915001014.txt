@&#MAIN-TITLE@&#Materialising a new architecture for a distributed MCU in the Cloud

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We propose a distributed architecture for video conferencing servers (MCUs).


                        
                        
                           
                           The MCU is divided into simple parts that broadcast streams, OneToManys.


                        
                        
                           
                           We describe and evaluate the new control architecture that allows distributed deployment.


                        
                        
                           
                           This solution provides improved scalability and deployment granularity.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Cloud Computing

Video conferencing

Scalability

WebRTC

MCU

@&#ABSTRACT@&#


               
               
                  New technologies are making videoconferencing more ubiquitous than ever. This imposes a big challenge for scaling software MCUs, the traditional videoconferencing servers. We propose, implement and test an architecture for a distributed MCU designed to be deployed in a Cloud Computing environment. The main design idea is to break monolithic MCUs into more simple parts: broadcasters. These broadcasters can be deployed independently on the fly. This achieves a higher deployment granularity and flexibility. We describe the control architecture that allows this distribution and prove the viability of the system with a fully developed implementation.
               
            

@&#INTRODUCTION@&#

Traditionally, video conferencing communications have been implemented in native applications or dedicated hardware devices. However, in the last few years and thanks to technologies such as Adobe Flash
                        1
                     
                     
                        1
                        
                           http://get.adobe.com/es/flashplayer/.
                      and HTML5
                        2
                     
                     
                        2
                        
                           http://dev.w3.org/html5/spec/.
                      with its real-time communications standard WebRTC [1], these applications are moving towards the Web. That implies that any user with a PC, a smart phone or a tablet, a compatible Web browser and an Internet connection can use a videoconferencing service in order to communicate with others in real-time.

This improved accessibility and ease of use encourages the presence of video conferencing systems in more Internet applications and services. Social networks, television applications and on-line video game platforms are rapidly adopting real-time communications. This trend also applies to business environments, where on-line meetings, long-distance interviews, call centres and conference streaming are increasingly adopted.

This evolution towards the Web imposes new flexibility and scalability requirements to video conferencing infrastructures. Dedicated servers with fixed connections limits are becoming obsolete in a fast-paced Web environment where new popular sites are experiencing exponential growth in the number of concurrent users. At the same time, the surge in Cloud Computing systems has allowed developers to improve scalability on traditional services. Having more computing power on demand on a pay-per-use basis has set in motion a new ecosystem of new Web applications that react to the increasing traffic.

The convergence of these developments has given rise to new types of systems that offer video conferencing as a Cloud service. By taking advantage of Cloud technologies, they can adapt to large variations in the number of users. This is particularly important in video conferencing between multiple participants. Here, communication usually occurs in virtual rooms, conceptual places on the Internet where users can communicate with others.

Software multipoint control units (MCUs) are at the core of the server side of scalable video conferencing systems. MCUs have been used for years in these systems to address the signalling and real-time transport of user video and audio content. However, they can also support advanced functionality such as recording, video and audio transcoding, video composition and audio mixing. Nowadays, MCUs commonly serve many virtual rooms with multiple users in each of them. Hence, Cloud video conferencing services focus on solving the problem of adapting the number of MCUs to a varying number of users.

In this paper we propose an architecture to achieve this scalability with very high flexibility and granularity. Scaling dynamically is a challenge in a traditional MCU. The machine in which the MCU is deployed imposes a limit that cannot be raised without interrupting the running sessions. Furthermore, the highest granularity that one can achieve is that of a video conferencing session: a session being the atomic unit of deployment in an MCU system.

In conclusion, a video conferencing room has to be managed by a single MCU and it can never be split into different MCUs. This involves a very significant limitation in the deployment and the scalability of this type of system, a factor that can be improved in terms of efficiency in the management of the resources and costs.

The solution we propose focuses on the most illustrative example, which is the scenario in which the MCU forwards each video and audio stream from every user to the rest of participants in the same room. The main task in this case is to copy every packet and send it to all participants. We have also defined the component that is responsible for carrying out this task within the MCU: the OneToMany processor. Our solution divides an MCU into multiple OneToMany components, one per audio and video stream that needs to be sent to the users. Moreover, we define the details we need to take into account for different layers of communication as well as general considerations to manage all the components in this system.

This work is built on top of the ideas proposed in [2] the distributed OneToManys (dOTMs), an atomic software component that receives a media stream published by a user and multiplexes it to its subscribers. In a video conference room with multiple participants there will be a OneToMany (OTM) for each user publishing a stream. The OTMs are managed by a control layer and each one is totally independent of the others. Thus, OTMs that belong to users connected to the same virtual room can be distributed between different machines. An immediate advantage of this architecture is that when you need more resources to attend to the demand in a videoconferencing session, you can add a new machine on-the-fly.

In the next section we extend the motivations introduced here and we analyse the requirements for achieving the goals of this research. In Section 3 we summarise the current work related to the problem. Then in Section 4 we present our solution by describing the architecture and its execution flow in a videoconferencing scenario. In Section 5 we show the implications of this architecture and, using a real open-source implementation of dOTMs (Licode
                        3
                     
                     
                        3
                        
                           https://github.com/ging/licode.
                     ), we demonstrate that the new model does not negatively affect the performance of the communications. Finally, in Section 6 we show the main conclusions of this work and we introduce some lines of research for future work related to it.

In this section we enunciate the problem this work aims to solve. In order to do that, we describe the scenario where the problem takes place, define the terms we will use throughout the paper and elaborate the problem description by providing an example on a real software MCU.

Video conferencing with multiple participants has traditionally been provided by MCUs, as opposed to peer-to-peer solutions. The lack of availability of IP Multicast [3] for end-users impairs the use of p2p for multi-party real time communications as the number of participants dictates the upload bandwidth needed. In such cases, an MCU-based application level multicast is used. The centralised architecture means that there is a bottleneck on the server side but, on the other hand, clients save uploaded bandwidth as they do not have to replicate the streams they are publishing for each participant.

A few years ago, videoconferencing systems were limited to native and desktop applications. But current new technologies also provide video conferencing on the Web browser. As Web applications are more easily accessible and impose fewer requirements in the devices, this enables a new and more dynamic way of communicating. However, it also imposes a great challenge for MCU deployments. The traditional dedicated and tightly scheduled infrastructures are not tailored to meet the fast changes in demand associated with Web applications. Cloud Computing, and its ability to provide resources on demand, has been widely used as a solution for scalability problems. Specifically when talking about video conferencing, the illusion of infinite computational power and bandwidth that the Cloud provides is very attractive for deploying resource demanding services. However, designing and deploying software in the Cloud is a challenge in itself.

Several works have tried to take advantage of the capabilities the Cloud provides applied to video conferencing. In [4] the authors provide a general overview on video conferencing services in the Cloud. By designing the conference as service oriented architecture, the final consumer application is separated from the video conference provider. Conferencing services are offered on request in a similar way that infrastructure is offered in the Cloud. The authors in [5] present an implementation that adheres to some of these principles. While the general idea on how to adapt video conferencing systems to the Cloud is clear in these works, it is not specified how to react to the variations on demand or how to provide the resources for the video conferencing capabilities dynamically.

On the other hand, since the term Cloud Computing was coined and its characteristics set out, there has been an abundance of studies on how to take different kinds of applications to the Cloud, taking advantage of its strengths and highlighting the risks. According to [6] the ability to scale quickly and efficiently is one of the top ten obstacles (and opportunities) for the growth of Cloud Computing and the applications that rely on it. Furthermore, the authors in [7] argue that cost and efficiency is one of the risks businesses have to take into account when transitioning to Cloud services. The authors in [8] emphasise the importance of taking into account the different requirements of the applications in terms of quality of experience (QoE) when adapting their Cloud deployment.

To sum up, Cloud Computing can be a very powerful tool for creating scalable video conferencing services. However, to maximise its value (economic and otherwise), the Cloud deployment has to be carefully designed, implemented and adapted to the service in order to provide a successful experience with minimal cost.

This paper proposes a way to improve the efficiency when it comes to deploying and providing resources for multiple participant video conferencing in Cloud systems.

To illustrate this, we will focus on the scenario of a multi-party video conference where an arbitrary number of users sends and receives real time audio and video streams. A use case of this scenario is, for instance, a business meeting in which all participants share their audio and video. These participants will send all the traffic through an MCU in order to save upload bandwidth as explained before. In the following subsections we will provide the definitions that will be used throughout the paper and will further explain the scenario of the problem.

The following terms will be used throughout this paper. These are illustrated in Fig. 1
                        :
                           
                              •
                              
                                 Participant: A device that takes part in a video conference. It can receive and send multimedia streams. Generally, it will represent the final users of the system.


                                 Room: A conceptual environment participants join to make their multimedia streams available and receive data from others. In general, we will say participants publish and subscribe to streams. The scope where these publications are visible is the room.


                                 MCU: A software device that hosts one or more rooms. It physically receives the media streams from the participants and is able to forward these streams to the others.

While there are many ways of designing and implementing an MCU, such as those detailed by the authors in [9] or, more recently, in [10], in this paper we will focus on a pure forwarding MCU. That is, the MCU receives packets from each participant and forwards them to the rest, without performing any altering operations in the streams themselves such as composing or mixing. Only the essential operations for transmitting the media will be carried out.

Furthermore, in the scope of this paper we will assume all the participants in a room subscribe to all the available media streams. This will maximise the amount of information (connections and bandwidth) the MCU has to process.

In the aforementioned scenario, the MCU acts at three different levels:
                           
                              •
                              
                                 Signalling: The MCU implements the required protocols that enable the negotiation needed to establish the direct communication with the participants.


                                 Media: The MCU is able to send and receive media streams using the video and audio codecs and protocols understood by the participants. These protocols are negotiated during the signalling phase.


                                 Control: The room concept is implemented in the MCU, allowing participants to publish and subscribe to media streams within that scope. This implies keeping a list of the participants in each room and being able to notify them when this list changes.

In this scenario, the MCU is the minimum complete piece of software that can be deployed and run on a server-side node. Thus, it acts as the deployment unit when taken to a data centre or a Cloud provider. That is, we will get more video conferencing capacity by adding more MCU nodes to the system.

The MCU is a process which is run on every server that we want to add to the server pool. As such, the number of participants that can be handled by an MCU is limited by the hardware it is installed on and the bandwidth available to it. We can deduce how being limited by the MCU as a unit will be a problem. For each participant in a room size N, the MCU receives 1 stream and forwards it (N
                        −1) times. In total, there are (1+(N
                        −1))∗
                        N
                        =
                        N
                        2 connections to the MCU at the same time. When the next participant comes, it publishes its stream so it is received by all the others (1+
                        N), and it subscribes to the streams available (N connections) so we have 2N
                        +1 new connections or (N
                        +1)2. That is, the connections grow quadratically. The amount of processing power, memory and bandwidth needed is directly proportional to the number of active connections that an MCU is maintaining. Thus, the number of participants that an MCU can handle is much lower when they are arranged in big rooms.

To show the implications of the number of connections on the overall load in the system, we have carried out tests on our open source WebRTC MCU, Licode. This experiment was carried out in a small instance in Amazon EC2,
                           4
                        
                        
                           4
                           
                              http://aws.amazon.com/ec2/.
                         roughly equivalent to 1GHz Intel Xeon family processor.

We have reproduced the aforementioned scenario in a conference room where all participants subscribe to all the streams. The MCU optimises the quality of the streams by maximising the bandwidth used.

The participants provided 3Mbps of traffic combining audio and video streams, for an ultra-high quality video conference. The result can be seen in Fig. 2
                        
                     

As we can see, the increase in CPU is very significant for each participant that joins the session. While the MCU is only copying and forwarding packets, the high CPU load is explained by the use of SRTP [11]. Each packet has to be decrypted and encrypted again to be transmitted to each of the participants as every connection has its own encryption keys. This, combined with the very high bit-rate of the media, is very CPU intensive.

These high CPU and bandwidth requirements for video conferencing applications make resource provisioning a key part in a successful Cloud deployment. One of the most popular approaches is to limit the number of people per room. In this case, we can estimate the resources needed for each room and plan the number of instances needed accordingly. This, however, is not optimal. If for any reason we need a room bigger than what was previously planned, we will be limited by the amount of connections the physical CPU and bandwidth can sustain, making this change impossible in some cases. On the other hand, provisioning hardware for rooms than are never fully filled can be a very significant amount of reserved but unused resources.

@&#RELATED WORK@&#

In this section, we analyse how existing research approaches the resolution of the aforementioned problem. As we will see it does not fully accomplish all the requirements exposed. So we have to present our own solution in the next sections.

The gradual transition to mobile and Web applications and the corresponding growth in demand of multimedia communications have motivated many commercial solutions to move away from pure hardware solutions to those of software. Software solutions can then be virtualised to be deployed on demand in controlled scenarios.

Approaches such as that explained in [12] are used to provide video conferencing as a service, effectively separating the management of rooms and users from the actual media layer.

Solutions such as Vidyo [13] or Pexip [14] provide fully featured distributed architectures that can be virtualised to run on a variety of infrastructures. More media processing capacity can be added on demand by deploying more of these virtualised solutions, new resources are identified and used on-the-fly. In order to do this, they are able to distribute conferences (Rooms) between several of their MCUs (or equivalent entities). There are many differences in terms of how they work but both use different flavours of cascading.

In a cascading configuration, MCUs redirect media streams (with or without processing) to other MCUs. In this scheme, rooms can be effectively distributed by forwarding streams between MCUs in a coordinated fashion. Participants can then be assigned to an MCU according to parameters such as CPU load, distance, and bandwidth. In [15], the authors present a video conferencing system using a protocol that takes advantage of intra-Cloud networks to forward traffic among peers. Participants send their media streams to the closest media node in a Cloud environment, these media nodes aggregate the streams and send them to other media nodes, closer to other participants in the conference. This is an example of cascading where media mixing is used to provide a media stream that will be forwarded among MCUs. In the solution we present in this work, media mixing and transcoding are optional; it can be avoided to save resources on the server side and optimise the use of the Cloud resources. Furthermore, while cascading has been proven to be a good solution for many scenarios, it entails an added delay related to the retransmissions of the packets to all of the MCUs. Delay is especially critical in multi-conferencing as opposed to broadcasting, where it can usually be better tolerated. When talking about multi-conferencing, as we can see in [2], the number of jumps has a limit in terms of user experience.

In the literature we can also find other ways of distributing the server load of a multiple participant video conference. The authors of [16] use Cloud deployed surrogates to assist in mobile video conferencing by unloading some of the more CPU intensive tasks such as transcoding, being able to exchange media streams among them. It could be argued that the surrogates act, in practise, as a distributed MCU, however, the design is highly optimised for mobile platforms.

An alternative solution to adapt to user demand in the MCUs is to move the media streams to another server when the needed resources vary. This can be achieved by implementing media stream mobility mechanisms like those explained by the authors in [17] and [18] where SIP modifications are used to move on-going SIP calls to different devices and even split the call into different components (video and audio). In a centralised scenario that would imply referring all the participants in a running room to a different MCU that could handle more load. With a similar idea in mind but using a relay, MICE protocol [19] proposes an extension of Traversal Using Relays around NAT (TURN) [20] in order to perform mobility. However, as explained in [21] all these mechanisms may produce small interruptions in the communication. In particular, they are targeted at communication devices whose network conditions are changing (a change of IP address, for instance), in these cases, the interruption in the communication is often unavoidable.

In the scenario we are studying, this would mean moving all the participants of a room that are in a saturated MCU to a new one that has enough capacity left to host the expanding room. We argue that, while moving streams to other servers is a very powerful tool, there is no reliable way of avoiding a degradation in the experience.

Taking all of this research into account, even though they successfully solve our scenario, they do not fully cover all the requirements.

In this section, we propose a mechanism that allows a single room to be distributed into several MCUs, giving us a more granular control of scalability in the system. We will do this by separating the media layer of the MCU from the control layer and dividing the media processing into more simple units. This distributed MCU can be used to improve efficiency in Cloud deployment.

The initial ideas behind the final architecture are detailed in [2]. The main function of a non-mixing MCU is to forward all the media packets to the participants of a session. This can be seen as an array of broadcasters, each of which forwards the media from one participant to the others. Fig. 3
                         shows how a session with three participants in a room can be explained as three different broadcast sessions. Our proposal is to separate these broadcasters into different coordinated processes that can be deployed in different physical CPUs.

In our architecture we will define two communication paths between the participants and the distributed MCU: the media path and the control path. We do not propose a dedicated signalling path and instead we use the control path to exchange signalling messages.


                        Fig. 4
                         shows the components present in the distributed architecture, each will play a role in one or more of these layers. The control paths are also indicated. We will explain each of the components and their relationships:
                           
                              •
                              
                                 OTM: The atomic media unit of the distributed MCU, which is in charge of receiving media from only one participant and forwarding it to others. In order to do so, it is able to exchange signalling messages and implements a full media protocol stack compatible with the participants. When started, an OTM gets a unique id. It receives control and signalling messages via a message bus and communicates directly with the participants in the media layer.


                                 Agent: The component in charge of starting and stopping OTM processes. One Agent has to be started in each of the machines in which we want to host OTMs. Agents are automatically discovered via a broadcasted message to the message bus.


                                 Controller: The implementation of the general Control layer, which is in charge of creating and destroying rooms as requested. Being the entry point and the control interface for the participants, it assigns participants to the appropriate room and keeps track of all the available rooms and connected users. The control of each room is delegated to the Room Controllers, a programmatic part of the Controller. The Controller also plays an important role in the signalling process. It acts as a signalling proxy. In order to maintain only one connection for control and signalling, the Controller is in charge of forwarding the signalling messages from the participants to the OTMs and back.


                                 Room Controller: It is a part of the Controller, implemented to handle the control of a single room. The Room Controller will communicate with the Agents to request new OTMs for new participants as well as acting as a proxy for signalling messages to specific OTMs.


                        Table 1
                         lists the messages that define the communication between Room Controllers and Agents. The Controller will decide which Agent will be responsible for providing a new OTM according to metrics which is out of the scope of this paper (CPU usage, geographical location, etc.). Once that decision is made, the messages are used to create and destroy OTMs remotely on demand.

The signalling and control messages that constitute the communication between the Room Controller and the OTM are listed in Table 2
                        . Control messages that add participants (subscribers or publishers) include establishment information encoded in the Session Description Protocol (SDP) [22] message, the answer to the SDP negotiation is included in the response to the request.

The Room Controller can also trigger an SDP renegotiation by sending the processSignalling message, the OTM then generates a Response SDP adapted to the new conditions. Most of these renegotiations will come from changes in the client side (video sizes, connectivity, etc.).

The combination of the messages in Tables 1 and 2 enables the distribution of the architecture in terms of control and signalling.


                        Fig. 5
                         shows the media layer of the distributed MCU. In accordance with the aforementioned explanation, the Controller, Room Controllers and Agents are not present in this layer. After the signalling and media negotiation is over, the OTMs and participants communicate directly during the course of the session. More importantly, this figure shows how the distributed MCU works in this layer: participants A, B and C are in the same room (session). Participant A sends its media stream to the OTM in Machine 1 that the Room Controller has assigned to it in a previous negotiation. Concurrently, Participant B is publishing on Machine 2 that, potentially, could be deployed anywhere as long as it is able to communicate via the message bus. Participant A and Participant B receive flows from each other by subscribing to their respective OTMs. Participant C is receiving media from both Participants A and B. It is important to highlight that Participant C will perceive a minimum difference in delay from A to B if the OTMs are deployed in machines that present a similar network delay (for instance, in the same public Cloud region).

In Fig. 6
                         we can see the message flow for the establishment of a conference. In this scenario there are two participants (Participant A and Participant B) that are going to connect to a video conferencing room in order to share their media streams (video and audio). In the figure the messages exchanged directly between the participants and the Controller are represented using a dotted line and the messages exchanged through the message bus are represented using a dash-dotted line.

First, both participants need to connect to the room by sending a connection message to the Controller with the room identifier (r-id). In order to make the room access secure they have to use an authorisation mechanism that is out of the scope of this paper. When the Controller receives the connection request from Participant A the room to which he will be connecting does not yet exist, so it creates a Room Controller for that room and identifies it with the id (r-id). The Controller also registers Participant A as a new participant in the room. Then, the Controller receives the connection request from Participant B and as the Room Controller already exists it only registers Participant B as a new participant in room r-id.

Once both participants are connected to the room, Participant A wants to publish its stream in the room so it sends a Publish message to the Controller including the room in which he is going to publish and its SDP offer as parameters. When the Controller receives a publish request it has to ask an Agent to create a new OTM. As there may be more than one Agent available, the first step is to choose one of them. There are multiple options and a very interesting line of research here. For instance, the decision can be based on each machine load level or on geographic criteria. Moreover, in more complex scenarios in which the Agents are deployed in public Cloud provider's virtual machines, the decision could take into account advanced rules such as the billing conditions. However, that decision is not relevant in understanding the implementation flow so we will assume that the message bus works in round-robin mode and dispatches the creation of OTMs to an Agent each time.

Back to the example, the Controller asks Agent 1 to create a new OTM. Agent creates it and returns an identifier (o-id). Then, the Controller stores that id as the OTM (OTM 1) related to the stream that is being published and sends a publish request to OTM 1 including the SDP offer of Participant A. OTM 1 carries out the necessary actions to create its SDP answer and returns it to the Controller. The Controller then sends the SDP answer to Participant A and when it is processed it responds with an SDP OK. At this point, the stream is published in the room and the Controller notifies all the participants connected to the room.

Participant B is notified of the publication of the stream identified by s-id and is going to subscribe to it. In order to do so it sends a Subscribe message to the Controller including the stream id and its SDP offer. The Controller searches the OTM that hosts that stream and sends the Subscribe message with the SDP offer. OTM 1 processes the request and responds with an SDP answer that is sent to Participant B by the Controller. Participant B processes the answer by returning an SDP OK if the processing is successful.

At this point Participant A is sending its media data to OTM 1 and OTM 1 is redirecting it to Participant B. The same process is done for the publication of Participant B's stream. When the Controller receives the Publish message it will ask an agent to create a new OTM, now OTM 2. So Participant B will send its media stream to OTM 2 and OTM 2 redirects it to Participant A. Any other participant who connects to the same room and wants to subscribe to a stream will send its SDP to the OTM that corresponds to the publisher of the stream via the Controller.

@&#RESULTS AND DISCUSSION@&#

In this section we show that the proposed distributed architecture provides a similar raw performance as a centralised MCU, while adding the benefits we have discussed throughout this work. In order to do so we analyse the implications of our design, we deploy a real implementation and we analyse its performance in a Cloud infrastructure.

The architecture presented in the previous section achieves the granularity and the scalability advantages pursued. The basic unit of communication now is the OTM and we can distribute OTMs of the same room between different computers. We have removed the limitation on the number of users. We are also improving the use of resources and making the scalability more flexible. Also, we are making the system more robust and stable because now each OTM runs in an independent process. Therefore, if there is a problem with an OTM process, the others will not be affected and will continue operating normally.

To make this behaviour possible, we have modified the traditional concept of an MCU. The components are not centralised in the same device and that complicates the model. A mechanism for pointing the clients to the right OTM has been designed. Thus, a new control layer that saves the registry of all the OTMs, the correspondence streams-rooms and a way of communicating all of them is required. With the proposed architecture we have reached this goal by adding the Controller component, directly connected to the users and the bus message in order to communicate it with the OTMs.

However, adding new components and connections to the system can impact the performance and functionality of the solution. We need to prove that the proposed architecture does not affect the performance of the communications. On the one hand, the total CPU consumption of the distributed OTMs should be equivalent to the consumption of the centralised MCU. On the other hand, the connection latency for the users should be increased as little as possible by the control layer and the message bus. In order to check both considerations we have implemented an MCU with support for distributed OTMs and we have made an exhaustive analysis of its performance.

@&#IMPLEMENTATION@&#

To test the validity of the claims listed above we have used an open source project named Licode, developed by the authors of this paper. The Licode project provides an MCU that is totally compatible with the WebRTC standard. WebRTC provides Web browsers Real-Time Communications (RTC) capabilities via simple JavaScript APIs. Our MCU builds on top of the standard and we can also carry out an advanced videoconferencing task such as transcoding, recording or broadcasting to multiple users. In order to be compatible with the standard, the MCU implements a signalling protocol based on SDP exchange. For the establishment of the media connection it implements the Interactive Connectivity Establishment (ICE) [23] standard and for the encryption of all the media data it implements Secure Real-time Transport Protocol (SRTP) [11] and Datagram Transport Layer Security (DTLS) [24] protocols.

Licode also provides a JavaScript client API that wraps the WebRTC API facilitating the development of the applications and adding the necessary modules to communicate with the MCU. To ensure the security in the signalling between clients and MCU, Licode includes an authorisation mechanism based on Nuve [12]. Nuve is also in charge of the management of rooms and it is able to start entire MCUs in different machines in order to scale the system.

The last Licode version implements the model presented in this paper as it is able to divide an MCU into OTMs and distribute them between processes and machines. Coming back to Fig. 4, in our implementation clients run in a Web browser application using the Licode's client API. The communication between them and the Controller is made using the WebSocket [25] protocol in order to have a direct channel to exchange signalling messages and room events. We have included the socket.io
                           5
                        
                        
                           5
                           
                              http://socket.io/.
                         implementation of the protocol in the client API. On the other hand, we use the Advanced Message Queueing Protocol (AMQP) [26] protocol for the message bus. Specifically the RabbitMQ
                           6
                        
                        
                           6
                           
                              http://www.rabbitmq.com/.
                         implementation is also an open source project available as a service in a number of public Cloud providers.

We have carried out experiments to measure the impact of dOTM and its limits in a normal videoconferencing application.

We have measured our implementation of the architecture in two different scenarios. To set up both we used Amazon EC2
                           7
                        
                        
                           7
                           
                              http://aws.amazon.com/ec2.
                         to host the Licode servers. In these scenarios, we will define “Machine” as an Amazon EC2 m1.micro instance. At the time of writing this paper, the characteristics of an m1.micro instance are:
                           
                              •
                              
                                 vCPU: 1


                                 Mem (GiB) 0.613

Burst capabilities for short CPU capacity increases.

The operative system used is Ubuntu
                           8
                        
                        
                           8
                           
                              http://www.ubuntu.com.
                         12.04 LTS.

The small size and computing power is not a problem in the scope of the scenarios we designed. It is very convenient to have a low enough processing power that we can saturate easily to test the ability to allocate OTM in other machines. The main concern for our purposes is the amount of bandwidth available for this type of instance. We will show in the tests that the throughput available was enough to sustain the amount of participants the CPU was able to handle. The clients were hosted in Core 2 Duo E8400 CPUs with at least 2GiB RAM running Google Chrome
                           9
                        
                        
                           9
                           
                              http://www.google.com/chrome/browser/.
                         version 39.

The objective of this experiment is to prove that a conference can be distributed between two or more machines. We want to prove the efficiency of the distribution and whether there is any overhead in terms of CPU when running a conference in a single process in a single machine (traditional MCU) versus in several processes distributed in two computers (distributed OTMs).

This scenario consists of a growing videoconferencing session deployed on these two different configurations. Clients join the session every 20s and they are fully interconnected, meaning all clients receive media from all the others. The clients are configured to emit only video at a maximum rate of 500Kbps.

We have run two different configurations running five times each, the results are shown on average over those five runs. The two configurations are illustrated in Fig. 7
                           :
                              
                                 
                                    Configuration 1 forces Licode to work as a non-distributed MCU. Control, signalling and media run on the same process and in the same machine. The dotted line in Fig. 7(a) represents components running within the same Unix process. In this case OTMs are logical parts within the MCU instead of separated processes.

In Configuration 2 Licode is running in a distributed MCU configuration across two machines. As we see in Fig. 7(b) the Controller is physically hosted by Machine 1, we will also measure the influence of this component in terms of CPU.

We used collectl
                              10
                           
                           
                              10
                              
                                 http://collectl.sourceforge.net.
                            to measure the total CPU load of the system, the inbound and outbound throughput and the memory use. The tool samples these parameters every second.

The results of the CPU measurements are shown in Fig. 8
                           . These are the most relevant results as memory and bandwidth proved not to be the bottleneck in the non-distributed configuration. Fig. 8(b) shows that the average out throughput in the traditional MCU configuration does not saturate until the CPU reaches its limit. We can infer that the bandwidth is not the bottleneck in this case.

As we can see, Fig. 8 (a) shows the progressive increase in CPU use. The quick variations respond to sudden changes in the videos provided by the clients. The CPU peaks at 100% at second 200, when the tenth client joins. In this non-distributed scenario, there is no way for the session to grow and accommodate a new participant. If we forced another participant to join, the saturation of the CPU in this micro instance would cause packet loss and decrease the quality of the whole communication.

In Fig. 8(c) and (d) we can see the CPU performance evolution for Machine 1 and Machine 2 in Configuration 2. In this configuration, a new OTM process is created for each new client that joins the session. The machine in which this OTM will be hosted is assigned by round-robin so both CPUs are assigned roughly the same workload when it comes to the media layer. We can see how both Machine 1 and 2 increase their workload at a similar rate. When the tenth client joins, both Machines consume around 50% CPU, allowing for plenty of room for new users to join, in terms of CPU. Additionally, Machine 1 is hosting a Controller, however, as shown in both 8 (c) and (d) this does not affect the overall CPU load of the system as the difference in CPU load for both systems is minimal and can be attributed to the variations in the media flows.

This experiment shows that the architecture allows for more participants in every room by distributing them on different virtual machines. Moreover, considering the aggregate of every OTM, the CPU percentages are roughly equivalent to those in the centralised scenario.

In this section we address the concern expressed at the beginning of this section regarding connection delay. We want to measure the influence the distribution of the OTMs has on the delay a client experiences when joining a videoconferencing session. As explained in Section 4, we propose the use of a message bus to exchange information between OTMs and the Controller. In Fig. 6 we simplified by not displaying the propagation and processing delay of the messages, however, we did not assume that it was irrelevant.

In order to test this, we will use the configurations used in the previous experiment, displayed in Fig. 7. In this case, however, the scenario will consist of four participants joining the session one after the other. We will measure the time between the Publish message and the SDP Answer as each participant joins, and we will call this time Publish Time. Also, for each subscription in the session, we will measure the time between the New stream message and the SDP Answer and call it Subscribe Time. We repeated the experiment ten times and calculated the average of these times for all the participants in each configuration.


                           Fig. 9
                            pictures the results of this experiment, where Machine 1 and Machine 2 are the same from the distributed scenario Fig. 7(b). Machine 1 has a Controller locally while Machine 2 receives control and signalling messages via the message bus. The Centralised column is the same configuration as Fig. 7(a). As expected, the longest delays take place in the distributed scenario where the controller is not present in the same computer (Machine 2). Meanwhile, the centralised configuration and Machine 1 demonstrate a similar performance, with Machine 1 being slightly faster. This performance from Machine 1 can be attributed to the greater isolation between the components in the implementation of the architecture, there are less blocking operations.

@&#CONCLUSIONS AND FUTURE WORK@&#

This paper presents an architecture designed to improve the scalability of videoconferencing systems deployed in public or private Clouds. To avoid the coarse granularity of a centralised MCU, our proposal divides it into more simple components that can be deployed in a distributed way. This distributed component can be deployed on the fly. This allows for a more efficient use of resources as the allocation of more processing power is more dynamic. However, this adds a noticeable amount of new management messages and components that could potentially affect the performance of the communications, such us the connection establishment time.

To test the performance of the proposal in a real distributed scenario we have refurbished our own open source WebRTC MCU and adapted it to the new topology. We have measured its performance in two different experiments to show that distributed sessions improve the scalability over non-distributed MCUs and that the additional control and management do not significantly delay the establishment of the communication.

This research opens up several lines for future work. First of all, the strategies to deploy these new atomic units that make up an MCU are out of the scope of this paper. Better deployment schemes could improve the performance of production systems by taking into account parameters such as the geographical distribution of the clients and the price of the different types of instances in public Cloud platforms.

An aspect that also improves scalability in terms of performance and costs is the dynamic mobility of the OTMs between computers. If we have several servers that are not using all their resources, we can move the OTMs by merging them into one of the servers and shutting down the others. Finding a way of achieving the mobility in a transparent way to the users is also a new line of work.

Another very interesting field is the ability to form distribution trees with OTMs that could even further improve the scalability of the system, which also has very interesting applications in real-time streaming of events.

@&#REFERENCES@&#

