@&#MAIN-TITLE@&#Efficient data selection for speech recognition based on prior confidence estimation using speech and monophone models

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           We propose a method to select highly accurate data for speech recognition.


                        
                        
                           
                           We rapidly estimate prior confidence before speech recognition.


                        
                        
                           
                           Our prior estimation uses the acoustic likelihood of speech and monophone models.


                        
                        
                           
                           The proposed technique is over fifty times faster than the conventional method.


                        
                        
                           
                           Our proposal provides equivalent data selection performance.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Speech recognition

Spoken document retrieval

Data selection

Context independent model

Gaussian mixture model

@&#ABSTRACT@&#


               
               
                  This paper proposes an efficient speech data selection technique that can identify those data that will be well recognized. Conventional confidence measure techniques can also identify well-recognized speech data. However, those techniques require a lot of computation time for speech recognition processing to estimate confidence scores. Speech data with low confidence should not go through the time-consuming recognition process since they will yield erroneous spoken documents that will eventually be rejected. The proposed technique can select the speech data that will be acceptable for speech recognition applications. It rapidly selects speech data with high prior confidence based on acoustic likelihood values and using only speech and monophone models. Experiments show that the proposed confidence estimation technique is over 50 times faster than the conventional posterior confidence measure while providing equivalent data selection performance for speech recognition and spoken document retrieval.
               
            

@&#INTRODUCTION@&#

Massive quantities of videos and dialogs are stored every day; typical examples are video sharing services on the Internet and call center services provided by companies. Speech recognition technologies can transcribe the spoken components of these items automatically thus making the items searchable via their transcripts (Albertia et al., 2009). Several studies have analyzed customer needs by employing text mining (Subramaniam et al., 2009; Garnier-Rizet et al., 2008) and extracting the reasons for the calls (Fukutomi et al., 2011) from stored conversational spoken documents. A typical call center will store several tens of thousands of calls per day, and we believe that not all calls should be transcribed for the following three reasons. (1) The computation cost involved in transcribing all calls is excessive. (2) An informative analysis can be achieved from a subset of the calls. (3) The quality of the recorded speech samples varies (Benzeghiba et al., 2007), and erroneous speech recognition (due to the poor input) will degrade the efficiency of subsequent spoken document retrieval (Sanderson and Shou, 2007) and analysis.

Several confidence measures have been proposed for identifying “accurate” speech samples (Jiang, 2005). Unfortunately, they require the computationally expensive step of speech recognition processing to obtain confidence scores, which are estimated from the recognition results; they waste considerable computer resources on samples that will eventually be rejected. Most conventional methods target word or utterance verification. A dialog (similar to spoken document) level confidence measure has been proposed (Litman et al., 1999), but it is also computationally inefficient because it requires several features including speech recognition results to estimate confidence. Several data selection methods have been proposed (Wu et al., 2007), but their target is to select training data, so they fail to reduce the computation cost significantly.

Our proposal efficiently identifies speech samples that will be well recognized with an extremely low computation cost prior to speech recognition. It can identify those samples that have high confidence levels from massive numbers of stored speech samples. Prior confidence must be estimated rapidly because speech recognition can only proceed after the estimation results have been received. The proposed estimation technique utilizes the acoustic model used for posterior speech recognition. The proposal uses only context independent (monophone) models and speech models to reduce the computation cost. For even greater efficiency, its confidence estimation step eliminates all processing other than the calculation of acoustic output likelihood from Gaussian mixture models (GMMs). The prior confidence is calculated frame by frame from the difference between the output log-likelihoods of the monophone and speech GMMs. This confidence formulation is an approximation of the state level posterior probability with the state occurrence probability. This paper evaluates the actual efficiency of our technique in speech recognition and spoken document retrieval tasks. Experiments show that the proposed technique is significantly faster than the conventional posterior confidence measure based on speech recognition, while maintaining equivalent data selection performance.

The rest of this paper is organized as follows. Related work is outlined in Section 2. The proposed technique is described in Section 3. Section 4 introduces experiments conducted to confirm the effectiveness of the proposed technique. Our conclusion is presented in Section 5.

Since there are many factors that cause variability in speech signals (Benzeghiba et al., 2007), the recognition accuracy is strongly dependent on the data. Several data selection methods have been proposed for training (Wu et al., 2007; Lin and Bilmes, 2009) and adapting (Cincarek et al., 2006) acoustic models for speech recognition. Wu et al. also selected data to be transcribed for training by using the confidence score (Wu et al., 2011); this technique is called active learning. A great number of confidence measure methods have been proposed (Jiang, 2005) and they could also be useful for selecting data during speech recognition processing, since inaccurately recognized data impacts negatively on the subsequent application. Stoyanchev et al. detected misrecognized words in spoken dialog systems (Stoyanchev et al., 2012). Seigel et al. estimated a confidence measure at the word/utterance level by using conditional random fields (CRF). Ogawa et al. also used CRF directly to estimate the recognition rate rather than the confidence score both per utterance and per lecture at the spoken document level (Ogawa et al., 2012). Asami et al. also estimated the spoken document confidence score by using contextual coherence (Asami et al., 2011). Senay et al. detected low-quality documents by using a confidence measure and semantic consistency based on the latent Dirichlet allocation (LDA) model for spoken document retrieval (Senay and Lina`res, 2012). Li et al. used semantic similarity to estimate a confidence measure for spoken term detection (Li et al., 2012). There are several confidence measure methods at a variety of levels depending on the application.

Conventional confidence measure estimations require speech recognition results; this means that a lot of computation time is required to recognize low-confidence and unuseful data, which should be rejected. Thus, we attempt to reject unuseful data at the document level to prevent harmful effects on the subsequent application prior to speech recognition. In a conventional approach, Lee et al. proposed rejecting data before speech recognition by using noise GMMs (Lee et al., 2004). However, this method could reject data at the utterance level and needs to know the noise type beforehand. Chang et al. also proposed a pre-rejection algorithm that enhances the robustness of speech recognition by using pitch correlation (Seo et al., 2003), which allows it reject seriously distorted speech signal during wireless communication. However, it fails to reject slightly distorted speech with pitch continuity. This paper proposes an efficient method for selecting useful data for speech recognition and subsequent spoken document retrieval at the document level, which consists of many utterances before speech recognition. In addition, since our main target speakers, i.e. operators (agents) in call centers, use headset-type close-talk microphones, the recorded speech has high SNR (speech to noise ratio) without distortion. In call center speech, it is more important to tackle spontaneous speech instead of noisy or distorted speech. Thus, we focus on acoustical confidence.

The most common confidence measure is based on the word posterior probability defined as follows:


                        
                           
                              (1)
                              
                                 P
                                 (
                                 
                                    
                                       
                                          
                                             W
                                             ˆ
                                          
                                       
                                    
                                 
                                 |
                                 
                                    
                                       O
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       P
                                       (
                                       
                                          
                                             
                                                
                                                   W
                                                   ˆ
                                                
                                             
                                          
                                       
                                       )
                                       P
                                       (
                                       
                                          
                                             
                                                
                                                   
                                                      O
                                                   
                                                
                                             
                                          
                                       
                                       |
                                       
                                          
                                             
                                                
                                                   W
                                                   ˆ
                                                
                                             
                                          
                                       
                                       )
                                    
                                    
                                       P
                                       (
                                       
                                          
                                             O
                                          
                                       
                                       )
                                    
                                 
                                 =
                                 
                                    
                                       P
                                       (
                                       
                                          
                                             
                                                
                                                   W
                                                   ˆ
                                                
                                             
                                          
                                       
                                       )
                                       P
                                       (
                                       
                                          
                                             O
                                          
                                       
                                       |
                                       
                                          
                                             
                                                
                                                   W
                                                   ˆ
                                                
                                             
                                          
                                       
                                       )
                                    
                                    
                                       
                                          ∑
                                          
                                             
                                                W
                                             
                                          
                                       
                                       P
                                       (
                                       
                                          
                                             W
                                          
                                       
                                       )
                                       P
                                       (
                                       
                                          
                                             O
                                          
                                       
                                       |
                                       
                                          
                                             W
                                          
                                       
                                       )
                                    
                                 
                              
                           
                        where 
                           O
                         and W are an acoustic observation feature sequence (
                           o
                        
                        1, 
                           o
                        
                        2, …, 
                           o
                        
                        
                           T
                        ) and its corresponding word sequence, respectively, P(W) is word occurrence probability as given by the language model, and “
                           
                              
                              ˆ
                           
                        ” means the word, state, or sequence with the highest likelihood. The normalization term P(
                           O
                        ) cannot be easily computed (Guo et al., 2004), so conventional schemes approximate it using the N-best list from speech recognition results as in Rueber (1997).

It requires a high computation cost to extract word sequences by using a language model that covers a large vocabulary. To avoid this cost, our strategy dispenses with the language model and instead targets the state sequence S in Hidden Markov Models (HMMs);


                        
                           
                              (2)
                              
                                 P
                                 (
                                 
                                    
                                       
                                          
                                             S
                                             ˆ
                                          
                                       
                                    
                                 
                                 |
                                 
                                    
                                       O
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       P
                                       (
                                       
                                          
                                             
                                                
                                                   S
                                                   ˆ
                                                
                                             
                                          
                                       
                                       )
                                       P
                                       (
                                       
                                          
                                             O
                                          
                                       
                                       |
                                       
                                          
                                             
                                                
                                                   S
                                                   ˆ
                                                
                                             
                                          
                                       
                                       )
                                    
                                    
                                       
                                          ∑
                                          
                                             
                                                S
                                             
                                          
                                       
                                       P
                                       (
                                       
                                          
                                             S
                                          
                                       
                                       )
                                       P
                                       (
                                       
                                          
                                             O
                                          
                                       
                                       |
                                       
                                          
                                             S
                                          
                                       
                                       )
                                    
                                 
                              
                           
                        
                     

Our proposal eliminates all processing steps other than frame-independent acoustic likelihood calculation to further reduce the computation cost; it ignores the transition probability in the same way as several speech recognition decoders (Glass, 2003), and uses only the Gaussian output probability from GMMs to estimate confidence frame by frame (i.e. frame-independent).

That is, the posterior probability 
                           P
                           (
                           
                              
                                 
                                    
                                       S
                                       ˆ
                                    
                                 
                              
                           
                           |
                           
                              
                                 O
                              
                           
                           )
                         is approximately calculated from the frame-independent state posterior probability 
                           P
                           (
                           
                              
                                 
                                    
                                       s
                                       ˆ
                                    
                                 
                              
                           
                           |
                           
                              
                                 
                                    o
                                 
                              
                              t
                           
                           )
                         with best state 
                           
                              
                                 
                                    s
                                    ˆ
                                 
                              
                           
                         against observed feature 
                           o
                        
                        
                           t
                         at frame t for data length T as shown below;


                        
                           
                              (3)
                              
                                 P
                                 (
                                 
                                    
                                       
                                          
                                             S
                                             ˆ
                                          
                                       
                                    
                                 
                                 |
                                 
                                    
                                       o
                                    
                                 
                                 )
                                 ≃
                                 
                                    ∏
                                    
                                       t
                                       =
                                       1
                                    
                                    T
                                 
                                 P
                                 (
                                 
                                    
                                       
                                          
                                             s
                                             ˆ
                                          
                                       
                                    
                                 
                                 |
                                 
                                    
                                       
                                          o
                                       
                                    
                                    t
                                 
                                 )
                              
                           
                        where the frame-independent state posterior probability 
                           P
                           (
                           
                              
                                 
                                    
                                       s
                                       ˆ
                                    
                                 
                              
                           
                           |
                           
                              
                                 
                                    o
                                 
                              
                              t
                           
                           )
                        , is calculated from the output probability b
                        
                           s
                        (
                           o
                        
                        
                           t
                        ) of state s frame by frame as follows;


                        
                           
                              (4)
                              
                                 P
                                 (
                                 
                                    
                                       
                                          
                                             s
                                             ˆ
                                          
                                       
                                    
                                 
                                 |
                                 
                                    
                                       
                                          o
                                       
                                    
                                    t
                                 
                                 )
                                 =
                                 
                                    
                                       P
                                       (
                                       
                                          
                                             s
                                             ˆ
                                          
                                       
                                       )
                                       
                                          b
                                          
                                             
                                                
                                                   s
                                                   ˆ
                                                
                                             
                                          
                                       
                                       (
                                       
                                          
                                             
                                                o
                                             
                                          
                                          t
                                       
                                       )
                                    
                                    
                                       
                                          ∑
                                          s
                                       
                                       P
                                       (
                                       s
                                       )
                                       
                                          b
                                          s
                                       
                                       (
                                       
                                          
                                             
                                                o
                                             
                                          
                                          t
                                       
                                       )
                                    
                                 
                              
                           
                        
                        
                           
                              (5)
                              
                                 
                                    b
                                    s
                                 
                                 (
                                 
                                    
                                       
                                          o
                                       
                                    
                                    t
                                 
                                 )
                                 =
                                 
                                    
                                       ∑
                                       
                                          m
                                          =
                                          1
                                       
                                       
                                          
                                             M
                                             s
                                          
                                       
                                    
                                    
                                       w
                                       
                                          s
                                          ,
                                          m
                                       
                                    
                                    
                                       N
                                       
                                          s
                                          ,
                                          m
                                       
                                    
                                    (
                                    
                                       
                                          
                                             o
                                          
                                       
                                       t
                                    
                                    |
                                    
                                       μ
                                       
                                          s
                                          ,
                                          m
                                       
                                    
                                    ,
                                    
                                       Σ
                                       
                                          s
                                          ,
                                          m
                                       
                                    
                                    )
                                 
                              
                           
                        where 
                           
                              s
                              ˆ
                           
                         is the best state in frame t. M
                        
                           s
                         is the number of distributions belonging to state s, 
                           
                              w
                              
                                 s
                                 ,
                                 m
                              
                           
                         is the mth mixture weight and N
                        
                           s,m
                        (·) is the mth Gaussian distribution function with mean vector μ
                        
                           s,m
                         and covariance matrix Σ
                        
                           s,m
                         of state s.

To increase the processing speed further, the proposed technique uses only monophones when calculating Eq. (4); 
                           
                              s
                              ˆ
                           
                         is the best state, i.e. the state with the maximum Gaussian output probability among the states of the monophone HMMs. The assumption is that triphones can be approximated by monophones, and this assumption is often used to improve the speed of speech recognition processing as in Lee et al. (2001). The monophone in our acoustic model is still trained by the acoustic features with triphone-based alignment. Accordingly, this assumption is a very reasonable approach to improving speed.

The denominator of Eq. (4), ∑
                           s
                        
                        P(s)b
                        
                           s
                        (
                           o
                        
                        
                           t
                        ) is the sum of all states’ (all phonemes’) output probabilities; it can be approximated by the speech model as follows;


                        
                           
                              (6)
                              
                                 
                                    ∑
                                    s
                                 
                                 P
                                 (
                                 s
                                 )
                                 
                                    b
                                    s
                                 
                                 (
                                 
                                    
                                       
                                          o
                                       
                                    
                                    t
                                 
                                 )
                                 ∼
                                 P
                                 (
                                 g
                                 )
                                 
                                    b
                                    g
                                 
                                 (
                                 
                                    
                                       
                                          o
                                       
                                    
                                    t
                                 
                                 )
                              
                           
                        where g is the state of the speech model (GMM) that is trained from the acoustic features of all phonemes, i.e. all states. Our speech model has only a single state, so the occurrence probability of the speech model, P(g), must be equal to 1 in speech frames. By assigning 1 to P(g) in Eq. (6), we obtain the following.


                        
                           
                              (7)
                              
                                 
                                    ∑
                                    s
                                 
                                 P
                                 (
                                 s
                                 )
                                 
                                    b
                                    s
                                 
                                 (
                                 
                                    
                                       
                                          o
                                       
                                    
                                    t
                                 
                                 )
                                 ∼
                                 
                                    b
                                    g
                                 
                                 (
                                 
                                    
                                       
                                          o
                                       
                                    
                                    t
                                 
                                 )
                              
                           
                        The second term in Eq. (7) is similar to the denominator in the second term in Eq. (1); thus this approximation is reasonable.

By substituting this expression into Eq. (4), the frame-independent posterior probability, 
                           P
                           (
                           
                              
                                 
                                    
                                       s
                                       ˆ
                                    
                                 
                              
                           
                           |
                           
                              
                                 
                                    o
                                 
                              
                              t
                           
                           )
                        , is approximately calculated as follows;


                        
                           
                              (8)
                              
                                 P
                                 (
                                 
                                    
                                       
                                          
                                             s
                                             ˆ
                                          
                                       
                                    
                                 
                                 |
                                 
                                    
                                       
                                          o
                                       
                                    
                                    t
                                 
                                 )
                                 ∼
                                 
                                    
                                       P
                                       (
                                       
                                          
                                             s
                                             ˆ
                                          
                                       
                                       )
                                       
                                          b
                                          
                                             
                                                
                                                   s
                                                   ˆ
                                                
                                             
                                          
                                       
                                       (
                                       
                                          
                                             
                                                o
                                             
                                          
                                          t
                                       
                                       )
                                    
                                    
                                       
                                          b
                                          g
                                       
                                       (
                                       
                                          
                                             
                                                o
                                             
                                          
                                          t
                                       
                                       )
                                    
                                 
                              
                           
                        The occurrence probability of state 
                           
                              s
                              ˆ
                           
                        , 
                           P
                           (
                           
                              
                                 s
                                 ˆ
                              
                           
                           )
                        , can be calculated from the appearance frequency of state s. We herein assume that there is no significant difference between the state appearance frequencies of the acoustic training speech data and the target speech data; in particular, we use only monophone states, and the difference is not significant at the monophone level. Under this assumption, 
                           P
                           (
                           
                              
                                 s
                                 ˆ
                              
                           
                           )
                         is given by the following equation by using total occupancy Γ(s), which reflects the appearance frequency of state s in the acoustic model training data.


                        
                           
                              (9)
                              
                                 P
                                 (
                                 
                                    
                                       s
                                       ˆ
                                    
                                 
                                 )
                                 ≃
                                 
                                    
                                       Γ
                                       (
                                       
                                          
                                             s
                                             ˆ
                                          
                                       
                                       )
                                    
                                    
                                       
                                          ∑
                                          s
                                       
                                       Γ
                                       (
                                       s
                                       )
                                    
                                 
                              
                           
                        
                     

The frame-independent confidence score, c(
                           o
                        
                        
                           t
                        ), is transformed in the log domain from Eq. (8) as follows.


                        
                           
                              (10)
                              
                                 c
                                 (
                                 
                                    
                                       
                                          o
                                       
                                    
                                    t
                                 
                                 )
                                 =
                                 log
                                 (
                                 P
                                 (
                                 
                                    
                                       s
                                       ˆ
                                    
                                 
                                 )
                                 
                                    b
                                    
                                       
                                          
                                             s
                                             ˆ
                                          
                                       
                                    
                                 
                                 (
                                 
                                    
                                       
                                          o
                                       
                                    
                                    t
                                 
                                 )
                                 )
                                 −
                                 log
                                 
                                    b
                                    g
                                 
                                 (
                                 
                                    
                                       
                                          o
                                       
                                    
                                    t
                                 
                                 )
                              
                           
                        If the speech model is adopted as the Universal Background Model (UBM) and we ignore the state occurrence probability 
                           P
                           (
                           
                              
                                 s
                                 ˆ
                              
                           
                           )
                        , Eq. (10) is similar to the likelihood ratio often used in speaker verification as in (Reynolds et al., 2000).

Prior confidence score C is calculated by normalizing the frame-level prior confidence score, c(
                           o
                        
                        
                           t
                        ), by data length T to allow a comparison of speech samples with different lengths as follows;


                        
                           
                              (11)
                              
                                 C
                                 =
                                 
                                    
                                       
                                          ∑
                                          
                                             t
                                             =
                                             1
                                          
                                          T
                                       
                                       c
                                       (
                                       
                                          
                                             
                                                o
                                             
                                          
                                          t
                                       
                                       )
                                    
                                    T
                                 
                              
                           
                        
                        Fig. 1
                         summarizes the above-mentioned relational expression from a conventional confidence measure based on posterior word probability to the proposed prior confidence measure. Since our proposed prior confidence can be estimated by using the acoustic likelihood from only speech and monophone models, it is significantly faster than a conventional confidence measure.


                        Fig. 2
                         shows the difference between the log-likelihoods of a monophone and speech model against clear and ambiguous speech; this is a simplified figure just for explanation as each model has only one state and one distribution. The speech model is trained from the acoustic features of all phonemes in speech frames, so the distributions in the speech model have broader variances than the distribution in the monophone model. Accordingly, the speech model provides a comparatively stable log-likelihood regardless of speech quality. If the input speech is clear and similar to the training acoustic data (the expectation is for high accuracy), the input acoustic features are located around the mean of either monophone's distributions. In this case, the log-likelihood of the best monophone is larger than that of the speech model, and the prior confidence becomes higher. In contrast, with ambiguous speech (the expectation is for low accuracy), the input features are located on the side of the distributions and the monophone's log-likelihood becomes smaller, so the prior confidence becomes small. As a result, the difference between the log-likelihoods of the best monophone and the speech model reflects the expected accuracy in posterior speech recognition.

The procedure of the proposed system is shown in Fig. 3
                        . The conventional system subjects all data to speech recognition. In contrast, the proposed system estimates confidence and selects the samples to be passed to speech recognition. In prior data selection, the speech data is ranked by the estimated prior confidence. The proposed system selects those samples that have high confidence scores and then performs speech recognition on the selected samples using triphones in the acoustic and language models. The computation cost falls since the speech recognition step is minimized, and our proposal can efficiently identify well-recognized speech samples.


                        Fig. 4
                         focuses on the prior confidence estimation process. It calculates the output probability frame by frame from GMMs of monophone HMMs and the speech model in the acoustic model for each complete speech sample. Frame-level prior confidence c(
                           o
                        
                        
                           t
                        ) is estimated from the difference between the log-likelihoods of the best states in the monophone (red filled circle in Fig. 4) and speech models as given by Eq. (10). Here, the acoustic features of speech and pause are significantly different. Thus, the proposed technique discriminates speech and pause by using the output probability of GMMs belonging to pause HMMs and the speech model, frame by frame. g is the best state (blue circle in Fig. 4) in the speech GMMs and pause HMMs in Eq. (10). Our proposed system adopts the utterance segmentation technique described in (Kobashikawa et al., 2013) before confidence estimation. Utterance segmentation uses speech GMMs and pause GMMs belonging to pause HMMs in acoustic model, and when the pause frame continues for longer than τ
                        
                           p
                        
                        au (e.g. 0.8s), the utterance is segmented as an end-point. The segments include some pause frames by using a hangover scheme (Sohn et al., 1999). Prior confidence score C is calculated by averaging frame level confidence score as given by Eq. (11). We select data to send for speech recognition by using the prior confidence score C.

@&#EXPERIMENTS@&#

In this experiment the acoustic analysis condition is as described below; sampling frequency: 16kHz, 20ms length Hamming window shifted by 10ms, and acoustic features: 25 orders (MFCC 12, ΔMFCC 12, Δpower). The evaluation task uses 240 calls (19.81h and 17,672 utterances) by 48 Japanese speakers (17 males and 31 females), and the speaking style is spontaneous speech in a two-party dialog, i.e. conversational speech. The recognition rates of the evaluation task are distributed over a wide range; 76.00% on average, 89.75% maximum and 47.98% minimum. The ratio of data with over 80% recognition rate is 30% (=72/240).

The training data of the acoustic model contains spontaneous and conversational speech in a simulated call center, and the size of training data is 119.51h (103,822 utterances) for males and 104.50h (97,209 utterances) for females. There are a total of 1958 states, 90 monophone states, and the number of phonemes is 30, the distribution number is 26,567 for males and 29,836 for females. There are 64 distributions in a single state of the speech model, 1429 for males and 1435 for females in monophones, and 26,567 for males and 29,836 for females in total. The maximum number of mixtures in each state is 16, but some states have fewer mixtures depending on the quantities of training data for each state.

The language model is a word trigram developed by using manual transcriptions of dialog speech, and its vocabulary size is 59,676 words. The size of the language model corpus is 44.69 mega words. The perplexity is 111.64, and the OOV (out of vocabulary) rate is 1.65%. There are 1500 queries for spoken document retrieval.

The speech recognition decoder is VoiceRex (Masataki et al., 2007). We used a dual-gender acoustic model and employed our proposed prior gender selection technique (Kobashikawa et al., 2013).

We start by investigating the impact of speech data selection and to this end adopt the metric of the average recognition rate of the selected speech samples. The effect of speech data selection is confirmed by comparing the following 5 conditions; “ideal”: selection in descending order of recognition rate, “average”: average recognition rate (should simulate random selection), “posterior”: selection in descending order of confidence score (estimated by using recognition results after speech recognition based on word trigrams), “mono-loop”: selection in descending order of confidence score (estimated by using phoneme recognition results with monophone loop grammar), and “prior”: selection in descending order of our proposed prior confidence score. “posterior” adopts our baseline confidence measure that uses the N-best list of recognition results as in (Rueber, 1997). The “mono-loop” confidence measure is calculated by the difference between the acoustic scores of the recognition results of monophone loop grammar and speech/pause loop grammar; it eliminates the effect of the language model used in “posterior”. This evaluation is based on character units to eliminate the influence of word length.

Furthermore, we evaluate the data selection performance in the spoken document retrieval task. We retrieve for the spoken documents (calls) by using text queries from 240 calls, the same data used in the previous speech recognition evaluation task. The queries are nouns, adjectives, and verbs. We adopt the mean average precision (MAP) for spoken document retrieval evaluation as in (Mamou et al., 2006), and the data selection performance in spoken document retrieval is evaluated by using the raw average precision (AP) of the selected speech. MAP and raw AP are defined (Hsieh et al., 2004) as follows;


                        
                           
                              (12)
                              
                                 MAP
                                 =
                                 
                                    1
                                    
                                       
                                          N
                                          q
                                       
                                    
                                 
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       
                                          N
                                          q
                                       
                                    
                                 
                                 
                                    1
                                    
                                       
                                          N
                                          i
                                       
                                    
                                 
                                 
                                    ∑
                                    
                                       k
                                       =
                                       1
                                    
                                    
                                       
                                          N
                                          i
                                       
                                    
                                 
                                 
                                    k
                                    
                                       
                                          rank
                                          ik
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (13)
                              
                                 AP
                                 =
                                 
                                    1
                                    
                                       
                                          N
                                          q
                                       
                                    
                                 
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       
                                          N
                                          q
                                       
                                    
                                 
                                 
                                    
                                       
                                          N
                                          i
                                       
                                    
                                    N
                                 
                              
                           
                        where N
                        
                           q
                         denotes the number of queries, N
                        
                           i
                         denotes the number of relevant speech samples contained in the N retrieved documents for query q
                        
                           i
                         and rank
                        
                           ik
                         denotes the rank of the kth relevant document for query q
                        
                           i
                        .

The speed of confidence estimation is the time taken to compute the confidence score of the speech. Thus, the “posterior” computation time includes speech recognition processing. Instead of generating the recognition result with maximum likelihood, the proposed technique searches for the best state in terms of monophones frame by frame, hence this computation time comparison is fair with regard to confidence estimation.

@&#RESULTS@&#

The recorded data selection performance is shown in Figs. 5 and 6
                        
                        . The horizontal axis is the speech data selection rate; it is calculated as the ratio of the number of selected speech samples to the number of all speech samples. The vertical axis in Fig. 5 is the average recognition rate of selected speech, and is the raw average precision for spoken document retrieval in Fig. 6. The mean average precision is shown in Table 1
                        . The confidence estimation time is shown in Table 2
                        . The computation time is normalized by that of “posterior” in Table 2.

The effect of our data selection proposal is also shown in Fig. 5. The proposed “prior” confidence estimation achieved better recognition rates than “average” under all selection rates, so our data selection proposal improved the average recognition rate of the selected speech. It also matched the recognition rate of “posterior” for most selection rates. There was a considerable difference between “prior” and the conventional “posterior” around a selection rate of 10%. However, we consider this to be due to the effect of the language model used in “posterior” because “mono-loop” also suffered a drop in recognition performance around this selection rate. Besides, recognition rates are degraded even if just a few bits of low-accurate data are selected. The recognition rate increased as the data selection rate decreased; e.g. the correct recognition rate exceeded 80% at a selection rate of 20%. Thus, our proposed technique can identify well-recognized speech data before speech recognition.

The evaluation results of our data selection proposal for the spoken document retrieval task are shown in Table 1 and Fig. 6. The proposed “prior” technique achieved equivalent performance in terms of both mean and raw average precision. In particular, our proposal achieved better than “average” at all selection rates and its improvement of precision increased as the selection rate was reduced as evaluated using the raw average precision. Therefore, our proposed data selection technique can retrieve spoken documents accurately.

The computation time of confidence estimation is shown in Table 2. Our proposal is significantly faster than the conventional “posterior” technique. It is over 50 times faster for confidence estimation. Another important point is that “prior” is also faster than “mono-loop”. This reveals that the proposed technique eliminates all processing except for acoustic Gaussian likelihood calculation.

@&#DISCUSSION@&#

At first, we focus on the difference between the posterior and prior confidence measures. The aim of our proposal is to estimate the overall confidence for each call with a large number of sentences. Fixed phrases (e.g. opening greetings), which are expected to be well recognized, constitutes just one factor in confidence estimation, thus the effect of fixed phrases is not very critical. However, the conventional confidence measure increases with fixed phrases, since fixed phrases provide a higher language score. Instead, our proposed prior confidence estimation approach uses only an acoustic model without the language model, and so the proposed data selection procedure is performed independently of the sentences and these word contexts in a call.

Second, we discuss the effectiveness of rejecting low-accuracy calls. If an important word is difficult to recognize (e.g. an unpronounceable service name), specific calls containing that important word cannot be retrieved because the prior confidence becomes low owing to the pronunciation problem. Since our call analysis assumes the use of spoken documents with the recognition results, specific calls also cannot be retrieved because of recognition error. It is therefore reasonable to ignore calls with many recognition errors.

Third, we discuss optimization of the data selection rate. Considering the subsequent text mining, we believe that the recognition rate should exceed roughly 80%, and so the adequate data selection rate is below around 30% in our experiment. If 10,000 calls arrive every day, 35 (∼19.81 [h]/240 [call]×10.000 [call]/24 [h/processor]) speech recognition processes are required to recognize all calls per day. In the case that we can uses only ten recognition processes, we have to select the data at the rate of 30% (∼10/35) by the following day. The data volume decreases significantly by around 1/3. However, since many data are stored every day at call centers, the volume can be recovered by increasing the number of days that data are retained.

Finally, we discuss the performance difference between ideal and confidence data selection. There is a large gap between ‘ideal’ and confidence scores in data selection. This is because the correlation between recognition rate and confidence score is not sufficiently high. Of particular note, since our target task is spontaneous speech which includes ambiguous utterances, the superiority of the recognition result is often not obvious compared to the other recognition candidates; the correlation between the estimated confidence score of the recognition result and the correct recognition rate became small.

@&#CONCLUSIONS@&#

This paper proposed a rapid prior confidence estimation technique for selecting speech samples that will yield accurate recognition results. It reduces the computation cost since it selects only the best samples for speech recognition based on prior confidence estimation; only a Gaussian acoustic likelihood computation with speech and context independent models is needed. Simulations showed that our confidence estimation technique is over 50 times faster than the conventional posterior confidence measure based on speech recognition, and about 3.9 times faster than the monophone-loop confidence measure based on phoneme recognition. The proposed technique matched the selection performance of the conventional technique, and also improved the precision of the spoken document retrieval task.

@&#ACKNOWLEDGEMENTS@&#

We are grateful to our project manager, Mr. Hirohito INAGAKI of NTT Media Intelligence Laboratories, for giving us the opportunity to pursue this work. We also wish to thank the members of the Audio, Speech and Language Media Project of NTT for their helpful advice.

@&#REFERENCES@&#

