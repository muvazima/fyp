@&#MAIN-TITLE@&#An efficient two-stage region merging method for interactive image segmentation

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Two-stage region merging interactive image segmentation.


                        
                        
                           
                           User-stroke labeled seed super-pixels.


                        
                        
                           
                           Adaptive region merging.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Image segmentation

Interactive method

Region merging

Nearest neighbor

@&#ABSTRACT@&#


               
               
                  Interactive image segmentation aims to extract user-specified regions from the background. In this paper, an efficient two-stage region merging based method is proposed for interactive image segmentation. An image is first over-segmented into many super-pixels using a bottom-up method. The color histogram is exploited to represent each super-pixel, and the Bhattacharyya coefficient is computed to measure the similarity of two adjacent super-pixels. Then some strokes, denoting the desired object and background, are manually labeled by the user on the over-segmented image. With the labeled seed super-pixels, a merging strategy is designed to realize adaptive region merging. The whole merging process is divided into two stages, which are repeatedly executed until no new merging occurs. In the first stage, some unlabelled super-pixels are merged into the labeled foreground or background super-pixels if the labeled ones are their nearest neighbors. In the second stage, any two unlabelled super-pixels are merged together if one super-pixel is the nearest neighbor of the other. Extensive experiments are conducted to evaluate the performance of the proposed method. The results show that the proposed method can extract the object reliably and quickly from the background.
               
            

@&#INTRODUCTION@&#

Image segmentation is an important research topic in computer vision, which aims to partition an image into a finite number of non-overlapping regions. Interactive segmentation is a semi-automatic method, and it exploits the user inputs to guide the segmentation procedure. In contrast to general segmentation, where algorithms try to find consistent image regions automatically, interactive segmentation is especially suitable for object extraction from the background. Therefore, it is also called foreground/background or figure-ground segmentation.

Some of the earliest work on interactive image segmentation can be traced back to active contours [1] and intelligent scissors [2], which relies on finding good edges in images. Formulating image segmentation as a labeling problem in the optimization framework based on graph cut has become another important method over the last decade. Boykov and Jolly [3] firstly used graph cut based inference for interactive segmentation, and they proposed an energy function allowing for the user interaction. Based on the optimization framework, Rother et al. [4] proposed GrabCut. They used the same energy function as Boykov and Jolly [3], but in addition to optimizing the segmentation labels, they also optimized the color models. Li et al. [5] put forward lazy snapping and they also used the same energy function, but accelerated the segmentation by pre-processing the image into super-pixels. Grady [6] proposed another energy formulation which was motivated by the idea of random walks. Liu et al. [7] presented paint selection for interactive image segmentation, which enforced the unary terms in the energy function by using localized color models based on recent brush strokes. The aforementioned methods extract the object from the background by solving an optimization problem. Besides the optimization based methods, lots of other schemes are put forward [8–11]. Ding and Yilmaz [8] presented an interactive framework for segmenting images using probabilistic hypergraphs which modeled the spatial and appearance relations among image pixels. The probabilistic hypergraph provides a means to pose image segmentation as a machine learning problem. Noma et al. [9] proposed a model-based graph matching approach for interactive image segmentation. It starts from an over-segmentation of the input image, and exploits color and spatial information among regions to propagate the labels from the regions marked by the user-provided seeds to the entire image. Protiere and Sapiro [10] proposed an interactive algorithm for soft segmentation of natural images. The segmentation result is obtained via fast, linear complexity computation of weighted distances to the user-provided scribbles. Zhang and Ji [11] proposed a new Bayesian network model for both automatic and interactive image segmentation. They constructed a multilayer Bayesian network from an over-segmentation to model the statistical dependencies among super-pixels, edge segments, vertices, and their measurements. Region growing or region merging is another class of direct and efficient method for image perception [12–15]. In these methods, the user can design ingenious merging strategies to realize region growing from seed regions. Recently, Ning et al. [15] applied seeded region merging to interactive segmentation and proposed the efficient MSRM (Maximal Similarity Region Merging) method. The conventional region merging methods merge two adjacent regions whose similarity is above a preset threshold [16], but these methods have difficulties in adaptive threshold selection. A big threshold will lead to incomplete merging of the regions belonging to the object, while a small threshold can easily cause over-merging. Moreover, it is difficult to judge when the region merging process should stop. The MSRM method overcomes the shortcomings of conventional methods by designing an adaptive merging process to merge image regions according to the defined maximal similarity rule. Zhou et al. [17] proposed a novel interactive segmentation method based on conditional random field models to use the location and color information provided by the user input. The conditional random field is configured with the optimal weights between two features, which are the color Gaussian mixture model and probability model of location information. Hichri et al. [18] proposed a new mechanism of user-machine interaction for interactive segmentation methods to solve the change detection problem in multitemporal remote-sensing images. The user needs to input markers related to change and no-change classes in the difference image. Then, the pixels under these markers are used by the support vector machine classifier to generate a spectral-change map.

In this paper, region merging is still considered and a more efficient merging strategy is proposed. An image is first over-segmented into super-pixels using a bottom-up method. The color histogram is used to represent each super-pixel, and the Bhattacharyya coefficient is computed to measure the similarity of two adjacent super-pixels. Then some strokes denoting the desired object and background are labeled on the over-segmented image manually by the user. With the labeled seed super-pixels, a merging strategy is designed to realize adaptive region merging. The merging process is repeatedly executed until all the super-pixels are labeled. The whole merging process includes two stages. In the first stage, some unlabelled super-pixels are merged into the labeled foreground or background super-pixels if the labeled ones are their nearest neighbors. In the second stage, any two unlabelled super-pixels are merged together if one super-pixel is the nearest neighbor of the other. The advantage of the proposed method is its simplicity and efficiency. Compared with the MSRM method [15], which outperforms the graph cut based algorithm [19–21] under the same user input strokes, the proposed method overall takes less time to produce more accurate results with fewer strokes.

The rest of this paper is organized as follows. In Section 2, feature representation and similarity measurement are introduced. In Section 3, the proposed merging process is detailed. In Section 4, extensive experiments are performed to evaluate the performance of the proposed method. Finally, concluding remarks are given in Section 5.

Effective features extracted from the super-pixels are crucial to the following classification task. The proposed method focuses on merging super-pixels instead of pixels mainly because the feature of a super-pixel is more robust than that of a pixel in pattern classification. Color histogram is a class of distinctive feature which is exploited in pattern recognition, object tracking, and image segmentation [15,22,23]. Ning et al. [15] used single-channel indexed color histograms of super-pixels as feature vectors. They applied indexed histograms in RGB color space to interactive image segmentation and obtained fine performance. Ning et al. [15] also demonstrated the effectiveness of indexed histograms in other color spaces. Though super-pixels obtained using the mean shift method [24] are different in size, and irregular in shape, indexed color histograms are discriminative enough to characterize the super-pixels.

We adopt the strategy of using super pixels instead of pixels, because the feature of a super-pixel is more robust than that of a pixel, and furthermore operations on super-pixels demand less computational cost. Any existing low-level over-segmentation methods, such as the graph cut based method [21], the mean shift method [24] and the watershed method [25], can be used in this pre-processing step. In this paper, we adopt the mean shift method proposed in [24] for initial segmentation because it can well preserve the object boundaries which contribute to excellent results.

Indexed color histogram representation is adopted in this paper because it is more robust than other features in merging based image segmentation. After the image I is over-segmented into N super-pixels using the mean shift method [24], the color histogram is computed in the quantized RGB color space to characterize each super-pixel. Each color channel is uniformly quantized into L levels. Let (xR, xG, xB
                        ) and (R, G, B) be the original color vector and quantized color vector of a pixel in RCB color space. (R, G, B) and (xR, xG, xB
                        ) have relations as,
                           
                              (1)
                              
                                 
                                    
                                       
                                          
                                             {
                                             
                                                
                                                   
                                                      
                                                         
                                                            R
                                                            =
                                                            
                                                               floor
                                                            
                                                            (
                                                            
                                                               x
                                                               R
                                                            
                                                            /
                                                            
                                                               (
                                                               256
                                                               /
                                                               L
                                                               )
                                                            
                                                            )
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         
                                                            G
                                                            =
                                                            
                                                               floor
                                                            
                                                            (
                                                            
                                                               x
                                                               G
                                                            
                                                            /
                                                            
                                                               (
                                                               256
                                                               /
                                                               L
                                                               )
                                                            
                                                            )
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         
                                                            B
                                                            =
                                                            
                                                               floor
                                                            
                                                            (
                                                            
                                                               x
                                                               B
                                                            
                                                            /
                                                            
                                                               (
                                                               256
                                                               /
                                                               L
                                                               )
                                                            
                                                            )
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 floor
                              
                              (
                              )
                           
                         denotes the Rounding operation toward minus infinity. Let Index be the single-channel indexed color value, and Index is computed according to the following equation,
                           
                              (2)
                              
                                 
                                    
                                       
                                          
                                             
                                                Index
                                             
                                             =
                                             R
                                             ·
                                             
                                                
                                                   L
                                                
                                                2
                                             
                                             +
                                             G
                                             ·
                                             L
                                             +
                                             B
                                          
                                       
                                    
                                 
                              
                           
                        Thus a single-channel indexed image is produced where the intensity of pixels may have L
                        3 different values. Therefore, the histograms are calculated in the feature space of L
                        3 bins. In the implementation, L is set to 16 and the histograms are normalized to eliminate the effect of their area.

After obtaining histograms for the super-pixels, a suitable metric should be selected to measure the similarity between two adjacent super-pixels. There are some well-known goodness-of-fit statistical measurements such as Euclidean distance, Bhattacharyya coefficient, and the log-likelihood ratio statistic. Here the Bhattacharyya coefficient is chosen to measure the similarity of two super-pixels. Denote two super-pixels by R and Q, and their similarity is defined as follows,
                           
                              (3)
                              
                                 
                                    
                                       
                                          
                                             
                                                sim
                                             
                                             
                                                (
                                                R
                                                ,
                                                Q
                                                )
                                             
                                             =
                                             
                                                ∑
                                                
                                                   u
                                                   =
                                                   1
                                                
                                                
                                                   
                                                      L
                                                   
                                                   3
                                                
                                             
                                             
                                                
                                                   
                                                      f
                                                      
                                                         R
                                                      
                                                      u
                                                   
                                                   ·
                                                   
                                                      f
                                                      
                                                         Q
                                                      
                                                      u
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where fR
                         and fQ
                         are the normalized histograms of R and Q; and 
                           
                              f
                              
                                 R
                              
                              u
                           
                         and 
                           
                              f
                              
                                 Q
                              
                              u
                           
                         are the corresponding uth elements in the histograms. Bhattacharyya coefficient can also be regarded as the cosine similarity of two normalized unit vectors. This quantity measures the cosine of the angle between 
                           
                              (
                              
                                 
                                    f
                                    
                                       R
                                    
                                    1
                                 
                              
                              ,
                              
                                 
                                    f
                                    
                                       R
                                    
                                    2
                                 
                              
                              ,
                              ⋯
                              ,
                              
                                 
                                    f
                                    
                                       R
                                    
                                    
                                       
                                          L
                                       
                                       3
                                    
                                 
                              
                              )
                           
                         and 
                           
                              (
                              
                                 
                                    f
                                    
                                       Q
                                    
                                    1
                                 
                              
                              ,
                              
                                 
                                    f
                                    
                                       Q
                                    
                                    2
                                 
                              
                              ,
                              ⋯
                              ,
                              
                                 
                                    f
                                    
                                       Q
                                    
                                    
                                       
                                          L
                                       
                                       3
                                    
                                 
                              
                              )
                           
                        . Unlike the Euclidean distance, the higher the Bhattacharyya coefficient of fR
                         and fQ
                         is, the higher their similarity of R and Q is.

We also tried the Euclidean distance and the log-likelihood ratio statistics here, but the result is much worse, which is partly due to the fact that, the Bhattacharyya distance can measure not only the means of sample sets, but the standard deviations as well.

In this section, an efficient region merging based interactive image segmentation method is proposed. The assumption in the merging process is that unlabelled super-pixels should be merged into their nearest neighbors. Base on this assumption, a two-stage merging strategy is designed to realize adaptive super-pixel merging.

Denote the sets of the foreground super-pixels, background super-pixels, and unlabelled super-pixels by MF, MB
                        , and MU
                         respectively. MF
                         and MB
                         are initialized with the super-pixels labeled by the user, and the unlabelled ones constitute MU
                        . The task of the merging process is to allocate the super-pixels in MU
                         to MF
                         or MB
                         according to the pre-defined rule. The guideline of the proposed method is that an unlabelled super-pixel should be merged into its nearest neighbor. The whole process can be divided into two stages, which are repeatedly executed until no new merging occurs.

In the first stage, the unlabelled super-pixels are merged with their adjacent labeled foreground or background super-pixels if the labeled super-pixels are their nearest neighbors. Specifically, for each super-pixel U ∈ MU
                        , its neighbor set 
                           
                              
                                 S
                                 ˜
                              
                              U
                           
                         is obtained, which contains its adjacent super-pixels. The similarity between U and each element in 
                           
                              
                                 S
                                 ˜
                              
                              U
                           
                         is calculated. If U and 
                           
                              
                                 
                                    P
                                 
                                 *
                              
                              ∈
                              
                                 
                                    S
                                    ˜
                                 
                                 U
                              
                           
                         satisfy the rule,
                           
                              (4)
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   P
                                                
                                                *
                                             
                                             =
                                             
                                                
                                                   
                                                      arg
                                                   
                                                   
                                                      max
                                                   
                                                
                                                
                                                   P
                                                   ∈
                                                   
                                                      
                                                         S
                                                         ˜
                                                      
                                                      U
                                                   
                                                
                                             
                                             
                                                sim
                                             
                                             
                                                (
                                                P
                                                ,
                                                U
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        and moreover P* is labeled, then U and P* are merged into one super-pixel with the same label as P*. The first stage is iteratively implemented until no new merging occurs. At the end of each iteration, the sets MB, MU
                        , and MF
                         are updated. Specifically, MB
                         and MF
                         expand, and MU
                         shrinks. When the first stage stops, MU
                         is usually not empty. This indicates that the nearest neighbors of the left unlabelled super-pixels are also unlabelled. When the first stage stops, the second stage starts.

In the second stage, any two unlabelled super-pixels are merged together if one super-pixel is the nearest neighbor of the other. The merging rule is the same as used in the first stage. Specifically, for each super-pixel U ∈ MU
                        , in its neighbor set 
                           
                              
                                 S
                                 ˜
                              
                              U
                           
                        , its nearest neighbor Q* is found according to the rule,
                           
                              (5)
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   Q
                                                
                                                *
                                             
                                             =
                                             
                                                
                                                   arg
                                                   
                                                      max
                                                   
                                                
                                                
                                                   Q
                                                   ∈
                                                   
                                                      
                                                         S
                                                         ˜
                                                      
                                                      U
                                                   
                                                
                                             
                                             
                                                sim
                                             
                                             
                                                (
                                                Q
                                                ,
                                                U
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Note that Q* must be unlabelled in the first iteration of the second stage otherwise the first stage will not end. If Q* is also unlabelled, U and Q* are merged together, and the new super-pixel retains the unlabelled state. At the end of each iteration, MU
                         is updated. The second stage is also performed iteratively and stops when no new merging occurs. When the second stage stops, the first stage starts again.

The two stages of the proposed merging process are executed repeatedly until all the super-pixels are classified as the foreground or background, i.e. 
                           
                              
                                 M
                                 U
                              
                              =
                              ∅
                           
                        . The whole algorithm can be summarized as Fig. 1
                        .

An assumption is made in the proposed method that an unlabelled super-pixels is assumed to belong to the same class as its nearest neighbor. The whole merging process includes two stages, and the merging process is iteratively executed in each stage. In each iteration, the nearest neighbor of each super-pixel is found, and the merging process is executed according to whether the neighbor is labeled or not. Because new super-pixels are probably generated by region merging, new computations must be performed for finding the new nearest neighbors in the next iteration.

In our deliberately designed merging process, only those super-pixel pairs which contain an unlabelled super-pixel and its neighbor are considered for merging. If two unlabelled super-pixels constitute a maximal similarity pair, they are merged into a new bigger unlabelled one. If one unlabelled super-pixel and its labeled neighbor constitute a nearest similar pair, the unlabelled super-pixel is labeled as foreground. Note that in this case, the two super-pixels are not merged together in order not to weaken the features of foreground super-pixels. Our merging strategy aims to merge two unlabelled super-pixels constituting a maximal similar pair and label the unlabelled super-pixel in a nearest similarity pair as foreground class.

The proposed two-stage merging process is an iterative algorithm and the two stages are executed iteratively until the foreground object is extracted from the background. The proposed method is convergent, i.e. every unlabelled super-pixel in MU
                         will be labeled as either the foreground object or background after a certain number of iterations. The convergence process is described as follows. In the first merging stage, the number of unlabelled super-pixels, N, decreases because super-pixels are progressively merged into seed regions. In the second stage of the proposed method, N also progressively decreases since unlabelled super-pixels are merged together. From the above-mentioned analysis, it can be seen that the number of unlabelled super-pixels in MU
                         will decrease in both stages of the proposed method. Since N is finite, the proposed algorithm converges when N is reduced to zero.

Now we outline the proof of the convergence of the proposed two-stage merging process:
                           Lemma 1
                           
                              Except for the final iteration, at each iteration of the merging process, there is at least one merging of super-pixels.
                           

This can be easily proved by contradiction. Assume that, there is no merging of super-pixels at a certain iteration, then the condition for the while loop to continue cannot meet, so the while loop will exit. Hence this certain iteration is exactly the final iteration. □


                              Except for the final iteration, at each iteration of the merging process, the number of super-pixels is decreased by at least one.
                           

This proposition can be obtained directly from Lemma 1. According to Lemma 1, there is at least one merging of super-pixels at each iteration of the merging process (except for the final iteration), and the merging of super-pixels will decrease the total number of super-pixels by at least one. Therefore the total number of super-pixels is decreased by at least one. □


                              Denote Nt
                                  as the total number of super-pixels, the proposed two-stage merging process will iterate for at most Nt
                                  times.
                           

Obviously, no matter how we do the merging, the total number of super-pixels cannot be decreased to zero, i.e. there is at least one super-pixel. So at most Nt
                              
                              −1 super-pixels can be decreased (or merged). According to Proposition 1, Nt
                               iterations will cause at least the decrease of Nt
                              
                              −1 super-pixels, so the proposed merging process will at most iterate for Nt
                               times. □

Based on the Proposition 2, we see that, as long as Nt
                         is finite, the proposed two-stage merging process will terminate within finite number of Nt
                         iterations. So the proposed process will converge within at most Nt
                         steps. Obviously Nt
                         cannot be infinite for any input natural image.

@&#EXPERIMENTS AND RESULTS@&#

Experiments are performed to compare the proposed method with the GrabCut method [4], the PHIM (Probabilistic Hypergraph Image Model) method [8] and the MSRM method [15]. There are two reasons for selecting the MSRM method as the competitor. Firstly, the proposed method is motivated by the MSRM method, and it is based on region merging, the same as the MSRM method. Secondly, the MSRM method has demonstrated higher performance than other well-know methods [19–21] under the same user input strokes. Figs. 2 and 3
                        
                         illustrate some compared results on ten images: the odd lines of the first column show the original images, the even lines of the first column show the over-segmented images, the odd and even lines of the second columns show the weakly and strongly labeled strokes on the over-segmented images respectively, the third, fourth, fifth and sixth columns show the results of the GrabCut method [4], the PHIM method [8]and the MSRM method [15], and the proposed method respectively, in which the odd lines correspond to the weak strokes, and the even lines correspond to the strong strokes. From Fig. 2, it can be seen that the proposed method can produce almost the same results as the MSRM method with the same strokes. In Fig. 3, the proposed method demonstrates better performance than the other three methods. The performance of the proposed method is especially distinct when weakly labeled strokes are used. From Fig. 3, it can be concluded that, overall the proposed method overall obtains the better results than the other three methods with the same strokes. Note that Fig. 3 is merely used to demonstrate the efficiency of the proposed method. If enough strokes are provided, all the four methods can produce good results for the images in Fig. 3.

To quantitatively evaluate the performance of the proposed method, the region-based segmentation accuracy [26], denoted by ACC, and negative rate metric [27], denoted by NRM, are computed with the manually labeled ground-truths. ACC and NRM are defined as,
                           
                              (6)
                              
                                 
                                    
                                       
                                          
                                             
                                                ACC
                                             
                                             =
                                             
                                                
                                                   TP
                                                
                                                
                                                   
                                                      TP
                                                   
                                                   +
                                                   
                                                      FP
                                                   
                                                   +
                                                   
                                                      FN
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (7)
                              
                                 
                                    
                                       
                                          
                                             
                                                NRM
                                             
                                             =
                                             
                                                
                                                   FN
                                                
                                                
                                                   2
                                                   (
                                                   
                                                      TP
                                                   
                                                   +
                                                   
                                                      FN
                                                   
                                                   )
                                                
                                             
                                             +
                                             
                                                
                                                   FP
                                                
                                                
                                                   2
                                                   (
                                                   
                                                      FP
                                                   
                                                   +
                                                   
                                                      TN
                                                   
                                                   )
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where TP (True Positives), FP (False Positives), TN (True Negatives), and FN (False Negatives) denote the number of correctly classified object pixels, the number of background pixels but classified as object, the number of correctly classified background pixels, and the number of object pixels but classified as background respectively. The higher ACC and the lower NRM are, the better the segmentation quality is. Table 1
                         lists the quantitative results of the proposed method compared with the other three methods on the four images in Fig. 3. The MSRM method and our proposed method obtain the same quantitative results on the image Dogs. The proposed method obtains more accurate results than the MSRM method on images Girl, Tiger, and Monkey. The performance of the PHIM method is not satisfactory and stable, which may be due to the fact that, its high-order graphical term is sensitive to complex background or outliers. Table 1 shows that the proposed method overall has the higher ACC and the lower NRM, which implies the better performance.

The proposed method not only produces more accurate results overall, but also takes less time to complete the segmentation task than the other three methods. Running time of the four methods are computed to compare their computational efficiency. To avoid the unnecessary duplication of the competitor work, the proposed method is implemented under the framework of the MSRM software [15] except the core merging algorithm. The experiments are conducted on a Dual Core 2.5GHz machine with 2GB RAM. Table 2
                         lists the running time of the four methods under the same settings on the ten images illustrated in Figs. 2 and 3. From the table, the conclusion can be drawn that the proposed method is faster in terms of the computational time when compared to the other three methods

@&#CONCLUSIONS@&#

Image segmentation is a key step in image processing and analysis, and a classic research branch in computer vision. Interactive segmentation, as a semi-automatic method, is well studied for its wide applications in practice. In this paper, an interactive image segmentation method based on region merging is proposed. The input image is first over-segmented into super-pixels using an unsupervised segmentation method, and normalized histograms are extracted from these super-pixels as their feature vectors. Users mark some foreground and background super-pixels as seed regions. The assumption of the proposed method is that unlabelled super-pixels should be merged into their nearest neighbors. Based on the assumption, a two-stage merging strategy is designed to realize adaptive super-pixel merging efficiently and effectively. In the first stage, unlabelled super-pixels are merged into their nearest seed regions according to their similarity with their neighbors. In the second stage, merging operations are performed among unlabelled super-pixels. The whole merging process is iteratively executed until all the unlabelled super-pixels are merged into the foreground or background seed regions. The proposed method is simple, but efficient and effective. Experimental results show that the proposed method can extract the foreground object reliably and quickly from the background.

@&#ACKNOWLEDGEMENTS@&#

This work is partially supported by the Doctoral Scientific Research Fund (No. BSQD20130154) and Scientific Project (No. xkj201407) of Qufu Normal University, the National Natural Science Foundation of China (No. 61202318), Fujian College Outstanding Young Research Personnel Training Project (No. JA13247), and the Technology Project of provincial University of Fujian Province (No. JK2014040).

@&#REFERENCES@&#

