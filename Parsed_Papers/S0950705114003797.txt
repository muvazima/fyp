@&#MAIN-TITLE@&#Developing new machine learning ensembles for quality spine diagnosis

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Computer aided diagnosis system integrated with MAP and SPC.


                        
                        
                           
                           Proposed hybrid machine learning models.


                        
                        
                           
                           New design for reliable inference system with higher software reliability and quality.


                        
                        
                           
                           KDE used for first time in distinguishing patients.


                        
                        
                           
                           DOE optimization in machine learning.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Machine learning ensembles

Multimethod assessment process

Statistical process control

Spine diagnosis

Bayesian network

Software reliability

@&#ABSTRACT@&#


               
               
                  This research work adduces new hybrid machine learning ensembles for improving the performance of a computer aided diagnosis system integrated with multimethod assessment process and statistical process control, used for the spine diagnosis based on noninvasive panoramic radiographs. Novel methods are proposed for enhanced accurate classification. All the computations are performed considering steep error tolerance rate with statistical significance level of 5% as well as 1% and established the results with corrected 
                        t
                     -tests. The kernel density estimator has been implemented to distinguish the affected patients against healthy ones. A new ensemble consisting of Bayesian network optimized by Tabu search algorithm as a classifier and Haar wavelets as the projection filter is used for relevant feature selection and attribute’s ranking. The performance analysis of each method along with major findings is discussed using various evaluation metrics and concludes with propitious results. The results are compared to the existing SINPATCO platform that uses MLP, GRNN, and SVM. The optimization of machine learning algorithms is obtained using Design of Experiments scheme to achieve superior prediction accuracy. The highest classification accuracy obtained is 96.55% with sensitivity, specificity of 0.966 and 0.987 respectively. The objective is to enhance the software reliability and quality of spine disorder diagnosis using medical diagnostic system and reinforce the viability of precise treatment.
               
            

@&#INTRODUCTION@&#

A medical diagnostic system [1] generally consists of a knowledge base and some methods for solving an intended problem. On the basis of the query requested to the system, it outputs appropriate solutions and provides assistance to medical physicians in diagnosing patients. The knowledge base of such medical systems relies upon on inputs that spring up from the clinical experience of field experts [2]. Knowledge acquisition is the process to transform human expertise knowledge and skills acquired through clinical practice to software, besides being quite time consuming and labor intensive task. Common methods like Case Based Reasoning (CBR) solves the knowledge acquisition problem to some extent, in which the previous clinical cases consisting of patient’s health symptoms in the form of the database is maintained with their possible remedies, past clinical decisions, preventive measures and expected diagnostic outcome measures. During patient diagnosis, the clinical database is explored for the most likely analogous past patient’s record and provides the suitable diagnosis decisions for a new subject [3]. The opinion of physicians regarding the use of inference system is also appreciable in the literature [4]. The advantages of machine learning (ML) methods are that it uses mathematical models, heuristic learning and rules acquisition process for decision making and thus provides controllability, observability, stability and easily updateable by adding a new patient’s record [5–7].

The application of machine learning models in the field of medicine for human disease diagnosis aids medical experts in the identification of diseases based on the symptoms at an early stage, though some diseases exhibit similar symptoms. Current advances in research for Spondylolisthesis and Disk Hernia treatment [8] since the past decades suggest that the related risk factors are identified. The state-of-the-art of technology is needed to be improved for significant accurate diagnostic decisions. The primary motivation of the research work is to design a reliable inference system and provide guidelines to caution against the selection of inaccurate inference system design for the clinical assessment of spine disorders, besides improving the quality of the diagnosis, prognosis, reduce delay in treatment and diagnosis cost [9].

@&#RELATED WORKS@&#

This section provides a bird’s eye view on the spine related advancements in the recent years. Kolta et al. [10] proposed a technique using the whole spine imaging based on standard DXA device (3D-XA) for 3D reconstruction of vertebral bodies in women. Masharawi [11] evaluated the shape characteristics of the Lumbar vertebral body (VB) and neural arch in Spondylolysis using a 3D digitizer and other parameters. Szeyko et al. [12] reported that vertebral infections caused by Coccidioides species need multidisciplinary management that always demands medical treatment and surgical intervention for stabilization. Dall’Ara et al. [13] reported that quantitative computed tomography based finite element modeling provide a quantitative and significant improvement in vertebral strength as compared to simulated dual energy X-ray absorptiometry. Yang et al. [14] introduced non-invasive fluoroscopy based image guided surgery that reduces radiation exposure due to vertebroplasty for vertebral compression fractures. Sapin-De et al. [15] gave subject-specific finite element models using low dose bi-planar X-ray devices for prediction of vertebral strength. Roberts et al. [16] achieved good accuracy for vertebral shape from lumbar radiograph using semi-automated active appearance models. Oh et al. [17] showed novel use of the liquid embolic agent Onyx injected within pseudoaneurysm, resulting in proximal and distal parent vessel closure. Wustrack et al. [18] introduced predictors of new vertebral fractures using data from the placebo arm of the HORIZON Pivotal Fractures. Roux et al. [19] showed that despite the use of PPI concomitant, there is reduced risk of new vertebral fractures during risedronate therapy as compared to placebo. Baum et al. [20] demonstrated that there is significant correlation between the volumes of vertebral bone marrow adipose tissue content to that of a volume of abdominal fat, spine bone mineral density using proton MR spectroscopy in women.

The major research contribution related to SINPATCO platform made by Rocha Neto et al. [21] in which they reported maximum accuracy of 85.9% with the implementation of SVM (linear and KMOD). In their second research article [22], they reported maximum accuracy of 90.32% by using Support Vector Machines (SVM), MLP (Multilayer Perceptron), GRNN (Generalized Regression Neural Network) and SOM (Self-Organizing Map). In this research paper, new ensembles are proposed and the results obtained over this problem using the same database are compared with the existing works. It reports higher precision, specificity and sensitivity values. Robust methods for treating vertebral column disorder including rotation forest ensemble comprising of bagging [23], Principal Components analysis (PCA) [24], decision trees [25], boosting methods [26], Haar wavelets [27], Naïve Bayes [28,29] and other machine learning algorithms are presented. A new ensemble for relevant feature selection is proposed that consists of rotation forest ensemble that employs Bayesian network [30] and Haar wavelets. The attribute ranking is procured using Ranker search algorithm. All the machine learning models are evaluated using statistical significance level of 5% and 1% along with corrected 
                        t
                     -tests [31].

This research article is succinctly organized into various sections as follows. The radiograph pelvic spine data set with its various attributes and the proposed hybrid machine learning algorithms are presented in Section 3. The theory and calculations are briefly illustrated in Section 4. The design of the proposed inference system and the results obtained from the experiments are elucidated in Section 5. Finally, the Section 6 concludes with the interpretations of the accomplished results and their significance for further research.

@&#MATERIAL AND METHODS@&#

The Vertebral Column dataset consists of 310 instances and six attributes, having one class without any missing values. There are three different categories comprising of Spondylolisthesis, Disk Hernia and normal cases. The two categories Disk Hernia and Spondylolisthesis are combined into one category labeled as “abnormal”. Each patient data was collected from a medical residence period in spine surgery at The Group of Applied Research in Orthopaedics (GARO), The Centre medico-Chirurgical de Réadaptation des Massues, Lyon, France by Dr. Henry Mota (Orthopedic Surgeons’ Hospital Monte Klinikum). The database is obtained from sagittal panoramic radiographs of the spine of format 30×90cm. The data are obtained from radiographs of patients operated for herniated discs (60 patients) and Spondylolisthesis (150 subjects) and the rest 100 subjects are volunteers who do not have any conditions in the column, called normal category. Each patient is described by six biomechanical attributes which are derived from the shape and orientation of the pelvis and lumbar spine: pelvic tilt, grade of spondylolisthesis, sacral slope, lumbar lordosis angle, pelvic incidence, and pelvic radius. All these biomechanical parameters of the vertebral column are discussed elaborately in [21,22].

The radiograph’s measured features along with their corresponding values are presented using parallel coordinates plot as shown in Fig. 1
                         after preprocessing for its visualization of high-dimensional geometry and analyzing the multivariate pelvic spine data. For showing a set of distinct points in multidimensional space, a backdrop is projected that consists of n parallel lines, projecting vertically and equally spaced. A point in the multidimensional space is reflected by a polyline with vertices lying on the parallel axes in such a way that the position of vertices on the kth axis corresponds to the kth coordinate of that point.

The machine learning ensembles have been trained with 80% of the entire clinical radiograph dataset and subsequently the degree of learning of these ensembles is tested on the remaining dataset, such that the testing is not performed on the same instances as it leads to biased evaluation. The data set is standardized in such a way that the overall mean and standard deviation is equal to 0 and 1 respectively. Then data cleaning methods are applied for dimension reduction as well as outlier’s [32] removal. A new ensemble is developed for the appropriate feature selection and their ranking for the model’s input and also the outliers from the dataset are removed based on the quantile information obtained statistically beyond 5% and 95%.

The research work primarily concentrates on the problem of finding optimal features needed for training and subsequent extirpate the frivolous features from the clinical dataset and additionally eliminates the latent outliers by applying felicitous techniques to develop a superior classifier for accurate prediction. In the literature, feature reduction techniques are commonly branched as extraction and selection techniques. The former converts the original features into a new feature space; the latter retains only minimum and best features from the available original ones. Both these techniques are used widely in data engineering applications for achieving computational complexity advantages while training the ensemble models. The Spearman’s correlation matrix is shown in Table 1
                         and it reflects the degree of association among the variables of pelvic spine.

Here multiple threads are used for the generation of CHAID decision trees as it gives a simple and better interpretable method for multivariate analysis of pelvic spine data, shown in Fig. 3
                        
                        . The major advantages are ease of coding with modern programming languages, threads share information more efficiently, enables parallel execution and saves execution time. It’s highly suitable for analyzing large medical data sets. The computation for goodness of split of the candidate attribute is optimized using a metric. The proposed pseudo code for splitting a node in the multithreaded CHAID decision tree is shown in Fig. 4
                        . The tree is constructed using training data and then tested to obtain the LIFT-ROC curve for the abnormal class as shown in Fig. 2. It performs multi level splits and the probabilities for merging and splitting are 0.05 and 0.001 respectively.

One of the important problems in multivariate techniques is to select relevant features from the available set of attributes. The common feature selection techniques [33,34] include wrapper subset evaluation, filtering and embedded models. In case of embedded models, classifiers are used in constructing ensemble models; whereas wrapper subset evaluation method provides ranks to features based on their importance using base algorithms and filter methods rank the features based on statistical measurements. Combining wrapper and filter techniques provides superior computational results as compared to embedded methods, due to its having independent nature of classification models. Also embedded feature selection models provide better accurate results as it takes classification methods for enhancing the accuracy of the feature selection process.

In this work, the relevant feature selection is done by employing rotation forest algorithm that evaluates the worth of the features using Bayesian network as base classifier and Haar wavelets as the projection filter for the transformation of the data. Bayesian network learning uses search algorithm and quality measures. Simple estimator is used for the estimation of conditional probability tables of the network after the structure has learnt. It estimates the probability directly from the dataset by using alpha as a parameter for estimating the probability tables and can be interpreted as the initial count on each value. Here alpha has been set as 0.50. The Bayes network learning algorithm uses Tabu search [35] approach for finding a well scoring Bayes network structure. The Tabu search method is a hill climbing method that searches till an optimum is reached. The following step is the worst possible step. The last steps are kept in a list and none of the steps in this so called Tabu list is considered in taking the next step. The best optimized network structure formed during this process is returned. Entropy [36] is used as scoring metric to determine the measure for judging the quality of a network structure. After the network structure is learnt, Markov Blanket correction is applied to the network structure. This ensures that all the nodes in the network are part of the Markov blanket of the classifier node. In the Bayes leaning, Active Directory (AD) tree is used, so that the learning time of classifier goes down typically. Also AD trees are memory intensive and run with less memory. The ranks of the individual attributes are obtained by Ranker search with leave-one-out 10-fold cross validation. This work focuses on the problem of optimizing the feature selection and thereby filtering out the irrelevant variable from the dataset and removes the outliers by using appropriate methods to obtain higher accurate classifier. The ranks of all the spine dataset attributes with their description are provided in Table 2
                        .

The methodology of the research work is grouped into four stages: (1) attribute extraction; (2) preprocessing and feature reduction; (3) optimization of classifiers by fine tuning their parameters; and (4) design of the proposed inference system. An important matter in any multivariate classification problem is handling suitable features in the sense that those input features combinations produce the best output. Even though lots of care has been taken during the data acquisition, it’s practically impossible to get noise free data, while recording the feature’s values using different automated software and tools. Also, all variables in the radiograph dataset may not be useful in the analysis. The working of machine learning methods and model are mentioned in this section with the parameters involved in each of them. This work addresses the advancement of diagnostic methods for pelvic spine diagnosis. We propose robust methods of treating vertebral column disorder, including rotation forest ensemble comprising random tree and Principal Component analysis, decision trees, boosting methods, Haar wavelets, Naïve Bayes and other machine learning algorithms.

Rotation forest (RF) [37] is an ensemble construction technique that consists of two components. The first is the heuristic component based on feature extraction. It is used to build a feature subset. This method improves the classification accuracy by improving the training space. The second is the classification component. A base classifier algorithm is used for constructing a classifier model. In this study, two flavors of RF are presented. In the first case, the base classifier is Random tree and Principal component analysis is used for feature extraction. The latter consist of decision table as classifier and haar wavelets as the feature extraction technique. The proposed ensemble is detailed below:-

Let x
                        =[x1
                        ,…,
                        xn]T
                         be an instance given by n variables and X be the training sample in a form of N
                        ×
                        n matrix. Let vector Y
                        =[y1
                        ,…,
                        yN] be class labels, where yj
                         takes a value from the set. Let D1, …, DL be the classifiers in ensemble and F is feature set.

In ensemble learning, choosing L in advance and training classifiers in parallel is necessary.

Follow the steps to prepare the training sample for classifier Di:
                           
                              1.
                              Divide F randomly into K disjoint or intersecting subsets. To maximize degree of diversity, disjoint subsets are chosen.

Let Fi,j
                                  be jth subset of features to train set of classifier Di.

Initialize Random tree with Fi,j
                         subset of features such that randomly k attributes are chosen at each node.

Draw a bootstrap sample of instances of size 75% by selecting randomly subset of classes for every such subset. Run PCA to extract features for only M features in F
                        i,j and the selected subset of X. Store the coefficients of the computed principal components, aij[1]
                        , …, ai,j[Mj]
                        , each of size M
                        ×
                        1.
                           
                              3.
                              Arrange obtained vectors using coefficients in a sparse “rotation” matrix Ri having dimensionality n
                                 ×
                                 ∑Mj. Compute the training sample for classifier Di by rearranging the columns of Ri. Represent the rearranged rotation matrix Ra
                                    i
                                  (size N x n). So the training sample for classifier Di is X Ra
                                    i.
                                 
                              

In the classification phase, for a given input x, compute the confidence of each class by m(x)
                                 
                                 =
                                 
                                 
                                    
                                       
                                          
                                             1
                                          
                                          
                                             L
                                          
                                       
                                       
                                          
                                             ∑
                                          
                                          
                                             i
                                             =
                                             1
                                          
                                          
                                             L
                                          
                                       
                                       di
                                       ,
                                       
                                       l
                                    
                                  
                                 (xRi
                                    a), 
                                 j
                                 =1, 2, …, c. Assign x to the class with largest confidence value.

Bagging is a procedure to increase the prediction accuracy of an algorithm by creating its multiple versions by aggregating and predicts a class based on plurality voting scheme. Here Gibbs classifier voting is used for aggregation first time and it provides better results.

The proposed method of bagging C4.5 is as follows:
                           
                              1.
                              Given training data set D, {(Cn
                                 ,
                                 dn), n
                                 =
                                 1, 2, …, N} where C is class labels, d is data points. D is randomly divided into test set (say T with 20%) and learning set (say L with 80%).

Let P(d,
                                 Lk)be predictor model where Lk
                                  is specific k sequence learning set. Replace P(d,
                                 Lk) by its average as PA(d)
                                 
                                 =
                                 
                                 ELP(d,
                                 L) where EL
                                  is expectations over L, PA
                                  is aggregation.

The aggregation is done using Gibbs classifier using rule: 
                                    
                                       
                                          
                                             y
                                          
                                          
                                             ̃
                                          
                                       
                                    
                                 
                                 
                                 =
                                 
                                 sign(wT
                                 
                                 
                                 x)
                                 
                                 =
                                 
                                 fGC(x,
                                 L) as it draws a w from the distribution p(w). Bootstrapping dataset {L(B)} and form {P(d,
                                 LB)}.
                              

Form C5 classification tree is constructed by 10-fold cross validation. If C is the class label, then {P(d,
                                 LB)} predict PB(d) with the highest probability of ∫maxjP′(C|d)
                                 ∗
                                 P′
                                    X(d). Pruning of based on confidence factor=0.25.

Above steps are repeated till specified iterations and reports the optimized learned tree. Then the test data is run over the tree.

Nearest-Neighbor Generalized Exemplars (NNGE) [38] learns incrementally by classifying and followed by generalizing each new instance using Minkowski distance function that manages hyper-rectangles, feature weights, symbolic features and exemplar. Minkowski distance is given by Eq. (1)
                        
                           
                              (1)
                              
                                 
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             i
                                             =
                                             1
                                          
                                          
                                             n
                                          
                                       
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         X
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   -
                                                   
                                                      
                                                         Y
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             p
                                          
                                       
                                    
                                    
                                       p
                                    
                                 
                              
                           
                        
                     

Numeric values are standardized by dividing with the range of observed values. It uses dynamic looping to modify exemplar and feature weights after each new instance to be classified. While classifying a data point, several hyper-rectangles may be subset of that new example, but may belong to wrong class. It prunes these hyper rectangles so that the new instance is no more a member. After classification, the new instance is generalized by merging with nearest exemplar belonging to same class. It resolves affected area of feature space that conflict with the proposed new hyper rectangle. NNGE classifies an input instance by computing nearest neighbor in the created exemplar set using Minkowski distance function. It chooses a class having least distance.

The exemplar weight WH is WH
                        =(p
                        +
                        n)/p. The feature weight Wi is initialized to 1 and increased or decreased according to the following rules:
                           
                              •
                              For correct classifications, raise Wi if feature in new exemplar not matched to that of chosen exemplar, otherwise reduce it.

For incorrect classifications, raise Wi if the feature matched, otherwise reduce it.

There are shortcomings with the original Repeated Incremental Pruning to Produce Error Reduction algorithm (RIPPER) with respect to optimization of rule set size and accuracy. The improved RIPPER is optimized with a different prune metric as described below:
                           
                              
                                 
                                 
                                    
                                       Initially, the rule set is empty.
                                    
                                    
                                       Grow ()
                                    
                                    
                                       {
                                    
                                    
                                       Grow the rule set by adding one rule
                                    
                                    
                                       Based on highest information gain: 
                                             
                                                p
                                                
                                                   
                                                      
                                                         log
                                                         
                                                            
                                                               
                                                                  
                                                                     
                                                                        p
                                                                     
                                                                     
                                                                        t
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                         -
                                                         log
                                                         
                                                            
                                                               
                                                                  
                                                                     
                                                                        P
                                                                     
                                                                     
                                                                        T
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       }
                                    
                                    
                                       Prune ()
                                    
                                    
                                       {
                                    
                                    
                                       Incrementally prune each rule
                                    
                                    
                                       Based on metric: (p
                                          +1)/(p
                                          +
                                          n
                                          +2)
                                    
                                    
                                       }
                                    
                                    
                                       Optimization ()
                                    
                                    
                                       {
                                    
                                    
                                       Optimize the initial rule set to obtain minimum specified description length.
                                    
                                    
                                       Repeat Grow () and Prune () based on prune metric (TP+TN)/(P
                                          +
                                          N).
                                    
                                    
                                       Delete rules that are irrelevant from the rule set.
                                    
                                    
                                       }
                                    
                                 
                              
                           
                        
                     

In Adaboosting method [39], the alternating decision tree as shown in Fig. 5
                         grow iteratively by adding one base rule from each iterations corresponding to a sub-tree with a decision node as root. This new sub-tree is appended as a child to a predictor node. The advantage of this method is that a decision node can be added to any location of tree and splitting criteria is based on weighted error of added rules. The algorithm is described below:
                           
                              1.
                              Provide a training sample to the initial AD tree classifier.

Denote r(x) as a set of base rules r that are associated with instance x.

Define the set of preconditions (P
                                 t) and rules (R
                                 t) on boosting iteration.

Each iteration associate a positive weight with each training instance.

Determine total weight W of the training sample that satisfy the predicate c. Then label the instances of the test set by as either class based on W
                                 +
                                 c, W
                                 −
                                 c respectively.

The real AdaBoost method [39] exploits class probability estimates P
                        m(x) to construct real valued contributions f
                        m(x). Here the base classifier is decision table/Naive Bayes hybrid classifier (DTNB). The detailed method of hybridization is given below:
                           
                              
                                 
                                 
                                    
                                       Initialize weights W
                                          i = 1/log N, i
                                          =1, 2, …, N.
                                    
                                    
                                       For(i
                                          =0; i
                                          <
                                          M; i+ +)
                                    
                                    
                                       {
                                    
                                    
                                       Adjust the DTNB classifier to obtain a class probability estimate P
                                          m(x)=
                                          P
                                          w(y
                                          =1|x) ε [0, 1], using Wi.
                                    
                                    
                                       Assign f
                                          m(x)=
                                          
                                             
                                                
                                                   
                                                      1
                                                   
                                                   
                                                      2
                                                   
                                                
                                             
                                           log P
                                          m(x)/(1−
                                          P
                                          m(x))
                                    
                                    
                                       Assign Wi=Wi exp[−y
                                          i 
                                          f
                                          m(x
                                          i)], i
                                          =1, 2, …, N and renormalize 
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      m
                                                      =
                                                      1
                                                   
                                                   
                                                      M
                                                   
                                                
                                                Wi
                                                =
                                                1
                                             
                                          .
                                    
                                    
                                       }
                                    
                                    
                                       Classifier sign: 
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      m
                                                      =
                                                      1
                                                   
                                                   
                                                      M
                                                   
                                                
                                                
                                                   
                                                      f
                                                   
                                                   
                                                      m
                                                   
                                                
                                                (
                                                x
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Communalities [40] reflect vital information regarding the amount of variance involved in each variable that is used in the analysis. The initial communalities are measures of the variance in each feature accounted for by all factors. The extraction communalities are measures of variance in each feature accounted for by other factors in the factor solution as shown in Table 4. Variables not fitting well in the factor solution should be deleted from the analysis. Raw communalities are computed on the basis of covariance and therefore they are on the same scale as the item variances. Rescaled communalities indicate the adjustment in proportions of variance for item variances and its values ranges 0 (completely unrelated)–1 (completely determined).

For a better analysis of the machine learning algorithms some of the metrics used in this work are described as follows. The abbreviations used to describe the metrics are as follows. TP is true positive cases, TN is true negative cases, FP is false positive cases and FN is false negative cases.

Accuracy is the percentage of correct classification of the test samples. P and N are positive and negative instances respectively, and is given by Eq. (2)
                     
                        
                           (2)
                           
                              Accuracy
                              =
                              
                                 
                                    TP
                                    +
                                    TN
                                 
                                 
                                    P
                                    +
                                    N
                                 
                              
                           
                        
                     
                  

Area Under Curve (AUC) is the area calculated under ROC curve plotted using TP rate vs. FP rate and is given by the Eq. (3). Its value ranges from 0 to 1.
                        
                           (3)
                           
                              AUC
                              =
                              
                                 
                                    1
                                 
                                 
                                    2
                                 
                              
                              
                                 
                                    
                                       
                                          
                                             TP
                                          
                                          
                                             TP
                                             +
                                             FN
                                          
                                       
                                       +
                                       
                                          
                                             TN
                                          
                                          
                                             TN
                                             +
                                             FP
                                          
                                       
                                    
                                 
                              
                           
                        
                     
                  

Cohen’s Kappa Statistics (K) value is the probability measure of classification that occurs due to chance and ranges from −1 to 1. Higher the value of K indicates greater confidence in classification and is measured by the Eq. (4)
                     
                        
                           (4)
                           
                              K
                              =
                              (
                              
                                 
                                    P
                                 
                                 
                                    0
                                 
                              
                              -
                              
                                 
                                    P
                                 
                                 
                                    c
                                 
                              
                              )
                              /
                              (
                              1
                              -
                              
                                 
                                    P
                                 
                                 
                                    c
                                 
                              
                              )
                           
                        
                     where P
                     0
                     =Total agreement probability and Pc
                     
                     =agreement probability due to chance.

Sensitivity (TPR) measures the ability to correctly classify healthy patients as positive and is calculated using the Eq. (5)
                     
                        
                           (5)
                           
                              Sensitivity
                              =
                              
                                 
                                    TP
                                 
                                 
                                    TP
                                    +
                                    FN
                                 
                              
                           
                        
                     
                  

Specificity (TNR) measures the ability of the classifier to correctly classify ill patients and is given by Eq. (6)
                     
                        
                           (6)
                           
                              Specificity
                              =
                              
                                 
                                    TN
                                 
                                 
                                    FP
                                    +
                                    TN
                                 
                              
                           
                        
                     
                  

False Negative Rate (FNR) is the number of individuals identified as healthy, but actually they are abnormal. It is calculated by Eq. (7)
                     
                        
                           (7)
                           
                              FNR
                              =
                              
                                 
                                    FN
                                 
                                 
                                    TP
                                    +
                                    FN
                                 
                              
                           
                        
                     
                  

Positive Predicted Value (PPV) is the number of individuals with positive test results and is diagnosed correctly and is given by Eq. (8)
                     
                        
                           (8)
                           
                              PPV
                              =
                              
                                 
                                    FN
                                 
                                 
                                    TP
                                    +
                                    FN
                                 
                              
                           
                        
                     
                  


                     F-value measures the performance of a test which is the harmonic mean of recall and precision and is given by Eq. (9)
                     
                        
                           (9)
                           
                              F
                              =
                              2
                              x
                              
                                 
                                    Precision
                                    ×
                                    Recall
                                 
                                 
                                    Precision
                                    +
                                    Recall
                                 
                              
                           
                        
                     
                  

False Positive Rate (FPR) is defined as the number of individuals identified as abnormal, but the patient is healthy and is given by Eq. (10)
                     
                        
                           (10)
                           
                              FPR
                              =
                              
                                 
                                    FP
                                 
                                 
                                    FP
                                    +
                                    TN
                                 
                              
                           
                        
                     
                  

Root Mean Square Error (RMSE) is the square root of the average squared error in the classification of the test dataset and is given by Eq. (11)
                     
                        
                           (11)
                           
                              RMSE
                              =
                              
                                 
                                    
                                       
                                          1
                                       
                                       
                                          N
                                       
                                    
                                    
                                       
                                          ∑
                                       
                                       
                                          J
                                          =
                                          1
                                       
                                       
                                          N
                                       
                                    
                                    
                                       
                                          (
                                          Ei
                                          -
                                          
                                             
                                                Ei
                                             
                                             
                                                ′
                                             
                                          
                                          )
                                       
                                       
                                          2
                                       
                                    
                                 
                                 
                                    2
                                 
                              
                           
                        
                     
                  

The computation of reliability statistics alpha is based on the number of members in the analysis (K) and the ratio of average inter-item covariance to average member variance. It is mathematically defined as follows:
                        
                           (12)
                           
                              α
                              =
                              
                                 
                                    K
                                 
                                 
                                    K
                                    -
                                    1
                                 
                              
                              
                                 
                                    
                                       1
                                       -
                                       
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                
                                                   K
                                                
                                             
                                             
                                                
                                                   σ
                                                
                                                
                                                   
                                                      
                                                         Y
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                
                                                
                                                   2
                                                
                                             
                                          
                                          
                                             
                                                
                                                   σ
                                                
                                                
                                                   X
                                                
                                                
                                                   2
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     where K
                     =number of members, 
                        
                           
                              
                                 σ
                              
                              
                                 X
                              
                              
                                 2
                              
                           
                        
                     
                     =observed variance and 
                        
                           
                              
                                 σ
                              
                              
                                 
                                    
                                       Y
                                    
                                    
                                       i
                                    
                                 
                              
                              
                                 2
                              
                           
                        
                     
                     =variance of component i for each member. The obtained value of alpha=0.710 suggests that the analysis performed on the radiographs pelvic spine data is acceptable and when it is extended for even larger data sets, the results will hold true or may vary to a negligible extent. The research work carried out signifies the important information about the validity of the work.

In this paper, the proposed method of visual distinction using Kernel Density Estimator (KDE) [6] of bivariate random vector for the pelvic spine diagnosis integrated with the inference system is presented (as shown in Fig. 7
                     
                     ). Here KDE is a metric for comparing the severity of illness based on the correlation coefficient between the features of the illness. If the correlation coefficient (cc, ranging between −1 and 1) is higher in magnitude then the severity of illness is very high else it’s low. The nature of plot shows the difference between the healthy as well as abnormal patients. The inference system evaluates the newly arrived patient data using the following method. The input variables are the measured parameters from the radiographs features obtained from the pelvic spine of the patient. Then the machine learning (ML) algorithms are run over the input data and the output from each ML model are obtained along with a correlation coefficient (cc). If the cc⩾0.9 then the abnormality is higher else it’s lower. Since the ML models are designed in parallel and independently. The estimation of reliability as discussed in [1] of the decision from the inference system is computed using the Eq. (13)
                     
                        
                           (13)
                           
                              
                                 
                                    R
                                 
                                 
                                    s
                                 
                              
                              =
                              1
                              -
                              
                                 
                                    
                                       ∏
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       n
                                    
                                 
                              
                              (
                              P
                              /
                              Q
                              )
                              ∗
                              P
                              (
                              E
                              )
                           
                        
                     where P
                     =# failures occurred in n runs. Q
                     =#runs sampled in input sub domains E. P(E)=probability of data E is accessed in the operational environment of inference system.

The proposed inference system consists of highly reliable machine learning algorithms for accurate diagnosis of Disc hernia and Spondylolisthesis. The system is trained with the past clinical records of patients having the spine disorders. The machine learning algorithms are trained over the clinical data after preprocessing of raw data and their optimization along with fine tuning is achieved after repeated experimentations. Once the system is learnt, then it is ready for diagnosis. The final decision is provided by a Gibbs classifier (GC) that takes the output of all machine learning algorithms. The GC draws w from distribution q(w) and produces output based on the rule 
                        
                           
                              
                                 y
                              
                              
                                 ̃
                              
                           
                        
                     
                     =
                     sign(wTx)
                     ≡
                     fGC(x,
                     w). The probability that the final output is a correct decision computed using Bayesian model given by
                        
                           (14)
                           
                              P
                              (
                              X
                              )
                              =
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       n
                                    
                                 
                              
                              P
                              (
                              X
                              ∩
                              Yn
                              )
                              =
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       n
                                    
                                 
                              
                              P
                              (
                              X
                              |
                              Yn
                              )
                              P
                              (
                              Yn
                              )
                           
                        
                     where 
                        X
                      is the correct decision from GC classifier and 
                        Yn
                        
                      is the correct decision from machine learning algorithms in the inference system. The radiographs data are repeatedly run and tested the inference system for 100 times by bootstrapping the dataset and find that the average accuracy of the inference system achieved is 98.9999%.

@&#EXPERIMENTS AND RESULTS@&#

Sensitivity reflects the ratio of abnormal patients who are correctly identified. Specificity gives the ratio of identifying a healthy patient as healthy. Therefore the target is to design an inference system with a maximized degree of specificity and sensitivity. In this work corrected 
                        t
                     -test is employed because it provides better results than 
                        t
                     -tests as it accounts for the reduction of dependency of involved variables. The experiments are conducted using mathematical models and new machine learning ensemble algorithms after optimization of the metrics involved for correctly identifying disc hernia and spondylolisthesis patients at early stage based on the radiograph measured parameters with confidence level of 95% and 99% as presented in Tables 9 and 10.

The component score coefficient matrix in rotated space using Principal Component Analysis (PCA) using varimax with Kaiser Normalization and scree plot with Eigen value in Y-axis and component number in the X-axis for pelvic spine data classes is shown in Fig. 6. The result reflects the coordinates of the variables plotted on each dimension. Table 6 shows the component matrix of the factor loadings for each variable on the unrotated factors for abnormal and normal class obtained from factor analysis based on covariance of the radiograph data. The Kendall’s W coefficient of concordance is a non parametric statistic that gives mean ranks between the involved parameters and the normality test values are presented in Table 3
                     
                     .

The vital information extracted from the factor analysis is the nature of the behavior of radiograph features regarding their dependency and independence among each other. The newly generated rotation axes give highly vital information about the features. While working with the machine learning ensembles for feature selection, the curse of dimensionality is given due care to be avoided, that is, too less input feature variables shall potentially lead to a simpler model increasing predictive accuracy. But the modeling of real life complex phenomena or problems cannot be mapped using simpler models as it is more prone to fragile system or model.

For each factor solution Eigen values, variance explained and cumulative variance explained are shown in Table 5
                     . The first column shows individual values depending on initial Eigen values. In case of the initial factor solution, the number of factors equals the number of variables. ‘Total” shows the degree of variance in the observed variables for factor. “% of Variance” indicates the percent of variance by each specific component, with respect to the total variance. “Cumulative%” shows the percent of variance by all components added up to as well as the current one. There will be a few factors that indicate the lion’s share of variance and remaining factors indicate little variance in a good factor analysis. Sum of squared loadings provide information about extracted components. The values are equal that are reported under Initial Eigenvalues for principal components, whereas in other extraction methods, the values may be lesser than initial values for errors in measurements. The rescaling affects extraction and rotation values so raw and rescaled values for initial Eigen values remains unchanged. All the rescaled values are standardized for accounting variances in initial Eigen values.


                     Table 6
                      shows the factor loadings for each variable on the unrotated components or factors. Each value represents the correlation between item and unrotated factor. These correlations give an important interpretation of the factors. It’s done by searching for a common thread among the features that have large loadings for a specific factor. The raw values are in the scale of the original variables. The rescaled values are standardized such that all values are measured on the same scale. It is observed that items with large loadings values of the unrotated factors, which can make interpretation difficult, therefore in such a case, it is worth examining a rotated solution.


                     Table 7
                      gives reproduced covariance and residual for the factor analysis solution. The solution is a good one, if the reproduced covariance will be close to the observed values. Residuals show the difference between the predicted and observed values. For a good factor analysis solution, most of these values will be small. The scree plot as shown in Fig. 6 shows the Eigen values for initial components. It is used to help determine the optimal number of factors or components to retain in the solution. For a good factor analysis, this chart will look roughly like the intersection of two lines. Generally, the factors to be kept are the ones on the steep slope. The ones on the shallow slope contribute relatively little to the solution, and can be excluded. The factor score coefficient matrix is shown in Fig. 6 values used to compute factor scores for each case. For each case, the factor score is computed by multiplying variable values by factor score coefficients. For principal component models, these give exact component scores. For other extraction methods, exact scores cannot be computed. The coefficients are used to generate estimates of the true factor scores. M-estimators as shown in Table 8
                      are robust measures of central tendency as they are not affected by deviations from normality. Vital information regarding the magnitudes of each variable is obtained by observing the magnitude of values.

The reason for achieving statistically improved results is mainly due to preprocessing stage (data mining) of the raw clinical data, relevant feature selection method, outlier’s identification and removal, development of new machine learning ensemble algorithms and design of robust inference system that included statistical quality control evaluation scheme. Also the new ensemble algorithms are optimized based on the metrics involved in achieving higher classification accuracy. The paper proposes the design of a reliable inference system that consists of a collection of tested robust algorithms machine learning algorithms to reduce the probability of overall misclassifications. The final decision of the inference system is not dependent on one model instead powerful and statistically heuristic learning models are employed. The kernel density estimator graph compared to the correlation coefficient provides vital information about the patient’s health condition. As per our knowledge, this method is used first time for pathologic diagnosis.

In this research study, the best prediction accuracy achieved is 96.55% for AdaBoost and improved RIPPER algorithm. The corrected 
                        t-
                     test is employed as it provides statistically confident results than 
                        t-tests due to reduction of dependency among variables. The experiments are conducted based on several reliable machine learning algorithms developed for distinguishing spondylolisthesis affected patients against healthy subjects based on non invasive radiograph measured parameters. Also the results obtained from corrected 
                        t-
                     test over the new ensembles with respect to the Rotation forest ensemble prove the robustness and reliability of the models for predictive accuracy as shown in Tables 9 and 10
                     
                     .

In the proposed inference system as shown in Fig. 8
                     , Design of Experiment (DOE) [6] is implemented to discover the optimized values of each parameter that affect the classification accuracy of the machine learning algorithms (MLA). The fractional factorial design technique is computationally applied to get the optimum values. The architecture presents an independent interaction of the machine learning algorithms and therefore parallel computation is carried out by feeding the preprocessed clinical data to each MLA. The training data are the clinical data acquired during medical examination of patients and is undergone through the data cleaning process. Feature selection and extraction is performed for identifying the minimum and sufficient number of attributes. The Reliability diagram as shown in Fig. 9
                      provides graphical visualization and evaluation of probabilistic forecasts. The mean score of the group and the observed relative frequency of positive class value are plotted on the X and Y axis respectively.

Statistical process control [41] technique is a valuable applied statistical method to monitor and control a process that operates to produce conforming results. Here we have used control charts that results in continuous quality improvement as shown in Fig. 10
                     . In primary health care service quality improvement processes have mostly met with limited success due to its inherent complexity. In such a situation, complexity science provides better edge to understand primary care services by using multimethod assessment process (MAP) [42]. Its principles include in-depth understanding vision and mission, learning and reflections that help organizations foster adaptability and insights for constructive change. Thus, complex adaptive systems (CAS) [43] demand technological innovations like electronic clinical records and study interactions among different components. Continuous quality improvement stimulates sustained organizational change that uses interdisciplinary teams to achieve measurable outcomes. The main reason for limited success is that existing approaches are based on enhancing the quality of individual factors rather than complex interactions of many interrelated parts. CAS dynamically evolves over time and exhibit self organization, emergence and co-evolution. Since the relationships are dynamic and non-linear, therefore, more attention is paid to the relationship between agents than to the quality of the individual agents. The concern is more for the development of agent’s learning capabilities, focus on co-evolution and encourage diversity among agents. MAP is a change process reflected from complexity science for guiding and informing its methods to foster sense-making, learning, improvisation and adaptation towards an impact of the change. It provides qualitative and quantitative data collection using multi methods such as observation, collection of practice documents, patients’ survey and pathways, review, key-informant and in-depth interviews, clinician and staff questionnaires and practice genograms. Reflective Adaptive Process is initiated by the outcome of MAP to identify priority improvement opportunities and potential solutions. The system update option enables the user to update the system from time to time and the new data resources can be added to the database.

The major findings are that the ensemble Rotation forest consisting of Random tree as a base classifier and PCA as a projection filter transformation that leads to improvement in the classification and predictive analysis of the inference system. The most appropriate machine learning ensemble methods and models that could be used to advance the existing technology for spine diagnosis is presented. The integration of statistical quality control evaluation with the inference system and the experimental results obtained in this work provides a new dimension towards quality spine diagnosis. The superior performance of proposed algorithms developed in this research work exhibits excellent results as compared to the current works accomplished by other researchers.

@&#DISCUSSION AND CONCLUSIONS@&#

The major contributions in this work are towards development of new hybrid machine learning ensembles, computation of software reliability and quality associated with medical diagnosis systems, integrating statistical quality control evaluation strategy to have accurate and robust decisions useful for assisting physicians. The discovery of statistical relationships among the features of pelvic spine using Principal component analysis and the design of a reliable inference system for making fast decisions and with greater degree of certainty to improve the spine treatment. Also, another major contribution lies in the relevant feature selection based on an ensemble of machine learning algorithms. Here MAP is implemented as a practical technique using complexity science principles as a theoretical platform for informing practice improvement and further it could be easily adapted to any organizations other than primary health care.

The primary aim of the work is to upgrade the software reliability of the spine diagnosis and reduce the rate of misdiagnosis and the obtained test and experimental results indicate that the goal is reached. But still there is lots of room at the bottom in this direction as the diagnosis is possible by several means. Also, these machine learning models need to be tested over large clinical data sets. To provide quality treatment, reduce costs and prevent misdiagnosis of patients, leading to prolonged suffering are the prime motivation of medical diagnostic systems. The work described here provides new direction towards telemonitoring applications. Recognition of disease in a patient accurately is a great challenge in medical field. Huge amount of capital investment is spent for research on medical equipments and advancing primary health care devices as they are largely considered as critical systems.

Also the use of such medical system solves the problem of availability of clinical data in the electronic form for further research as against the use of paper based systems. Some of the major problems confronted while developing an expert diagnosis system such as medical experts are less interested to share acquired skill knowledge with others. The knowledge gained over experience (called common sense) is practically impossible to be separated. And designing an ideal expert system for medical application is difficult as each system is suitable for a specific health disorders due to the unique behavior of algorithms in solving specific problems. The similar diagnostic system can be developed for other chronic diseases.

@&#REFERENCES@&#

