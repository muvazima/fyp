@&#MAIN-TITLE@&#PICO element detection in medical text without metadata: Are first sentences enough?

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Two sets of naive Bayes classifiers were developed for PICO detection.


                        
                        
                           
                           We trained one set with first sentences and the other with all sentences.


                        
                        
                           
                           The first-sentence classifier performs slightly better for patient (P) elements.


                        
                        
                           
                           The all-sentence classifier performs better for intervention (I) elements.


                        
                        
                           
                           The performances are about the same for outcome (O) elements.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Text mining

Information retrieval

Natural language processing

Question answering

Information extraction

Evidence-based medicine

@&#ABSTRACT@&#


               
               
                  Efficient identification of patient, intervention, comparison, and outcome (PICO) components in medical articles is helpful in evidence-based medicine. The purpose of this study is to clarify whether first sentences of these components are good enough to train naive Bayes classifiers for sentence-level PICO element detection. We extracted 19,854 structured abstracts of randomized controlled trials with any P/I/O label from PubMed for naive Bayes classifiers training. Performances of classifiers trained by first sentences of each section (
                        
                           CF
                        
                     ) and those trained by all sentences (
                        
                           CA
                        
                     ) were compared using all sentences by ten-fold cross-validation. The results measured by recall, precision, and F-measures show that there are no significant differences in performance between 
                        
                           CF
                        
                      and 
                        
                           CA
                        
                      for detection of O-element (F-measure=0.731±0.009 vs. 0.738±0.010, p
                     =0.123). However, 
                        
                           CA
                        
                      perform better for I-elements, in terms of recall (0.752±0.012 vs. 0.620±0.007, p
                     <0.001) and F-measures (0.728±0.006 vs. 0.662±0.007, p
                     <0.001). For P-elements, 
                        
                           CF
                        
                      have higher precision (0.714±0.009 vs. 0.665±0.010, p
                     <0.001), but lower recall (0.766±0.013 vs. 0.811±0.012, p
                     <0.001). 
                        
                           CF
                        
                      are not always better than 
                        
                           CA
                        
                      in sentence-level PICO element detection. Their performance varies in detecting different elements.
               
            

@&#INTRODUCTION@&#

Evidence-based medicine or practice (EBM) involves answering clinical questions by analysis of related articles from literature databases such as PubMed [1,2]. Successful EBM application, reliant on abundant research-based evidence combined with clinical expertise [3] and systematic review, can efficiently help clinical decision making. However, raising a question is a must for a meaningful search, and the PICO model is a popular one for clinical problem construction. The model describes a clinical problem using the dimensions of patient/problem (P), intervention (I), comparison (C) and outcome (O).

Although there are some critiques against PICO [4], most authors agreed that this systematic approach is beneficial for EBM [5]. Schardt et al. concluded that asking a good question lays the base for search success, and PICO helps enhance citation retrieval. This approach is more useful when applied to formulating treatment questions than to other types of clinical questions [6].

In the era of information explosion, literature search and critical appraisal require an overwhelmingly heavy workload. This increases the need to utilize machine learning methods to assist with the systematic review process. In practice, some abstracts of these articles are well-structured, making it easier to locate the PICO components. However, a significant portion of the medical literature contains either unstructured or suboptimally structured abstracts. The difficulty in quickly locating the PICO components poses a barrier against EBM applications. Therefore, it would be a great help for EBM if we have effective information retrieval tools for PICO element detection. Much relevant work already exists, led by the seminal study of Demner-Fushman and Lin [7].

In the past, articles manually annotated by human experts were used as the gold standard for classifier training and testing in research on medical category element detection. The downside of the knowledge-based approaches is tedious manual work resulting in small labeled data sets [8]; furthermore, there may be discrepancy among annotators or information searchers [6,9,10].

In this paper, we propose an automated approach to recognize PICO elements in clinical studies with less human expert involvement. This can potentially support the systematic review process, and can be used as a first step in selecting potential targets from medical literature databases such as MEDLINE.

Previous studies have sought to resolve the issues mentioned above and to develop better applicable algorithms and efficient classification methods [11]. Chung brought in rhetorical roles (i.e., Aim, Method, Results and Conclusion), using a sequential framework with Conditional Random Fields (CRFs) to label PICO at the sentence level [12]. Based on this, Boudin et al. trained classifiers with larger data sets [13], and then added term weighting based on location analysis [9]. Most of them retrieved information from a specific portion of data, but a few of them took in the remaining data to perform the classification task. Demner-Fushman and Lin, and Kim et al. [7,14] added an “other” category and collected non-targeted information into it. Kim et al. found that among various criteria, section headings can help detection task. As aforementioned, Boudin et al. compared different complex classifiers focusing on first sentences. They claimed that the majority of essential information is contained in the very first sentence of each section and tagging all sentences in a P/I/O section may not be more pragmatic. Nevertheless, there has previously been no mention of the applicability of first-sentence data sets.

It is arguable that the second and latter sentences also contain important information and can contribute to classifiers performance. In this study, the aim is to determine whether a classifier trained only by the first sentences of explicitly labeled sections, without the labels themselves, is sufficient for PICO element detection at the sentence level in text.

@&#MATERIAL AND METHODS@&#

MEDLINE is a free access database on medical articles. Until June 28, 2012, there were 21,906,254 articles indexed, and 58% of those came with abstracts. According to the MeSH label, 307,122 articles belonged to the randomized controlled trial (RCT) publication type. Among these, 146,152 articles came with a structured abstract, which means that each section is headed by a label.

We manually selected relevant labels for P/I/O components, as listed in Table 1
                        . A total of 19,854 abstracts contain at least one of the P/I/O labels. There are 15,986 abstracts with P-labels, 13,029 with I-labels and 10,778 with O-labels (Table 2
                        ).

In this study, C component was incorporated into “I” category. Since “comparison” can refer to other intervention or no intervention in a RCT [3], it should be covered in the intervention category. In fact, there are very few abstracts with comparison labels found in PubMed. Lastly, most PICO studies practically merge C and I into the same category because they are categorized as one semantic group [5,13,15].

@&#METHODS@&#

The flowchart of this study is shown in Fig. 1
                        . NLTK WordNet stemmer was used for preprocessing, and NLTK naive Bayes classifier was used for classification [16–18]. Naive Bayes classifier, a probabilistic classifier, assumes that each feature is conditionally independent of other features given a conditional event. The criterion of feature selection is based on the word frequency, which was claimed as an efficient method by Yang and Pedersen [19], and also proved to enhance performance of the naive Bayes classifier when used on the same data set [13,20].


                           
                              1
                              The log transform was used with smoothing via pseudocounts.
                           
                           
                              1
                           By the Bayes’ rule, given features f
                           =(f
                           1,…,
                           f
                           
                              n
                           ), the probability of each class c belonging to the positive class c
                           
                              p
                            or the negative class c
                           
                              n
                            could be expressed as
                              
                                 (1)
                                 
                                    P
                                    (
                                    c
                                    |
                                    f
                                    )
                                    =
                                    P
                                    (
                                    c
                                    )
                                    ×
                                    
                                       
                                          P
                                          (
                                          f
                                          |
                                          c
                                          )
                                       
                                       
                                          P
                                          (
                                          f
                                          )
                                       
                                    
                                    ,
                                    
                                    c
                                    ∈
                                    {
                                    
                                       
                                          c
                                       
                                       
                                          p
                                       
                                    
                                    ,
                                    
                                       
                                          c
                                       
                                       
                                          n
                                       
                                    
                                    }
                                    .
                                 
                              
                           Under the assumption of independence, P(f∣c) could be decomposed to
                              
                                 (2)
                                 
                                    P
                                    (
                                    f
                                    |
                                    c
                                    )
                                    =
                                    
                                       
                                          
                                             ∏
                                          
                                          
                                             i
                                             =
                                             1
                                          
                                          
                                             n
                                          
                                       
                                    
                                    P
                                    (
                                    
                                       
                                          f
                                       
                                       
                                          i
                                       
                                    
                                    |
                                    c
                                    )
                                    .
                                 
                              
                           With Eq. (2), the probability of a class c given features f
                           1,…,
                           f
                           
                              n
                            becomes
                              
                                 (3)
                                 
                                    P
                                    (
                                    c
                                    |
                                    f
                                    )
                                    =
                                    P
                                    (
                                    c
                                    )
                                    
                                       
                                          
                                             
                                                ∏
                                             
                                             
                                                i
                                                =
                                                1
                                             
                                             
                                                n
                                             
                                          
                                          P
                                          (
                                          
                                             
                                                f
                                             
                                             
                                                i
                                             
                                          
                                          |
                                          c
                                          )
                                       
                                       
                                          P
                                          (
                                          f
                                          )
                                       
                                    
                                    .
                                 
                              
                           It should be noted that during comparison P(f) is usually ignored because it is common in all classes. Then the instance with features f will be classified into the class c with higher P(c∣f), which can be expressed as
                              
                                 (4)
                                 
                                    arg
                                    
                                       
                                          
                                             max
                                          
                                          
                                             c
                                             ∈
                                             {
                                             
                                                
                                                   c
                                                
                                                
                                                   p
                                                
                                             
                                             ,
                                             
                                                
                                                   c
                                                
                                                
                                                   n
                                                
                                             
                                             }
                                          
                                       
                                    
                                    P
                                    (
                                    c
                                    |
                                    f
                                    )
                                    .
                                 
                              
                           
                        

To clarify whether the classifiers generated from the first sentence of each section were sufficient, two sets of classifiers for P/I/O categories were designed. One classifier set, named as 
                              
                                 CF
                              
                           , contained the classifiers trained only with every first sentence in each section for each P/I/O category (
                              
                                 CF
                                 =
                                 {
                                 
                                    
                                       CF
                                    
                                    
                                       P
                                    
                                 
                                 ,
                                 
                                    
                                       CF
                                    
                                    
                                       I
                                    
                                 
                                 ,
                                 
                                    
                                       CF
                                    
                                    
                                       O
                                    
                                 
                                 }
                              
                           ). Within the abstract, only the sentence following pre-defined labels of a specific category (Table 1) was considered positive in training. The sentences immediately following other labels were considered negative. That means, P/I/O classifications were processed independently. The other classifier set, named as 
                              
                                 CA
                              
                           , contained the classifiers trained with all sentences for each P/I/O category (
                              
                                 CA
                                 =
                                 {
                                 
                                    
                                       CA
                                    
                                    
                                       P
                                    
                                 
                                 ,
                                 
                                    
                                       CA
                                    
                                    
                                       I
                                    
                                 
                                 ,
                                 
                                    
                                       CA
                                    
                                    
                                       O
                                    
                                 
                                 }
                              
                           ). For each specific category, all the sentences under pre-defined labels were regarded as positive ones. All sentences under other labels were considered negative. Therefore, six different classifiers were independently generated in the use of naive Bayes algorithm.

The difference between 
                              
                                 CF
                              
                            and 
                              
                                 CA
                              
                            is demonstrated in Fig. 2
                           . In the example abstract, it can be seen that in the case of 
                              
                                 
                                    
                                       CF
                                    
                                    
                                       P
                                    
                                 
                              
                           , only the first sentences of each section were used for training. Only the first sentence following “SUBJECTS” highlighted in yellow with frame was positive, and other underlined first sentences in red are negative. In contrast, to train 
                              
                                 
                                    
                                       CA
                                    
                                    
                                       P
                                    
                                 
                              
                           , all sentences of these abstracts were used, and both sentences following “SUBJECTS” were positive.

The stemmed words of selected sentences for each classifier were sorted by descending order of their term frequency. The most frequent 200–2000 terms were selected as features in experiments to investigate the effect of increasing amounts of features.

In the training and testing process, ten-fold cross-validation was employed to assess the results statistically. In other words, abstracts selected for specific categories were randomly split into ten equal partitions for training and testing. Nine of them were used for training and the remaining one for testing. This step repeats for ten rounds. The average performance measures thereof were then used for evaluation in terms of precision, recall, and F-measure.

Since each classifier was applied independently in our study, the data set for training and testing was dependent on which classifier was used. For example, P category contains 15,986 articles, and 
                              
                                 
                                    
                                       CF
                                    
                                    
                                       P
                                    
                                 
                              
                            contains 117,400 sentences, and 
                              
                                 
                                    
                                       CA
                                    
                                    
                                       P
                                    
                                 
                              
                            contains 209,689 sentences. In each round for 
                              
                                 
                                    
                                       CF
                                    
                                    
                                       P
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       CA
                                    
                                    
                                       P
                                    
                                 
                              
                           , sentences selected from 9/10 of the articles (about 14,387) were used for training and those of the residual ones were for testing (see Table 2). While applied on testing, positive and negative sentences were defined the same way as in 
                              
                                 CA
                              
                            training, for both 
                              
                                 CF
                              
                            and 
                              
                                 CA
                              
                           . (The data sets are available at http://kimiko.biome.tk/2013_PICO/.)

@&#RESULTS@&#

The performances in terms of precision, recall, and F-measures are reported in this section. For 
                           
                              CF
                           
                        , there were 117,400 sentences belonging to P category, 99,333 for I category, and 80,257 for O category (Table 2). As for 
                           
                              CA
                           
                        , there are 209,689, 174,763, and 141,578 sentences for P, I and O data sets respectively (Table 2). These data sets are larger than those used in previous studies because sentences following non-PICO labels are also included [7,12,14].

The performances of six classifiers with feature counts between 200 and 2000 are charted in Fig. 3
                        . As the number of features increase, the performance improves, while the marginal benefit for feature count decrease.

The performances of 
                           
                              CF
                           
                         and 
                           
                              CA
                           
                         trained by 2000 features are shown in Table 3
                        . For P-element detection, 
                           
                              
                                 
                                    CF
                                 
                                 
                                    P
                                 
                              
                           
                         had higher precision (0.714±0.009 vs. 0.665±0.010, p
                        <0.001) but lower recall (0.766±0.013 vs. 0.811±0.012, p
                        <0.001). The 
                           
                              
                                 
                                    CA
                                 
                                 
                                    I
                                 
                              
                           
                         performed better for I-elements, in terms of recall (0.752±0.012 vs. 0.620±0.007, p
                        <0.001) and F-measures (0.728±0.006 vs. 0.662±0.007, p
                        <0.001). As for O-element detection, there were no significant differences in performance between 
                           
                              
                                 
                                    CF
                                 
                                 
                                    O
                                 
                              
                           
                         and 
                           
                              
                                 
                                    CA
                                 
                                 
                                    O
                                 
                              
                           
                         (F-measures=0.731±0.008 vs. 0.738±0.010, p
                        =0.123).


                        Table 4
                         lists the top and bottom ten features with the highest and lowest odds ratio, determinant of the likelihood of a sentence being positively or negatively identified as a P/I/O sentence. For example, in terms of P-sentence detection, a sentence containing “asa” is more likely to be detected as a positive P-sentence, while a sentence containing “can” is likely to be detected as a negative one. All in all, when a sentence was composed of more high-positive-odds-ratio features used by classifiers of Patient category, it would be more likely to be identified as a P sentence by the classifiers.

Regarding the issue of whether the automatically assigned PICO categories in the test/training set are accurate, we assessed the accuracy of 100 randomly selected abstracts containing all P/I/O headings from the database. After removing all headings, 1305 sentences were annotated by two physicians. The accuracy between manual and automated categorization for P/I/O are 0.980±0.000, 0.966±0.002, and 0.975±0.001, respectively. It indicates that the method could be complementary to manual annotations and save tedious work.

@&#DISCUSSION@&#

Common supervised machine learning algorithms include naive Bayes (NB), support vector machine (SVM) and maximum entropy (ME). They are also commonly used with bag-of-words model in text classification. Wang et al. compared the performance of these algorithms for text classification and reported that the accuracy of ME is slightly better than NB, which is, in turn, better than SVM [21].

Boudin et al. compared some classifier for PICO element detection [9]. In their study, multi-layer perceptron (MLP) is the single best classifier, while there is no clear winner among NB, decision trees, and SVM. For example, the performance of O-component detection in terms of F-measures is 0.557 for MLP and 0.481 for NB. They also suggested combining these classifiers for best performance (f
                        =0.566).

It is true that naive Bayes is based on a strong independent assumption and not the one with best accuracy. We chose it because it is simple, comparable, and with low computational cost. Although we use NB for our study, our methodology should be easily applied to other classifiers.

As shown in Table 3 and 5
                        , P-element is better detected in terms of F-measure and precision by applying 
                           
                              
                                 
                                    CF
                                 
                                 
                                    P
                                 
                              
                           
                        . However, 
                           
                              
                                 
                                    CA
                                 
                                 
                                    P
                                 
                              
                           
                         should be used if better recall is required. It is preferable to use 
                           
                              
                                 
                                    CF
                                 
                                 
                                    P
                                 
                              
                           
                         since its overall performance is better. When the goal is to find as many potential articles as possible to avoid missing data, 
                           
                              
                                 
                                    CA
                                 
                                 
                                    P
                                 
                              
                           
                         can help more.

Regarding I-element detection, classifier performance lagged compared to P- and O-element detection. Between these two classifiers, 
                           
                              
                                 
                                    CA
                                 
                                 
                                    I
                                 
                              
                           
                         is better than 
                           
                              
                                 
                                    CF
                                 
                                 
                                    I
                                 
                              
                           
                         in recall. When the number of features exceeds 1300, no significant difference was observed between 
                           
                              
                                 
                                    CF
                                 
                                 
                                    I
                                 
                              
                           
                         and 
                           
                              
                                 
                                    CA
                                 
                                 
                                    I
                                 
                              
                           
                         in terms of precision (p
                        =0.216). However, the F-measure still supports that 
                           
                              
                                 
                                    CA
                                 
                                 
                                    I
                                 
                              
                           
                         should be adopted no matter how many features are used.

As for O-element detection, there was no significant difference in precision, recall, and F-measure. In this case, 
                           
                              
                                 
                                    CF
                                 
                                 
                                    O
                                 
                              
                           
                         is recommended because of its lower computation cost.

The results indicate that recall would be better for classifiers using all sentences, and precision for those using “first sentences”. The increased recall when utilizing 
                           
                              CA
                           
                         is expected because the first sentence is part of all sentences, and collection with more sentences makes the feature set of 
                           
                              CA
                           
                         less specific than the feature set of 
                           
                              CF
                           
                        .

With such a huge data set, stemming and stop word removal are common in feature preprocessing for dimension reduction [10]. In this study, we made a trial test without stemming words and the results showed no significant difference to stemming. For trimming data quantity and making classification more efficient, it is suggested to use the stemming step. Stop word removal was skipped to prevent any feature loss, although it is believed by some to achieve better result.

Stemming was applied to reduce the data size and improve the efficiency of the classification task. Apply stemming using WordNet stemmer reduces the total number of terms by 7.9% as shown in Table 2.

The impact of stemming was tested on each of 
                              
                                 CF
                              
                            with feature counts up to 1000. There is no significant difference of precision between classifiers trained with or without stemming. The recall of classifiers trained with stemmed data is significantly better only when the feature count is less than or equal to 500. In other words, if sufficient number of features are used, there is no significant difference between the performance of classifiers that use a stemmer and those that do not.

Some authors suggested removal of stop words when simple frequency is used as the criteria to select features [19]. However, the list of stop words may differ across various fields in different areas. We can find some stop words on the list of important features (Table 4). For example, in feature lists of 
                              
                                 CA
                              
                            we can find “whom” (
                              
                                 
                                    
                                       CA
                                    
                                    
                                       P
                                    
                                 
                              
                           , odds ratio
                           =10.2), “then” (
                              
                                 
                                    
                                       CA
                                    
                                    
                                       I
                                    
                                 
                              
                           , odds ratio
                           =13.0) and “before” (
                              
                                 
                                    
                                       CA
                                    
                                    
                                       O
                                    
                                 
                              
                           , odds ratio
                           =2.9). We believe that with a large data set and enough feature numbers, term frequency method used for feature selection can automatically screen out the impact of stop word removal. It is observed in Fig. 3 that the results become stable with increasing number of features. Our results suggest some terms that commonly occur on stopword lists are useful distinguishing features, therefore stoplists seem to be less relevant than they are believed to be.

In this section, we discuss the effect of feature counts to classifier performance and the characteristics of features in classifying sentences into P/I/O categories.

The results of each pair of 
                              
                                 CF
                              
                            and 
                              
                                 CA
                              
                            classifiers for P/O categories are in parallel to some extent. Also, we cannot see a significant difference in I-element detection when the number of features reaches 1300.

In this study, 
                              
                                 CA
                              
                            were also trained with the whole package of words, i.e., more than 30,000 features, of P/I/O categories for comparison. The results show F-measures of 0.731±0.008 for P, 0.737±0.006 for I, and 0.769±0.013 for O category. While the features increased 17–20 times, F-measures increased by 0.0%, 1.2%, and 4.0% for P/I/O, respectively. In consideration of computing cost with little loss in performance, 2000 was chosen as the upper limit in this study.

The most informative features (listed in Table 4) offer some specific information. These features in P category are mostly numbers, implying patients’ age or number, and the remaining ones are verbs relating to the recruiting process (e.g. scheduled, recruited and elective). “Asa” is the top feature identified because it is a popular classification system describing the patient’s physical status [22]. For the I category, the majority of words involve the idea of frequency and time (e.g. then, session, randomly, twice, weekly, and every), and other information-related content (receive, mg, assigned, and microgram). In O category, most features seem to be descriptive words assessing the outcome.

The approach proposed in this study minimizes manual intervention for automated application without a heavy load of preprocessing. In the information retrieval domain, efforts were recently made to reduce human involvement and enhance efficiency of classifiers [15,23]. However, for achieving better results, most researches relied on some human judgment. In our experiment, only the label words are manually defined and other kinds of manual intervention are avoided. While the stemmed terms with top term frequency are tagged as features and fed to naive Bayes classifiers, we demonstrated a more systematic, automated, and reproducible approach to training set extraction from PubMed. Therefore it can be widely applied without tremendous expert involvement.

Our study was based on Boudin’s work, in which the negative cases for each classifier were limited to sentences correctly classified as some other PICO element. We broadened the data set to include sentences under other non-PICO labels and originally expected non-PICO text more likely to be confused with text related to a specific PICO element than PICO text from some other element. However, the results show that ours are not always inferior.

As this might bring in “noise” in the classification task, it is possible that our classifiers underperform because our materials contained data with more noise. Taking P-detection as an example, in their work, the aim was to find 14,279 positive sentences out of 25,768 sentences. In our 
                           
                              
                                 
                                    CF
                                 
                                 
                                    P
                                 
                              
                           
                        , the 15,986 positive sentences were to be found out of 117,400 ones.

The other difference is that their feature words were determined manually. We use term frequency for feature selection, which is a common practice for text classification [21]. They used only the first sentences, like our 
                           
                              CF
                           
                        , because they thought that potentially more important information is in the first sentence.

For P-detection, our 
                           
                              
                                 
                                    CF
                                 
                                 
                                    P
                                 
                              
                           
                         is inferior to Boudin’s MLP (F-measures: 73.9 vs. 85.4), but 
                           
                              
                                 
                                    CF
                                 
                                 
                                    I
                                 
                              
                           
                         and MLP perform as well for I-detection (F-measures: 66.2 vs. 66.3). On the other hand, 
                           
                              
                                 
                                    CF
                                 
                                 
                                    O
                                 
                              
                           
                         performs better for O-detection (F-measures: 73.1 vs. 55.7). Therefore, Boudin’s conservative extraction approach might minimize the noise, but did not always generate better classifiers.

We used naive Bayes classifiers in this study to test whether first sentences were sufficient for P/I/O detection at sentence levels. It is unclear if the same conclusion could be generalized to other classification algorithms. Future work should test the first sentence approach proposed in this study to other types of classifiers to solidify the findings.

Another substantial limitation with the study is that there is no manual review to determine whether sentences identified as containing PICO elements actually contain these elements in answering evidence-based medicine (EBM) questions. We relied on the headings of these structured abstracts, which were created by the original authors of these articles. In other words, we presumed that the original authors had assigned the sentences into the suitable PICO categories. This presumption might not be always true.

This study was based on structured abstracts. However, if used to mine the literature, these classifiers would encounter unstructured abstracts as well. We anticipate slightly inferior performance under such circumstances because PICO text and non-PICO text are more likely to be mixed in one sentence.

@&#CONCLUSIONS@&#

To facilitate systematic review, we propose a reproducible approach of training sentence preparation from PubMed abstracts, and used naive Bayes classifier to evaluate pure bag-of-word features for PICO element detection. Comparison of this study with other recent ones shows that exact pattern match and choosing words sorted by term frequency as features for naive Bayes classifiers is simple and efficient, and minimizes manual intervention.

Our study shows that the very first sentence in a section does not always contain all the essential information for information retrieval. It is found that 
                        
                           CF
                        
                      is indeed more applicable in achieving better precision in P-detection, and is comparable to 
                        
                           CA
                        
                      for O-element detection since it can be efficiently generated. But 
                        
                           CA
                        
                      has higher recall for P/I detection. Based on a combined precision and recall measure (i.e. F-measures), 
                        
                           CF
                        
                      can be used for P detection and 
                        
                           CA
                        
                      can be used for I detection.

In addition to the specific results presented in this manuscript, a further contribution of this work is the generation of a large data set containing PICO labels was semi-automatically generated from PubMed in this study. We have made the data set publicly available, as the means to advance science in this research area.

@&#REFERENCES@&#

