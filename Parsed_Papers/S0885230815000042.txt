@&#MAIN-TITLE@&#Conversational system for information navigation based on POMDP with user focus tracking

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           We address a spoken dialogue system which conducts information navigation.


                        
                        
                           
                           We formulate the problem of dialogue management as a module selection with POMDP.


                        
                        
                           
                           The reward function of POMDP is defined by the quality of interaction.


                        
                        
                           
                           The POMDP tracks user's focus of attention to make appropriate actions.


                        
                        
                           
                           The proposed model outperformed the conventional systems without focus information.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Spoken dialogue system

Dialogue management

Partially observable Markov decision process (POMDP)

Focus in dialogue

@&#ABSTRACT@&#


               
               
                  We address a spoken dialogue system which conducts information navigation in a style of small talk. The system uses Web news articles as an information source, and the user can receive information about the news of the day through interaction. The goal and procedure of this kind of dialogue are not well defined. An empirical approach based on a partially observable Markov decision process (POMDP) has recently been widely used for dialogue management, but it assumes a definite task goal and information slots, which does not hold in our application system. In this work, we formulate the problem of dialogue management as a selection of modules and optimize it with POMDP by tracking the dialogue state and focus of attention. The POMDP-based dialogue manager receives a user intention that is classified by a spoken language understanding (SLU) component based on logistic regression (LR). The manager also receives a user focus that is detected by the SLU component based on conditional random fields (CRFs). These dialogue states are used for selecting appropriate modules by policy function, which is optimized by reinforcement learning. The reward function is defined by the quality of interaction to encourage long interaction of information navigation with users. The module which responds to user queries is based on a similarity of predicate-argument (P-A) structures that are automatically defined from a domain corpus. It allows for flexible response generation even if the system cannot find exact matching information to the user query. The system also proactively presents information by following the user focus and retrieving a news article based on the similarity measure even if the user does not make any utterance. Experimental evaluations with real dialogue sessions demonstrate that the proposed system outperformed the conventional rule-based system in terms of dialogue state tracking and action selection. Effect of focus detection in the POMDP framework is also confirmed.
               
            

@&#INTRODUCTION@&#

In the past decades, a large number of spoken dialogue systems have been investigated. Many systems are now deployed in the real world, most typically as smart phone applications, which interact with a diversity of users. In the future, interactive robots will be deployed as a communication partner of users. However, a large majority of current applications, such as weather information systems (Zue et al., 2000) and train information systems (Aust et al., 1995; Lamel et al., 2002), are based on a specific task description which includes a definite task goal and necessary slots, such as place and date, for task completion. Users are required to share and follow these concepts; they need to have a clear task goal and specify it according to the system's capability. Some recent systems incorporate general question-answering capability, but it is usually limited to factoid questions such as “when” or “how tall”, or pre-defined templates such as “what is your name?”. When users ask something beyond the system's capability, the system replies “I can’t answer the question”, or turns to the Web search and returns the retrieval list in the display. This kind of dialogue is not natural in interaction with humanoid robots since people want to converse with them besides simple commands. A user-friendly conversational system should not reply with “I can’t answer the question” even if it cannot find the result exactly matching the user query. Instead, it should present relevant information according to the user's intention and preference. Moreover, robots do not have a display to present a document. They must make a concise verbal reply.

The goal of this work is a conversational system with speech media only which can engage in information navigation. By information navigation, we do not assume a specific task goal, but assume a domain such as sports and travel. The system should present relevant information even if the user request is not necessarily clear and there is not a matching result to the user query. Moreover, the system can occasionally present potentially useful information even without the user's explicit request by following the dialogue context. In this work, we design and develop a news navigation system that uses Web news articles as a knowledge source and presents information based on the users’ preference and queries.

There are several studies towards this direction (Kawahara, 2009), but there is not a clear principle nor established methodology to design and implement casual conversation systems. Dialogue management of this kind of systems was usually made in a heuristic manner and often based on simple rules (Bratman et al., 1988; Lucas and, 2000; Bohus et al., 2003). The companions project (Catizone et al., 2008; Cavazza et al., 1630) designed conversational agents that would engage elderly users in sustained conversations based on rules. Misu and Kawahara (2007) developed a Kyoto navigation system that conducts question-answering and proactive presentation by defining a topic structure based on Wikipedia articles. The information state approach to dialogue management (Traum and Larsson, 2003; Kronlid and Lager, 2007) allows for dialogue control to put a topic on hold and return to it later. WikiTalk (Wilcock, 2012; Wilcock and Jokinen, 2013) is a dialogue system that talks about topics in Wikipedia. This system works on the pre-defined scenario that is represented with an automaton, but it forces users to follow the system scenario. Moreover, developers need to implement a new scenario for a new domain or task. A data-driven approach based on phrase-based statistical machine translation (SMT) (Ritter et al., 2011) tries to train response generation from micro-blog data. This approach enables the system to output a variety of responses, but it does not track any user intention or dialogue state to fulfil what the user want to know.

In the past years, machine learning, particularly reinforcement learning (RL), has been investigated for dialogue management. Markov decision processes (MDPs) and partially observable Markov decision processes (POMDPs) are the most successful and are now widely used to model and train dialogue managers (Roy et al., 2000; Levin et al., 2000; Williams and Young, 2007; Young et al., 2010; Yoshino et al., 2013b). However, the conventional scheme assumes that the task and dialogue goal are clearly stated and readily encoded in the RL reward function. This is not true in casual conversation or information navigation addressed in this work.

Some previous work has tackled with this problem. Pan et al. (2012) designed a spoken document retrieval system whose goal is user's information need satisfaction, and defined rewards by using the structure of the target document set. This is possible only for well-defined document search problems. The strategy requires a structure of the document set and definition of user demand satisfaction. Shibata et al. (2014) developed a conversational chatting system. It asks users to make evaluation at the end of each dialogue session to define rewards for reinforcement learning. Meguro et al. (2010) proposed a listening dialogue system. In their work, levels of satisfaction were annotated in the log of dialogue sessions to train a discriminative model. These approaches require costly input from users or developers, who provide evaluation and supervision labels. In this work, we present a framework in which reward is defined for the quality of system actions and also for encouraging long interactions, in contrast to the previous approaches. Moreover, user focus is tracked to make appropriate actions, which are more rewarded.

Descriptions of the proposed conversational information navigation system are provided in Section 2. In Section 3, details of dialogue modules based on the predicate-argument (P-A) structure are explained. In Section 4, we describe spoken language understanding (SLU) modules based on logistic regression (LR) and conditional random fields (CRFs). In Section 5, we give a belief explanation of POMDP and its extension by incorporating user focus. Experimental evaluations of the proposed POMDP-based system with dialogue sessions are reported in Section 6.

Information navigation does not assume a designed task and goal, but provides useful information according to the users’ interest. When the user demands are not clear, the system clarifies the user demands through an interaction. The system presents relevant information even if there is not exactly matching result to the user query. Moreover, the system presents potentially useful information even when the user does not make any explicit request.

In natural human–human conversations, participants usually have topics they plan to talk about, and they progress the dialogue in accordance with the topics (Schegloff and Sacks, 1973). An example is shown in Fig. 1
                        . First, the speaker offers a new topic and probes the interest of the listener. If the listener shows interest, the speaker describes details of the topic. If the listener asks a specific question, the speaker answers it. On the other hand, if the listener is not interested in the topic, the speaker avoids the details of that topic and changes the topic. We realize information navigation with this dialogue style.

In the proposed scheme, the system presents topics that it can talk about, describes the details of the current topic, or presents a topic related to the dialogue history when the system has an initiative. The user can explicitly designate the topic of his interest, or request the system to change the topic, or ask a question regarding the current or related topics. The initiative of dialogue comes and goes between the system and the user because it depends on the specification of the user demand. In this kind of dialogue, discourse features such as focus play a critical role on the decision of what to speak (Grosz and Sidner, 1986). The proposed scheme introduces the user focus as one of the parameters of dialogue management.

In this paper, we present a news navigation system (Yoshino et al., 2011) that is one realization of information navigation. The system provides topics collected from Web news texts, and the user gets information according to his interests and queries. Here, a “topic” is content of newspaper articles, and a “domain” is defined as the category of the news. For example, an article that describes the game of New York Yankees is one topic. The domain of the system in this work is baseball, and the system exploits the domain knowledge to conduct effective query matching. While the system proceeds a dialogue by following the today's topics, the user can ask anything, even related to the past events, within the baseball domain, at any time.

An overview of the proposed system is depicted in Fig. 2
                        . The system has seven modules, each of which implements a class of actions. Each module takes as input a recognized user utterance, an analysed predicate-argument (P-A) structure and the detected user intention and focus.

The system begins a dialogue with the “topic presentation ( TP)” module, which presents a new topic selected from news articles. The system chooses the next module based on the user's response. In this work, we assume that each news article corresponds to a single topic, and the system presents a headline of the news in the TP module. If the user shows interest (positive response) in the topic without any specific questions, the system selects the “story telling ( ST)” module to give details of the news. In the ST module, the system provides a summary of the news article by using lead sentences. The system can also provide related topics with the “proactive presentation ( PP)” module. This module is invoked by the system's initiative; this module is not invoked by any user request. If the user asks a specific question regarding the topic, the system switches to the “question answering ( QA)” module to answer the question. This module deals with questions on the presented topic and related topics.

The modules of PP and QA are based on a dialogue framework which uses the similarity of P-A structure (Yoshino et al., 2011) between user queries and news articles, and retrieves or recommends an appropriate sentence from the news articles. This method searches for appropriate information from automatically parsed documents by referring to domain knowledge that is automatically extracted from a domain corpus. The details are described in Section 3.

Transitions between the modules are allowed as shown in Fig. 2. The modules “greeting ( GR)”, “keep silence ( KS)” and “confirmation ( CO)” are also implemented. The GR module generates fixed greeting patterns by using regular expression matching. The CO module makes a confirmation if the system does not have certainty about the user query. In terms of dialogue flow, these modules can be called at any time.

“Focus” in discourse is “attentional state (that) contains information about the objects, properties, relations, and discourse intentions that are most salient at any given point” (Grosz and Sidner, 1986). The user has specific attention to an object if the user utterance contains the focus. In this work, we define the user focus as “the main piece of information of interest to the user.” It makes a central component when making a reply or selecting a relevant topic at the current dialogue state. For example, given “Did Ichiro perform brilliantly?,” user focus is “Ichiro” because the system reply should include information on Ichiro. This information is annotated on content words or named entities in user utterances. In the system proposed in this paper, we parameterize whether any user focus is detected in the user's utterance instead of the focused object.

In this section, we describe flexible matching of P-A structure on which the proposed question answering ( QA) and proactive presentation ( PP) modules are based (Yoshino et al., 2011). Text of news articles and user utterances are parsed to extract a P-A structure (an example is shown in Fig. 3
                     ). A P-A structure represents a sentence with a predicate, arguments and their semantic role labels (Johansson and Nugues, 2008; Hajič et al., 2009; Matsubayashi et al., 2012). We used the Japanese text parser KNP
                        1
                     
                     
                        1
                        
                           http://nlp.ist.i.kyoto-u.ac.jp/index.php?KNP, 2014/11/06
                      (Kawahara and Kurohashi, 2006) to obtain a P-A structure, a structure of the predicate phrase and a set of the argument phrases that depend on the predicate. In the example, the sentence “Did Ichiro hit a home-run” is parsed as, arguments, “Ichiro (subject)” and “home-run (object)”, depending on the predicate “hit” with respective semantic roles. This structure is a classic concept in natural language processing, but recently automatic semantic parsing has reached a practical level thanks to corpus-based learning techniques (Kawahara and Kurohashi, 2006).

In the QA module, the system searches for a sentence that matches the user utterance in the P-A structure. When the system fails to find exact information that matches the user query, it tries to find relevant sentences. In the PP module, when the user does not speak for a while, the system tries to make proactive information presentation. It is based on partially matched entries of the current or latest query. The fall-back is similar to collaborative response generation in the conversational spoken dialogue systems (Sadek, 1999), but it is intended for proactive information presentation using general documents of news articles.

We describe the response generation used in the QA module. For preference among multiple components in the P-A structure of the user query, we define a significance score based on Naive Bayes. The score is used when relaxing search queries in the QA module, as a weight of the score in the PP module, and as a feature of spoken language understanding. The score is a probability of phrase p
                        
                           i
                         (either predicate or argument) being in the document set (news articles in this work) of a particular domain D (baseball in this work) by assuming the other set of documents 
                           
                              D
                              ¯
                           
                         of different domains,


                        
                           
                              (1)
                              
                                 P
                                 (
                                 D
                                 |
                                 
                                    p
                                    i
                                 
                                 )
                                 =
                                 
                                    
                                       P
                                       (
                                       
                                          p
                                          i
                                       
                                       |
                                       D
                                       )
                                       ×
                                       P
                                       (
                                       D
                                       )
                                    
                                    
                                       P
                                       (
                                       
                                          p
                                          i
                                       
                                       )
                                    
                                 
                              
                           
                        
                        
                           
                              (2)
                              
                                 ≃
                                 
                                    
                                       C
                                       (
                                       
                                          p
                                          i
                                       
                                       ,
                                       D
                                       )
                                       +
                                       P
                                       (
                                       D
                                       )
                                       ×
                                       γ
                                    
                                    
                                       C
                                       (
                                       
                                          p
                                          i
                                       
                                       )
                                       +
                                       γ
                                    
                                 
                                 ,
                              
                           
                        where C(·) stands for a word count and P(D) is a normalization factor determined by the size of D and 
                           
                              D
                              ¯
                           
                        . γ is a smoothing factor estimated with a Dirichlet prior using the Chinese Restaurant Process (CRP). The score is calculated for every element of P-A structures.

The proposed scheme of partial matching is shown in Fig. 3. First, the module searches for the exactly matched information. If it fails, the module searches for partially matched information. Specifically, we relax (ignore) the component of the least significance score, then search for relevant information. If any entry is not still matched, we relax the next less significant component. If multiple entries are found with this matching, the system selects the most relevant entry by using the relevance measure of the predicate or arguments. The relevance measure of an argument is based on the co-occurrence of words in a document. The relevance of argument phrase 
                           
                              p
                              
                                 
                                    arg
                                    i
                                 
                              
                           
                         and 
                           
                              p
                              
                                 
                                    arg
                                    j
                                 
                              
                           
                         is defined as,


                        
                           
                              (3)
                              
                                 
                                    R
                                    arg
                                 
                                 
                                    
                                       
                                          
                                             p
                                             
                                                
                                                   arg
                                                   i
                                                
                                             
                                          
                                          ,
                                          
                                             p
                                             
                                                
                                                   arg
                                                   j
                                                
                                             
                                          
                                       
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      C
                                                      
                                                         
                                                            
                                                               
                                                                  p
                                                                  
                                                                     
                                                                        arg
                                                                        i
                                                                     
                                                                  
                                                               
                                                               ,
                                                               
                                                                  p
                                                                  
                                                                     
                                                                        arg
                                                                        j
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                             2
                                          
                                       
                                       
                                          C
                                          
                                             
                                                
                                                   
                                                      p
                                                      
                                                         
                                                            arg
                                                            i
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                          ×
                                          C
                                          
                                             
                                                
                                                   
                                                      p
                                                      
                                                         
                                                            arg
                                                            j
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        Here, 
                           
                              p
                              
                                 
                                    arg
                                    i
                                 
                              
                           
                         is in the original query and relaxed (ignored) in the partial matching, and 
                           
                              p
                              
                                 
                                    arg
                                    j
                                 
                              
                           
                         of the best relevance score is retrieved for response generation.

The relevance measure of a predicate is based on the distributional similarity (Harris, 1951; Pantel et al., 2009) of a vector of arguments that depend on the predicate. The relevance measure of predicates is calculated by cosine similarity,


                        
                           
                              (4)
                              
                                 
                                    R
                                    pred
                                 
                                 
                                    
                                       
                                          
                                             p
                                             
                                                
                                                   pred
                                                   i
                                                
                                             
                                          
                                          ,
                                          
                                             p
                                             
                                                
                                                   pred
                                                   j
                                                
                                             
                                          
                                       
                                    
                                 
                                 =
                                 cos
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      p
                                                      
                                                         
                                                            pred
                                                            i
                                                         
                                                      
                                                   
                                                
                                                →
                                             
                                          
                                          ,
                                          
                                             
                                                
                                                   
                                                      p
                                                      
                                                         
                                                            pred
                                                            j
                                                         
                                                      
                                                   
                                                
                                                →
                                             
                                          
                                       
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   p
                                                   
                                                      
                                                         pred
                                                         i
                                                      
                                                   
                                                
                                             
                                             →
                                          
                                       
                                       ·
                                       
                                          
                                             
                                                
                                                   p
                                                   
                                                      
                                                         pred
                                                         j
                                                      
                                                   
                                                
                                             
                                             →
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            p
                                                            
                                                               
                                                                  pred
                                                                  i
                                                               
                                                            
                                                         
                                                      
                                                      →
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            p
                                                            
                                                               
                                                                  pred
                                                                  j
                                                               
                                                            
                                                         
                                                      
                                                      →
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                                 ,
                              
                           
                        where 
                           
                              
                                 
                                    
                                       p
                                       
                                          
                                             pred
                                             i
                                          
                                       
                                    
                                 
                                 →
                              
                           
                           =
                           (
                           C
                           (
                           
                              p
                              
                                 
                                    arg
                                    1
                                 
                                 &
                                 
                                    srl
                                    1
                                 
                              
                           
                           )
                           ,
                           …
                           ,
                           C
                           (
                           
                              p
                              
                                 
                                    arg
                                    n
                                 
                                 &
                                 
                                    srl
                                    m
                                 
                              
                           
                           )
                           )
                        .

Here, 
                           
                              p
                              
                                 
                                    pred
                                    i
                                 
                              
                           
                         is the original predicate and 
                           
                              p
                              
                                 
                                    pred
                                    j
                                 
                              
                           
                         is the compared predicate. arg
                           n
                        
                        &srl
                           m
                         is an argument phrase 
                           
                              p
                              
                                 
                                    arg
                                    n
                                 
                              
                           
                         of the semantic role srl
                           m
                        .

The PP module also uses the significance score and the relevance measure of the predicate and arguments. The PP module selects information that is related to the dialogue history. The sentence-level relevance measure is defined as,


                        
                           
                              (5)
                              
                                 
                                    R
                                    s
                                 
                                 (
                                 
                                    u
                                    i
                                 
                                 ,
                                 
                                    u
                                    v
                                 
                                 )
                                 =
                                 
                                    R
                                    pred
                                 
                                 
                                    
                                       
                                          
                                             p
                                             
                                                
                                                   pred
                                                   i
                                                
                                             
                                          
                                          ,
                                          
                                             p
                                             
                                                
                                                   pred
                                                   v
                                                
                                             
                                          
                                       
                                    
                                 
                                 ×
                                 
                                    
                                       P
                                       
                                          
                                             
                                                D
                                                |
                                                
                                                   p
                                                   
                                                      
                                                         pred
                                                         i
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       +
                                       P
                                       
                                          
                                             
                                                D
                                                |
                                                
                                                   p
                                                   
                                                      
                                                         pred
                                                         v
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    2
                                 
                                 +
                                 
                                    ∑
                                    j
                                 
                                 
                                    R
                                    arg
                                 
                                 
                                    
                                       
                                          
                                             p
                                             
                                                
                                                   arg
                                                   j
                                                
                                             
                                          
                                          ,
                                          
                                             p
                                             
                                                
                                                   arg
                                                   v
                                                
                                             
                                          
                                       
                                    
                                 
                                 ×
                                 
                                    
                                       P
                                       
                                          
                                             
                                                D
                                                |
                                                
                                                   p
                                                   
                                                      
                                                         arg
                                                         j
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       +
                                       P
                                       
                                          
                                             
                                                D
                                                |
                                                
                                                   p
                                                   
                                                      
                                                         arg
                                                         v
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    2
                                 
                                 .
                              
                           
                        
                     

Here, u
                        
                           i
                         is a sentence in the dialogue history and 
                           
                              u
                              v
                           
                         is a candidate information to present. p
                        pred and p
                        arg are a predicate and arguments, included in u
                        
                           i
                         and 
                           
                              u
                              v
                           
                        . R
                        pred(·) and R
                        arg(·) are relevance of a predicate and arguments. The scores P(D|p
                        pred) and P(D|p
                        arg) are included to weigh the important phrases. The relevance measure is defined to select the related information on the basis of an assumption that the priority order depends on the importance in the domain. The system considers the dialogue history of both the user and the system, and calculates the dialogue-level relevance measure by using the latest h utterances,


                        
                           
                              (6)
                              
                                 
                                    R
                                    pp
                                 
                                 =
                                 
                                    ∑
                                    
                                       i
                                       ∈
                                       h
                                    
                                 
                                 
                                    R
                                    s
                                 
                                 (
                                 
                                    u
                                    i
                                 
                                 ,
                                 
                                    u
                                    v
                                 
                                 )
                                 .
                              
                           
                        In this work we set h
                        =2.

The PP module presents information that has the highest score of Eq. (6).

In this section, we present the spoken language understanding (SLU) components of our system. It detects the user's focus and intention and provides them to the dialogue manager. The SLU modules are formulated with a statistical model to give likelihoods which are used in POMDP.

We collected 606 utterances (from 10 users) with a rule-based dialogue system (Yoshino et al., 2011), and 312 utterances (from 8 users) with a preliminary statistical-based dialogue system which was constructed by using the data collected with the rule-based system. An example of annotation is shown in Fig. 4
                        . We highlighted annotation points in the bold font.

To prepare the training data, each utterance was labelled with one of the six modules, indicating the best module to respond. In addition, each phrase or P-A element is labelled whether it is the user's focus or not. The user focus is determined by the attributes (=specifications of words in the domain) and preference order of phrases to identify the most appropriate information that the user wants to know. For example, in the second user utterance in Fig. 4, the user focus is on the phrase “the Giants’ manager” instead of “Matsui” as the user's interest is whether Matsui will be a manager.

To detect the user focus, we use a conditional random field (CRF). 
                           2
                        
                        
                           2
                           CRFsuite (Okazaki, 2007).
                         The problem is defined as a sequential labelling of the focus labels to a sequence of the phrases of the user utterance. Features used are listed in Table 1
                        . ORDER features are the order of the phrase in the sequence and in the P-A structure. We incorporate these features because the user focus often appears in the first phrase of the user utterance. POS features are part-of-speech (POS) tags and their pairs in the phrase. P-A features are the semantic role of the P-A structure.

We also incorporate the P-A significance score defined in Section 3 ( P-A Score). The score is discretized to 0.01, 0.02, 0.05, 0.1, 0.2, and 0.5.


                        Table 2
                         shows the accuracy of user focus detection, which was conducted via five-fold cross-validation of the entire training data. “Phrase” is phrase-level accuracy and “sentence” indicates whether the presence of any user focus phrase was correctly detected, regardless of whether the correct phrase was identified. This table shows that WORD features are effective for detecting the user focus, but they are not essential in the sentence-level accuracy. For portability across domains, we adopt the sentence-level focus information, and do not use the WORD features.

The CRF gives a probability of whether any user focus is detected for each phrase h
                        
                           l
                        . The probability of focus existence o
                        
                           f
                         for a sentence h is calculated by combining the sequence of probabilities of user focus label 0;


                        
                           
                              (7)
                              
                                 P
                                 (
                                 
                                    o
                                    f
                                 
                                 |
                                 h
                                 )
                                 =
                                 1
                                 −
                                 
                                    ∏
                                    l
                                 
                                 P
                                 
                                    
                                       
                                          
                                             o
                                             
                                                
                                                   f
                                                   l
                                                
                                             
                                          
                                          =
                                          0
                                          |
                                          
                                             h
                                             l
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

The module classifies the user intention from the user utterance. We define six intentions as below.
                           
                              •
                              
                                 TP: request to the TP module.


                                 ST: request to the ST module.


                                 QA: request to the QA module.


                                 GR: greeting to the GR module.


                                 NR: silence longer than a threshold.


                                 II: irrelevant input due to ASR errors or noise.

We adopt the logistic regression (LR)-based dialogue act tagging approach (Tur et al., 2006; Higashinaka et al., 2006). The probability of user intention o
                        
                           s
                         given an ASR result of the user utterance h is defined as,


                        
                           
                              (8)
                              
                                 P
                                 (
                                 
                                    o
                                    s
                                 
                                 |
                                 h
                                 )
                                 =
                                 
                                    
                                       exp
                                       (
                                       ω
                                       ·
                                       ϕ
                                       (
                                       h
                                       ,
                                       
                                          o
                                          s
                                       
                                       )
                                       )
                                    
                                    
                                       
                                          ∑
                                          i
                                       
                                       
                                          exp
                                          (
                                          ω
                                          ·
                                          ϕ
                                          (
                                          h
                                          ,
                                          
                                             o
                                             
                                                s
                                                ,
                                                i
                                             
                                          
                                          )
                                          )
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

Here, ϕ(h, o
                        
                           s
                        ) is a feature vector and ω is a feature weight. We use POS, P-A and P-A score as a feature set. In addition, we add a typical expression feature (TYPICAL) to classify TP, ST or GR tags. For example, typical expressions in conversations are “Hello” or “Go on,” and those in information navigation are “News of the day” or “Tell me in detail.” Backchannel of the user is a sign for the system to continue the explanation. We add the binary feature of backchannels (BC). Features for the classifier are listed in Table 3
                        .

The classification accuracy in five-fold cross-validation is shown in Table 4
                        . The result for no request (NR) is not shown here, because the state is invoked by a prefixed time interval (5s in this system). The TYPICAL feature improves the classification accuracy especially for ST while keeping the domain portability.

ASR and the intention analysis involve errors. Here, s is a true user intention and o
                        
                           s
                         is an observed intention. The observation model P(o
                        
                           s
                        |s) is given by the confidence measure score of the ASR result P(h|u) (Komatani and Kawahara, 2000) and the likelihood of the intention analysis P(o
                        
                           s
                        |h). The ASR confidence measure is a smoothed posterior probability given N-best hypotheses.


                        
                           
                              (9)
                              
                                 P
                                 (
                                 
                                    o
                                    s
                                 
                                 |
                                 s
                                 )
                                 =
                                 
                                    ∑
                                    h
                                 
                                 
                                    P
                                    (
                                    
                                       o
                                       s
                                    
                                    ,
                                    h
                                    |
                                    s
                                    )
                                 
                              
                           
                        
                        
                           
                              (10)
                              
                                 ≈
                                 
                                    ∑
                                    h
                                 
                                 
                                    P
                                    (
                                    
                                       o
                                       s
                                    
                                    |
                                    h
                                    )
                                    P
                                    (
                                    h
                                    |
                                    u
                                    )
                                 
                                 .
                              
                           
                        
                     

Here, u is an utterance of the user. We combine the N-best (N
                        =5) hypotheses of the ASR result h.

The user focus detection also needs to consider ASR errors. The probability of user focus is given by the likelihood of the ASR result P(h|u) and the likelihood of the user focus detection P(o
                        
                           f
                        |h),
                           
                              (11)
                              
                                 P
                                 (
                                 
                                    o
                                    f
                                 
                                 |
                                 f
                                 )
                                 =
                                 
                                    ∑
                                    h
                                 
                                 
                                    P
                                    (
                                    
                                       o
                                       f
                                    
                                    ,
                                    h
                                    |
                                    f
                                    )
                                 
                              
                           
                        
                        
                           
                              (12)
                              
                                 ≈
                                 
                                    ∑
                                    h
                                 
                                 
                                    P
                                    (
                                    
                                       o
                                       f
                                    
                                    |
                                    h
                                    )
                                    P
                                    (
                                    h
                                    |
                                    u
                                    )
                                 
                                 .
                              
                           
                        
                     

The POMDP-based statistical dialogue management is formulated as below. The random variables involved at a dialogue turn t are as follows:
                        
                           •
                           
                              s
                              ∈
                              I
                              
                                 s
                              : user state

User intention.


                              a
                              ∈
                              K: system action

Module that the system selects.


                              o
                              ∈
                              I
                              
                                 s
                              : observation

Observed user state, including ASR and intention analysis errors.


                              P(o|s): observation probability

Output of SLU with its confidence score, which is defined in Eqs. (10) and (12).


                              
                                 P
                                 (
                                 
                                    s
                                    j
                                    
                                       t
                                       +
                                       1
                                    
                                 
                                 |
                                 
                                    s
                                    i
                                    t
                                 
                                 ,
                                 
                                    
                                       
                                          a
                                          ˆ
                                       
                                    
                                    k
                                    t
                                 
                                 )
                              : state transition probability

Model to predict the next user state 
                                 
                                    s
                                    j
                                    
                                       t
                                       +
                                       1
                                    
                                 
                               given the previous user state 
                                 
                                    s
                                    i
                                    t
                                 
                               and system action 
                                 
                                    
                                       
                                          a
                                          ˆ
                                       
                                    
                                    k
                                    t
                                 
                              .


                              
                                 
                                    b
                                    
                                       
                                          s
                                          i
                                       
                                    
                                 
                                 =
                                 P
                                 (
                                 
                                    s
                                    i
                                 
                                 |
                                 
                                    o
                                    
                                       1
                                       :
                                       t
                                    
                                 
                                 )
                              : belief

Stochastic variable of the user state.


                              π: policy function

This function determines a system action a given a belief of user b. π
                              * is the optimal policy function that is acquired by the training.


                              r: reward function

This function gives a reward to a pair of the user state s and the system action a.

The aim of the statistical dialogue management is to output an optimal system action 
                        
                           
                              
                                 a
                                 ˆ
                              
                           
                           t
                        
                      given a sequence of observation o
                     1:t
                      from 1 to t time-steps.

Next, we give the belief update that includes the observation and state transition probabilities. The belief update of user state s
                     
                        i
                      in time-step t is defined as,


                     
                        
                           (13)
                           
                              
                                 b
                                 
                                    
                                       s
                                       j
                                       
                                          t
                                          +
                                          1
                                       
                                    
                                 
                                 
                                    t
                                    +
                                    1
                                 
                              
                              ∝
                              P
                              (
                              
                                 o
                                 
                                    t
                                    +
                                    1
                                 
                              
                              |
                              
                                 s
                                 j
                                 
                                    t
                                    +
                                    1
                                 
                              
                              )
                              
                                 ∑
                                 
                                    
                                       s
                                       i
                                    
                                 
                              
                              P
                              (
                              
                                 s
                                 j
                                 
                                    t
                                    +
                                    1
                                 
                              
                              |
                              
                                 s
                                 i
                                 t
                              
                              ,
                              
                                 
                                    
                                       
                                          
                                             a
                                             k
                                          
                                       
                                       ˆ
                                    
                                 
                                 t
                              
                              )
                              
                                 b
                                 
                                    
                                       s
                                       i
                                    
                                 
                                 t
                              
                              .
                           
                        
                     
                  

The state transition probability 
                        P
                        (
                        
                           s
                           j
                           
                              t
                              +
                              1
                           
                        
                        |
                        
                           s
                           i
                           t
                        
                        ,
                        
                           
                              
                                 
                                    
                                       a
                                       k
                                    
                                 
                                 ˆ
                              
                           
                           t
                        
                        )
                      is calculated from the training corpus annotated with user states s and system actions a. Once the system estimates the belief 
                        
                           b
                           
                              
                                 s
                                 i
                              
                           
                           t
                        
                     , the policy function outputs the optimal action 
                        
                           a
                           ˆ
                        
                      as follows:


                     
                        
                           (14)
                           
                              
                                 
                                    a
                                    ˆ
                                 
                              
                              =
                              
                                 π
                                 *
                              
                              (
                              b
                              )
                              .
                           
                        
                     
                  

We applied Q-learning (Monahan, 1982; Watkins and Dayan, 1992) to acquire the optimal policy π
                        *. Q-learning relies on the estimation of a Q-function, which maximizes the discounted sum of future rewards of the system action a
                        
                           t
                         at a dialogue turn t given the current belief b
                        
                           t
                        . Q-learning is performed by iterative updates on the training dialogue data:
                           
                              (15)
                              
                                 Q
                                 (
                                 
                                    b
                                    t
                                 
                                 ,
                                 
                                    a
                                    t
                                 
                                 )
                                 ⇐
                                 (
                                 1
                                 −
                                 ɛ
                                 )
                                 Q
                                 (
                                 
                                    b
                                    t
                                 
                                 ,
                                 
                                    a
                                    t
                                 
                                 )
                                 +
                                 ɛ
                                 
                                    
                                       
                                          R
                                          (
                                          
                                             s
                                             t
                                          
                                          ,
                                          
                                             a
                                             t
                                          
                                          )
                                          +
                                          γ
                                          
                                             max
                                             
                                                
                                                   a
                                                   
                                                      t
                                                      +
                                                      1
                                                   
                                                
                                             
                                          
                                          Q
                                          (
                                          
                                             b
                                             
                                                t
                                                +
                                                1
                                             
                                          
                                          ,
                                          
                                             a
                                             
                                                t
                                                +
                                                1
                                             
                                          
                                          )
                                       
                                    
                                 
                                 ,
                              
                           
                        where ɛ is a learning rate and γ is a discount factor of a future reward. We experimentally decided ɛ
                        =0.01 and γ
                        =0.9. The optimal policy given by the Q-function is determined as,


                        
                           
                              (16)
                              
                                 
                                    π
                                    *
                                 
                                 (
                                 b
                                 )
                                 =
                                 
                                    argmax
                                    a
                                 
                                 Q
                                 (
                                 b
                                 ,
                                 a
                                 )
                                 .
                              
                           
                        
                     

However, it is impossible to calculate the Q-function for all possible real values of belief b. Thus, we train a limited Q-function given by a grid-based value iteration (Bonet, 2002). The belief is given by a function,
                           
                              (17)
                              
                                 
                                    b
                                    s
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   η
                                                
                                                
                                                   
                                                      
                                                         if
                                                      
                                                   
                                                   
                                                   
                                                   s
                                                   =
                                                   
                                                      s
                                                      i
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         1
                                                         −
                                                         η
                                                      
                                                      
                                                         |
                                                         
                                                            I
                                                            s
                                                         
                                                         |
                                                         −
                                                         1
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         if
                                                      
                                                   
                                                   
                                                   
                                                   s
                                                   ≠
                                                   
                                                      s
                                                      i
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

Here, η is an approximated likelihood of s
                        =
                        s
                        
                           i
                        , that is the best hypothesis of SLU is s
                        
                           i
                        . It is quantized into 11 discrete points from 0.0 to 1.0 by 0.1. For example, if η is 0.6, s
                        
                           i
                         is s
                        1 and there are four user states, the function selects a point of 
                           
                              b
                              
                                 
                                    s
                                    1
                                 
                              
                           
                           =
                           0.600
                        , 
                           
                              b
                              
                                 
                                    s
                                    2
                                 
                              
                           
                           =
                           0.133
                        , 
                           
                              b
                              
                                 
                                    s
                                    3
                                 
                              
                           
                           =
                           0.133
                        , 
                           
                              b
                              
                                 
                                    s
                                    4
                                 
                              
                           
                           =
                           0.133
                        . We also added the case of uniform distribution, that is 
                           
                              b
                              
                                 
                                    s
                                    i
                                 
                              
                           
                           =
                           0.25
                         for all four user states.

The proposed POMDP-based dialogue management refers two kinds of belief information: the user intention s and the user focus f (0 or 1). Accordingly, two observations come from SLU: result of user intention analysis o
                        
                           s
                         and result of user focus detection o
                        
                           f
                        .

The equation of the belief update (Eq. (13)) is extended by introducing the previous focus 
                           
                              f
                              l
                              t
                           
                         and current focus 
                           
                              f
                              m
                              
                                 t
                                 +
                                 1
                              
                           
                        ,


                        
                           
                              (18)
                              
                                 
                                    b
                                    
                                       
                                          s
                                          j
                                          
                                             t
                                             +
                                             1
                                          
                                       
                                       ,
                                       
                                          f
                                          m
                                          
                                             t
                                             +
                                             1
                                          
                                       
                                    
                                    
                                       t
                                       +
                                       1
                                    
                                 
                                 =
                                 P
                                 (
                                 
                                    o
                                    s
                                    
                                       t
                                       +
                                       1
                                    
                                 
                                 ,
                                 
                                    o
                                    f
                                    
                                       t
                                       +
                                       1
                                    
                                 
                                 |
                                 
                                    s
                                    j
                                    
                                       t
                                       +
                                       1
                                    
                                 
                                 ,
                                 
                                    f
                                    m
                                    
                                       t
                                       +
                                       1
                                    
                                 
                                 )
                                 
                                    ∑
                                    i
                                 
                                 
                                    ∑
                                    l
                                 
                                 P
                                 (
                                 
                                    s
                                    j
                                    
                                       t
                                       +
                                       1
                                    
                                 
                                 ,
                                 
                                    f
                                    m
                                    
                                       t
                                       +
                                       1
                                    
                                 
                                 |
                                 
                                    s
                                    i
                                    t
                                 
                                 ,
                                 
                                    f
                                    l
                                    t
                                 
                                 ,
                                 
                                    
                                       
                                          
                                             
                                                a
                                                k
                                             
                                          
                                          ˆ
                                       
                                    
                                    t
                                 
                                 )
                                 
                                    b
                                    
                                       
                                          s
                                          i
                                          t
                                       
                                       ,
                                       
                                          f
                                          l
                                          t
                                       
                                    
                                    t
                                 
                                 .
                              
                           
                        
                     

The observation probability is approximated as,


                        
                           
                              (19)
                              
                                 
                                    
                                       
                                          P
                                          (
                                          
                                             o
                                             s
                                          
                                          ,
                                          
                                             o
                                             f
                                          
                                          |
                                          
                                             s
                                             j
                                          
                                          ,
                                          
                                             f
                                             m
                                          
                                          )
                                       
                                       
                                          =
                                          P
                                          (
                                          
                                             o
                                             s
                                          
                                          |
                                          
                                             o
                                             f
                                          
                                          ,
                                          
                                             s
                                             j
                                          
                                          ,
                                          
                                             f
                                             m
                                          
                                          )
                                          P
                                          (
                                          
                                             o
                                             f
                                          
                                          |
                                          
                                             s
                                             j
                                          
                                          ,
                                          
                                             f
                                             m
                                          
                                          )
                                       
                                    
                                    
                                       
                                       
                                          ≈
                                          P
                                          (
                                          
                                             o
                                             s
                                          
                                          |
                                          
                                             s
                                             j
                                          
                                          )
                                          P
                                          (
                                          
                                             o
                                             f
                                          
                                          |
                                          
                                             f
                                             m
                                          
                                          )
                                          .
                                       
                                    
                                 
                              
                           
                        
                     

Here, we assume that information of user focus f
                        
                           m
                         and o
                        
                           f
                         does not affect the observation of the user state o
                        
                           s
                        , and the user intention s
                        
                           j
                         does not affect the observation of the user focus o
                        
                           f
                        . These probabilities are calculated from the probability of the user intention analysis (Eq. (10)) and the probability of the user focus detection (Eq. (12)). The resultant trained policy is,


                        
                           
                              (20)
                              
                                 
                                    
                                       a
                                       ˆ
                                    
                                 
                                 =
                                 
                                    π
                                    *
                                 
                                 (
                                 
                                    b
                                    t
                                 
                                 )
                                 =
                                 
                                    π
                                    *
                                 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   b
                                                   
                                                      
                                                         s
                                                         i
                                                      
                                                      ,
                                                      
                                                         f
                                                         l
                                                      
                                                   
                                                   t
                                                
                                             
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

To train the policy, the equation of Q-learning (Eq. (15)) is modified as,


                        
                           
                              (21)
                              
                                 Q
                                 (
                                 
                                    b
                                    t
                                 
                                 ,
                                 
                                    a
                                    t
                                 
                                 )
                                 ⇐
                                 (
                                 1
                                 −
                                 ɛ
                                 )
                                 Q
                                 (
                                 
                                    b
                                    t
                                 
                                 ,
                                 
                                    a
                                    t
                                 
                                 )
                                 +
                                 ɛ
                                 
                                    
                                       
                                          R
                                          (
                                          
                                             s
                                             t
                                          
                                          ,
                                          
                                             f
                                             t
                                          
                                          ,
                                          
                                             a
                                             t
                                          
                                          )
                                          +
                                          γ
                                          
                                             max
                                             
                                                
                                                   a
                                                   
                                                      t
                                                      +
                                                      1
                                                   
                                                
                                             
                                          
                                          Q
                                          (
                                          
                                             b
                                             
                                                t
                                                +
                                                1
                                             
                                          
                                          ,
                                          
                                             a
                                             
                                                t
                                                +
                                                1
                                             
                                          
                                          )
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

The state transition probability in Eq. (18) is developed as,


                        
                           
                              (22)
                              
                                 P
                                 (
                                 
                                    s
                                    j
                                    
                                       t
                                       +
                                       1
                                    
                                 
                                 ,
                                 
                                    f
                                    m
                                    
                                       t
                                       +
                                       1
                                    
                                 
                                 |
                                 
                                    s
                                    i
                                    t
                                 
                                 ,
                                 
                                    f
                                    l
                                    t
                                 
                                 ,
                                 
                                    
                                       
                                          
                                             
                                                a
                                                k
                                             
                                          
                                          ˆ
                                       
                                    
                                    t
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          P
                                          (
                                          
                                             s
                                             j
                                             
                                                t
                                                +
                                                1
                                             
                                          
                                          ,
                                          |
                                          
                                             f
                                             m
                                             
                                                t
                                                +
                                                1
                                             
                                          
                                          ,
                                          
                                             s
                                             i
                                             t
                                          
                                          ,
                                          
                                             f
                                             l
                                             t
                                          
                                          ,
                                          
                                             
                                                
                                                   
                                                      
                                                         a
                                                         k
                                                      
                                                   
                                                   ˆ
                                                
                                             
                                             t
                                          
                                          )
                                       
                                       ︸
                                    
                                    
                                       
                                          
                                             state
                                                
                                             model
                                          
                                       
                                    
                                 
                                 
                                    
                                       
                                          P
                                          (
                                          
                                             f
                                             m
                                             
                                                t
                                                +
                                                1
                                             
                                          
                                          |
                                          
                                             s
                                             i
                                             t
                                          
                                          ,
                                          
                                             f
                                             l
                                             t
                                          
                                          ,
                                          
                                             
                                                
                                                   
                                                      
                                                         a
                                                         k
                                                      
                                                   
                                                   ˆ
                                                
                                             
                                             t
                                          
                                          )
                                       
                                       ︸
                                    
                                    
                                       
                                          
                                             focus
                                                
                                             model
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

Thus, the observations o
                        
                           s
                         and o
                        
                           f
                         are controlled by hidden states f and s that are decided by the state transition probabilities,


                        
                           
                              (23)
                              
                                 
                                    
                                       
                                          focus
                                             
                                          model
                                       
                                    
                                 
                                 =
                                 P
                                 (
                                 
                                    f
                                    
                                       t
                                       +
                                       1
                                    
                                 
                                 |
                                 
                                    f
                                    t
                                 
                                 ,
                                 
                                    s
                                    t
                                 
                                 ,
                                 
                                    a
                                    t
                                 
                                 )
                                 ,
                              
                           
                        
                        
                           
                              (24)
                              
                                 
                                    
                                       
                                          state
                                             
                                          model
                                       
                                    
                                 
                                 =
                                 P
                                 (
                                 
                                    s
                                    
                                       t
                                       +
                                       1
                                    
                                 
                                 |
                                 
                                    f
                                    
                                       t
                                       +
                                       1
                                    
                                 
                                 ,
                                 
                                    f
                                    t
                                 
                                 ,
                                 
                                    s
                                    t
                                 
                                 ,
                                 
                                    a
                                    t
                                 
                                 )
                                 .
                              
                           
                        
                     

A graphical model of the proposed model is shown in Fig. 5
                        .

We constructed a user simulator based on these probabilities that are calculated from the annotated data described in Section 4.1 with the maximum likelihood estimation and Dirichlet smoothing. The user simulator outputs a user state and a user focus with an accordance of the probabilities of Eqs. (23) and (24).

We introduce two kinds of rewards. The first is turn-based dialogue control reward r
                        C, that rewards the system for appropriate or acceptable actions to the user. The system has another parameter, frustration point, that will terminate the dialogue when accumulated. Table 5
                         defines a reward list at the end of each turn. The reward of +1 is given to appropriate actions, 0 to acceptable actions, and −1 to inappropriate actions. In Table 5, pairs of a state and its apparently corresponding action, TP and TP, ST and ST, QA and QA, GR and GR, and II and KS, have positive rewards.

Rewards in bold fonts ( +1) are defined for the following reasons. If the user asks a question (QA) without a focus (e.g. “What happened on the game?”), the system can continue by story telling ( ST). If the system cannot find an answer, it can present relevant information ( PP). When the user says nothing (NR), the system action should be decided by considering the user focus; present a new topic if the user is not interested in the current topic (f
                        =0) or present an article related to the dialogue history (f
                        =1).

Keeping silence ( KS) is a safe action to the user silence (NR), thus, its reward is 0. However, we give 1 frustration point if the system selects KS in this case because the strategy conflict with the concept of information navigation. Confirmation ( CO) is a safe action to every user input, but it also frustrates the user. Thus, reward of CO is defined as 0 for every intention, and 2 frustration points are given to the system. If the system selects an inappropriate action (action of r
                        =−1), 2 frustration points are given to the system. We give a large penalty −20 to the system and terminate the dialogue if the frustration points accumulate more than 10. Reward of +20 is given if 20 turns are passed to reward a long continued dialogue.

The second reward is a reward for the success of information navigation (r
                        I). The system will receive a positive reward +1 if the modules QA and PP succeed in presenting appropriate information. If they fail, the system will receive a negative reward −1. For the training of POMDP, the function that gives r
                        I randomly outputs a positive or negative reward with an accordance of the success rate of each module (Yoshino et al., 2011).

The total reward that the system will receive is,


                        
                           
                              (25)
                              
                                 
                                    r
                                    C
                                 
                                 +
                                 
                                    r
                                    I
                                 
                                 .
                              
                           
                        
                     

For evaluation of the system, we collected additional 626 utterances (12 users, 24 dialogues; 2 dialogues by each user) with the proposed dialogue system with speech input (Yoshino et al., 2013a). There are 58 cases regarded as no request (NR) when a user did not say anything for longer than 5 seconds. The gold-standard is annotated by two annotators. The agreement for the user states was 0.958 and Cohen's kappa (Carletta, 1996) was 0.932. The agreement for the system actions was 0.944 and Cohen's kappa was 0.915. We reprioritized the first annotator who is familiar with the task if the annotation was not agreed. For training, we used the data set described in Section 4.1 (918 utterances in total).

Dialogue state tracking (DST) is a task of tracking the correct user state with noisy inputs including ASR and SLU errors (Williams et al., 2013). It tries to maximize the probability of the belief of the correct states, but we evaluated the accuracy of the 1-best result of the belief update. The user state and the best system action were annotated by human. We also evaluated accuracy of the system action and average reward for dialogue sessions. The accuracy of the system action shows not only the effect of the belief update but also the effect of the trained policy. We have two baseline systems. The first baseline system is a rule-based dialogue manager that is operated by a score of the question-answering module using P-A structure (Yoshino et al., 2011) and regular expressions for the TP and GR modules. The other baseline system is operated by the POMDP-based dialogue manger that does not refer to user focus (POMDP w.o. focus).

The DST accuracy, accuracy of action selection and average reward are summarized in Table 6
                        . This result shows that the proposed method tracks the dialogue state of the users with high accuracy, and responds with more appropriate modules. A breakdown of the DST accuracy is shown in Table 7
                        . The table shows precision (P), recall (R) and F-measure (F) of each intention tag. The performance for greeting (GR) and irrelevant input (II) is not shown because the number of these tags was very small (#
                        GR=2, #
                        II=4). Our proposed framework improved the SLU accuracy and robustness against ASR errors, especially reducing misclassification to question answering (QA) that were actually topic presentation (TP) or story telling (ST). Moreover, the belief update can detect the ST state even if the SLU incorrectly predicts QA or TP.

The proposed method also improved the accuracy of action selection. A breakdown is shown in Table 8
                        . We show the results of TP, ST, QA and PP because the number of KS and GR was very small (#
                        GR=2, #
                        KS=4), and CO was not labelled as a correct action. The proposed method outperformed the baseline systems for all actions. The overall improvement over the other two systems shown in Table 6 is statistically significant (p
                        <0.01). In detail, there are significant differences for ST and PP module selection (p
                        <0.01). The proposed method improved the accuracy for topic presentation ( TP) and proactive presentation ( PP) especially when the user intention was no request (NR). The POMDP without user focus always selected the keep silence ( KS) module if the user said nothing (NR).

The proposed method also made more effective confirmations ( CO) when the SLU result was not correct. It made confirmations (CO) 18 times, and 15 times of them were done when SLU was incorrect (15/18=83.3%). The POMDP without user focus made only two confirmations, when the detected user intention was correct (0/2=0.0%).

The proposed method made 35 proactive presentations ( PP) and 17 times of them (17/35=48.6%) invoked new user questions. This result demonstrates that the proposed system encouraged interactions in news navigation.

An example dialogue is shown in Fig. 6
                        . In the example, the system selects appropriate actions even if the observation likelihood is low. At the 4th turn of Dialogue 1 in this example, the system with the user focus responds with an action of proactive presentation a
                        =
                        PP, but the system without the user focus responds with an action of topic presentation a
                        =
                        KS. At the 2nd turn of Dialogue 2, the user asks a question without a focus. The confidence of s
                        =
                        QA is lowered by the belief update, and the system selects the story telling module a
                        =
                        ST. These examples show that the trained policy reflects our design described in Section 5.3: it is better to make a proactive presentation when the user is interested in the topic.

@&#CONCLUSIONS@&#

We have designed and implemented a spoken dialogue system for information navigation of Web news articles updated day-by-day. The system presents relevant information according to the user's interest. We have introduced a user focus detection model, and developed a POMDP framework which tracks the user focus to select the appropriate action module of the dialogue system. In the experimental evaluations, the proposed dialogue management approach determines the state of the user more accurately than the existing rule-based system and the POMDP-based system without user focus information. The proposed method also improved the accuracy of action selection.

In the future, it is expected to refine the system with large-scale data. When we collect more data with the current system, we can incorporate them with the automatic annotation as the annotation accuracy (Tables 2 and 4) is very high. We should also investigate other discourse features for the dialogue management of information navigation such as a more elaborate dialogue topic model.

We plan to apply the system to other domains such as football and stock market in order to confirm the domain portability of the proposed scheme. Then, we can also design a multi-domain navigation system that can converse about a variety of news. Furthermore, other types of document sets that have a category or cluster such as Wikipedia can be explored as the source of information navigation.

@&#REFERENCES@&#

