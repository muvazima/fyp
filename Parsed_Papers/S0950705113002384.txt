@&#MAIN-TITLE@&#A fast algorithm for kernel 1-norm support vector machines

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           This paper proposes a Column Generation Newton (CGN) algorithm for finding solution of the kernel 1-norm SVM.


                        
                        
                           
                           CGN is combining the Column Generation and the Newton Linear Programming SVM method.


                        
                        
                           
                           CGN is fast when solving the kernel 1-norm SVM.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

1-Norm SVM

Linear programming

Column generation

Newton algorithm

Kernel function

@&#ABSTRACT@&#


               
               
                  This paper presents a fast algorithm called Column Generation Newton (CGN) for kernel 1-norm support vector machines (SVMs). CGN combines the Column Generation (CG) algorithm and the Newton Linear Programming SVM (NLPSVM) method. NLPSVM was proposed for solving 1-norm SVM, and CG is frequently used in large-scale integer and linear programming algorithms. In each iteration of the kernel 1-norm SVM, NLPSVM has a time complexity of O(ℓ3), where ℓ is the sample number, and CG has a time complexity between O(ℓ3) and O(n
                     ′3), where n′ is the number of columns of the coefficient matrix in the subproblem. CGN uses CG to generate a sequence of subproblems containing only active constraints and then NLPSVM to solve each subproblem. Since the subproblem in each iteration only consists of n′ unbound constraints, CGN thus has a time complexity of O(n
                     ′3), which is smaller than that of NLPSVM and CG. Also, CGN is faster than CG when the solution to 1-norm SVM is sparse. A theorem is given to show a finite step convergence of CGN. Experimental results on the Ringnorm and UCI data sets demonstrate the efficiency of CGN to solve the kernel 1-norm SVM.
               
            

@&#INTRODUCTION@&#

In recent years, 1-norm Support Vector Machine (SVM) has attracted substantial attentions for its good sparsity [1–4]. 1-Norm SVM uses 1-norm regularization to replace 2-norm regularization in the standard SVM and approaches a more sparse model representation [5–9]. In fact, the optimization problem of 1-norm SVM is a linear program. Many methods have been proposed for solving 1-norm SVM. But, none of them is efficient enough for solving the kernel 1-norm SVM.

Since 1-norm SVM can be posed as a linear program, it can be solved by general mathematical programming methods, such as the simplex method and the interior-point method. However, these optimization methods are usually time consuming for problems in which the sample dimensionality is much less than the number of training samples.

Some specific algorithms have proposed for solving the linear program of 1-norm SVM. Fung et al. [10] presented the Newton Linear Programming SVM (NLPSVM) method to solve 1-norm SVM by minimizing the exterior penalty function for the dual problem of 1-norm SVM. In NLPSVM, each iteration has a time complexity of O(min(m,
                     n)3) when applying the Sherman–Morrison–Woodbury identity equation [10], where m and n are the number of rows and columns of the sample matrix or the kernel gram matrix, respectively. It is quite efficient when NLPSVM is used to solve the linear 1-norm SVM. But in the case of using kernel functions, m could be equal to n, which implies that NLPSVM is still time consuming when solving the kernel 1-norm SVM. Demiriz et al. [11] introduced CG to the linear programming boosting, which can also be applied to solve 1-norm SVM by a simple generalization. In CG, the time complexity of each iteration is usually between O(m
                     3) and O(n
                     3) when using the simplex method [12] to directly solve the subproblem, where m and n are the number of rows and columns of the active constraint coefficient matrix in the subproblem for each iteration, respectively. More specifically, for 1-norm SVM, the complexity of the last iteration of CG is cubic of the SV number. In the standard SVM, the idea of CG is widely applied to speed up the optimization of quadratic programming [13–16].

In order to obtain a fast algorithm for the kernel 1-norm SVM, we propose a Column Generation Newton (CGN) method. This method is a combination of CG and NLPSVM. For the kernel 1-norm SVM, the coefficient matrix is a matrix with ℓ rows and ℓ columns, where ℓ is the sample number. Thus, NLPSVM would have a time complexity of O(ℓ3). Let n′ be the number of active constraints. Then CG thus has a time complexity between O(ℓ3) and O(n
                     ′3).

In CGN, CG is used to construct a sequence of subproblems containing only active constraints, and NLPSVM is exploited to solve each subproblem. This means that CGN has a time complexity of O(n
                     ′3) in each intermediate iteration and a time complexity which is cubic of the number of ESVs in the last iteration. Since the complexity of CGN is much smaller than that of NLPSVM, it is expected that CGN is faster than NLPSVM when applied to the kernel case. Furthermore, CGN is faster than CG when the number of ESVs is smaller than that of SVs. As pointed out in [9], SVs is usually a small part of training samples and ESVs is only a small subset of SVs.

The rest of the paper is organized as follows. Section 2 gives a brief review of 1-norm SVM, the NLPSVM method, and the CG method. Section 3 presents the CGN algorithm for 1-norm SVM and some schemes to speed up CGN. Experimental results on the Ringnorm and UCI data sets are presented in Section 4. Section 5 concludes the paper.

@&#RELATED WORK@&#

Other than CG and NLPSVM, several other algorithms exist for 1-norm SVM. Similar to CG and NLPSVM, these methods also have high computational complexity for the kernel 1-norm SVM. In [17], three decomposition techniques for LP of 1-norm SVMs were proposed. Bradley et al. [18] proposed the Linear Programming Chunking (LPChunking) algorithm to solve the linear 1-norm SVM. LPChunking can be viewed as a block column generation method. In [19], a general technique was proposed for generalizing almost all available 2-norm SVM algorithms to the corresponding soft version by using 1-norm regularization. The soft 1-norm SVMs constructed by the proposed technique have the same convergence and an almost identically computational cost to that of the corresponding hard ones. Below, a brief review on 1-norm SVM, the NLPSVM method, and the CG method is given.

Consider a training sample set of two classes 
                           X
                        
                        ={(x
                        1,
                        y
                        1), (x
                        2,
                        y
                        2), …, (x
                        ℓ,
                        y
                        ℓ)}, where 
                           
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    d
                                 
                              
                              ,
                              
                                 
                                    y
                                 
                                 
                                    i
                                 
                              
                              ∈
                              {
                              -
                              1
                              ,
                              +
                              1
                              }
                           
                         is the corresponding label for x
                        
                           i
                        , d is the dimensionality of samples, and ℓ is the number of training samples. Although different LP forms exist for 1-norm SVM, they can be shown as equivalent when parameters are appropriately selected. Here, we use the LP formulation proposed in [9], which is a variant of the LP form presented in [10]. In [9], the primal problem of 1-norm SVM is described as:
                           
                              (1)
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   min
                                                
                                                
                                                   
                                                      
                                                         α
                                                      
                                                      
                                                         +
                                                      
                                                   
                                                   ,
                                                   
                                                      
                                                         α
                                                      
                                                      
                                                         -
                                                      
                                                   
                                                   ,
                                                   
                                                      
                                                         β
                                                      
                                                      
                                                         +
                                                      
                                                   
                                                   ,
                                                   
                                                      
                                                         β
                                                      
                                                      
                                                         -
                                                      
                                                   
                                                   ,
                                                   γ
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   j
                                                   =
                                                   1
                                                
                                                
                                                   d
                                                
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                         α
                                                      
                                                      
                                                         j
                                                      
                                                      
                                                         +
                                                      
                                                   
                                                   +
                                                   
                                                      
                                                         α
                                                      
                                                      
                                                         j
                                                      
                                                      
                                                         -
                                                      
                                                   
                                                
                                             
                                          
                                          +
                                          σ
                                          (
                                          
                                             
                                                β
                                             
                                             
                                                +
                                             
                                          
                                          +
                                          
                                             
                                                β
                                             
                                             
                                                -
                                             
                                          
                                          )
                                          +
                                          C
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                
                                                   ℓ
                                                
                                             
                                          
                                          
                                             
                                                γ
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                    
                                       
                                          s
                                          .
                                          t
                                          .
                                       
                                       
                                          
                                             
                                                y
                                             
                                             
                                                i
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                         x
                                                      
                                                      
                                                         i
                                                      
                                                      
                                                         T
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  α
                                                               
                                                               
                                                                  +
                                                               
                                                            
                                                            -
                                                            
                                                               
                                                                  α
                                                               
                                                               
                                                                  -
                                                               
                                                            
                                                         
                                                      
                                                   
                                                   +
                                                   (
                                                   
                                                      
                                                         β
                                                      
                                                      
                                                         +
                                                      
                                                   
                                                   -
                                                   
                                                      
                                                         β
                                                      
                                                      
                                                         -
                                                      
                                                   
                                                   )
                                                
                                             
                                          
                                          ⩾
                                          1
                                          -
                                          
                                             
                                                γ
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                          
                                             
                                                α
                                             
                                             
                                                j
                                             
                                             
                                                +
                                             
                                          
                                          ,
                                          
                                          
                                             
                                                α
                                             
                                             
                                                j
                                             
                                             
                                                -
                                             
                                          
                                          ⩾
                                          0
                                          ,
                                          
                                          j
                                          =
                                          1
                                          ,
                                          …
                                          ,
                                          d
                                          ;
                                          
                                          
                                             
                                                β
                                             
                                             
                                                +
                                             
                                          
                                          ,
                                          
                                          
                                             
                                                β
                                             
                                             
                                                -
                                             
                                          
                                          ⩾
                                          0
                                          ,
                                          
                                          
                                             
                                                γ
                                             
                                             
                                                i
                                             
                                          
                                          ⩾
                                          0
                                          ,
                                          
                                          i
                                          =
                                          1
                                          ,
                                          …
                                          ,
                                          ℓ
                                       
                                    
                                 
                              
                           
                        where C is a positive penalty factor, σ is a small positive constant to ensure a unique solution, 
                           
                              
                                 
                                    α
                                 
                                 
                                    +
                                 
                              
                              =
                              
                                 
                                    
                                       
                                          
                                             
                                                α
                                             
                                             
                                                1
                                             
                                             
                                                +
                                             
                                          
                                          ,
                                          …
                                          ,
                                          
                                             
                                                α
                                             
                                             
                                                d
                                             
                                             
                                                +
                                             
                                          
                                       
                                    
                                 
                                 
                                    T
                                 
                              
                           
                        , 
                           
                              
                                 
                                    α
                                 
                                 
                                    -
                                 
                              
                              =
                              
                                 
                                    
                                       
                                          
                                             
                                                α
                                             
                                             
                                                1
                                             
                                             
                                                -
                                             
                                          
                                          ,
                                          …
                                          ,
                                          
                                             
                                                α
                                             
                                             
                                                d
                                             
                                             
                                                -
                                             
                                          
                                       
                                    
                                 
                                 
                                    T
                                 
                              
                           
                        , 
                           
                              
                                 
                                    β
                                 
                                 
                                    +
                                 
                              
                              ∈
                              R
                           
                         and 
                           
                              
                                 
                                    β
                                 
                                 
                                    -
                                 
                              
                              ∈
                              R
                           
                         are model coefficients for 1-norm SVM, and 
                           
                              γ
                              =
                              
                                 
                                    [
                                    
                                       
                                          γ
                                       
                                       
                                          1
                                       
                                    
                                    ,
                                    …
                                    ,
                                    
                                       
                                          γ
                                       
                                       
                                          ℓ
                                       
                                    
                                    ]
                                 
                                 
                                    T
                                 
                              
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    ℓ
                                 
                              
                           
                         is a loss vector. The decision function of 1-norm SVM for classification takes the form:
                           
                              (2)
                              
                                 f
                                 (
                                 x
                                 )
                                 =
                                 sign
                                 
                                 (
                                 
                                    
                                       (
                                       
                                          
                                             α
                                          
                                          
                                             +
                                          
                                       
                                       -
                                       
                                          
                                             α
                                          
                                          
                                             -
                                          
                                       
                                       )
                                    
                                    
                                       T
                                    
                                 
                                 x
                                 +
                                 (
                                 
                                    
                                       β
                                    
                                    
                                       +
                                    
                                 
                                 -
                                 
                                    
                                       β
                                    
                                    
                                       -
                                    
                                 
                                 )
                                 )
                              
                           
                        where 
                           
                              x
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    d
                                 
                              
                           
                         is a sample and sign(·) is a sign function. It has been showed that most of entries in both 
                           α
                        
                        + and 
                           α
                        
                        − take zeros for the sparsity of 1-norm SVM.

Rewrite (1) in its matrix form, we have
                           
                              (3)
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   min
                                                
                                                
                                                   u
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                c
                                             
                                             
                                                T
                                             
                                          
                                          u
                                       
                                    
                                    
                                       
                                          s
                                          .
                                          t
                                          .
                                       
                                       
                                          Au
                                          ⩾
                                          b
                                       
                                    
                                    
                                       
                                       
                                          u
                                          ⩾
                                          0
                                       
                                    
                                 
                              
                           
                        where c
                        =[1
                        
                           T
                        ,
                        1
                        
                           T
                        ,
                        σ,
                        σ,
                        C
                        1
                        
                           T
                        ]
                           T
                        , the variable vector u
                        =[
                           α
                        
                        +T
                        ,
                        
                           α
                        
                        −T
                        ,
                        β
                        +,
                        β
                        −,
                        
                           γ
                        
                        
                           T
                        ]
                           T
                        , b
                        =
                        1, the constraint coefficient matrix 
                           
                              A
                              =
                              [
                              
                                 
                                    D
                                 
                                 
                                    y
                                 
                              
                              X
                              ,
                              -
                              
                                 
                                    D
                                 
                                 
                                    y
                                 
                              
                              X
                              ,
                              y
                              ,
                              -
                              y
                              ,
                              I
                              ]
                              ,
                              
                              X
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    ℓ
                                    ×
                                    d
                                 
                              
                           
                         is the sample matrix in which each row vector is a training sample, y
                        =[y
                        1,
                        y
                        2,…,
                        y
                        ℓ]
                           T
                        , D
                        
                           y
                         is the diagonal matrix with the diagonal line of y, and 1 and I are the column vector of all ones and the identity matrix with proper sizes, respectively. The dual problem of (7) can be expressed as follows:
                           
                              (4)
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   max
                                                
                                                
                                                   v
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                b
                                             
                                             
                                                T
                                             
                                          
                                          v
                                       
                                    
                                    
                                       
                                          s
                                          .
                                          t
                                          .
                                       
                                       
                                          
                                             
                                                A
                                             
                                             
                                                T
                                             
                                          
                                          v
                                          ⩽
                                          c
                                       
                                    
                                    
                                       
                                       
                                          v
                                          ⩾
                                          0
                                       
                                    
                                 
                              
                           
                        
                     

In order to generalize the linear 1-norm SVM to the nonlinear 1-norm SVM, a group of nonlinear mapping functions ϕ
                        
                           j
                        (x), j
                        =1, …, D are introduced to map the sample x into a feature space: x
                        →
                        Φ(x)=[ϕ
                        1(x),ϕ
                        2(x), …, ϕ
                        
                           D
                        (x)]
                           T
                        . Currently the most popular mapping functions are still the Mercer kernel functions [20,21],
                           
                              
                                 Φ
                                 (
                                 x
                                 )
                                 =
                                 
                                    
                                       [
                                       k
                                       (
                                       
                                          
                                             x
                                          
                                          
                                             1
                                          
                                       
                                       ,
                                       x
                                       )
                                       ,
                                       k
                                       (
                                       
                                          
                                             x
                                          
                                          
                                             2
                                          
                                       
                                       ,
                                       x
                                       )
                                       ,
                                       …
                                       ,
                                       k
                                       (
                                       
                                          
                                             x
                                          
                                          
                                             ℓ
                                          
                                       
                                       ,
                                       x
                                       )
                                       ]
                                    
                                    
                                       T
                                    
                                 
                              
                           
                        where k(x
                        
                           i
                        ,
                        x) is a Mercer kernel function, such as the Radial Basis Function (RBF) kernel 
                           
                              k
                              (
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              ,
                              x
                              )
                              =
                              exp
                              
                                 
                                    
                                       -
                                       ‖
                                       
                                          
                                             x
                                          
                                          
                                             i
                                          
                                       
                                       -
                                       x
                                       
                                          
                                             ‖
                                          
                                          
                                             2
                                          
                                          
                                             2
                                          
                                       
                                       /
                                       2
                                       
                                          
                                             p
                                          
                                          
                                             2
                                          
                                       
                                    
                                 
                              
                           
                         with p being the RBF kernel parameter. Therefore, the nonlinear 1-norm SVM is often referred to as the kernel 1-norm SVM. The linear programming of the kernel 1-norm SVM has the same form as that of the linear 1-norm SVM except that the sample matrix X is replaced by the kernel Gram matrix 
                           
                              K
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    ℓ
                                    ×
                                    ℓ
                                 
                              
                           
                         in the constraint coefficient matrix A, i.e. A
                        =[D
                        
                           y
                        
                        K,−D
                        
                           y
                        
                        K,
                        y,−y,
                        I], where K
                        
                           ij
                        
                        =
                        k(x
                        
                           i
                        ,
                        x
                        
                           j
                        ). To unify the expression of A, we let A
                        =[A
                        
                           s
                        ,−A
                        
                           s
                        ,
                        y,−y,
                        I], where A
                        
                           s
                        
                        =
                        D
                        
                           y
                        
                        X in the case of the linear 1-norm SVM and A
                        
                           s
                        
                        =
                        D
                        
                           y
                        
                        K in the case of the kernel 1-norm SVM.

Now we give some notations borrowed from [9], which are important to analyze the complexity of the algorithms of CG and NLPSVM. Assume that 1-norm SVM (7) has an optimal solution u
                        ∗, the set of Exact Support Vectors (ESVs) is defined as ESV
                        ={x
                        
                           i
                        ∣y
                        
                           i
                        
                        f(x
                        
                           i
                        )=1, i
                        =1, …, ℓ}, the set of Saturated Support Vectors (SSVs) is defined as SSV
                        ={x
                        
                           i
                        ∣y
                        
                           i
                        
                        f(x
                        
                           i
                        )<1, i
                        =1, …, ℓ}, the set of Non-Support Vectors (NSVs) is defined as NSV
                        ={x
                        
                           i
                        ∣y
                        
                           i
                        
                        f(x
                        
                           i
                        )>1, i
                        =1, …, ℓ}, and the set of Support Vectors (SVs) is thus the union of ESVs and SSVs: i.e., SV
                        =
                        ESV
                        ∪
                        SSV.

The following theorem is about the sparsity of 1-norm SVM [9], which indicates that 1-norm SVM has a better sparsity than the standard SVM and motivates our CGN method.
                           Theorem 1
                           
                              Let 
                              u
                              ∗ 
                              be an optimal solution of the primal problem 
                              
                                 (7)
                              
                              . The number of nonzero coefficients for 1-norm SVM is upper bounded by
                              
                                 
                                    
                                       |
                                       NC
                                       |
                                       ⩽
                                       |
                                       ESV
                                       |
                                       ,
                                    
                                 
                              
                              where 
                              
                                 
                                    NC
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   α
                                                
                                                
                                                   j
                                                
                                                
                                                   ∗
                                                
                                             
                                             |
                                             
                                                
                                                   α
                                                
                                                
                                                   j
                                                
                                                
                                                   ∗
                                                
                                             
                                             =
                                             
                                                
                                                   α
                                                
                                                
                                                   j
                                                
                                                
                                                   +
                                                   ∗
                                                
                                             
                                             -
                                             
                                                
                                                   α
                                                
                                                
                                                   j
                                                
                                                
                                                   -
                                                   ∗
                                                
                                             
                                             
                                             ≠
                                             
                                             0
                                             ,
                                             j
                                             =
                                             1
                                             ,
                                             …
                                             ,
                                             d
                                             (
                                             orD
                                             )
                                          
                                       
                                    
                                 
                              
                              . If the LP of 1-norm SVM is non-degenerate with respect to 
                              u
                              ∗
                              , the following equality
                              
                                 
                                    
                                       |
                                       NC
                                       |
                                       =
                                       |
                                       ESV
                                       |
                                       -
                                       1
                                    
                                 
                              
                              holds.
                           

In the standard SVM, the number of nonzero coefficients equals to the number of SVs. The SSVs set is usually nonempty, especially in some difficult recognition problems which have much more SSVs than ESVs. Accordingly, 1-norm SVM usually has a better sparsity than the standard SVM. By the Complementary Slackness Theorem [12] and Theorem 1, it is straightforward that there are at least ∣ESV∣ active constraints among 
                           
                              
                                 
                                    A
                                 
                                 
                                    s
                                 
                                 
                                    T
                                 
                              
                              v
                              ⩽
                              1
                           
                         and 
                           
                              -
                              
                                 
                                    A
                                 
                                 
                                    s
                                 
                                 
                                    T
                                 
                              
                              v
                              ⩽
                              1
                           
                         in the dual problem (4).

In NLPSVM [10,22], the dual problem of 1-norm SVM can be transformed into a convex unconstrained optimization problem by constructing the following asymptotic exterior penalty function. Here, the convex unconstrained optimization problem of the dual problem (4) can be rewritten as:
                           
                              (5)
                              
                                 min
                                 
                                 L
                                 (
                                 v
                                 )
                                 =
                                 -
                                 ε
                                 
                                    
                                       1
                                    
                                    
                                       T
                                    
                                 
                                 v
                                 +
                                 
                                    
                                       1
                                    
                                    
                                       2
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               A
                                                            
                                                            
                                                               s
                                                            
                                                            
                                                               T
                                                            
                                                         
                                                         v
                                                         -
                                                         1
                                                      
                                                   
                                                
                                                
                                                   +
                                                
                                             
                                          
                                       
                                    
                                    
                                       2
                                    
                                 
                                 +
                                 
                                    
                                       1
                                    
                                    
                                       2
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         -
                                                         
                                                            
                                                               A
                                                            
                                                            
                                                               s
                                                            
                                                            
                                                               T
                                                            
                                                         
                                                         v
                                                         -
                                                         1
                                                      
                                                   
                                                
                                                
                                                   +
                                                
                                             
                                          
                                       
                                    
                                    
                                       2
                                    
                                 
                                 +
                                 
                                    
                                       1
                                    
                                    
                                       2
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   (
                                                   
                                                      
                                                         y
                                                      
                                                      
                                                         T
                                                      
                                                   
                                                   v
                                                   -
                                                   σ
                                                   )
                                                
                                                
                                                   +
                                                
                                             
                                          
                                       
                                    
                                    
                                       2
                                    
                                 
                                 +
                                 
                                    
                                       1
                                    
                                    
                                       2
                                    
                                 
                                 ‖
                                 
                                    
                                       (
                                       -
                                       
                                          
                                             y
                                          
                                          
                                             T
                                          
                                       
                                       v
                                       -
                                       σ
                                       )
                                    
                                    
                                       +
                                    
                                 
                                 
                                    
                                       ‖
                                    
                                    
                                       2
                                    
                                 
                                 +
                                 
                                    
                                       1
                                    
                                    
                                       2
                                    
                                 
                                 ‖
                                 
                                    
                                       (
                                       v
                                       -
                                       C
                                       1
                                       )
                                    
                                    
                                       +
                                    
                                 
                                 
                                    
                                       ‖
                                    
                                    
                                       2
                                    
                                 
                                 +
                                 
                                    
                                       θ
                                    
                                    
                                       2
                                    
                                 
                                 ‖
                                 
                                    
                                       (
                                       -
                                       v
                                       )
                                    
                                    
                                       +
                                    
                                 
                                 
                                    
                                       ‖
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        where (·)+
                        =
                        max (·,0), both θ and ε are positive penalty parameters. In terms of Proposition 1 in [10], we can show that for some ε
                        0, ∀ε
                        <
                        ε
                        0, the optimal solution to the exterior penalty problem (5) provides the following exact least 2-norm solution to the primal problem (4) of 1-norm SVM:
                           
                              (6)
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         α
                                                      
                                                      
                                                         +
                                                      
                                                   
                                                   =
                                                   
                                                      
                                                         1
                                                      
                                                      
                                                         ε
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     A
                                                                  
                                                                  
                                                                     s
                                                                  
                                                                  
                                                                     T
                                                                  
                                                               
                                                               v
                                                               -
                                                               1
                                                            
                                                         
                                                      
                                                      
                                                         +
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         α
                                                      
                                                      
                                                         -
                                                      
                                                   
                                                   =
                                                   
                                                      
                                                         1
                                                      
                                                      
                                                         ε
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            
                                                               -
                                                               
                                                                  
                                                                     A
                                                                  
                                                                  
                                                                     s
                                                                  
                                                                  
                                                                     T
                                                                  
                                                               
                                                               v
                                                               -
                                                               1
                                                            
                                                         
                                                      
                                                      
                                                         +
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         β
                                                      
                                                      
                                                         +
                                                      
                                                   
                                                   =
                                                   
                                                      
                                                         1
                                                      
                                                      
                                                         ε
                                                      
                                                   
                                                   
                                                      
                                                         (
                                                         
                                                            
                                                               y
                                                            
                                                            
                                                               T
                                                            
                                                         
                                                         v
                                                         -
                                                         σ
                                                         )
                                                      
                                                      
                                                         +
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         β
                                                      
                                                      
                                                         -
                                                      
                                                   
                                                   =
                                                   
                                                      
                                                         1
                                                      
                                                      
                                                         ε
                                                      
                                                   
                                                   
                                                      
                                                         (
                                                         -
                                                         
                                                            
                                                               y
                                                            
                                                            
                                                               T
                                                            
                                                         
                                                         v
                                                         -
                                                         σ
                                                         )
                                                      
                                                      
                                                         +
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        In order to optimize the exterior penalty function, Fung et al. [10] proposed the Newton method to solve the unconstrained problem. They called it the NLPSVM algorithm, in which a generalized Hessian matrix was defined since the gradient of the unconstrained objective function is not differentiable. In addition, the complexity of each iteration for NLPSVM is O(min(m,
                        n)3) when applying the Sherman–Morrison–Woodbury identity equation, where m and n are the dimensions of matrix A
                        
                           s
                        . For 1-norm SVM, m
                        =ℓ, n
                        =
                        d in the linear case and n
                        =ℓ in the kernel case. Actually, if n
                        <
                        m (or d
                        <ℓ in the case of the linear 1-norm SVM), the complexity of NLPSVM in each iteration depends on the number of active constraints in 
                           
                              
                                 
                                    A
                                 
                                 
                                    s
                                 
                                 
                                    T
                                 
                              
                              v
                              ⩽
                              1
                           
                         and 
                           
                              -
                              
                                 
                                    A
                                 
                                 
                                    s
                                 
                                 
                                    T
                                 
                              
                              v
                              ⩽
                              1
                           
                         (which are included in the dual problem (4)), and is independent of the number of bounded constraints of v
                        ⩽
                        C
                        1.

Although the CG method is not yet reported for solving 1-norm SVMs, it is frequently used in large-scale integer and linear programming algorithms. In Ref. [11], Demiriz et al. introduced CG into the linear programming boosting. In this case, CG is repeatedly called by the simplex method. For detailed introduction about CG, the reader is referred to Refs. [11,23].

For a clear description of our algorithm, some notations are first introduced. Let J
                        
                           W
                         be the working index set for working variables, and 
                           
                              
                                 
                                    
                                       
                                          J
                                       
                                       
                                          ¯
                                       
                                    
                                 
                                 
                                    W
                                 
                              
                           
                         be the non-working index set for non-working variables. Of course, 
                           
                              
                                 
                                    
                                       
                                          J
                                       
                                       
                                          ¯
                                       
                                    
                                 
                                 
                                    W
                                 
                              
                           
                         is the complementary set of J
                        
                           W
                        . Thus, the corresponding constraint coefficient matrix A is also divided into two parts. A
                        ·j
                         denotes the jth column of A, and 
                           
                              
                                 
                                    A
                                 
                                 
                                    ·
                                    
                                       
                                          J
                                       
                                       
                                          W
                                       
                                    
                                 
                              
                           
                         consists of the jth columns of A for all j
                        ∈
                        J
                        
                           W
                        . In the dual problem, 
                           
                              
                                 
                                    A
                                 
                                 
                                    ·
                                    
                                       
                                          J
                                       
                                       
                                          W
                                       
                                    
                                 
                                 
                                    T
                                 
                              
                              v
                              ⩽
                              
                                 
                                    c
                                 
                                 
                                    
                                       
                                          J
                                       
                                       
                                          W
                                       
                                    
                                 
                              
                           
                         are called active (working) constraints.

In each iteration, only working variables are optimized while other non-working variables keep their values constant. In other word, we only solve the following restricted primal LP problem:
                           
                              (7)
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   min
                                                
                                                
                                                   
                                                      
                                                         u
                                                      
                                                      
                                                         
                                                            
                                                               J
                                                            
                                                            
                                                               W
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                c
                                             
                                             
                                                
                                                   
                                                      J
                                                   
                                                   
                                                      W
                                                   
                                                
                                             
                                             
                                                T
                                             
                                          
                                          
                                             
                                                u
                                             
                                             
                                                
                                                   
                                                      J
                                                   
                                                   
                                                      W
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          s
                                          .
                                          t
                                          .
                                       
                                       
                                          
                                             
                                                A
                                             
                                             
                                                ·
                                                
                                                   
                                                      J
                                                   
                                                   
                                                      W
                                                   
                                                
                                             
                                          
                                          
                                             
                                                u
                                             
                                             
                                                
                                                   
                                                      J
                                                   
                                                   
                                                      W
                                                   
                                                
                                             
                                          
                                          ⩾
                                          b
                                       
                                    
                                    
                                       
                                       
                                          
                                             
                                                u
                                             
                                             
                                                
                                                   
                                                      J
                                                   
                                                   
                                                      W
                                                   
                                                
                                             
                                          
                                          ⩾
                                          0
                                       
                                    
                                 
                              
                           
                        
                     

Combining the values of both working variables and non-working variables generates the current solution. By doing so, the simplex method requires determining whether the current solution is optimal. The optimality conditions here mean the constraints of the dual programming (4). If the current solution satisfies all constraints, it is optimal. Otherwise, some constraints are violated.

If the current solution is not optimal, an immediate issue is how to select some column that violates the optimality conditions. In the simplex method, the max-appending rule is typically used to select some column, which can be described as
                           
                              (8)
                              
                                 
                                    
                                       j
                                    
                                    
                                       v
                                    
                                 
                                 =
                                 arg
                                 
                                 
                                    
                                       
                                          max
                                       
                                       
                                          j
                                          ∈
                                          
                                             
                                                
                                                   
                                                      J
                                                   
                                                   
                                                      ¯
                                                   
                                                
                                             
                                             
                                                W
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                A
                                             
                                             
                                                ·
                                                j
                                             
                                             
                                                T
                                             
                                          
                                          
                                             
                                                v
                                             
                                             
                                                ∗
                                             
                                          
                                          -
                                          
                                             
                                                c
                                             
                                             
                                                j
                                             
                                          
                                          -
                                          
                                             
                                                θ
                                             
                                             
                                                0
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where θ
                        0
                        >0 is a tolerance, and v
                        ∗ is the optimal solution of (4). In order to keep the working variable number constant, the simplex method needs some schemes for deleting variable from the working set. Typically, the max-deleting rule is used. This rule selects the variable which satisfies the constrain most. In the case of degeneracy, however, it could result in cycling when using both the max-appending rule and the max-deleting rule. For 1-norm SVM, the complexity of the last iteration of CG is cubic of the SV number.

As mentioned earlier, CGN uses CG to generate a sequence of subproblems containing only active constraints and then solves each of them by using NLPSVM. The main idea behind the subproblem construction is a decomposition strategy. In each subproblem, the variables of the primal problem are split into two categories: the working variables and non-working variables. Working variables will be optimized in the subproblem, while non-working variables are fixed as constants. Only these columns of A corresponding to the working variables are selected as active constraints of the dual problem in each subproblem. Let n′ be the number of active constraints. As mentioned above, the complexity of NLPSVM for solving a subproblem cubicly depends on min(ℓ,n′). Since the number of dual variables is always ℓin the dual problem (4), only n′ varies. In order to reduce the time complexity, it is necessary to make n′ as small as possible.

The number of the nonzero coefficients of 
                        
                           
                              
                                 α
                              
                              
                                 j
                              
                              
                                 +
                              
                           
                        
                      and 
                        
                           
                              
                                 α
                              
                              
                                 j
                              
                              
                                 -
                              
                           
                           ,
                           
                           j
                           =
                           1
                           ,
                           …
                           ,
                           d
                        
                      (or ℓ), is no more than that of ESVs in 1-norm SVM. As a result, the number of active constraints in 
                        
                           
                              
                                 A
                              
                              
                                 s
                              
                              
                                 T
                              
                           
                           v
                           ⩽
                           1
                        
                      and 
                        
                           -
                           
                              
                                 A
                              
                              
                                 s
                              
                              
                                 T
                              
                           
                           v
                           ⩽
                           1
                        
                      is no more than that of ESVs by the Complementary Slackness Theorem [12]. Since SVs is a small part of the training samples and ESVs is only a small subset of SVs, CGN, which combines CG and NLPSVM, is thus a promising method for 1-norm SVM.

In each iteration, we need to determine whether a constraint is active. Since it could result in cycling when using both the max-appending rule and the max-deleting rule in degenerate case, we provide a novel rule, or the semi-deleting rule, and use it together with the max-appending rule to make a decision. The semi-deleting rule is used to delete an index from the working index set and move it into the non-working index set if the corresponding constraint in the dual problem is the most satisfied one. This rule can be defined as:
                           
                              (9)
                              
                                 
                                    
                                       j
                                    
                                    
                                       r
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   ∅
                                                   ,
                                                
                                                
                                                   if
                                                   
                                                   
                                                      
                                                         L
                                                      
                                                      
                                                         t
                                                      
                                                      
                                                         ∗
                                                      
                                                   
                                                   <
                                                   
                                                      
                                                         L
                                                      
                                                      
                                                         t
                                                         -
                                                         1
                                                      
                                                      
                                                         ∗
                                                      
                                                   
                                                   +
                                                   
                                                      
                                                         θ
                                                      
                                                      
                                                         1
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   arg
                                                   
                                                   
                                                      
                                                         max
                                                      
                                                      
                                                         j
                                                         ∈
                                                         
                                                            
                                                               J
                                                            
                                                            
                                                               W
                                                            
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  c
                                                               
                                                               
                                                                  j
                                                               
                                                            
                                                            -
                                                            
                                                               
                                                                  A
                                                               
                                                               
                                                                  ·
                                                                  j
                                                               
                                                               
                                                                  T
                                                               
                                                            
                                                            
                                                               
                                                                  v
                                                               
                                                               
                                                                  ∗
                                                               
                                                            
                                                            -
                                                            
                                                               
                                                                  θ
                                                               
                                                               
                                                                  0
                                                               
                                                            
                                                         
                                                      
                                                   
                                                   ,
                                                
                                                
                                                   otherwise
                                                   .
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where θ
                        1
                        >0 is a tolerance, and 
                           
                              
                                 
                                    L
                                 
                                 
                                    t
                                 
                                 
                                    ∗
                                 
                              
                           
                         is the optimal objective value of (5) at the t th iteration. In addition, the max-appending rule is adopted to delete an index from the non-working index set and move it into the working index set if the corresponding constraint is the most violated one.
                           Algorithm 1
                           Column Generation Newton Algorithm
                                 
                                    
                                       
                                       
                                          
                                             Set tolerances θ
                                                0 and θ
                                                1; initialize the flag variable solutionstatus
                                                =0, the iteration number t
                                                =0, the optimal objective value of (5) in the tth iteration 
                                                   
                                                      
                                                         
                                                            L
                                                         
                                                         
                                                            t
                                                         
                                                         
                                                            ∗
                                                         
                                                      
                                                      =
                                                      0
                                                   
                                                ; initialize the working index set J
                                                
                                                   W
                                                 to contain only one random index; and let non-working index set 
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  J
                                                               
                                                               
                                                                  ¯
                                                               
                                                            
                                                         
                                                         
                                                            W
                                                         
                                                      
                                                   
                                                 be the complementary set of J
                                                
                                                   W
                                                .
                                          
                                          
                                             
                                                while 
                                                solutionstatus
                                                =0 do
                                             
                                          
                                          
                                             
                                                
                                                t
                                                =
                                                t
                                                +1;
                                          
                                          
                                             
                                                Construct the tth subproblem:
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  min
                                                               
                                                               
                                                                  -
                                                                  
                                                                     
                                                                        b
                                                                     
                                                                     
                                                                        T
                                                                     
                                                                  
                                                                  v
                                                               
                                                            
                                                            
                                                               
                                                                  s
                                                                  .
                                                                  t
                                                                  .
                                                               
                                                               
                                                                  
                                                                     
                                                                        A
                                                                     
                                                                     
                                                                        ·
                                                                        
                                                                           
                                                                              J
                                                                           
                                                                           
                                                                              W
                                                                           
                                                                        
                                                                     
                                                                     
                                                                        T
                                                                     
                                                                  
                                                                  v
                                                                  ⩽
                                                                  
                                                                     
                                                                        c
                                                                     
                                                                     
                                                                        
                                                                           
                                                                              J
                                                                           
                                                                           
                                                                              W
                                                                           
                                                                        
                                                                     
                                                                  
                                                                  ,
                                                                  
                                                                  v
                                                                  ⩾
                                                                  0
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             
                                                and use NLPSVM to solve it. Let v
                                                ∗ be the obtained optimal solution and 
                                                   
                                                      
                                                         
                                                            L
                                                         
                                                         
                                                            t
                                                         
                                                         
                                                            ∗
                                                         
                                                      
                                                   
                                                 be the optimal objective value in the tth iteration.
                                          
                                          
                                             
                                                
                                                if No feasible solution exists then
                                             
                                          
                                          
                                             
                                                
                                                solutionstatus
                                                =−1;
                                          
                                          
                                             
                                                
                                                else
                                             
                                          
                                          
                                             
                                                Test 
                                                   
                                                      
                                                         
                                                            A
                                                         
                                                         
                                                            ·
                                                            j
                                                         
                                                         
                                                            T
                                                         
                                                      
                                                      
                                                         
                                                            v
                                                         
                                                         
                                                            ∗
                                                         
                                                      
                                                      ⩽
                                                      
                                                         
                                                            c
                                                         
                                                         
                                                            j
                                                         
                                                      
                                                      -
                                                      
                                                         
                                                            θ
                                                         
                                                         
                                                            0
                                                         
                                                      
                                                   
                                                , 
                                                   
                                                      ∀
                                                      j
                                                      ∈
                                                      
                                                         
                                                            
                                                               
                                                                  J
                                                               
                                                               
                                                                  ¯
                                                               
                                                            
                                                         
                                                         
                                                            W
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             
                                                
                                                if all are satisfied then
                                             
                                          
                                          
                                             
                                                
                                                
                                                solutionstatus
                                                =1;
                                          
                                          
                                             
                                                
                                                else
                                             
                                          
                                          
                                             
                                                
                                                Choose the index j
                                                
                                                   r
                                                 of the most relaxed constraint in the semi-deleting rule (9);
                                          
                                          
                                             
                                                
                                                Choose the index j
                                                
                                                   v
                                                 of the most violated constraint in the max-appending rule (8);
                                          
                                          
                                             
                                                
                                                
                                                J
                                                
                                                   W
                                                
                                                =(J
                                                
                                                   W
                                                ⧹{j
                                                
                                                   r
                                                })∪{j
                                                
                                                   v
                                                }, 
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  J
                                                               
                                                               
                                                                  ¯
                                                               
                                                            
                                                         
                                                         
                                                            W
                                                         
                                                      
                                                      =
                                                      (
                                                      
                                                         
                                                            
                                                               
                                                                  J
                                                               
                                                               
                                                                  ¯
                                                               
                                                            
                                                         
                                                         
                                                            W
                                                         
                                                      
                                                      ⧹
                                                      {
                                                      
                                                         
                                                            j
                                                         
                                                         
                                                            v
                                                         
                                                      
                                                      }
                                                      )
                                                      ∪
                                                      {
                                                      
                                                         
                                                            j
                                                         
                                                         
                                                            r
                                                         
                                                      
                                                      }
                                                   
                                                .
                                          
                                          
                                             
                                                
                                                end if
                                             
                                          
                                          
                                             
                                                
                                                end if
                                             
                                          
                                          
                                             
                                                end while
                                             
                                          
                                       
                                    
                                 
                              
                           


                        Algorithm 1 gives the main steps of CGN. Particularly for the dual problem (4) of 1-norm SVM, CG is only applied to the constraints in 
                           
                              
                                 
                                    A
                                 
                                 
                                    s
                                 
                                 
                                    T
                                 
                              
                              v
                              ⩽
                              1
                           
                         and 
                           
                              -
                              
                                 
                                    A
                                 
                                 
                                    s
                                 
                                 
                                    T
                                 
                              
                              v
                              ⩽
                              1
                           
                         to construct a subproblem. For the other constraints y
                        
                           T
                        
                        v
                        ⩽
                        σ, −y
                        
                           T
                        
                        v
                        ⩽
                        σ and v
                        ⩽
                        C
                        1, it is not necessary to use CG. Since θ
                        0 typically takes a larger value than σ does, the constraints y
                        
                           T
                        
                        v
                        ⩽
                        σ and y
                        
                           T
                        
                        v
                        ⩽
                        σ are always active.

We require that indices of the constraints of v
                        ⩽
                        C
                        1 be kept in the working index set J
                        
                           W
                         such that there always exists an optimal solution to each subproblem. Unlike the all-deleting rule, which deletes all relaxed constraints even if the objective value is not increased compared to the previous one, the semi-deleting rule deletes the most relaxed constraint only when the objective value is increased. As is indicated in the simplex method, the all-deleting rule cannot guarantee convergence when a degeneracy occurs. However, the proposed semi-deleting rule can ensure that, which is summarized in the following Theorem 2.
                           Theorem 2
                           
                              The CGN algorithm has the following properties:
                              
                                 
                                    (1)
                                    
                                       Each NLPSVM converges to an optimal solution for each subproblem if the tolerance Tol
                                       
                                       =
                                       
                                       0, the maximum number of iterations MaxIters
                                       
                                       =
                                       ∞, and ε
                                       >
                                       0 is sufficiently small in each NLPSVM.
                                    


                                       If each NLPSVM converges to an optimal solution for each subproblem, the objective values 
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            L
                                                         
                                                         
                                                            t
                                                         
                                                         
                                                            ∗
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                        
                                       is a non-decreasing sequence.
                                    


                                       If θ
                                       
                                          0
                                        
                                       is sufficiently small, then the CGN algorithm solves the dual problem 
                                       
                                          (4)
                                        
                                       of 1-norm SVM in a finite number of iterations.
                                    


                        Theorem 2 can be briefly proved as follows. Property (1) follows directly from Theorem 1 in [10].

For Property (2), appending the violated constraints into J
                        
                           W
                         makes the tth subproblem be a restricted problem of the (t
                        −1)th subproblem. In other words, the solution of the tth subproblem has more nonzero entries than that of the (t
                        −1)th subproblem, which leads to a larger value for the objective function 
                           
                              
                                 
                                    L
                                 
                                 
                                    t
                                 
                                 
                                    ∗
                                 
                              
                           
                        .

The simple proof of Property (3) is as follows. We first show that the number of feasible solutions is finite, and then demonstrate that CGN does not cycle. It has already been shown that the number of vertices of the convex polyhedron defined by the constraints of the dual problem (4) is finite [12]. The solution to each subproblem must be one of these vertices, which implies that the number of different 
                           
                              
                                 
                                    L
                                 
                                 
                                    t
                                 
                                 
                                    ∗
                                 
                              
                           
                        s is finite. In other words, CGN can converge in finite iterations if CGN does not cycle. As we know, cycling only occurs in degenerate case. Thus, we need to guarantee that CGN does not cycle in the presence of degeneracy. Since the objective value sequence 
                           
                              
                                 
                                    
                                       
                                          
                                             L
                                          
                                          
                                             t
                                          
                                          
                                             ∗
                                          
                                       
                                    
                                 
                              
                           
                         is non-decreasing, we only define two kinds of states for 
                           
                              
                                 
                                    L
                                 
                                 
                                    t
                                 
                                 
                                    ∗
                                 
                              
                           
                        . If 
                           
                              
                                 
                                    L
                                 
                                 
                                    t
                                 
                                 
                                    ∗
                                 
                              
                              >
                              
                                 
                                    L
                                 
                                 
                                    t
                                    -
                                    1
                                 
                                 
                                    ∗
                                 
                              
                           
                        , then 
                           
                              
                                 
                                    L
                                 
                                 
                                    t
                                 
                                 
                                    ∗
                                 
                              
                           
                         is called a varied state; otherwise 
                           
                              
                                 
                                    L
                                 
                                 
                                    t
                                 
                                 
                                    ∗
                                 
                              
                           
                         is called an unvaried state. It is not difficult to show that both varied and unvaried states are finite, since it is the semi-deleting rule that is utilized to avoid an infinite loop in degenerate case. Therefore, the CGN algorithm converges in a finite number of iterations.

In CGN, the complexity of each iteration is no more than O(∣ESV∣3). This is because the complexity of NLPSVM cubicly depends on the number of the active constraints in 
                           
                              
                                 
                                    A
                                 
                                 
                                    s
                                 
                                 
                                    T
                                 
                              
                              v
                              ⩽
                              1
                           
                         and 
                           
                              -
                              
                                 
                                    A
                                 
                                 
                                    s
                                 
                                 
                                    T
                                 
                              
                              v
                              ⩽
                              1
                           
                        . By Theorem 1 and the Complementary Slackness Theorem, the number of the active constraints in 
                           
                              
                                 
                                    A
                                 
                                 
                                    s
                                 
                                 
                                    T
                                 
                              
                              v
                              ⩽
                              1
                           
                         and 
                           
                              -
                              
                                 
                                    A
                                 
                                 
                                    s
                                 
                                 
                                    T
                                 
                              
                              v
                              ⩽
                              1
                           
                         is no more than ∣ESV∣ if the dual program is non-degenerate.

Originally, the step size of appending the violated constraints and deleting the relax constraints in each iterations is set to be one. By doing so, it is possible that more iterations might be needed. To decrease the number of iterations, a potential way is to increase the step size. In CG, a larger step size would lead to a serious tailing off effect [23]. Thus, the step size should not be too large.

In each iteration of CGN, it is easy to choose the most relax constraints. But it is time consuming to test whether all the constraints are satisfied and choose the most violated ones. To speed up the test procedure, we only check a random set of constraints in 
                              
                                 
                                    
                                       A
                                    
                                    
                                       s
                                    
                                    
                                       T
                                    
                                 
                                 v
                                 ⩽
                                 1
                              
                            and 
                              
                                 -
                                 
                                    
                                       A
                                    
                                    
                                       s
                                    
                                    
                                       T
                                    
                                 
                                 v
                                 ⩽
                                 1
                              
                            in each iteration, which can be implemented by randomly grouping the constraints into several subgroups and testing only one of them in each iteration. Consequently, the selection of the most violated constraints is also performed on the subgroup. Similar idea also appeared in some other large scale kernel methods [18,24].

In Algorithm 1, NLPSVM is used to solve a subproblem in each iteration. But the solution v is re-initialized with some fixed vector or randomly generated vector in each iteration in NLPSVM. It is irresponsible and inefficient to discard those useful information generated in previous iterations and to re-initialize v. One available way to efficiently use the previous information is to initialize v with the solution to the previous subproblem, because it is possibly closer to the optimum than a random one.

When the tolerance Tol takes a very small value, NLPSVM needs a lot of iterations to terminate. For a sequence of subproblems, it is not necessary to find the exact solution for each one; we only need some information to determine which constraints should be appended or deleted. Thus we set the maximum number of iterations MaxIters to be a relative small number, say 100 in our approach. An approximate solution is sufficient when there exist some obvious violated constraints. Otherwise, we need to run NLPSVM several times until NormGrad
                           <
                           Tol, where NormGrad is the norm of the difference between solutions obtained in the last two iterations. Consequently, the initialization of v, as given above, is required.

To validate the efficiency of CGN, we compare CGN with other four methods, including the simplex method, CG, NLPSVM and LPChunking [18]. All these methods are implemented in MATLAB on the personal computer with a 2.93GHz Intel (R) Core (T)2 Duo CPU and 2G bytes of memory. This computer runs on Windows 7, with MATLAB 7.1 and VC++ 6.0 compiler installed.

At first glance, there are so many parameters need to be set in the CGN algorithm, which may be obscure. Furthermore, since the scale of each subproblem varies with the generated columns, the choice of parameters is critical for the stability and speed of CGN. Now we begin with a discussion of some basic schemes of parameter setting. The parameter setting for both CGN and NLPSVM is shown in Table 1
                        .

Some parameters can be simply set to constants, such as θ, θ
                        0 and θ
                        1. Similar to [10], we set θ
                        =103. The parameter θ
                        0 can control the precision of satisfied constraints. In our approach, θ
                        0
                        =ℓ×10−8, where ℓ is the number of training samples. θ
                        1
                        =10−3. Now we consider the parameters MaxIters, Tol and ε. As described in Section 3, MaxIters
                        =100. In the following, the performance of CGN will be compared with the simplex algorithm and CG; both of them are exact methods for solving linear programming. We expect CGN is as accurate as them, thus we set Tol
                        =10−10. In practical applications, Tol can be chosen much looser. A lot of empirical evidence shows that the recognition rate could be constant when Tol
                        <10−8 in our experiments. ε is another parameter related to the precision of the solution, since an inappropriate choice of ε could make the satisfaction of NormGrad
                        <
                        Tol become very difficult. ε should approach to zero in theory. Fortunately, the parameter ε is not so sensitive to the solution precision. In our approach, we choose ε from the set {10−6,10−5,…,10−1} in an ascending order. Once the satisfaction of NormGrad
                        <
                        Tol becomes easy, the process of choice will be immediately terminated.

Similar to [10], the Armijo rule is also not employed in our approach for the consideration of time, which makes the setting of parameter δ in NLPSVM become relatively difficult. The reason is that δ is sensitive to the convergence and converging speed. An asymptotic rule for δ is introduced in our approach. Namely,
                           
                              
                                 δ
                                 =
                                 
                                    
                                       
                                          
                                             δ
                                          
                                          
                                             0
                                          
                                       
                                    
                                    
                                       1
                                       +
                                       
                                          
                                             t
                                          
                                          
                                             2
                                          
                                       
                                    
                                 
                                 ,
                              
                           
                        where t is the index of the current iteration. Now the choice of δ is transferred to δ
                        0. Fortunately, δ
                        0 has a less sensibility to the convergence than δ. Here, δ
                        0 is chosen by tuning the value 10
                           j
                         with j
                        =−5, −4, …, 0, 1.

The simplex method is used to solve the primal linear programming (7) of 1-norm SVM. The CG algorithm is essentially row generation only for the constraints 
                           
                              
                                 
                                    A
                                 
                                 
                                    s
                                 
                                 
                                    T
                                 
                              
                              v
                              ⩽
                              1
                           
                         and 
                           
                              -
                              
                                 
                                    A
                                 
                                 
                                    s
                                 
                                 
                                    T
                                 
                              
                              v
                              ⩽
                              1
                           
                         in the dual problem (4) with the rest constraints y
                        
                           T
                        
                        v
                        ⩽
                        σ, −y
                        
                           T
                        
                        v
                        ⩽
                        σ and v
                        ⩽
                        C
                        1 fixed in the working set. In CG, each of the subproblems is solved by the simplex method and the parameters θ
                        0 and θ
                        1 have the same values as in CGN. In LPChunking, the subproblem is also solved by the simplex method and the group size is eight, which usually is the optimal choice as shown in [18].

In this experiment, an artificially generated data set, called Ringnorm [25–27] is used to test the performance of CGN. There are two classes in this set, and each sample has d features. Class one is multivariate normal with a mean of zero and a covariance matrix of 4I, where I is an d
                        ×
                        d identity matrix. Class two is also multivariate norm with a mean 
                           
                              [
                              a
                              ,
                              a
                              ,
                              …
                              ,
                              a
                              ]
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    d
                                 
                              
                           
                         and a covariance matrix I, where 
                           
                              a
                              =
                              2
                              /
                              
                                 
                                    d
                                 
                              
                           
                        .

We analyze the effect of the step size in our method. Let d
                           =20 and ℓ=200 in the Ringnorm data set. The step size varies from 1 to 50. The linear 1-norm SVM and the RBF kernel are trained by CGN, respectively. The regularization parameter C and the kernel parameter are selected from the set {0.1,1,10,100,1000} and {20,21,…,28} by applying the 3-fold cross validation method, respectively. Finally, we set C to be 10 and the kernel parameter to be 24.

We repeat 20 trials and report the average results in Fig. 1
                           . From Fig. 1a and c, we can see that the training time sharply decreases when the step size increases at first, and varies slightly when the step size is no less than certain value. The classification errors have variations similar to that of the training time. Generally, a larger step size may lead to a serious tailing off effect in the CG method [23]. Since CGN is the combination of CG and NLPSVM, it is expected to have this drawback when using a larger step. But we do not observe such a tailing off effect. The reason is that we set the maximum iteration number to be 100, which could stop the algorithm in time. In the following experiments, we set the step size to be 5 for the linear 1-norm SVM and 3 for the kernel 1-norm SVM.

We have two schemes when performing experiments, one scheme is to vary the training sample number while fixing the dimensionality d, the other is to change the dimensionality d while fixing the training sample number.

In the classical Ringnorm data, d
                              =20 which is used here. The training sample number for each class takes value from the set {50,100,…,800}. We use 1000 test samples for each class. We report the average results on 20 trials. The curves of the training time, the training error, the test error and the standard deviation of test error versus (vs.) the total training sample number for five methods are shown in Fig. 2
                              .


                              Fig. 2a implies that the training time of five methods increases with increasing the training sample number. Note that the increasing ratio of CGN is relative small compared to other methods except for NLPSVM which has the fastest training speed. Inspection of Figs. 2b–d indicates that the five methods are comparable on their classification performance. These methods deal with the same optimization problem (1). If all of these methods can obtain the optimal solution, the optimal solution must be the same. In this experiment, these methods almost find the same optimal solution when the training number is larger than 200. Thus, the curves in Fig. 2b and c are overlapping when the training number is larger than 200. In addition, an overfitting problem occurs when the sample number is small, say ℓ=100. In order words, the training error is small, while the test error is relative large. This problem is solved by increasing the training number.

We generalize the Ringnorm data and let d vary from 2 to 50 at intervals of 4. The training sample number is 100 for each class. The other experimental conditions are the same as before. The variations of the training time, the training error, the test error and the standard deviation of test error vs. the sample dimensionality d are shown in Fig. 3
                              .


                              Fig. 3b and c show the training error decreases and the test error increases when the sample dimensionality increases, which indicates that all five methods come across an overfitting problem because the training number is insufficient. In addition, the training time is also increased when increasing the sample dimensionality. The main reason is that it requires more nonzero coefficients to represent the classifier model of the linear 1-norm SVM. For CGN, it takes more time to find the solution of the linear 1-norm SVM. The experimental results indicate that CGN is not fast when solving the linear 1-norm SVMs.

The experiment procedure of the kernel 1-norm SVM is similar to the linear case.

The total training sample number takes value from the set {100,200,…,1600}. We also report the average results on 20 runs. The curves of the training time, the training error, the test error and the standard deviation of test error versus (vs.) the total training sample number for all five methods are shown in Fig. 4
                              .

In Fig. 4b and c, the training error curves vary slightly from 0% to 0.9% and the test error curves vary from 4.1% to 1.5% when the sample number is changed from 100 to 1600. These two figures show that the larger the training number is, the better the classification performance is. From the training time curves in Fig. 4a, we can see that CGN is the fastest one among five methods and NLPSVM is only better than CG. In addition, CGN and NLPSVM have the same performance on the training and test error.

Similarly, let d vary from 2 to 50. The variations of the training time, the training error, the test error and the standard deviation of test error vs. the sample dimensionality d are shown in Fig. 5
                              . All methods have a comparative training and test errors. Both the training and test error decreases with the increasing of sample dimensionality from Fig. 5b and c, which indicates that the overfitting problem is avoided by applying kernel tricks. In the RBF kernel case, the number of nonzero coefficients is decreased for NLPSVM and CGN when increasing dimensionality. Thus, the training time curves of both NLPSVM and CGN is dropped as increasing dimensionality. CGN still consists with NLPSVM on the two performance indexes, or training error and test error. However, CGN is much faster than NLPSVM when training the RBF kernel 1-norm SVM.

From the results mentioned above, we have the following conclusions on the training speed, although the classification performance of these methods are comparative.
                                 
                                    •
                                    CG and LPChunking are relatively slow, while the simplex method is middle-ranking.

NLPSVM works well for the linear 1-norm SVM instead of the kernel 1-norm SVM, which has been stated in Section 1.

For the kernel case, CGN has the fastest training speed among these five methods. In a nutshell, CGN is very efficient for the kernel 1-norm SVM.

We evaluate the CGN algorithm on eight public data sets from the UCI Machine Learning Repository [28], including Ionosphere, Cleveland Heart, Pima Indians, BUPA Liver, Housing, Wine, Vehicle and Segment. The first five data sets are binary classification ones, and the other three are multi-class ones. Table 2
                         shows attributes of these data sets.

Here, binary classification problems are considered. Based on the conclusions in Section 4.2, we adopt the RBF kernel 1-norm SVM. In addition, we also compare the results of the RBF kernel 1-norm SVM and the standard SVM. A simple 3-fold cross validation is exploited on the simplex method to select the penalty factor C and RBF kernel parameter p in terms of the averaged classification error prior to the averaged training time. After the phase of parameters selection, a 10-fold cross validation is implemented to evaluate the performance of all methods.

The numerical results of the RBF kernel 1-norm SVM solved by six methods are given in five tables. Tables 3–7
                           
                           
                           
                           
                            show the average performance on BUPA Liver, Cleveland Heart, Housing Ionosphere, and Pima Indians, respectively. The performance indexes in these tables are explained in the following:
                              
                                 •
                                 
                                    Training Error denotes the classification error on the training set.


                                    Test Error denotes the classification error on the test set.


                                    Time is the averaged training time for one trial.

#NC, #ESV and #SV represent the number of nonzero coefficients, exact support vectors and support vectors, respectively.

#Con and #Var denote the number of the constraints and variables of the subproblem in the last iteration, respectively.

Note that the bracketed value of #Con for CGN is the number of active constraints only in 
                              
                                 
                                    
                                       A
                                    
                                    
                                       s
                                    
                                    
                                       T
                                    
                                 
                                 v
                                 ⩽
                                 1
                              
                            and 
                              
                                 -
                                 
                                    
                                       A
                                    
                                    
                                       s
                                    
                                    
                                       T
                                    
                                 
                                 v
                                 ⩽
                                 1
                              
                           , which determines the complexity of NLPSVM in CGN in the last iteration.

The optimization problems for SVM can be solved by using the LIBSVM software package [29], which is mixed programming with MATLAB and C++. Thus, the training time of 1-norm SVM is not comparable with that of SVM. Thus, we only give three indexes for SVM, or Training Error, Test Error and #SV. Note that #NC is the number of nonzero coefficients for 1-norm SVM instead of #SV which is the number of nonzero coefficients for SVM. According to the index of Test Error, 1-norm SVM outperforms SVM in three data sets. Moreover, the sparseness of 1-norm SVM (#NC) is much better than that of SVM (#SV).

The inspection of these tables shows that CGN is much faster than the other four methods in five data sets. One main reason is that each subproblem in CGN has only a little constraints from 
                              
                                 
                                    
                                       A
                                    
                                    
                                       s
                                    
                                    
                                       T
                                    
                                 
                                 v
                                 ⩽
                                 1
                              
                            and 
                              
                                 -
                                 
                                    
                                       A
                                    
                                    
                                       s
                                    
                                    
                                       T
                                    
                                 
                                 v
                                 ⩽
                                 1
                              
                           . Moreover, the number of constraints in the last iteration is as much as the number of ESVs. Therefore, NLPSVM applied in CGN has a time complexity of O(∣ESV∣3) in the last iteration. But the NLPSVM algorithm itself has a time complexity of O(ℓ3) in the nonlinear case. That is, the less fraction of the number of nonzero coefficients with the number of variables, the greater speed advantage CGN exhibits.

From these tables, the cubic relationship of the elapsed training time with the number of ESVs is not clear. Through a further observation, we find that since NLPSVM used in CGN for each iteration has a small complexity, the computation time of CGN is more dependent on the procedure of constraints testing on the non-working set and the total iteration times in the experiments. Furthermore, these tables also show that the five methods for 1-norm SVM have the compared recognition rates. Note that in order to test the accuracy of the solutions, we let all the methods run on the same ten folds. In contrast, little differences among the results obtained by these five methods imply that some methods are not exact. As shown in these five tables, for the algorithms of simplex, CG and LPChunking, the number of nonzero coefficients exactly equals to the number of ESV minus one. By Theorem 1, we know what these three algorithms attained are the exact solutions, while the CGN and NLPSVM are not so exact, though Tol
                           =10−10 is rather small.

Since we want to compare the performance of methods for solving 1-norm SVMs, two-tailed t-tests with the significant level 0.05 are performed to determine whether there is a significant difference between CGN and other methods for each UCI data set. A Win–Loss–Tie (W–L–T) summarization based on t-test is shown in Table 8
                           . The results of the table indicate that there is no significant difference between CGN and the other four methods on the classification performance, but there is significant difference between CGN and the other four methods on the performance of training time.

The experiments of multi-class problems for Wine, Vehicle and Segment data sets are reported here. Similar to SVMs, we solve a multi-class problem by considering the problem as a collection of binary classifications. This idea can be implemented by the one-against-all method [30], the one-against-one method [31], and the decision tree method [32], etc. Here, we use the fastest one, or the one-against-one method. For each data set, we perform 10 trials where the training set contains 2/3 of samples (randomly selected) of each class, and the test set contains the remaining 1/3.

Two kernels are adopted here, including the linear kernel and the RBF kernel. A 3-fold cross validation is exploited on the training set to select the penalty factor C and RBF kernel parameter p. Then, the selected parameters are applied to the whole training set. The average training time, training error and test error for the three data sets are reported in Tables 9–11
                           
                           
                           , respectively.

For each UCI data set, two-tailed t-tests with the significant level 0.05 are performed to determine whether there is a significant difference between CGN and other methods. On the Wine data set, the classification performance are identical to each other. But on the performance of training time, CGN is significantly faster than LPChunking for both linear kernel and RBF kernel. There is no significant difference between CGN and NLPSVM on the training time because the training number is small (only about 120). The simplex method is significantly faster than CGN in the case of linear kernel. On the Vehicle data set, CGN is significantly faster than the other four methods. There is no significant difference on classification performance between CGN and the other four methods. On the Segment data sets, we get the similar conclusion as the Vehicle data set, except that CGN is significantly better than NLPSVM on classification performance.

The CGN algorithm, a combination with CG and NLPSVM, is proposed to find the optimal hyperplane of 1-norm SVM. The complexity of NLPSVM depends on the number of active constraints 
                        
                           
                              
                                 A
                              
                              
                                 s
                              
                              
                                 T
                              
                           
                           v
                           ⩽
                           1
                        
                      and 
                        
                           -
                           
                              
                                 A
                              
                              
                                 s
                              
                              
                                 T
                              
                           
                           v
                           ⩽
                           1
                        
                      in the dual problem of 1-norm SVM if the number of active constraints is less than the number of variables. The CG algorithm is used to select only active constraints. By the Complementary Slackness Theorem and Theorem 1, we know that the number of active constraints in 
                        
                           
                              
                                 A
                              
                              
                                 s
                              
                              
                                 T
                              
                           
                           v
                           ⩽
                           1
                        
                      and 
                        
                           -
                           
                              
                                 A
                              
                              
                                 s
                              
                              
                                 T
                              
                           
                           v
                           ⩽
                           1
                        
                      equals to the number of nonzero coefficients when no degeneracy occurs. As a result, NLPSVM applied in CGN has a time complexity of O(∣ESV∣3) in the last iteration. Usually, support vectors are a small part of training examples, while the exact support vectors are a small subset of the support vectors. An asymptotic bound of support vectors for 2-norm regularization SVM is provided in [33], which can informally be stated as: With probability tending to 1, the fraction of support vectors is essentially greater than the Bayes risk when the number of training examples tends to infinite. An convergence of CGN in finite steps is also shown in this paper.

We conduct experiments on eight UCI data sets and compare our method with the other four methods: simplex, CG, LPChunking and NLPSVM. Experimental results show that all these methods have a similar classification performance. In addition, the CGN algorithm has a speed advantage over the other four methods in the kernel case. In a nutshell, the CGN algorithm is efficient to solve the kernel 1-norm SVM.

In CGN, only column generation is used. If both row generation and column generation were used to construct a subproblem, this subproblem would have a small scale, which means the subproblem can be solved faster. In the future, we will use both generation methods for solving 1-norm SVM and obtain a faster method. Presently, the CGN algorithm cannot be applied to large scale classification tasks due to the limitation of MATLAB. We plan to improve it and apply it on a large scale problem by using other programming tools.

@&#ACKNOWLEDGMENTS@&#

The authors thank Editor-in-Chief J. Lu and the two anonymous reviewers for their valuable comments and suggestions, which have helped to significantly improve the quality of this paper. We are also grateful to Professor Jinhui Xu for checking this paper. This work was supported in part by the National Natural Science Foundation of China under Grant Nos. 61373093 and 61033013, by the Natural Science Foundation of the Jiangsu Higher Education Institutions of China under Grant No. 13KJA520001, by the Natural Science Foundation of Jiangsu Province of China under Grant Nos. BK2011284 and BK201222725, by the Natural Science Pre-research Project of Soochow University under Grant No. SDY2011B09 and by the Qing Lan Project.

@&#REFERENCES@&#

