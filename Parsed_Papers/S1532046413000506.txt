@&#MAIN-TITLE@&#With how many users should you test a medical infusion pump? Sampling strategies for usability tests on high-risk systems

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Legacy approaches to sample size prediction in usability studies are faulty.


                        
                        
                           
                           Late control means adding new users until a preset target for discovery is met.


                        
                        
                           
                           The LNBzt model predicts completeness of discovery.


                        
                        
                           
                           Low effectiveness and high volatility call for large samples.


                        
                        
                           
                           A novel idiosyncrasy score discovers atypical users (for embracing them).


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Usability

Infusion pump

Patient safety

Ergonomics

User testing

Sample size

@&#ABSTRACT@&#


               
               
                  Usability testing is recognized as an effective means to improve the usability of medical devices and prevent harm for patients and users. Effectiveness of problem discovery in usability testing strongly depends on size and representativeness of the sample. We introduce the late control strategy, which is to continuously monitor effectiveness of a study towards a preset target.
                  A statistical model, the LNBzt model, is presented, supporting the late control strategy. We report on a case study, where a prototype medical infusion pump underwent a usability test with 34 users. On the data obtained in this study, the LNBzt model is evaluated and compared against earlier prediction models.
                  The LNBzt model fits the data much better than previously suggested approaches and improves prediction. We measure the effectiveness of problem identification, and observe that it is lower than is suggested by much of the literature. Larger sample sizes seem to be in order. In addition, the testing process showed high levels of uncertainty and volatility at small to moderate sample sizes, partly due to users’ individual differences. In reaction, we propose the idiosyncrasy score as a means to obtain representative samples. Statistical programs are provided to assist practitioners and researchers in applying the late control strategy.
               
            

@&#INTRODUCTION@&#

Well-designed medical devices of good quality are necessary for providing safe and effective clinical care for patients. Capturing the user requirements and incorporating them into the design is essential. Therefore, the field of Human Factors has an important role to play in the development of medical devices, all the more so because numerous reports show clear links between hazards and usability problems [1,2].

The field of usability engineering has developed an array of methods to identify usability problems, most importantly empirical usability testing. However, the practices for usability testing have been established for evaluating non-critical systems, such as commercial websites. A major impact factor for effective usability tests is the sample size, but the prevalent recommendations for usability testing studies may not be adequate for safety critical systems, such as medical devices. Moreover, due to mathematical misconceptions [3], prevalent recommendations generally understate the adequate sample size. In consequence, a considerable number of usability problems can go unnoticed, placing severe risks on patients and users. In this paper we present a rigorous approach to sample size estimation, that, in essence, continuously tracks the completeness of problem discovery.

The presented approach bases on an updated mathematical model for sample size estimation (previously introduced in [4]). We applied it in a case study, where the prototype of a medical infusion pump is tested. First, we measure the observed effectiveness and compare it to classic models. Next, we examine the reliability of predictions and compare it to the volatility of the usability testing progress. Finally, we compare the impact of two professional groups (nurses and anesthesiologists) and extend the approach to also assist in compiling representative user samples.

The report “To err is human” from the Institute of Medicine [41] greatly increased people’s awareness about the frequency, magnitude, complexity, and seriousness of medical accidents. As many as 100,000 deaths or serious injuries each year in the US result from medical accidents. Similar reports have been issued by other authorities, e.g. France [42] and the UK [43]. Between 2005 and 2009, the FDA collected approximately 56,000 reports of adverse events associated with the use of infusion pumps, which are medical devices that deliver fluids into a patient’s body in controlled amounts [21]. A significant number of reported adverse events, many of them led to injuries and deaths, is due to device use errors. These are errors in how a medical device is used, rather than a technical malfunction. It is now widely recognized that poorly designed user interfaces induce errors and operating inefficiencies [44], even when operated by well-trained, competent users.

The recognition of the role of good design has resulted in a number of studies investigating the usability of medical devices, most notably infusion pumps [1,2,5–7]. User interfaces of medical equipment demand a high level of reliability in order to create prerequisites for safe and effective equipment operation, installation and maintenance [8]. Poorly designed human–machine interfaces in medical equipment increase the risk of human error [1,9], as well as incidents and accidents in medical care. Medication errors are estimated to be the major source in those errors that compromise patient safety [10–15]. These, together with other common problems with infusion pump design, may predispose health care professionals to commit errors that lead to patient harm [16]. The most common cause in erroneous handling during drug delivery tasks stems from the fact that operators have to remember (recall) everything that was previously entered, as well as detecting and recovering from errors in confusing and complex programming sequences [1,17]. Not surprisingly, most reported problems are identified as originating from lack of feedback during programming, even though interfaces should function as an external mental map (cognitive artifact) in supporting monitoring and decision making processes [17]. Infusion pumps contain numerous modes of functioning, and often present poor feedback about the mode in which they are currently set. Also, buttons are often illogically placed and marked [6]. Previous research indicated that causes for programming and monitoring difficulties resulted from infusion device complexity (flexibility), hidden behind simplified pump interfaces not designed from a human performance and fallibility point of view [18]. Users therefore become more and more a victim of clumsy automation [19], loss of situational awareness and mode confusion, often unrecognized as cause in many of the problems reported.

That user-interface issues with infusion pumps are widely regarded serious, is reflected by the FDA’s recent initiative to improve pump safety [21]. In order to assure that use-related hazards have been adequately controlled, the FDA states that three central steps are essential [22]:
                           
                              1.
                              Identify anticipated use-related hazards (derived analytically, for instance by heuristic analysis) and unanticipated use-related hazards (derived through formative evaluations, for instance simulated use testing).

Develop and apply strategies to mitigate or control use-related hazards.

Demonstrate safe and effective device use through human factors validation testing (either simulated use validation testing or clinical validation testing).

The analytical approaches and formative evaluations are complementary, each having unique strengths and weaknesses with respect to identifying, evaluating, and understanding use-related hazards early in the design process. Formative evaluations can demonstrate sufficient use-safety for an infusion pump. Formative evaluation has its strengths in a focus on critical tasks, challenging or unusual use scenarios and the follow-up to determine the cause of task failures. Potential limitations of formative evaluation include artificial testing conditions and limited range of users and use conditions. Clinical validation testing has its strengths in realistic testing conditions (e.g., time pressure, distractions, noise, glare), a broader range of users, and unanticipated use conditions, but potential limitations include lack of control over use scenarios and testing conditions.

In reaction to what one could call the “ergonomic crisis”, numerous works have aimed at transferring established concepts of user-centered design to the domain of medical devices. In particular, usability evaluation gained attention: Martin et al. review a number of user-centered methods for requirements analysis and usability evaluation of medical devices [17]. In their conclusion, they clearly favor usability testing over expert inspection methods such as heuristic evaluation and cognitive walkthrough. Liljegren reports on a survey on the importance of several usability criteria for medical equipment and found ‘difficulty to make errors’ ranked highest [20]. From a subsequent assessment of several evaluation methods it was concluded that usability testing is the most effective evaluation method.

In the current paper, we take the position that measuring and controlling the effectiveness of formative evaluation, usability testing in particular, is crucial for risk reduction in the development of medical devices. Undiscovered design faults decrease performance (e.g., by imposing higher cognitive workload) and raise the probability of hazard (e.g., mistakes made when inserting or modifying the dosage), harming peoples’ health. While many studies have addressed various impact factors on effectiveness of usability evaluation, there is general agreement on one factor: the sample size. However, Bastien [23] reviews the usability testing method for medical applications and concludes that the “question of the number of users to test is far from being solved and requires further research” (p. 20). While the importance of sample size is beyond doubt, quantifying its impact has seen a long and heated discussion [24]. Several authors suggested so-called magic numbers [25,26]. Others introduced mathematical models to estimate the required sample size and a third fraction claims the whole issue practically irrelevant [27].

A central assumption in this paper is that usability researchers in the domain of medical devices have at least three good reasons to strive for effective discovery of usability problems: First, medical devices are high risk systems: many past incidents have shown that poor usability can cause use-related hazards and, in consequence cost lives [16]. Second, authorities have acknowledged the problems and manufacturers are now liable for thorough testing of the devices [28]. And third, medical devices are embedded devices, with much of the functionality still provided in hardware. It is well known fact in systems engineering that late fixes of safety–critical embedded devices are extremely costly [29].

In the following sections we give an overview on possible strategies for sample size managements, as well as basic statistical ideas to estimate effectiveness and sample size. The mathematical background of these ideas is introduced in Section 2.

The question of sample size is typically posed as: how many subjects are required for testing so that at least, say 85%, of the existing usability problems are discovered? The usability researcher aiming for effective usability testing of a medical device, in principle has three approaches at her disposal for controlling the sample size. The magic number approach assumes that all studies are similar in how fast they reach completeness with increasing sample size, hence it sets the sample size a priori. Lewis [30] introduced early control where the sample size gets estimated from the first few sessions, which may still be early enough for assigning resources to a project. Unfortunately, it seems that with small samples, early estimates are far too uncertain to be of practical value [4]. In consequence, a late control strategy has been suggested that can guide the process towards the targeted completeness of discovered usability problems [3]. The usability researcher continuously monitors the progress, and invites further participants to the testing lab, until the preset target is reached with sufficient confidence. In the current paper, we show how the late control strategy applies to usability testing of high risk systems, by example of a prototype medical infusion pump.

All above-mentioned strategies ground on mathematical estimators for the effectiveness of a usability evaluation process, in order to predict the required sample size. Virzi [31] was among the first to propose that the discovery rate d of usability problems follows a geometric series, depending on the probability of detection p and sample size n.
                              
                                 (1)
                                 
                                    d
                                    =
                                    1
                                    -
                                    
                                       
                                          (
                                          1
                                          -
                                          p
                                          )
                                       
                                       
                                          n
                                       
                                    
                                 
                              
                           
                        

Reviewing 11 usability evaluation studies, Nielsen and Landauer [32] found that p averages to approximately .31. However, p seemed to vary considerably between studies (sd=.12), leaving considerable uncertainty about the effectiveness of a particular study. Still, many practitioners and academics have come to the belief that this average p holds for any usability study, hence the often made recommendation that five users suffice to find 85% of the problems. Hwang and Salvendy [33] set out to correct the number 5 by an updated meta study, concluding that the magic number is to be found in the range of 10±2. In a recent review of this debate, the first author reached the conclusion that magic numbers are simply meaningless [3].

Two mathematical misconceptions led past researchers to overrate the average effectiveness of usability testing: assuming homogeneity of problem visibility (i.e., a problem’s likelihood of being discovered) and completeness of problems. Lewis [30] pointed out the problem of incompleteness with the original geometric series model. The naïve estimator for p depends on the true number of problems as:
                              
                                 (2)
                                 
                                    
                                       
                                          p
                                       
                                       
                                          ˆ
                                       
                                    
                                    =
                                    
                                       
                                          #
                                          
                                          successful
                                          
                                          discovery
                                          
                                          events
                                       
                                       
                                          #
                                          
                                          problems
                                          ×
                                          sample
                                          
                                          size
                                       
                                    
                                 
                              
                           
                        

In principle, the true number of problems is unknown to the researcher, since the set of usability problems progressively emerges with increasing sample size. In the early stages of an evaluation study, the number of known usability problems can be much lower than the true number of usability problems. Hence, using the number of so-far-discovered problems in (1) will grossly overstate p. In (1), an overstated p yields an overly optimistic estimation of discovery rate d and an underestimation of sample size necessary for a preset target.

A second insufficiency of the geometric series model is that it assumes homogeneous visibility of all usability problems. That is, p has the same value for every type of problem [34]. This is an unrealistic assumption and several researchers have expressed their disbelief of homogeneous visibility [35,36]. What has long been overlooked is that variance in visibility of problems substantially decelerates the progress of finding problems. The 2006 edition of the International Encyclopedia of Ergonomics and Human Factors says, “There is no compelling evidence that a probability density function would lead to an advantage over a single value for p.”[37]. The opposite seems to hold: the assumption homogeneous visibility is typically false and ignoring visibility variance results in severely overestimating the true progress in problem discovery [34].

The now classic geometric series model for sample size estimation is optimistically biased for two reasons: incompleteness is not regarded and the model assumes homogeneous visibility. Lewis [30] suggested a first solution to incompleteness, a smoothing method for binomial data known as the Good-Turing adjustment. Another solution is to use zero-truncated distributions. Zero truncation is the more general solution than the Good-Turing adjustment as it applies to a wider range of count data models, especially those incorporating visibility variance.

Earlier [4], we introduced the zero-truncated logit-normal binomial model (LNBzt), accounting for both issues: incompleteness and visibility variance.

With the LNBzt model it is possible to
                              
                                 •
                                 estimate the proportion of usability problems that rest undiscovered at a given point in time,

extrapolate the evaluation process and predict the required sample size for a given discovery target, say 85% of the usability problems,

determine the accuracy of predictions by constructing confidence intervals.

In Section 2, the mathematical background is explained in more detail. Furthermore, the appendix of this paper provides the basic statistical programs necessary to perform the late control strategy by virtue of the LNBzt model.

The current EU guidelines on usability of medical devices NEN-EN-IEC 62366 [28] recognizes that diversity of users is an issue (p. 48) and explicitly asks for representative user samples.

As Caulton [35] argued, completeness of problem discovery can be very much a question of representative sampling. Many factors may play a role for users’ expectation, interaction style and performance in operating a device. Different professional groups may use a device with different backgrounds, have different tasks and work under different conditions. Previous experience may have positive or negative consequences on performance with a newly designed device [38]. While domain expertise may prevent a user from making certain mistakes, experience with legacy devices may cause a negative transfer [39].

Following the arguments of Caulton [35], discovery of usability problems likely is incomplete, if a certain subpopulation of users is omitted. If a user type is omitted or under-represented in the sample, the usability researcher is at risk to overlook usability problems that in practice may cause hazards.

The differences between professional groups is explicitly mentioned in the FDA draft guidelines [40], pointing out that members of professional groups potentially differ in their requirements and, in consequence, experience different problems when working with a device. The draft guidelines recommend testing 15 users of each major user group during validation testing.

The current FDA guidelines [22] are less explicit about the sample size per user group, but make another important point about user diversity: “Outlier data from performance measures is often informative and should be investigated to determine the nature and pattern of the use scenarios associated with them.” (p. 26). Not all user traits influencing interaction with the device can be known in advance, and sampling by professional groups may not capture the full diversity. Later in this paper, we suggest a procedure to discover under-represented user groups by identification of untypical subjects in the sample. This lends itself to an improved, adaptive sampling strategy, beyond pure sample size considerations.

The aim of this paper is to advance strategies for rigorous usability testing of medical devices. In the past, much has been said about effectiveness of usability evaluation methods, see [45] for a critical review. The focus here is on strategies for managing the sample. More specifically, we examine how the LNBzt model applies to late control of usability testing studies.

First, the overall fit of the LNBzt model will be compared to the legacy geometric series model. Second, a Monte-Carlo sampling experiment shows how well the LNBzt model interpolates the observed progress of problem discovery. Third, we examine how consistent sample size predictions are. Fourth, we evaluate reliability of the discovery process and precision of estimates, by assessing the amount of uncertainty and volatility in problem discovery. Fifth, we examine the differences between professional groups and, finally, propose a procedure to identify untypical subjects (“outliers”) in the sample.

The following sections explain the mathematical background of the LNBzt model.
                        2
                        This section can safely be skipped by the impatient or mathematically inapt reader.
                     
                     
                        2
                      The statistical programs for doing basic sample size control with the LNBzt model are provided with the electronic copy of this paper and demonstrated in Appendix A.

The classic geometric series model (1) for sample size prediction can be derived in a number of ways: first, it is a growth curve with diminishing returns. The number of discovered problems asymptotically reaches the true number of problems (preview Fig. 3). A consequence of this asymptotic behavior is that with increasing number of test sessions the gain in terms of newly discovered problems decreases. Second, the geometric series model is the cumulative distribution function (CDF) of the geometric probability distribution. The geometric probability function expresses the probability for a certain number of failures before the first success in a Bernoulli experiment. In the usability evaluation process the Bernoulli trials are the participants’ “attempts” to stumble upon a usability problem.

Finally, the geometric series model can be derived from the better known binomial distribution. The binomial probability distribution function pdfBin(k|p,
                           n) expresses the probability of k successes with n trials and a basic probability of success p.
                           
                              (3)
                              
                                 
                                    
                                       pdf
                                    
                                    
                                       Bin
                                    
                                 
                                 (
                                 k
                                 |
                                 n
                                 ,
                                 p
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   n
                                                
                                             
                                             
                                                
                                                   p
                                                
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       p
                                    
                                    
                                       k
                                    
                                 
                                 
                                    
                                       (
                                       1
                                       -
                                       p
                                       )
                                    
                                    
                                       n
                                       -
                                       k
                                    
                                 
                              
                           
                        
                     

The binomial pdf predicts the number of successes. In usability testing, however, progress of discovery occurs when a problem has been observed at least once. The relevant question is: how likely is it that a problem is discovered at least once, hence k
                        >0? The mathematical problem simplifies by taking the opposite event: how likely is it that a problem remains undiscovered after n sessions, hence k
                        =0. Let PD
                        (n|p) denote the probability of successful discovery with basic probability p and sample size n, obtains the geometric series formula:
                           
                              (4)
                              
                                 
                                    
                                       P
                                    
                                    
                                       D
                                    
                                 
                                 (
                                 n
                                 |
                                 p
                                 )
                                 =
                                 1
                                 -
                                 
                                    
                                       pdf
                                    
                                    
                                       Bin
                                    
                                 
                                 (
                                 k
                                 =
                                 0
                                 |
                                 n
                                 ,
                                 p
                                 )
                                 =
                                 1
                                 -
                                 
                                    
                                       (
                                       1
                                       -
                                       p
                                       )
                                    
                                    
                                       n
                                    
                                 
                              
                           
                        
                     

The binomial model, from which the geometric series formula derives, has a remarkable property: variance depends strictly on the parameter p, as var
                        =
                        np(1−
                        p). If the observed variance exceeds this term, this is called overdispersion. Overdispersion indicates that probability of success is not fixed for all observations and instead varies.

When overdispersion occurs, the distribution has fatter left and right tails compared to the binomial distribution.
                           3
                           This is formally expressed by the Two-Crossings Theorem, see [69]. See [4] for an illustration.
                        
                        
                           3
                         A fatter left tail means that there is an excess in zero successes, k
                        =0, problems that have not been discovered at all. In consequence, when the basic probability p varies over problems, the binomial model underestimates the number of zero successes, i.e. the unseen problems (preview Fig. 1
                        ). It is easily seen from (4), that the geometric series model overestimates the progress in presence of an excess in zero, arising from overdispersion.

The issue is solved by adding a prior to the binomial distribution. The prior is another probability distribution to underlie parameter p. Priors are commonly used in Bayesian statistics to model previous belief. Here, the prior reflects the random variation of the parameter in the population of problems. A prior representing a random effect, not a belief, is often called empirical Bayesian prior. Statistical models with parameters allowed to vary by a prior distribution are also referred to as hierarchical models or mixture models.

The prior for p has to satisfy the range of p, which is the interval [0;1]. A commonly used prior for binomial problems is the beta distribution [46]. Here, we chose another distribution as prior, the logit-normal (LN) distribution pdfLN(x|m,
                        s
                        2) [47]. The LN distribution features a parameter m for the central tendency and s
                        2 for the variance. It is less common for modeling mixture models than the beta distribution, but has a few advantages. In several pilot trials of modeling evaluation process data, the LN estimation predicted very similar to the beta distribution, but yielded better precision for the parameters of interest (especially the number of remaining defects). Another useful model for researching usability evaluation processes is the Rasch model from psychometric test theory [48]. The logit is the inverse of the logistic function in the Rasch model. Under the assumption that the latent variable is normally distributed, both mathematical models are fully compatible. Last but not least, interpretation of the LN parameters m and s
                        2 as average and variance of visibility is quite natural for a majority of researchers being familiar with the normal distribution.

Letting p vary according to the LN distribution results in the logit-normal binomial (LNB) probability distribution of the form:
                           
                              (5)
                              
                                 
                                    
                                       pdf
                                    
                                    
                                       LNB
                                    
                                 
                                 (
                                 k
                                 |
                                 n
                                 ,
                                 m
                                 ,
                                 s
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   n
                                                
                                             
                                             
                                                
                                                   k
                                                
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       1
                                    
                                    
                                       
                                          
                                             2
                                             π
                                             
                                                
                                                   s
                                                
                                                
                                                   2
                                                
                                             
                                          
                                       
                                    
                                 
                                 
                                    ∫
                                    
                                       0
                                    
                                    
                                       1
                                    
                                 
                                 
                                    
                                       p
                                    
                                    
                                       k
                                       -
                                       1
                                    
                                 
                                 
                                    
                                       (
                                       1
                                       -
                                       p
                                       )
                                    
                                    
                                       n
                                       -
                                       k
                                       -
                                       1
                                    
                                 
                                 
                                    
                                       e
                                    
                                    
                                       
                                          
                                             
                                                
                                                   (
                                                   logit
                                                   (
                                                   p
                                                   )
                                                   -
                                                   m
                                                   )
                                                
                                                
                                                   2
                                                
                                             
                                          
                                          
                                             2
                                             
                                                
                                                   s
                                                
                                                
                                                   2
                                                
                                             
                                          
                                       
                                    
                                 
                                 dp
                              
                           
                        
                     

This function does not simplify further, hence solving it requires quadrature methods for integration. Still, the LNB is a discrete probability distribution; therefore, deriving the cumulative distribution and quantile (percentile) functions is straightforward. The cumulative logit-normal geometric distribution function applies for predicting the rate of discovery. It is derived from the LNB in the same way the geometric series model was derived from the binomial distribution in (4).

Given the frequency distribution of how many times usability problems were encountered, the LNB model allows to estimate the parameters m (reflecting the average visibility) and s
                        2 (reflecting the variation in visibility) using the method of maximum likelihood. By virtue of the added variance parameter, the LNB model accounts for over-dispersion in the observed frequency of detection. In particular, it captures the fat left tail of the distribution, resulting in a more plausible estimate for the number of unseen problems, which is the point k
                        =0 (preview Fig. 2
                        
                        ).

The LNB distribution, as introduced so far, has a valid range of zero to the maximum possible number of discoveries, which is the sample size n. However, with zero successes as its lower bound, the model “expects” that the observed data contains the problems that have not yet been discovered. This is insufficient, because the researcher does not have any knowledge on the number of undiscovered problems. And, as will be shown soon, estimating the number of undiscovered problems, basically is the same as estimating effectiveness. In the following it is outlined, how the model is adjusted accordingly, which naturally leads to an estimator for the number of undiscovered problems.

In contrast to the smoothing methods suggested by Lewis [30] (see Section 1.3.3), here the issue is resolved by limiting the range of the LNB distribution to exclude k
                        =0 and re-adjusting the probability mass to 1. The so called zero-truncated LNBzt function derives as follows:
                           
                              (6)
                              
                                 
                                    
                                       LNB
                                    
                                    
                                       zt
                                    
                                 
                                 (
                                 k
                                 |
                                 n
                                 ,
                                 m
                                 ,
                                 
                                    
                                       s
                                    
                                    
                                       2
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               pdf
                                                            
                                                            
                                                               LNB
                                                            
                                                         
                                                         (
                                                         k
                                                         |
                                                         n
                                                         ,
                                                         m
                                                         ,
                                                         
                                                            
                                                               s
                                                            
                                                            
                                                               2
                                                            
                                                         
                                                         )
                                                      
                                                      
                                                         1
                                                         -
                                                         
                                                            
                                                               pdf
                                                            
                                                            
                                                               LNB
                                                            
                                                         
                                                         (
                                                         k
                                                         =
                                                         0
                                                         |
                                                         n
                                                         ,
                                                         m
                                                         ,
                                                         
                                                            
                                                               s
                                                            
                                                            
                                                               2
                                                            
                                                         
                                                         )
                                                      
                                                   
                                                
                                                
                                                   k
                                                   >
                                                   0
                                                
                                             
                                             
                                                
                                                   0
                                                
                                                
                                                   k
                                                   =
                                                   0
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

By virtue of the LNBzt probability function, the parameters m and s
                        2 can be estimated from the observed frequencies of problem encounters, excluding the undiscovered problems. These estimates are useful in two ways: First, one can derive a function for progress of discovery in the same way as the geometric series function is derived from the binomial distribution (see Eq. (4)). Second, one can easily obtain an estimator for the number of not yet discovered usability problems
                           4
                           Throughout, D and d are used as designators for usability problems, as p too easily is confused with probability or the binomial parameter. The reader may imagine “d” as denoting “defect” or “design flaw”.
                        
                        
                           4
                         
                        d
                        0 by entering the obtained estimates for m and s
                        2 into the non-truncated distribution function. By solving the equation with k
                        =0 and multiplying by the number of discovered problems d, one obtains an estimate d
                        0 for the number of not yet discovered problems:
                           
                              (7)
                              
                                 
                                    
                                       d
                                    
                                    
                                       0
                                    
                                 
                                 =
                                 
                                    
                                       pdf
                                    
                                    
                                       LNB
                                    
                                 
                                 (
                                 k
                                 =
                                 0
                                 |
                                 n
                                 ,
                                 
                                    
                                       m
                                    
                                    
                                       ˆ
                                    
                                 
                                 ,
                                 
                                    
                                       s
                                    
                                    
                                       ˆ
                                    
                                 
                                 )
                                 d
                              
                           
                        
                     

In the late control strategy, the estimator for undiscovered problems serves as an indicator for incompleteness, or in the opposite, the current level of effectiveness is calculated as 1−
                        d
                        0/d.

@&#METHOD@&#

The following sections describe the procedures and materials used for usability testing, data gathering and preparation. The statistical procedures are presented later, together with the results.

In our study we tested the computer simulation of a newly designed medical infusion pump, which was developed through an extensive user-centered process. The study aimed at finding usability problems and fixing them during a subsequent redesign phase.

Within two professional fields, OR anesthesiologists (N
                        =18) and ICU nurses (N
                        =18), were recruited as a convenience sample (14 males, 22 females). Subjects were employed at the University Medical Center Utrecht, The Netherlands. Complete and accountable video data from 34 subjects were available for analysis, excluding two participants due to incomplete video data. Distribution across age categories was as follows: 20–29years (n
                        =13, 38.2%), 30–39years (n
                        =10, 29.4%), 40–49years (n
                        =7, 20.6%), and 50–59years (n
                        =4, 11.8%). The number of years of infusion pump experience varied between half a year up to 30years (with a total average of almost 12years; an ICU average of 14.16years and an OR average of 9.81years). In both user groups men were, on average, more experienced than women. All OR subjects (N
                        OR
                        =17) were experienced with the Arsena Alaris infusion pump and 35.3% of them were also experienced in handling the Braun infusion pump. For the ICU subjects, all (N
                        ICU
                        =17) were experienced in handling the Braun infusion pump and 5.9% were also experienced in handling the Arsena Alaris. None of the participants had used other models, although several have previously worked in other hospitals. Of the 34 subjects, 28 (82%) replied to the post questionnaires (13 males, 15 females).

All subjects had normal or corrected to normal vision. All gave their written consent prior to the test trial and were informed about the goals of the experiment. No rewards were given for participation.

For this study we formulated a fixed set of 11 tasks covering the main functions (user goals) of the infusion pump. These tasks were representative of the work procedures of the user groups. All tasks were run through beforehand with three experts (anesthesiologists) with a view to external validity. These experts did not participate in the experiments. All tasks were estimated by the experts to be of equal difficulty and could be carried out independently of each another to prevent subjects ‘getting stuck’ during the experiment.

Known (risky) problems in controlling infusion pumps, as described in the literature [7,16] were captured in the tasks presented. Typical tasks were: interpreting the meaning of an alarm, adjusting values and type of medication, checking pump status and checking pump history after shift changeover.

@&#PROCEDURE@&#

The usability testing study was conducted in a hospital setting in a quiet room with regular artificial lighting and in the presence of the facilitator conducting the experiment (WV), who observed and took notes. At the start of the study, subjects were requested to complete a consent form and a questionnaire regarding their demographic details, their experiences in handling infusion pumps and in using computers in general. Next, subjects were seated in front of a table on which the apparatus was placed. On the display of the touch-screen computer, the simulation of the infusion pump was presented on a blue background. Eleven independent tasks were programmed into the simulation and task instructions were presented on paper. Each subject was instructed to perform a complete set of 11 tasks with the use of the touch-screen prototype and to think aloud concurrently during the performance of the task. No clues about the tasks were given beforehand or during the task. Subjects were instructed not to turn to the facilitator for support or advice during the performance of a task. Before starting the tasks, subjects were briefed on what the think-aloud protocol entailed. With their consent, video and audio data were gathered during the experiment to capture task slips and mistakes made by subjects. Screen captures were also recorded. After completing each task, subjects had to independently reflect aloud on their previous task performance, without guidance of the facilitator. One minute was available for providing retrospective feedback after which the next task was loaded for completion. All eleven tasks were presented and evaluated this way. After completion of the whole test (i.e., all 11 tasks), subjects were asked to complete three post questionnaires, concerning (1) their experiences with having to think aloud, (2) the appearance of the prototype and (3) handling the pump during task performance. The third questionnaire was structured according to cognitive and ergonomic design principles. For planning, designing and conducting this usability testing study and for related questionnaires, we used Rubin’s handbook [49]. In conducting the usability test, first the anesthesiologist user group was exposed to the simulation, followed by the ICU user group.

Video-taped observational data and audio-recorded interviews were examined for critical incidences – observations and retrospective comments. These were aggregated to 123 usability problem descriptions using the method of similar changes [50].

Problem sets from usability evaluations reportedly contain false positives; experts or users may comment on design aspects that in fact do not harm usability [51]. Consequently, a subsequent review was undertaken to sort out false positives.

The 123 potential usability problems were reviewed in a three-step triage-like procedure to separate usability problems from irrelevant observations: First, problems that were directly observed during interaction were always taken as valid problems. Second, the remaining problems were individually mapped to the matching questions of the post-test questionnaire. Problems that were related to at least one negative rating were taken as valid. Problems related to unambiguously positive satisfaction ratings were taken as potential false positives and finally underwent an expert screening (WV).
                           5
                           More than 10 years of professional experience as an inspector for industrial safety qualify WV as an expert.
                        
                        
                           5
                         By this procedure, 16 problems were discarded as almost sure false positives and NP
                           
                        =107 problems remained in the data set. Note that the following data analysis is strongly based on frequency of problem occurrence. In the triage, problem frequency was not taken as a criterion for problem validity, in order to prevent circular conclusions.

We created a collection of statistical routines within the scientific computing environments R [52] and provide it in Appendix A. The collection contains the standard probability functions to work with the LNBzt. A number of high level functions are provided to perform the late control strategy. Appendix A is a tutorial on installation and demonstration of use of the programs.

@&#RESULTS@&#

We present the results from four perspectives: First, the state of affairs after the study has completed with n
                     =34 is analyzed a posteriori. The LNBzt model’s fit is assessed and compared to the legacy geometric series model. Then follows a chronological analysis, describing how the study progressed, and showing how the LNBzt supports the late control strategy. Third, the as-if analysis compares the chronological process to alternative sequences of participants. During the chronological and as-if analyses, unexpected jumps in the progress are observed. The final analysis looks at atypical detection patterns of participants as a possible cause and introduces the idiosyncrasy score.

The data set contains n
                     =34 independent sessions. After the triage a set of 107 confirmed usability problems remained, for example:
                        
                           •
                           Parameter ‘weight patient’ is most often filled in at first, but presented at the bottom of the list in the supporting calculation function.

Meaning of button BOLUS not clear. Often misinterpreted as a mark.

OK button has different meaning in selecting parameters, affirming and navigating. Several subjects were unsure about the consequences of pressing OK in several situations.

Absence of feedback after activating the bolus.

Absence of the option to first turn off the alarm sound and then take action (an ICU has to be as quiet as possible).

A green light is on, even when the pump is not running, although this can be an undesired or harmful state.

Participants discovered a problem 877 times. According to (2), the naïve discovery rate is 24.1%. The least sensitive participant discovered 14 problems, and the most sensitive 39 problems, with a mean of 25.79 discoveries per participant (24%). 14 participants contributed at least one problem they discovered exclusively.

Problems ranged from 1 to 28 in how often they were recorded. 19 problems were discovered by at least half of the participants. 18 problems (17%) are singletons; they were recorded on one participant only.

Both, the binomial and LNB model are estimated via the maximum likelihood method using the margin sum on problems (i.e., the frequency of discovery). Figs. 1 and 2 show the observed margin sums compared to the estimated models. Apparently, the observed distribution has a much fatter left tail and a longer right tail than the binomial model. The observed variance of 49.3 exceeds the variance expected under the binomial model of 107×0.24×(1–0.24)=19.5. This excess in variance is an indication for overdispersion; problems vary in visibility.


                        Table 1
                         shows the results of the two competing models: The zero-truncated binomial model predicts that the study is complete – no problems remain undiscovered (d
                        0
                        =0). The LNBzt model, in contrast, predicts that d
                        0
                        =15 problems remain undiscovered, which equals a discovery rate of only 88%. The lower Akaike Information Criterion (AIC) of the LNBzt model confirms the much better fit, even though this model is more complex (i.e., less parsimonious).
                           6
                           For an introduction to model selection with information criteria see [70].
                        
                        
                           6
                         The parameter s
                        2 is clearly positive, indicating strong variance in problem visibility.

The following analysis interpolates the progress of finding problems. A parametric interpolation of the progress is created using the estimates from Table 1, comparing the geometric and logit-normal geometric series models. Figs. 3 and 4
                         show the match between interpolation by the two models and the observed progress of the study. The observed progress is represented in two different ways: First, the chronological progress of finding problems as it happened during the study is plotted. Second, a Monte-Carlo (MC) sampled progress with 500 random realizations of every possible sample size between 1 and 34
                           7
                           Note that there are only 34 different realizations at n
                              =33 and only one at n
                              =34.
                        
                        
                           7
                         was created. A problem for the MC interpolation arises from the potential incompleteness. As we have seen, the LNBzt estimation indicates around 15 (12%) not-yet observed problems. Based on the 107 observed problems alone, the chronological progress and the MC interpolation both will necessarily arrive at 100%, whereas the parametric interpolation arrives at 88%. We solved this pragmatically by imputing d
                        0
                        =15 virtual problems (i.e., the most likely number of unseen problems). This procedure may appear somewhat tautological. However, it merely fixes the position of the end point at n
                        =34. The overall curvature of the progress function is not affected; deviations may well occur, showing that the interpolation does not fit well.

No such imputation was necessary for the binomial/geometric model, as it predicts zero unobserved problems. Fig. 3 shows that the chronological and MC progress are very close, with some waviness of the chronological progress. In contrast, the geometric series interpolation shows a curvature very different from the observed progress. At any given moment, the parametric interpolation overstates the progress. For example, according to the geometric series interpolation the 85% target was met with eight sessions already. In fact, only 85% of the known problems were discovered after session 13. This calculation does not even consider possibly undiscovered problems (as indicated by the LNBzt model).

In contrast, Fig. 4 reveals a good match between the LNBzt parametric interpolation and the observed progress (MC and chronological). Whereas the MC interpolation closely resembles the parametric interpolation, the chronological progress shows a few deviations. The strongest deviation is observed for the first participant, finding 39 (32%), instead of the expected 25 (21%) problems. Also, participants 3, 12 and 22 show an above average yield in new problems. Note that due to the cumulative nature of the process, an above average contribution declines only gradually with further session. For example, at session 5 the observed progress is still above expectation, but we cannot attribute this to an extraordinary contribution of participant 5. In a later section, we will scrutinize further the volatility of the process due to individual contributions of participants.

The poor fit of the binomial marginal sum (Fig. 1), the lower AIC and unrealistic estimate for d
                        0 (Table 1) and the strongly deviating interpolation (Fig. 3), add to the growing body of evidence that the geometric series formula is inappropriate for predicting the usability evaluation progress [3,4,34,53]. Consequently, the following sections will pursue analysis with the LNBzt model only.

Finally, the 88% discovery rate obtained with 34 participants is sufficient information to initiate another iteration in the development cycle. If this were a final validation study, 88% appears rather low. With the LNBzt estimates we can extrapolate the process of discovery: the 90% discovery target (109 problems) is met with n
                        =42, while the 95% discovery target (115 problems) is met with n
                        =79. A 99% rate will require testing 255 participants.

The previous section has shown a good fit of the LNBzt, raising expectations that researchers can use the model for monitoring and prediction of the usability testing study. The following analysis reverts to the chronological order of events in the study, and introduces confidence intervals for making decisions under uncertainty. For demonstrating the late control strategy, it is assumed that the researcher is aiming at a target of 85% problems discovered. After testing every new subject, the researcher estimates the proportion of discovered problems d
                        0. Employing the full data set, the 85% target seems to be reached at about 28 or 29 sessions.

Starting now with session 3, the discovery rate is estimated as the number of discovered problems divided by estimated total number of problems at every new session.
                           8
                           Three sessions is the smallest possible sample size for obtaining LNB estimates.
                        
                        
                           8
                         As shown in Fig. 5
                        , the point estimator for completeness deviates strongly from the true and interpolated progress at small sample sizes. Relying solely on the point estimate, the researcher is at risk to stop the study prematurely after session 8, where 76 problems were discovered and the number d
                        0
                        =11 undiscovered problems are estimated. Hence, the estimated total number of problems is grossly understated at that point (as compared to the posterior estimation). With session 12, estimated completeness again drops below 85% and starts converging to chronological progress and parametric interpolation, with some ongoing optimistic bias.

Apparently, point estimates are not very precise. As expected by the law of large numbers, the strongest deviations happen at small sample sizes. For decision making under uncertainty, confidence intervals are recommended practice. In the next step of our analysis, confidence intervals are constructed via the bootstrapping method.
                           9
                           Often researchers resort to asymptotic normality of the likelihood function and derive confidence intervals from the Fisher information function. In contrast, bootstrapping is a resampling method, which has better accuracy at small sample sizes und with exotic models. While bootstrapping is easy to implement, it is computing intensive. The intervals here were computed by 500 bootstrap samples in steps of four sessions.
                        
                        
                           9
                         
                        Fig. 5 shows three ranges (50%, 80% and 95% confidence). Even the most liberal 25% limit effectively guards against prematurely stopping the study with an 85% target. However, for small to moderate sample sizes, the chronological progress crosses both inner lower confidence limits (25% and 10%). Even the 2.5% limit would not prevent premature termination under all circumstances. Setting a lower target, say 55% would have resulted in a mistaken termination at n
                        =7. However, one can hardly think of a scenario where the researcher wants to apply quantitative sample size management and at the same time have such a low target.

The previous section has shown that the LNBzt model is consistent with the empirical progress a posteriori, when estimates are obtained with the maximum data available at n
                        =34. However, there were serious deviations observed when tracing the chronological decision process. Unfavorably, the deviations were mostly optimistic, leaving a risk of stopping the study too early. It is unclear whether this happened due to a systematic bias of the LNBzt model or due to randomness in the process. Next, we examine both possibilities with two Monte-Carlo studies. The first MC study examines whether the LNBzt estimates depend on sample size, possibly under-estimating at smaller samples. The second study examines the volatility of the stochastic process, by sampling 100 alternative sequences from the data.

If the LNBzt estimator is unbiased at small sample sizes, it can be expected to give the same estimation for the total number of problems and the required sample size at different sample sizes, on average. In a Monte-Carlo experiment, 500 participant groups of sizes 10, 15, 20, 25 and 30 are randomly picked from the full data set. For every subsample, estimates for m and s
                        2, the total number of problems and the required sample size for a 90% target
                           10
                           The reason to choose 90% this time is that this target is not met in the study, hence it is a real prediction.
                        
                        
                           10
                         are obtained.

As can be seen in Fig. 6
                        , the average predicted sample size is not fully independent of the mean, but steadily increases from 32 to 40. The same small-sample bias happens with the estimated number of problems (Fig. 7
                        ). Again, there is strong uncertainty at small to moderate sample sizes, which diminishes steadily, but is still significant at n
                        =30.

The chronological estimations in Fig. 5 deviated significantly from the chronological progress, in an optimistic direction, typically. It seems obvious to attribute this to the sample size bias, just observed. However, this bias is rather small and would not explain the roughness of the estimated completeness curve. As we have outlined in the beginning (and will scrutinize further in the following section), subjects differed in how many observations they contributed. Hence, the order of subjects may cause deviations from the ideal progress curve. Another MC analysis may give an idea of the influence of the sequence at which subjects are tested. One hundred random sequences from the data set were generated for the following descriptions. Fig. 8
                         serves as an illustration with the number of random sequences limited to 25 for better legibility. The “caterpillar” plot below the curves indicates which proportion of alternative sequences have a lower or higher performance compared to the chronological. The chronological (i.e., true) sequence outperforms most other sequences between sessions 1 and 5, then gradually drops below the median at sessions 8–10, with poorest performance at session 9. Between session 12 and 17, the chronological sequence takes another over-performing turn, then briefly moves back to average, followed by another high performing episode.

It appears that the evaluation progress significantly depends on the sequence at which participants arrive in the lab. The chronological sequence in our study, on average is among the better performing ones. The strongly over-estimated completeness at small sample size, as observed in Fig. 8, can be attributed to the fact that the first few sessions were truly far above expectations, for whatever reasons. In conclusion, the small sample bias is hardly deniable; but does not stand out against the level of randomness and volatility observed.

The European [28] as well as US [22,40]. Human Factors guidelines for medical devices emphasize representative user sampling for effective validation testing. The common assumption is that different professional groups have their own requirements and in consequence experience different usability problems.

The sunflower plot in Fig. 9
                         shows the discovery frequency of usability problems depending on the professional group. The cloud of points appears fairly coherent and mostly resembles the diagonal; the differences between the two groups may not be very pronounced. One can spot a leaning towards the OR axis, indicating a higher sensibility of anesthesiologists. Still, a number of problems were exclusively discovered by either anesthesiologists (20, 19%) or nurses (12, 11%). But, the most of these problems have a very low overall discovery rate.

Another way to look at group differences is to compare the effectiveness of mixed group samples to “pure” samples [54]. If the two groups differ in their sensitivity for individual problems (not to be confused with the average effectiveness), then mixed groups must have an advantage in effectiveness due to a complementarity effect [55]. In a Monte-Carlo experiment, 500 samples of n
                        =10 users were created in three conditions: pure OR groups, pure IC groups and half-half mixed groups. Fig. 10
                         shows the distribution of effectiveness (number of discovered problems) in the three sampling conditions. Pure OR groups have an overall higher effectiveness (m
                        =82.8, sd=2.93) compared to pure IC groups (m
                        =77.65, sd=2.91). The mixed groups have quite the same mean effectiveness as the OR group (m
                        =82.66, sd=4.2). If both professional groups differ in overall sensitivity only, without any qualitative differences, we would expect the mean effectiveness of the mixed groups somewhere between the pure groups. As this is not the case, the disadvantage of the IC group’s overall lower sensitivity is compensated for – qualitative differences between the two professional groups exist.

In Section 4.3, it was observed that the progress strongly depends on the order of subjects entering the study. Predictors from sessions grossly deviated from the interpolation. Fig. 5 already provided some indications as to the reasons why: Subject 1 has an extreme sensitivity without doubt, discovering about one third of all problems. What is surprising at first glance is that progress and estimated completeness often seem to take opposite directions. This is particularly the case at session 8: not a single new problem is discovered, in consequence chronological completeness drops below expectation and at the same time, estimated completeness peaks.

In fact, this observation is well in line with the mathematical relation between the distribution of margin sum and the progress curve: When a subject discovers many new problems, this increases the left tail of the frequency distribution (review Fig. 2). This has two effects: variance increases and average discovery rate decreases. Increasing variance and decreasing mean both result in higher estimates for d
                        0, indicating a lower completeness. The exact opposite happens, if a subject only rediscovers problems (such as subject 8): the left tail gets thinner and the distribution is shifted to the right; higher estimates for completeness are obtained.

A chronological plot, as in Fig. 5, is not optimal to judge individual predispositions for several reasons: First, the extreme effects of one untypical subject decays only gradually when further subjects are added. In turn, the effect a subject depends on the history. Second, at small sample sizes the process is highly volatile; even rather typical subjects may show extreme peaks. Third, the process stabilizes with increasing sample size; even highly untypical subjects no longer stand out. And finally, the curvature makes visual examination cumbersome.

Based on the above considerations, atypical subjects may be identified by measuring individual subjects’ relative contribution to the estimated total number of problems ND
                         (or d
                        0, likewise). Subjects being very representative for the overall sample, pull the estimate down, subjects with rather uncommon discovery patterns push it up.

For determining the relative contribution of individual subjects to the estimate ND
                        , we devise a jackknife estimator. Jackknife estimation is a resampling method, where one omits one observation in every run. In the case of usability testing data, an idiosyncrasy score is constructed for subject i as the ratio between ND
                         estimated from the full sample and ND
                        
                        (
                        
                           i
                        
                        ) estimated by omitting subject i. When the idiosyncrasy score ND
                        /ND
                        
                        (
                        
                           i
                        
                        )
                        >1, subject i by tendency contributed rarely discovered problems and can be called atypical. The researcher may then investigate deeper to find out what makes this subject special, for instance by looking at demographic data, a post test interview, or looking for a common theme in the problems recorded on subject i.

A second property of interest is a subjects’ overall sensitivity to usability problems. A scale for sensitivity is straight forwardly constructed as the individual discovery rate divided by the average discovery rate (24.1%).


                        Fig. 11
                         displays all subjects on a plane spanned by sensitivity and idiosyncrasy. A linear regression with idiosyncrasy as predictor for sensitivity confirms near independence (F(1,32)=1.11, p
                        =0.30, η2
                        =0.03). Apparently, the two properties are well separable in the detection patterns.

Subject 1 turns out to be extremely sensitive, at the same time being rather representative. That means, chances are good that direct followers of subject 1 have a higher rate of re-discoveries. High rates of re-discoveries are an indicator for approaching completeness. This explains the strong over-statement of estimated completeness in the early process. As can be seen from Fig. 5, estimated completeness takes a sharp rise with subject 5, being highly sensitive as well. Subject 7 has a similar profile, slightly less idiosyncratic and sensitive and the corresponding rise in estimated completeness is clearly visible. Subject 28 shows the strongest idiosyncrasy of all, which is impossible to spot in Fig. 5 because the process has mostly stabilized. In turn, most subjects in the range 8–13 are rather representative (i.e. they tend to rediscover problems), explaining the steadily climbing completeness.

Subjects 5 and 7 (both OR) and 28 (ICU) reveal the strongest idiosyncrasy. Based on the limited demographic data available in this post hoc analysis, none of these subjects had one or a combination of eye-catching properties.

@&#DISCUSSION@&#

Our results showcase the LNBzt model for quantitative control of usability testing high risk systems. The LNBzt model provides estimates of much better consistency than the legacy geometric series model. It may therefore serve the late control strategy for sample size. Volatility and uncertainty of the evaluation process are strong. In evaluation of high risk systems this calls for large sample sizes, far beyond past recommendations. Furthermore, we analyzed our data with respect to the composition of samples. Differences between users exist, and must be taken into account for effective problem discovery.

The required sample size in this study is much larger than predicted by the prevalent binomial/geometric model and the magic numbers [26]. Common arguments why Nielsen’s claim of “85% with five users” may not hold, are: the complexity of modern products [24] and the diversity of users [35]. This insufficiently explains our findings, because we tested a fairly simple device (compare the few controls and parameters to, let’s say, an office productivity suite), with a rather homogeneous sample of professional users. In conclusion, we find our previous results confirmed that proposed magic numbers are far too small [3,4].

Medical devices are embedded systems for professional use; as such they have lower innovation rates than consumer products. Furthermore, developing devices for safety–critical tasks in distracting work environments justifies rigorous testing. As of writing this, the FDA is working on a standard for user-centered development of medical devices [40]. The FDA Draft Guidance also addresses the issue of sample size, but is cautious with definitive recommendations. Fortunately, it does not resort to any magic numbers, but instead illustrates the increase in effectiveness and stability by a figure taken from [56]. However, it has happened before that a statement like “on average the probability of discovery is p
                        =.31 and this means finding 85% of the problems with 5 users” degenerates over time to “Five users will find 85% and this is enough.” Proponents of “discount usability” prolong the “Five users is enough” claim [57], without seriously referring to the growing body of counter-evidence and mathematical considerations.

In our study, a reasonable discovery rate started at a sample size of 25–30. This was not intended as a validation study, but to initiate another iteration of design. Validation studies may call for much larger sample sizes, maybe in the magnitude of pharmaceutical clinical trials.

Faulkner [56] examined the effect of sample size on effectiveness; her primary conclusion was that increasing sample size makes the process more reliable. Our results confirm that effectiveness of usability testing is highly uncertain with small to moderate sample sizes. Accordingly, estimators for completeness are highly volatile.

Hence, large sample sizes are required to get stable results and precise estimations of completeness, if this is desired. We strongly recommend that researchers always compute confidence limits when making decisions in the fashion of late control. In our study, LNBzt estimates were more credible than the binomial model, but still are at best asymptotically consistent. In small samples, required sample size and number of problems are slightly underestimated. With moderate to large samples sizes, estimates seem to converge with the truth.

Sauro [58] argues that rarely and late occurring problems are less severe, because they only affect a small fraction of users. In our study we did not rate severity of problems. What comes closest to a severity judgment is the three-level triage to sort out the false alarms. Indeed, problems classified as false alarms appeared to occur at lower frequencies. Virzi [31] reported that highly critical usability problems are found quickly. In contrast, Lewis could not replicate this relationship [59]. So, it may or may not turn out that critical problems are observed at a quick pace and sample size requirements can be relaxed. However, Virzi tested a voice command system with severity judged by six experts, whereas Sauro refers to an e-commerce system. Expert judgments may carry biases and blind spots; and for medical infusion pumps, what is critical can mean something very different compared to a 90s voice command system or a webshop.

Most studies on effectiveness of user testing have web sites, business systems or consumer products as object of evaluation; not embedded medical systems, where the loss due to an undesired event can have extremely adverse consequences for patients. Use errors due to unfixed usability problems my turn out “black swans”, low frequency events with an extremely high loss [60].

In consequence, any procedure of limiting or discarding observations, has to involve either a complete risk analysis, or at least be very conservative, like the triage in our study. Stop rules for sample size must give rare events enough headroom. In fact, it follows from our mathematical considerations that the decline of once-discovered problems is a better indicator for approaching completeness than the average discovery rate.

Finally, one should never forget that usability testing studies try to predict unfavorable outcomes of millions of daily events, in a variety of complex and dynamic situations and involving diverse users, by a few dozens of video tapes, recorded under more or less controlled lab conditions.

The two professional groups, anesthesiologists and nurses, differed in discovery of problems. Anesthesiologists were more effective, and exhibited a partially different set of problems. These differences are moderate. In an earlier publication, the first author applied the same Monte-Carlo approach to compare evaluation methods and found much stronger differences [55]. Furthermore, the difference in average effectiveness found in our study is not a general fact. Sampling strategies must not be based on false generalizations, like higher educated users being more effective at problem discovery. Diverse user samples make problem discovery more efficient by the mutual compensation of “blind spots”.

Individuals differed in their disposition to discover usability problems, in quantity and quality. Some subjects are generally more sensitive to usability problems, others seem to have more or less different problems when operating the device. Like problem visibility, variance in subject’s sensitivity seems to be the norm [34]. However, as argued there, sensitivity variance does not have the same unfavorable impact as visibility variance. Quite the opposite, highly sensitive subjects make an above average contribution to the incremental progress of problem discovery.

One can also view the progress of discovery as a saturation process. Imagine, a usability researcher has identified two potential user groups A and B, and has invited representatives of A and B to the usability lab. After testing 35 participants, the discovered number of problems starts to converge with the estimated number of problems. Then, it is futile to test further participants of these groups. But, the researcher may have missed a user group C, having different expectations on how to interact with an infusion pump. Fortunately, a few members of group C were in the sample. Their number is not sufficient to discover most of their specific problems, but maybe they can be identified by untypical patterns of discovery.

For this particular purpose we suggest the idiosyncrasy score. Usability researchers should beware of seeing idiosyncrasy as unfavorable, in the fashion of social science researchers removing outliers in their samples. Untypical subjects report on less frequent problems, pushing the discovery process forward. As a disclaimer, the idiosyncrasy score cannot identify what is completely outside the study. Researchers must take care of the composition of their samples.

The results and conclusions reported were based on observations in a single case study, generally limiting the level of generalizability. In particular, all estimates reported here are limited to the system and the sample studied. Mean and variance of problem visibility, as well as required samples size vary widely between studies [4,32,34], and one must not derive any general rule, such as “85% of problems are found with 13 participants” (Section 4.1). The same caution is in order for any other findings, such as the level of uncertainty (Section 4.2), volatility (Section 4.3) and differences between professional groups (Section 4.4). However, the general finding, that ignoring visibility variance leads to an underestimation of the required sample size, matches previous results [3,4,34].

While it is our main stance that rigorous testing of medical devices requires larger sample sizes than is suggested in most of the usability literature, we have deliberately chosen not to argue against another principle of discount usability engineering, iterative testing. In particular, iterative testing has two merits: first, redesigning an interface may remove usability problems, but may also introduce new ones [61], which is only discovered by re-testing the updated design. Second, major usability problems may distract from or even obscure other problems. These may become visible only after removing the “catastrophe” in another redesign cycle (see also the following section).

In most analyses performed, we resorted to a target of 85%, but did not evaluate the performance of the model at more liberal or strict targets. To some extent, choosing the 85% target is too lenient as we are aiming at rigorous testing in high risk environments. The reason for our choice is to make comparison easy, since many past papers refer to this “magical” target. Also, we could not reach a much higher target with our comparably large sample size.

The severity of usability problems was not directly assessed. It may turn out that severe problems tend towards being discovered early, justifying smaller sample sizes. This has been discussed in Section 5.3.

Subject 1 was outstanding with unmatched sensitivity and strong representativeness. One may suspect that this is an order effect as subject 1 also was the first session analyzed. Possibly, the classification scheme for usability problems was influenced by the early observations, as kind of an anchoring effect. Indeed, several studies show that analyzing data from usability testing studies is a highly subjective process [62–64]. Further effort is needed to standardize the whole process of qualitative data analysis of usability testing medical devices.

The presented approach for estimating the required sample size acknowledges variance in visibility of problems, but does not account for variability of the population of users. As we have argued above (5.4), this issue is in fact two-part: users can differ in overall sensitivity or sub-groups of users can exist, that differ qualitatively, experiencing different set of problems.

By virtue of the LNBzt model it is well possible to estimate variance in sensitivity, as has been demonstrated earlier [34]. The reason to not include a parameter for sensitivity variance is that, in fact, it has little impact on the estimation of undiscovered problems. Compared to the strong liberal bias when omitting variance in visibility (the main stance in Section 4.1), variance in sensitivity adds a very small bias, that is conservative. Rivest proved this mathematically [65], and we verified Rivest’s conclusion in internal simulations. So, adding another parameter for sensitivity variance is possible, but complicates matters without significant practical value.

It is likely, that in a situation where distinct user groups experience different sets of problems, larger sample sizes are required. We presume that, as long as all subgroups are adequately represented, the LNBzt accounts for this sort of heterogeneity. If a subset of problems is specific for a subgroup of, let’s say, one third of the participants, this would simply mean that a certain number of problems has a lower overall visibility, which is readily accounted for by the LNBzt model. Ultimately, compiling a representative sample is at the discretion of the researcher. In principle, no mathematical model can correct for omissions of relevant user groups. By introducing the idiosyncrasy score we believe to have contributed to the identification of under-represented user groups.

However, the idiosyncrasy score has been suggested here, but not fully validated. While we identified a few “outlying” subjects, the available demographic data could not explain high idiosyncrasy of some subjects. Note however, that the very idea of idiosyncrasy does not lend itself to expecting commonalities between highly idiosyncratic subjects. Further research is required to validate and perhaps adjust the idiosyncrasy score.

Besides user heterogeneity, there are other factors that impact identification effectiveness, but are not directly accounted for by the LNBzt estimation. Most notably, Lindgaard and Chattratichart found that task coverage had a strong impact on effectiveness of problem identification [66]. Like with sampling users, preparing a valid, representative and well-formulated set of tasks is at the discretion of the researcher, as it can hardly be accounted for mathematically. However, the LNBzt approach potentially lends itself to also steer the administration of tasks. By estimating on the subset of problems that have been identified within a particular task, one can track the exhaustion of individual tasks. When the number of undiscovered problems approaches zero, an exhausted task can be replaced, ultimately maximizing the information gain. Researchers can readily try this strategy with the provided programs.

The obscuring effect of major problems (“catastrophes”, see Section 5.5) likely exists, but is not directly accounted for in the LNBzt model. Too little is currently known about this effect to attempt proper mathematical modeling. Statistical models for association rules (e.g., for market basket analysis) may be applicable, but require impractically large data sets [67]. Furthermore, the obscuring effect not necessarily imposes a strong bias on the LNBzt prediction. For illustration, assume two problems A and B, that have about the same visibility. However, A often (but not always) leads to a catastrophe, obscuring B, thereby reducing its factual visibility. In such a case the LNBzt model would “detect” a lower mean visibility and stronger visibility variance, and ultimately predict a large number of undiscovered problems. So, the mere fact that some problems are discovered at a low rate, lets the model “assume” that there must be more undiscovered problems, which is plausible. Furthermore, detecting the obscuring effect does not require a statistical model: if a participants are repeatedly unable to complete a task due to a major problem, the researcher is advised to stop the study and initiate another redesign cycle.

While the LNBzt, may sufficiently account for a number of peculiarities that are not explicitly modeled, there is one principal limitation: The model is parametric and, in particular, makes the assumption that problem visibility is normally distributed. This is an approximation, and as such, prone to violation by real data, leading to biased predictions. For example, Cahoy and Phoha [68] propose Bernstein polynomials as an alternative to model the probability density. Further studies are needed to explore potential biases that arise from the mathematical assumptions, using real and simulated data. It is our hope that the provided programs stimulate critical evaluation of the LNBzt model in particular, and the practices of testing high risk systems in general.

Developers of interactive medical devices are at risk to overlook usability problems, and in consequence end up with design flaws, potentially causing hazards. For effective usability testing in high risk domains, we recommend:
                        
                           •
                           Plan for large sample sizes, far beyond what “discount usability” practices suggest.

Apply the late control strategy: Continuously monitor progress of the study using preset targets and predictive models.

Use predictive models that account for visibility variance and unseen events. Ignore legacy approaches like the geometric series model and magic numbers.

Calculate confidence intervals to account for uncertainty, especially at small to moderate sample sizes.

Use representative samples of users, include all professional groups and think of other user traits in advance.

Monitor the results for untypical users. Never discard outliers, but take them as guides to adjust the composition of the sample.


                        
                           
                              1.
                              Go to www.r-project.org and download a recent copy of R for your operation system.

Run the setup program and follow the instructions.

When working with R more frequently, it is recommended to use an integrated development environment, such as Rstudio (www.rstudio.org).


                        
                           
                              3.
                              Create a directory 〈dir〉 for the library and data to reside in.

A library, a program and a data file are provided with the electronic version of this paper. Download all files to 〈dir〉.

Open the file tutorial.R with an R editor.

In the first line, change setwd(…) to reflect the working directory 〈dir〉 (Windows users are reminded to use “/”, instead of “/” for directory paths). The statistical routines are collected in the file “LNBPrediction.R”. They are read in by the subsequent source(…)command.

R is an interactive programming language; it is recommended to walk through the tutorial by issuing the command one by one.

The basic data format used is the binary detection matrix, with problems in rows and subjects in columns. Cell i,
                        j in the detection matrix is set to 1, if problem j was discovered by subject i, otherwise 0. Boolean values may be used instead.

Most easily, a detection matrix is prepared in a spreadsheet program, such as Microsoft Excel. As R cannot read the native format of spreadsheet programs, one has to save the detection matrix as a CSV file. The following command reads a plain detection matrix (without any row or column names) in CSV format, and creates a data frame variable DM:
                           
                              
                                 DM <- read.csv(“detectionmatrix.csv”)
                              

Later, the data can be saved in native R format by issuing:
                           
                              
                                 save(DM, file=”detectionmatrix.Rdata”)
                              

and reloaded by
                           
                              
                                 load(file=”detectionmatrix.Rdata”)
                              

The example file “infpump.Rdata” provided with the electronic version of this paper is a partial detection matrix of the infusion pump study, with 20 randomly chosen subjects. It is used in following walkthrough.

The central high-level function provided is fit.LNB, which fits the LNBzt distribution to the data, more specifically, the margin sum on problems. The margin sum is easily obtained by issuing first:
                           
                              
                                 MS <- rowSums(DM)
                              

Then, the fitting results are stored to a variable fit by:
                           
                              
                                 fit <- fit.LNB(MS, 20)
                              

If all goes well, the estimated completeness can be obtained by
                           
                              
                                 D.hat(fit)
                              

The number of undiscovered problems is obtained by:
                           
                              
                                 D.null(fit)
                              

Remember, that fitting the LNB strongly bases on numerical integration and optimization, which can fail if the data is sparse, or parameter values are extreme. In case the program throws errors, changing the starting values for the maximum likelihood estimation typically helps. For example the following statement sets the starting value for m to −2 and the standard deviation to 4 (startval defaults to −1,2).
                           
                              
                                 fit <- fit.LNB(MS, 20, startval=c(−2,4))
                              

It is also recommended to try different start values and observe whether they arrive at the same parameters.

In the previous step, point estimates for completeness were obtained. The provided boot strapping routine obtains the confidence limits for parameters of interest. First, a set of 100 boot strapping samples is obtained by:
                           
                              
                                 boot <- boot.CI(MS, 20, noruns=100, startval=c(−1.8,2.2))
                              

The function boot.LNB returns a table with 100 bootstraps and columns for m, s, D.hat and D.null. The 80% limits for estimated completeness is obtained via:
                           
                              
                                 quantile(boot
                                 
                                    
                                 
                                 D.hat, c(0.1, 0.9), na.rm=TRUE)
                              

Note that due to the fallibility of numerical optimization, a small number of bootstraps typically fails and NA is returned.

The point estimator for required sample size, say for an 85% target, is obtained by the logit-normal geometric quantile function:
                           
                              
                                 qlngeom(0.85, fit
                                 
                                    
                                 
                                 m, fit
                                 
                                    
                                 
                                 s)

The function operates on vectors of estimates, allowing one to obtain confidence limits from bootstrapping data already obtained:
                           
                              
                                 boot.85 <- qlngeom(0.85, boot
                                 
                                    
                                 
                                 m, boot
                                 
                                    
                                 
                                 s)
                              


                                 quantile(boot.85, c(0.1,0.9), na.rm=TRUE)
                              

Plots like those contained in the paper are obtained by combining the provided functions with the rich built-in plotting capabilities of R. For example, a plot similar to Fig. 2 is obtained by:
                           
                              
                                 plot(dlnbinom(c(1:20), 20, fit
                                 
                                    
                                 
                                 m, fit
                                 
                                    
                                 
                                 s),ylim=c(0,0.3), t=”l”)
                              


                                 points(tabulate(MS)/(fit
                                 
                                    
                                 
                                 problem.n + D.null(fit)),t=’h’)
                              

Note, how the estimate for unobserved problems is added to scale the observed distribution.

The parametric interpolation curve (like in Fig. 4, with an extrapolation to n
                        =40) is plotted via:
                           
                              
                                 plot(plngeom(0:40, fit
                                 
                                    
                                 
                                 m, fit
                                 
                                    
                                 
                                 s))
                              

Statistical software and example data as demonstrated in Appendix B can be found, in the online version, at http://dx.doi.org/10.1016/j.jbi.2013.04.007.


                     
                        
                           Supplementary data 1
                           
                              Data set
                           
                           
                        
                     
                     
                        
                           Supplementary data 2
                           
                              Library of statistical routines
                           
                           
                        
                     
                     
                        
                           Supplementary data 3
                           
                              Example data analysis
                           
                           
                        
                     
                  

@&#REFERENCES@&#

