@&#MAIN-TITLE@&#Congestive heart failure detection using random forest classifier

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Heartbeat classification is substantial for diagnosing heart failure.


                        
                        
                           
                           Machine learning methods classify normal and congestive heart failure (CHF).


                        
                        
                           
                           The random forest method gives 100% classification accuracy in detecting CHF.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Electrocardiogram (ECG)

Congestive heart failure (CHF)

Autoregressive (AR) modeling

Machine learning

Random forest

@&#ABSTRACT@&#


               
               
                  Background and objectives
                  Automatic electrocardiogram (ECG) heartbeat classification is substantial for diagnosing heart failure. The aim of this paper is to evaluate the effect of machine learning methods in creating the model which classifies normal and congestive heart failure (CHF) on the long-term ECG time series.
               
               
                  Methods
                  The study was performed in two phases: feature extraction and classification phase. In feature extraction phase, autoregressive (AR) Burg method is applied for extracting features. In classification phase, five different classifiers are examined namely, C4.5 decision tree, k-nearest neighbor, support vector machine, artificial neural networks and random forest classifier. The ECG signals were acquired from BIDMC Congestive Heart Failure and PTB Diagnostic ECG databases and classified by applying various experiments.
               
               
                  Results
                  The experimental results are evaluated in several statistical measures (sensitivity, specificity, accuracy, F-measure and ROC curve) and showed that the random forest method gives 100% classification accuracy.
               
               
                  Conclusions
                  Impressive performance of random forest method proves that it plays significant role in detecting congestive heart failure (CHF) and can be valuable in expressing knowledge useful in medicine.
               
            

@&#INTRODUCTION@&#

Human heart is the most important and hardest working muscle in human body, that together with blood vessels compose cardiovascular system. It pumps the blood into the every cell of the human body. Furthermore, heart muscle is the engine of the human body [1]. Heart failure is the common syndrome that progresses slowly but causes cardiac dysfunction resulted from the inability of the heart to pump the blood to all the cells of the human body efficiently. The heart weakens by heart attacks, long-term high blood pressure or an anomaly of one of the heart valves. Yet, heart failure is generally not recognized until it comes to the more progressive phase, entitled congestive heart failure, which causes fluid to flow to lungs, feet and abdominal cavity. Congestive heart failure is a condition that can be caused by heart diseases, such as coronary artery disease, damage of the heart after the heart attack, high blood pressure, valvular heart disease, diabetes and even the alcoholism [2]. According to European Heart Network and European Society of Cardiology [3], over 4 million people die from cardiovascular diseases in Europe and 1.9 million in European Union (EU) which is 47% deaths in Europe and 40% in EU.

Considering that there is no definite diagnosis of heart failure, medical diagnosis such as history or physical examinations, electrocardiography (ECG), chest radiography or echocardiography is crucial for detecting the congestive heart failure. The electrocardiogram (ECG) is noninvasive tool that records electrical activity of the heart and shows irregularities of the heartbeats. It safely examines and records the electrical impulses of the heart and show possible damages of the heart or irregularities of the heartbeats [4]. Thus, ECG is an important tool for determining the function and the health of the cardiovascular system. Moreover, it is significant to define accurate and timely diagnosis of physicians to avoid more damage and to determine proper methods and approaches [5]. Still, the problem occurs when there is insufficient number of physicians to meet the needs of patients. Therefore, it is necessary to develop an effective and automated diagnostic systems based on ECG recordings, combined with application of machine learning techniques for classification of heart diseases. These diagnostic systems will aid medical experts in detecting the irregularities in the cardiovascular system. The diagnostic system will firstly process the ECG signals taken from different subjects and hence decomposed into few features performing feature extraction. Extracted ECG signals are used to detect different types of heart failure by using various machine learning techniques [6].

According to the number of researches done, the field of heartbeats classification using different techniques is very popular. Beth Israel Deaconess Medical Center (BIDMC) congestive heart failure database were used in different studies. Baim et al. [7] were used this database to show the effect of treatment with oral milrinone. The larger group of 100 patients, with severe congestive heart failure belonging to NYHA3 and NYHA4 groups was treated with oral milrinone with an average initial dose of 27±8mg/day. Causes of congestive heart failure were different (e.g. ischemic heart disease, dilated cardiomyopathy, valve replacement, etc.). Thuraisingham used [8] BIDMC congestive heart failure database in detecting CHF from normal heartbeats, using k-nearest neighbor algorithm and features from the second-order difference plot (SODP) obtained from Holter monitor cardiac RR intervals. Six features are obtained from second-order difference plot and k-nearest neighbor algorithm is applied with the value of k
                     =1. Authors obtained a success rate of 100% in separating CHF and normal heartbeats. The same database was used by Kuntamalla and Reddy [9] and they applied sequential trend analysis to differentiate CHF patients from patients with normal heart beats. The accuracy of the proposed method is 96.68%. Furthermore, Kuntamalla and Reddy [10] applied multiscale entropy (MSE) to HRV signals, to differentiate healthy young and elderly subjects from CHF patients. They applied Reduced Data Dualscale Entropy Analysis method to reduce the data size for clear differentiation of subjects. They achieved 100% accuracy. Hossen and Al-Ghunaimi [11] used technique based on recognition of power spectral densities pattern of decomposed sub-bands of R–R interval to identify patients with congestive heart failure. They used 12 subjects from BIDMC congestive heart failure database and reached accuracy of 90%. The same authors, Hossen and Al-Ghunaimi [12] used different wavelet decomposition filters with soft decision algorithm to estimate power spectral density of RR interval data for screening patients with congestive heart failure. The accuracy value of test data with almost all wavelet filters used was 88.6%. Yu and Lee [13] focused on selecting the best feature selector to get high accuracy in recognizing the CHF. The authors proposed conditional mutual information feature selector (CMIFS) with support vector machine (SVM) classifier. The combination achieved 97.59% accuracy using only 15 features. Işler and Kuntalp [14] combined classical HRV and wavelet entropy measures to distinguish healthy patients from patients with congestive heart failure. They applied genetic algorithm to select the best ones among all possible combinations of measures. Furthermore, they used k-nearest neighbor classifier with different values of k. Finally, they got accuracy value of 96.39% with value of k
                     =5 and 7, using 8–10 features. Similar study was done by Asyali [15] and author used linear discriminant analysis to examine the discrimination power of 9 long-term HRV measures and validated the result by applying Bayesian classifier. Sensitivity and specificity value of the stated classifier are 81.8% and 98.1%, respectively. Pecchia et al. [16] investigated discrimination power of heart rate variability (HRV) in distinguishing normal subject and subjects with congestive heart failure (CHF). They performed time and frequency analysis in order to measure HRV features. The result is evaluated by applying classification and regression tree (CART) classifier. Furthermore, authors introduced two non-standard features: average of normal intervals and low/high frequencies for recording over the 24h and reached the success rate of 89.7% for sensitivity and 100% for specificity.

Feature extraction methods are important in ECG signal classification. In this paper, features are extracted by use of autoregressive Burg method. Consequently, specified signal is categorized as normal or with congestive heart failure by using machine learning techniques. In previous works [8,9,11–15,17], various machine learning techniques were applied. However, k-NN method is not capable of dealing with high dimensional data without dimension reduction, where SVM is not strong enough to handle large number of trivial data without data selection [18]. Decision tree method might not perform well with more complex interactions and overfitting might lead to the instability of the model [19]. Method efficient to achieve excellent performance, but not applied in previously mentioned studies [8–15] is random forest (RF). In this paper, RF method is used to investigate the performance of classifier for ECG signals classification in diagnosing the congestive heart failure. Moreover, performance of different machine learning techniques such as C4.5 decision tree, ANN, k-NN and SVM in ECG signal classification is compared (see Fig. 1
                     .).

The rest of the paper is organized as follows: Section 2 gives explanation about the data used and present methods applied in each step of the ECG signal classification process. Experimental results are presented and discussed in Section 3. Finally, conclusions derived from the results are summarized in Section 4.

To evaluate the performance of the proposed methods, ECG signals are taken from the group of PhysioNet databases. The paper includes ECG signals from 15 subjects from Beth Israel Deaconess Medical Center (BIDMC) Congestive Heart Failure (CHF) database. The BIDMC CHF database contains recordings of 11 male patients, aged 22–71 and 4 female patients, aged 54–63 with severe congestive heart failure, belonging to New York Heart Association (NYHA) class 3 and 4. This group of subjects is part of larger study group, treated with the oral inotropic agent, milrinone. Duration of each recording, containing two ECG signals is about 20h, sampled at 250 samples per second in a range of ±10mV, with 12-bit resolution. Each ECG recording contains around 17 million sample intervals. The larger study is based on 100 patients with severe congestive heart failure treated with digitalis glycosides, diuretics and one or more oral vasodilators [20].

Second data set include ECG signals from 3 patients suffering from congestive heart failure, obtained from PTB Diagnostic ECG database. First patient is 74-years old female, belonging to NYHA 2, second patient is 70-years old male, belonging to NYHA 3 and third patient is 61-year old female, belonging to NYHA 4. These ECG signals are part of larger data group, collected from healthy volunteers and patients with various heart diseases, at the Department of Cardiology of University Clinic Benjamin Franklin, in Germany. Each ECG record includes 15 simultaneously measured signals: the conventional 12 leads with 3 Frank lead ECGs. Signal form lead I is taken for purpose of this study. Original signals are digitized at 1000 samples per second, with 16 bit resolution over a range of ±16.384mV [20].

Normal heartbeats are taken from 13 subjects from MIT–BIH Arrhythmia database which totally contains 48 ECG recordings lasting for 30min, obtained from 47 subjects by the BIH Arrhythmia Laboratory, between 1975 and 1979. These recordings are randomly chosen from a larger set of recordings at Boston's Beth Israel Hospital, collected from a mixed population of inpatients and outpatients. These recordings are taken using recording bandwidth of 0.1–100Hz. Each of these recordings is sampled at 360 samples per second over the 10mV range, with 11-bit resolution. This database is freely available on http://physionet.org/physiobank/database/mitdb/ 
                        [20].

The ECG signals from both databases are preprocessed and features are extracted using autoregressive (AR) Burg algorithm. The autoregressive model is well-known feature extraction method, especially for biological signals. The important step in AR modeling is selection of the optimal order p. A process of model order p in autoregressive model is given by following formula [21]:
                           
                              (1)
                              
                                 
                                    x
                                    [
                                    n
                                    ]
                                    =
                                    −
                                    
                                       ∑
                                       
                                          k
                                          =
                                          1
                                       
                                       p
                                    
                                    
                                       
                                          a
                                          k
                                       
                                       x
                                       [
                                       n
                                       −
                                       k
                                       ]
                                       +
                                       e
                                       [
                                       n
                                       ]
                                    
                                 
                              
                           
                        where x[n] represents data of the signal at point n, p represents model of order, a
                        
                           k
                         is an autoregressive coefficient and e[n] represents noise error [21]. The Burg method is used for estimating a real valued autoregressive coefficient a
                        
                           k
                         recursively using a
                        
                           k
                         of previous order p−1. It is the form of order-recursive least squares method used to estimate parameters by minimizing the forward and backward errors of the linear system. The Burg method is precise because it uses many data points at the time, minimizing the backward and forward error [21]. However, Burg algorithm involves prediction error powers defined by formula [22]:
                           
                              (2)
                              
                                 
                                    
                                       δ
                                       k
                                       2
                                    
                                    =
                                    
                                       δ
                                       
                                          k
                                          −
                                          1
                                       
                                       2
                                    
                                    (
                                    1
                                    −
                                    |
                                    
                                       a
                                       k
                                    
                                    
                                       |
                                       2
                                    
                                    )
                                 
                              
                           
                        where 
                           
                              
                                 δ
                                 k
                                 2
                              
                           
                         is prediction error power which decreases when model of order p is increasing, producing the stationary model.

There is no straightforward way to determine the correct model order. As we increase the order of the model, the root mean square (RMS) error usually decreases quickly up to some order and then more slowly. An order just after the point at which the RMS error flattens out is usually an appropriate order. Much research has been done to discover good model order criteria. One of the criterion most often used is called the final prediction error (FPE). It is related to the variance of the modeling error.
                           
                              (3)
                              
                                 
                                    FPE
                                    =
                                    
                                       s
                                       p
                                       2
                                    
                                    
                                       
                                          N
                                          +
                                          p
                                          +
                                          1
                                       
                                       
                                          N
                                          −
                                          p
                                          −
                                          1
                                       
                                    
                                    ,
                                 
                              
                           
                        where 
                           
                              
                                 s
                                 p
                                 2
                              
                           
                         is the variance of the modeling error and N is the number of points in the signal. Model of order used in this study is 32. The length of each feature vector extracted using AR Burg is 33.

Each signal is divided into subsections of signals, consisting of 1000 data points (window length of 1000). Each subsection was used to extract features using AR Burg. Figs. 2 and 3
                        
                         shows input to feature extraction and output data after AR Burg has been applied, respectively.

In decision tree method, instances are classified by arranging them down the tree, from root to leaf node. Each internal node is a test for some attribute of the tree. The root node is the starting point of the classification process and is without incoming edges. The other nodes have exactly one incoming edge and are called leaves. The classification starts at the root node with testing the attribute specified by this node and continue down the tree branch according to the value of the attribute in the example. When a leaf node is reached, the instance is classified according to the class of the leaf [19,23].

C4.5 algorithm is based on ID3 algorithm, a very simple decision tree algorithm, presented by Quinlan [24]. This algorithm passes through decision tree, visits each node and select optimal split. It is achieved by using the gain ratio, represented by following formula [19]:
                           
                              (4)
                              
                                 
                                    GainRatio
                                       
                                    (
                                    S
                                    ,
                                    A
                                    )
                                    =
                                    
                                       
                                          InformationGain
                                             
                                          (
                                          S
                                          ,
                                          A
                                          )
                                       
                                       
                                          Entropy
                                             
                                          (
                                          S
                                          ,
                                          A
                                          )
                                       
                                    
                                 
                              
                           
                        where entropy is the term which describes how equally the attribute splits the data [19,23,24] and is calculated by formula:
                           
                              (5)
                              
                                 
                                    Entropy
                                       
                                    (
                                    y
                                    ,
                                    S
                                    )
                                    =
                                    
                                       ∑
                                       
                                          
                                             c
                                             j
                                          
                                          ∈
                                          d
                                          o
                                          m
                                          (
                                          y
                                          )
                                       
                                    
                                    
                                       −
                                       
                                          
                                             
                                                σ
                                                y
                                             
                                             =
                                             
                                                c
                                                j
                                             
                                             S
                                          
                                          
                                             
                                                S
                                             
                                          
                                       
                                       ⋅
                                       
                                          
                                             log
                                          
                                          2
                                       
                                       
                                          
                                             
                                                σ
                                                y
                                             
                                             =
                                             
                                                c
                                                j
                                             
                                             S
                                          
                                          
                                             
                                                S
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     


                        Information gain is the impurity-based criterion which uses an entropy measure as the impurity measure, for some training set S with respect to the attribute A and is presented by formula [19]:
                           
                              (6)
                              
                                 
                                    InformationGain
                                       
                                    (
                                    
                                       a
                                       i
                                    
                                    ,
                                    S
                                    )
                                    =
                                    Entropy
                                       
                                    (
                                    y
                                    ,
                                    S
                                    )
                                    −
                                    
                                       ∑
                                       
                                          
                                             v
                                             
                                                i
                                                ,
                                                j
                                             
                                          
                                          ∈
                                          d
                                          o
                                          m
                                          (
                                          
                                             a
                                             i
                                          
                                          )
                                       
                                    
                                    
                                       
                                          
                                             |
                                             
                                                σ
                                                
                                                   
                                                      a
                                                      i
                                                   
                                                
                                             
                                             =
                                             
                                                v
                                                
                                                   i
                                                   ,
                                                   j
                                                
                                             
                                             S
                                             |
                                          
                                          
                                             |
                                             S
                                             |
                                          
                                       
                                       ⋅
                                       Entropy
                                          
                                       (
                                       y
                                       ,
                                       
                                          σ
                                          
                                             
                                                a
                                                i
                                             
                                          
                                       
                                       =
                                       
                                          v
                                          
                                             i
                                             ,
                                             j
                                          
                                       
                                       S
                                       )
                                    
                                 
                              
                           
                        
                     

The optimal accuracy is reached when the size of tree was 15 and number of leaves 8.

Fix and Hodges [25] proposed a classification method which is easy to implement and give high accuracy. k-NN is non-parametric method for pattern classification based on finding the “nearest” training set T. The first step is to reduce the dimension of feature space by using distance function (commonly used Euclidian distance) between a test set and specified training set [26]:
                           
                              (7)
                              
                                 
                                    d
                                    (
                                    
                                       x
                                       i
                                    
                                    ,
                                    
                                       x
                                       j
                                    
                                    )
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                r
                                                =
                                                1
                                             
                                             n
                                          
                                          
                                             
                                                
                                                   (
                                                   
                                                      a
                                                      r
                                                   
                                                   (
                                                   
                                                      x
                                                      i
                                                   
                                                   )
                                                   −
                                                   
                                                      a
                                                      r
                                                   
                                                   (
                                                   
                                                      x
                                                      j
                                                   
                                                   )
                                                   )
                                                
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where a
                        
                           r
                         is the value of rth attribute of instance x.

A mathematical expression of k-NN for Y is expressed below:
                           
                              (8)
                              
                                 
                                    Y
                                    =
                                    
                                       1
                                       k
                                    
                                    
                                       ∑
                                       
                                          
                                             x
                                             i
                                          
                                          ∈
                                          
                                             N
                                             k
                                          
                                          (
                                          x
                                          )
                                       
                                    
                                    
                                       
                                          y
                                          i
                                       
                                    
                                 
                              
                           
                        where Y is a local mean vector and N
                        
                           k
                        (x) is the neighborhood of x defined by the k closest points x
                        
                           i
                         in the training set [27]. In k-nearest neighbor, the effective number of parameters is N/k, which is greater than p that represents parameters in least-square fits, and is getting lower value when increasing the value of k. So, there would be N/k neighborhoods and one parameter would fit in each of them, if the neighborhood were not overlapping [28]. Different values of k were applied to the dataset, such as k
                        =1, 3, 5, 7 and 10, but accuracy value reached the highest number when using 3-nearest neighbor for classification.

Concept of artificial neural network was presented by McCulloch and Pitts in 1943 [29], where artificial neuron is based on biological neuron. Neural network consist of huge number of processing units called neurons, nodes, processing units or cells. Each is connected to another by related weight. Weight is a term which represents synapses in biological neural network. Neural network consists of input nodes which receives data from outside and form input layer, output nodes which send data out of the network and form output layer and hidden node whose signals remain in the network and form hidden layer. In this study, 2800 instances are classified into two classes of output (normal and congestive heart failure). Network is created with one input layer, consisting of 33 inputs and 10 nodes in the hidden layer, as these parameters give the optimal classification accuracy. Every artificial neuron has internal state, called activation function of the inputs received. Many activation functions tested but sigmoid function gave the best accuracy. A sigmoid (S shaped curves) function presented by formula below [30]:
                           
                              (9)
                              
                                 
                                    f
                                    (
                                    x
                                    )
                                    =
                                    
                                       1
                                       
                                          1
                                          +
                                          
                                             e
                                             
                                                −
                                                x
                                             
                                          
                                       
                                    
                                 
                              
                           
                        and gives only two outputs. Network architecture is the way nodes are organized in layers and interconnected. In this study, feed-forward backpropagation architecture is applied to the dataset [30–32].

Backpropagation neural network architecture is proposed by Rumelhart et al. [32]. It is the classification algorithm that uses gradient descent method to adjust the connection weights in network model. In backpropagation algorithm, output of each neuron will be found by aggregating the neurons of previous level and multiplied by determined weight [33].

Support vector machines (SVM) concept is first presented by Boser et al. [34]. They presented a training algorithm that will maximize the margin between training patterns and decision boundaries [28]. SVM is a hyperplane that splits positive and negative sets of examples, with maximal margin. SVMs belong to the category of kernel methods, used in high dimensional feature space for computing a dot product. Kernel methods solve the problems of quadratic increase in memory when storing the features and time required calculating the classifier's discriminant function by avoiding mapping the data into the high-dimensional feature space [35]. When considering the dataset mentioned above, consisting of pairs (x
                        1, y
                        1), (x
                        2, y
                        2), …, (x
                        
                           N
                        , y
                        
                           N
                        ), where x
                        
                           i
                        
                        ∈ℜ
                        
                           P
                         and y
                        
                           i
                        
                        ∈{−1, 1}, hyperplane is defined by [28]:
                           
                              (10)
                              
                                 
                                    {
                                    x
                                    :
                                    f
                                    (
                                    x
                                    )
                                    =
                                    
                                       x
                                       T
                                    
                                    β
                                    +
                                    
                                       β
                                       0
                                    
                                    =
                                    0
                                    }
                                 
                              
                           
                        where β is a unit vector: ||β||=1.

A classification rule created by f(x) is
                           
                              (11)
                              
                                 
                                    G
                                    (
                                    x
                                    )
                                    =
                                    s
                                    i
                                    g
                                    n
                                    [
                                    
                                       x
                                       T
                                    
                                    β
                                    +
                                    
                                       β
                                       0
                                    
                                    ]
                                 
                              
                           
                        
                     

Now, when classes are separable, a function f(x) with y
                        
                           i
                        
                        f(x
                        
                           i
                        )>0 for all i values and hyperplane that produces the biggest margin between the training points for classes −1 and 1 can be determined. The optimization problem for this concept is presented by following formula [28]:
                           
                              (12)
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      max
                                                   
                                                   
                                                      β
                                                      ,
                                                      
                                                         β
                                                         0
                                                      
                                                      ,
                                                      |
                                                      |
                                                      β
                                                      |
                                                      |
                                                      =
                                                      1
                                                   
                                                
                                                M
                                             
                                          
                                       
                                       
                                          
                                             
                                                subject to
                                                   
                                                
                                                   y
                                                   i
                                                
                                                (
                                                
                                                   x
                                                   i
                                                   T
                                                
                                                β
                                                +
                                                
                                                   β
                                                   0
                                                
                                                )
                                                ≥
                                                M
                                                ,
                                                 
                                                i
                                                =
                                                1
                                                ,
                                                …
                                                ,
                                                N
                                                ,
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

So, margin is created and it is M unit away from hyperplane from both sides [28]. In this research, we used Puk kernel for SVM classifier. Linear, nonlinear and RBF kernels were also tested, but computational time increased in nonlinear RBF kernels, besides accuracies were significantly reduced. Hence, we used Puk kernel for SVM classifier which gives best accuracy among other kernels with ω
                        =1 and σ
                        =1. These values give optimal classification accuracy with reasonable computational time. However, based on literature [36], we tested different values of ω and σ such as 0.5 and 2, 5 and 0.5, respectively. Classification accuracy decreased, whereas computational time increased.

Random forest is a classification method which combines multiple tree predictors in that way that each tree depends on a value of randomly chosen vector distributed among all trees in forest in the same way. So, in the random forest algorithm, a random vector θ
                        
                           k
                         is produced, independent from the previous random vectors and distributed to all trees, and each tree is grown using training set and random vector θ
                        
                           k
                        , which results in collection of tree-structured classifiers {h(x, θ
                        
                           k
                        ), k
                        =1, …} at input vector x. In random forest algorithm, generalization error is given by [37]:
                           
                              (13)
                              
                                 
                                    PE
                                    *
                                    =
                                    
                                       P
                                       
                                          X
                                          ,
                                          Y
                                       
                                    
                                    (
                                    mg
                                    (
                                    X
                                    ,
                                    Y
                                    )
                                    <
                                    0
                                    )
                                 
                              
                           
                        where subscripts X and Y are random vectors that indicate the probability is over the X, Y space and mg is the margin function which measures the extent to which the average number of votes at random vectors for the right output exceeds the average vote for any other output. Margin function is defined as
                           
                              (14)
                              
                                 
                                    mg
                                    (
                                    X
                                    ,
                                    Y
                                    )
                                    =
                                    a
                                    
                                       v
                                       k
                                    
                                    I
                                    (
                                    
                                       h
                                       k
                                    
                                    (
                                    X
                                    )
                                    =
                                    Y
                                    )
                                    −
                                    
                                       
                                          max
                                       
                                       
                                          j
                                          ≠
                                          Y
                                       
                                    
                                    a
                                    
                                       v
                                       k
                                    
                                    I
                                    (
                                    
                                       h
                                       k
                                    
                                    (
                                    X
                                    )
                                    =
                                    j
                                    )
                                 
                              
                           
                        where I(·) is the indicator function [37].

Two parameters that measure accuracy of individual classifier and dependence between classifiers are strength and correlation, respectively. Random forest with random features is formed by choosing a small group of input variables on each node, randomly [37]. In this research, random forest consisted of 20 trees, each constructed while considering 6 random features. Optimal values for trees and random features are obtained by inserting different values of trees and random features, using the classification accuracy as fitting function. Therefore, 20 as the number of trees gave the optimal accuracy results.

@&#RESULTS AND DISCUSSION@&#

In this study, two automated system are designed to classify normal (N) heart beats and heart beats with congestive heart failure (CHF). In first case, the ECG signals used in the study are taken from BIDMC congestive heart failure and MIT–BIH Arrhythmia databases, both freely available on PhysioNet [20]. A total number of 2800 feature vectors, where 1500 belong to 15 patients with congestive heart failure and 1300 belong to 13 patients that have normal heartbeats are extracted using AR Burg method. Consequently, different machine learning techniques are applied to evaluate the performance of the proposed system. In second case, the same methodology is applied on second data set, composed of 171 congestive heart failure samples from PTB Diagnostic ECG database, combined with 1300 normal heart beats samples from MIT–BIH Arrhythmia database [20].

In determining the performance of the machine learning techniques, different approaches are used. The common approach is to divide the whole dataset into three subsets: training subset, validation subset and testing subset. All of them should be chosen independently from each other. Training set is then used to train the data, validation set to optimize parameters of classifiers, and finally test data is used to evaluate error rate of optimized classifier. Use of this method is not practical, because of limited number of data for both, training and testing. Moreover, samples in the training or testing subsets may not be representative, because one class should be represented in both subsets in right proportion. [38].

More efficient statistical technique is cross-validation. Dataset is divided into equal number of folds (partitions), where one fold is used for testing and remaining number of folds for training. It will be repeated as many times as number of fold is where each fold is used for testing and each fold is in group of training set. In this study, 10-fold cross validation is applied on the whole dataset, where dataset is divided into 10 folds of (roughly) equal size. The model is trained and tested 10 times; each time is tested on one of k folds and trained using the remaining 9 folds. Then, average cross validation accuracy is found of the k individual accuracies [39]:
                           
                              (15)
                              
                                 
                                    CVA
                                    =
                                    
                                       1
                                       k
                                    
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       k
                                    
                                    
                                       
                                          A
                                          i
                                       
                                    
                                 
                              
                           
                        where k is the number of folds and A
                        
                           i
                         accuracy of each fold.

When evaluating the performance of the classifiers, two more terms should be introduced: sensitivity and specificity. Sensitivity specifies how good the classifier can recognize positive samples and is defined by [40]:
                           
                              (16)
                              
                                 
                                    Sensitivity
                                    =
                                    
                                       
                                          TP
                                       
                                       
                                          TP
                                          +
                                          FN
                                       
                                    
                                    ×
                                    100
                                 
                              
                           
                        where TP is number of true positive samples and FN is the number of false negative samples. Sensitivity defines number of people having congestive heart failure and having positive test results. Specificity specifies how good the classifier can recognize negative samples and is defined by [40]:
                           
                              (17)
                              
                                 
                                    Specificity
                                    =
                                    
                                       
                                          TN
                                       
                                       
                                          TN
                                          +
                                          FP
                                       
                                    
                                    ×
                                    100
                                 
                              
                           
                        where TN is number of true negative samples and FP is the number of false positive samples. Specificity defines number of people without heart failure and having negative test results. The accuracy is then calculated by [40]:
                           
                              (18)
                              
                                 
                                    Accuracy
                                    =
                                    
                                       
                                          Sensitivity
                                          +
                                          Specificity
                                       
                                       2
                                    
                                 
                              
                           
                        
                     

Another statistical index is receiver operating characteristics (ROC) curve, which is evaluation metrics of observer performance, where learner is trying to select the samples with higher portion of positive values [38]. It is used to show discrimination ability of different statistical methods that combine different results to predict something [41]. ROC curve is created by plotting the number of true positive values on vertical axis (sensitivity) and false positive on the horizontal axis (1-specificity). The ideal point on the ROC curve would be (0, 1), meaning that all positive instances are classified as positive and none of negative instances are misclassified as positive. For each fold in 10-fold cross validation, data are weighted for choosing different cost ratios, the system on each weighted set is trained, true positive and false positive values are calculated, and the points on the ROC axes are plotted. The classification performance is measured by mean area under the curve (AUC), which is useful metric of classifier performance. The bigger area it is, the better classifier model is [40].

Third statistical index used to show the performance of the classification model is F-measure which shows the performance and efficiency of the model created, and looks for potential imbalance problems. F-measure is calculated as [42]:
                           
                              (19)
                              
                                 
                                    F
                                    -measure
                                    =
                                    
                                       
                                          2
                                          
                                             TP
                                             i
                                          
                                       
                                       
                                          2
                                          
                                             TP
                                             i
                                          
                                          +
                                          
                                             FP
                                             i
                                          
                                          +
                                          
                                             FN
                                             i
                                          
                                       
                                    
                                 
                              
                           
                        where TP
                           i
                         is number of correctly classified data of ith class; FP
                           i
                         is number of heartbeats incorrectly classified to ith class; FN
                           i
                         is number of heartbeats of class i, classified in another class [42].

@&#EXPERIMENTAL RESULTS@&#

An automated classifier is designed to classify heartbeat signals belonging to two categories: N (normal heartbeats) and CHF (congestive heart failure). In both cases, the whole data set is divided into 10 subsets of (roughly) equal size and 10-fold cross validation presented by Salzberg [43] was applied to have accurate validation of the results. The results of each fold (subset) are summed and averaged and average accuracy is found. The signals were normalized before classification. Furthermore, experiments in this research are done to examine the capability of classification methods in classifying and detecting heartbeat signals. To be precise, those classification methods are C4.5 decision tree, k-nearest neighbor (k-NN), artificial neural networks (ANN), support vector machines (SVM) and random forest (RF) algorithms.

Three different statistical indices are used to show the performance of the above mentioned algorithms in classifying the ECG heartbeat signals: receiver operating characteristics (ROC), F-measure and accuracy for both classes (N, CHF). All statistical parameters are given in Tables 1 and 2
                        
                        , and Figs. 4–6
                        
                        
                        .

Generally, all the classifiers performed well with the all statistical indices of high value. However, performance of random forest classifier stands out. It reached classification accuracy of 100% with ROC curve and F-measure of value 1 in both cases. Moreover, SVM classifier reached quite high performance, with accuracy value of 99.96%, ROC with value 1 and F-measure with value 1 in first case. Only one instance belonging to heartbeats with congestive heart failure was misclassified. However, performance of SVM is not quite significant in second case. The lowest accuracy value showed the ANN model, in both cases. The performance of C4.5 decision tree and k-NN is significant, as well. With accuracy values of 99.86 and 99.75 in first case, and 99.93 and 99.73 for C4.5 and k-NN, respectively, it is indicated that both of the classifiers are capable to separate different classes of heartbeats. ROC and F-measure are 0.998 and 0.999 in first case and 1 and 0.999 in second case for C4.5 classifier and 0.999 and 0.998 in first case and 0.997 and 0.997 in second case for k-NN classifier. In addition, it can be seen in Tables 1 and 2 that all classifiers do not significantly outperform each other.

@&#DISCUSSION@&#

The results in previous section confirmed the advantage of the proposed RF method over the C4.5, SVM, ANN and k-NN methods. As it can be seen from the Tables 1 and 2, RF method gives the best results when considering all three statistical indices, meaning that all instances are correctly classified. Number of trees and random features affects the performance of the random forest classifier. The classification performance of the SVM in Table 1 shows that it can be applicable method for identification of the heart failure. Only one instance belonging to the group of congestive heart failure was misclassified; however SVM classifier did not perform fine in second case, shown in Table 2. Noticeable performance is achieved by C4.5 decision tree in both cases. C4.5 misclassified only 3 instances belonging to CHF in first case and none of the CHF samples in second case. This implies that C4.5 is suitable for classification of the heart beat signals as well.

Obviously, the characteristics of the classifiers and its combination with feature selection algorithm determine the classifier's performance. However, the accuracy of the ECG signal classification may be affected by numerous different factors, like the quality of the signals obtained, performance of the feature extraction algorithm as well as the subsets used for purpose of training and testing [6].

Random forest classifier achieved significant advantage over the classifiers applied in previous papers, noticeable in Table 3
                     . Thuraisingham [8] combined SODP with k-NN classifier, which resulted in accuracy of 100%. The author achieved remarkable success with k-NN classifier in distinguishing ECG signals. When we compared k-NN and RF in speed and complexity, it is obvious that k-NN modeled the classification model faster than RF in this study (Tables 1 and 2), and it is less complex than RF. Still, RF achieved higher classification accuracy, ROC and F-measure values. Considering the overall performance of both classifiers, RF is preferred over k-NN. However, Hossen and Al-Ghunaimi [11,12] did not achieved quite remarkable success when compared to our result. Yu and Lee [13] pointed out that SVM classifier can truly achieve reasonable success.

Moreover, we investigated the success of random forest classifier in previous paper and various databases, presented in Table 4
                     . Thus, Emanet obtained signals from MIT–BIH Arrhythmia database according to five types of ECG beats [44]. The classifier achieved remarkable success in differentiating the beats.

Belgacem et al. [45] obtained ECG signals from four PhysioNet ECG databases (MIT–BIH, ST-T, NSR, PTB) and one collected from their students. They applied random forest classifier to recognize healthy subjects and achieve 100% success.

MIT–BIH Arrhythmia database was used by Kumar and Kumaraswamy [46]. They tried to differentiate two arrhythmias (LBBB and RBBB) and normal beats. They applied random forest classifier to differentiate heartbeats and achieved 92.2% classification success.

For purpose of their study, Jovic and Bogunovic [47] used signals obtained from four PhysioNet database to differentiate normal, any arrhythmia, supraventricular arrhythmia and congestive heart failure by applying random forest classifier. The classifier achieved 93% success in differentiating normal and supraventricular arrhythmia.

All above emphasizes the strong power of random forest classifier in correctly differentiating ECG signals. Moreover, our proposed systems reached 100% of sensitivity and specificity, implying that all normal signals were classified correctly with false negative (FN) rate of 0 and all signals with congestive heart failure were classified correctly with false positive (FP) rate of 0. The system might help in early diagnosis of the failure and right treatment. Moreover, the proposed system can be applied in detecting different classes of ECG signals.

According to the results of the present study and understanding of ECG signals classification problems, the following can be emphasized:
                        
                           •
                           High classification accuracy of RF classifier gives the insight into the feature that defines the ECG signals. The results in the study demonstrated that AR Burg coefficients are the features that represented ECG signals and facilitate the aim of classifier in distinguishing two classes of signals.

C4.5 decision tree, k-NN, SVM and ANN are suitable classifiers for diagnosis and identification of heart failure; but RF classifier has an advantage over mentioned classifiers based on the high classification accuracy and values of ROC curve and F-measure.

For training the RF classifier, appropriate number of trees and random features is selected. Optimal values for trees and random features are obtained by inserting different values of trees and random features, using the classification accuracy as fitting function.

The impressive testing performance of the RF classifier indicated that the model can be used in clinical studies and practice, to support doctors’ decisions.

@&#CONCLUSION@&#

In this paper, automated heartbeat classification system is developed for detecting congestive heart failure. ECG signals with congestive heart failure are obtained from BIDMC database and ECG signals with normal heartbeats are obtained from MIT–BIH Arrhythmia database. The feature vectors of datasets are extracted by using autoregressive (AR) Burg method. Dataset is extracted into 2800 ECG data segments where 1500 belongs to heartbeats with congestive heart failure and remaining 1300 data segments belongs to normal heart beats. The same dataset was used with five different classification methods, namely C4.5 decision tree, k-NN, SVM, ANN and RF. Among all classifiers, which achieved high statistical measures (accuracy, ROC curve, F-measure), random forest algorithm has significant role in identification and classification of ECG signals, demonstrated by accuracy of 100%, ROC curve of value 1 and F-measure of value 1, as well by success rate of the classifier presented. However, impressive performance of RF is proved by applying it to the second data set, composed of congestive heart failure, obtained from PTB Diagnostic ECG database. Autoregressive (AR) Burg method is applied to extract features and five classification methods to classify features into normal and congestive heart failure, respectively. Random forest proved its impressive performance by achieving 100% classification accuracy, ROC curve of value 1 and F-measure of value 1. Performance of RF classifier demonstrated that this classifier can be valuable to express knowledge useful in medicine.

@&#REFERENCES@&#

