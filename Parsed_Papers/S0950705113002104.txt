@&#MAIN-TITLE@&#Feature selection in corporate credit rating prediction

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We employ feature selection for corporate credit rating modeling.


                        
                        
                           
                           Feature selection procedure improves the classification accuracy of the credit ratings.


                        
                        
                           
                           The number of features required for credit rating classification can be significantly reduced.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Feature selection

Credit rating

Classification

Wrapper

Mixed feature selection method

@&#ABSTRACT@&#


               
               
                  Credit rating assessment is a complicated process in which many parameters describing a company are taken into consideration and a grade is assigned, which represents the reliability of a potential client. Such assessment is expensive, because domain experts have to be employed to perform the rating. One way of lowering the costs of performing the rating is to use an automated rating procedure. In this paper, we assess several automatic classification methods for credit rating assessment. The methods presented in this paper follow a well-known paradigm of supervised machine learning, where they are first trained on a dataset representing companies with a known credibility, and then applied to companies with unknown credibility. We employed a procedure of feature selection that improved the accuracy of the ratings obtained as a result of classification. In addition, feature selection reduced the number of parameters describing a company that have to be known before the automatic rating can be performed. Wrappers performed better than filters for both US and European datasets. However, better classification performance was achieved at a cost of additional computational time. Our results also suggest that US rating methodology prefers the size of companies and market value ratios, whereas the European methodology relies more on profitability and leverage ratios.
               
            

@&#INTRODUCTION@&#

There are many instances when the credibility of a company needs to be measured. Bond investors, debt issuers, governmental officers, and companies that provide credit use credit ratings to assess the investment risk. These ratings are a basis for important decisions, and therefore there is a need for them to be as accurate as possible. Unfortunately, this usually means that many details of a company’s profile have to be taken into consideration. Such a detailed analysis can be performed by experts, but this is often costly and time-consuming.

Because manual analysis of a company’s profile is slow and costly, currently significant emphasis is placed on computational methods of credit rating assessment. The methods applied to this task can be broadly divided into two groups: traditional statistical methods (e.g. [36]) and artificial intelligence (AI) methods (e.g. [6,32,26]). Using traditional statistical methods is difficult because of the complexity of dependencies between various factors that influence the final rating. Nevertheless, methods such as multiple discriminant analysis (MDA) and linear regression (LR) have been applied to credit rating prediction in the literature.

AI methods are often employed when the relationships between input parameters and the outcome are too complex to describe analytically. In the case of credit rating prediction, the most promising group of methods are various classifiers that can be trained on examples – a dataset describing companies previously labeled by experts. A previously trained classifier is a model that represents dependencies between input parameters and object classification (in the case of corporate credit rating prediction – a grade representing the credibility of a company). This type of classifier has a generalization ability – it can produce ratings for yet unseen companies.

Apart from generating credit ratings for companies, corporate credit rating prediction models can be used as early warning indicators. In fact, such models may detect the start of financial crises. In addition, automatic corporate credit rating prediction can be effectively used by supervisory institutions for determining regulatory capital requirements [21].

Companies for which a credit rating is performed are described by a number of variables (features) that are used as input data for modeling. These parameters mainly reflect various aspects of economic and financial performance of a company.

An important problem in modeling credit ratings is the selection of the most appropriate set of variables. The influence of various parameters on the classification result must be quantified. Thus, only important variables can be used for the credit rating process. There is a wide variety of feature selection methods that can be used to select the appropriate set of variables. Sensitivity analysis (stepwise procedure) may be used to select features that have the most influence on the classification result.

In this paper a different method was used, whereby features are selected using the wrapper approach (an iterative selection procedure that selects features based on the evaluation of performance of the classifier using selected features). In addition, our research is aimed at comparing feature selection methods used for credit rating prediction. To the best of our knowledge, no previous study has attempted to compare filters and wrappers as feature selection methods within the classification process in this or related business domains. We hypothesized that wrapper approaches would contribute to a higher classification accuracy of the employed classifiers compared to filter approaches. In our research we compared filter and wrapper methods in order to verify this hypothesis.

Since two datasets were used, namely data from the US and Europe, we also address the impact of country-related determinants. Thus, our results may lead to a richer understanding of the role of individual input variables and the categories of input variables, respectively, in corporate credit rating prediction.

This paper is structured as follows. In Section 2, details of the credit rating process are presented and methods currently used in the literature for corporate credit rating prediction are described. Section 3 contains data description and a discussion of input variables used for classification. In Section 4 the feature selection process is described and four feature selection methods are introduced, namely two wrappers and two filters. The results obtained in the experiments using various classifiers are presented in Section 5. Section 6 concludes the paper.

Corporate credit rating is a process in which a grade 
                        
                           ω
                           ∈
                           Ω
                        
                      from a predefined rating scale Ω is assigned to a company. Rating agencies, such as Standard & Poor’s (S&P’s), Moody’s, and Fitch have their own rating scales. For example, the rating scale of the S&P’s is Ω
                     ={AAA,AA,A,BBB,BB,B,CCC,CC,C,D} – a total of 10 grades (rating classes) that are ordered from AAA, the most promising for investors, to D, the most risky one.

Prior studies on credit rating prediction vary with respect to assessed objects, input variables used, and the set of rating classes Ω. Traditional statistical methods and AI methods have been previously employed in the literature for corporate credit rating prediction.

Studies comparing traditional statistical methods showed that the most successful methods of that type are the ordered logistic regression (OLR) and ordered probit model (OPM) [38]. The two methods have outperformed other statistical methods such as linear regression (LR) and multiple discriminant analysis (MDA) [36,37]. This may be due to the fact that OLR and OPM take the ordering of rating classes into consideration.

To use statistical methods, one has to first choose a model with a predefined structure to represent observations. Then, the parameters of the model are estimated tofit the model to the observational data. The advantage of such an approach is that the models are relatively easy to explain. However, statistical models require various assumptions to be theoretically valid.

Another approach is to use AI methods. The AI methods differ from traditional statistical methods in that they allow learning the model from data [32]. The advantage of such an approach is that AI methods usually do not require specific assumptions on the distribution of data.

Using concepts from the machine learning paradigm, the problem of credit rating prediction can be formulated as a classification problem in which rating classes used by a particular rating agency are known in advance. A typical classification procedure is performed as supervised learning. This learning type requires a sample of companies that were initially assigned proper ratings. A classifier of a chosen type is first trained using this sample. Then, the trained classifier can be used to predict the ratings of previously unseen companies.

Neural networks (NNs) are commonly used in the literature for credit rating prediction. NNs were found to be significantly more accurate than traditional statistical methods in previous studies (e.g. [6]). Hajek [26] compared the performance of a variety of NNs. Radial basis function neural networks (RBF) and probabilistic neural networks (PNNs) significantly outperformed methods such as multilayer perceptron (MLP), group method of data handling (GMDH), MDA, and LR. Because of high generalization ability, support vector machines (SVMs) also produced good results in terms of classification accuracy (e.g. [32,47]). For a small proportion of labeled companies, kernel-based approaches with semi-supervised learning [29] have provided better results than supervised learning methods.

Fuzzy logic based classifiers, such as adaptive fuzzy rule based systems (AFRBs), fuzzy decision trees (FDTs) and the Wang–Mendel algorithm have been employed by Hajek [28]. The main advantage of these classifiers is the fact that the model obtained can be interpreted in terms of membership functions and fuzzy if-then rules. However, the model can be very complex in the case of credit ratings. As a result, hundreds of fuzzy if-then rules have to be generated in order to obtain an accurate prediction model.

Other AI methods used for credit rating prediction include artificial immune systems (AISs) (e.g. [13]), case-based reasoning (CBR) (e.g. [43,65,47]), evolutionary algorithms [5], and ant colony optimization [54].


                     Table 1
                      summarizes the literature on corporate credit rating prediction. In all presented studies, data used in the tests were obtained from US companies. Rating classes were provided by two rating agencies: S&P’s or Moody’s. Nevertheless, it would be inappropriate to compare these studies among themeselves as they are based on different datasets (the companies included might not be the same and data were obtained from different time periods, and thus they describe companies operating in different macroeconomic conditions).

In addition to studies focused on US data, some studies have explored data from other countries and rating agencies. The assessments of Korean or Japanese rating agencies have been used in the following studies. Methods such as NNs [47], SVMs [1,2], CBR [65], Bayesian networks [72,10], and hybrid methods combining AI methods [43,4,23] have been used in these studies. Furthermore, credit ratings of sub-sovereign and municipal entities have been studied recently (e.g. [20,27]).

Considering the process of feature selection, statistical tests (one-way analysis of variance [ANOVA], Kruskal–Wallis test), factor analysis, and stepwise procedure have been used previously in various combinations [32,65,43,47,42,58]. Huang et al. [32] used one-way ANOVA to find statistically significant input variables for two datasets: Korean and US. In the case of the US dataset, 14 out of 19 features were selected with a P<
                     0.1. In particular, liquidity ratios did not have a significant impact with regard to the credit rating decision. Shin and Han [65] applied a two-stage feature selection process. At the first stage, 27 variables were selected using factor analysis, one-way ANOVA, and Kruskal–Wallis test (for qualitative variables). In the second stage, they used a step-wise procedure of MDA to reduce the dimensionality to 12 final input variables. A wide range of variables’ categories was included in the resulting set of variables. In a similar manner, Kim and Han [43] performed one-way ANOVA and Kruskal–Wallis in the first stage and factor analysis with stepwise procedure of CBR in the second and third stage, respectively. Thus, the original set of 129 input variables was significantly reduced to only 13 variables. Lee [47] used a combination of one-way ANOVA and stepwise procedure of MDA to reduce the initial 297 financial ratios to the final set of 10 concerning leverage ratios in particular.

Huang [35] predicted the credit ratings of Taiwanese companies using an integration of a nonlinear graph-based dimensionality reduction scheme with SVMs. This feature extraction method provided a significant improvement of classification accuracy achieved by SVMs compared to the traditional feature extraction methods (principal component analysis and independent component analysis) and recursive feature elimination method, respectively.

Recently, it has been demonstrated that the classification performance of AI methods can be significantly improved using a correlation-based filter [30] for corporate credit ratings [26,28]. Here, the optimum set of input variables was constructed considering the correlations between the input variables and rating classes. In addition, the impact of inter-correlated input variables was limited.

Yeh et al. [71] used random forests to select the 18 most important features which caused the highest mean decrease in classification accuracy when removed.

In related works dedicated to bankruptcy prediction, several methods for feature selection have also been applied. For example, Tsai [68] performed a comparison of the following feature selection methods: correlation matrix, t-test, factor analysis, principal component analysis, and stepwise procedure. MLP was used as a classifier. The t-test method provided the best results with regard to classification accuracy, while stepwise procedure provided the highest feature reduction rate.

Rating agencies do not reveal the parameters used to perform credit rating assessment. In order to have the best possible description of the status of companies that are to be rated, we use as many measures of economic and financial performance as possible. In particular, we include the parameters that have been used by other authors in previous studies.

Our datasets contain information on 852 US and 244 European non-financial companies, respectively. In the European dataset, only companies from the EU states were represented, and specifically the UK with 52 companies, Germany with 35, France with 30, and others. We handled the two dataset separately because financial ratios of companies outside the US are not directly comparable with statistics published for US industries. There are several reasons for this approach. The companies outside the US differ in the treatment of goodwill, asset valuation practices, contingent liabilities reporting, accounting techniques, and other factors. We did not consider country risk, which is thought to be important primarily for emerging markets.

For each variable, we used the mean values calculated over the years 2006–2008. The reason of using a three year average follows a process known as “rating through the cycle”, which is adopted by rating agencies to achieve rating stability and to minimize the business cycle effect [3,64,19]. This longer-term perspective is usually implemented by considering the three-year averages of relevant financial ratios. Rating stability has been confirmed empirically by Mizen and Tsoukas [57]. In the study presented by these authors, credit ratings showed a high autocorrelation and previous years’ ratings were a key input variable when predicting current credit ratings.

We used ratings assigned by the S&P’s rating agency in the year 2009 as target classes. Rating agencies take into account many diverse national considerations, but express their ratings on a single scale. This allows the debt-holders to compare issues of equivalent credit quality.

Parameters used in our study for describing companies can be divided into two main categories: business position and financial indicators.

The business position of a company can be described using factors such as industry risk, size, character, management skills, and others. The size of a company can be expressed by measuring market capitalization, assets, equity, cash flow, and others. The ability of a company to pay off its loans is determined, among other factors, by company size. Company size is also correlated with diversification and market power. The character (reputation) of a company is difficult to measure. To some extent this factor can be inferred from information about insiders’ and institutional holdings. Industry risk represents the sensitivity of companies in a particular industry or market to external business factors, such as macroeconomic changes. Finally, reputation and industry risk have only been involved to a very small extent in previous studies.

Financial indicators are the second important category of factors taken into account in the corporate credit rating process. Financial indicators can further be divided into several subcategories: profitability ratios, activity ratios, liquidity ratios, leverage ratios, and market value ratios.

Profitability ratios measure the influence of asset management, financing, and liquidity on the profit of a company. Profitability ratios used by other authors (e.g. [36]) include the absolute size of profit, the effect of return on total assets, return on equity, return on sales, operating margin, and net margin.

Activity ratios measure the effectiveness of asset management. However, the influence of asset management on the credit rating of a company is rather indirect, because asset management belongs to common financial decision-making areas. Consequently, only sales to net worth and fixed assets were used in previous studies (e.g. [44]).

Most authors used current ratio as a representative of liquidity ratios. There are also other parameters, such as quick ratio and cash ratio but these have been rarely used in the literature.

Leverage ratios (indebtedness) have been represented by total debts to total assets and long-term debts to total assets, respectively, in the literature (e.g. [6]). The assessment of the capability of a company to pay off debt from the generated profit was also an important factor in credit rating prediction. This aspect can be measured using different approaches, such as with the interest coverage parameter.

Market value ratios reflect how the past activity of a company and its future outlook are perceived by the market. The impact of market value ratios on corporate credit rating was demonstrated by Hajek [28] and Hajek and Olej [29]. In these studies, the correlation of stock returns with market index, high/low stock price, and dividend yield were regarded as important factors of corporate rating process.


                     Table 2
                      shows the list of input variables used in previous studies. Since our dataset only covers US and European companies, we did not report input variables used in predicting Korean and Japanese credit ratings in this table. In general, most authors used the size of a company as well as its profitability, liquidity, and leverage ratios as input variables to credit rating prediction.

The choice of input variables for our experiments is presented in Table 3
                     . The US dataset contained 81 input variables obtained from the Value Line Database and S&P’s database, while the European dataset covers only those input variables marked in italics (i.e., 43 input variables extracted from the Bloomberg and Capital IQ databases). The input variables are divided into 9 categories: size of a company, corporate reputation, profitability ratios, activity ratios, asset structure, business situation, liquidity ratios, leverage ratios, and market value ratios. In this paper we have performed a feature selection step to select a subset of parameters from the sets presented in Table 3. Companies are classified into 9 output rating classes 
                        
                           ω
                           ∈
                           Ω
                        
                     , Ω
                     ={AAA,AA,…,D} (Fig. 1
                     ).

We performed a principal component analysis to closely assess the nature of the data. Only 14 principal components with eigenvalues greater than 1 were extracted both from the original set of 81 variables (US dataset) and 43 variables (European dataset), respectively. The first principal component explained 45.12% and 15.38% of the total variance, respectively (see the Pareto charts in Figs. 2 and 3
                     
                     ). For the US dataset, it represents the input variables from several categories that correlated with the size of companies. The second component shows the capital market position of a company. For the European companies, the first component can be labeled as the capital market position, the second as the size of companies, and so forth. We can conclude that there are several significantly intercorrelated variables in the dataset, which could have a strong impact on the results of the feature selection process [12].

The set of features should be reduced before they are used for classification. Performing this step not only makes the calculations faster and the parameters easier to collect, but also may improve classification accuracy [73]. The benefits of feature selection are as follows: the dimensionality of the feature space is reduced, learning algorithms operate faster and classification accuracy can be improved. Recently, several feature selection have been proposed which are based on computational intelligence methods such as genetic algorithms [15,48], rough sets [9], memetic algorithms [41], SVMs [52,53], or hybrid algorithms [39,22].

The variety of feature selection methods is usually divided into filters, wrappers, and embedded feature selection methods [40]. Filters perform feature selection based on the characteristics of data itself (e.g., by employing statistical measures). Filters operate independently of any learning algorithm by estimating the usefulness of features using anevaluation function [63]. Features that are not expected to provide valuable information for classification are filtered out of the dataset before classification starts. This process can be performed, among the others, using correlation ranking [17], a two-sample t-test [66], penalized pseudo-likelihood [7,18], and mutual information [16].

Embedded methods measure the importance of features while building a model, and therefore the feature selection step is aninherent part of the training process (e.g. [69]).

Wrappers use some type of enumeration algorithm to explore the space of feature subsets. The enumeration algorithm may be a simple sequential search [14], but more sophisticated methods such as genetic algorithms [34,59,70] or Particle Swarm Optimization are also used [51]. Performance of the classification algorithm is tested on each enumerated feature set and the performance measure is used as evaluation of the subset quality. This approach may be slow since the classifier must be retrained on all candidate subsets of the feature set and its performance must also be measured. On the other hand, wrappers may produce better results than filters, because specific requirements of the classifier can be taken into account. In comparative studies, wrappers have performed better than filters when the sample size was sufficiently large [33].

Wrapper feature selection requires enumerating feature subsets, because in general, the number of all possible feature subsets is very large. Therefore, it is necessary to employ a search procedure that only iterates over a portion of all of the possible subsets.

Since the number of possible subsets of the feature set is too large to evaluate all of them in most cases, iterated search is most often employed. For example, a method proposed by Kittler [45] is based on the following step-wise procedure. The procedure starts with an empty set of features. In the next steps the feature set is expanded using the variables that provide the largest improvement in classification accuracy. The procedure is stopped when the addition of more variables provides no improvement in classification accuracy.

If features are selected one-by-one as it is done in the method described above, then it is possible that important dependencies between features are not taken into account. Therefore, multivariate approaches are sometimes employed.

Evaluating features in pairs has been proposed in the literature [61,31] in order to take into account at least some of the dependencies between features. The pairwise approach results in quadratic complexity with respect to the number of features, which is clearly far better than evaluating all possible feature sets.

Quadratic complexity is still quite high when the number of features is large. Michalak and Kwasnicka [55,56] proposed a mixed feature selection (MFS) method that evaluates features individually or in pairs. The decision on how to evaluate features is made based on aquantitative criterion that measures the level of dependence between features. As only afraction of features is evaluated in pairs the complexity of the MFS method is lower than the complexity of the pairwise method.

The overview of the Mixed Feature Selection Method is as follows:
                        
                           
                              
                              
                                 
                                    
                                       F
                                       0
                                       =∅
                                 
                                 
                                    
                                       i
                                       =1
                                 
                                 
                                    while |Fi
                                       
                                       −1|<
                                       n
                                       max do
                                 
                                 
                                    
                                       if |Fi
                                       
                                       −1|<
                                       n
                                       max
                                       −1 then
                                 
                                 
                                    
                                       
                                       
                                       Fi
                                       
                                       =SelectMixed(Fi
                                       
                                       −1)
                                 
                                 
                                    
                                       else
                                 
                                 
                                    
                                       
                                       add one, the best feature to Fi
                                       
                                       −1
                                    
                                 
                                 
                                    
                                       end if
                                 
                                 
                                    
                                       
                                       i
                                       =
                                       i
                                       +1
                                 
                                 
                                    end while
                                 
                              
                           
                        
                     
                  

In the above algorithm, i is the iteration number, n
                     max is the number of features required, and Fi
                      are iteratively constructed feature sets. The SelectMixed procedure performs each feature selection iteration using evaluation function δ to decide whether to evaluate each feature individually or in pairs with all other features. The SelectMixed procedure is implemented as follows:
                        
                           
                              
                              
                                 
                                    
                                       Q
                                       max
                                       =0
                                 
                                 
                                    for each 
                                          
                                             
                                                
                                                   f
                                                
                                                
                                                   k
                                                
                                             
                                             ∉
                                             
                                                
                                                   F
                                                
                                                
                                                   i
                                                   -
                                                   1
                                                
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       if 
                                          
                                             ∃
                                          
                                        
                                       fp
                                       
                                       ·
                                       δ(fk
                                       , fp
                                       )>
                                       θ then
                                 
                                 
                                    
                                       
                                       
                                       Q
                                       test
                                       =the quality of the best feature set 
                                          
                                             
                                                
                                                   F
                                                
                                                
                                                   i
                                                   -
                                                   1
                                                
                                             
                                             ∪
                                             {
                                             
                                                
                                                   f
                                                
                                                
                                                   k
                                                
                                             
                                             ,
                                             
                                                
                                                   f
                                                
                                                
                                                   q
                                                
                                             
                                             }
                                          
                                       , where 
                                          
                                             
                                                
                                                   f
                                                
                                                
                                                   k
                                                
                                             
                                             ∉
                                             
                                                
                                                   F
                                                
                                                
                                                   i
                                                   -
                                                   1
                                                
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       
                                       
                                       F
                                       test
                                       ={fk
                                       , fq}
                                 
                                 
                                    
                                       else
                                 
                                 
                                    
                                       
                                       
                                       Q
                                       test
                                       =the quality of the feature set 
                                          
                                             
                                                
                                                   F
                                                
                                                
                                                   i
                                                   -
                                                   1
                                                
                                             
                                             ∪
                                             {
                                             
                                                
                                                   f
                                                
                                                
                                                   k
                                                
                                             
                                             }
                                          
                                       
                                    
                                 
                                 
                                    
                                       
                                       
                                       F
                                       test
                                       ={fk
                                       }
                                 
                                 
                                    
                                       end if
                                 
                                 
                                    
                                       if Q
                                       test
                                       >
                                       Q
                                       max
                                    
                                 
                                 
                                    
                                       
                                       
                                       
                                          
                                             
                                                
                                                   F
                                                
                                                
                                                   i
                                                
                                             
                                             =
                                             
                                                
                                                   F
                                                
                                                
                                                   i
                                                   -
                                                   1
                                                
                                             
                                             ∪
                                             
                                                
                                                   F
                                                
                                                
                                                   test
                                                
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       end if
                                 
                                 
                                    end for
                                 
                              
                           
                        
                     
                  

In the above procedure δ is an evaluation function that numerically represents the level of dependence between features. If the estimated level of dependence between a given feature fk
                      and at least one other feature fp
                      exceeds a predefined threshold θ, then the feature fk
                      is evaluated in pairs with all (not yet selected) features and from that evaluation step the best pair {fk
                     ,
                     fq
                     } is selected.

In this paper, feature selection was performed using individual feature selection (IFS) proposed by Kittler [45] and the MFS method proposed by Michalak and Kwasnicka [55,56]. An absolute value of correlation coefficient was used for the evaluation function (δ
                     =|ρk
                     
                     ,
                     
                        p
                     |). The threshold was set to θ
                     =0.5. Furthermore, we compared the performance of the presented wrapper approaches with three filter methods: correlation-based filter (CORF), which was developed by Hall [30], consistency-based filter (CONF), which was proposed by Liu and Setiono [49], and genetic algorithm filter (GAF) [50].

In the CORF method the quality of a set of variables is measured based on the individual predictive ability of each feature together with the degree of redundancy between them. The objective function f(λ), based on Pearson’s correlation coefficient, can be expressed as:
                        
                           (1)
                           
                              f
                              (
                              λ
                              )
                              =
                              
                                 
                                    λ
                                    ×
                                    
                                       
                                          ζ
                                       
                                       
                                          cr
                                       
                                    
                                 
                                 
                                    
                                       
                                          λ
                                          +
                                          λ
                                          ×
                                          (
                                          λ
                                          -
                                          1
                                          )
                                          ×
                                          
                                             
                                                ζ
                                             
                                             
                                                rr
                                             
                                          
                                       
                                    
                                 
                              
                              ,
                           
                        
                     where λ is the subset of features, ζ
                     cr is the average feature to class correlation, and ζ
                     rr is the average feature to feature correlation. The numerator in Eq. (1) expresses the predictive power of a set of features λ. The denominator represents the degree of redundancy among the features in λ. Irrelevant features have low correlation with the class, and therefore they receive a low evaluation. Redundant features are discriminated against because they are usually highly correlated with one or more of the other features, and therefore produce high values of the denominator. The correlations between features ζ
                     cr and ζ
                     rr are computed from the discretized values of the features. Symmetrical uncertainty (SU) is used as a correlation measure to estimate the degree of association between discrete features (X and Y):
                        
                           (2)
                           
                              SU
                              =
                              2.0
                              ×
                              
                                 
                                    
                                       
                                          
                                             H
                                             (
                                             X
                                             )
                                             +
                                             H
                                             (
                                             Y
                                             )
                                             -
                                             H
                                             (
                                             X
                                             ,
                                             Y
                                             )
                                          
                                          
                                             H
                                             (
                                             X
                                             )
                                             +
                                             H
                                             (
                                             Y
                                             )
                                          
                                       
                                    
                                 
                              
                              .
                           
                        
                     After computing a correlation matrix, CORF applies a heuristic search strategy to find a good subset of features according to Eq. (1).

The CONF uses an inconsistency criterion that specifies to what extent the dimensionality of reduced data can be accepted. The inconsistency rate γ of selected features is checked against a pre-specified rate γ
                     max. The inconsistency rate is calculated based on the count of inconsistent instances (matching instances except for their class labels). The CONF algorithm can be written as follows:
                        
                           
                              
                              
                                 
                                    for i
                                       =1 to MAX-TRIES
                                 
                                 
                                    
                                       
                                       Fi
                                       
                                       =random feature set
                                 
                                 
                                    
                                       
                                       ni
                                       
                                       =number of features in Fi
                                       
                                    
                                 
                                 
                                    
                                       
                                       n
                                       max
                                       =number of features
                                 
                                 
                                    
                                       
                                       n
                                       best
                                       =the best number of features
                                 
                                 
                                    
                                       if ni
                                       
                                       <
                                       n
                                       max
                                    
                                 
                                 
                                    
                                       
                                       if γ
                                       <
                                       γ
                                       max
                                    
                                 
                                 
                                    
                                       
                                       
                                       
                                       F
                                       best
                                       =
                                       Fi
                                       
                                    
                                 
                                 
                                    
                                       
                                       
                                       
                                       n
                                       best
                                       =
                                       ni
                                       
                                    
                                 
                                 
                                    
                                       
                                       else if (ni
                                       
                                       =
                                       n
                                       best) and (γ
                                       <
                                       γ
                                       max)
                                 
                                 
                                    
                                       
                                       end if
                                 
                                 
                                    end for
                                 
                              
                           
                        
                     
                  

The GAF was originally developed with the aim of choosing the neurons in the hidden layer of a hybrid NN [50]. The objective function θ
                     =
                     V
                     −
                     P of this method is based on both the mutual information between feature and class I(X,
                     Y) and the mutual information between features I(Xi
                     ,
                     Xj
                     ):
                        
                           (3)
                           
                              I
                              (
                              
                                 
                                    X
                                 
                                 
                                    i
                                 
                              
                              ,
                              Y
                              )
                              =
                              H
                              (
                              
                                 
                                    X
                                 
                                 
                                    i
                                 
                              
                              )
                              +
                              H
                              (
                              Y
                              )
                              -
                              H
                              (
                              
                                 
                                    X
                                 
                                 
                                    i
                                 
                              
                              ,
                              Y
                              )
                              ,
                           
                        
                     
                     
                        
                           (4)
                           
                              I
                              (
                              
                                 
                                    X
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    X
                                 
                                 
                                    j
                                 
                              
                              )
                              =
                              H
                              (
                              
                                 
                                    X
                                 
                                 
                                    i
                                 
                              
                              )
                              +
                              H
                              (
                              
                                 
                                    X
                                 
                                 
                                    j
                                 
                              
                              )
                              -
                              H
                              (
                              
                                 
                                    X
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    X
                                 
                                 
                                    j
                                 
                              
                              )
                              .
                           
                        
                     
                     
                        
                           (5)
                           
                              V
                              =
                              
                                 
                                    1
                                 
                                 
                                    n
                                 
                              
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       n
                                    
                                 
                              
                              I
                              (
                              
                                 
                                    X
                                 
                                 
                                    i
                                 
                              
                              ,
                              Y
                              )
                              ,
                           
                        
                     
                     
                        
                           (6)
                           
                              P
                              =
                              
                                 
                                    1
                                 
                                 
                                    
                                       
                                          n
                                       
                                       
                                          2
                                       
                                    
                                 
                              
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       n
                                    
                                 
                              
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       j
                                       =
                                       1
                                    
                                    
                                       n
                                    
                                 
                              
                              I
                              (
                              
                                 
                                    X
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    X
                                 
                                 
                                    j
                                 
                              
                              )
                              .
                           
                        
                     The GAF algorithm can be expressed as follows:


                     Input:
                     
                        
                           
                              N
                              pop
                              =population size (the number of random feature sets Fi
                              )

Ngen
                              =number of generations


                              ni
                              
                              =number of features in Fi
                              
                           


                              n
                              max
                              =number of features


                              α
                              =selective pressure


                              I(X,
                              Y), I(Xi
                              ,
                              Xj
                              )=mutual information


                     Output:
                     
                        
                           {i} – selected feature set


                     Algorithm:
                     
                        
                           
                              
                              
                                 
                                    generate initial population
                                 
                                 
                                    for j
                                       =1 to N
                                       gen
                                    
                                 
                                 
                                    
                                       for i
                                       =1 to N
                                       pop
                                    
                                 
                                 
                                    
                                       
                                       
                                       θi
                                       
                                       =
                                       Vi
                                       
                                       −
                                       Pi
                                       
                                    
                                 
                                 
                                    
                                       end for
                                 
                                 
                                    
                                       rank the individuals according to their fitness θi
                                       
                                    
                                 
                                 
                                    
                                       store the genes of the best individual in {i}
                                 
                                 
                                    
                                       
                                       k
                                       =0
                                 
                                 
                                    
                                       for j
                                       =1 to N
                                       pop
                                    
                                 
                                 
                                    
                                       
                                       
                                       k
                                       =
                                       k
                                       +1
                                 
                                 
                                    
                                       
                                       
                                       ϑm
                                       
                                       =random number from [0,1] with uniform distribution, m
                                       =(1,2)
                                 
                                 
                                    
                                       
                                       parent
                                          m
                                       
                                       =round(N
                                       pop(e
                                          αϑm
                                       
                                       −1)/(e
                                          α
                                       
                                       −1))
                                 
                                 
                                    
                                       
                                       store the indexes which are absentees in both parents in {i
                                       abs}
                                 
                                 
                                    
                                       
                                       for i
                                       =1 to ni
                                       
                                    
                                 
                                 
                                    
                                       
                                       
                                       randomly select a parent (parent1 or parent2) to give the ith gene for the kth individual of the new generation
                                 
                                 
                                    
                                       
                                       
                                       if there is a duplication of indexes then pick up a new index from {i
                                       abs}
                                 
                                 
                                    
                                       
                                       end if
                                 
                                 
                                    
                                       end for
                                 
                                 
                                    end for
                                 
                              
                           
                        
                     
                  

@&#EXPERIMENTAL RESULTS@&#

We used supervised learning to train various classifiers and perform credit rating prediction. The experiments were conducted using the following classifiers: Multilayer Perceptron (MLP) neural network, Radial Basis Function (RBF) neural network, Support Vector Machines (SVM), Naive Bayes (NB), Random Forest (RF), Linear Discriminant Classifier (LDC), and Nearest Mean Classifier (NMC).

The U.S. and European datasets consisted of 852 and 244 vectors, respectively, representing companies described by 81 parameters (43 parameters) introduced in Section 3. The missing values of each feature were replaced by the median value calculated using existing values of the same feature. The data were standardized to mean zero and unit standard deviation. To avoid overfitting, 60% of the data were used for training theclassifier and 40% were treated as unseen data and used for testing. Rating classes 
                        
                           ω
                           ∈
                           Ω
                        
                     , Ω
                     ={AAA,AA,…,D} assigned by the S&P’s rating agency were known for all companies, so it was possible to train the classifier and calculate the classification accuracy on the test set.

The MLP neural network classifier used in the experiments had the following parameters. The number of input neurons was equal to the number n of selected features. The models were tested for different numbers h of hidden layer neurons h
                     ={5,10,15}. The neurons used logistic activation function. The learning process was performed using scaled conjugate gradient algorithm with the learning rate of 0.1, momentum of 0.2, and the number of epochs of 2000.

The RBF neural network classifier used in the experiments had the following parameters. The number of neurons in the hidden layer was modified in the range h
                     ={q
                     ×2, q
                     ×3, q
                     ×4}, i.e. 2, 3 and 4 neurons for each of the q
                     =9classes. The radius of the RBF was set to 0.2.

The training of SVM was performed using the sequential minimal optimization algorithm proposed by Platt [62]. Kernel functions of the SVM were represented by RBFs with parameter gamma set to 0.3. The complexity parameter C was chosen from the range C
                     ={1.0,2.0,3.0}.

The remaining classifiers were tested with the following parameters. The Naive Bayes classifier used the normal distribution estimator for numeric features. In the case of Random Forest, 10 classification trees were generated in each experiment.

The NMC does not require any parameters. It uses one separate mean vector for each of the classes, and one variance estimation for all of the classes and prior class probabilities. All of these parameters are estimated from the training sample during the classifier learning process.

The LDC also does not require any parameters. Apart from mean vectors and prior probabilities of the classes it uses a single covariance matrix for all of the classes. The covariance matrix is estimated as a mean of covariance matrices for all classes.

For each classifier and each feature selection method 10 runs of the entire training–testing cycle were completed. Classification accuracy CA
                     test [%], Type I error rate [%] [defined as (1−specificity)×100], and misclassification cost (MCC) on the test set were recorded in each of the 10 iterations. Type I error rate (a false acceptance of the rating class) was calculated as a weighted average of all classes. Similarly, it is also possible to calculate Type II error rates (false rejections of the correct rating class), which can be expressed as 100−
                     CA
                     test [%] in multiclass problems. Both Type I and Type II error rates have been considered important measures of credit rating prediction models [11]. In addition, we calculated MCC to take into account the fact that the rating classes are ordered from the best to the worst, so it is a more serious mistake to classify a D-class company as AAA than to give the same AAA rating to an AA-class company. The corresponding cost matrix is highlighted in Table 4
                     . The greater the difference between actual and predicted class is, the higher the MCC.


                     Tables 5 and 6
                     
                      summarize the results of the experiments. In these tables, average values from the 10 runs are presented along with standard deviations. For each classification method, the best and statistically similar results (at P<
                     0.05 using a paired t-test) are separately marked in bold. The average numbers of selected features n (with standard deviations) for the 10 runs are also provided.

For the CORF and CONF, we applied the best first procedure that searches the space of feature subsets by greedy hillclimbing augmented with a backtracking facility. The level of backtracking was controlled by setting the number of consecutive non-improving nodes allowed to 5. We used forward search in both filters. For the GAF, we used population size N
                     pop
                     =200, maximum number of generations N
                     gen
                     =8, and selective pressure α
                     =6. Besides that, the GAF requires the setting of the target number of selected features. We examined this number n over the range n
                     ={1,2,…,
                     n
                     max}. The final number of features n was selected based on the shape of the objective function θ. In the area behind n there was no significant decrease in the objective function θ.

In the case of the US dataset, Random Forest classifier employing the MFS method outperformed the rest of the methods. For this learning scheme, 12.25±6.42 features were selected (out of 81 original features; 15.1% on average). For the European dataset, the NB classifier with 14.30±5.18 features (33.3% of all features) selected by the MFS worked best. The wrapper feature selection strategies improved the classification accuracies for the remaining classification methods in both datasets. For the MLP, the IFS worked best for both datasets with 17.5% and 45.8% of all features, respectively. The wrappers significantly improved the classification performance of the RBF and SVM as well.

In the case of the US dataset, the reason for a worse performance of the NB classifier lies in the nature of the data. In addition to mutual correlations between features, we also tested the correlations between features and predicted class. Only weak correlations were identified, and all were less than 0.1. As the NB classifier is a linear separator unable to represent disjunction or conjunction, it performs worse in cases when the correlated feature is not included. This classifier achieved a much better performance with the European dataset, where the correlations between features (especially profitability and leverage ratios) and predicted class were statistically significant.

High correlations between the features themselves and the results of the Kruskal–Wallis ANOVA test also imply that many features are redundant and irrelevant when applied alone, respectively. It is the suitable combination (subset) of features that enables the classifiers to perform well on our datasets. However, as pointed out by Guyon and Elisseeff [24], adding features that are presumably redundant may help to reduce noise and consequently to obtain a better class separation. In addition, a feature that is completely useless when used individually may provide a significant performance improvement when used together with other features. In this respect, filters have a disadvantage, since they are unable to include irrelevant features that may actually help performance [46]. Concerning the filters, the GAF (with 23.0% of all features on average) generally outperformed both the CORF (28.4% of all features) and the CONF (18.3% of all features) for both datasets. In the case of the European dataset, the filters provided worse classification results in comparison with the wrappers. Nevertheless, only 18.4% (CORF) to 35.8% (GAF) of all features was selected using the filters, in contrast to wrappers: 45.6% (MFS) and 48.9% (IFS) of all features on average.

In addition to the measures of prediction accuracy, we recorded computation time as well. In our experiments, the feature selection using filters was completed in less than 0.5s for both datasets. Moreover, the LDC and NMC were trained in less than 0.2s. In contrast, feature selection using wrappers was slower. It took more than 1.4s for the IFS to perform feature selection using both the LDC and the NMC in the case of European data. For the MFS, it was 13.8s (LDC) and 23.1s (NMC), respectively. In the analysis of US data, it took even more time to select the features. For instance, feature selection using the MFS method required 591s and 261s when used with the LDC and the NMC, respectively.

For most of the classifiers, the wrappers selected less consistent sets of features (with higher standard deviations). In Tables 7–9
                     
                     
                     , we provide an overview of the most frequently used features for all of the feature selection methods. Only those features that occurred in at least 4 runs of 10 were reported in these tables. For the wrapper methods, the results show that different learning algorithms perform better with different core features, even if using the same training set, which is in agreement with previous findings [46]. There are also only few coincidences among the features selected by the filter and wrapper approaches. The filter approaches select features that are closely related with the class label while the wrappers take the predictive capacity of features into account. Even if considered in the objective function, several highly correlated features were selected using the CORF method (e.g. in the size of company category). This finding indicates another disadvantage of filter methods, as they are unable to remove correlated features that may hurt performance [46].

@&#CONCLUSIONS@&#

In this study we carried out an automatic corporate credit rating assessment using several classification methods. Because rating agencies do not publicly reveal what parameters are used in the credit rating process, we decided to collect as many parameters describing the companies as possible. The US and European datasets consisted of 81 features and 43 features, respectively, which included various financial and non-financial factors. In the training of the classifiers as well as during testing on previously unseen data, we used credit ratings previously assigned to the companies by acredit rating agency.

To reduce the complexity of the classification process and improve the accuracy of classification process, we performed feature selection prior to classification. The feature selection was done using two different wrapper algorithms. Our approach maintained or even improved the accuracy after removing redundant or irrelevant features that may degrade the classification accuracy. In most of the cases, the classification accuracy was between 45% to 65%, which is quite good for a 9-class problem. Feature selection using IFS and MFS methods improved the classification accuracy in most of the cases. This improvement can lead to a substantial reduction of the cost related to credit risk management. As pointed out by Tong et al. [67], with sizable loan portfolios, even a slight improvement in the accuracy of credit evaluations can reduce the creditors’ risk and can also be translated into considerable savings in the future.

Feature selection using wrapper methods seems to be a very useful preprocessing step for an automatic corporate credit rating assessment performed using classification methods. The number of features required for classification has been significantly reduced, and in most of the cases, was reduced to less than 25 features. Thus, we conclude, that a relatively small enumeration of features decides the rating class assignment. In agreement with Huang et al. [32], this occurrence appears consistently in various financial markets. Since, in general, it is unknown what parameters are used by credit rating companies, the usual approach is to include as many parameters as possible. Feature selection makes it possible to choose the parameters that should actually be used. Additionally, lowering the dataset dimensionality may improve classification accuracy, because the influence of the so called “curse of dimensionality” isreduced. It is worth noting that the best classification accuracies (59.39% for the Random Forest classifier – the US companies, and 68.78% for the NB classifier – the European companies) were achieved for a feature set reduced using the MFS method and not for a set of all of the available features.

The filter methods were outperformed by the wrapper methods in this specific problem. The reason for this might be the ability of the wrappers to make use of both correlated and uncorrelated features for a particular classifier. There are several problems with wrappers that have been described in the literature, especially overfitting and the large amount of CPU time required. We did not observe problems with overfitting, since it mainly concerns small training sets. In our experiments the wrappers tended to produce smaller feature subsets with higher classification accuracies. Although the wrapper approaches exhibited more accurate behavior than the filter methods, this improvement comes at a price of a high computational load.

Regarding the selected features with the wrapper approaches, it was confirmed that different algorithms have different biases and the optimal subsets of features differed greatly.

Our results suggest that both the size of a company and market value ratios were the most important parameters in the US rating methodology. In contrast, the rating process of the European companies to a large extent relied on the profitability and leverage ratios. To the best of our knowledge, in the only other market comparative study, Huang et al. [32] demonstrated that the size of banks is the most important determinant of U.S. bond raters, whereas Taiwan raters focus more on profitability.

The use of feature selection algorithms seems to be a promising approach in corporate credit rating prediction. The algorithms allow for a reduction in problems arising from the fact that credit rating agencies do not publicly reveal the parameters they use to assess the credibility of companies. Further work in this area may include using feature selection methods specialized for particular classifiers (such as salience-based methods for NNs). We also envision the use of wrappers and filters while automatically fixing the number of features. This study should also encourage the expansion of the wrapper approaches to related business domains, such as bankruptcy prediction, stock price trend prediction, and credit card fraud detection. In our work, we selected a 1-year prediction horizon. In view of the fact that this horizon may vary from short to long, appropriate accuracy measures, such as Harrell’s C 
                     [60], could be developed and incorporated into the AI classifiers.

@&#ACKNOWLEDGMENTS@&#

We gratefully acknowledge the help provided by constructive comments of the anonymous referees. This work was supported by a grant provided by the Ministry of Interior of the Czech Republic No. VF20112015018 and by the scientific research project of the Czech Sciences Foundation Grant No: 13-10331S.

@&#REFERENCES@&#

