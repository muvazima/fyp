@&#MAIN-TITLE@&#Information density and overlap in spoken dialogue

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           Information density, related to entropy, is related to overlaps in spoken language.


                        
                        
                           
                           Humans prefer overlaps based on information density and suprasegmental features.


                        
                        
                           
                           This is confirmed in a speech-based rating study (p
                              <0.0001).


                        
                        
                           
                           Our results are relevant for spoken dialogue systems, especially incremental ones.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Overlap

Turn-taking

Information density

Incremental processing

Spoken dialogue systems

@&#ABSTRACT@&#


               
               
                  Incremental dialogue systems are often perceived as more responsive and natural because they are able to address phenomena of turn-taking and overlapping speech, such as backchannels or barge-ins. Previous work in this area has often identified distinctive prosodic features, or features relating to syntactic or semantic completeness, as marking appropriate places of turn-taking. In a separate strand of work, psycholinguistic studies have established a connection between information density and prominence in language—the less expected a linguistic unit is in a particular context, the more likely it is to be linguistically marked. This has been observed across linguistic levels, including the prosodic, which plays an important role in predicting overlapping speech.
                  In this article, we explore the hypothesis that information density (ID) also plays a role in turn-taking. Specifically, we aim to show that humans are sensitive to the peaks and troughs of information density in speech, and that overlapping speech at ID troughs is perceived as more acceptable than overlaps at ID peaks. To test our hypothesis, we collect human ratings for three models of generating overlapping speech based on features of: (1) prosody and semantic or syntactic completeness, (2) information density, and (3) both types of information. Results show that over 50% of users preferred the version using both types of features, followed by a preference for information density features alone. This indicates a clear human sensitivity to the effects of information density in spoken language and provides a strong motivation to adopt this metric for the design, development and evaluation of turn-taking modules in spoken and incremental dialogue systems.
               
            

@&#INTRODUCTION@&#

Traditionally, the smallest unit of processing in spoken dialogue systems has been a full utterance with strict, rigid turn-taking. More recently, however, work on incremental systems has shown that processing smaller ‘chunks’ of user input can improve the user experience by providing faster responses and allow more flexibility in turn-taking (Skantze and Schlangen, 2009; Purver and Otsuka, 2003; Skantze and Hjalmarsson, 2010; Baumann et al., 2011; Raux and Eskenazi, 2009; Dethlefs et al., 2012b). Incrementality in spoken dialogue systems enables the system designer to model several dialogue phenomena that play a vital role in human conversation (Levelt, 1989), but have so far been absent from most systems. These include more natural turn-taking and grounding through the generation of backchannels and barge-ins—which we will refer to jointly as overlaps in this article.

Previous studies on the triggers of backchannels and barge-ins in human–human conversation have revealed the importance of prosodic features, such as pitch, duration, and energy, and features relating to syntactic and semantic completeness (Koiso et al., 1998; Ward and Tsukahara, 2000; Cathcart et al., 2003; Morency et al., 2008; Gravano and Hirschberg, 2009; Oertel et al., 2012). The latter can refer to the grammatical completeness of constituents, e.g., such as a full NP versus just the determiner. We will refer to such features jointly as suprasegmental. Most previous studies have relied on manually annotated corpora for their analyses and reported results from held-out datasets, and few findings have been implemented in real spoken dialogue systems.

In a separate strand of research, psycholinguistic studies have shown that humans distribute information across linguistic units in a way so that more prominence is given to units that are less expected in a given context (Genzel and Charniak, 2002; Bell et al., 2003; Aylett and Turk, 2004; Levy and Jaeger, 2007). This evidence led us to hypothesise that there is a relation between information density and suitable places for backchannels or barge-ins in spoken conversation. Information density can be seen as a measure of entropy in human language and is computed from a language model of the domain at hand (Shannon, 1948). One advantage is therefore that it can easily be obtained incrementally for incoming strings of user speech. A further advantage of information density over other features, relating e.g. to syntactic completeness, is that it can be seen as an ‘abstract’ type of information. Information is estimated solely based on n-grams and we do not need to understand what is being said on a semantic level.

In a study that explored the relationship between information density and overlaps (Dethlefs et al., 2012a), we trained a hierarchical reinforcement learner that could generate backchannels and barge-ins in conversations with human users. The model compared a reward function that was sensitive to information density against a reward function that was not. Results showed that significantly higher human ratings were obtained for the version that took information density into account. While these results are promising, they were drawn from an exclusively text-based rating study, which potentially does not account for the peculiarities of spoken language. In this article, we therefore replicate our earlier experiments in a speech-based rating study, involving word-based as well as suprasegmental features, in order to see whether the earlier results hold in a realistic dialogue setting. Results show a clear human preference for a model that generates overlapping speech based on both suprasegmental and information density features. This is followed by overlaps based on information density features alone and then suprasegmental features alone. The results indicate a strong human sensitivity to the peaks and troughs in evolving information density in spoken language. These results hold even in the face of ASR errors.

We will start Section 2 by discussing related work on overlap in spoken dialogue systems, mainly from the perspective of incremental processing architectures. We will then describe the types of features that previous work has identified as predicting different types of overlaps, and finally the information density effects that have been observed across linguistic units in human language. Section 3 will introduce the notion of information density and exemplify some of its effects on a spoken corpus from the information-seeking dialogue domain. The relation between information density and suprasegmental features in spoken language is also discussed. In Section 4, we describe our experimental setting, data and methodology, and present results on the effect of information density on spoken overlap in dialogue. Section 5 finally draws conclusions and lays out the directions for future research.

@&#RELATED WORK@&#

The production of backchannels and barge-ins has long been recognised to facilitate grounding, feedback and clarifications in human spoken dialogue (e.g., Yankelovich et al., 1995). With the rise of incremental processing architectures (Schlangen and Skantze, 2009; Dethlefs et al., 2012b; Selfridge et al., 2011; DeVault et al., 2009), we now have the opportunity to integrate these phenomena into spoken dialogue systems. This section reviews the state of the art in incremental processing and the identification of triggers for backchannels and barge-ins in human dialogue. Finally, we discuss findings from information density applied to spoken language and draw conclusions on how all aspects can be brought together into an effective model.

Traditionally, the smallest processing unit in a dialogue system has been a full user utterance with correspondingly rigid turn-taking. With the rise of incremental architectures in recent years, however, it has become possible to model several discourse phenomena that have previously been exclusive to human–human conversation. These phenomena include faster turn-taking, grounding through the generation of backchannels and feedback, and facilitated clarification through barge-ins. Recent work has shown that including such phenomena into human–computer interaction can significantly improve the user's experience in terms of automatic speech recognition (Baumann et al., 2011), dialogue management (Buss et al., 2010), dialogue act recognition (Cuayáhuitl et al., 2013) and speech generation (Skantze and Hjalmarsson, 2010).

The smallest unit of processing in incremental systems is called an incremental unit (IU) (Schlangen and Skantze, 2009). Its instantiation depends on the particular processing module. In speech recognition, IUs can correspond to phoneme sequences that are mapped onto words (Baumann and Schlangen, 2011). In dialogue management, IUs can correspond to dialogue acts (Buss et al., 2010). In natural language and speech generation, IUs can correspond to single words, phrases or full dialogue acts (Skantze and Hjalmarsson, 2010; Dethlefs et al., 2012b). Finally, in speech synthesis, IUs can correspond to speech unit sequences which are mapped to segments and speech plans (Skantze and Hjalmarsson, 2010).


                        Fig. 1
                         illustrates the contrast between traditional processing units and incremental units, where an advantage of the latter is that they allow more flexible turn-taking. While the non-incremental case in the figure would process a full dialogue act, e.g., inform(restaurant, venueName=Beluga, priceRange=moderate, foodType=Italian, area=city centre) without giving a user the opportunity to barge-in, the incremental case is able to process smaller unit dialogue acts, such as inform(restaurant, venueName=Beluga), inform(restaurant, priceRange=moderate), etc. The advantage of the latter model is that a user barge-in over an incremental dialogue act would not lead to the entire system utterance being re-prompted at the next turn.

Phenomena of turn-taking have been the focus of several studies in incremental processing. For example, Raux and Eskenazi (2009) optimise turn-taking in a dialogue system based on a cost matrix and decision theoretic principles, assuming that users prefer no gap and no overlap at turn boundaries. DeVault et al. (2009) allow a small “responsive” overlap between user and system utterances by predicting the completion and thus the end of a user utterance. Selfridge et al. (2011) incrementally predict the stability and accuracy of speech recognition hypotheses so as to enhance system performance without causing delays at turn boundaries.

Initial evidence therefore suggests that incremental architectures are able to offer the turn-taking flexibility required to model more of the discourse phenomena found in human language. Backchannels, barge-ins and some studies that aim to predict them in human conversation will be discussed in the following.

An important advantage of incremental architectures is that they are able to generate and process backchannels and barge-ins—often adding to the system's reactiveness. Fig. 2
                         shows examples of both phenomena. Backchannels can often be interpreted as signals of grounding. Produced by the user, the system may infer that the user is following the presentation of information or is confirming a piece of information without trying to take the turn. Similarly, we could allow a system to generate backchannels to the user to confirm that it understands the user's preferences. An important decision for a dialogue system then would be when to generate a backchannel. Barge-ins typically occur in different situations. The user may barge-in on the system to correct an ASR error (such as ‘Italian’ instead of ‘Indian’ in Fig. 2). A system may want to barge-in on a user in order to confirm a low-confidence ASR hypothesis immediately so as to start its database look-up. In the latter case, the system will need to decide if and when to generate a barge-in. Both overlap phenomena are presumably particularly relevant to hands-free, eyes-free scenarios. Previous work has confirmed that users of spoken dialogue systems do require feedback so as to know whether the system is still listening to them or processing their request (Yankelovich et al., 1995). However, it has also been shown that feedback needs to occur at the right moment in order not to confuse the user rather than helping (Hirasawa et al., 1999). Several studies have therefore investigated the linguistic cues that signal suitable points for backchannels or barge-ins in human–human dialogue. Common findings have been a final falling or rising pitch or final low/high pitch levels as distinctive prosodic features indicating the end of a turn (Ward and Tsukahara, 2000; Koiso et al., 1998). Duration and energy can sometimes play a role, as well as features relating to semantic or syntactic completeness (Ward and Tsukahara, 2000; Cathcart et al., 2003; Morency et al., 2008). The latter tend to denote the completion of a grammatical clause or constituents as indicated through its part-of-speech (POS) sequence. Several authors have observed that a combination of features leads to improved performance Koiso et al. (1998), Gravano and Hirschberg (2009).

A common approach to investigating turn-taking signals has been to annotate data sets of human–human spoken conversation and then train a statistical prediction model from them. Focusing on predicting locations of overlapping speech, for example, Oertel et al. (2012) analyse a corpus of spoken human multiparty conversations. They demonstrate in a classification study that locations of overlapping speech are prosodically different from locations of non-overlapping speech. The prosodic features of a 5s window surrounding an overlap are characterised by significantly higher intensity and F0 frequency and significantly smaller F0 range than in windows of non-overlapping speech. The authors interpret this as representing a potentially higher level of involvement of one of the speakers which leads to the observed prosodic patterns at overlaps.

Another study (Gravano and Hirschberg, 2011) looks into the prosodic, syntactic and lexical cues that precede turn switches and backchannels in human–human conversation. Based on a classification study from human-labelled data, the authors identify seven cues that precede smooth turn switches (i.e., without overlap). Their unit of analysis is the inter-pausal unit (IPU), a sequence of words preceded and followed by a silence period of more than 50ms. The following cues signalled suitable places for turn transitions with little variation in the authors’ dataset:
                           
                              1.
                              a falling or high-rising intonation at the end of an IPU,

a reduced lengthening of IPU-final words,

a reduced intensity level,

a reduced pitch level,

a point of textual completion,

a higher variability in frequency, amplitude of vocal-fold vibration or energy ratio of noise to harmonic components in the voiced speech signal,

longer duration of the IPU.

Regarding relevant places for backchannels, six cues were identified:
                           
                              1.
                              a rising intonation at the end of an IPU,

an increased intensity level,

an increased pitch level,

a final POS bigram out of ‘DT NN’, ‘JJ NN’, ‘NN NN’,

a reduced noise-to harmonics ratio (NHR), and

an increased duration of the IPU.

While the authors reliably found some of these cues present when backchannels occurred in the data, the reverse is not true—there need not be a backchannel whenever the cues occur. This can likely be related to the optional nature of backchannels and varies between individual speakers.

Good progress has been made in identifying the triggers of backchannels and barge-ins in human–human conversation. Unfortunately, not many of these results have been implemented within real spoken dialogue systems—even incremental ones—and tested with human users. This may be to some extent because several of the suprasegmental features used in classification can be computationally intensive to obtain online (e.g., the noise-to-harmonics ratio) so that authors have preferred manual annotation. Another reason could be that backchannels, and even barge-ins, are often optional in dialogue so that human production cannot be seen as much of a gold standard.

Humans have a tendency to distribute information across linguistic units in a way that all information is transmitted within the bounds of a communicative channel, where information-dense segments are marked with an increased prominence (Bell et al., 2003; Aylett and Turk, 2004; Levy and Jaeger, 2007; Rajkumar and White, 2011). This finding has been reported at different linguistic levels, including the prosodic, syntactic, syllable and word levels. As an example, Bell et al. (2003) study the origins of variability in the pronunciation of function words, such as the, that, and and of. Based on the observation that these words receive a fuller or reduced pronunciation in different linguistic contexts, the authors investigate the variation in the length of words, the form of their vowel (basic, full or reduced) and the presence of final obstruents. They find that the entropy of words (i.e., how expected they are in their given context) is one of three factors determining the pronunciation of function words. The other two factors are neighbouring disfluencies and the word's position in an utterance. These findings have been confirmed for prosody. Aylett and Turk (2004) show that prosodic prominence in spoken language is strongly related to entropy—the less expected a section of speech is in an utterance, the more likely it is to be prosodically prominent.

Other studies have shown evidence for a role of information density—or entropy—in syntactic reduction (Levy and Jaeger, 2007; Jaeger, 2010). It is shown that speakers are more likely to produce an optional syntactic complementiser (e.g., that) when entropy is high rather than when it is low. Given that complementisers such as that often have low entropy, they can be used to reduce the cognitive load on the listener in high-entropy sections. These findings have also been applied to surface realisation. Using features from information density, Rajkumar and White (2011) show that the prediction accuracy of a realisation ranking model is substantially improved for the use of optional that complementisers. Results by Genzel and Charniak (2002) are in line with this result, where the authors study the entropy of words in English text and find that all words in a text have roughly the same entropy.

Several studies thus seem to suggest a strong relationship between information density and linguistic prominence, including prosodic prominence. From the previous section we know that there is a relation between suprasegmental features and overlapping speech among humans. It is therefore worth asking whether a relationship can be established between information density and overlapping speech. In an earlier study (Dethlefs et al., 2012a), we trained a hierarchical reinforcement learner to predict the best point for a barge-in or backchannel in human–computer interaction. Results showed that a reward function that draws on information density helps to obtain significantly higher user ratings than baselines that are not sensitive to information density. While these findings seemed to point in a positive direction, they were drawn exclusively from a text-based rating study. In this article, we aim to extend them to spoken language and observe the relationship between information density and overlaps in speech.

This section will introduce the concepts behind information density and present some examples from actual interactions with a spoken dialogue system in the restaurant domain. We will also compare the information density in spoken utterances to suprasegmental features used in previous studies. Finally, we show that information density can be obtained from ASR analyses so that use within spoken dialogue systems is feasible.

Information theory (Shannon, 1948) is based on two main concepts: a communicative channel through which information is transferred in bits and the information gain, i.e., the information load carried by each bit. For natural language, the assumption is that humans aim to communicate as closely as possible to the channel's capacity. If they exceed it, the cognitive load of the listener gets too high. If they stay too far below, too little information is transferred per bit and the utterance is uninformative. The information gain of each word, which is indicative of how close we are to the channel's capacity, can be computed using measures of entropy. A related measure to entropy is information density which measures the distribution of information across an utterance.

Psycholinguistic research as discussed in Section 2.3 has shown that humans have a tendency to distribute information across the linguistic units in an utterance, e.g. words, syllables or phonetic units, in a way that keeps the overall information density within the bounds of the communicative channel. While the exact bits transmitted per unit can vary, practically they seem to lie between 2 and 6 bits with an average of about 4 bits per linguistic unit. This is shown by our own experiments in this article and in Dethlefs et al. (2012a) but also in examples shown in Jaeger (2010).

Relating information density to likelihood of words, we can say that the less frequent a word is, the more information it is likely to carry (Jaeger, 2010). In other words, the lower the probability of a word or n-gram, the higher its information density will be. Compare, for example, the word ‘the’ in a corpus of restaurant recommendations against the word ‘Nepalese’. Similarly, Jaeger (2010) has shown that the notion of information density can be used to predict the occurrence of ‘that’ in relative clauses where it is optional.

Information density is defined as the log-probability of an event (i.e., a word, a phrase or a whole utterance) (Shannon, 1948; Levy and Jaeger, 2007), so that for a utterance formed by N words 
                           {
                           
                              w
                              1
                           
                           …
                           
                              w
                              N
                           
                           }
                        , we can compute the incremental point-wise information density (for each word 
                           
                              w
                              i
                           
                        ) as:
                           
                              (1)
                              
                                 ID
                                 (
                                 
                                    w
                                    i
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   log
                                                   
                                                      1
                                                      
                                                         P
                                                         (
                                                         
                                                            w
                                                            1
                                                         
                                                         )
                                                      
                                                   
                                                
                                                
                                                   for
                                                      
                                                   i
                                                   =
                                                   1
                                                
                                             
                                             
                                                
                                                   log
                                                   
                                                      1
                                                      
                                                         P
                                                         (
                                                         
                                                            w
                                                            2
                                                         
                                                         )
                                                      
                                                   
                                                   +
                                                   log
                                                   
                                                      1
                                                      
                                                         P
                                                         (
                                                         
                                                            w
                                                            2
                                                         
                                                         |
                                                         
                                                            w
                                                            1
                                                         
                                                         )
                                                      
                                                   
                                                
                                                
                                                   for
                                                      
                                                   i
                                                   =
                                                   2
                                                
                                             
                                             
                                                
                                                   log
                                                   
                                                      1
                                                      
                                                         P
                                                         (
                                                         
                                                            w
                                                            3
                                                         
                                                         )
                                                      
                                                   
                                                   +
                                                   log
                                                   
                                                      1
                                                      
                                                         P
                                                         (
                                                         
                                                            w
                                                            3
                                                         
                                                         |
                                                         
                                                            w
                                                            2
                                                         
                                                         )
                                                      
                                                   
                                                   +
                                                   log
                                                   
                                                      1
                                                      
                                                         P
                                                         (
                                                         
                                                            w
                                                            3
                                                         
                                                         |
                                                         
                                                            w
                                                            1
                                                         
                                                         ,
                                                         
                                                            w
                                                            2
                                                         
                                                         )
                                                      
                                                   
                                                
                                                
                                                   for
                                                      
                                                   i
                                                   =
                                                   3
                                                
                                             
                                             
                                                
                                                   …
                                                
                                                
                                             
                                             
                                                
                                                   log
                                                   
                                                      1
                                                      
                                                         P
                                                         (
                                                         
                                                            w
                                                            i
                                                         
                                                         )
                                                      
                                                   
                                                   +
                                                   log
                                                   
                                                      1
                                                      
                                                         P
                                                         (
                                                         
                                                            w
                                                            i
                                                         
                                                         |
                                                         
                                                            w
                                                            
                                                               i
                                                               −
                                                               1
                                                            
                                                         
                                                         )
                                                      
                                                   
                                                   +
                                                   log
                                                   
                                                      1
                                                      
                                                         P
                                                         (
                                                         
                                                            w
                                                            i
                                                         
                                                         |
                                                         
                                                            w
                                                            
                                                               i
                                                               −
                                                               1
                                                            
                                                         
                                                         ,
                                                         
                                                            w
                                                            
                                                               i
                                                               −
                                                               2
                                                            
                                                         
                                                         )
                                                      
                                                   
                                                
                                                
                                                   for
                                                      
                                                   i
                                                   >
                                                   3
                                                
                                             
                                             
                                                
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Note that while typically the context of a word is given by all preceding words of the utterance, several authors have restricted themselves to trigrams for practical reasons (Genzel and Charniak, 2002; Jaeger, 2010).

To utilise information density in our own study, we first need to estimate an n-gram model for the domain of interest, in our case information-seeking dialogues in the restaurant domain. To compute the point-wise information density of user utterances (in the form of human transcriptions) at each word that is spoken, we estimated an n-gram language model based on the CLASSiC corpus, a corpus of spoken human–system dialogues in the restaurant domain (Lemon and Pietquin, 2012). The corpus consists of 1500 dialogues and is freely available.
                           1
                        
                        
                           1
                           Corpus available from http://www.macs.hw.ac.uk/ilabarchive/classicproject/data/login.php.
                         We base our analysis on 1-grams, 2-grams and 3-grams, which has yielded good results in previous work. The language model was trained with the Kylm Language Modelling Toolkit
                           2
                        
                        
                           2
                           
                              http://www.phontron.com/kylm/.
                         and applied Good-Turing smoothing.


                        Fig. 3
                         shows examples of the evolving information density of two spoken utterances from different speakers from CLASSiC. In accordance with Eq. (1), for the utterance “I need to find a Chinese restaurant in the Girton area”, we can compute the information density of each word 
                           
                              w
                              i
                           
                         as:


                        
                           
                              (2)
                              
                                 ID
                                 (
                                 
                                    w
                                    i
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   log
                                                   
                                                      1
                                                      
                                                         P
                                                         (
                                                         I
                                                         )
                                                      
                                                   
                                                
                                                
                                                   for
                                                      
                                                   i
                                                   =
                                                   1
                                                
                                             
                                             
                                                
                                                   log
                                                   
                                                      1
                                                      
                                                         P
                                                         (
                                                         need
                                                         )
                                                      
                                                   
                                                   +
                                                   log
                                                   
                                                      1
                                                      
                                                         P
                                                         (
                                                         need
                                                         |
                                                         I
                                                         )
                                                      
                                                   
                                                
                                                
                                                   for
                                                      
                                                   i
                                                   =
                                                   2
                                                
                                             
                                             
                                                
                                                   log
                                                   
                                                      1
                                                      
                                                         P
                                                         (
                                                         to
                                                         )
                                                      
                                                   
                                                   +
                                                   log
                                                   
                                                      1
                                                      
                                                         P
                                                         (
                                                         to
                                                         |
                                                         need
                                                         )
                                                      
                                                   
                                                   +
                                                   log
                                                   
                                                      1
                                                      
                                                         P
                                                         (
                                                         to
                                                         |
                                                         I
                                                         ,
                                                         need
                                                         )
                                                      
                                                   
                                                
                                                
                                                   for
                                                      
                                                   i
                                                   =
                                                   3
                                                
                                             
                                             
                                                
                                                   …
                                                
                                                
                                             
                                             
                                                
                                                   log
                                                   
                                                      1
                                                      
                                                         P
                                                         (
                                                         area
                                                         )
                                                      
                                                   
                                                   +
                                                   log
                                                   
                                                      1
                                                      
                                                         P
                                                         (
                                                         area
                                                         |
                                                         Girton
                                                         )
                                                      
                                                   
                                                   +
                                                   log
                                                   
                                                      1
                                                      
                                                         P
                                                         (
                                                         area
                                                         |
                                                         Girton
                                                         ,
                                                         the
                                                         )
                                                      
                                                   
                                                
                                                
                                                   for
                                                      
                                                   i
                                                   =
                                                   11
                                                
                                             
                                             
                                                
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

We can observe that information is distributed across linguistic units that all transmit information between 2 and 6 bits. Peaks or rising information density occur at keywords, such as Chinese, Girton, French restaurant or Edinburgh and troughs at function words such as a and the. Note that while Eq. (1) might suggest that information density is ever increasing throughout an utterance (by computing the sums of previous information density scores), one cause of the peaks and troughs we can observe in information density is the fact that they are computed from trigrams of words. This means that whenever the sum of point-wise information density scores for a trigram is lower than the sum of the previous trigram, we would see a trough.

From the CLASSiC corpus, we compute our language model based on 1200 dialogues (which correspond to 11,000 user utterances) and hold the remainder out for testing. This is to ensure that the information density of user utterances is not computed for the same word string as occurring in the training data.

@&#EXPERIMENTS@&#

Our experiments are based on spoken excerpts of interactions between a human and a spoken dialogue system in the restaurant domain. For each excerpt, we compare three alternative points of the system generating overlapping speech over a user utterance. Each triplet contains each of the following:
                           
                              1.
                              An overlap generated based on features identified by previous work (Gravano and Hirschberg, 2011; Oertel et al., 2012). This includes prosodic information, but also features on bigrams of POS tags indicating completeness of a constituent. We will refer to this set as suprasegmental features. Section 2.2 gave an overview of the general findings of related work and Table 1
                                  shows the features used to mark appropriate locations of barge-ins and backchannels in a spoken user utterance. An appropriate location was marked when at least two of the features were observed at the same location in a user utterance.

An overlap generated based on information density features alone. To this end, we used the language models trained in Section 3.3 to compute the information density after each word in a user utterance. Noticeable troughs in information density (by at least a measure of 2) were marked as appropriate locations for an overlap, either backchannel or barge-in.

An overlap generated based on both types of features described in points 1 and 2 above. An appropriate location was marked whenever the conditions for both 1 and 2 above were met at the same location. This was the case at least once in all user utterances.

All tokens for the evaluation study were prepared based on automatically extractable features to avoid subjective judgement. We used the Praat software for the extraction of prosodic features and the Stanford POS tagger
                           3
                        
                        
                           3
                           
                              http://nlp.stanford.edu/software/tagger.shtml.
                         for POS tags. Based on these features, we extracted 60 interactions between users and systems (i.e. pairs of user utterances and overlapping system utterances) from the CLASSiC corpus and prepared three versions of overlap for each extract. Half of them involved a backchannel and the other half involved a barge-in so that potential differences could be observed. Specifically, barge-ins were produced according to the rules in the top half of Table 1 and backchannels were produced according to the rules in the bottom half of the table. Table 2
                         shows an example excerpt with the three overlap options, here using the backchannel “Okay” as overlap. The overlap is shown in the three different places predicted by the different models. As can be seen, sys-both produces the latest backchannel in the utterance but still overlaps with the word “area” uttered by the user. While in our particular system, the area slot is the most likely follow-up to “central”, a system with wider coverage, e.g. for slots park or town, could have missed part of the user's intention in this case.

We deliberately chose to investigate overlap with a spoken dialogue system, rather than in human–human dialogue, because our ultimate research objective is to integrate our results into spoken dialogue systems, such as the PARLANCE system (Hastie et al., 2013) for the restaurant domain. In the following, we will present experiments in two conditions: (a) overlaps based on transcriptions of user utterances, and (b) overlaps based on the ASR 1-best results obtained during interactions.


                        Methodology. 200 users took part in our rating study and rated altogether 529 triplets of speech overlaps. All users were recruited via the CrowdFlower crowdsourcing platform
                           4
                        
                        
                           4
                           
                              http://www.crowdflower.com/.
                         and were all self-rated native or fluent speakers of English. From CrowdFlower, participants were provided with a link to an external webpage, where the actual rating study was hosted. The webpage is shown in Fig. 4
                         along with instructions on how to use it. Participants were presented with three short excerpts of interactions between a human user and a spoken dialogue system. While the human was trying to obtain information from the system, the system would produce overlapping speech at three alternative points during the user's speech. These alternatives corresponded to our three models of producing overlap. Our research question is thus that assuming the system wants to produce an overlap, where is the best or most natural place to do so. While we would in general assume that a spoken dialogue system would always have the option to overlap or not overlap, in this experimental setup we deliberately did not offer users the option to click “no interruption”. Previous work has shown that generating system overlaps can be advantageous under certain conditions, e.g. producing a backchannel to signal continued attention or clarifying a known ASR error early on to avoid follow-up errors (Yankelovich et al., 1995; Hirasawa et al., 1999), so that in this article we are particularly interested in the question so as when would be the best point to produce such overlap. Participants in our study were asked to listen to all three versions carefully and then choose one option as their preferred one. Table 2 showed an example of a triplet.


                        Results. The results will be analysed from two perspectives, (a) overall user preferences for our three different models, and (b) the predictive power of different features with respect to user preferences based on a regression study.


                        User preferences. Table 3
                         shows the user preference results organised into three groups: (a) preferences for overlaps based on suprasegmental features only, (b) preferences based on information density features only, and (c) preferences based on both types of features. We can see that users showed a clear preference for the third model, which combines different types of features. For this model, the overall preference lies at 64.46%, corresponding to 341 out of 529 ratings, and is significant at p
                        <0.001 using a Chi-Squared test with 2 degrees of freedom. In comparison, the model based on information density features alone is preferred 148 times, corresponding to 28%, which is preferred significantly more often than the version based on suprasegmental features alone (40 ratings, 7.56%). The differences between these two is again significant with p
                        <0.0001. In addition, we can analyse the effect that particular types of overlaps had, i.e., whether overlapping backchannels were perceived differently from overlapping barge-ins. Results are shown in rows 2–3 and 5–6 in the third column Table 3. None of the differences found are significant, though. Since users rated overlaps from the same set of samples, the variance between user preferences can be analysed. We found an average variance of 0.25 across utterances with a maximum of 0.63 for one overlap and a minimum of 0.03 for another overlap. There is no difference in the variance between preferences for barge-ins and overlaps.

Our results largely confirm the findings of previous work in highlighting the importance of suprasegmental—i.e., prosodic and grammatical completeness—features to predict overlaps in spoken language. Moreover, the results provide evidence that information density has a strong influence on the perception of appropriate points for overlapping speech, which has so far been overlooked. The effect of information density appears drastic when comparing human preferences for the suprasegmental system of only 7.56% (which can be said to represent the current state of the art in overlap prediction) and the system which takes information density into account in addition (64.46%). The overall human preference for a model that takes both types of features into account seems to suggest that information density adds further information over previously used features, which is possibly particularly advantageous in human–computer interaction. Since spoken dialogue systems tend to lack the sophisticated turn-taking strategies observable in human–human conversation, information density might provide valuable cues in where overlap is acceptable to humans and where it is not.


                        Variability in user preferences. Given the fact that users preferred a model that takes a mixture of features into account to produce overlaps, we were interested in the different ways that each of the features contributes to the overall user preference found. We therefore used pairs of feature vectors characterising each overlap point in our data set and their assigned user preference (1 for preferred, 0 for not-preferred) in a classification experiment. Since we found no difference between backchannel and barge-in overlaps in Section 4.2, both are treated in the same way in this experiment. Feature vectors contained the same features as shown in Table 1 (shown as bold-face in parentheses) at the point that an overlap occurred. In addition, we used a “position” feature indicating the position of the word at which the overlap occurs. Using the Weka toolkit (Witten and Frank, 2005), we trained a J48 decision tree classifier. In a 10-fold cross-validation, the classifier reached an accuracy of 86%. In comparison, a simple majority baseline on the same data only achieved an accuracy of 78%.


                        Fig. 5
                         shows an illustration of our learnt tree, where more important features can be seen as occurring higher in the tree. Again, we can observe that while the features identified in previous work play a critical role, information density is an important factor in determining the overall user preference for system overlaps. The tree also provides some insights into the cases where our combined system was not the preferred user option. This occurred most often when the system would (a) overlap over a keyword e.g. in order to clarify a previous misrecognised keyword, which is shown by the information density tree nodes and to a lesser extent by the POS-tag sequences (users did not like the system to overlap over noun phrases); and (b) when the system would overlap too early in the utterance because of a longer user silence. In addition to this, there appears to be subjectivity in the preferences of different versions of overlap as indicated in the analysis of variance.


                        Results. To demonstrate that our earlier results hold even in the face of potential ASR errors, we repeated the experiments described above with a separate set of 60 tokens, i.e. pairs of user utterances with overlapping system utterances. In the new tokens, overlaps were estimated based on information density in ASR 1-best hypotheses, which could contain errors in recognition. The language model used was the same as previously, i.e. trained on transcribed utterances, so as to make sure that the system would not be trained on ASR errors and thus “expect” them. 200 users took part in an online rating study that was identical in its setup to the earlier study. 452 utterances were rated all together. The results are presented in Table 4
                        .

We can see that the results largely confirm the earlier results obtained for transcribed utterances. An overall preference is revealed in favour of the model that compares suprasegmental and information density features (54%). All differences are significant at p
                        <0.001 based on a Chi-Square test with 2 degrees of freedom. The variance between user ratings lies at 0.3 on average with a maximum variance of 0.58 for one utterance and a minimum variance of 0 for three utterances.


                        User preferences. Interestingly, we can observe a slight increase in preference for the model that relies on information density features only in comparison to the combined model. A closer qualitative analysis reveals that even in the case of misrecognitions, it is often still possible to identify keywords. For example, in one case the utterance “Hi (0.52) I’m (0.07) looking (0.07) for (0.39) a (2.28) Thai (1.046) restaurant (0.05)” was misrecognised as “Hi (0.52) I’m (0.07) looking (0.07) for (0.39) a (2.28) five (5.55) restaurant (4.90)”. Despite the error, the information density distribution is similar across both utterances as indicated in parentheses after the respective words. A difference however occurs around the misrecognised “five”, which results in a sharp increase in information density. This would lead to an overlap occurring following the misrecognised keyword because information density falls again after the word. In this particular example, this leads to an overlap occurring later in the utterance than in for its transcribed counterpart: while in the transcribed utterance, the overlap would occur just after “Thai”, in the ASR utterance, it would get delayed until just after “restaurant”. It is likely that such differences could have led to the overall increase in user preferences for the information density model. Incidentally, when analysing to what extent taking account of the full ASR N-best list would help to improve results, we find that the correct utterance is only part of the N-best list in 41.7% when it is not ranked as 1-best. The automatic speech recogniser that was used in the CLASSiC corpus, on which our results are based had a WER of 53.6, see Rieser et al. (2011).


                        Variability in user preferences. Similarly to the experiments based on transcribed utterances, we can train a J48 decision tree classifier to reveal the most important determining factors in user preference. The resulting model is shown in Fig. 6
                        . Our decision tree classifier achieved an accuracy of 85% in a 10-fold cross-validation, while a simple majority baseline only achieved in accuracy of 79% in the same experimental setup. In contrast to the tree trained from transcribed utterances, we can see here that user preferences were sensitive to certain POS-sequences as well the duration and pitch of overlapped segments. Information density plays a role in connection with POS-sequences as was also observed in the decision tree for transcribed utterances.

@&#CONCLUSIONS AND FUTURE WORK@&#

Previous work investigating spoken overlap in human–human conversation has often highlighted the importance of suprasegmental features in marking suitable points for backchannels or barge-ins. A separate strand of investigations has established that there is a strong relationship between entropy and prominence in language, manifesting itself in less expected linguistic units being realised in a marked form. The notion of entropy or expectation in language has often been related to information density and has been shown to be operational at the phonetic and prosodic levels of language, among others.

In this article, we bring these two strands together. We explore the hypothesis that information density can also be related to the occurrence of overlapping speech, such as backchannels and barge-ins, in spoken dialogue. In an experiment with human judges, we collected ratings of three models that generate overlaps based on features relating to (1) prosody and semantic and syntactic completeness, (2) information density, and (3) both types of information. Human raters showed a significant preference for the third model (p
                     <0.001). This demonstrates that besides prosodic and completeness features, humans are indeed sensitive to the evolving information density in spoken language. They significantly preferred overlaps at points of low information density over such occurring at points of high information density. These results are relevant to research on spoken dialogue systems, especially those that make use of incremental processing, and are therefore able to address communicative features such as backchannels or barge-ins. Previous work has provided evidence that interactive feedback mechanisms such as backchannels and barge-ins have important positive effects on spoken dialogue systems in terms of indicating the system's status to the user (Yankelovich et al., 1995; Hirasawa et al., 1999). Our work represents a further step in the direction of equipping spoken dialogue systems with interactive feedback mechanisms, which are easy to implement based on a measure of entropy that requires access to only the words spoken and a language model of the domain. In particular, we have made the following novel contributions:
                        
                           •
                           We have presented the first study that investigates experimentally the effects of different suprasegmental features on human turn-taking preferences in spoken dialogue (previous studies have relied on classification experiments only).

Our experiment shows that humans are (at least partially) sensitive to the peaks and troughs of information density in spoken language.

We have demonstrated that the effects found hold for transcribed utterances and for (potentially erroneous) outputs of an ASR module equally.

Our results show that humans are sensitive to the troughs and peaks in information density in spoken language and that this sensitivity does at least partially guide their preferences on system-initiated overlap. Equipping spoken dialogue systems therefore with more interactive feedback mechanisms, such as spoken overlaps in the form of backchannels or barge-ins, could help to make them more responsive to user overlaps, more communicative of their own status and more natural and human-like to interact with.

Future research will explore the following directions:
                        
                           1.
                           In this article, we have been able to confirm the positive effect of information density on overlap in spoken language reported earlier in Dethlefs et al. (2012a). While the present speech-based scenario was more natural than the earlier text-based rating study, we plan further experiments using a full spoken dialogue system. This step is essential to show that users perceive and approve of the points of overlap predicted by our method even in a task-based scenario when they are not particularly focused on the differences. This will also be important to show whether system-initiated overlaps are preferable to users over repetition or clarification requests.

Information density has been shown to be operational across linguistic levels in human language. However, its effects in natural language processing have not been explored, with a few exceptions (Rajkumar and White, 2011; Dethlefs et al., 2012a). It would be interesting and important to further explore the role that information density can play within spoken dialogue systems or natural language generators. This could address the rankings of a set of competing dialogue acts or surface realisations in the face of the communicative channel and its capacity at different times during an interaction.

While this paper has focused on system-initiated overlaps, future work could also explore phenomena of user-initiated overlap. For example, information density might help the system to decide whether the user is offering a backchannel or is barging-in on the system. In the latter case, the system could weigh up the importance of what the user is saying in order to decide whether to yield or try to keep the current turn.

Our experiments were based on conventional n-gram language models in accordance with earlier work on information density in language. Since the quality of overlap prediction crucially depends on the robustness of the underlying language model, future work could explore alternative methods. A candidate for investigation are recurrent neural networks, which have been shown to superior to conventional n-gram models in a number of respects (Mikolov et al., 2010).

Information density has so far only been explored at the linguistic level. However, it has been shown that phenomena of salience and information structure are also at work in visual and multimodal language scenarios (Elsner et al., 2014). Future work could attempt to establish a relationship between information density and multimodal dialogue or natural language generation scenarios or investigate the effect of information density on multimodal turn-taking scenarios (Chao and Thomaz, 2013; Nalin et al., 2012).

Throughout this article, we have followed previous work in assuming that the principles of human–human turn taking are readily transferable to turn taking in human–computer interaction. In a recent study on the occurrence of barge-ins of users over a spoken dialogue system, however, Cuayáhuitl et al. (2013) find that the most likely place for a user to barge-in on a system is after the first or second dialogue act. This substantially differs from human–human communication, where barge-ins tend to occur much later, supposedly because very early barge-ins may be less socially acceptable among humans. As a conclusion, it may be that not all findings from human–human data are as readily transferable to spoken dialogue systems as is often assumed. Future work could provide a systematic comparison of the differences between the two modes of interaction.

@&#ACKNOWLEDGEMENTS@&#

The research leading to this work was supported by the European Commission's FP7 programme under grant agreement number 287615 (PARLANCE) and the EPSRC grant no. EP/L026775/1 “Generation for Uncertain Information (GUI)”. Thanks to Michael White for making the initial suggestion to explore information density in dialogue and to Chika Ugwuanyi for her help in setting up the website for experimentation.

@&#REFERENCES@&#

