@&#MAIN-TITLE@&#Flexible architecture for cluster evolution in cloud computing

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           FACE supports system primitives that allow application developers to develop various applications in clouds.


                        
                        
                           
                           FACE allows application developers to customize data partitioning, localization, and processing procedures.


                        
                        
                           
                           FACE designs its system primitives in a language-independent and platform-independent way.


                        
                        
                           
                           FACE makes extensible the Master of a MapReduce system by application developers.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Architecture

Bandwidth

MapReduce

Cloud computing

Cluster

Operating system

@&#ABSTRACT@&#


               
               
                  MapReduce is considered the key behind the success of cloud computing because it not only makes a cluster highly scalable but also allows applications to use resources in a cluster. However, MapReduce achieves this simplicity at the expense of flexibility for data partitioning, localization, and processing procedures by handling all issues on behalf of application developers. Unfortunately, MapReduce currently has no solution capable of giving application developers flexibility in customizing data partitioning, localization, and processing procedures. To address the aforementioned flexibility constraints of MapReduce, we propose an architecture called Flexible Architecture for Cluster Evolution (FACE) which is both language-independent and platform-independent. FACE allows a MapReduce cluster to be designed to match various application requirements by customizing data partitioning, localization, and processing procedures. We compare the performance of FACE with that of a general MapReduce system and then demonstrate performance improvements with our implemented procedures.
               
            

@&#INTRODUCTION@&#

MapReduce [1] is a programming model proposed by Google to process a large number of datasets in a cluster [2]. MapReduce is the key behind the success of cloud computing [3] today because it not only makes a cluster highly scalable but also allows applications to use resources in a cluster easily. When working for an application in a cluster, MapReduce can make computers (also known as nodes) process well-partitioned data simultaneously without interfering with each other. MapReduce relies on its runtime system to partition input data automatically and distribute intermediate results [1] over nodes in a cluster. MapReduce hides the issues of cooperatively distributing data over nodes working for applications from application developers. All MapReduce requires from application developers is the preparation of a Map function (also known as a Mapper) and a Reduce function (also known as a Reducer) to process the application data. Technically, MapReduce runs a Mapper to process input data and produce intermediate results constructed with a series of key/value pairs while running a Reducer to merge values in intermediate results associated with the same key.

MapReduce contributes to the success of cloud computing due to its simplicity, but it does so at the expense of several other potential benefits. To achieve simplicity, MapReduce handles all parallel and distributed computing issues on behalf of application developers, but as a result, it suffers from several constraints:
                        
                           •
                           MapReduce partitions input data into a series of fixed-size blocks (e.g., 64MB in Google and Hadoop MapReduce implementations [1,4]) as the working units for Mappers. However, a cloud is often composed of nodes with various hardware configurations along with different performances, and a fixed but appropriate block size is not easily determined to give all applications their optimal performances. Application developers of current implementations cannot dynamically adjust the granularity of a Map task at runtime to balance workloads among nodes.

MapReduce makes use of a built-in hash function to distribute intermediate results automatically over the corresponding nodes. Consequently, application developers cannot choose nodes to perform certain location-aware computations (e.g., to transfer intermediate results among intra-rack nodes to avoid overloading links between racks). This is because MapReduce automatically selects the node with a free slot (usually indicating an available quota of CPU resources) [1,4] to execute a task. Thus, application developers cannot change the node selection policy according to their specific criteria.

MapReduce automatically executes a Reducer to handle intermediate results produced by a Mapper, so application developers cannot process application data outside of Mappers and Reducers. Sometimes, application developers require a post-processing procedure so that they can process outputs collected from all Reducers for certain application requirements (e.g., as inputs for the next iteration in iterative applications).

To achieve simplicity, MapReduce loses many potential benefits such as data partitioning, localization, and processing procedures because it automatically cares about most issues with the procedures without leaving application developers any room and flexibility to modify the procedures. If a MapReduce system supports application developers with flexibility in the data partitioning procedure, they can dynamically adjust the sizes of partitioned data to balance task loads at an appropriate granularity. If a MapReduce system supports application developers with the flexibility in the procedure of data localization, they can choose a certain node to run a Mapper for processing a block of input data or a Reducer for processing some intermediate results. If a MapReduce system supports application developers with flexibility in the data processing procedure, the developers can program behaviors of Mappers or Reducers like current MapReduce systems and arrange certain post-processing operations for outputs collected from all Reducers at the end of application execution, e.g., for implementing iterative applications or applying more variant computing styles to data in addition to the two-phase MapReduce computations.

In this paper, we propose a Flexible Architecture for Cluster Evolution (FACE). FACE is a flexible design architecture intended to provide application developers with system primitives that allow them to develop applications based on specific application requirements. Due to the high flexibility of the system primitives, FACE allows a MapReduce cluster to be designed for various application requirements such as load balancing, location-aware computation, special node selection policies, and customization data processing. FACE allows application developers to: submit input data in files of any size to a cloud computing environment, specify the location of intermediate results to facilitate the processing of data by local Reducers, specify which node should be responsible for running a Mapper to process input data or a Reducer to process intermediate results, and arrange a post-processing operation on outputs from all Reducers at the end of application execution. In addition to processing data with a Mapper or a Reducer, FACE allows application developers to enhance the functionality of applications with other user-defined functions (e.g., by applying certain post-processing operations to outputs collected from Reducers). Above and beyond the system primitives’ support designed to help the development of an application, FACE also provides application developers with node runtime information not only to monitor progress during application execution but also to facilitate the selection of a node to perform a specific function. To optimize performance, FACE implements most components in the C language. However, FACE does allow application developers to implement their applications using other languages because FACE executes user-defined functions and provides runtime node information with language-independent interfaces.

The rest of the paper is organized as follows. In Section 2, we briefly review MapReduce, discuss related works, and highlight the research contributions of this paper. We present the proposed FACE design in Section 3 and describe its implementation in Section 4. In Section 5, we present a performance evaluation of FACE, and Section 6 concludes the paper.

MapReduce [1] is a programming model composed of three programs: a Master, a Mapper, and a Reducer, which can be distributed over nodes in a cluster to work co-operatively on an application. MapReduce usually has only one Master that runs on a node to monitor and control the progress of application execution. However, MapReduce may have many Mappers to process different parts of input data and many Reducers to process different parts of intermediate results produced by the Mappers. MapReduce uses a runtime system (usually a library or a standalone process) to transfer input data and intermediate results between Mappers and Reducers. According to the existing prototypes, MapReduce is often implemented to deploy the Master inside the runtime system to facilitate monitoring and controlling the progress of application execution.

The runtime system of MapReduce handles most issues such as finding a suitable node to run Mappers or Reducers, loading input data to Mappers, shuffling intermediate results among nodes, and collecting results from Reducers when an application is executed. The runtime system typically divides the process procedures into the Map stage and the Reduce stage. In the Map stage, the runtime system reads input data from a Master or a distributed file system such as the Google File System (GFS) and the Hadoop Distributed File System (HDFS). The runtime system divides input data into a series of fixed-size blocks and gives a Mapper one of the blocks. When a Mapper produces intermediate results, the runtime system usually uses a hash function to partition them according to the number of Reducers and then saves them in different files, i.e., intermediate files. The runtime system then enters the Reduce stage and shuffles the intermediate files by downloading them from nodes where Mappers reside to the nodes running Reducers. Then, the runtime system gives Reducers the data in the intermediate files. Finally, the runtime system collects the results the Reducers produce and reports the results to the Master as the final output of the application. To simplify the development of an application, the runtime system usually arranges input data and intermediate results on behalf of Mappers and Reducers at runtime.

A Mapper is one of the user-defined functions and is often programmed to process input data that the runtime system structures in key and value pairs. A Mapper can translate the key and value pairs into a different list of key and value pairs according to application requirements. Then, a Mapper calls an emit function supported by the runtime system in order to generate the list of different key and value pairs as intermediate results. When a Mapper outputs intermediate results, the runtime system usually uses a hash function to classify them according to their keys and saves them in different intermediate files. In previously proposed MapReduce prototypes, a Mapper has no way to specify files or locations for keeping its intermediate results.

Another required user-defined function is a Reducer that processes intermediate results the runtime system automatically arranges and downloads from certain Mappers. A Reducer processes intermediate results in the format of a list of values associated with the same key. After processing the intermediate results, a Reducer calls an emit function to output the results and allows the runtime system to arrange and forward them to the Master as part of the final result. Since the runtime system automatically prepares intermediate results and arranges outputs for Reducers, a Reducer neither requests intermediate results specific Mappers produce nor applies an extra action to the outputs collected from all Reducers.

Word Count [1] is a canonical example used to explain the development of an application in MapReduce. According to Fig. 1
                        , Word Count uses a Mapper to process a list of words as input data and outputs a pair of words and one for each word it finds. In a Reducer, Word Count receives a word and a list of values from the runtime system and then adds each value (usually 1 in the example) to a variable as the count of the word. Finally, Word Count in all Reducers outputs the counts that the runtime system will merge later for each word in the document. However, Word Count neither adds additional functions to the built-in procedures runtime system controls nor customizes the layout of input data or intermediate results that the runtime system has already defined. In other words, Word Count has to process input data structured as a list of words separated by a space as the parameter passed to its Mapper, and intermediate results in the format of a word followed by a list of values as the parameters passed to its Reducer.

MapReduce does not provide flexibility in data partitioning, localization, and processing procedures. Due to the lack of flexibility in procedures of data partitioning, MapReduce partitions and organizes input data in a specific manner before giving it to a Mapper. Due to the lack of flexibility in data localization procedures, MapReduce chooses not only nodes to run Mappers and Reducers but also locations to hold intermediate results. Due to the lack of flexibility data processing procedures, MapReduce runs Mappers and Reducers sequentially, giving application developers no opportunity to customize data processing with additional functions. In the last few years, we have witnessed several extensions [5–13] to MapReduce and its runtime system. Nevertheless, MapReduce still lacks a solution capable of giving application developers the flexibility to develop their applications in clusters as we discussed below.

@&#RELATED WORKS@&#

To implement their runtime systems, current MapReduce prototypes usually follow the principle of Google’s MapReduce, which focuses on distributed and parallel programming issues on behalf of application developers to simplify the development of cloud computing applications. However, this simplicity loses flexibility in data partitioning, localization, and processing. Therefore, we focus our review of existing MapReduce prototypes on the flexibility they offer for operations such as partitioning input data in a specific scheme or size either to facilitate distributed database processing or to alleviate intermediate data skew problems, changing the location of intermediate results at runtime to balance loads, and supplying an extra phase such as a merge phase for the post-processing of outputs collected from Reducers.

Map-Reduce-Merge [5] revises MapReduce by adding a third merge stage after a reduce stage and combines reduced outputs from different jobs. Map-Reduce-Merge does not limit the data processing to two phases originally proposed by Google. Map-Reduce-Merge supports flexibility of data processing by a adding a third merge stage but does not have flexibility data partitioning and data localization procedures because application developers cannot dynamically change the block size of input data, location of intermediate results, and target node for running a Mapper or Reducer at runtime.

Dryad [6] allows application developers to design and execute a data flow graph mapped onto physical resources by a job scheduler at runtime. Dryad requires application developers to implement data processing procedures and connect them with one-way channels so that the runtime system can handle job creation and management, resource management, job monitoring and visualization, scheduling and accounting, etc., on behalf of application developers. Dryad allows application developers to customize data processing procedures at the time of application development without the limitation of MapReduce’s two-phase data processing. However, Dryad does not allow application developers to customize procedures for data partitioning and localization at runtime.

Nephele [7] utilizes Parallelization Contract (PACT), a generalized model of MapReduce, to implement a process of data flow by executing sequential building blocks in parallel and deals with communication, synchronization, and fault tolerance. Instead of requiring application developers to set up communication patterns explicitly (as is in Dryad), Nephele proposes a new programming model centered on key and value pairs. Nephele provides application developers with Input Contracts and Output Contracts to define not only how input data is organized into subsets and processed independently but also how output data is optimized to generate efficient parallel data flows. Nephele has flexibility data partitioning and processing procedures, but it does not allow application developers to customize the data localization procedure at runtime. Nephele works for applications integrated with PACT after it is compiled, but loses some flexibility as well. In contrast, our FACE integrates applications with a portable script language and executes them immediately without pre-compilations.

MapReduce Online [8] aggregates intermediate results in Mappers to reduce communication overheads and return early results to users. MapReduce Online pipelines intermediate results from Mappers to Reducers in order to make Reducers output results as soon as possible. MapReduce Online periodically applies the Reduce function to intermediate results that a Reduce task has received in order to implement a snapshot operation [8]. MapReduce Online allows application developers to generate approximate results whose accuracy depends on the progress score in the runtime system. MapReduce Online is flexible with the data processing procedures used at the end of application execution, but it has no flexibility for data partitioning and localization.

Twister [9] avoids overheads incurred during the frequent loading of static data from disks in each data processing phase of an iterative MapReduce application. Twister keeps invariant data in the distributed memory of nodes so that data can be quickly provided at runtime. Twister extends MapReduce with “broadcast” and “scatter” type data transfers not only to give application developers more flexibility in customizing data localization procedures but also to achieve high performance in iterative applications. However, Twister lacks flexibility when it comes to data partitioning and data processing.

LEEN [10] deals with the intermediate data skew (i.e., the unfair distribution of intermediate results over reducer tasks). Intermediate data skew consumes considerable network bandwidth while transferring data and incurs load unbalances of reduce tasks which degrade application performance. LEEN uses locality-aware and fairness-aware key partitioning to distribute intermediate results over nodes in clusters fairly. LEEN can change the target location of intermediate results to improve performances at runtime, but it does not allow application developers to partition input data or to post-process outputs collected from all Reducers.

Blanas et al. [11] describe the implementation of well-known join strategies in MapReduce and have conducted tests to compare these join techniques. They use MapReduce to process logs rather than a parallel Relational Database Management System (RDBMS) because: (1) loading the sheer amount of data is a challenge for a parallel RDBMS, (2) different schemas in log records are not suitable for a parallel RDBMS, (3) analyzing all records together is not possible for a parallel RDBMS, and (4) implementing fault tolerance is much more difficult for a parallel RDBMS. They investigate and implement the repartition join, the broadcast join, the semi-join, and the pre-split semi-join operations. They demonstrate how various features make their implementation more efficient and show that the selection of an appropriate join strategy is crucial for high performance. They focus on data partitioning procedures for database schemas of different join algorithms but their procedures have no flexibility in data localization and processing.

h-MapReduce [12] alleviates the load balancing problem stemming from the data skew. h-MapReduce is a hierarchical MapReduce capable of identifying a heavy task by a properly defined cost function and dividing the heavy task into child tasks distributed among available workers as a new job. h-MapReduce deals with the challenges (such as deadlocks and inheritance conflicts) of splitting heavy tasks on behalf of application developers. h-MapReduce has the ability to change the data partitioning procedure but cannot support application developers with the flexibility data localization and processing procedures.

Sampling-based partitioning [13] deals with the data skew by sampling a MapReduce job to gather the distribution of keys’ frequencies. Sampling-based partitioning estimates the overall distribution and makes a partition scheme for input data in advance. Sampling-based partitioning is applied to the Map phase of a MapReduce job so that it can provide a load-balanced partition scheme and maintain high performance in the synchronous mode of MapReduce. Although sampling-based partitioning does allow application developers to customize data partitioning procedure, it has no flexibility for data localization and processing procedures at runtime.


                        Fig. 2
                         shows the main differences between a traditional MapReduce system and our FACE design. We highlight the components that application developers can customize. In addition to the customizable components (i.e., a Mapper and a Reducer) supported by a traditional MapReduce, our FACE has four extra customizable procedures to facilitate flexibility in data partitioning, localization, and processing.

We compare related works with our FACE design in Table 1
                        . Several other existing MapReduce prototypes such as Hadoop are not listed because they provide no flexibility in data partitioning, localization, and processing procedures at runtime. As Table 1 shows, we observe that most of the approaches (with the exception of Nephele which support two of the features) support at most one of the three aforementioned features. In contrast to previously proposed designs, we argue that our proposed FACE design is unique because it provides application developers with full flexibility to customize all three features mentioned previously.

Our performance evaluation results of FACE demonstrate its flexibility compared to the native case based on a general MapReduce prototype without flexibility in data partitioning, localization, and processing procedures at runtime. It is worth pointing out that we did not compare FACE to some of the previously proposed MapReduce prototypes shown in Table 1 because: (1) it is impossible to evaluate all potential advantages (such as performance improvements, load balancing, and location-aware computations) brought about by the flexibility criterion; (2) it is not possible to test the previously proposed prototypes because they work on different operating system platforms (e.g., Linux and Windows) and different architectures (e.g., the shared memory architecture in Twister); and (3) it is difficult to port these past prototypes to the Windows environment we used to develop FACE. As a result, we demonstrate the flexibility of FACE in terms of data partitioning, localization, and processing procedures by evaluating their impact on performance.

Flexible Architecture for Cluster Evolution (FACE) is an architecture that places a strong emphasis on flexibility. FACE defines a cloud with physical nodes connected through network devices or Virtual Private Networks (VPN) across the Internet. FACE allows a cloud to be partitioned into a union of different clusters constructed by nodes in order to serve different applications. For flexibility, FACE does not always assign a specific node to a particular Master, Mapper, or Reducer. To optimize resource utilization, FACE allows a node to work for different clusters by taking multiples roles simultaneously (e.g., running as a Master for one cluster but as a Reducer for another cluster).

As shown in Fig. 3
                        , similar to current MapReduce prototypes, FACE has a front-end server (e.g., a web server or service portal) [2] to relay service requests and results between a cloud and users and to hide the implementation details of the cloud from the users. FACE can construct a cloud with many physical nodes but separate them into members of logical clusters responsible for serving different applications. Accordingly, FACE supports the philosophy of grid computing [14] where computation resources are connected across different geographic locations or management domains such as cluster 2 in Fig. 3. If the physical nodes are mobile devices, then small clusters are organized locally to allow FACE to support the infrastructure of mobile cloud computing [15]. In a cloud environment, FACE makes all nodes (including the front-end server) share their runtime information between each other. Accordingly, FACE can choose any node to run as a Master, Mapper, or Reducer on demand. FACE has high scalability and fault tolerance because it allows any node to fail or join a cluster at any time. Once the front-end server chooses a node to serve as a Master (based on CPU utilization or management considerations), FACE delegates the execution of an application to the Master that can further decide which nodes are responsible for running Mappers and Reducers in order to complete the functions of the application.

FACE gives application developers flexibility in customizing data partitioning, localization, and processing procedures for applications at runtime. To this end, FACE not only defines various language-independent and platform-independent system primitives; it also supports an Extensible Master that application developers can extend easily to support additional functions required for customizing data partitioning, localization, and processing procedures according to application requirements (e.g., dynamically changing input data block sizes or transferring certain intermediate results to some nodes in order to balance loads among Mappers or Reducers). FACE allows application developers to use system primitives to deal with various issues when running applications in a cluster. To relieve the burden of additional programming, FACE prepares the Extensible Master as a basic Master in a typical MapReduce system to support most functions workable for applications in a cluster with just Mappers, Reducers, and some procedure code that application developers provide. FACE uses the Extensible Master to do most of what a Master does in other MapReduce prototypes to interact with nodes through system primitives as well. FACE modularizes and implements the Extensible Master using a portable script language to facilitate customizing its data partitioning, localization, and processing procedures by application developers on demand.

In FACE, application developers can use system primitives to distribute over nodes any input data file size from a specific location (e.g., a Master or a distributed file system), specified by a file name or a Uniform Resource Locator (URL). Application developers can use system primitives to obtain node runtime information such as resources’ utilization and application execution progress at runtime so that they can choose a node to run Mappers or Reducers according to specific application requirements. Furthermore, application developers can use system primitives to call any user-defined function to process data at runtime. As mentioned previously, FACE is highly portable because system primitives are both language-independent and platform-independent and because the Extensible Master is written in a portable script language.

Since FACE relies on nodes to perform most jobs in a cluster, components inside a node are considered as the system components and should be explored further. FACE does not depend on how a front-end server is implemented because the front-end server only needs to comply with the interfaces of the node in a cluster. FACE has a node constructed with several components as shown in Fig. 4
                        .

A node in FACE consists of a File Transfer Client, a File Transfer Server, a File Rename Client, a File Rename Server, a Task Executor, a Node Database, a Command Dispatcher, and an Information Broadcaster. A node uses the Operating System (OS) Application Programming Interface (API) [16] to communicate with the underlying OS. A node uses inter-component communication based on subroutine calls to exchange messages among components. A node uses network communication provided with loopback or native network interfaces to communicate with other processes and nodes.

In FACE, a Command Dispatcher accepts and parses commands from network interfaces. The commands may come from local processes or remote nodes. Based on the commands, a Command Dispatcher sends requests to other components (as shown in Fig. 4) and may receive replies from them. A Command Dispatcher may complete a command immediately by requesting the services of a single component or multiple components. Depending on the command and its parameters, a Command Dispatcher, may hand over data to a File Transfer Server, requests a File Rename Server to rename it on disks, or uses a Task Executor to execute a user-defined function to handle the file. To complete a command, a Command Dispatcher sometimes needs to make a component work cooperatively with another component in a remote node. For example, a Command Dispatcher may ask a File Transfer Client to communicate with a File Transfer Server in a remote node (through a Command Dispatcher at the remote node) in order to download or upload a file. Furthermore, a Command Dispatcher can receive commands from an Information Broadcaster to save a node’s information and intermediate file summary in a Node Database.

A Task Executor can run a user-defined function such as a Mapper or Reducer. At runtime, a Task Executor can monitor the files (such as intermediate files or result files) the user-defined function produces. A Task Executor registers the files the user-defined function produced with an Information Broadcaster so that the Information Broadcaster can broadcast the summary of the files such as name, size, and modification time to the network later.

In addition to monitoring updates in files registered by a Task Executor, an Information Broadcaster periodically queries the OS for node information, e.g. CPU utilization, and broadcasts information such as node information and intermediate file summary to networks so that a Master, a front-end server, and Command Dispatchers in all nodes can hear and utilize the information on demand.

In FACE, system primitives are a suite of APIs [16] to interact with nodes in a cluster. System primitives can be used to distribute node information and intermediate file summary over networks and manipulate files, e.g., uploading, downloading, renaming, deleting, and registering files. In addition, system primitives can also execute user-defined functions such as a Mapper, a Reducer, or other functions. To achieve high flexibility, system primitives can assign a node as an agent to perform certain operations on another node. Application developers can use system primitives to develop their applications according to specific application requirements and customize data partitioning, localization, and processing procedures.

In the current prototype, FACE uses system primitives that application developers can use as the commands defined in Table 2
                        , where a command with an AG prefix stands for the receiver as an agent of the command. The system primitives form an interface like a platform-independent and language-independent protocol used to communicate with nodes in a cluster.

FACE has an Extensible Master to facilitate application development according to various application requirements. FACE uses the Extensible Master to handle progress in application execution through system primitives at runtime. While the existing MapReduce prototypes implement an unchangeable Master inside the runtime system, FACE extracts a Master from the runtime system (i.e., node components in Fig. 4) and makes it extensible to application developers.

As shown in Fig. 5
                        , the Extensible Master has an Information Listener, a Progress Sequencer, a Default Dispatcher, and five extensible modules namely, an Input Module, a Map Module, a Dispatch Module, a Reduce Module, and a Result Module. The Extensible Master allows application developers to implement functions of the modules with system primitives. The Extensible Master uses an Information Listener to collect and organize node information and intermediate file summary broadcasted by nodes into node runtime information. The Extensible Master uses a Progress Sequencer to issue requests to the five extensible modules and supplies the modules with node runtime information. The Extensible Master allows an application developer to optionally and selectively implement the modules based on needs. At runtime, the Extensible Master passes the control to the five modules on demand to facilitate the data partitioning, localization, and processing operations which application developers can customize.

As the flow chart in Fig. 6
                         shows, the Progress Sequencer (as the kernel of the Extensible Master) is responsible for controlling the progress of application execution based on the reception and update of node runtime information from an Information Listener. If application developers do not implement a module, the Progress Sequencer does not pass control to it. However, the Progress Sequencer always has a Default Dispatcher to implement a default node selection policy for choosing a node to hold input files and run a Mapper or Reducer at runtime, in case application developers do not implement a Dispatcher Module.

We implemented the FACE prototype on Windows Server 2003 because Windows is a popular OS with robust support for application development tools and Graphic User Interfaces (GUIs). We used the C language to implement a node with the support of multithreads and its components in different subroutines. In the node, we create a TCP server that uses a TCP socket to accept commands from networks and passes the commands to the Command Dispatcher to verify their formats and parameters. We created a thread to run the corresponding subroutine in order to serve a command.

To facilitate the implementation of system primitives in a node, we implemented a different subroutine for serving each command and its parameters defined in Table 2. We define the system primitives in Table 2 as commands encapsulated in messages from networks so that they are portable across different platforms (i.e., the platform-independent feature in FACE) no matter what language is used to implement node components in FACE. We implement the Task Executor to call the corresponding subroutine directly if the command requests for the execution of a built-in, user-defined function (e.g., a Mapper implemented in the C language), but use the system shell to run a user-defined function implemented in other languages (either interpreted by the corresponding language interpreter or compiled into a native executable file executed by the Operating System (OS)). With such a design philosophy, we can easily make FACE support user-defined functions implemented in various languages (such as Java or Python) without modifying any component of a node (i.e., the language-independent feature of FACE).

We implemented the Information Broadcaster to obtain node information such as CPU utilization, available memory and disk space, and network and disk activity through APIs of the Windows Management Instrumentation (WMI) [17], the infrastructure for management data and operations on Windows-based operating systems. In addition, we implemented the Information Broadcaster to accept intermediate file registers, monitor updates in the files, and broadcast node information and intermediate file summary to networks through OS APIs.

The Extensible Master is implemented in PHP because it is a portable script language interpreted by its engine to support various operations such as message display, file manipulation, and network access. In addition, the PHP engine supports both Common Gateway Interface (CGI) in a Web server [2,18] and Command-Line Interface (CLI) in a shell [19] to interpret a PHP script thereby enabling the Extensible Master to interact with a Web server and the OS and use system primitives given in Table 2 to control nodes through network APIs.

We implement the Extensible Master to check at runtime whether application developers already prepare input, map, dispatch, reduce, and result functions in the PHP language as shown in Fig. 7
                        , which are entry points corresponding to the five user-defined modules in Fig. 5. If any of the entry points exists, we call it at the Progress Sequencer in the Extensible Master on demand according to the flow chart in Fig. 6, and supply it with node runtime information (such as CPU utilization and intermediate file names and sizes) broadcasted by each node. Although application developers can do any operation in the five modules and interact with nodes through system primitives, as far as security issues are concerned [20], FACE should be open to the application developers who are employees or experts authorized by administrators rather than non-authorized users or application developers.


                        Fig. 8
                        , Experimental Testbed Configuration and Performance Metrics Used.


                        Fig. 8 shows our testbed cluster with 8 identical PCs as nodes for running Mappers and Reducers, and a notebook as the Extensible Master that can use computational resources of the 8 PCs and offer them the input files of an application. In the following experiments, we first measured the native performances of three canonical applications, i.e., Word Count [1], Radix Sort [21], and Pi Approximation [22], as the baseline for comparison with other cases. Next, we used three experimental configurations to demonstrate flexibility benefits of FACE, i.e., examples corresponding data partitioning, localization, and processing procedures that application developers can customize for their applications in FACE. We briefly introduce the three applications before presenting the experimental results obtained with them.

Word Count [1] counts the number of words in a document. In a Mapper, Word Count parses and searches the input file for words separated by space characters. For each word, Word Count emits a string “word,1” to different intermediate files according to the length of the word. In a Reducer, Word Count parses intermediate files an Extensible Master downloads from nodes at where Mappers run. Finally, Word Count collects results from all Reducers and outputs them to a file as the final result. Word Count completes most computations in Mappers and consumes considerable network bandwidth to transport input files and intermediate files.

Radix Sort [21] sorts integers by grouping the integers according to individual digits sharing the same significant position and values. In a Mapper, Radix Sort parses and searches the input file for integers separated by space characters, and then outputs the integers to different intermediate files according to the digit of the integer. In a Reducer, Radix Sort processes the integers in intermediate files by comparing their individual digits that share the same significant position. In our experiments, Radix Sort uses the Least Significant Digit (LSD) algorithm [21] that processes the least significant digit of integers and moves towards the most significant digit. Radix Sort requires Reducers to perform many computations and consumes similar network bandwidth to transport input data, intermediate results, and result files.

Pi Approximation calculates the value of pi with a Monte Carlo experiment [22] composed of a dartboard and darts emulated with a series of two-dimension coordinate points. In a Mapper, Pi Approximation reads the input file that indicates how many coordinate points should be generated randomly, and then outputs them to intermediate files. In a Reducer, Pi Approximation reads the intermediate files and counts the coordinate points that hit the dartboard. Pi Approximation consumes little bandwidth to transport input files and result files, requires few computations in Mappers and Reducers, and generates a huge amount of intermediate results.

We implemented a traditional MapReduce system to measure the performance of the three applications in our testbed, and we refer to the performances as native performances in this paper. We prepared 210 identical input files for a test and activated a different number of nodes in each test in order to evaluate the scalability of the applications. Because Radix Sort and Pi Approximation require considerably more time than Word Count, we gave Word Count 64MB input files but gave Radix Sort and Pi Approximation 16MB input files. Accordingly, we gave Word Count a workload of 13,440MB (i.e. 210∗64MB) while Radix Sort and Pi Approximation received a workload of 3360MB (i.e. 210∗16MB).

In Fig. 9
                        , we observe that the performance is improved according to the increase of nodes and the scalability is limited to the frequency of node runtime information periodically reported from nodes to an Extensible Master (5s per announcement in the current configuration). To deal with scalability issues, we can use intuitional solutions such as (1) increasing the frequency of node runtime information reported from nodes, and (2) partitioning and distributing input data over nodes in multiple clusters controlled by multiple Extensible Masters to work cooperatively for an application. Because this paper mainly addresses the flexibility benefits that FACE can bring to applications, we do not focus on native performances of the three applications in a cluster. Instead, we have demonstrated in the following subsections what flexibility FACE can give to application developers and the performance gains FACE can give to applications due to its flexibility.

In this experiment, we partitioned input files into small files in order to demonstrate the benefit of flexibility FACE provides; unlike FACE, the existing MapReduce prototypes have a default input file size for applications. We partitioned all input files (i.e., 210 files) hierarchically into different small files without changing the total quantity of input data. Specifically, we divided a file into two files (each half the size of the original file), and then applied the division operation to one of the two files while keeping the other file intact. We repeated the division operations until we reached the configured minimal input file size.

We used input files of 32MB, 16MB, 8MB, 4MB, 2MB, and 1MB to test Word Count with 13,440MB of input data as the total workload in the native performance test. If we configured 8MB as the minimal input file size to test Word Count, for example, it would only process input files having 32MB, 16MB, and 8MB in order. We used input files of 8MB, 4MB, 2MB, and 1MB to feed Radix Sort and Pi Approximation with 3360MB of input data as the total workload in the native performance test because their original files are 16MB.

In Fig. 10
                        , we observe that (1) partitioning input files into smaller files allows applications to perform better than their native performances in most cases; (2) because Word Count requires Mappers to handle many computations, it always achieves performance improvements from data partitioning by balancing workloads among nodes; (3) using small input files (e.g., the case of 1MB minimal input file) incurs many connection establishment overheads that reduce performance gains in Word Count and Radix Sort; and (4) if the dominant overhead is in Reducers, such as when they perform many computations in Radix Sort or receive many intermediate results in Pi Approximation, coarsely partitioning input files (e.g., the case of 8MB minimal input file) may further degrade performances due to input data skew [23] because of the idling of certain nodes.


                        Fig. 11
                         shows the average CPU utilization of the three applications, and the results confirm the performance impact of hierarchical input file sizes: (1) the curve of CPU utilization lasts longer in the timeline if the case performs poorly; (2) using hierarchical input file sizes quickly improves the CPU utilization in the early part of the application execution; and (3) Radix Sort does not require much CPU utilization at the end but takes more than 930s for execution time in Fig. 10 in all cases because of the remaining performance degradation, which is due to the overhead of results waiting to be delivered from the nodes. According to Figs. 10 and 11, we demonstrate that FACE can give application developers an opportunity to improve application performances by using data partitioning techniques such as implementing hierarchical input file sizes as shown in this experiment.

In the experiment, we selected different locations to save intermediate files in order to demonstrate the benefit of flexibility FACE provides in contrast to MapReduce prototypes that use a default disk in each node to cache intermediate files. We designed an Extensible Master to save the intermediate files of a node on its local virtual disk emulated by RAM so that a Mapper can quickly output intermediate files and a Reducer chosen to run on the same node can quickly read them without the delay of using a real disk. Given that the RAM disk small in size (but sufficient to verify the benefit of flexibility FACE provides), we: (1) moved intermediate files to a local disk at runtime when the RAM disk was full, and (2) downloaded intermediate files from a remote node to a local disk instead of a RAM disk when a Reducer was responsible for the processing intermediate files Mappers produced at the remote node.


                        Fig. 12
                         shows the experimental native and FACE results. We observe that (1) outputting intermediate results to a RAM disk can result in a 22.23% (i.e. (1016.8–791.8)/1016.8) Word Count performance improvement and a 2.90% (i.e. (938.2–911)/938.2) Radix Sort performance improvement in comparison to the native performance, (2) the performance of Pi Approximation improved very little because many intermediate results the Mappers produced quickly overwhelmed the small RAM disk size, and (3) increasing the RAM disk size can result in further improvements in applications’ performance in the experiments because RAM is inexpensive today.

According to Fig. 13
                        , where each line expresses the CPU utilization of a node, we note that Word Count with RAM disks can have more than 40% stable CPU utilization in all nodes but its native performance cannot. These results confirm that using RAM disks resulted in the significant performance improvement in Word Count in Fig. 12. Radix Sort and Pi Approximation require more CPU resources in Reducers and may have strugglers due to data skew [23], so we observe that certain nodes in Fig. 13 are idle near the end of application execution thereby negating certain performance gains that RAM disks provide. By comparing the graphs on the left and right sides of Fig. 13, we confirm that using RAM disks indeed reduces the time of waiting disk I/O completion and greatly increases the CPU utilization.

In this experiment, we applied compression functions to Reducers’ output files in order to demonstrate the benefit of flexibility FACE provides compared to MapReduce prototypes that do not allow application developers to customize data processing. We designed an Extensible Master to execute well-known compression functions, such as BZ (short for BZIP2) [24] and GZ (short for GZIP) [25], on results outputted from Reducers in all nodes before collecting them, so network bandwidth consumption between nodes and the Extensible Master can be reduced. Keeping network bandwidth usage low is especially important when nodes are distributed over several geographic locations and are connected to each other through limited bandwidth connections. Although our environment connected nodes to the Extensible Master through Gigabit Ethernet instead of limited bandwidth media, our results show promise in real world settings in which a mix of heterogeneous communication media with different network bandwidth capacities may be used.

Based on the performance results shown in Fig. 14
                         and the final output sizes in Fig. 15
                        , we observe that data compression may improve performance even though the tests are executed in a Gigabit Ethernet environment. We note that both BZ and GZ can improve the performance of Word Count and compress results from 78.8KB to less than 26.3KB to reach about 66.63% compression efficiency (i.e., (78.8–26.3)/78.8). However, we observe that the execution time of Word Count is not significantly reduced because its small amount of data result does not exhaust the bandwidth of a Gigabit Ethernet environment. In Figs. 14 and 15, we observe that GZ improves in performance by 17.58% (i.e. (938.2–773.3)/938.2) with Radix Sort, but BZ does not because BZ cannot amortize the high processing time with its performance gains; nevertheless, it is important to note that both of them can save more than 73.26% (i.e., (3356.4–897.4)/3356.4) bandwidth. We note that data compression does not improve in performance in Pi Approximation because the data result is fairly small, although more than 52.50% (i.e. (64–30.4)/64) compression efficiency can be achieved. The experimental results show strong potential for improved performance in the real world where nodes are distributed over several geographic locations and connected to each other through lower bandwidth connections than with the Gigabit Ethernet tested here.

@&#CONCLUSION@&#

In this paper, an architecture called Flexible Architecture for Cluster Evolution (FACE) is proposed to give application developers flexibility in customizing the ways in which data is partitioned, localized, and processed based on specific application requirements. FACE not only defines various language-independent and platform-independent system primitives but also includes an Extensible Master that application developers can extend. For high performance, FACE implements its main components in the C language. In addition, FACE allows application developers to implement their applications with various languages through user-defined functions and provides node runtime information with language-independent interfaces.

FACE allows application developers to distribute over nodes input files having any size of input data from any location. In the test using hierarchical input file sizes, FACE can improve performances for all cases in Word Count and for most cases in Radix Sort and Pi Approximation. FACE allows application developers not only to utilize node runtime information such as resource utilization and application execution progress at runtime but also to choose a node to run Mappers or Reducers according to application requirements. In the case of RAM disks tests, FACE yields a 22.23% and 2.9% performance improvement with Word Count and Radix Sort respectively. FACE also allows application developers to invoke any user-defined function to process their application data at runtime. The results obtained with BZIP2 and GZIP functions demonstrate that FACE can save bandwidth by producing compression efficiencies of 66.63%, 73.26%, and 52.5% for Word Count, Radix Sort, and Pi Approximation applications respectively. Although it takes time to compress data to save network bandwidth FACE still yields performance improvements for Word Count and Radix Sort applications. The experimental results obtained with three widely different applications demonstrate both FACE’s flexibility, which is not supported in other current MapReduce prototypes and its potential in cluster design architectures.

@&#ACKNOWLEDGEMENTS@&#

We gratefully acknowledge the National Science Council of Taiwan for its support of this project under Grant number NSC 102-2221-E-262-014. We thank Lunghwa University of Science and Technology for kindly providing us with the hardware equipment used to implement the prototype described in this work. We also thank the anonymous reviewers for their useful comments and Manu Malek for his kind advice and support throughout the preparation of the final revised version of this paper.

@&#REFERENCES@&#

