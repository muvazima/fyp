@&#MAIN-TITLE@&#A Machine-to-Machine protocol benchmark for eHealth applications – Use case: Respiratory rehabilitation

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           The software tool presented allows the deployment of different benchmarking tests for M2M protocols.


                        
                        
                           
                           The most relevant M2M protocols were evaluated considering different specific performance metrics.


                        
                        
                           
                           Benchmark results allowed to select the most suitable M2M protocol a clinical case of use: respiratory rehabilitation.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Respiratory rehabilitation

IoT

M2M

Publish/subscribe

eHealth

Benchmark

@&#ABSTRACT@&#


               
               
                  Background
                  M2M (Machine-to-Machine) communications represent one of the main pillars of the new paradigm of the Internet of Things (IoT), and is making possible new opportunities for the eHealth business. Nevertheless, the large number of M2M protocols currently available hinders the election of a suitable solution that satisfies the requirements that can demand eHealth applications.
               
               
                  Objectives
                  In the first place, to develop a tool that provides a benchmarking analysis in order to objectively select among the most relevant M2M protocols for eHealth solutions. In the second place, to validate the tool with a particular use case: the respiratory rehabilitation.
               
               
                  Methods
                  A software tool, called Distributed Computing Framework (DFC), has been designed and developed to execute the benchmarking tests and facilitate the deployment in environments with a large number of machines, with independence of the protocol and performance metrics selected.
               
               
                  Results
                  DDS, MQTT, CoAP, JMS, AMQP and XMPP protocols were evaluated considering different specific performance metrics, including CPU usage, memory usage, bandwidth consumption, latency and jitter. The results obtained allowed to validate a case of use: respiratory rehabilitation of chronic obstructive pulmonary disease (COPD) patients in two scenarios with different types of requirement: Home-Based and Ambulatory.
               
               
                  Conclusions
                  The results of the benchmark comparison can guide eHealth developers in the choice of M2M technologies. In this regard, the framework presented is a simple and powerful tool for the deployment of benchmark tests under specific environments and conditions.
               
            

@&#INTRODUCTION@&#

Advances in Information and Communication Technologies (ICT) have become an indispensable tool in most sectors of modern society, including healthcare [1]. These technological progresses are transforming the traditional healthcare landscape through a better optimization of the available resources and a more efficient administrative and clinical management. In response to these new opportunities the concept of eHealth emerges [2], which is presented as an alternative to traditional telemedicine systems where the patient was considered as a passive element in the healthcare practice [3]. In this sense, the eHealth concept comprises not only the care of the patient, but also includes the citizen her/himself, who is more responsible for and involved in her/his treatment and healthcare [4]. Furthermore, the concept of citizen empowerment is part of this new health ecosystem, extending the care process, which has traditionally been located exclusively in hospital facilities [5], toward a scenario of ubiquitous computing [6].

In this context, the paradigm of IoT (Internet of Things) [7,8], “promises to create a world where all the objects around us are connected to the Internet and communicate with each other with minimum human intervention” (sic). IoT is also defined as the interconnection of communications devices using a variety of devices and communication Machine to Machine (M2M) protocols [9]. However, despite some remarkable interoperability efforts [10], there is currently a lack of standardization [11], although some recent initiatives have already headed in that direction [12]. In the meantime, given the extensive fragmentation of M2M technologies for IoT purposes [13], different papers have addressed comparative analysis from several perspectives [14] – architectures, standards, media access interfaces –, whilst others are focused on light communication protocols for portable and embedded devices [15–17]. In [18], a comparison between the generic architectural models of request/reply and publish/subscribe is presented, and it is shown the optimal performance of the latter. Besides, under the premise of providing the best quality of service (QoS), the M2M publisher/subscriber model stands out showing significant advantages in terms of latency [19] and energy [20]. This way, different communication protocols have emerged in the last years that implement the publish/subscribe pattern and that are specifically designed for IoT [21]. However, most of these comparative studies are preliminary [14], focusing in most cases in only two specific protocols that are tested with few hosts, which is far away from a real massive device-connected scenario like the IoT [22]. To this end, recent studies [5,14,23] highlight the current need of more extensive benchmarking and performance analysis from realistic working conditions. In particular, and to the best of the authors’ knowledge, there are no studies that address the specific requirements demanded by eHealth environments.

In order to address the above needs, the present paper describes a benchmarking of a variety of M2M protocols oriented to IoT communication based on the publish/subscribe pattern. Taking into account the level of thoroughness that implies comparing different technologies not interoperable with each other in the same conditions, as well as the existing variety of features and configurations, it was necessary to implement in the first place a software tool, referred to as Distributed Computing Framework (DCF), in order to implement the different benchmarking tests. The flexibility, modular design and ease of use of DCF allow to obtain in real-time the most relevant performance metrics whilst hiding to the users the complexity of the different communication technologies and topologies evaluated. Thus, the results obtained can serve as a general guide to help developers of eHealth solutions for the selection of M2M protocols based on the publish/subscribe model, according to their particular needs. For this purpose, it is presented as case of use an eHealth platform aimed at the supervised monitoring of a training program for respiratory rehabilitation (RR) of COPD patients (from now on called e-RR platform), which has been developed in the context of several a R&D project being carried out by the authors [24]. The platform encompasses many of the M2M challenges detailed as it requires the real-time communication of the data gathered by sensing devices attached to patients when performing their training routines indoor and in mobility. In both training cases, ambulatory and home-based, the supervised RR program is similar, and mainly consists in the development of moderate-intensity aerobic physical activity (PA) (SPWT, 6MWT test [25,26]) that can be assessed through the monitoring of heart rate and blood oxygen saturation [27], as well as caloric consumption [28]. Finally, it is also relevant to add contextual information for home-based training: for instance, patient's location is vital regarding the occurrence of episodes of panic attacks and dyspnea, which are common in COPD [29].

The paper is divided into the following sections: Section 2.1 describes the framework developed for the implementation of the benchmarking, Section 2.2 presents an analysis of the technologies chosen for the development of the performance testing and a methodological description of the benchmarking itself is presented in Section 2.3. Then, in Section 3.1 the use case of the e-RR platform is presented for the two scenarios considered, which demand different technological needs. In Section 3.2, the results of the benchmarking are presented for the most important performance metrics. Considering the results obtained and the requirements for scenarios of the case of use presented, Section 3.3 chooses the M2M protocols more suitable for each situation and the data model is presented hierarchically following good practices in the use of the communication technologies used. Finally, the discussion and conclusions analyze the results obtained by breaking down each protocol in terms of its strengths and weaknesses.

@&#MATERIAL AND METHODS@&#

The rapid emergence of new technologies and the need of performance evaluation require general purpose benchmarking tools characterized by a modular, scalable and portable architecture. Thus, the present landscape motivates the development of the Distributed Computing Framework (DFC), a tool which allows evaluating different communication technologies in terms of performance, considering specific environments and conditions. The DCF has been designed in order to ease the measurement of performance metrics, abstracting the complexity of the benchmark deployment. It is mainly designed for controlled scenarios although its flexibility allows adapting to other specific conditions with a large number of machines, which presents a significant advantage with respect to other approaches.

The DCF is based on the master–slave communication architecture, where the slaves are considered the machines that execute benchmark tests under the direction of the master. In this sense, the communication between master and slaves follows the publish/subscribe pattern and more particularly the DDS (Data Distribution Service) specification. The slaves exchange data using different communication technologies and generate statistics based on performance metrics. After the execution of benchmark tests, all slaves send their results to a central database for future processing. The communication technologies and performance metrics studied are scalable and configurable by the user by the insertion of specific modules in the framework.

In particular, the DFC defines two applications for master and slave. Both applications are designed by means of a modular approach with different levels of abstraction, from specific communication modules in the lower layers to user interaction modules in the upper layers. Interfaces are defined for each level, providing a robust set of features and services to the surrounding levels. Fig. 1
                         shows the multilayered architecture for master and slave applications, and is explained below.

Model-view-controller (MVC), which is placed in the middle of the layered architecture in Fig. 1, is a software architectural pattern that supports the development of the two applications. MVC is usually used to provide standardized view for web applications [30], separating the application from the presentation layer (the interface with the user). This feature is used in the DFC, where the controller is responsible for responding to events (usually triggered by the user) and sending commands to the model for data access. The View gathers these data of the Model and formats it appropriately for the user. With respect to the Model, two additional sublevels of different responsibility are considered: the ORM (Object-Relational Mapping) and the OTM (Object-Topic Mapping).

ORM is a technique for converting data between an object-relational database and an object-oriented programming language [31]. This allows the developer to focus on the object model regardless the database structure or the SQL (Structured Query Language) language used. On the other hand, the OTM is a technique introduced in this paper that is inspired by ORM, but based on the publish/subscribe communication paradigm instead of an object-relational database. Consequently, in OTM the mapping is defined between the object-oriented programming language and the model-data communication protocol. The basic concepts used in databases/ORM were translated to the OTM technique in a context of publish/subscribe model, as revealed in Table I
                        .

The DFC was implemented using Java programming language and the OTM technique was applied to the DDS middleware to reduce development complexity. The OTM works internally with special objects called TopicBeans, which they are the result of the mapping of DDS topics with Java objects. TopicBeans consist in a set of getters and setters methods for obtaining or assigning the variables defined in the DDS topic and additionally inherit others methods that are related to the publication, subscription, reading, writing and filtering data.

Among the advantages of the OTM technique with respect to the DDS API (Application Programming Interface) we include the ease of use, readability and avoiding the use of IDL (Interface Description Language), since data structure of topics is built dynamically using Dynamic Topic Types for DDS [32]. Also, the OTM provides a means of abstraction to develop with independence of the DDS vendor.

Finally, the user interacts with the DFC with a console (Fig. 1, top left), using a JSON (JavaScript Object Notation) configuration file in the master application. The Console is an interactive command-line interface which allows the user to communicate with the slaves, while the configuration file includes the specific features of the benchmark tests that aims to run. On the other hand, Technology and Benchmark modules (see Fig. 1, top right) are integrated at the higher levels of the slave application. These two modules are responsible of expanding DFCs functionalities by including submodules related to different technological solutions and performance metrics.

Despite the flexibility of the framework to support different technological solutions based on the master–slave communication architecture, the authors have considered a set of requirements in order to select M2M protocols to be evaluated. Protocols should meet the following conditions:
                           
                              1.
                              To be based on standards and open technologies.

Open source implementations.

Highly tested and used.

Integration in e-health applications.

Interoperability between different vendors.

Active community.

The tests were performed in an isolated and controlled environment synchronizing the clocks on the machines with the NTP (Network Time Protocol) protocol. The hardware used consisted of 41 slave machines (20 publishers and 20 subscribers) and 1 master machine (that acted as broker or server for centralized protocols). In order to facilitate the deployment and the benchmarking setup, 2vCPU virtual machines with 1 GB of RAM and a Gigabit Ethernet interface were used. According to preliminary experiments which have been carried out, there were no significant differences between physical or virtual machines for such communication tests. On the other hand, the operating system was Ubuntu Server, while the physical machines were connected with a star LAN topology.

The communication link among all publisher and subscriber machines is an abstract data container (topic or message, although it depends on the communication technology), where a sample is defined as the minimum portion of information with the data structure defined by the container. The fields of the data structure are based on the case of use considered, being data per second generated randomly for each publisher.

This way, each sample sent by the publisher is redirected at each of the subscribers. In centralized protocols, publishers send samples to a broker, which is responsible of redirecting to each of the subscribers. Nevertheless, in decentralized protocols like DDS, publishers send samples directly to subscribers.

Each protocol was configured with the default parameters based on the publish/subscribe communication paradigm. Delivery is not acknowledged by the subscriber (aka “fire and forget”), so the publisher or broker does not guarantee delivery of samples to the subscribers. Table III
                         shows the APIs client libraries and the broker or server used (installed in the master machine).

The DCF presented in Section II-B was used to perform a performance analysis of the M2M protocols previously selected. Considering the tests provided by other authors [36], and more particularly those focused on M2M protocols [37–41], performance metrics that have been selected for this work has been CPU usage, memory usage, bandwidth consumption, latency and jitter. SIGAR API [42] was used to measure CPU usage and memory usage while jnetpcap library [43] was used to measure bandwidth consumption. Latency was calculated by measuring the communication time delay between the publisher and the subscriber for each sample. In the results, all values were taken from the weighted arithmetic mean of the measures from all machines. Finally, experiments lasted 30s for CPU, memory and bandwidth tests and 5s for latency tests because of the huge number of samples generated for sampling rates greater than 10 sample/second.

@&#RESULTS@&#

Despite the strong evidence of the clinical benefits of ambulatory RR [27], it is a clinical practice with relatively low adherence [44]. This drawback has promoted the research and adoption of the domiciliary RR as a supplementary alternative in order to minimize the drop rate [45]. The development of exercises in the domiciliary RR requires the remote supervision by medical staff, paving the way for the deployment of an eHealth platform, called e-RR and shown in Fig. 2
                        , which is currently being developed by the authors. The e-RR platform must coordinate patient-specific information to be exchanged in real-time between the two scenarios defined – ambulatory and domiciliary –, and represents an excellent opportunity to evaluate how M2M technologies behave within them.


                        Fig. 2 shows the two main scenarios considered for the use case studied – ambulatory and home-based –, which demand specific challenges of the platform regarding the operating conditions of hospital IT infrastructures and a context of mobility, respectively (detailed in the second column of Table IV
                        ). Other common requirements for both scenarios are the confidentiality and security of the real-time data exchanged. In any case, a wide area network (WAN) network (e.g. Internet) is only needed for the system operation to gather the data from the smart electromedical devices that allow the remote monitoring of RR. Due to the unavailability of real data, publishers have randomly generated data in the experiments considering the variables defined in Table IV. In this sense, the simulated communication tests for this work fit a real scenario due to the trivial amount of computational work that implies the random generation of such data.

In case the panic button is pressed, the patient may require to be moved to the hospital, thus aborting the normal course of the RR program and triggering an ER protocol that may lead to the hospitalization of the patient until her/his definitive diagnosis and specific treatment [46]. This protocol falls out from the scope of this paper, and for a detailed explanation of the alarm system needed for dealing with this and other events (e.g. devices failure), the authors refer to [47].

CPU percentage usage of the subscribers is shown in Fig. 3
                        , and is obtained regarding the number of samples per second performed by publishers. The blue line represents the amount of memory consumed by subscribers when the application is inactive. The results of the publishers are omitted since those related to the subscribers have been considered more interesting, where the increased CPU usage is more pronounced when the sampling rate increases.

Memory usage of the subscribers is shown in Fig. 4
                        , and is measured in kB considering the number of samples per second performed by publishers. This test shows no significant differences between publishers and subscribers.

The (upload and download) traffic sent and received by all machines based on the number of samples per second is shown in Figs. 5 and 6
                        
                         respectively, being the blue line the traffic generated when the application is inactive, which corresponds to the broadcast packets that DDS uses for the automatic discovering of information.

Latency and jitter of the subscribers is shown in Figs. 7 and 8
                        
                        , based on the number of samples per second.

Based on the results obtained from the benchmark and considering the set of requirements defined in Section II-D, DDS was the protocol selected for the Ambulatory scenario in the case of use considered. In particular, DDS presents advantages with respect to the low latency and jitter (see Figs. 7 and 8), high scalability and reliability, automatic discovery of information and rich set of QoS policies that enhance the predictability of data delivery. This decision is also supported by the OpenICE initiative [48], which focuses on the DDS middleware for the communication between standards-based medical devices. On the other hand, MQTT protocol was selected for the Home-Based scenario, because of its lightweight design, simplicity and low power usage. Thus, this protocol is ideal for resource-constrained sensors and devices like the wearables used for monitoring users in this scenario (see Table II).

One of the fundamental requirements for both scenarios is related to information and communications security. In this sense, security in DDS is currently being standardized by the OMG [49] so as to provide access control services through topic or topic-specific fields, as well as read/write fields for different DDS entities [50]. By contrast, MQTT does not provide a privacy policy in accordance with the requirements of IoT, because the standard and existing implementations provide support only for authentication and simple authorization policies considering just the subscription of clients to message topics [51]. Nevertheless, these security mechanisms allow sufficient granularity to provide an acceptable level of privacy and authorization for the access to information in the use case. With respect to confidentiality, most DDS and MQTT implementations support TLS (Transport Layer Security) cryptographic protocol for data encryption.

Considering the information handled in Table IV for this use case, data types have been modeled using a hierarchical design based on topics, as recommended by the MQTT standard [52]. Fig. 9
                         presents the hierarchical data model, where the root node contains all patients by ID. Data types handled for each scenario were grouped in function of the sensors that monitor them, facilitating granular or broad subscriptions.

In MQTT, topics are also defined as a hierarchy, using a slash (/) as a separator to navigate the hierarchy from the root down to the leafs. Also, the use of wildcards allows the subscriptions to several topics at the same time. For example, to access the GPS location of all patients, clients can subscribe to “RR program/+/Home-Based/GPS” topic, using the (+) operator to include any patient. In DDS each tree leaf is considered a topic, while the parent nodes can be modeled like contentfilteredtopics, multitopics or both simultaneously. For example, in order to access the GPS location and Time Log of the patient #001, clients can use a multitopic that include GPS and Time Log topics, and a contentfilteredtopic to filter by patient ID. Data mapping between DDS and MQTT topics is one of the future developments of this work and falls out from the scope of this paper.

As recommended in the MQTT and DDS specifications, clients can discover information by subscribing to the topic $SYS in MQTT or by subscribing to the DCPSTopic topic in DDS.

With respect to implementation, the application has been developed with the API (Application Programming Interface) provided by the RTI Connext DDS Professional [53] software using the OTM technique discussed in Section II-A. Thus, the publication and subscription procedures in the programming code were similar in DDS and MQTT.

@&#DISCUSSION@&#

In a simple communication using publish/subscribe paradigm between only two machines, performance is almost similar for all M2M protocols [54]. However, an increase in the number of machines can reduce general performance, mainly in those protocols that use centralized brokers, adding an additional overhead. Some brokers may be unable to handle the load of incoming messages when traffic is very intense and the broker may fail. At this point, machines lost communication because in a centralized architecture, the broker represents the single point of failure. In order to evaluate the behavior of M2M protocols in terms of performance, a scenario with many more machines seems to fit the specific requirements of IoT environment.

In all benchmarking tests, performance metrics from XMPP protocol for sampling rates of 100 and 1000 samples per second could not be obtained, as well as for COAP protocol regarding a publication rate of 1000 samples per second. In both cases, brokers failed because of the high frequency of data sent by publishers.

CPU usage is one of the least features considered in benchmarking tests of M2M protocols. However, this metric has a high impact on the energy consumption of highly constrained devices like those used by IoT [55]. The results show that the CPU usage is relatively low and similar in the entire group of considered protocols, regardless the number of publishers or subscribers. Nevertheless, there are some significant differences for centralized or decentralized protocols. In centralized protocols, as the publication ratio rises, the CPU consumption has a lineal increase whereas in DDS (decentralized protocol) the increase is exponential. This issue is related to the sample sending pattern employed: in DDS, the publishers themselves are the ones who directly send the sample to the subscribers. On the contrary, in centralized protocols, the publisher only sends the sample to the broker. This behavior can also be noticed in the subscribers even more significantly. Particularly, the XMPP protocol has a high CPU usage in comparison with the rest of centralized protocols for sampling rates of 1 and 10 samples per second. This may occur since the XMPP is a XML-based protocol (Extensible Markup XML), so the Parsing XML adds an extra processing overload compared with the other protocols in which the codification is binary.

On the other hand, as far as memory is concerned, DDS technology has a higher consumption than the rest of protocols since its mechanism of automatic discovery of information requires additional computing resources, which in fact is a matter under study [53,54,56]. MQTT and CoAP have a similar result whereas AMQP highlights as the protocol with the lowest memory consumption.

Considering the bandwidth consumption, the results make clear that DDS is the technology with the higher consumption when the publishers increase their sampling rate. However, these results may be imprecise because of the decentralized nature of DDS technology and its high reliability [57], which ensures the correct delivery of a great number of samples, even with high sampling rates. In this sense, all the publishers cooperate to deliver the samples to the subscribers, whereas in centralized technologies the whole task is carried out by a machine: the broker. This certainly implies a consequent increase in the bandwidth consumption in DDS. As for centralized protocols, MQTT is introduced as the technology with the lowest bandwidth consumption, even properly addressing publication frequencies of 1000 samples per second. On the contrary, the XMPP protocol presents a higher consumption even for low sampling rates. This occurs due to the fact that this protocol was not designed for constrained devices and its performance can be degraded in terms of bandwidth and battery consumption [18], as it has been demonstrated. CoAP makes very low use of the bandwidth as a consequence of the UDP protocol employed [58]. In this sense, it can be noted that CoAP uses less bandwidth than MQTT with the same payload, which coincides with the results of other authors [14].

In terms of latency, DDS has a better performance than the rest of protocols. The results for AMQP and JMS are analogous, while latency for MQTT grows in a sampling rate of 1000 samples per second. The results are similar for the jitter: DDS is considered as the most reliable technological alternative for scenarios of communication that need a high predictability in the delivery of information, as it occurs in real-time applications. On the contrary, the outcomes confirm that XMPP and CoAP protocols are not suitable for applications that require a low latency and a high predictability [54].


                     Table V
                      shows a summary of the strengths and weaknesses of each protocol studied in this work.

@&#CONCLUSIONS@&#

This paper presents a comparative study of different M2M protocols in terms of several performance metrics. For the execution of tests, it was designed and developed a Distributed Computing Framework based on master–slave communication architecture that supports the deployment of the benchmark. The Framework stands out for its flexibility, modular design, and ease of use, which makes it an excellent tool to aid developers to choose among M2M protocols for eHealth solutions. The results obtained were validated by means of a particular case of use: the respiratory rehabilitation of COPD patients to be performed in two scenarios with different types of requirements, which are tackled by an eHealth platform developed by the authors. In the first place, the specific challenges of the Ambulatory scenario include mainly real-time communication, reliability and platform scalability. These requirements are met thanks to the features of DDS in terms of its data-centric approach, predictability of information delivery and real-time QoS policies. On the other hand, the characteristics of mobility of the Home-Based scenario are best satisfied by MQTT, a light-weight and low latency transport protocol with a wider range of implementations for mobile platforms. With respect to security, which is a key requirement for any eHealth implementation, DDS and MQTT implementations support features to ensure confidentiality and privacy of the data exchanged for both scenarios.

@&#ACKNOWLEDGMENTS@&#

This work was supported in part by the Fondo de Investigaciones Sanitarias under Grant PI11/00111, in part by the Dirección General de Investigación, Tecnología y Empresa, Government of Andalucía, under Grants TIC-6214, P08-TIC-04069 and PI-0010-2013, and in part by the CIBER de Bioingeniería, Biomateriales y Nanomedicina (CIBER-BBN) under intramural project PERSONA. CIBER-BBN is an initiative funded by the VI National R&D&i Plan 2008–2011, Iniciativa Ingenio 2010, Consolider Program, CIBER Actions and financed by the Instituto de Salud Carlos III with assistance from the European Regional Development Fund.

@&#REFERENCES@&#

