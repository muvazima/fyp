@&#MAIN-TITLE@&#Probability distribution function-based classification of structural MRI for the detection of Alzheimer’s disease

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           PDF based feature extraction is introduced for the representation of sMRI.


                        
                        
                           
                           Voxels within statistically obtained 3D masks are used to generate localized PDFs.


                        
                        
                           
                           Dimensionality reduction of PDF is provided by determining optimum number of bins.


                        
                        
                           
                           Fisher criterion is used to determine the optimal dimension of PDF vector.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Alzheimer’s disease

Voxel-based morphometry

Probability distribution function

Structural MRI

Statistical feature extraction

Computer-aided diagnosis

Classification

Fisher criterion

@&#ABSTRACT@&#


               
               
                  High-dimensional classification methods have been a major target of machine learning for the automatic classification of patients who suffer from Alzheimer’s disease (AD). One major issue of automatic classification is the feature-selection method from high-dimensional data. In this paper, a novel approach for statistical feature reduction and selection in high-dimensional magnetic resonance imaging (MRI) data based on the probability distribution function (PDF) is introduced. To develop an automatic computer-aided diagnosis (CAD) technique, this research explores the statistical patterns extracted from structural MRI (sMRI) data on four systematic levels. First, global and local differences of gray matter in patients with AD compared to healthy controls (HCs) using the voxel-based morphometric (VBM) technique with 3-Tesla 3D T1-weighted MRI are investigated. Second, feature extraction based on the voxel clusters detected by VBM on sMRI and voxel values as volume of interest (VOI) is used. Third, a novel statistical feature-selection process is employed, utilizing the PDF of the VOI to represent statistical patterns of the respective high-dimensional sMRI sample. Finally, the proposed feature-selection method for early detection of AD with support vector machine (SVM) classifiers compared to other standard feature selection methods, such as partial least squares (PLS) techniques, is assessed. The performance of the proposed technique is evaluated using 130 AD and 130 HC MRI data from the ADNI dataset with 10-fold cross validation
                        1
                     
                     
                        1
                        Data used in this article were obtained from the Alzheimer׳s Disease Neuroimaging Initiative (ADNI) database (www.loni.ucla.edu/ADNI). ADNI investigators other than those listed above contributed to study design, implementation or data provision but did not participate in the analyses or writing of this report. The complete listing of ADNI investigators is available at http://www.loni.ucla.edu/ADNI/Data/ADNI_Authorship_List.pdf.
                     . The results show that the PDF-based feature selection approach is a reliable technique that is highly competitive with respect to the state-of-the-art techniques in classifying AD from high-dimensional sMRI samples.
               
            

@&#INTRODUCTION@&#

In older adults, Alzheimer’s disease (AD) is a brain disorder that gradually impairs regions of the brain that are responsible for memory, learning, and higher executive functioning [1,2]. Currently, the detection of AD is based on clinical examinations and assessments of perception and behavior as indicators emerging in the later disease stages. Neuroimaging measures of structural changes and functional activities in the brain may be a good method for early detection of AD. Among the several neuroimaging techniques used in AD diagnosis, such as magnetic resonance imaging (MRI), positron emission tomography (PET), and single-photon emission computed tomography (SPECT), MRI is more widely used because of its excellent spatial resolution with good tissue contrast [3] without the need for radioactive pharmaceutical injection, as is required with PET and SPECT [4,5]. Regional and global (whole-brain) atrophy measurements are provided via MRI, and atrophy measured on structural MRI is a powerful biomarker of the stage and intensity of the neurodegenerative aspect of AD pathology [6]. Several studies have used structural MRI feature extraction for classification. Some of these studies are based on morphometric methods [7–9], region of interest (ROI)/volume of interest (VOI) [10–12], and gray matter voxels in the automatic segmentation of images [13].

The aim of this study was to introduce a novel statistical feature selection method based on the probability distribution function (PDF) of the VOI, which can be considered a lower-dimensional feature vector representing sMRI images. The PDF is assumed to be the statistical pattern of the VOI representing the entire sMRI.

The dimensionality of the PDF-based feature vector can be adjusted by changing the number of bins of the PDF. The proposed PDF-based method not only extracts the selected statistical features but also reduces the dimensionality of the input vectors to feature vectors. The PDF-based feature vector calculation process does not require matrix operations, making the feature extraction process computationally cheaper compared to alternative dimensionality reduction methods such as partial least squares (PLS). In this context, it is apparent that the computational cost of PDF calculation is negligibly low when compared to PLS. The proposed work was accomplished using four steps to develop an automatic computer-aided diagnosis (CAD) technique for AD diagnosis. First, a statistical method was used based on the VBM technique plus Diffeomorphic Anatomical Registration using the Exponentiated Lie algebra (DARTEL) approach to analyze group-wise comparisons between a cross-sectional structural MRI scans diseased group and normal controls [6,14,15]. Based on the VBM plus DARTEL approach, overall and regional structural gray matter alterations were investigated to define regions with a significant decline of gray matter in patients with AD compared to the healthy controls (HCs). Second, these specified areas (gray matter loss in AD patients) were employed as masks with the template and extracted voxel values from the VOI to form the raw feature vectors. These raw feature vectors went through further data reduction or selection processes before being used by the classifier. Third, a novel statistical feature vector generation using probability distribution functions (PDFs) extracted from the respective 3D mask regions of sMRI was used for classification. The PDF approach can help in two ways: (1) dimensionality reduction and (2) compressing the statistical information of the high-dimensional data into a lower-dimensional vector. PDF pattern recognition has been used successfully in a number of applications, including face recognition [16–18]. In addition, an automatic approach based on the Fisher criterion was used to determine the optimal number of bins of the histogram generating the PDF. This approach adaptively determines the number of PDF bins based on the training data in each fold instead of using a fixed one. Fourth, the performance of the proposed statistical feature-selection technique was evaluated using SVM classifiers.

The remainder of this paper is arranged as follows: Section 2 provides statistics of the data used in the work and Section 3 describes the methodology used to design an automatic CAD tool based on the PDF. The evaluation experiments and an analysis of the proposed method are described in Section 4, and the conclusion is drawn in Section 5.

MRI images and data used in this work were obtained from the 3T MRI protocol of the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (www.loni.ucla.edu/ADNI). Briefly, the protocol included T1-weighted MRI images based on a scanner by Siemens with acquisition plane=sagittal, acquisition type=3D, coil=PA, flip angle=9.0 degrees, matrix X/Y/Z=240.0/256/176pixel, mfg model=Skyra, pixel spacing X/Y=1.0/1.0mm, pulse sequence=GR/IR, slice thickness=1.2mm, and TE/TI/TR=2.98/900/2300ms.

The group of patients with AD contained 130 people aged 57 to 91 years (mean 75.88±7.54 years). The Mini Mental State Examination (MMSE) and Clinical Dementia Ratio (CDR) scores ranged from 10 to 28 (mean 22.33±3.27) and 0.5 to 2 (mean 0.80±0.37), respectively. The second group contained 130 HCs aged 56 to 88 years (mean 74.49±6.13 years). The MMSE for this group ranged from 27 to 30 (mean 29.26±0.80) and the CDR was zero. In a direct comparison between the HC and AD groups, there were no significant differences in age or the number of gender subjects.

In this section, the methodology is presented based on the PDF approach to design an automatic CAD system for MRI classification. First, the VBM plus DARTEL approach process was used to perform pre-processing on 3D MRI data. Second, a feature-extraction method was employed based on VBM plus DARTEL analysis. Third, an adaptive PDF-based data-selection method was proposed, as a novel statistical data-selection mechanism representing the statistical pattern of VOI of high-dimensional sMRI data in a low-dimensional space. The dimension of the PDF-based vector depended directly on the number of bins used in the histogram of the VOI, which was then normalized into the PDF. The optimal number of bins was obtained by maximizing the Fisher criterion among the possible number of bins. Finally, to evaluate the proposed technique, classifiers such as SVM were used. 
                     Fig. 1 illustrates the framework of the proposed CAD system.

Data pre-processing was performed using Statistical Parameter Mapping (SPM) software version 8 (Welcome Trust Centre for Neuroimaging, London, UK; available at: http://www.fil.ion.ucl.ac.uk/spm) and the voxel-based morphometry toolbox version 8 (http://dbm.neuro.uni-jena.de/vbm). VBM, introduced by Ashburner and Friston, is a method used to assess whole-brain structure with voxel-by-voxel comparisons, which has been developed to analyze tissue concentrations or volumes between subject groups to distinguish degenerative diseases with dementia [4,14]. Recently, VBM has been applied to detect early atrophic changes in AD [3,19–21]. It can provide statistical results in comparisons of patients with AD to HCs [3,22]. To enhance inter-subject registration of the MRI images, DARTEL was applied [23,24], which has been found to optimize the sensitivity of such analyses by using the Levenberg–Marquardt strategy as compared to standard VBM [23,25]. Moreover, the DARTEL algorithm provides precise and accurate localization of structural damage on the MRI images [3,4]. In the VBM8 toolbox, registration to standard Montreal Neurological Institute (MNI) space is an important process, which contains linear affine transformation and nonlinear deformation by using high-dimensional DARTEL normalization. This process involves using the DARTEL template generated from 550 healthy control participants (defined by default settings of VBM8) [26]. The normalized segmented images were modulated by using a nonlinear deformation, which allows for comparing absolute amounts of tissue corrected for individual differences in brain size [26]. Finally, the segmented images were spatially smoothed with an 8mm full-width-half-maximum (FWHM) Gaussian kernel. After spatial pre-processing, the smoothed, modulated, DARTEL warped and normalized gray matter datasets were used for statistical analysis. Regional gray matter volume changes were generated by voxel-based analysis over the whole brain. 
                        Fig. 2 illustrates the processing pipeline of the VBM analysis. To detect gray matter volume reductions in patients with AD, a two-sample t-test in SPM8 was used. Age was applied into the matrix design as a nuisance variable. To avoid possible edge effects between gray matter and white matter or cerebrospinal fluid (CSF), the absolute threshold masking was 0.1. Significance was set at a p-value of □0.01 with correction for family-wise error (FWE) and an extent threshold of 1400 adjacent voxels for two-sample comparisons. Between-group differences in demographics and clinical parameters among or between subgroups were executed by Statistical Package for Social Sciences software (SPSS version 16.0) by using an independent sample t-test, and p□0.05 was considered significant.

A feature-extraction procedure based on VBM plus DARTEL analysis was applied to isolate the VOI. The regions of decreased gray matter volume obtained using VBM plus DARTEL analysis in patients who suffered from AD were segmented using a 3D mask. This mask was applied to the gray matter density volumes resulting from the VBM plus DARTEL analysis to extract voxel values as raw feature vectors. It is important to separate the data used for VBM 3D mask generation from the data used for classification. In other words, the data to model the 3D mask must explicitly come from the training set. In this context, we divided the dataset for VBM mask generation within each outer cross-validation fold separately. In other words, we randomly divided our subjects into 10 folds with the same number of AD and HC subjects in each fold. In each iteration, we used one fold for testing and 9 folds for training. Based on each training dataset, we performed VBM plus DARTEL analysis to reveal regions of decreased gray matter volume in patients as a 3D mask. In total, we defined 10 different masks with different lengths (e.g. from 59,395 to 69,170 voxels). The respective 3D masks were used in the respective iteration to extract features from the training and testing datasets. The raw feature space in the VBM extracted feature set was very high in comparison to the number of samples. Because the sample feature vectors spanned a very small region in the feature vector space, data reduction was desired in post-processing. In this context, it is preferable to reduce the dimensionality of sMRI datasets. Therefore, the dimensionality of extracted raw feature vectors is reduced statistically by means of PLS and PDF.

PLS is a statistical algorithm for modeling the relationship between two datasets: 
                              X
                              ⊂
                              
                                 
                                    R
                                 
                                 N
                              
                            and 
                              Y
                              ⊂
                              
                                 
                                    R
                                 
                                 M
                              
                           . Recently, the PLS data-reduction approach has been used successfully in a number of applications for machine-learning in AD [27–30]. After observing 
                              n
                            data samples, PLS decomposes the 
                              n
                              ×
                              N
                            and the 
                              n
                              ×
                              M
                            matrices of zero mean variables 
                              X
                            and 
                              Y
                           , respectively, into the following form [27,31]:
                              
                                 (1)
                                 
                                    X
                                    =
                                    T
                                    
                                       
                                          P
                                       
                                       T
                                    
                                    +
                                    E
                                    Y
                                    =
                                    U
                                    
                                       
                                          Q
                                       
                                       T
                                    
                                    +
                                    F
                                 
                              
                           where 
                              T
                            and 
                              U
                            are 
                              n
                              ×
                              A
                            matrices of the 
                              A
                            extracted score vectors, 
                              P
                            and 
                              Q
                            are 
                              N
                              ×
                              A
                            and 
                              M
                              ×
                              A
                            matrices of loadings, and 
                              E
                            and 
                              F
                            are 
                              n
                              ×
                              N
                            and the 
                              n
                              ×
                              M
                            error matrices [27]. In this study, in each fold the PLS algorithm was applied to 
                              X
                            (training dataset) and 
                              Y
                            (training data label) in order to obtain score and loading matrices. In addition, a weight matrix was obtained from the training dataset to compute a score matrix for the testing dataset [27]. Next, score vectors obtained from the training and test datasets were used as feature vectors by SVM classifiers. 
                           Fig. 3 illustrates the pipeline of the PLS feature-reduction procedure.

The PDF of a raw feature vector extracted from VOI is a statistical description of the distribution of occurrence probabilities of voxel values that can be considered a feature vector representing a high-dimensional vector in a lower-dimensional space. In a mathematical sense, a PDF can be defined as a vector of probabilities representing the probability of the voxel values that fall into various disjointed intervals, known as bins. Given a raw vector extracted from VOI, the PDF, 
                              H
                           , of the raw vector met the following conditions [16,18]:
                              
                                 (2)
                                 
                                    H
                                    =
                                    [
                                    
                                       
                                          p
                                       
                                       
                                          1
                                       
                                    
                                    ,
                                    
                                       
                                          p
                                       
                                       
                                          2
                                       
                                    
                                    ,
                                    
                                       
                                          p
                                       
                                       
                                          3
                                       
                                    
                                    ,
                                    …
                                    ,
                                    
                                       
                                          p
                                       
                                       
                                          m
                                       
                                    
                                    ]
                                    ,
                                    
                                    
                                       
                                          p
                                       
                                       
                                          i
                                       
                                    
                                    =
                                    
                                       
                                          
                                             
                                                η
                                             
                                             
                                                i
                                             
                                          
                                       
                                       N
                                    
                                    ,
                                    
                                    i
                                    =
                                    1
                                    ,
                                    2
                                    ,
                                    …
                                    ,
                                    m
                                 
                              
                           where 
                              
                                 
                                    η
                                 
                                 
                                    i
                                 
                              
                           , is the number of voxels falling into the 
                              i
                              th
                            bin, 
                              m
                            is the number of bins, and 
                              N
                            is the total number of voxels in the 3D mask. In the classification stage, the PDF, 
                              H
                           , of raw vectors was used in the representation of the training and test data. The number of bins adjusts the dimensionality of a PDF vector. In this work, the number of bins was assumed to vary from 2 to 100.

To select the optimal number of bins, an automatic method was used, based on the Fisher criterion, 
                              J
                              (
                              w
                              )
                           , given in Eq. (3):
                              
                                 (3)
                                 
                                    J
                                    (
                                    w
                                    )
                                    =
                                    
                                       
                                          
                                             
                                                w
                                             
                                             T
                                          
                                          
                                             
                                                S
                                             
                                             
                                                B
                                             
                                          
                                          w
                                       
                                       
                                          
                                             
                                                w
                                             
                                             T
                                          
                                          
                                             
                                                S
                                             
                                             
                                                W
                                             
                                          
                                          w
                                       
                                    
                                 
                              
                           where 
                              
                                 
                                    S
                                 
                                 
                                    B
                                 
                              
                            is the between-class scatter matrix and 
                              
                                 
                                    S
                                 
                                 
                                    W
                                 
                              
                            is the within-class scatter matrix, respectively [32]. For the two classes, 
                              
                                 
                                    C
                                 
                                 
                                    1
                                 
                              
                            and 
                              
                                 
                                    C
                                 
                                 
                                    2
                                 
                              
                           , the between-class scatter and within-class scatter matrices are defined as:
                              
                                 (4)
                                 
                                    
                                       
                                          S
                                       
                                       
                                          B
                                       
                                    
                                    =
                                    (
                                    
                                       
                                          μ
                                       
                                       
                                          1
                                       
                                    
                                    −
                                    μ
                                    
                                       )
                                       
                                       
                                       
                                       2
                                       
                                    
                                    
                                       
                                          (
                                          
                                             
                                                μ
                                             
                                             
                                                1
                                             
                                          
                                          −
                                          μ
                                          
                                             )
                                             
                                             
                                             
                                             2
                                             
                                          
                                       
                                       T
                                    
                                 
                              
                           
                           
                              
                                 (5)
                                 
                                    
                                       
                                          S
                                       
                                       
                                          W
                                       
                                    
                                    =
                                    
                                       ∑
                                       
                                          
                                             
                                                H
                                             
                                             
                                                i
                                             
                                          
                                          ∈
                                          
                                             
                                                C
                                             
                                             
                                                1
                                             
                                          
                                       
                                    
                                    
                                       (
                                       
                                          
                                             H
                                          
                                          
                                             i
                                          
                                       
                                       −
                                       
                                          
                                             μ
                                          
                                          
                                             1
                                          
                                       
                                       )
                                       
                                          
                                             (
                                             
                                                
                                                   H
                                                
                                                
                                                   i
                                                
                                             
                                             −
                                             
                                                
                                                   μ
                                                
                                                
                                                   1
                                                
                                             
                                             )
                                          
                                          T
                                       
                                    
                                    +
                                    
                                       ∑
                                       
                                          
                                             
                                                H
                                             
                                             
                                                i
                                             
                                          
                                          ∈
                                          
                                             
                                                C
                                             
                                             
                                                2
                                             
                                          
                                       
                                    
                                    
                                       (
                                       
                                          
                                             H
                                          
                                          
                                             i
                                          
                                       
                                       −
                                       
                                          
                                             μ
                                          
                                          
                                             2
                                          
                                       
                                       )
                                       
                                          
                                             (
                                             
                                                
                                                   H
                                                
                                                
                                                   i
                                                
                                             
                                             −
                                             
                                                
                                                   μ
                                                
                                                
                                                   2
                                                
                                             
                                             )
                                          
                                          T
                                       
                                    
                                 
                              
                           where 
                              
                                 
                                    μ
                                 
                                 
                                    1
                                 
                              
                            is the mean of the PDF vectors in class 1 and 
                              
                                 
                                    μ
                                 
                                 
                                    2
                                 
                              
                            is the mean of the PDF vectors in class 2, and 
                              w
                              =
                              
                                 
                                    S
                                 
                                 
                                    W
                                 
                                 
                                    −
                                    1
                                 
                              
                              (
                              
                                 
                                    μ
                                 
                                 
                                    1
                                 
                              
                              −
                              
                                 
                                    μ
                                 
                                 
                                    2
                                 
                              
                              )
                           .

The main steps in the proposed algorithm are summarized in the pseudo code shown in algorithm 1. The number of bins (
                              
                                 
                                    N
                                 
                                 
                                    b
                                    i
                                    n
                                 
                              
                           ) of histogram 
                              
                                 
                                    H
                                 
                                 
                                    i
                                 
                              
                            was iteratively incremented from 2 to 100, using a training set of each fold for calculating the respective Fisher criterion values. The optimal number of bins,
                              
                                 
                                    N
                                 
                                 
                                    o
                                    p
                                    t
                                 
                              
                           , maximizing the Fisher criterion was selected to be used as the optimal dimension of the test and training data in each fold through the cross-validation process.
                              Algorithm 1
                              Optimal number of bins selection procedure.
                                    
                                       
                                          
                                          
                                             
                                                1:
                                                   
                                                      V
                                                      ←
                                                      c
                                                      o
                                                      m
                                                      p
                                                      o
                                                      n
                                                      e
                                                      n
                                                      t
                                                      _
                                                      s
                                                      e
                                                      t
                                                      (
                                                      D
                                                      a
                                                      t
                                                      
                                                         
                                                            a
                                                         
                                                         
                                                            T
                                                            r
                                                            a
                                                            i
                                                            n
                                                         
                                                      
                                                      ,
                                                      
                                                      
                                                      L
                                                      a
                                                      b
                                                      e
                                                      
                                                         
                                                            l
                                                         
                                                         
                                                            T
                                                            r
                                                            a
                                                            i
                                                            n
                                                         
                                                      
                                                      )
                                                   
                                                
                                             
                                             
                                                2:
                                                   number of bin← Ø, 
                                                      
                                                         
                                                            N
                                                         
                                                         
                                                            b
                                                            i
                                                            n
                                                         
                                                      
                                                      =
                                                      100
                                                   
                                                
                                             
                                             
                                                3:
                                                   
                                                   for 
                                                   n=2 to 
                                                      
                                                         
                                                            N
                                                         
                                                         
                                                            b
                                                            i
                                                            n
                                                         
                                                      
                                                    
                                                   do
                                                
                                             
                                             
                                                4:
                                                   
                                                   
                                                   
                                                      
                                                         
                                                            H
                                                         
                                                         
                                                            i
                                                         
                                                      
                                                      ←
                                                      c
                                                      o
                                                      m
                                                      p
                                                      u
                                                      t
                                                      e
                                                      _
                                                      h
                                                      i
                                                      s
                                                      t
                                                      o
                                                      g
                                                      r
                                                      a
                                                      m
                                                      (
                                                      
                                                         
                                                            X
                                                         
                                                         
                                                            i
                                                         
                                                      
                                                      ,
                                                      
                                                      
                                                      n
                                                      )
                                                   
                                                
                                             
                                             
                                                5:
                                                   
                                                   
                                                   
                                                      (
                                                      
                                                         
                                                            S
                                                         
                                                         
                                                            B
                                                         
                                                      
                                                      ,
                                                      
                                                      
                                                      
                                                         
                                                            S
                                                         
                                                         
                                                            W
                                                         
                                                      
                                                      )
                                                      ←
                                                      c
                                                      o
                                                      m
                                                      p
                                                      u
                                                      t
                                                      e
                                                      _
                                                      s
                                                      c
                                                      a
                                                      t
                                                      t
                                                      e
                                                      r
                                                      (
                                                      
                                                         
                                                            H
                                                         
                                                         
                                                            i
                                                         
                                                      
                                                      ,
                                                      
                                                      
                                                      L
                                                      a
                                                      b
                                                      e
                                                      
                                                         
                                                            l
                                                         
                                                         
                                                            T
                                                            r
                                                            a
                                                            i
                                                            n
                                                         
                                                      
                                                      )
                                                   
                                                
                                             
                                             
                                                6:
                                                   
                                                   
                                                   
                                                      
                                                         
                                                            μ
                                                         
                                                         
                                                            1
                                                         
                                                      
                                                      ←
                                                      m
                                                      e
                                                      a
                                                      n
                                                      (
                                                      
                                                         
                                                            H
                                                         
                                                         
                                                            i
                                                            
                                                            c
                                                            l
                                                            a
                                                            s
                                                            s
                                                            1
                                                         
                                                      
                                                      )
                                                   
                                                
                                             
                                             
                                                7:
                                                   
                                                   
                                                   
                                                      
                                                         
                                                            μ
                                                         
                                                         
                                                            2
                                                         
                                                      
                                                      ←
                                                      m
                                                      e
                                                      a
                                                      n
                                                      (
                                                      
                                                         
                                                            H
                                                         
                                                         
                                                            i
                                                            
                                                            
                                                            c
                                                            l
                                                            a
                                                            s
                                                            s
                                                            2
                                                         
                                                      
                                                      )
                                                   
                                                
                                             
                                             
                                                8:
                                                   
                                                   
                                                   
                                                      w
                                                      =
                                                      
                                                         
                                                            S
                                                         
                                                         
                                                            W
                                                         
                                                         
                                                            −
                                                            1
                                                         
                                                      
                                                      (
                                                      
                                                         
                                                            μ
                                                         
                                                         
                                                            1
                                                         
                                                      
                                                      −
                                                      
                                                         
                                                            μ
                                                         
                                                         
                                                            2
                                                         
                                                      
                                                      )
                                                   
                                                
                                             
                                             
                                                9:
                                                   
                                                   
                                                   
                                                      φ
                                                      (
                                                      n
                                                      )
                                                      ←
                                                      
                                                         
                                                            
                                                               
                                                                  w
                                                               
                                                               T
                                                            
                                                            
                                                               
                                                                  S
                                                               
                                                               
                                                                  B
                                                               
                                                            
                                                            w
                                                         
                                                         
                                                            
                                                               
                                                                  w
                                                               
                                                               T
                                                            
                                                            
                                                               
                                                                  S
                                                               
                                                               
                                                                  W
                                                               
                                                            
                                                            w
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                10:
                                                   end for
                                             
                                             
                                                11
                                                   
                                                   
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     N
                                                                  
                                                                  
                                                                     o
                                                                     p
                                                                     t
                                                                  
                                                               
                                                               ←
                                                               arg
                                                               
                                                               max
                                                               
                                                               φ
                                                               (
                                                               n
                                                               )
                                                            
                                                         
                                                         
                                                            
                                                               
                                                               
                                                               
                                                               n
                                                               ∈
                                                               
                                                                  {
                                                                  
                                                                     2
                                                                     ,
                                                                     …
                                                                     ,
                                                                     
                                                                        
                                                                           N
                                                                        
                                                                        
                                                                           b
                                                                           i
                                                                           n
                                                                        
                                                                     
                                                                  
                                                                  }
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                
                                             
                                          
                                       
                                    
                                 
                              

To distinguish patients with AD from the HCs, the classification model in terms of the SVM algorithm was established [33].

The SVM is a powerful classifier based on statistical learning principles. In several papers, SVM is used to correctly classify unseen patterns [9,34–36]. During SVM training, SVM maximizes the distance from patterns to the class-separating hyper-plane. Generally, the patterns are not linearly separable; therefore, nonlinear kernel transformation is performed. There are various kernels that can be used during SVM training, including linear, quadratic, polynomial, and radial basis function (RBF) kernels. SVM was performed using the LIBSVM software package (http://www.csie.ntu.edu.tw/~cjlin/libsvm/). In this work, SVM with linear and RBF kernels were used. The RBF model has two parameters that need to be selected, 
                              C
                            (regularization) and 
                              γ
                            (controls the kernel width), in which the performance of the classifier depends on these parameters. To evaluate the performance of the classifier, a procedure of two cross-validations (CVs) was combined with a grid search. This was done to elude unwarp bias in the estimation of accuracies produced by the CV procedure. This procedure consisted of two nested loops. In the outer loop, the data were split into 
                              
                                 
                                    K
                                 
                                 
                                    1
                                 
                              
                            folds. At each step, one fold was used as a test and the remaining 
                              
                                 
                                    K
                                 
                                 
                                    1
                                 
                              
                              −
                              1
                            folds were used for training and validation. In the inner loop, the training data (
                              
                                 
                                    K
                                 
                                 
                                    1
                                 
                              
                              −
                              1
                            folds) were further divided into 
                              
                                 
                                    K
                                 
                                 
                                    2
                                 
                              
                            folds. For each combination of 
                              C
                            and 
                              γ
                           , the classifier was trained using the training data and its performance was assessed using the fold left for validation by estimating the classification accuracy. One fold was left for validation and the remaining 
                              
                                 
                                    K
                                 
                                 
                                    2
                                 
                              
                              −
                              1
                            fold for training was combined with the grid search to determine the optimal parameters. In the grid search, the values of 
                              C
                            and 
                              γ
                            varied logarithmically from 
                              
                                 
                                    2
                                 
                                 
                                    −
                                    5
                                 
                              
                              
                              to
                              
                              
                                 
                                    2
                                 
                                 
                                    20
                                 
                              
                            and from 
                              
                                 
                                    2
                                 
                                 
                                    −
                                    15
                                 
                              
                              
                              to
                              
                              
                                 
                                    2
                                 
                                 
                                    15
                                 
                              
                           , respectively. The inner loop was repeated 
                              
                                 
                                    K
                                 
                                 
                                    2
                                 
                              
                            times and the accuracy of the classifier was obtained across the 
                              
                                 
                                    K
                                 
                                 
                                    2
                                 
                              
                            folds for every combination of 
                              C
                            and 
                              γ
                           . Optimal parameters were selected that produced maximum average accuracy across the 
                              
                                 
                                    K
                                 
                                 
                                    2
                                 
                              
                            folds. Then, the class label of the test data was predicted, which was left out in the outer loop using selected optimal parameters. The above procedure was repeated 
                              
                                 
                                    K
                                 
                                 
                                    1
                                 
                              
                            times by leaving a different fold as test data, which was used to compute the classification accuracy. For SVM with a linear kernel, only the 
                              C
                            parameter was optimized. In this work, 
                              
                                 
                                    K
                                 
                                 
                                    1
                                 
                              
                              =
                              10
                            and 
                              
                                 
                                    K
                                 
                                 
                                    2
                                 
                              
                              =
                              10
                            were used. 
                           Fig. 4 illustrates the pipeline of the 10-fold cross-validation procedure.

The performance of a classifier is measured by using the accuracy (ACC), sensitivity (SEN), specificity (SPE), and area under the curve (AUC) based on 10-fold cross validation. These parameters are computed as follows [37]:
                              
                                 (6)
                                 
                                    ACC
                                    =
                                    (
                                    TP
                                    +
                                    TN
                                    )
                                    /
                                    (
                                    TP
                                    +
                                    FP
                                    +
                                    FN
                                    +
                                    TN
                                    )
                                 
                              
                           
                           
                              
                                 (7)
                                 
                                    SEN
                                    =
                                    TP
                                    /
                                    (
                                    TP
                                    +
                                    FN
                                    )
                                 
                              
                           
                           
                              
                                 (8)
                                 
                                    SPE
                                    =
                                    TN
                                    /
                                    (
                                    TN
                                    +
                                    FP
                                    )
                                 
                              
                           where TP, TN, FN, and FP are the number of true positives, true negatives, false negatives, and false positives, respectively. TP, TN, FN, and FP are determined as follows:
                              
                                 a)
                                 TP: By counting the number of patients with AD correctly identified as AD.

TN: By counting the number of HCs correctly identified as HCs.

FN: By counting the number of patients with AD incorrectly identified as HCs.

FP: By counting the number of HCs incorrectly identified as AD.

In this section, the experimental results of VBM plus DARTEL analysis on 3D MRI are reported to reveal the significance of the volumetric regions with atrophy in patients, contributing to VOI. The performance of the classification of AD using a 10-fold cross-validation is also presented for four cases: (1) performance of the raw features (VBM features) dataset, (2) performance of the PLS method, (3) performance of the proposed PDF technique, and (4) performance of the PDF technique using the optimal number of bins. Two types of SVM classifiers, namely SVM-linear and SVM-RBF, were used for AD classification. ACC (%), SEN (%), SPE (%), and AUC (%) performance metrics were used to assess the different scenarios.

VBM plus DARTEL revealed a significant decline of gray matter volume in the right hippocampus, left hippocampus, right inferior parietal lobe, and right anterior cingulate in patients with AD compared to the HCs. 
                        Fig. 5 shows the brain regions where there was significant atrophy in gray matter volume in AD patients compared to HCs in fold 1 training. The voxel locations of these significant regions were used as a 3D mask in each fold. This mask was applied to the gray matter density volume results from the segmentation step in the VBM plus DARTEL analysis to extract voxel values as raw feature vectors.

The complete MRI dataset from the ADNI database consisted of 260 samples. 
                        Table 1 presents the ACC, SEN, SPE, and AUC obtained by 10-fold cross validation using SVM-linear and SVM-RBF classifiers for raw feature vectors obtained by masking after VBM plus DARTEL analysis.

The feature reduction using PLS was accomplished by extracting raw feature data from VOI obtained from VBM analysis. The extracted raw feature vectors were reduced to lower-dimensional feature vectors of up to 100 components using PLS. 
                        Table 2(a) presents the ACC, SEN, SPE, and AUC obtained from 10-fold cross-validation for SVM classifiers for changing dimensionality. According to Table 2(a), it is clear that the maximum accuracy (90.76%) is yielded with SVM-RBF when the dimensionality is 80. The accuracy is 4.74% higher than the same classifier with all raw features used in Table 1. The reset of the results in Table 2(a) are also higher than the raw data for SEN, SPE, and AUC. The results reported in Tables 1 and 2(a) indicate that the PLS performance using SVM-linear and SVM-RBF classifiers is higher than with the raw data.

The feature selection using PDF was accomplished by extracting raw feature data from VOI obtained using VBM analysis. The extracted raw feature vectors were reduced to lower-dimensional feature vectors of up to 100 components by changing the number of bins of the PDF. Table 2(b) and 
                        Fig. 6 present the ACC, SEN, SPE, and AUC obtained by 10-fold cross-validation using SVM-linear and SVM-RBF classifiers. The results reported in Table 2(a) and (b) show that the PDF-based method is with higher ACC than the PLS-based method in most of the dimensions using linear and SVM-RBF classifiers. For example, for 20 components, the PDF-based ACC performance is 88.50% while PLS ACC performance is 81.96% using SVM-linear. There are few cases in which PLS ACC is higher. The same observation is valid for AUC and SPE, where the PDF-based method is mostly superior to the PLS-based method. On the other hand, although for SEN the PDF-based method is better than the PLS-based method in SVM-linear, the PLS-based method is higher for the SVM-RBF classifier.

As proposed in Section 3.2.3, the optimal number of bins is determined by maximizing the Fisher criterion applied to the two classes (AD and HC) of the training data in each fold through the cross-validation process. 
                        Table 3 presents the average of the performances of the classifiers with the optimal number of bins obtained in each fold, through 10-fold cross-validation. The proposed method of determining the optimal number of components (i.e. the number of bins) is also applied to PLS. By examining the results of Table 3, it was observed that the overall performance of the proposed PDF-based method with the optimal number of bins is superior to PLS for SVM-linear, where the results of both methods are comparable for SVM-RBF.

Recently, several studies have reported classification results to distinguish AD and HC based on MRI. Zhang et al. [38] used multimodal classification of AD based on the combination of MRI, CSF, and PET. They reported ACCs of 86.2%, 82.1%, and 86.5% in the classification of AD/HC by MRI, CSF, and PET imaging modalities, respectively. Also, they achieved a high accuracy performance (93.2%) by combining the MRI, CSF, and PET results. Querbes et al. [39] achieved an ACC of 85% based on the cortical thickness feature from MRI data. Hinriches et al. [40] reported an ACC of 75.27% based on MRI data and increased it to 81% by combining MRI and PET. Vemuri et al. [41] announced an SEN/SPE of 86/86% in 380 subjects using the STructural Abnormality iNDex (STAND) score from MRI data. Westman et al. [42] presented an ACC of 87% from MRI data and increased it to 91.8% by combining MRI data with CSF measures. Papakostas et al. [43] applied two methods to analyze MRI data, namely, VBM and deformation-based morphometry (DBM), on 98 female subjects. They extracted features based on three different models: MSD, displacement magnitude (DM), and Jacobian determinant (JD). They also investigated their methods with several classifiers. They reported ACCs of 85%, 84%, and 79% for the three models, respectively. Aguilar et al. [44] used FreeSurfer software to compute cortical thickness and volumetric measures, yielding an ACC of 84.9% for the artificial neural network (ANN) classifier from MRI data and of 88.8% for the SVM classifier by combining MRI data with educational and demographic data. Zhou et al. [45] employed FreeSurfer software to calculate 55 volumetric variables from MRI data. They reported an ACC of 78% for MRI data and 92.4% by combining MRI data with the MMSE. Savio et al. [9] studied the feature-extraction process with VBM analysis on 98% female subjects only and achieved the best results with 86% accuracy for the RBF-AB-SVM classifier. Khedher et al. [30] reported an ACC of 88.49% by combining GM and WM modalities in MRI. Klöppel et al. [13] employed leave-one-out as a validation method in three different groups (Groups I, II, and III) with different severity of atrophy in AD. The ACC of Group I was 95%, of Group II was 92.9%, and of Group III was 81.1%. The severity of atrophy in Group I was the highest, making this group the most successful among the three. A study by Cuingnet et al. [46] comprised 10 methods using the ADNI database. They reported a SEN of 81% and a SPE of 95% as the best performances. In this paper, a set of a total of 260 MRI samples was used in the AD and HC groups, with superior results with respect to ACC, SEN, and AUC in 
                        Table 4 except for the results of Klöppel et al. [13] for Groups I and II. One of the main reasons for this observation stems from the fact that the severity of the atrophy of Groups I and II was higher than that of Group III and our dataset. Additionally, using the leave-one-out method already gives an advantage to the method employed by Klöppel et al. [13] against the 10-fold cross-validation technique used in the proposed method. The experimental results using the proposed PDF-based approach with SVM by linear Kernel generates 89.65% accuracy, 87.73% sensitivity, 91.57% specificity, and 95.33% AUC. The details of the parameters used in classification performance with different methods by using MRI data are provided in Table 4. Some of the results reported in Table 4 use ADNI data-set, where the others use different or private data-sets. Additionally, the results from ADNI data-set are using different number of AD/HC samples. In order to have comparable results, we have used ADNI data-set with high number of samples (130 AD and 130 HC), which we believe provides a suitable ground for acceptable comparisons.

@&#CONCLUSION@&#

In this paper, an automatic CAD technique was introduced based on a novel statistical feature-selection process, namely, PDF of VOI, for the classification of AD. The proposed feature-selection method compresses the statistical information of high-dimensional data into a lower-dimensional vector. This approach was used for high-dimensional classification, especially for feature-extracted VOI of gray matter atrophy. The PDF-based feature-selection approach was compared to the standard PLS-based classification using SVM classifiers. The results clearly indicated that the PDF-based feature-selection method is a reliable alternative to the PLS-based method, in which the performance of the proposed PDF-based method with the optimal number of bins is superior to PLS for SVM-linear, and the results of both methods are comparable for SVM-RBF. Moreover, PDF generation does not require complex matrix operations, making the feature-extraction process computationally cheaper than alternative dimensionality-reduction methods, such as PLS. The proposed PDF-based method not only extracts the selected statistical features but also reduces the dimensionality of the input vectors to feature vectors with acceptably low dimensions. It is apparent that the computational cost of PDF calculation is negligibly low when compared to PLS. As part of future prospects on PDF-based pattern recognition in neuroimaging, it is suggested to use data fusion techniques for the proposed MRI modality with other modalities, such as PET, CSF, and WM, and to combine them using the proposed PDF-based approach in order to achieve higher accuracy. The PDF-based data fusion technique has already been used successfully in recent studies for the improvement of face-recognition performance [16,17].

None.

@&#REFERENCES@&#

