@&#MAIN-TITLE@&#An energy saving audio compression scheme for wireless multimedia sensor networks using spatio-temporal partial discrete wavelet transform

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Energy saving data compression for WMSNs with randomly deployed nodes.


                        
                        
                           
                           Energy is primarily saved by exploiting spatial and temporal correlation together.


                        
                        
                           
                           It is further saved by proposing a tree based routing which ensures unidirectional data flow toward the sink.


                        
                        
                           
                           A lossless encoding scheme is proposed to save energy further.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Wireless multimedia sensor network

Compression

Partial discrete wavelet transform (PDWT)

Spatio-temporal Partial discrete wavelet transform (STPDWT)

Spatio-temporal Partial discrete wavelet transform with Encoding (STPDWTE)

Tree based routing

@&#ABSTRACT@&#


               
               
                  Due to strict inherent limitations in terms of processing power, storage and bandwidth, data processing is a challenge in the wireless multimedia sensor networks. In this paper, we provide an energy-saving audio data compression technique for such a network using lifting based partial discrete wavelet transform (PDWT). Unlike existing implementation of such a lifting based PDWT, we exploit both spatial and temporal correlation of data together, with an objective to save energy while achieving acceptable signal-to-noise ratio (SNR). We subsequently design a tree based routing scheme and an encoding scheme to be used with the proposed compression scheme with a target to reduce energy further. Finally, the design feasibility along with simulation results, including statistical analysis is presented to evaluate efficacy of the scheme in terms of two conflicting parameters viz. energy consumption and SNR. The comparative results confirm our scheme’s supremacy over a few competing schemes.
               
            

@&#INTRODUCTION@&#

The availability of low-cost hardware and advancement in short range radio communication has enabled the development of wireless multimedia sensor networks (WMSNs). The WMSNs can process multimedia data such as video and audio streams, still images collected from the application area [1,3]. Energy is one of the scarcest resources [4] in such networks. In-network processing is one of the techniques to save energy and data compression is one of the implementing techniques of in-network processing.

Generally, the sensor nodes are densely spaced and therefore, the sensory data are bound to be spatially correlated. On the other hand, in the majority of real time signal processing (e.g. audio signal) applications, high temporal correlation exists [2] in addition to spatial correlation. Typically in the continuous monitoring application, e.g. road-traffic pollution control, sensory data comprises of both spatially and temporally correlated data. We consider such a continuous monitoring application domain. The various transform techniques (e.g. DWT, DCT) usually exploit spatial correlation of data. However, in order to save energy further while maintaining quality retrieval of compressed data, in presence of both types of correlation, the obvious choice is to have compression technique, which exploits spatial and temporal correlation of data together.

@&#MOTIVATION@&#

In WMSN due to strict inherent limitations in terms of processing power, storage and bandwidth data processing is a challenge. Most of the state-of-the-art techniques used in compression applicable to WMSN exploit spatial correlation to save transmission energy. However, in order to save energy further while maintaining quality reconstruction in terms of good SNR usage of temporal correlation is also desirable. This motivates us to exploit spatial and temporal correlation together in order to achieve energy-saving while keeping reconstruction quality (SNR) to an acceptable limit.

Precisely our contributions are as follows:
                           
                              •
                              We propose an energy-saving data compression scheme for WMSNs where nodes are randomly deployed.

Unlike conventional compression techniques, energy is primarily saved by exploiting spatial and temporal correlation together.

Energy is further saved by proposing a tree based routing, which ensures a unidirectional data flow toward the sink.

A lossless encoding scheme is proposed to save energy further.

We establish the supremacy of our technique by experimental verification and comparison with other state-of-the-art techniques by taking real time multimedia data.

The rest of the paper is organized as follows. Literature review is placed in Section 2. Brief on preliminary version of this work is provided in Section 3. System model is described in Section 4. The proposed data compression scheme along with routing and encoding is elaborated in Section 5. Performance evaluation of the scheme is described in Section 6. Finally, the work is concluded in Section 7.

@&#LITERATURE REVIEW@&#

Many works have so far been reported where energy-saving data processing technique in WSN/WMSN is addressed. In one such work [5], a distributed wavelet compression algorithm has been proposed for data compression in WMSN. The algorithm implements distributed wavelet compression using lifting method. This method helps to decorrelate data by exchanging information among neighboring sensor nodes. The advantage of this is in-place computation reduces the transmission cost. However, measurements at the sensor node may contain vectorial data, which means each sensor node would acquire data over time.

Another wavelet lifting based scheme is proposed in [6] where the compression algorithm has been made adaptive. The scheme is made adaptive by selecting an appropriate number of decomposition level based on the network structure, power constraint in sensor nodes and data correlation among sensors. In choosing the best level of decomposition the same step size of the quantizer is considered to make the quantizer error negligible. This helps to reduce energy consumption with good reconstruction. However, both the schemes [5,6] do not consider temporal correlation and assume one-dimensional linear placement of the nodes which is not a very realistic assumption.

In the work [7] the authors have designed a lifting-based 2D transform exploiting spatial correlation for WSNs with flat/arbitrary architecture. This transform enables unidirectional computation found in existing path-wise transforms, thereby eliminating costly backward transmissions while achieving greater data decorrelation than those path-wise transforms. The transform is also optimized by exploiting the trade-off between higher local overhead for complex coding and lower transmission overhead. This scheme also has not exploited temporal correlation.

The authors of [8] have also proposed a lifting-based wavelet scheme by appropriate designing of even/odd splitting of nodes in a network with flat/arbitrary architecture. The split is done in such a way that minimizes the number of even nodes to reduce correlated sensory data while ensuring at least one even node in the vicinity of the odd nodes. Since an even node is generally a raw data node, it transmits data first, followed by computation at the odd nodes. The odd nodes upon receiving all the data from the even node compute predicted data values and after that transmit its own data to the next node. The communication between the nodes is actually achieved through a transmission schedule based routing, which states that if a communication path is established by two nodes, then that path cannot be utilized for other transmission. Here energy consumption due to raw data transmission by even nodes is reduced, however, network delay increases. Similar to many other techniques, this work also exploited spatial correlation only.

The authors of [9] have presented an energy efficient compressed data steam protocol (CDP) where they exploit temporal correlation of sensory data. To start with a General predictive coding (GPC) is employed where the difference between the current and previous data samples is estimated based on which either the data is encoded, or it is transmitted as a raw sample. In order to minimize the packet overhead and memory use in CDP, a data stream is defined as an aggregate flow of multiple individual sensor data flows from a single mote employing the same compression algorithm. The design of compressed data-stream protocol (CDP) is generic in the sense that other lossless or lossy compression algorithms can be easily ‘plugged’ into the proposed protocol system without any changes to the rest of the CDP. However, they have used only temporal correlation and are silent about quality of reconstruction.

In the work [10] a data aggregation scheme is proposed, which use compressed sensing to achieve reconstruction accuracy and energy efficiency. Precisely they use diffusion wavelet in getting high-fidelity reconstruction by exploiting both spatial and temporal correlations of data. The authors opine that the conventional signal/image processing techniques (e.g., DCT, wavelets) require regular WSN deployments (e.g., grids), which make them less practical and therefore, they have explored compressed sensing technique for WSNs with arbitrary topology. However, we argue that we successfully used DCT in one of our previous works [11] where WSNs with arbitrary topology is considered. The present work also uses DWT, another conventional compression technique, in the same scenario.

In one of our works [11] energy-saving data compression scheme in WMSN for randomly placed nodes is reported. Here considering the spatial correlation of data, partial DCT is used for compression and the component of the last DCT coefficient is propagated thereby saves energy by transmitting reduced number of bits. The sink, using this last coefficient generates the remaining DCT coefficients and finally reconstructs the original signal. A tree based routing scheme is also proposed with an aim to reduce energy consumption further.

Unlike conventional energy-saving approaches of compression, a few works are also reported where dedicated routing techniques are designed toward achieving energy-saving. A couple of works are briefed here.

In [12] a routing algorithm is proposed for video transmission over wireless multimedia sensor networks. It works dynamically according to the network requirements and ensures that only the nodes with the highest residual power and the paths with the minimum distance are used during the routing. In this scheme actuation of sensor is initialized on-demand basis. This is achieved by providing a mechanism, where a few nodes are actuated in the sensing event arena. This helps to reduce the redundant information thereby increases the network lifetime. This algorithm suits in a cluster architecture but may be difficult to implement in a network with random placement of nodes.

In view of the orphan problem and poor coverage resulting from Distributed Address Assignment Mechanism (DAAM), a conventional address mechanism used in Zigbee tree topology, the authors of [13] have proposed three alternative schemes improving DAAM in different aspects. One of these is 2-layer DAAM, which divides the address scheme into two layers, where each layer runs a separate DAAM scheme with its own parameters. The other alternative is Location-DAAM where each node knows its own location, and this information is used in the child selection during the tree formation process. The last alternative is RSSI based where information of received signal strength is used in the construction of a tree. However, the overheads in obtaining the location information or the heuristics in selection of child are major concerns in implementing the protocols.

Most of the works on compression focus on either usage of spatial correlation or usage of temporal correlation toward achieving energy-saving compression. In our previous works [2,3], we exploit both types of correlations for uniformly placed nodes in 1-D and 2-D network area respectively. To be more specific, in the preliminary version [3] of the present work we have used, red–black wavelet transform for uniformly placed sensor nodes in a 2D grid. The Red–black wavelet transform is one type of lifting based discrete wavelet transform (DWT). However, this type of transform may not be implementable for the network where nodes are randomly deployed. If a node has to perform this transform on its sensory data, it requires having neighboring nodes at a certain location, and that is not guaranteed in randomly deployed nodes. Thus in the present work, we have chosen partial DWT, one lifting based wavelet transform, which overcomes the said limitations of Red–black wavelet transform. We improve the work further by considering spatial and temporal correlation together in PDWT to make the compression not only energy-saving but also getting acceptable SNR [17]. The scheme also proposes a suitable energy-saving routing scheme. This routing neither requires any additional hardware as in [13] nor it incurs huge energy overhead as in [12], which is unacceptable in our scenario. Finally, we propose a light-weight encoding scheme to reduce transmission energy further for the compressed data.

In the preliminary version [3] of this work, we have proposed an energy-saving audio data compression technique for WMSN combining wavelet lifting with a difference detection technique for an application domain where data are correlated and the nodes are placed uniformly in a 2D grid.

The wavelet lifting based compression algorithm has been implemented using the red–black wavelet lifting scheme. Red–Black wavelet transform works in two steps Horizontal-Vertical Lifting and Diagonal Lifting. Each of the Horizontal-Vertical Lifting and Diagonal Lifting comprises of three steps viz. split, predict and update. During Horizontal-Vertical Lifting, the nodes are split alternately as red and black coloured nodes as a checkerboard in a square grid. As a result of the split, red nodes are horizontal and vertical neighbours of a black node and vice versa. During the predict step a black node approximates the detailed coefficients after receiving data from four adjacent horizontal and vertical red nodes. Lastly in the update step of horizontal vertical lifting, red nodes calculate the smoothing coefficients by averaging the data received from horizontal and vertical adjacent black nodes. In Diagonal lifting, only the red nodes participate and the nodes are split alternately into blue and yellow. Similar to splitting in Horizontal-vertical lifting, here also the arrangement forms a checkerboard. In this stage of diagonal lifting, a yellow node approximates the detailed coefficient further after receiving data from four adjacent diagonal blue nodes. In update step of diagonal lifting, red nodes calculate the smoothing coefficients by averaging the data of the diagonal adjacent yellow nodes.

The operation of red–black wavelet transform is employed on uniformly deployed nodes in a 2D grid structure (Fig. 1
                     ) where distance between any two adjacent nodes within a row/column is fixed (d) and sink placed at the centre position of the grid. It is assumed that a node has maximum eight1-hop neighbours and out of these neighbours, two (H) are placed in the same horizontal axis, another two (V) are placed in the same vertical axis and rest four (D) are placed diagonally of the node.

The algorithm harnesses both the spatial and temporal correlation of data. The scheme uses red–black wavelet lifting accompanied by difference detection technique for capturing spatial and temporal correlation respectively. In every difference detection round, instead of sending raw sample values, only the difference between two consecutive sample values is sent thereby reducing average data to be transmitted. The difference rounds continue as long as it saves transmission cost and it reconstructs the signal with an average permissible SNR. Once the sink finds the difference round is no more profitable, it sends an interrupt to nodes, and the nodes go back to wavelet round. In view of the fact that energy-saving and quality reconstruction are two conflicting requirements, it is desirable to trade-off between the two requirements. This scheme guarantees such a trade-off by judicious combination of wavelet with difference detection. A routing scheme which is greedy in nature is also proposed with a target to reduce energy consumption further.

The system model consists of the network architecture, including deployment and routing. The model also gives a brief overview of one fundamental transform technique based on which our proposed compression method is designed.

We consider randomly placed nodes in a 2-D area with a×a size where the sink is placed at the centre as in Fig. 2
                        . We assume all the nodes have same communication range, which is shown as Rc
                        . So a node (B) within Rc
                         of another node (A) is considered as the 1-hop neighbor of A. The nodes communicate with the sink through the neighbor nodes i.e. in multi-hop.

This section gives a brief overview of the transform technique relevant to understand the scheme to be proposed. Out of many implementing techniques of wavelet transform, lifting scheme is a low-overhead technique [2]. However, the update state of lifting wavelet transform comprises of backward flow of data away from the nodes thereby cause energy wastage. This wastage is eliminated in partial discrete wavelet transform (PDWT).

Similar to lifting scheme, in PDWT also nodes are split into ‘odd’ and ‘even’. During the transform, initially data of even nodes remain unchanged and become the input to the next odd node. The partial coefficient is calculated by considering the data at the odd node, and half of the data sensed by the even node preceding the odd node. This is shown in Eq. (1a).
                           
                              (1a)
                              
                                 
                                    
                                       
                                          odd
                                       
                                       
                                          i
                                       
                                    
                                    (
                                    Pcoeff
                                    )
                                    =
                                    
                                       
                                          odd
                                       
                                       
                                          i
                                       
                                    
                                    -
                                    
                                       
                                          1
                                       
                                       
                                          2
                                       
                                    
                                    ×
                                    
                                       
                                          even
                                       
                                       
                                          j
                                       
                                    
                                 
                              
                           
                        where i and j are node numbers of the odd and even nodes respectively and 
                           
                              j
                              ∈
                              Adj
                              (
                              i
                              )
                           
                        . This partial coefficient (Pcoeff) is transmitted to the next even node, which computes the full coefficient (Fullcoeff) by taking the difference between the partial coefficient of the preceding odd node and half of even data of the same node. This is described in Eq. (1b).
                           
                              (1b)
                              
                                 
                                    
                                       
                                          even
                                       
                                       
                                          i
                                       
                                    
                                    (
                                    Fullcoeff
                                    )
                                    =
                                    
                                       
                                          odd
                                       
                                       
                                          i
                                       
                                    
                                    (
                                    Pcoeff
                                    )
                                    -
                                    
                                       
                                          1
                                       
                                       
                                          2
                                       
                                    
                                    ×
                                    
                                       
                                          even
                                       
                                       
                                          j
                                       
                                    
                                 
                              
                           
                        
                     

The above PDWT is further modified by exploiting both spatial and temporal correlation together. This spatio-temporal PDWT (STPDWT) is elaborated during describing the entire scheme in the next section.

We propose an energy-saving audio compression scheme intended for Wireless multimedia sensor network (WMSN). In the proposed scheme, we exploit both spatial and temporal correlation of data together toward achieving compression. We also propose an encoding scheme with a target to save energy further. In this scheme once the data are transformed, quantized and encoded, it is routed toward the sink, where the data are reconstructed. Although according to ordering of processing, routing is the last step, for better understanding of the compression scheme proposed routing is described first.

The proposed routing scheme considers random deployment of nodes with no initial knowledge of topology. Here each of the nodes stores a 2-tuple (own_ID, parent_ID) and initially parent_ID is set to null. When a node is deployed, it knows its location, i.e. own_ID. Once the nodes are deployed, the sink starts the pre-computation by broadcasting its own_ID. The nodes within communication range of the sink receive the broadcast and set own_ID of sink as their parent_ID. The receiver nodes again broadcast their own_ID. The nodes which are yet uninitialized and located within communication range of these nodes receive the broadcast packet, set the broadcast own_ID as parent_ID. In this way, the process is continued until all the nodes have been assigned their own_ID and parent_ID. It is to be noted that in this process, a tree as shown in Fig. 3
                        (a) is formed. During the pre-computation phase, a unique path is set from the sink to each node through their respective parent nodes as shown in Fig. 3(b).

Once the routes are established the nodes get ready to capture data. The nodes use a 4-tuple (own_ID, parent_ID, position_bit, data) packet for the data transmission. Now the network starts operating by sensing data and forwards the transformed data along the established routing path toward the sink as shown in Fig. 3(c). When a node forms a new packet, it sets the position_bit as 0. This can happen in two cases – firstly, if the node is a leaf node of the routing tree (for the entire period of operation) and secondly if it is the very first instance of operation (for all other nodes). But, in general when a node receives a packet from its child node, complements the position_bit of that packet and compresses data accordingly. Since this position bit is just one bit, so this overhead doesn’t affect the overall energy consumption of the nodes significantly, but it becomes an essential parameter for odd or even node identification to calculate the transform coefficients during the compression phase. This compression process will be discussed later elaborately.

Thus the routing follows a greedy approach, where not necessarily it follows always the shortest path, but it ensures a minimum number of hops. This means minimum number of quantization de-quantization cycles ultimately minimizing the scope of noise introduction. Thus, we aim to achieve better SNR. However, by proposing this tree based routing, unidirectional data flow toward the sink is maintained. As a result, transmission energy is saved.

When the nodes are deployed with own_IDs, the pre-computation for routing starts from the sink, and depending on the depths of the nodes, every node sets their parent_ID along an established path. The sink initiates the following algorithm, and all the nodes are finally involved.
                           
                              
                                 
                                 
                                    
                                       
                                          Input: Null
                                    
                                    
                                       
                                          Output: Formation of routing tree
                                    
                                    
                                       
                                          1. Begin
                                       
                                    
                                    
                                       
                                          2.parent_ID=null;
                                    
                                    
                                       
                                          3.
                                          if own_ID=0 /∗ this is sink ∗/
                                    
                                    
                                       
                                          4.
                                          
                                          Broadcast packet containing (own_ID);
                                    
                                    
                                       
                                          5.
                                          else/∗ nodes other than sink ∗/
                                    
                                    
                                       
                                          6.
                                          
                                          wait for a broadcast packet to arrive
                                    
                                    
                                       
                                          7.
                                          if a broadcast packet (bd_rcv) is received and parent_ID is null
                                    
                                    
                                       
                                          
                                          /∗ i.e. if parent not set ∗/
                                    
                                    
                                       
                                          8.
                                          
                                          
                                          compute parent_ID = bd_rcv.node_ID;
                                    
                                    
                                       
                                          9.
                                          
                                          
                                          wait for a small random time /∗ to avoid packet collision ∗/
                                    
                                    
                                       
                                          10.
                                          
                                          
                                          broadcast packet containing (own_ID);
                                    
                                    
                                       
                                          11.
                                          
                                          else/∗ if parent is already set ∗/
                                    
                                    
                                       
                                          12.
                                          
                                          
                                          ignore packet bd_rcv
                                    
                                    
                                       
                                          13.
                                          End if
                                    
                                    
                                       
                                          14.End if
                                    
                                    
                                       
                                          15. End
                                       
                                    
                                 
                              
                           
                        During routing phase if a node with the parent set to null receives a broadcast packet, it sets the sender of that packet as its parent. Henceforth, it ignores all the broadcast packets (if any). For this reason, under no circumstances a node can have multiple parents. Besides, as our aim is to minimize the number of hops using greedy approach, optimization of the tree is not targeted. Further, appropriate measure has been taken in simulation to prevent a node from receiving more than one packet simultaneously, as, in reality, this would make the packets interfere resulting in packet loss.

In this section, we provide the complexity analysis in respect to both time and space requirement of the algorithm for routing tree formation. The algorithm has worst case time complexity O(n). The worst case arises when all the nodes are arranged in a straight line and distance between any two consecutive nodes is Rc
                           . Clearly, packet broadcast by a node is received by one node only, and therefore, time complexity is O(n). The best case arises when all the nodes are in radio-communication range (Rc
                           ) of the sink. During tree formation, all the nodes receive broadcast packets from the sink and set sink as their parent forming a star topology. Here, one packet transmission is sufficient, so time complexity is O(1). In both the cases, space required for storing routing information is constant. So space complexity of routing algorithm is O(1).

This section provides a tabular comparison of the proposed tree-based routing with a number of standard competing routing schemes.


                           Table 1
                            represents the comparative features of all the competing routing schemes. From the table, we can say that merit of the scheme lies on the fact that it gives guaranteed message delivery. As our algorithm is tree based, there is no scope of loops in the path. Moreover, care has been taken during simulation to achieve time synchronization for avoiding collision. Therefore, message delivery is guaranteed if channel loss is not considered, whereas remaining routing algorithms does not have this property.

In this scheme, once a node starts sensing data, it employs partial wavelet transform on the difference of sensory data in two consecutive instances. Each node has a memory buffer that can hold data of previous instance. It is to be noted that pure wavelet transform [3,14] is inefficient in terms of transmission energy as there is wastage of energy due to backward transmission of data. Therefore, partial discrete wavelet transform (Section 4.2) is used where partial coefficients are accumulated in forward direction (toward sink) only eliminating energy wastage due to backward transmission. In implementing the scheme STPDWT, the time-wise activities of the nodes in terms of sensing, computations of partial/full coefficients are represented by the following equations:
                           
                              (2a)
                              
                                 
                                    F
                                    _
                                    C
                                    (
                                    2
                                    n
                                    ,
                                    t
                                    )
                                    =
                                    S
                                    _
                                    D
                                    (
                                    2
                                    n
                                    ,
                                    t
                                    )
                                    -
                                    S
                                    _
                                    D
                                    (
                                    2
                                    n
                                    ,
                                    t
                                    -
                                    1
                                    )
                                 
                              
                           
                        
                        
                           
                              (2b)
                              
                                 
                                    P
                                    _
                                    C
                                    (
                                    2
                                    n
                                    +
                                    1
                                    ,
                                    t
                                    +
                                    1
                                    )
                                    =
                                    {
                                    S
                                    _
                                    D
                                    (
                                    2
                                    n
                                    +
                                    1
                                    ,
                                    t
                                    +
                                    1
                                    )
                                    -
                                    S
                                    _
                                    D
                                    (
                                    2
                                    n
                                    +
                                    1
                                    ,
                                    t
                                    )
                                    }
                                    -
                                    
                                       
                                          1
                                       
                                       
                                          2
                                       
                                    
                                    ×
                                    F
                                    _
                                    C
                                    (
                                    2
                                    n
                                    ,
                                    t
                                    )
                                 
                              
                           
                        
                        
                           
                              (2c)
                              
                                 
                                    F
                                    _
                                    C
                                    (
                                    2
                                    n
                                    +
                                    2
                                    ,
                                    t
                                    +
                                    2
                                    )
                                    =
                                    P
                                    _
                                    C
                                    (
                                    2
                                    n
                                    +
                                    1
                                    ,
                                    t
                                    +
                                    1
                                    )
                                    -
                                    
                                       
                                          1
                                       
                                       
                                          2
                                       
                                    
                                    ×
                                    
                                       
                                          
                                             S
                                             _
                                             D
                                             (
                                             2
                                             n
                                             +
                                             2
                                             ,
                                             t
                                             +
                                             2
                                             )
                                             -
                                             S
                                             _
                                             D
                                             (
                                             2
                                             n
                                             +
                                             2
                                             ,
                                             t
                                             +
                                             1
                                             )
                                          
                                       
                                    
                                 
                              
                           
                        
                     

To start with, even nodes (2n) compute the difference of sensory data (S_D) value in two consecutive instances (t
                        th & (t
                        −1)th) using Eq. (2a) and forward the computed data value to the next odd nodes (parent node) along the routing path toward the sink. As per Eq. (2b), at (t
                        +1)th instant the odd node (2n
                        +1) computes partial coefficient (P_C) using the differences of sensory data of the current node in two consecutive instances (t
                        +1)th & t
                        th and forwarded data of the previous node. Using Eq. (2c), the full coefficient (F_C) is calculated at the next even (2n
                        +2) node using the partial coefficient of the previous (2n
                        +1) node and S_D of the current node. The entire scheme is illustrated with the help of the diagram in Fig. 4
                        .

The merit of the scheme is that it harnesses temporal correlation by taking differences of sensory data for two consecutive instances while partial discrete wavelet transform reduces spatial correlation. Hence data range reduces; thereby lowering the effect of error introduced even when small quantizer size is used.

The following compression algorithm is run at all the nodes.
                           
                              
                                 
                                 
                                    
                                       
                                          Compression algorithm (STPDWT)
                                       
                                    
                                    
                                       
                                          Input: Receive a packet (pkt_recvd);
                                    
                                    
                                       /∗ pkt_recvd is a 4-tuple (own_ID, parent_ID, position_bit, data) ∗/
                                    
                                    
                                       
                                          Output: Transmit a packet (pkt_transmtd);
                                    
                                    
                                       1. Begin
                                       
                                    
                                    
                                       
                                          /∗ Compression at a node ∗/
                                    
                                    
                                       2. if (pkt_recvd.position_bit = 0)
                                    
                                    
                                       3.
                                          compute even _node_compression
                                    
                                    
                                       
                                          /∗ using Eqs. (2a) and (2c) ∗/
                                    
                                    
                                       4. else
                                    
                                    
                                       5.compute odd_node_compression; /∗using Eq. (2b) ∗/
                                    
                                    
                                       6. end if
                                    
                                    
                                       
                                          /∗ Output compressed data ∗/
                                    
                                    
                                       
                                          /∗ End of compression at the node ∗/
                                    
                                    
                                       
                                          /∗ Generation of packet to be transmitted ∗/
                                    
                                    
                                       7. src=pkt_recvd.own_ID;
                                    
                                    
                                       8. dest=pkt_recvd.parent_ID;
                                    
                                    
                                       9. position_bit=complement (pkt_recvd.position_bit);
                                    
                                    
                                       10. compute packet pkt_transmtd=(src, dest, position_bit, compressed_data);
                                    
                                    
                                       11. transmit pkt_trnsmtd;
                                    
                                    
                                       12. End
                                       
                                    
                                 
                              
                           
                        
                     

Once the data is compressed as described in Section 5.3, it is quantized using a standard technique, e.g. uniform quantization [19]. We propose an encoding scheme in order to reduce the amount of redundant data bits after quantization with a target to reduce transmission energy further. In conventional encoding techniques [20] statistical table needs to be stored at every node and continual update of the table is required. Moreover, periodic broadcast of the recently updated table for the rest of the nodes is essential. These incur a huge overhead in terms of storage, computation and communication. So we propose an encoding technique which can reduce the data volume to be transmitted without the knowledge of the data stored in other nodes.

In the proposed technique, the input quantized bit-stream is traversed from the least significant bit (LSB) till the complement of LSB is found. Now truncating all the bits from LSB till this point, the rest of the bit-stream is transmitted. This technique ensures reduction of data bits for all types of bit-streams except with all ones and all zeros. As input is quantized bit-stream and all the nodes, including the sink are aware of the size of the quantizer, at the receiving end, data can easily be reconstructed only by padding remaining number of bits by the complement of the last bit received. The encoding algorithm which is run at a transmitting node is stated below:
                           
                              
                                 
                                 
                                    
                                       
                                          Encoding algorithm
                                       
                                    
                                    
                                       
                                          Input: Quantized data (data_quantz) and length (data_len) of the quantized data
                                    
                                    
                                       
                                          Output: encoded data (data_encoded)
                                    
                                    
                                       1. Begin
                                       
                                    
                                    
                                       2. x
                                          =LSB of data_quantz; i
                                          =1;
                                    
                                    
                                       3. while (complement of x not found)
                                    
                                    
                                       4.begin
                                    
                                    
                                       5.	traverse the data_quantz from LSB toward MSB
                                    
                                    
                                       6.
                                          
                                          i
                                          =
                                          i
                                          +1
                                    
                                    
                                       7.end
                                    
                                    
                                       8.if (all bits are exhausted)
                                    
                                    
                                       9.
                                          data_encoded=data_quantz
                                    
                                    
                                       10.else
                                    
                                    
                                       11.
                                          data_encoded=truncated (i bits from LSB) data_quantz
                                    
                                    
                                       12.end if
                                    
                                    
                                       13. End
                                       
                                    
                                 
                              
                           
                        Finally, we develop another variant of STPDWT with encoding (STPDWTE) by employing the proposed encoding instead of using any existing encoding technique.

@&#PERFORMANCE EVALUATION@&#

This section presents qualitative analysis of STPDWT and STPDWTE in terms of design trade-off between computation overhead and sensory data loss. Moreover, this section presents a qualitative comparison between the proposed scheme and its variant in terms of impact of the proposed encoding.

The design of the proposed audio compression technique resolves the trade-off between processing complexity/computation overhead and loss of sensory data. Considering the mote with Strong-ARM SA-1100 [18] micro controller and by using Eq. (2) we first tabulate computation overhead in terms of execution time as shown in Table 2
                           .

To maintain an acceptable audio quality [17], sampling rate of 44.1kHz needed (to achieve audio CD quality), which means that between two consecutive data samples time interval Tsampling
                           
                           =1/44.1×1000=0.267×10−6
                           s. We observe from the table that for both the processor speed (133 & 206MHz) execution time T
                           pDWT
                           <
                           Tsampling
                           , and hence no data sample is missed.

Similar to STPDWT, considering the mote with Strong-ARM SA-1100 [18] micro controller and by using Eq. (2) we first tabulate computation overhead (assuming m bits are truncated after encoding) in terms of execution time as shown in Table 3
                           .

We observe from the above table that STPDWTE requires 4+3m instruction cycles, where m is the number of bits truncated. We can see that computational overhead increases in case of STPDWTE. However, in the next subsection, we show that significant amount of energy in terms of the transmission bit is saved by encoding. Summarily, at the cost of the increase of execution time, the transmission bit is saved thereby saving energy.

The qualitative comparison between STPDWT and its variant, STPDWTE is made in terms of percentage gain, which is defined as follows:
                              
                                 
                                    
                                       Percentage
                                       
                                       gain
                                       =
                                       
                                          
                                             Total
                                             
                                             no
                                             .
                                             
                                             of
                                             
                                             bits
                                             
                                             saved
                                          
                                          
                                             Total
                                             
                                             no
                                             .
                                             
                                             of
                                             
                                             bits
                                             
                                             transmitted
                                             
                                             (
                                             with
                                             
                                             no
                                             
                                             encoding
                                             )
                                          
                                       
                                       ×
                                       100
                                       %
                                    
                                 
                              
                           
                        

Let us consider the quantized data of length n is used as input of encoding scheme and m bits are truncated after employing the encoding where, 
                              
                                 m
                                 <
                                 n
                              
                           . So the number of bits to be transmitted is 
                              
                                 (
                                 n
                                 -
                                 m
                                 -
                                 1
                                 )
                              
                           . Now the 
                              
                                 (
                                 n
                                 -
                                 m
                                 -
                                 1
                                 )
                              
                            locations can be filled-up by 
                              
                                 
                                    
                                       2
                                    
                                    
                                       (
                                       n
                                       -
                                       m
                                       -
                                       1
                                       )
                                    
                                 
                              
                            ways. For each of these cases m bits are saved and total number of bits saved
                           =
                           
                              
                                 m
                                 ×
                                 
                                    
                                       2
                                    
                                    
                                       (
                                       n
                                       -
                                       m
                                       -
                                       1
                                       )
                                    
                                 
                              
                           .


                           Ths, if length of quantizer is n, then total number of bits saved: 
                              
                                 
                                    
                                       B
                                    
                                    
                                       s
                                    
                                 
                                 =
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       n
                                       -
                                       1
                                    
                                 
                                 
                                    
                                       
                                          i
                                          ×
                                          
                                             
                                                
                                                   
                                                      2
                                                   
                                                   
                                                      n
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      2
                                                   
                                                   
                                                      i
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        


                           On the other hand, if encoding is not applied, total number of bits required to be transmitted 
                           
                              
                                 =
                                 
                                    
                                       2
                                    
                                    
                                       n
                                    
                                 
                                 ×
                                 n
                              
                           .
                              
                                 
                                    
                                       So
                                       ,
                                       
                                       the
                                       
                                       percentage
                                       
                                       gain
                                       =
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            ∑
                                                         
                                                         
                                                            i
                                                            =
                                                            1
                                                         
                                                         
                                                            n
                                                            -
                                                            1
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               i
                                                               ×
                                                               
                                                                  
                                                                     
                                                                        
                                                                           2
                                                                        
                                                                        
                                                                           n
                                                                        
                                                                     
                                                                  
                                                                  
                                                                     
                                                                        
                                                                           2
                                                                        
                                                                        
                                                                           i
                                                                        
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             (
                                             
                                                
                                                   2
                                                
                                                
                                                   n
                                                
                                             
                                             ×
                                             n
                                             )
                                          
                                       
                                       =
                                       
                                          
                                             
                                                
                                                   
                                                      1
                                                   
                                                   
                                                      n
                                                   
                                                
                                                ×
                                                
                                                   
                                                      
                                                         ∑
                                                      
                                                      
                                                         i
                                                         =
                                                         1
                                                      
                                                      
                                                         n
                                                         -
                                                         1
                                                      
                                                   
                                                
                                                
                                                   
                                                      i
                                                   
                                                   
                                                      
                                                         
                                                            2
                                                         
                                                         
                                                            i
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       ×
                                       100
                                       %
                                    
                                 
                              
                           
                        

Now, let us analyze to fix the maximum allowable size of the quantizer without missing any sample. Considering n-bit data, we require 
                              
                                 4
                                 +
                                 3
                                 m
                              
                            cycles where, 
                              
                                 m
                                 <
                                 n
                                 .
                              
                            so, the following condition needs to be satisfied to fulfill the target of not allowing any sample to miss:
                              
                                 
                                    
                                       
                                          
                                             T
                                          
                                          
                                             PDWT
                                          
                                       
                                       +
                                       (
                                       4
                                       +
                                       3
                                       m
                                       )
                                       ×
                                       execution
                                       
                                       time
                                       
                                       for
                                       
                                       1
                                       
                                       instruction
                                       
                                       cycle
                                       ⩽
                                       
                                          
                                             T
                                          
                                          
                                             sampling
                                          
                                       
                                    
                                 
                              
                           
                        

Now considering 133MHz processor, we get the following:
                              
                                 
                                    
                                       
                                          
                                             
                                             
                                                
                                                   30.72
                                                   ×
                                                   
                                                      
                                                         10
                                                      
                                                      
                                                         -
                                                         9
                                                      
                                                   
                                                   +
                                                   (
                                                   4
                                                   +
                                                   3
                                                   m
                                                   )
                                                   ×
                                                   7.518
                                                   ×
                                                   
                                                      
                                                         10
                                                      
                                                      
                                                         -
                                                         9
                                                      
                                                   
                                                   ⩽
                                                   0.267
                                                   ×
                                                   
                                                      
                                                         10
                                                      
                                                      
                                                         -
                                                         6
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             
                                             
                                                
                                                   Or
                                                   ,
                                                   
                                                   m
                                                   ⩽
                                                   9
                                                   (
                                                   approx
                                                   )
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

Thus in 133MHz processor, for data up to 9bits, we can encode without missing any data sample. Similarly for 206MHz processor, this number is 16. Therefore, we can fix 9 and 16 as the maximum allowable size of quantizer for 133MHz and 206MHz processors respectively.

Now, if we consider a 9-bit quantizer with all possible values occurring with equal probability, the average gain is around 21.98% and average number of bits truncated is 2 (approx).

As, transmission and reception energy are significantly large compared to computation energy, we save a significant amount of energy by saving 2bits per packet transmission. Computing in the same way, the average gain for 4, 6, 8-bit quantizer is 34.3%, 31.25%, 24.12% respectively.

This section provides the quantitative analysis from two different perspectives. Primarily simulation is performed to estimate energy consumption of the nodes and SNR quality of the reconstructed data. Subsequently, statistical analysis on the simulated result is performed specifically to justify the simulation results.

We have collected data for an hour from a busy traffic of a metropolitan city in India and used as input data. The data have been supplied by one public sector research organization on road research. To the best of our knowledge, during data collection Microflown 1/2in. PU mini probes were placed over a busy road crossing to measure sound intensity of traffic. The probes were placed 25m apart from each other to sense the traffic intensity of around 1900m2. The 1/2in. PU mini is a probe combining two sensors: a traditional microphone and a microflown. It directly measures sound pressure and acoustic particle velocity at one spot.

Experiments are carried out by taking data from the collected sensed audio data set for duration of 17min using MATLAB. A wave (.wav) file has been created extracting the audio from the dataset and is sampled at a sampling rate 44100Hz using standard MATLAB functions ‘wavread’. We have considered a network area of 100m×100m where the Sink is placed at the centre, and 100 sensor nodes are deployed randomly. We consider the first-order energy dissipation model with Eelec
                           
                           =50nJ/bit, εamp
                           
                           =0.0013pJ/bit/m2, εfs
                           
                           =10pJ/bit/m4 
                           [5,15,16]. Extensive simulation has been performed, and an average result of 100 independent runs has been taken while plotting the simulation graphs.

@&#RESULTS AND DISCUSSION@&#

This section provides simulation results of the scheme, including routing followed by statistical analysis of the entire result set.

We simulate the network with 100 nodes and employ the proposed routing (Section 5.1). The simulated result of the proposed tree based routing up to maximum path length of 5 (considering sink as 0) is shown in Fig. 5
                              . Now using this simulated network, we conduct primarily three sets of experiments to evaluate the comparative performance of our scheme.

In each set of experiments, results of the two variants of our scheme spatio-temporal partial DWT (STPDWT) and spatio-temporal partial DWT with encoding (STPDWTE) are compared primarily with one existing competing scheme namely Compressed data stream protocol (CDP), which uses temporal correlation. While implementing CDP, we have plugged the PDWT technique into the protocol proposed by the authors [9]. In addition, the scheme is compared with PDWT and DCT, which are two basic compression schemes.

In the first set of experiments, average energy consumption (including transmission and reception energy) of nodes is plotted (Fig. 6
                              ) with varying quantizer size, which is represented by quantized bits/sensor node. This set of results is important since it guides us in determining the levels of quantization. We observe from the plot that average energy consumption of nodes for all the schemes increases for an increase of quantizer size. We further observe that both the variants of our scheme STPDWT and STPDWTE consume less energy compared to all the competing schemes, DCT, CDP and PDWT. To be more specific, on an average, STPDWT and STPDWTE save 49.03% and 53.76% respectively over DCT. Similarly, these values are 40.4% and 46.07% over CDP and 38.28% and 44% over PDWT.

In the second set of experiment, we measure SNR (signal-to-noise ratio) with varying quantized bits per sensor node as shown in Fig. 7
                              . We observe from the plot that the scheme CDP has the same level of SNR when compared with our scheme. This is because CDP is a transport protocol where any compression method can be plugged in. In this case, we have plugged the same STPDWT in CDP method of compression. This results in achieving same level of SNR; however, STPDWT consumes less energy compared to CDP. In designing STPDWTE, no effort is made to improve SNR; rather attempt has been made to save energy further. So SNR remains same. This justifies the apparent overlapping of these three curves. We also notice that SNR for PDWT and DCT overrides our scheme. However, considering acceptable SNR in a wireless network [17] as 25–40dB, it is observed that difference of SNR between PDWT/DCT, and our scheme is not much significant, when the target is to save considerable energy. Finally, we notice that to achieve this acceptable range of SNR, quantizer size needs to be 5–6bits.

Now from Fig. 6, we see that in the above mentioned range of quantized bit/sensor, the average energy consumption of DCT, CDP and PDWT is significantly more than that of our scheme. Therefore, we conclude that our scheme gives acceptable SNR [17] with much less energy consumption compared to all the competing schemes.

In the third set of experiment, average energy consumed by the network is plotted against SNR in Fig. 8
                              . This helps us to find out the design trade-off between these two parameters. If we need more improvement in terms of SNR at the sink, the more energy has to be consumed at each node and therefore, the nature of the curve is justifiable. Further, to achieve 25–40dB acceptable SNR [17], while STPDWTE and STPDWT require 48.1μJ and 59μJ respectively, the PDWT, CDP, DCT requires 78.3μJ, 130μJ and 119μJ respectively. This clearly proves our scheme’s dominance over PDWT, CDP and DCT based schemes.

Now to validate our simulation, we repeat the experiments by plotting energy (Fig. 9
                              ) and SNR (Fig. 10
                              ) with varying number of runs at 95% confidence interval. We observe from Fig. 9, that for all the schemes, the mean of the energy consumption remains almost same irrespective of the number of runs. For example, STPDWT has a mean energy consumption of 67.8μJ, 67.32μJ and 67.6μJ for 10, 30 and 60 runs respectively while for STPDWTE, these values are 58.03μJ, 57.7μJ and 57.5μJ. So there is a 0.7% variation for STPDWT and 0.9% variation for STPDWTE. This indicates the stability of the schemes.

Similarly, in Fig. 10 where the confidence interval of SNR is plotted against the number of runs at 95% confidence interval, we observe that scheme STPDWT has a mean SNR 46.1dB, 46.16dB and 46.16dB for 10, 30 and 60 runs respectively while for STPDWTE, these values are 46.2dB, 46.1dB and 46.1dB. This justifies the precision and accuracy of the plots which are very much consistent with the plots of Figs. 6 and 7 for same set of parameters.

We proceed further to check, whether variation of the number of nodes deployed affect energy consumption and SNR. We observe from Fig. 11
                               that the plots follow the same pattern of energy consumption in line of the plots of Fig. 6. This implies that number of nodes has no influence on the comparative performance. However, in Fig. 12
                               we notice that as the number of nodes in the network is increased there is degradation of SNR. As the number of node increases, number of hops increases in routing the data. Thus at every hop, there will be quantization error resulting in degradation of SNR. These two plots provide us a design guideline to decide the number of nodes needs to be deployed to achieve a target SNR with affordable energy consumption.

We gather the sensory and reconstructed data after running the simulation and statistically analyze the data so found to study the following.

Box plot is a statistical analysis which provides a convenient way of graphically depicting groups of numerical data through their quartiles. Fig. 13
                              (a) shows that the median and the data ranges of sensory data, and both the reconstructed data (using PDWT and STPDWT) have almost same median and range of values. But, in case of noise we see in both the cases, range is very small and close to 0. Also noise has a very small range. This proves the magnitude of the noise is significantly smaller than that of the reconstructed data.


                              Fig. 13(b) depicts the reduction of noise with an increase of quantizer size, signifying betterment of SNR.

Another significant observation is that for all the three quantizer sizes (2-bit, 8-bit, 15-bit) there are more whiskers in case of STPDWT than PDWT, which explains why we get little low SNR in case of STPDWT than PDWT.


                              Fig. 14
                               shows the histogram of sensory data and reconstructed data using PDWT and STPDWT with quantizer sizes 2bit, 8bit, 15bit. From the figure, we notice that in both the cases, reconstructed data is almost equal to the sensory data when 15bit quantizers are used, whereas the difference increases as the quantizer size reduces. This explains the improvement of SNR with an increase in quantizer size.

Another important observation is, for the same quantizer size, the sensory data is closer to reconstructed data in PDWT than STPDWT. This is the reason why PDWT results in slightly better SNR.

@&#CONCLUSION@&#

The present audio compression scheme using lifting based partial discrete wavelet transform for WMSN exploits both spatial and temporal correlation in an integrated manner toward achieving energy-saving while maintaining an acceptable SNR. In view of the fact that implementation of conventional statistical table based encoding scheme requires a huge overhead in terms of computation and communication, we propose a light-weight encoding technique with a target to reduce energy consumption further after compression. Care has also been taken to reduce energy wastage by proposing a tree based pre-computed routing, which guarantees reduction of energy wastage due to backward data flow toward the sink. Finally, the performance of the scheme is measured both qualitatively and quantitatively. When compared to existing schemes, our scheme is superior not only in terms of energy consumption but also in getting an acceptable SNR. Our observation on simulation results is also justified by statistical analysis.

As a future extension, the proposed scheme may be improved by considering two or three-dimensional data sets, e.g. image and video with more viable routing and encoding schemes.

@&#REFERENCES@&#

