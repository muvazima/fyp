@&#MAIN-TITLE@&#Automatic differentiation of melanoma from dysplastic nevi

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           Melanoma vs. dysplastic classification.


                        
                        
                           
                           The features extracted globally outperformed the local approach.


                        
                        
                           
                           Individual texture features and their combination achieved better results than others.


                        
                        
                           
                           Random forest achieved better results in comparison with SVM and GB.


                        
                        
                           
                           The framework achieved the highest SE of 98% and SP of 70%.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Melanoma

Dysplastic

Dermoscopy imaging

Classification

Machine learning

Texture

Colour

Shape features

@&#ABSTRACT@&#


               
               
                  Malignant melanoma causes the majority of deaths related to skin cancer. Nevertheless, it is the most treatable one, depending on its early diagnosis. The early prognosis is a challenging task for both clinicians and dermatologist, due to the characteristic similarities of melanoma with other skin lesions such as dysplastic nevi. In the past decades, several computerized lesion analysis algorithms have been proposed by the research community for detection of melanoma. These algorithms mostly focus on differentiating melanoma from benign lesions and few have considered the case of melanoma against dysplastic nevi. In this paper, we consider the most challenging task and propose an automatic framework for differentiation of melanoma from dysplastic nevi. The proposed framework also considers combination and comparison of several texture features beside the well used colour and shape features based on “ABCD” clinical rule in the literature. Focusing on dermoscopy images, we evaluate the performance of the framework using two feature extraction approaches, global and local (bag of words) and three classifiers such as support vector machine, gradient boosting and random forest. Our evaluation revealed the potential of texture features and random forest as an almost independent classifier. Using texture features and random forest for differentiation of melanoma and dysplastic nevi, the framework achieved the highest sensitivity of 98% and specificity of 70%.
               
            

@&#INTRODUCTION@&#

Malignant melanoma is a type of skin cancer and although it accounts for less than 2% of all skin cancer cases, it is the deadliest type and causes the vast majority of deaths [1]. According to the latest report, melanoma caused over 20,000 deaths annually in Europe [2]. American Cancer Society also reported the estimated deaths of melanoma in 2013 as 9480 individuals and new cases as 76,690 individuals. Nevertheless melanoma is the most treatable kind of cancer emphasizing the importance of early diagnosis. Currently in the clinical field, the “ABCDE” rule is the clinical routine which is used to detect malignant melanoma at its earliest stage [3]. This routine seeks for characteristics synonymous with malignant melanoma at its early stage and can be listed as: asymmetry, irregular borders, variegated colours, diameters greater than 6mm and evolving stages. The detection is performed by visual inspection and further analysis, using clinical imaging techniques such as dermoscopic imaging. Visual inspection, similarity between different lesions and the necessity to perform patient follow-up over years make the diagnosis task difficult for the dermatologists and more prone to errors notably the repeatability of the detection. Due to this fact and to the importance of early diagnosis of melanoma, the research communities have dedicated their efforts to developing computerized lesion analysis algorithms. These computer-aided systems implement automatic processing to facilitate the task of specialists and focus on different aspects such as segmentation, detection and classification of the lesions. In this work we focus on classification task, particularly discriminating melanoma and dysplastic nevi. It has been reported that from 2 to 8% of the Caucasian population have dysplastic nevi (also known as atypical moles, i.e. unusual benign moles that may resemble melanoma) [4]. Individuals who have dysplastic nevi syndrome or dysplastic nevi with family history of melanoma face a higher risk of developing melanoma. However, only a small number of these dysplastic nevi might develop into melanoma and most dysplastic nevi will never become cancer [5]. Notably discrimination of these two categories is more challenging due to their similarities [6].

In the past decade, different approaches have been proposed for detection of melanoma. The developed methods used the common classification framework in the computer vision field: the pipeline of this framework usually consists of the four following steps: (i) segmentation, (ii) feature detection, (iii) feature selection/extraction and (iv) classification. Furthermore, those methods were based on two different screening imaging techniques, either clinical or dermoscopic images. Clinical imaging was the first screening technique and the initial research focused on this modality. This technique acquires images (digital or not) of skin lesions and represents what a clinician sees with naked eyes [29].

Later, clinical imaging was replaced by a more suitable technique such as dermoscopy imaging (also known as epiluminescence microscopy). This techniques has been used extensively by dermatologists and researchers, and several frameworks based on this modality has been proposed by the research community to detect malignant melanoma [30–32].


                     Fig. 1
                     , depicts a summary of most of these methods by reporting their results in terms of sensitivity, specificity and dataset size. Sensitivity or recall (SE) refers to the number of correctly identified cancer cases to the total number of cancer cases in the dataset and specificity or true negative rate (SP) refers to the proportion of negatives or correctly identified non-cancer cases to the total number of non-cancer cases in the dataset. The methods are also categorized based on their differentiation scope, melanoma from benign (M vs B), melanoma from benign and dysplastic (M vs B+D) and melanoma versus dysplastic only (M vs D). The summary of the state of the art methods showed that fewer attention has been paid to the discrimination of melanoma and dysplastic in the past, which we believe is more challenging for both specialists and automated algorithms.

It is difficult to offer a fair comparison among the methods since their performances are reported in the literature using different datasets. These methods also use multiple machine learning approaches to distinguish melanoma lesions, such as AdaBoost (AdB), artificial neural network (ANN), support vector machines (SVM), k-nearest neighbour (kNN) or random forest (RF). However, AdB, SVM and ANN appear to be the most popular methods. The detected features to feed these classification methods can also be categorized. Table 1
                      summarizes these features with their corresponding references, by using a similar methodology as in [29].

Features can be extracted either from a global or local manner. A global feature is extracted by taking the lesion as a whole, while local features are extracted more densely. A grid can be defined over the lesion and each descriptor is extracted for each small portion of the grid. The use of local features increases the size of the feature vector. It also increases its computation cost and the complexity of the feature space. Hence, the bag of features (BoF) approach was commonly used to tackle these drawbacks [14,17,37,13,22,12,38].

In this work we focus our research on dermoscopic images and propose an automatic framework for detection of melanoma from dysplastic lesions since their distinction, based on their similarities, is more challenging for both the classifier algorithm, the general practitioner and even the expert dermatologists and fewer studies have addressed the issues (see Fig. 1). We evaluate the performance of well-known texture descriptors besides common colour and shape features to discriminate the two classes. To the best of our knowledge although a combination of colour, shape and texture features were used previously [25,23,21], these features were mostly designed to mimic the characteristics of “ABCD” rule. In this research we consider those features besides well-known texture features such as local binary pattern, grey-level co-occurrence matrix, Gabor filter and histogram of gradients and present a comparison between their individual and combined performances, while they are extracted by both global and local methods. We also consider three classifiers such as Support Vector Machine, Gradient Boosting and Random Forest.

The rest of the paper is organized as follows: Section 2 describes the proposed framework. Section 3 illustrates the experiments and proposed validation. The results and discussion are illustrated in Section 4 and finally, the conclusions are presented in Section 5.

Different sections of our pipelines as illustrated in Fig. 2
                      are explained in the following.

Previous to classification, we perform lesion segmentation on the dermoscopic images to focus our processing only in the lesion area. It was observed through empirical validation, that the CIE XYZ was the most suitable colour space in which segmentation can be performed. More precisely, all lesions are segmented by analysing the probability distribution of Z component (see Fig. 3
                        b). In the probability distribution, a Gaussian mixture with two components can be observed. The first Gaussian corresponds to the pixel intensities of the lesion whereas the second Gaussian corresponds to the pixel intensities belonging to the skin. Thus, finding the valley between these two Gaussian bells allows us to separate the lesion from the skin as depicted in Fig. 3a. The valley corresponds to the global minimum between the two Gaussian distributions. This global minimum is detected by finding, first the peaks (red crosses in Fig. 3b) of the smoothed probability density distribution and then finding the minimum between them (green cross in Fig. 3). The original distribution is smoothed using cubic interpolation.

Reviewing the literature, one could observe that some features have been used more widely than others such as colour and shape characteristics. This is due to the fact that computerized systems are developed to mimic dermatologists’ assessment, by using for instance the discriminative characteristics defined in the clinical “ABCD” rule. Thus, we also incorporate some common shape and colour descriptors, along with texture features. The shape (S) and first colour (C
                        1) features and grey level co-occurrence matrix (T
                        2) are chosen since they have been used widely in the past. However the rest of the features, such as opponent colour space (C
                        2), Local binary pattern (T
                        1), Gabor filter (T
                        3), histogram of oriented gradients (T
                        4) and SIFT(T
                        5) are chosen since they barely have been used for detection of melanoma and they are well-known colour and texture descriptors in pattern recognition. These features are summarized in Table 2
                         and are explained in the following.
                           
                              −
                              
                                 Shape features were created with reference to [39]. This descriptor measures thinness ratio, border asymmetry, distance variance of border points to the centre and statistics (minimum, maximum, average and variance) of gradient operator along the lesion border.


                                 Colour variance and colour histogram is a well-known feature descriptor which has been used widely in the past for detection of melanoma features. This descriptor contains, the mean and variance of nine colour channels (R, G, B
                                 
                                 H, S, V
                                 
                                 L, A, B) and histogram of R, G and B channels. Each histogram for each channel is constructed with 42-bins, leading to a final descriptor size of (9×2)+(42×3)=144.


                                 Opponent colour space angle and hue histogram were first proposed by [40] as local colour features. These descriptors were chosen due to their robustness to photometric variations (shadow, shading, specularities and changes of the light source) and geometrical variations (viewpoint, zoom and object orientation). These rotation invariant and robust descriptors are derived from RGB channels using the following equations:
                                    
                                       (1)
                                       
                                          
                                             
                                                
                                                   
                                                   hue
                                                   =
                                                   arctan
                                                   
                                                      
                                                         
                                                            3
                                                         
                                                         (
                                                         R
                                                         −
                                                         G
                                                         )
                                                      
                                                      
                                                         R
                                                         +
                                                         G
                                                         −
                                                         2
                                                         B
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      ang
                                                      dim
                                                      O
                                                   
                                                   =
                                                   arctan
                                                   
                                                      
                                                         
                                                            3
                                                         
                                                         (
                                                         
                                                            R
                                                            dim
                                                            ′
                                                         
                                                         −
                                                         
                                                            G
                                                            dim
                                                            ′
                                                         
                                                         )
                                                      
                                                      
                                                         
                                                            R
                                                            dim
                                                            ′
                                                         
                                                         +
                                                         
                                                            G
                                                            dim
                                                            ′
                                                         
                                                         −
                                                         2
                                                         
                                                            B
                                                            dim
                                                            ′
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                
                                             
                                          
                                       
                                    
                                 where dim denotes the spatial coordinates of (x, y) and 
                                    
                                       R
                                       dim
                                       ′
                                    
                                    ,
                                    
                                       G
                                       dim
                                       ′
                                    
                                    ,
                                    
                                       B
                                       dim
                                       ′
                                    
                                  denote the first order derivatives of R, G, B with respect to the coordinates.

The colour feature descriptor is created by taking histogram of opponent angle 
                                    
                                       ang
                                       dim
                                       O
                                    
                                  and hue channels. These feature descriptors are created with 42 bins as well.


                                 Completed local binary pattern (CLBP) is a discriminative rotation invariant feature descriptor proposed by Guo et al. [41]. CLBP is a completed version of local binary pattern (LBP) descriptor, proposed by [42]. In both descriptors, a central pixel (g
                                 
                                    c
                                 ) in a defined neighbourhood by R radius is compared to its neighbourhood pixels (g
                                 
                                    p
                                 , with distance of R from the central pixel) and their differences are encoded in terms of binary patterns. The binary patterns are calculated for each pixel in the given image and their histogram, defines the final descriptor. The CLBP is a complete version of LBP since, beside considering the sign of the differences as LBP, it also considers the magnitude differences, from the central point to its neighbourhood, and the grey level of the central points. Fig. 4
                                  represents this process. In this figure, the sign, magnitude and central grey level binary pattern are represented by CLBP
                                 
                                    S
                                 , CLBP
                                 
                                    M
                                  and CLBP
                                 
                                    C
                                  which are created by encoding the local distance components and the central grey levels to binary patterns. In the proposed framework, we consider the radius of 24 pixels and we calculate the rotation invariant, uniform and normalized CLBP features. This radius was chosen after testing the algorithm with different neighbourhoods such as {8, 16, 24}.


                                 Grey-level co-occurrence matrix (GLCM) is one of earliest texture descriptor methods, proposed by Haralick et al. [43]. This approach has been used widely for texture analysis applications including melanoma detection. In this approach, texture features are extracted based on statistics measurement of co-occurrence probabilities. The co-occurrence distribution represents the occurrence probabilities of all pairwise combinations of the gray levels in the defined window [44]. In other words, it counts how often a pixel with gray intensity of i occurs adjacent to a pixel with gray intensity of j. The spatial distance and orientation of interests, between the pixels are defined by distance D and angle of θ. The co-occurrence probability between grey level i and j is defined using expression (2), where P
                                 
                                    ij
                                  represents the conditional probability of occurrences of grey value i adjacent to grey value of j, given the distance and orientation of D and θ, respectively and G is the quantized number of grey levels.
                                    
                                       (2)
                                       
                                          
                                             C
                                             ij
                                          
                                          =
                                          
                                             
                                                
                                                   P
                                                   ij
                                                
                                             
                                             
                                                
                                                   ∑
                                                   
                                                      i
                                                      ,
                                                      j
                                                      =
                                                      1
                                                   
                                                   G
                                                
                                                
                                                   P
                                                   ij
                                                
                                             
                                          
                                       
                                    
                                 
                              

In this framework, we quantize the images to 32 grey levels and calculate co-occurrence probabilities given the distance (D) of 9 pixels and four different orientations of {θ
                                 =0°, 45°, 90°, 135°}. For each orientation 22 texture features as a combination of features proposed by [43–45] are calculated. The final texture descriptor is an average of stated four measurements. This approach allows us to have rotation invariant descriptor. The proposed distance and quantized value were chosen after testing variety of distances {1, 3, 7, 9} and quantized grey-levels {16, 32, 64}.


                                 Gabor filter is a linear filter which extracts edges and texture information from the image and was found to perform similar to human visual perception. Gabor filter is defined as a modulation of a Gaussian kernel with a sinusoidal wave. The Gabor expression is shown in Eq. (3). This function is basically a Gaussian with standard deviations of σ
                                 
                                    x
                                  and σ
                                 
                                    y
                                  that vary along x and y axes and it is modulated by a complex sinusoidal with a wavelength of λ. In this equation, θ represents the orientation of the Gabor filter and ψ is the phase offset and s is the scale factor.
                                    
                                       (3)
                                       
                                          
                                             
                                                
                                                   g
                                                   (
                                                   x
                                                   ,
                                                   y
                                                   ;
                                                   θ
                                                   ,
                                                   ψ
                                                   ,
                                                   
                                                      σ
                                                      x
                                                   
                                                   ,
                                                   
                                                      σ
                                                      y
                                                   
                                                   )
                                                   =
                                                   exp
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  −
                                                                  1
                                                               
                                                               2
                                                            
                                                            
                                                               
                                                                  
                                                                     
                                                                        
                                                                           
                                                                              x
                                                                              
                                                                                 ′
                                                                                 2
                                                                              
                                                                           
                                                                        
                                                                        
                                                                           
                                                                              σ
                                                                              x
                                                                              2
                                                                           
                                                                        
                                                                     
                                                                     +
                                                                     
                                                                        
                                                                           
                                                                              y
                                                                              
                                                                                 ′
                                                                                 2
                                                                              
                                                                           
                                                                        
                                                                        
                                                                           
                                                                              σ
                                                                              y
                                                                              2
                                                                           
                                                                        
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                   
                                                   cos
                                                   
                                                      
                                                         
                                                            2
                                                            π
                                                            
                                                               
                                                                  
                                                                     x
                                                                     ′
                                                                  
                                                               
                                                               λ
                                                            
                                                            +
                                                            ψ
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      x
                                                      ′
                                                   
                                                   =
                                                   s
                                                   (
                                                   x
                                                   cos
                                                   θ
                                                   +
                                                   y
                                                   sin
                                                   θ
                                                   )
                                                
                                             
                                             
                                                
                                                   
                                                      y
                                                      ′
                                                   
                                                   =
                                                   s
                                                   (
                                                   −
                                                   x
                                                   sin
                                                   θ
                                                   +
                                                   y
                                                   cos
                                                   θ
                                                   )
                                                
                                             
                                          
                                       
                                    
                                 
                              

In this work as described by [46], the images are convolved with a set of Gabor filters characterized by different orientations and scales. The features are created by considering the mean and variance of the resulting filtering. We used 6 different orientations ({π/6, π/3, π/2, 2π/3, 5π/6, π}) along 4 scales, downsizing by the factor of two between each scale.


                                 Histogram of oriented gradient was proposed by [47]. In simple terms, HoG counts occurrence of gradient orientations in a localized patches of an image. This descriptor is invariant to geometric and photometric transformations. HoG calculates the gradient values of the image by applying derivative filters ([−1, 0, 1], [−1, 0, 1]
                                    T
                                 ) or Sobel mask to the image and measuring the magnitude and orientations of the gradients. It then creates the weighted histograms of gradient orientations in patches of the image. The orientations are either weighted by magnitude of gradients or a function of gradient magnitudes. In order to account for changes in illumination and contrast, the cells are grouped into larger spatially connected blocks and the final descriptor is a vector of the normalized cell histograms from all of the blocks.


                                 SIFT or Scale invariant feature transform, proposed by Lowe et al.[48], is a dense rotation, translation and scale invariant feature descriptor, which is used successfully by computer vision society. This descriptor, first identifies the key locations by convolving the original image (I(x, y)) with Gaussian kernels (G(x, y, kσ)) at different scales (kσ) and defining the maxima and minima of difference of successive Gaussian blurred images (L(x, y, kσ)).
                                    
                                       (4)
                                       
                                          
                                             
                                                
                                                   D
                                                   (
                                                   x
                                                   ,
                                                   y
                                                   ,
                                                   σ
                                                   )
                                                   =
                                                   L
                                                   (
                                                   x
                                                   ,
                                                   x
                                                   ,
                                                   
                                                      k
                                                      i
                                                   
                                                   σ
                                                   )
                                                   −
                                                   L
                                                   (
                                                   x
                                                   ,
                                                   x
                                                   ,
                                                   
                                                      k
                                                      j
                                                   
                                                   σ
                                                   )
                                                
                                             
                                             
                                                
                                                   
                                                   
                                                   L
                                                   (
                                                   x
                                                   ,
                                                   y
                                                   ,
                                                   
                                                      k
                                                      i
                                                   
                                                   σ
                                                   )
                                                   =
                                                   G
                                                   (
                                                   x
                                                   ,
                                                   y
                                                   ,
                                                   
                                                      k
                                                      i
                                                   
                                                   σ
                                                   )
                                                   *
                                                   I
                                                   (
                                                   x
                                                   ,
                                                   y
                                                   )
                                                
                                             
                                          
                                       
                                    
                                 
                              

In the next step, the identified key-points are filtered, to reject the ones with low contrast or the ones localized along the edges. The remaining key-points are then assigned with one or more orientations, based on local image gradient directions. In this step the magnitude and orientation gradient for each pixel in a neighbouring region of the key-point in the Gaussian blurred image L is calculated, and a 36-bin histogram is defined by adding weighted measures of the orientations. Each orientation is weighted by its magnitude and a Gaussian-weighted window. The key-point is then assigned with the highest orientation peak of the histogram and the local peaks within its 80%. The final key-point descriptor is then created by taking a 16×16 neighbourhood around the key-point. This neighbourhood is divided to 16 sub blocks of 4×4 and for each sub block an 8-bin orientation histogram is created. Concatenation of these histograms (i.e. 16 regions) leads to a 128-dimensional feature vector as the final descriptor.

As already stated in the introduction, we will use both global and local features. The global features are extracted from a bounding box defined around the previously segmented lesions. In the case of the local features, we are using the bag of features (BoF) approach which is illustrated in Fig. 5
                        .

Following the segmentation, a patch detection step is carried out by identifying informative regions of the lesions. These patches can either be sampled densely or sparsely [17]. The dense sampling extracts more information regarding the lesion appearance. However, it might retain redundant features. In the contrary sparse sampling is based on detecting salient key points from the most informative regions of the lesions [17]. Our proposed BoF approach is based on the first strategy. Thus first a grid is centred on each segmented lesion and only the patches in which the proportion of lesion is greater than a third of the patch size are selected. Defining N as the number of selected patches for each lesion and d as the number of feature dimensions, then each lesion will be characterized by a N
                        ×
                        d feature matrix (see Fig. 5, “feature extraction”). The next step in bag of features consists of building the dictionary of “visual-words”. All the feature matrices of the training set are concatenated together and k-means clustering method (see Fig. 5, “clustering”) is used to define the “visual-words”. K-means is an iterative algorithm which finds k centroids by alternating assignment and update steps. The assignment steps is based on L
                        2 norm (Euclidean) distance. Different initialization methods can be used in order to assign the initial k clusters [49], here the initial k clusters are selected based on greedy k-means++ method [50]. Depending on the framework and application, different choices of the number of “visual-words” (number of k clusters) can be made. In our framework, this number is found through exhaustive search, varying from 15 to 700 clusters. The k which leads to the highest performance of classifier is chosen as the final number of clusters. Finally, the probability distribution (e.g., histogram) of the “visual-words” of each feature matrix is computed (feature quantization) and is used to feed the classifier to be trained. In the prediction stage, the histogram of the feature matrix corresponding to the new lesion is computed using the previously learned dictionary and, finally, it is classified by the previously trained classifier.

Prior to classification, all the feature sets are linearly rescaled (normalized) between −1 and 1. Feature normalization is essential in order to improve the discrimination capabilities of the similarity measures [51]. Posterior to feature normalization, solely for global features, in order to remove correlated features and to find the optimal dimension, we apply a linear dimension reduction approach, such as principal component analysis (PCA) [52]. The optimum dimension of PCA normally is found by considering the minimum dimension (eigenvectors), accountable for 95% of sum of eigenvalues. However, there is no definitive rule on how to choose the number of eigenvectors. Thus for our framework, the optimum number of dimensions is found through exhaustive search over the feature dimensions. All the possible number of dimensions (from 1 to original size) are tried and the optimal number of dimensions is found when the best classification accuracy is achieved.

In this paper, we provide a comparison between the following well-known classifiers: SVM, GB and RF. A brief description of each classification method is given below.
                           
                              −
                              
                                 Support vector machines: SVM [53] is a well known machine learning approach which aims to separate two classes by finding the best hyperplane which maximizes the margin between these two classes. Maximizing the margin is equivalent to minimizing the norm of the normal vector of the hyperplane with the constraint that no points should lie in the margin and can be solved as an optimization problem.


                                 Gradient boosting: GB is a generalization form of AdB, which is able to use real-value weak learners and minimizes different loss functions [54,55]. GB builds the ensemble in a greedy manner and iteratively selects weak learners and adjust their weights so that they minimize the loss function. Common choice for the weak learners are decision stumps or regression trees while the loss function is generally an exponential loss or a logarithmic loss [56].


                                 Random forest: RF is an ensemble of decision trees and was introduced in [57]. In the classification stage, multiple decision trees are trained with a different bootstrap sample of the original data. Each bootstrap sample of dimension D is used for training one decision tree and at each node of the decision tree, the best split among the randomly (d
                                 <<
                                 D) selected subset of descriptors is chosen. Each tree is grown to its maximum length without any pruning. In the prediction stage, a sample is propagated and voted by each tree and is usually labelled by majority voting.

Except for the shape, which was only used as global feature and for SIFT which was only used as local-feature, for the rest, global-features (from the whole lesion) and local-features (bag of feature) were extracted and we evaluated their individual and combination attributes using the three proposed classifiers. Comparison and the applied experiments are explained in following sections.

The experiments have been carried out using a fraction of the dermoscopy dataset of the Vienna General Hospital. This is a large-scale dataset with over 5000 lesions including 4277 benign lesions, 1002 dysplastic lesions and 101 malignant melanoma images. In this dataset the dysplastic and melanoma lesions were surgically removed and their ground truth were provided by histological diagnosis [20]. This dataset was used as well in [58,39]. Using the proposed segmentation approach we were able to correctly segment and use around 95.43% of the images consisting of 90 melanoma, 950 dysplastic and 4090 benign lesions. The excluded 4.57% of the images were not reliably segmented due to existence of artefacts such as hair and nonuniform illumination.

@&#EVALUATION@&#

In this work, we consider a subset consisting of dysplastic and melanoma lesions. These two classes are chosen since their distinction has been shown to be more challenging. In order to perform the experiments with a balanced dataset, randomly selected a subset composed of a total of 180 lesions with 90 melanoma and 90 dysplastic among the 950 dermoscopic images of dysplastic nevi. Working with unbalanced data could induce a bias towards one of the classes and boost the results only due to the superiority in number of one of the classes.

In order to have a fair and general representation of the dataset, we consider 10 subsets of 180 randomly selected datasets. Melanoma cases (90 lesions) are identical in the 10 subsets. However the 90 cases of dysplastic lesions for each subset are randomly selected among 950 lesions and are not identical. For each set, 70% of the data were used for training, 15% for the validation and finally 15% for testing. The validation set is used for the SVM classifier to find the optimum regularization (γ) and soft margin (C) parameters over 10-fold cross-validation. The validation set is ignored for RF and GB since, RF is using bootstraps to create its tree and GB randomly selects a subset of training data for each split so both are automatically generalizing the data and do not need any additional validation set.

@&#EXPERIMENTS@&#

The algorithm is tested using two experiments, one concerning global features (Exp1) and the other using BoF approach (Exp2). Nineteen feature sets are considered (individual features and their combination) for Exp1. These features are tested using the three classifiers. Excluding the shape features as local-features, and including SIFT, 18 feature sets are considered for Exp2. The optimal number of “visual-words” of BoF approach for each feature set and each classification approach is found empirically. Depending on the classifier and feature set, this number is changing from a few (i.e. 15) to higher number (i.e. 700) of “visual-words”. This is expected, since classifiers such as RF can handle higher feature dimensions in contrary to classifiers such as SVM which performs better on lower feature dimensions.

@&#RESULTS AND DISCUSSION@&#

The obtained results for global-features are illustrated in Table 3
                     . For each feature set the average performance of each classifier is presented over 10 sets, in terms of SE and SP. In the case of melanoma or in general cancer-case classification it is very important to correctly classify the cancer cases i.e. high sensitivity or recall is required. For this reason high sensitivity is considered as the priority measure. In this table, the individual features are sorted with reference to their performance.

According to the listed results in this table, RF yields to the highest sensitivities for most of the cases (12 out of 19 cases, RF outperforms GB and SVM). The highest sensitivity is achieved for the combination of the texture features (T
                     1, T
                     2, T
                     3, T
                     4) and by RF classifier (98.46%). In this case the classifier achieved a specificity of 70%, which is a very good result, considering the difficulties of distinguishing melanoma from dysplastic nevi. The classification results of this set over the 10 randomly selected subsets are also shown in the bottom part of Table 3. These measurements highlight how datasets can affect the classification results and how complicated is to have a fair comparison among the developed methods. Comparing the performance of the individual global-features, it is observed that HoG (T
                     4) outperforms the others, followed by opponent colour angle and hue histogram (C
                     2), colour statistics and RGB histogram (C
                     1) and Gabor filter (T
                     3) respectively. Based on these results and considering machine learning concepts, one can argue that SVM provides the best results considering that the difference between SE and SP is rather small. However as mentioned before, in the case of cancer classification, it is more important not to miss any cancer case than to classify non-cancer cases.

Observing the stated results in Table 3, it is obvious that texture features, in general outperform the colour features. The combination of colour features ({C
                     1, C
                     2}) are highlighted by italic font and the combination of texture features {T
                     1, T
                     2, T
                     3, T
                     4} are underlined in this table, and it could be seen that the three classifiers perform better and achieve higher SE and SP with texture features, in comparison to colour descriptors. This could be explained, due to the variety and unknown conditions of lighting and acquisition process of the images. The images in the dataset were obtained under different conditions and with different equipments, thus are not calibrated in terms of colour and colour information are not representative of feature descriptors. This issue and how different devices often present problems of faithful colour reproduction was mentioned and addressed in [59–63].

The obtained results of Exp2, illustrating the performance of BoF is presented in Table 4
                     . Similar to the previous test, the average performance of each classifier is illustrated over 10 sub-sets, in terms of SE and SP. The suited number of “visual-words” for each feature set is unique and depends on the feature characteristics. Therefore the proper number of “visual-words” for each feature set was found through comprehensive search. Same as previous table, the individual features are listed with reference to their performance in this table as well.

The stated results in Table 4 illustrate that, CLBP (T
                     1) in comparison to other individual features, has a better performance, considering the average of three classifiers (SE and SP of 83.33% and 60.51%, respectively). This result is followed by HoG (T
                     4), Gabor filter (T
                     3) and opponent colour angle and hue histogram (C
                     2). The worst performance is caused by GLCM (T
                     2) descriptor. All three classifiers performed least concerning this feature set. The two highest individual performances were achieved by T
                     1 and C
                     2 descriptors with SVM and RF classifiers, respectively. These descriptors achieved the sensitivity of 88.46% and 81.54%, respectively, although as it is shown in Table 4, the specificity obtained by SVM and T
                     1 is considerably low.

Considering the listed results, it could be observed that combination of local-features did not affect the sensitivity ratio as much as specificity. The best results for the combination of local-features were obtained by combination of two texture features of CLBP and Gabor filter (T
                     1, T
                     3). This set achieved the SE of 84.62% and SP of 56.92% using RF classifier. The second highest SE with combination of features was achieved, when T
                     1, C
                     1 and C
                     2 were joined. This combination achieved SE and SP of 83.08% and 64.62%, respectively. Besides the texture and colour features, we also considered a more traditional feature descriptor for BoF, by using SIFT descriptors. However the results were not satisfactory. SIFT presented a stable performance with the three classifiers, with SE and SP around 73%. However, concerning the importance of SE, it falls behind the other sets.

Comparing the obtained results for the two approaches of local and global features, it was observed that global-features perform muchbetter than local-features. Global-features, while classified with RF, were able to achieve higher sensitivities than 90% in thirteen cases out of nineteen. Considering the poor quality and low contrast of the images, the extracted local-features are expected to be more correlated and so degrading in the classifier performance.

Beside evaluating the features while comparing the performance of the three classifiers on the two tests, the potential of RF was proved, by having the best results in 12 and 14 cases out of 19 and 18 experiments for first and second test, respectively. Beside RF, GB represents a good potential concerning global-features as well, while SVM showed a poor performance in both cases.

As illustrated in Fig. 1, very few works were dedicated to classification of melanoma vs. dysplastic lesions [6,18,19]. Considering the reported SE and SP of the previous methods, the superiority of the proposed framework in this study is evident. However a fair comparison cannot be made since different datasets have been used.

@&#CONCLUSION@&#

In this work we proposed an automatic framework for differentiation of melanoma lesions from dysplastic nevi. These classes were chosen since their distinction, due to their similarities, is a challenging task for both classifiers and specialists. The framework consists of automatic segmentation, feature extraction, feature modification and classification stage. Our proposed segmentation algorithm were able to segment 95.43% of the lesions, while the rest of the images were excluded due to artefacts such as hairs or unbalanced illumination. Further using the segmented lesions, we propose to extract several well-known texture, shape and colour features considering the two approaches of global and local-feature extraction. In the first approach the features were detected from the whole lesion, while in the former approach Bag of Feature method was used and a dictionary of “visual-words” from the locally detected features was created. Finally in classification stage we propose three classifiers, such as SVM, RF and GB to evaluate the extracted features from both approaches. Our proposed framework was able to achieve high sensitivities (above 90%) in most of the cases, which is a very good result, concerning the importance of sensitivity in cancer-detection problems.

This study evaluated the performance of local and global detected features, and it was observed that global features perform better, probably due to the quality of the images since the dataset was created, back in 2001 with older dermoscopes and the images were captured under different conditions. The obtained results of this work on global-features proved the potential of the texture features for detection of melanoma and it highlighted the potential of new feature descriptors in this field such as histogram of oriented gradients rather than GLCM, which was widely used in the past for detection of melanoma. It also highlighted the impact of opponent colour space besides the colour descriptors which was previously used. These two descriptors outperform the others as an individual feature sets. The combination of all the texture features had a significant performance in comparison to colour and shape features as well. In general, texture descriptors allow to obtain better classification performances compared with colour features. This is due to the fact that colour is not an indicative and consistent descriptor through the whole datasets, since images were acquired under different conditions, and could not be colour calibrated. Assessment of the classifiers in this study confirms the potential of RF in comparison with GB and SVM. Random forest is a less parametric classifier, computationally less expensive, requires minimum user interactions and obtains the highest results.

In general the proposed framework in this study has achieved higher sensitivities in comparison to the previous methods.

@&#ACKNOWLEDGEMENT@&#

We would like to acknowledge, Dr. Harald Kittler, Dr. Michael Binder and Dr. Harald Ganster, who kindly provided the dermoscopy dataset of Vienna General Hospital for this research [20].

@&#REFERENCES@&#

