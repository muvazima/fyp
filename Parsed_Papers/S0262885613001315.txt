@&#MAIN-TITLE@&#Human activity recognition in videos using a single example

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A hierarchical structure for accurate video to video matching and event recognition


                        
                        
                           
                           Incorporating contextual information to the “bag of video words” framework


                        
                        
                           
                           Coding spatio-temporal compositions of video volumes by a probabilistic framework


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Action recognition

Bag of video words

Hierarchical codebook

Spatio-temporal contextual information

Probabilistic modeling

Context

Ensemble of volumes

@&#ABSTRACT@&#


               Graphical abstract
               
                  
                     
                        
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

Human activity analysis is required for video surveillance systems, human–computer interaction, sports interpretation, and video retrieval for content-based search engines [1,2]. Moreover, given the tremendous number of video data available online these days, there is a great demand for automated systems that analyze and understand the contents of these videos. Recognizing and localizing human actions in a video is the primary component of such a system, and also the most important, as it affects the performance of the whole system significantly. Although there are many methods to determine human actions in highly controlled environments, this task remains a challenge in real world environments due to camera motion, cluttered background, occlusion, and scale/viewpoint/perspective variations [3–6]. Moreover, the same action performed by two persons can appear to be very different. In addition, clothing, illumination and background changes can increase this dissimilarity [7–9].

To date, in the computer vision community, “action” has largely been taken to be a human motion performed by a single person, taking up to a few seconds, and containing one or more events. Walking, jogging, jumping, running, hand waving, picking up something from the ground, and swimming are some examples of such human actions [1,2,6]. In this paper, our main goal is to address the problem of action recognition and localization in real environments using a hierarchical probabilistic video-to-video matching framework. This problem is also referred to as action spotting 
                     [10]. To achieve this, we have developed a fast data-driven approach, which finds similar videos in a “target” set to a single labeled “query” video. Assuming that the latter contains an action of interest, e.g., walking, we find all videos in the target set that that are similar to the query, which implies the same activity. This video-to-video comparison also makes it possible to label activities, the so-called action classification problem. An overview of the algorithm is presented in Fig. 1
                     . The major benefit of our approach is that it does not require long video training sequences, object segmentation, tracking or background subtraction. The method can be considered as an extension to the original bag of video words (BOV) approach for action recognition.

Although an initial spatio-temporal volumetric representation of human activity may eliminate some pre-processing steps, for example background subtraction and tracking, it suffers from some major drawbacks. For example, in general, BOV-based approaches for activity recognition in the literature involve salient point detection. They usually ignore the geometrical and temporal structure of these visual volumes, as they store STVs in an unordered manner. Also they are unable to handle scale variations (spatial, temporal, or spatio-temporal) because they are too local, in the sense that they consider just a few neighboring video volumes (e.g., five nearest neighbors in [11] or just one neighbor in [4]). To overcome these issues, we have developed a multi-scale, hierarchical codebook of BOVs for densely sampled videos, which incorporates spatio-temporal compositions and their uncertainties. This permits the use of statistical inference to recognize the activities. We also note that, in order to measure similarity between a query and a target dataset, it is necessary to use information regarding the most informative spatio-temporal video volumes (STVs) in the video, i.e., the salient foreground objects. To select these space-time regions, we use the information obtained from our hierarchical BOV method, which in a sense, can be viewed as being a context-based spatio-temporal segmentation method.

In this paper we present a hierarchical probabilistic codebook method for action recognition in videos, which is based on STV construction. The method uses both local and global compositional information of the volumes, which are obtained by dense sampling at various scales. Similar to other volumetric methods, we do not require background subtraction, motion estimation, or complex models of body configurations and kinematics. Moreover, the method tolerates variations in appearance, scale, rotation, and movement.

As shown in Fig. 1, the proposed algorithm consists of two main components, hierarchical codebook construction of salient STVs and an inference mechanism for measuring the similarity between salient STVs of the query and target videos. Hierarchical codebook construction consists of four steps: coding the video to construct STVs and low-level probabilistic codebook formation while considering the uncertainties in the STVs; constructing ensembles of video volumes for each pixel in a video frame containing a large number of STVs and probabilistic models of their spatio-temporal compositions; high-level codebook construction of the ensembles; and finally, analyzing codewords as a function of time in order to construct a codebook of salient regions. The inference mechanism is based on a set of codewords constructed for each query video. It determines the most similar compositions of STVs in the target videos that match the query video. There are two important differences between our proposed hierarchical approach and previously reported ones. First, the latter are unable to handle both local and global compositional information. Second, they always select the informative regions at the lowest level of the hierarchy.

The main contributions of this paper are as follows:
                        
                           •
                           We introduce a hierarchical codebook structure for action detection and labeling. This is achieved by considering a large volume containing many STVs and constructing a probabilistic model of this volume to capture the spatio-temporal configurations of STVs. Consequently, similarity between two videos is calculated by measuring the similarity both between spatio-temporal video volumes and their compositional structures.

We select the salient pixels in the video frames by analyzing codewords obtained at the highest level of the hierarchical codebook's structure. This differs from conventional background subtraction and salient point detection methods.

In order to evaluate the capability of our approach for action matching and classification we have conducted experiments using three datasets: KTH [12], Weizmann [13] and MSR II [14].
                        2
                     
                     
                        2
                        
                           http://research.microsoft.com/en-us/um/people/zliu/ActionRecoRsrc/.
                      Three types of experiments were performed: action matching and retrieval, single dataset video classification, and cross-dataset action recognition. A preliminary version of this paper appeared in the International Conference on Computer and Robot Vision [15]. This paper is different from [15] in the following respects: 1) we provide more detailed descriptions of how the proposed algorithm learns visual context, 2) we have formulated the contextual graphs and similarity measurement in a spatio-temporal context, 3) a multiscale approach is implemented to deal with large variations in the scale of the actions, and 4) effects of different parameters has been evaluated by conducting extensive experiments. The rest of this paper is organized as follows. Section 2 reviews recent work on action recognition. Section 3 describes the proposed approach for hierarchical codebook construction and the steps of the algorithm. Section 4 describes the action matching algorithm. Section 5 then presents the experimental results and finally, Section 6 concludes the paper.

@&#RELATED WORK@&#

Many studies have focused on the action recognition problem by invoking human body models, tracking-based methods, and local descriptors [1]. The early work often depended on tracking [16–19], in which humans, body parts, or some interest points were tracked between consecutive frames to obtain the overall appearance and motion trajectory. Clearly, the performance of these algorithms is highly dependent on tracking, which sometimes fails for real world video data [20]. Recently, tracking a fixed number of interest points between video frames has become more popular than other tracking-based approaches since they are capable of coding some contextual information regarding local spatio-temporal features. This method functions by tracking the interest point features between consecutive frames and thereby obtaining a set of trajectories [21,22,19]. The contextual information is then computed as the spatial relationship between trajectories [21] or temporal associations between interest points on a single trajectory [22]. In addition to the normal issues associated with tracking, these approaches are based on an implicit assumption of a static background, since moving objects in the background might produce trajectories similar to an object in the region of interest.

Alternatively, shape template matching has been employed for activity recognition; e.g., 2D shape matching [23] or its 3D extensions, optical flow matching [13,24,25]. In this case, action templates are constructed to model the actions and used to locate similar motion patterns. Other studies have combined both shape and motion features to achieve more robust results [26,27], claiming that this representation is somewhat robust to object appearance [26]. In a more recent study [27], shape and motion descriptors are employed to construct a shape-motion prototype for human activities in a hierarchical tree structure and action recognition is performed in the joint shape and motion feature space. Although it seems that the previous approaches are likely well suited to action localization, they do require a priori high-level representations of the human motion. Moreover, they depend on such image pre-processing stages as segmentation, object tracking, and background subtraction [28], which are extremely challenging in real-world unconstrained environments.

In order to eliminate such pre-processing, Derpanis et al. [10] have proposed so-called “action templates”. These are calculated as oriented local spatio-temporal energy features that are computed as the response of a set of tuned 3D Gaussian third order derivative filters. Sadanand et al. [29] introduced action banks to make these template-based recognition approaches more robust to viewpoint and scale variations. Recently, tracking and template-based approaches have been combined to improve the action detection accuracy [18,30].

In a completely different vein, models based on a bag of local visual features
                        3
                     
                     
                        3
                        Essentially the probabilistic topic models, such as the Latent Dirichlet Allocation (LDA), can also be considered as BOV approaches since they ignore the spatio-temporal order of the local features.
                      have recently been studied extensively and shown promising results for action recognition [7,31,32,26,33,3,11,4,9,28,34]. These approaches extract and quantize the video data to produce a set of video volumes that form a “visual vocabulary”. In general, the potential real-time performance of these methods is related to the number of video volume samples and their associated features [26]. Usually, these features are gradients (spatial, temporal, or spatio-temporal), body landmarks, or color information. Combining them makes it possible to capture motion and the scene context simultaneously without requiring reliable trajectories of the objects of interest [35]. The video volumes are constructed either by extracting a limited set of interest points or densely sampling the video. In the former, due to the sparse nature of the space–time interest points, the method becomes computationally efficient and hence is popular in the action recognition literature [3,12,36,34,37]. On the other hand, the selection of appropriate interest points that are guaranteed to contain a salient and discriminative motion pattern in their local context is a difficult challenge [38]. In addition, it has been shown recently that densely sampling the video always achieves better results than a sparse set of interest points [39].

A major advantage of using volumetric representations of videos is that it permits the localization and classification of actions using data-driven nonparametric approaches instead of requiring the training of sophisticated parametric models. In the literature, action inference is usually determined by using a wide range of classification approaches, ranging from sub-volume matching [24], nearest neighbor classifiers [40] and their extensions [37], support [32] and relevance vector machines [11], and even more complicated classifiers employing probabilistic Latent Semantic Analysis (pLSA) [3]. On the other hand, Boiman et al. [40] have shown that a rather simple nearest neighbor image classifier in the space of the local image descriptors is equally as efficient as these more sophisticated classifiers. This also implies that the particular classification method chosen is not as serious as might be thought, and that the main challenge for action representation is using appropriate features.

However, we note that classical BOV approaches suffer from a significant challenge. That is, the video volumes are grouped solely based on their similarity, in order to reduce the vocabulary size. Unfortunately, this destroys the compositional information concerning the relationships between volumes [41,3]. Thus, the likelihood of each video volume is calculated as its similarity to the other volumes in the dataset, without considering the spatio-temporal properties of the neighboring contextual volumes. This makes the classical BOV approach excessively dependent on very local data and unable to capture significant spatio-temporal relationships. In addition, it has been shown recently that detecting actions using an “order-less” BOV does not produce acceptable recognition results [7,31,33,38,41–43]. To overcome this challenge, contextual information must be included in the original BOV framework. One solution is to employ visual phrases instead of visual words. This has been proposed in [43] where a visual phrase is defined as a set of spatio-temporal video volumes with a specific pre-ordained spatial and temporal structure. The main drawback of this approach is that it cannot localize different activities in a video frame. Alternatively, the solution presented by Boiman and Irani [7] is to densely sample the video and store all video volumes for a video frame, along with their relative locations in space and time. Consequently, the likelihood of a query in an arbitrary space-time contextual volume can be computed and thereby used to determine an accurate label for an action using just simple nearest neighbor classifiers [40]. However, the main problem with this approach is that it requires excessive computational time and a considerable amount of memory to store all of the volumes as well as their spatio-temporal relationships. We present a competent alternative to this in the next section.

In addition to [7], several other methods have been proposed to incorporate spatio-temporal structure in the context of BOV [61]. These are often based on co-occurrence matrices that are employed to describe contextual information. For example, the well-known correlogram exploits spatio-temporal co-occurrence patterns [4]. However, only the relationship between the two nearest volumes was considered. This makes the approach too local and unable to capture complex relationships between different volumes. Another approach is to use a coarse grid and construct a histogram to subdivide the space-time volumes [35]. Similarly, in [36], contextual information is added to the BOV by employing a coarse grid at different spatio-temporal scales. An alternative that does incorporate contextual information within a BOV framework is presented in [42], in which three-dimensional spatio-temporal pyramid matching is employed. While not actually comparing the compositional graphs of image fragments, this technique is based on the original two-dimensional spatial pyramid matching of multi-resolution histograms of patch features [41]. Likewise in [44], temporal relationships between clustered patches are modeled using ordinal criteria (e.g., equals, before, overlaps, during, after, etc.) and expressed by a set of histograms for all patches in the whole video sequence. Similar to [44], in [45] ordinal criteria are employed to model spatio-temporal compositions of clustered patches in the whole video frame during very short temporal intervals. The main problems associated with this are the large size of the spatio-temporal relationship histograms and the many parameters associated with the spatio-temporal ordinal criteria. In [46] the spatial information is coded through the concatenation of video words detected in different spatial regions as well as data mining techniques, which are used to find frequently occurring combinations of features. Similarly, [47] addresses this issue by using the spatial configuration of the 2D patches by incorporating their weighted sum. In [38], these patches were represented using 3D Gaussian distributions of the spatio-temporal gradient and the temporal relationship between these Gaussian distributions was modeled using HMMs. An interesting alternative is to incorporate mutual contextual information of objects and human body parts by using a random tree structure [28,34] to partition the input space. The likelihood of each spatio-temporal region in the video is then calculated. The primary issue with this approach [34] is that it requires background subtraction, interest point tracking and detection of regions of interest.

Hierarchical clustering seems to be an attractive way of incorporating the contextual structure of video volumes, as well as preserving the compactness of their description [33,11]. Thus a modified version of [7] was presented in [11]. It uses a hierarchical approach, in which a two-level clustering method is employed. At the first level, all similar volumes are categorized. Then clustering is performed on randomly selected groups of spatio-temporal volumes while considering the relationships in space and time between the five nearest spatio-temporal volumes. However, the small number of spatio-temporal volumes involved again makes this method local in nature. Another hierarchical approach is presented in [33], which attempts to capture the compositional information of a subset of the most discriminative video volumes. In all of these proposed solutions to date, although a higher level of quantization in the action space produces a compact subset of video volumes, it also significantly reduces the discriminative power of the descriptors, an issue addressed in [40]. Generally, all of the earlier work described above for modeling the mutual relationships between the video volumes have one or more limitations such as: considering relationships between only a pair of local video volumes [42,4]; being too local and unable to capture interactions of different body parts [33,48]; and considering either spatial or temporal order of volumes [4].

In this paper we present a hierarchical probabilistic codebook method for action recognition and localization in videos. The proposed codebook structure has two important characteristics: it codes the compositional information of the 3D video volumes and selects the most informative ones in the video.

Considering the structure presented in Fig. 1, our aim is to find the similarity between the query and all of the target videos. Our work is based on the bag of space–time features approach in that a set of STVs is used for measuring similarity. The proposed recognition algorithm in Fig. 1 consists of two main steps: densely sampling videos from which hierarchical codebooks are constructed (see Fig. 2
                     ) and using an inference mechanism for finding the appropriate action in the target videos. In this section, we focus on the former and Section 4 describes the inference mechanism. We first explain the sampling strategy and then describe the hierarchical codebook structure.

The first stage of the algorithm is to represent a query video by meaningful spatio-temporal descriptors. This is achieved by dense sampling, thereby producing a large number of spatio-temporal video volumes. Then similar video volumes are clustered to from a codebook. Since this is actually done on-line, frame-by-frame, the codebook is adaptive. The constructed codebook at this level is called the low-level codebook, as illustrated in Fig. 2.

Similar to all BOV approaches, 3D STVs in a video are constructed at the lowest level of the hierarchy. Although there are many methods for sampling the video for volume construction, dense sampling has been shown to be superior to the others in terms of retaining the informative features of a video [61]. Therefore, performance almost always increases with the number of sampled spatio-temporal volumes, making dense sampling the preferable choice [39,7,61].

The 3D spatio-temporal video volumes, 
                              
                                 
                                    v
                                    i
                                 
                                 ∈
                                 
                                    R
                                    
                                       
                                          n
                                          x
                                       
                                       ×
                                       
                                          n
                                          y
                                       
                                       ×
                                       
                                          n
                                          t
                                       
                                    
                                 
                              
                            are constructed by assuming a volume of size nx
                           
                           ×
                           ny
                           
                           ×
                           nt
                            around each pixel (in which nx
                           
                           ×
                           ny
                            is the size of the spatial (image) window and nt
                            is the depth of the video volume in time). Spatio-temporal volume construction is performed at several spatial and temporal scales of a Gaussian space-time video pyramid. This yields a large number of volumes at each pixel in the video. Fig. 2 illustrates the process of spatio-temporal volume construction. These volumes are then characterized by a descriptor, which is the histogram of the spatio-temporal oriented gradients in the video, expressed in polar coordinates [49,51]. Assume that Gx
                            (x,y,t) and Gy
                            (x,y,t) are spatial gradients and Gt
                            (x,y,t) is the temporal gradient for each pixel at (x,y,t). The spatial gradient used to calculate the 3D gradient magnitude is normalized to reduce the effect of local texture and contrast. Hence, let:
                              
                                 (1)
                                 
                                    
                                       
                                          
                                             
                                                G
                                                s
                                             
                                             
                                                x
                                                y
                                                t
                                             
                                             =
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               G
                                                               x
                                                            
                                                            
                                                               x
                                                               y
                                                               t
                                                            
                                                         
                                                      
                                                      2
                                                   
                                                   +
                                                   
                                                      
                                                         
                                                            
                                                               G
                                                               y
                                                            
                                                            
                                                               x
                                                               y
                                                               t
                                                            
                                                         
                                                      
                                                      2
                                                   
                                                
                                             
                                             ,
                                             
                                             
                                                x
                                                y
                                                t
                                             
                                             ∈
                                             
                                                v
                                                i
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      G
                                                      ˜
                                                   
                                                
                                                s
                                             
                                             
                                                x
                                                y
                                                t
                                             
                                             =
                                             
                                                
                                                   
                                                      G
                                                      s
                                                   
                                                   
                                                      x
                                                      y
                                                      t
                                                   
                                                
                                                
                                                   
                                                      ∑
                                                      
                                                         
                                                            x
                                                            y
                                                            t
                                                         
                                                         ∈
                                                         
                                                            v
                                                            i
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         G
                                                         s
                                                      
                                                      
                                                         x
                                                         y
                                                         t
                                                      
                                                      +
                                                      
                                                         ε
                                                         max
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where 
                              
                                 
                                    
                                       G
                                       ˜
                                    
                                 
                                 s
                              
                            is the normalized spatial gradient and ε
                           max is a constant, set to 1% of the maximum spatial gradient magnitude in order to avoid numerical instabilities. Hence, the 3D normalized gradient is represented in polar coordinates (M (x,y,t), θ (x,y,t), ϕ (x,y,t)):
                              
                                 (2)
                                 
                                    
                                       
                                          
                                             M
                                             
                                                x
                                                y
                                                t
                                             
                                             =
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  G
                                                                  ˜
                                                               
                                                               s
                                                            
                                                            
                                                               x
                                                               y
                                                               t
                                                            
                                                         
                                                      
                                                      2
                                                   
                                                   +
                                                   
                                                      
                                                         
                                                            
                                                               G
                                                               t
                                                            
                                                            
                                                               x
                                                               y
                                                               t
                                                            
                                                         
                                                      
                                                      2
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             θ
                                             
                                                x
                                                y
                                                t
                                             
                                             =
                                             
                                                
                                                   tan
                                                
                                                
                                                   −
                                                   1
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         G
                                                         y
                                                      
                                                      
                                                         x
                                                         y
                                                         t
                                                      
                                                   
                                                   
                                                      
                                                         G
                                                         x
                                                      
                                                      
                                                         x
                                                         y
                                                         t
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             ϕ
                                             
                                                x
                                                y
                                                t
                                             
                                             =
                                             
                                                
                                                   tan
                                                
                                                
                                                   −
                                                   1
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         G
                                                         t
                                                      
                                                      
                                                         x
                                                         y
                                                         t
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            
                                                               G
                                                               ˜
                                                            
                                                            s
                                                         
                                                         
                                                            x
                                                            y
                                                            t
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where M (x,y,t) is the 3D gradient magnitude, and ϕ (x,y,t) and θ (x,y,t) are the orientations within 
                              
                                 
                                    
                                       −
                                       π
                                    
                                    2
                                 
                                 
                                    π
                                    2
                                 
                              
                            and [−π, π], respectively. The descriptor vector for each video volume, taken as a histogram of oriented gradients (HOG), is constructed using the quantized θ and ϕ into nθ
                            and nϕ
                            bins, respectively, weighted by the gradient magnitude, M. The descriptor of each video volume will be referred to as 
                              
                                 
                                    h
                                    i
                                 
                                 ∈
                                 
                                    R
                                    
                                       
                                          n
                                          θ
                                       
                                       +
                                       
                                          n
                                          ϕ
                                       
                                    
                                 
                              
                           . This descriptor represents both motion and appearance and possesses some degree of robustness to unimportant variations in the data, such as illumination changes [49]. However, it should be noted that our algorithm does not rely on a specific descriptor for the video volumes, and other descriptors might enhance the performance of the approach. Examples of more complicated descriptors are the ones in [9], the spatio-temporal gradient filters in [52], the spatio temporal oriented energy measurements [10] and the popular three-dimensional Scale Invariant Feature Transform (SIFT) [50].

As the number of these volumes is extremely large (for example, about 106 in a one minute video) it is advantageous to group similar STVs to reduce the dimensions of the search space. This is commonly performed in all BOV approaches [42,9,61]. Here, similar video volumes are also grouped when constructing a codebook. The procedure is straightforward [15,61]. The first codeword is made equivalent to the first observed spatio-temporal volume. After that, by measuring the similarity between each observed volume and the codewords already existing in the codebook, either the codewords are updated or a new one is formed. Then, each codeword is updated with a weight of w
                           
                              i,j
                           , which is based on the similarity between the volume and the existing codewords. Here, we utilize the Euclidean distance for this purpose. Thus, the normalized weight of assigning codeword cj
                            to video volume vi
                            is given by
                              4
                           
                           
                              4
                              Throughout the rest of the paper, each video volume will be represented by its descriptor vector.
                           :
                              
                                 (3)
                                 
                                    
                                       
                                          w
                                          
                                             i
                                             ,
                                             j
                                          
                                       
                                       =
                                       
                                          1
                                          
                                             
                                                ∑
                                                j
                                             
                                             
                                                
                                                   1
                                                   
                                                      distance
                                                      
                                                         
                                                            v
                                                            i
                                                         
                                                         
                                                            c
                                                            j
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       ×
                                       
                                          1
                                          
                                             distance
                                             
                                                
                                                   v
                                                   i
                                                
                                                
                                                   c
                                                   j
                                                
                                             
                                          
                                       
                                       .
                                    
                                 
                              
                           
                        

Another important parameter is the number of times, fj
                           , that a codeword has been observed [61]. The codebook is continuously being pruned to eliminate codewords that are either infrequent or very similar to the others, which ultimately generates 
                              
                                 
                                    M
                                    L
                                 
                              
                            different codewords that are taken as the labels for the video volumes, 
                              
                                 
                                    C
                                    L
                                 
                                 =
                                 
                                    
                                       
                                          
                                             c
                                             i
                                          
                                       
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       M
                                       L
                                    
                                 
                              
                           .

After the initial codebook formation,
                              5
                           
                           
                              5
                              Recall that initialization requires a minimum of one video frame.
                            each new 3D volume, vi
                           , can be assigned to all labels, cj
                           's, with a degree of similarity, w
                           
                              i,j
                           , as shown in Fig. 3a. We note that the number of labels (shown in color), 
                              
                                 
                                    M
                                    L
                                 
                              
                           , is much less than the number of volumes, N. Moreover, codebook construction can be performed using any other clustering method, such as k-means, online fuzzy c-means [51], or mutual information [42].

At the previous step, similar video volumes were grouped in order to construct the low level codebook. The outcome of this is a set of similar volumes, clustered regardless of their positions in space and time. This is the point at which all other BOV methods stop. As stated in the previous section, the main drawback of many BOV approaches is that they do not consider the spatio-temporal composition (context) of the video volumes. Certain methods for capturing such information have appeared in the literature (see [7,41,47]). In this paper, we present a probabilistic framework for quantifying the arrangement of the spatio-temporal volumes.

Suppose a new video is to be analyzed; we refer to it as the query. The goal is to measure the likelihood of each pixel in the target videos given the query. To accomplish this, it is necessary to analyze the spatio-temporal arrangement of the volumes in the clusters that have been determined in Section 3.1. Thus, we next consider a large 3D volume around each pixel in (x,y,t) space. This large region contains many volumes with different spatial and temporal sizes as shown in Fig. 3b. Thus it captures both the local and more distant information in the video frames. Such a set is called an ensemble of volumes around the particular pixel in the video. The ensemble of volumes, E(x,y,t), surrounding each pixel (x,y) in the video at time t, is defined as:
                              
                                 (4)
                                 
                                    
                                       E
                                       
                                          x
                                          y
                                          t
                                       
                                       =
                                       
                                          
                                             
                                                
                                                   v
                                                   j
                                                   
                                                      E
                                                      
                                                         x
                                                         y
                                                         t
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             j
                                             =
                                             1
                                          
                                          J
                                       
                                       ≜
                                       
                                          
                                             
                                                
                                                   
                                                      v
                                                      j
                                                   
                                                   :
                                                   
                                                      v
                                                      j
                                                   
                                                   ⊂
                                                   
                                                      R
                                                      
                                                         x
                                                         y
                                                         t
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             j
                                             =
                                             1
                                          
                                          J
                                       
                                    
                                 
                              
                           where R
                           (x,y,t)
                           ∈ℝ3 is a region with pre-defined spatial and temporal dimensions centered at point (x,y,t) in the video (e.g., rx
                           
                           ×
                           ry
                           
                           ×
                           rt
                           ) and J indicates the total number of volumes inside the ensemble. These large contextual 3D spaces are employed to construct higher-level codebooks.

To capture the spatio-temporal compositions of the video volumes, we use the relative spatio-temporal coordinates of the volume in each ensemble, as shown in Fig. 3c. Assume that the ensemble of video volumes at point (xi
                           ,yi
                           ,ti
                           ) is Ei
                            and the central video volume inside that ensemble is called vo
                           . Assume that vo
                            is located at the point (xo
                           ,yo
                           ,to
                           ) in absolute coordinates. Therefore, 
                              
                                 
                                    Δ
                                    
                                       v
                                       j
                                    
                                    
                                       E
                                       i
                                    
                                 
                                 ∈
                                 
                                    R
                                    3
                                 
                              
                            is the relative position (in space and time) of the jth video volume, vj
                           , inside the ensemble of volumes:
                              
                                 (5)
                                 
                                    
                                       
                                          Δ
                                          
                                             v
                                             j
                                          
                                          
                                             E
                                             i
                                          
                                       
                                       =
                                       
                                          
                                             
                                                x
                                                j
                                             
                                             −
                                             
                                                x
                                                o
                                             
                                             ,
                                             
                                                y
                                                j
                                             
                                             −
                                             
                                                y
                                                o
                                             
                                             ,
                                             
                                                t
                                                j
                                             
                                             −
                                             
                                                t
                                                o
                                             
                                          
                                       
                                       .
                                    
                                 
                              
                           
                        

Then each ensemble of video volumes at point (xi
                           ,yi
                           ,ti
                           ) is represented by a set of such video volumes and their relative positions, and hence Eq. (4) can be rewritten as:
                              
                                 (6)
                                 
                                    
                                       E
                                       
                                          
                                             x
                                             i
                                          
                                          
                                             y
                                             i
                                          
                                          
                                             t
                                             i
                                          
                                       
                                       =
                                       
                                          
                                             
                                                
                                                   Δ
                                                   
                                                      v
                                                      j
                                                   
                                                   
                                                      E
                                                      i
                                                   
                                                
                                                
                                                   v
                                                   j
                                                
                                                
                                                   v
                                                   o
                                                
                                             
                                          
                                          
                                             j
                                             =
                                             1
                                          
                                          J
                                       
                                       .
                                    
                                 
                              
                           
                        

An ensemble of volumes is characterized by a set of video volumes, the central video volume, and the relative distance of each of the volumes in the ensemble to the central video volume, as represented in Eq. (6). This provides a view-based graphical spatio-temporal multi-scale description at each pixel in every frame of a video.

A common approach for calculating similarity between ensembles of volumes is to use the star graph model in [7,11,49]. This model uses the joint probability between a database and a query ensemble to decouple the similarity of the topologies of the ensembles and that of the actual video volumes [11]. To avoid such a decomposition, we estimate the pdf of the volume composition in an ensemble and then measure similarity between these estimated pdfs.

During the codeword assignment process described in Section 3.1.2, each volume vj
                            inside each ensemble was assigned to a label cm
                           
                           ∈
                           
                              
                                 
                                    C
                                    L
                                 
                              
                            with some degree of similarity w
                           
                              j,m
                            using Eq. (3). Given the codewords assigned to the video volumes, each ensemble of volumes can be represented by a set of codewords and their spatio-temporal relationships. Let cm
                           
                           ∈
                           
                              
                                 
                                    C
                                    L
                                 
                              
                            be the codeword assigned to the video volume vj
                            and cn
                           
                           ∈
                           
                              
                                 
                                    C
                                    L
                                 
                              
                           , the codeword assigned to the central video volume vo
                           . Therefore, Eq. (6) can be rewritten as
                              6
                           
                           
                              6
                              ← symbolizes value assignment.
                           :
                              
                                 (7)
                                 
                                    
                                       
                                          
                                             
                                                v
                                                j
                                             
                                             ←
                                             
                                                c
                                                m
                                             
                                          
                                       
                                       
                                          
                                             
                                                v
                                                o
                                             
                                             ←
                                             
                                                c
                                                n
                                             
                                          
                                       
                                       
                                          
                                             E
                                             
                                                
                                                   x
                                                   i
                                                
                                                
                                                   y
                                                   i
                                                
                                                
                                                   t
                                                   i
                                                
                                             
                                             =
                                             
                                                
                                                   ∪
                                                   
                                                      
                                                         
                                                            m
                                                            =
                                                            1
                                                            :
                                                            
                                                               M
                                                               L
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            n
                                                            =
                                                            1
                                                            :
                                                            
                                                               M
                                                               L
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      Δ
                                                      
                                                         c
                                                         m
                                                      
                                                      
                                                         c
                                                         n
                                                      
                                                   
                                                
                                                
                                                   j
                                                   =
                                                   1
                                                   :
                                                   J
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where ∆ denotes the relative position of the codeword cm
                            inside the ensemble of volumes. By representing an ensemble as a set of codewords and their spatio-temporal relationships, the topology of the ensemble, Γ, is defined as:
                              
                                 (8)
                                 
                                    
                                       Γ
                                       =
                                       
                                          
                                             ∪
                                             
                                                
                                                   
                                                      m
                                                      =
                                                      1
                                                      :
                                                      
                                                         M
                                                         L
                                                      
                                                   
                                                
                                                
                                                   
                                                      n
                                                      =
                                                      1
                                                      :
                                                      
                                                         M
                                                         L
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                Γ
                                                
                                                   m
                                                   ,
                                                   n
                                                
                                             
                                             
                                                Δ
                                             
                                          
                                       
                                    
                                 
                              
                           where Γ is the topology of an ensemble of video volumes that encodes the spatio-temporal relationships between codewords inside the ensemble. Γ
                              m,n
                           (∆)∈
                           Γ is taken to be the spatio-temporal relationship between two codewords, cm
                            and cn
                            in the ensemble.
                              7
                           
                           
                              7
                              These topological models, Γ
                                    m,n
                                 (Δ), are obtained by assuming that the codeword entries are independent. Although in the case of overlapping video volumes such an assumption is not true, this is the standard Markovian assumption made for BOV.
                            Therefore,
                              
                                 (9)
                                 
                                    
                                       
                                          Γ
                                          
                                             m
                                             ,
                                             n
                                          
                                       
                                       
                                          Δ
                                       
                                       =
                                       
                                          Δ
                                          
                                             c
                                             m
                                          
                                          
                                             c
                                             n
                                          
                                       
                                       .
                                    
                                 
                              
                           
                        

Let v denote an observation, which is taken as a video volume inside the ensemble. Assume that its relative location is represented by Δ
                              v
                           , and vo
                            is the central volume of the ensemble. The aim is to measure the probability of observing a particular ensemble model. Therefore, given an observation, 
                              
                                 
                                    Δ
                                    
                                       v
                                       j
                                    
                                    
                                       E
                                       i
                                    
                                 
                                 
                                    v
                                    j
                                 
                                 
                                    v
                                    o
                                 
                              
                           , the posterior probability of each topological model, Γ
                              m,n
                           Δ, is written as:
                              
                                 (10)
                                 
                                    
                                       P
                                       
                                          
                                             
                                                Γ
                                                
                                                   m
                                                   ,
                                                   n
                                                
                                             
                                             Δ
                                             |
                                             
                                                
                                                   Δ
                                                   
                                                      v
                                                      j
                                                   
                                                   
                                                      E
                                                      i
                                                   
                                                
                                                
                                                   v
                                                   j
                                                
                                                
                                                   v
                                                   o
                                                
                                             
                                          
                                       
                                       =
                                       P
                                       
                                          
                                             Δ
                                             ,
                                             
                                                c
                                                m
                                             
                                             ,
                                             
                                                c
                                                n
                                             
                                             |
                                             
                                                Δ
                                                
                                                   v
                                                   j
                                                
                                                
                                                   E
                                                   i
                                                
                                             
                                             ,
                                             
                                                v
                                                j
                                             
                                             ,
                                             
                                                v
                                                o
                                             
                                          
                                       
                                       .
                                    
                                 
                              
                           
                        

The posterior probability in Eq. (10) defines the probability of observing the codewords cm
                            and cn
                            and their relative location, Δ, given the observed video volumes 
                              
                                 
                                    Δ
                                    
                                       v
                                       j
                                    
                                    
                                       E
                                       i
                                    
                                 
                                 
                                    v
                                    j
                                 
                                 
                                    v
                                    o
                                 
                              
                            in an ensemble of volumes. Eq. (10) can be rewritten as:
                              
                                 (11)
                                 
                                    
                                       P
                                       
                                          
                                             Δ
                                             ,
                                             
                                                c
                                                m
                                             
                                             ,
                                             
                                                c
                                                n
                                             
                                             |
                                             
                                                Δ
                                                
                                                   v
                                                   j
                                                
                                                
                                                   E
                                                   i
                                                
                                             
                                             ,
                                             
                                                v
                                                j
                                             
                                             ,
                                             
                                                v
                                                o
                                             
                                          
                                       
                                       =
                                       P
                                       
                                          
                                             Δ
                                             ,
                                             
                                                c
                                                n
                                             
                                             |
                                             
                                                c
                                                m
                                             
                                             ,
                                             
                                                Δ
                                                
                                                   v
                                                   j
                                                
                                                
                                                   E
                                                   i
                                                
                                             
                                             ,
                                             
                                                v
                                                j
                                             
                                             ,
                                             
                                                v
                                                o
                                             
                                          
                                       
                                       P
                                       
                                          
                                             
                                                c
                                                m
                                             
                                             |
                                             
                                                Δ
                                                
                                                   v
                                                   j
                                                
                                                
                                                   E
                                                   i
                                                
                                             
                                             ,
                                             
                                                v
                                                j
                                             
                                             ,
                                             
                                                v
                                                o
                                             
                                          
                                       
                                       .
                                    
                                 
                              
                           
                        

Since now the unknown video volume, vj
                           , has been replaced by a known interpretation, cm
                           , the first factor on the right hand side of Eq. (11) can be treated as being independent of vj
                           . Moreover, it is assumed that video volumes are independent. Thus vo
                            can be removed from the second factor on the right hand side of Eq. (11) and hence, it can be rewritten as follows:
                              
                                 (12)
                                 
                                    
                                       P
                                       
                                          
                                             Δ
                                             ,
                                             
                                                c
                                                m
                                             
                                             ,
                                             
                                                c
                                                n
                                             
                                             |
                                             
                                                Δ
                                                
                                                   v
                                                   j
                                                
                                                
                                                   E
                                                   i
                                                
                                             
                                             ,
                                             
                                                v
                                                j
                                             
                                             ,
                                             
                                                v
                                                o
                                             
                                          
                                       
                                       =
                                       P
                                       
                                          
                                             Δ
                                             ,
                                             
                                                c
                                                n
                                             
                                             |
                                             
                                                c
                                                m
                                             
                                             ,
                                             
                                                Δ
                                                
                                                   v
                                                   j
                                                
                                                
                                                   E
                                                   i
                                                
                                             
                                             ,
                                             
                                                v
                                                o
                                             
                                          
                                       
                                       P
                                       
                                          
                                             
                                                c
                                                m
                                             
                                             |
                                             
                                                Δ
                                                
                                                   v
                                                   j
                                                
                                                
                                                   E
                                                   i
                                                
                                             
                                             ,
                                             
                                                v
                                                j
                                             
                                          
                                       
                                       .
                                    
                                 
                              
                           
                        

On the other hand, the codeword assigned to the video volume is independent of its position, 
                              
                                 Δ
                                 
                                    v
                                    j
                                 
                                 
                                    E
                                    i
                                 
                              
                           . Therefore Eq. (12) can be reduced to:
                              
                                 (13)
                                 
                                    
                                       P
                                       
                                          
                                             Δ
                                             ,
                                             
                                                c
                                                m
                                             
                                             ,
                                             
                                                c
                                                n
                                             
                                             |
                                             
                                                Δ
                                                
                                                   v
                                                   j
                                                
                                                
                                                   E
                                                   i
                                                
                                             
                                             ,
                                             
                                                v
                                                j
                                             
                                             ,
                                             
                                                v
                                                o
                                             
                                          
                                       
                                       =
                                       P
                                       
                                          
                                             Δ
                                             ,
                                             
                                                c
                                                n
                                             
                                             |
                                             
                                                c
                                                m
                                             
                                             ,
                                             
                                                Δ
                                                
                                                   v
                                                   j
                                                
                                                
                                                   E
                                                   i
                                                
                                             
                                             ,
                                             
                                                v
                                                o
                                             
                                          
                                       
                                       P
                                       
                                          
                                             
                                                c
                                                m
                                             
                                             |
                                             
                                                v
                                                j
                                             
                                          
                                       
                                       .
                                    
                                 
                              
                           
                        

Rewriting Eq. (13) gives:
                              
                                 (14)
                                 
                                    
                                       P
                                       
                                          
                                             Δ
                                             ,
                                             
                                                c
                                                m
                                             
                                             ,
                                             
                                                c
                                                n
                                             
                                             |
                                             
                                                Δ
                                                
                                                   v
                                                   j
                                                
                                                
                                                   E
                                                   i
                                                
                                             
                                             ,
                                             
                                                v
                                                j
                                             
                                             ,
                                             
                                                v
                                                o
                                             
                                          
                                       
                                       =
                                       P
                                       
                                          
                                             Δ
                                             |
                                             
                                                c
                                                m
                                             
                                             ,
                                             
                                                c
                                                n
                                             
                                             ,
                                             
                                                Δ
                                                
                                                   v
                                                   j
                                                
                                                
                                                   E
                                                   i
                                                
                                             
                                             ,
                                             
                                                v
                                                o
                                             
                                          
                                       
                                       P
                                       
                                          
                                             
                                                c
                                                n
                                             
                                             |
                                             
                                                c
                                                m
                                             
                                             ,
                                             
                                                Δ
                                                
                                                   v
                                                   j
                                                
                                                
                                                   E
                                                   i
                                                
                                             
                                             ,
                                             
                                                v
                                                o
                                             
                                          
                                       
                                       P
                                       
                                          
                                             
                                                c
                                                m
                                             
                                             |
                                             
                                                v
                                                j
                                             
                                          
                                       
                                       .
                                    
                                 
                              
                           
                        

Similarly, by assuming independency between codewords and their locations, Eq. (14) can be reduced to:
                              
                                 (15)
                                 
                                    
                                       P
                                       
                                          
                                             Δ
                                             ,
                                             
                                                c
                                                m
                                             
                                             ,
                                             
                                                c
                                                n
                                             
                                             |
                                             
                                                Δ
                                                
                                                   v
                                                   j
                                                
                                                
                                                   E
                                                   i
                                                
                                             
                                             ,
                                             
                                                v
                                                j
                                             
                                             ,
                                             
                                                v
                                                o
                                             
                                          
                                       
                                       =
                                       P
                                       
                                          
                                             Δ
                                             |
                                             
                                                c
                                                m
                                             
                                             ,
                                             
                                                c
                                                n
                                             
                                             ,
                                             
                                                Δ
                                                
                                                   v
                                                   j
                                                
                                                
                                                   E
                                                   i
                                                
                                             
                                          
                                       
                                       P
                                       
                                          
                                             
                                                c
                                                n
                                             
                                             |
                                             
                                                v
                                                o
                                             
                                          
                                       
                                       P
                                       
                                          
                                             
                                                c
                                                m
                                             
                                             |
                                             
                                                v
                                                j
                                             
                                          
                                       
                                       .
                                    
                                 
                              
                           
                        

The first factor on the right hand side of Eq. (15) is the probabilistic vote for a spatio-temporal position, given the codewords assigned to the central video volume of the ensemble, the codeword assigned to the video volume, and its relative position. We note that, given a set of ensembles of video volumes, the probability distribution function (pdf) in Eq. (15) can be formed using either a parametric model or non-parametric estimation. Here, we approximate 
                              
                                 P
                                 
                                    
                                       Δ
                                       |
                                       
                                          c
                                          m
                                       
                                       ,
                                       
                                          c
                                          n
                                       
                                       ,
                                       
                                          Δ
                                          
                                             v
                                             j
                                          
                                          
                                             E
                                             i
                                          
                                       
                                    
                                 
                              
                            describing each ensemble in Eq. (15) using (nonparametric) histograms. P (cm
                           |vj
                           ) and P (cn
                           |vo
                           ) in Eq. (15) are the votes for each codeword entry and they are obtained in the codeword assignment procedure in Section 3.1.2. Eventually, each ensemble of volumes can be represented by a set of pdfs as follows:
                              
                                 (16)
                                 
                                    
                                       P
                                       
                                          
                                             Γ
                                             |
                                             
                                                E
                                                i
                                             
                                          
                                       
                                       =
                                       
                                          
                                             ∪
                                             
                                                
                                                   
                                                      m
                                                      =
                                                      1
                                                      :
                                                      
                                                         M
                                                         L
                                                      
                                                   
                                                
                                                
                                                   
                                                      n
                                                      =
                                                      1
                                                      :
                                                      
                                                         M
                                                         L
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             P
                                             
                                                
                                                   
                                                      Γ
                                                      
                                                         m
                                                         ,
                                                         n
                                                      
                                                   
                                                   
                                                      Δ
                                                   
                                                   |
                                                   
                                                      E
                                                      i
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where P(Γ|Ei
                           ) is a set of pdf modeling topology of the ensemble of volumes. Therefore, similarity between two video sequences can be computed simply by matching the pdfs of the ensembles of volumes at each pixel.

Once a video clip has been processed, each ensemble of spatio-temporal volumes has been represented by a set of pdfs as given in Eq. (16). Having performed the first level of clustering in Section 3.1.2, and given the representation of each ensemble obtained in Eq. (16), the aim now is to cluster the ensembles. This will then permit us to construct a behavioral model for the query video. Although clustering can be performed using many different approaches, spectral clustering methods are currently in vogue due to their superior performance to traditional methods. Moreover, they can be computed efficiently. Spectral clustering constructs a similarity matrix of feature vectors and seeks an optimal partition of the graph representing the similarity matrix using eigen-decomposition [53]. Usually, this is followed by either k-means or fuzzy c-means clustering. We utilize the normalized decomposition method of [54].

Employing the overall pdf P (Γ|Ei
                           ) in Eq. (16) to represent each ensemble of volumes makes it possible to use divergence functions from statistics and information theory as the appropriate dissimilarity measure. Here we use the symmetric Kullback–Leibler (KL) divergence to measure the difference between the two pdfs, f and g 
                           [55]:
                              
                                 (17)
                                 
                                    
                                       d
                                       
                                          f
                                          g
                                       
                                       =
                                       KL
                                       
                                          
                                             f
                                             |
                                             |
                                             g
                                          
                                       
                                       +
                                       KL
                                       
                                          
                                             g
                                             |
                                             |
                                             f
                                          
                                       
                                    
                                 
                              
                           where KL (f||g) is the Kullback–Leibler (KL) divergence of f and g. Therefore, given the pdf of each ensemble of volumes in Eq. (16) the similarity between two ensembles of volumes, E (xi
                           ,yi
                           ,ti
                           ) and E (xj
                           ,yj
                           ,tj
                           ), is defined as:
                              
                                 (18)
                                 
                                    
                                       
                                          s
                                          
                                             
                                                E
                                                i
                                             
                                             ,
                                             
                                                E
                                                j
                                             
                                          
                                       
                                       =
                                       
                                          e
                                          
                                             −
                                             
                                                
                                                   
                                                      
                                                         d
                                                         2
                                                      
                                                      
                                                         
                                                            P
                                                            
                                                               
                                                                  Γ
                                                                  |
                                                                  
                                                                     E
                                                                     i
                                                                  
                                                               
                                                            
                                                            ,
                                                            P
                                                            
                                                               
                                                                  Γ
                                                                  |
                                                                  
                                                                     E
                                                                     j
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                   
                                                   
                                                      2
                                                      
                                                         σ
                                                         2
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where P(Γ|E(x
                           
                              i
                           ,y
                           
                              i
                           ,t
                           
                              i
                           )) and P(Γ|E(x
                           
                              j
                           ,y
                           
                              j
                           ,t
                           
                              j
                           )) are the pdfs of the ensembles E (xi
                           ,yi
                           ,ti
                           ) and E (xj
                           ,yj
                           ,tj
                           ), respectively, obtained in Section 3.2.2. d is the symmetric KL divergence between the two pdfs in Eq. (17) and σ is the variance of the KL divergence over all of the observed ensembles of STVs in the query.

Given the similarity measurement of the ensembles in Eq. (18), the similarity matrix, S
                           
                              N
                           , for a set of ensembles of volumes is formed and the Laplacian calculated as follows:
                              
                                 (19)
                                 
                                    
                                       L
                                       =
                                       
                                          D
                                          
                                             −
                                             
                                                
                                                   1
                                                   2
                                                
                                             
                                          
                                       
                                       
                                          S
                                          N
                                       
                                       
                                          D
                                          
                                             
                                                1
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                           where D is a diagonal matrix whose ith diagonal element is the sum of all elements in the ith row of S
                           
                              N
                           . Subsequently, an eigenvalue decomposition is applied to L and the eigenvectors corresponding to the largest eigenvalues are normalized and form a new representation of the data to be clustered [54]. This is followed by online fuzzy single-pass clustering [56] to produce 
                              
                                 M
                                 H
                              
                            different codewords for the high-level codebook of ensembles of STVs, 
                              
                                 
                                    C
                                    H
                                 
                                 =
                                 
                                    
                                       
                                          
                                             c
                                             i
                                          
                                       
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       M
                                       H
                                    
                                 
                              
                           , for each pixel.

In order to select a particular video in a target set that contains a similar activity to the one in the query video, the uninformative regions (e.g., background) must obviously be excluded from the matching procedure. This is conventionally performed in all activity recognition algorithms. Generally, for shape-template and tracking based approaches this is done at the pre-processing stages using such methods as background subtraction and ROI selection. These have their inherent problems discussed in Section 2. On the other hand, selecting informative rather than uninformative regions is a normal aspect of almost every BOV-based approach that constructs STVs at interest points. Clearly, these are intrinsically related to the most informative regions in the video. When we consider the framework for activity recognition proposed in this paper, the high-level codebook of ensembles of STVs is used to generate codes for all pixels in each video frame. Therefore it is crucial to select only the most informative codewords and their related pixels. Given the high-level codebook, 
                              
                                 C
                                 H
                              
                           , constructed in Section 3.2.3, we saw that a codeword is assigned to each pixel p(x,y) at time (t) in the video. Therefore, in a video sequence of temporal length 
                              T
                           , a particular pixel p(x,y) is represented as a sequence of assigned codewords at different times:
                              
                                 (20)
                                 
                                    
                                       p
                                       
                                          x
                                          y
                                       
                                       =
                                       
                                          
                                             p
                                             
                                                x
                                                y
                                             
                                             ←
                                             
                                                c
                                                i
                                             
                                             :
                                             ∀
                                             t
                                             ∈
                                             T
                                             ,
                                             
                                             
                                                c
                                                i
                                             
                                             ∈
                                             
                                                C
                                                H
                                             
                                          
                                       
                                       .
                                    
                                 
                              
                           
                        

A sample video frame and the assigned codewords are illustrated in Fig. 4
                           . In order to remove non-informative codewords (e.g., codewords which represent the scene background), each pixel and its assigned codewords are analyzed as a function of time. As an example, Fig. 4 plots the assigned codewords to the sampled pixels in the video over time. It is observed that the pixels related to the background or static objects show stationary behavior. Therefore the associated codewords can be removed by employing a simple temporal filter at each pixel. This method was inspired by the pixel-based background model presented in [57], where a time series of each of the three quantized color features was created at each pixel. A more compact model of the background is then determined by temporal filtering, based on the idea of the Maximum Negative Run-Length (MNRL). The MNRL is defined as the maximum amount of time between observing two samples of a specific codeword at a particular pixel [57]. The larger the MNRL, the more likely the codeword is not the background. The main difference from [57] is that we employ the assigned codewords as the representative features for every pixel, as obtained from the high level codebook 
                              
                                 C
                                 H
                              
                            (see Eq. (20)).

The major advantage of selecting informative codewords at the highest level of the coding hierarchy is that compositional scene information comes into play.
                              8
                           
                           
                              8
                              Some advanced approaches for background modeling also incorporate spatio-temporal compositions of the motion-informative regions to build a background model [58,51].
                            Hence the computational cost is greatly reduced and the need for a separate background subtraction algorithm is eliminated.

In summary, at first, the query video is densely sampled at different spatio-temporal scales in order to construct the video volumes. Then a low level codebook is formed and each volume vj
                            is assigned to a codeword ci
                           , ci
                           
                           ∈
                           
                              
                                 
                                    C
                                    L
                                 
                              
                           , with similarity w
                           
                              j,i
                           . Then a larger 3D volume around each pixel, containing many STVs, the so-called ensemble of STVs, is considered. The spatio-temporal arrangement of the volumes inside each ensemble is model based on a set of pdfs. At the next level of the hierarchical structure, another codebook is formed for these ensembles of STVs, 
                              
                                 C
                                 H
                              
                           . The two codebooks are then employed for finding similar videos to the query.

Two main features characterize the constructed probabilistic model of the ensembles. First the spatio-temporal probability distribution is defined independently for each codebook entry. Second, the probability distribution for each codebook entry is estimated using (non-parametric) histograms. The former renders the approach capable of handling certain deformations of an object's parts while the latter makes it possible to model the true distribution instead of making an oversimplifying Gaussian assumption.

The overall goal is to find similar videos to a query video in a target set and consequently label them according to the labeled query video using the hierarchical codebook presented in Section 3. Fig. 5
                      summarizes the process of determining the hierarchical codebooks and how the similarity maps are constructed.

The inference mechanism is the procedure for calculating similarity between particular spatio-temporal volume arrangements in the query and the target videos. More precisely, given a query video containing a particular activity, 
                        Q
                     , we are interested in constructing a dense similarity map for every pixel in the target video, 
                        V
                     , by utilizing pdfs of the volume arrangements in the video. At first, the query video is densely sampled and a low level codebook is constructed for local spatio-temporal video volumes. Then the ensemble of video volumes is formed. These data are used to create a high level codebook, 
                        
                           C
                           H
                        
                     , for coding spatio-temporal compositional information of the video volumes, as described in Section 3. Finally, the query video is represented by its associated codebooks.
                        9
                     
                     
                        9
                        The query is represented by two codebooks: the low level codebook of spatio-temporal video volumes, 
                              
                                 C
                                 L
                              
                           , and the high level codebook of the ensembles of video volumes, 
                              
                                 C
                                 H
                              
                           .
                      In order to construct the similarity map for the target video, 
                        V
                     , it is densely sampled at different spatio-temporal scales and the codewords from 
                        
                           
                              C
                              L
                           
                        
                      are assigned to the video volumes. Then the ensembles of video volumes are formed at every pixel and the similarity between the ensembles in 
                        V
                      and the codewords in 
                        
                           C
                           H
                        
                      is measured using Eq. (18). In this way, a similarity map is constructed at every pixel in the target video, 
                        
                           
                              S
                              
                                 Q
                                 ,
                                 V
                              
                           
                           
                              x
                              y
                              t
                           
                        
                     . The procedure for similarity map construction has been described in detail in Fig. 5. Note again that no background and foreground segmentation and no explicit motion estimation are required in the proposed method.

Having constructed a similarity map, it remains to find the best match to the query video.
                        10
                     
                     
                        10
                        The inference mechanism is relatively simple as our aim is to introduce and formulate a hierarchical structure for constructing a similarity map between videos based on densely sampled STVs and their spatio-temporal compositions. However, it could be replaced by a more sophisticated one.
                      Generally two scenarios are considered in activity recognition and video matching: (1) Detecting and localizing an activity of interest and (2) Classifying a target video given more than one query, which is usually referred to as action classification. For both of these, the region in the target video that contains a similar activity to the query must be selected at an appropriate scale. We perform multi-scale activity localization, so that ensembles of volumes are generated at each scale independently. Hence, we produce a set of independent similarity maps for each scale. Therefore, for a given ensemble of volumes, E (x,y,t) in the target video, a likelihood function is formed at each scale:
                        
                           (21)
                           
                              
                                 p
                                 
                                    
                                       
                                          S
                                          
                                             Q
                                             ,
                                             V
                                          
                                       
                                       
                                          x
                                          y
                                          t
                                       
                                       |
                                       scale
                                    
                                 
                              
                           
                        
                     where 
                        
                           
                              S
                              
                                 Q
                                 ,
                                 V
                              
                           
                           
                              x
                              y
                              t
                           
                        
                      is the similarity between the ensemble of volumes in the target video, E(x,y,t), and the most similar codeword in the high-level codebook, 
                        
                           
                              c
                              
                                 k
                                 ∗
                              
                           
                           ∈
                           
                              C
                              H
                           
                        
                     , and scale represents the scale at which the similarity is measured. In order to localize the activity of interest, i.e., finding the most similar ensemble of volumes in the target video to the query, the maximum likelihood estimate of the scale at each pixel is employed. Therefore, the most appropriate scale at each pixel is the one that maximizes the following likelihood estimate:
                        
                           (22)
                           
                              
                                 
                                    
                                       scale
                                    
                                    ∗
                                 
                                 =
                                 arg
                                 
                                    
                                       max
                                       p
                                    
                                    scale
                                 
                                 
                                    
                                       
                                          S
                                          
                                             Q
                                             ,
                                             V
                                          
                                       
                                       
                                          x
                                          y
                                          t
                                       
                                       |
                                       scale
                                    
                                 
                                 .
                              
                           
                        
                     
                  

In order to find the most similar ensemble to the query, a detection threshold was employed. Hence, an ensemble of volumes is said to be similar to the query and contains the activity of interest if 
                        
                           
                              S
                              
                                 Q
                                 ,
                                 V
                              
                           
                           
                              x
                              y
                              t
                           
                           ≥
                           γ
                        
                      at scale
                     ⁎. In this way, the region in the target video that matches the query is detected.
                        11
                     
                     
                        11
                        The threshold, γ was set empirically to 0.7 of the maximum similarity value for every query video in all experiments.
                     
                  

For action classification problem, we consider a set of queries, 
                        
                           Q
                           =
                           ∪
                           
                              
                                 Q
                                 i
                              
                           
                        
                     , each containing a particular activity.
                        12
                     
                     
                        12
                        In most of the reported approaches for activity recognition it is implicitly assumed that the query contains a single activity.
                      Then the target video is labeled according to the most similar video in the query. For each query video, 
                        
                           Q
                           i
                        
                     , two codebooks are formed and then the similarity maps are constructed as described in Fig. 5. This produces a set of similarity maps for all activities of interest. Therefore, the target video contains a particular activity, i
                     ∗, that maximizes the accumulated similarity between all ensembles of volumes in the target video as follows:
                        
                           (23)
                           
                              
                                 
                                    i
                                    ∗
                                 
                                 =
                                 arg
                                 
                                    max
                                    i
                                 
                                 
                                    
                                       
                                          
                                             ∑
                                             
                                                E
                                                
                                                   x
                                                   y
                                                   t
                                                
                                                ∈
                                                V
                                             
                                          
                                       
                                       
                                          S
                                          
                                             
                                                Q
                                                i
                                             
                                             ,
                                             V
                                          
                                       
                                       
                                          x
                                          y
                                          t
                                       
                                    
                                 
                                 ,
                                 
                                 
                                    Q
                                    i
                                 
                                 ∈
                                 Q
                                 .
                              
                           
                        
                     
                  

Despite the simple inference mechanism employed here for action recognition and localization, the obtained experimental results show the strength of our approach for similarity map construction between two videos. We also note that the proposed statistical model of codeword assignment and the arrangement of the spatio-temporal volumes permit small local misalignments in the relative geometric arrangement of the composition. This property, in addition to the multi-scale volume construction in each ensemble, enables the algorithm to handle certain non-rigid deformations in space and time. This, of course, is necessary since human actions are not exactly reproducible, even for the same person. Obviously, activity recognition from a single example eliminates the need for a large number of training videos for model construction and significantly reduces computational costs. On the other hand, it imposes some limitations by its nature. It appears that learning from a single example is not as general as the models constructed using many training examples, and therefore our approach may not be as general as the model-based approaches. However, it should be emphasized that constructing a generic viewpoint and scale invariant model for an activity requires a large amount of labeled training data, which do not currently exist. Moreover, imposing strong priors by assuming particular types of activities reduces the search space of possible poses considered, which limits their application to action recognition.

We conclude this section by examining the computational complexity of our algorithm. Suppose there are K video volumes available in each ensemble and the number of codewords in the low- and high-level codebooks are 
                        
                           
                              M
                              L
                           
                        
                      and 
                        
                           M
                           H
                        
                     , respectively. For each ensemble, the time complexity of the low level and high level codeword assignment are O (K
                     ×
                     
                        
                           
                              M
                              L
                           
                        
                     ), and 
                        
                           O
                           
                              
                                 M
                                 H
                              
                           
                        
                     , respectively. Therefore the complexity of calculating each point in a similarity map is 
                        
                           O
                           
                              
                                 K
                                 ×
                                 
                                    M
                                    L
                                 
                                 ×
                                 
                                    M
                                    H
                                 
                              
                           
                        
                     .

@&#EXPERIMENTAL RESULTS@&#

The algorithm was tested on three different datasets: KTH [12], Weizmann [13] and MSR II [14] to determine its capabilities for action recognition. The Weizmann and KTH datasets are the standard benchmarks in the literature used for action recognition. The Weizmann dataset consists of ten different actions performed by nine actors, and the KTH action data set contains six different actions, performed by twenty-five different persons in four different scenarios (indoor, outdoor, outdoor at different scales, outdoor with different clothes). The MSR II consists of 54 video sequences, recorded in different environments with cluttered backgrounds in crowded scenes, and contains three types of actions similar to the KTH: boxing, hand clapping, and hand waving. We evaluated our approach for three different scenarios. The first one is “action matching and retrieval using a single example”, in which both target and query videos are selected from the same dataset. This task measures the capability of the proposed approach for video matching. The second scenario is the “single dataset action classification” task in which more than one query video is employed to construct the model of a specific activity. Here, single dataset classification implies that both query and target videos are selected from the same dataset. Finally, in order to measure the generalization capability of our algorithm to find similar activities in videos recorded in different environments, “cross-dataset action detection” was performed. This scenario implies that that the query and target videos could be selected from different datasets.

Video matching and classification were performed using KTH and Weizmann, which are single-person, single-activity videos. We used them to compare with the current state-of-the-art even though they were collected in controlled environments. For cross-dataset action recognition, we used the KTH dataset as the query set, while the target videos were selected from the more challenging MSR II dataset. Our experiments demonstrate the effectiveness of our hierarchical codebook method for action recognition in these various categories. In all cases, we have assumed that local video volumes are of size nx
                     
                     =
                     ny
                     
                     =
                     nt
                     
                     =5, and the HOG is calculated assuming nθ
                     
                     =16 and nϕ
                     
                     =8. The ensemble size was set to rx
                     
                     =
                     ry
                     
                     =
                     rt
                     
                     =50. The number of codewords in the low- and high-level codebooks was set to 55 and 120, respectively.
                        13
                     
                     
                        13
                        These parameters are similar to the ones in a similar study [15].
                      Later in this section we will thoroughly examine the effect of different parameters on the performance of the algorithm.

Since our proposed method is a video-to-video matching framework, it is not necessary to have a training sequence. This means that we can select one labeled query video for each action, and find the most similar one to it in order to perform the labeling. For the Weizmann dataset, we used one person for each action as a query video and the rest (eight other persons) as the target sets. This was done for all persons in the dataset and the results were averaged. The confusion matrix for the Weizmann dataset is shown in Fig. 6a, achieving an average recognition rate of 91.9% over all 10 actions. The columns of the confusion matrix represent the instances to be classified, while each row indicates the corresponding classification results.

We carried out the same experiment on the KTH dataset. The confusion matrix is shown in Fig. 6b. The average recognition rate was 81.2% over all 6 actions. The results indicate that the method proposed in this paper outperforms state-of-the-art approaches, even though the former requires no background/foreground segmentation and tracking. The average accuracy of the other methods is presented in Table 1
                        .

The overall results on the Weizmann dataset are better than those on the KTH dataset. This is predictable, since the Weizmann dataset contains videos with more static backgrounds and more stable and discriminative actions than the KTH dataset.

In order to measure the capabilities of our approach in dealing with scale and illumination variations, we reported the average recognition rate for different recording scenarios in the KTH dataset. According to [12], KTH contains four different recording conditions which are: s1) outdoors; s2) outdoors with scale variations; s3) outdoors with different clothes; and s4) indoors. The evaluation procedure employed here is to construct four sets of target videos, each having been obtained under the same recording condition. Then, a query is selected from one of these four scenarios and the most similar video to the query is found in each target dataset in order to perform the labeling. The average recognition rates are reported in Table 2
                        . When the target and query videos are selected from the same subset of videos with the same recording conditions, the average recognition rate is higher than when they are taken under different recording conditions. Moreover, although we have claimed that our method is scale- and illumination-invariant, it appears that, in these experiments, the recognition rate decreases when the query and target videos have been taken under different recording conditions. This is particularly evident when the target videos are recorded at different scales (see the second column in Table 2). Thus scale and clothing variations degrade the performance of our algorithm more than changes in illumination. Therefore, as we might have expected, an activity model constructed using just a single example cannot adequately account for all scale/illumination variations in a scene.

In order to make an additional quantitative comparison of our algorithm with the state-of-the-art, we have extended it to the action classification problem. This refers to the more classical situation in which we use a set of query videos instead of just a single one, as discussed previously. We have evaluated our algorithm's ability to apply the correct label to a given video sequence, when both the training
                           14
                        
                        
                           14
                           Although our method does not actually require any specific training sequences, we refer to the query videos as the training set for consistency with the literature.
                         and target datasets are obtained from the same dataset. We tested the Weizmann and KTH datasets, and applied the standard experimental procedures in the literature. For the Weizmann dataset, the common approach for classification is to use leave-one-out cross-validation, i.e., eight persons are used for training and the videos of the remaining person are matched to one of the ten possible action labels. Consistent with other methods in the literature, we mixed the four scenarios for each action in the KTH dataset. We followed the standard experimental procedure for this dataset [12], in which 16 persons are used for training and nine for testing. This is done 100 times and after which the average performance over these random splits is calculated [12]. The confusion matrix for the Weizmann dataset is reported in Fig. 7a and the average recognition rate is 98.7% over all 10 actions in the leave-one-out setting. As expected from earlier experiments reported in the literature, our results indicate that the “skip” and “jump” actions are easily confused, as they appear visually similar. For the KTH dataset, we achieved an average recognition rate of 95% for the six actions as shown in the confusion matrix in Fig. 7. As observed from Fig. 7b, the primary confusion occurs between jogging and running, which was also problematical for the other approaches. Obviously, this is due to the inherent similarity between the two actions. The recognition rate was also compared to other approaches (see Table 3
                        ). Comparing our results with those of the state-of-the-art, we observe that they are similar, though again we do not require any background/foreground segmentation and tracking.

Similar to other approaches for action recognition [60], we use cross-dataset recognition to measure the robustness and generalization capabilities of our algorithm. In this paradigm, the query videos are selected from one dataset (the KTH dataset in our experiments) and the targets from another (MSR II dataset), so that we compare similar actions performed by different persons in different environments. We selected three classes of actions from the KTH dataset as the query videos: boxing, hand waving, and hand clapping, including 25 persons performing each action. A hierarchical codebook was created for each action category and the query was matched to the target videos. We varied the detection threshold, γ, to obtain the precision/recall curves for each action type, as shown in Fig. 8
                        . This achieved an overall recognition rate of 79.8%, which is comparable to the state-of-the-art (see Table 4
                        ).

As our proposed method creates two codebooks to group similar video volumes and ensembles of video volumes, it is necessary to analyze the effect of different codebook sizes on the performance of the algorithm. Therefore, the overall recognition rate for different codebook sizes was determined as described previously using the KTH dataset. Various codebook sizes (
                           
                              M
                              H
                           
                         and 
                           
                              
                                 M
                                 L
                              
                           
                        ) were employed and the average recognition rate calculated. In Fig. 9
                        , the average recognition rate is plotted as a function of the both low- and high-level codebook sizes (number of codewords). We observe that small low level codebooks will not produce acceptable results, even with a large number of high level codewords. Therefore preserving information at the lowest level is necessary to achieve acceptable results. Recall that we have shown in the previous section how the number of codewords affects the computational cost of our algorithm.

Similarly, using larger high level codebooks demands more memory and dramatically increases computational time. Therefore the number of codewords must be kept as small as possible. Although there is a trade-off between codeword size and the performance of the algorithm, it can be inferred from our experiments that using relatively small codebooks at both low and high levels (e.g., 
                           
                              
                                 M
                                 L
                              
                           
                        
                        =55 and 
                           
                              
                                 M
                                 H
                              
                              =
                              120
                           
                        ) achieves acceptable results for action recognition.

We have presented a new hierarchical approach based on spatio-temporal volumes for the challenging problem of video-to-video matching and tested for the problem of human action recognition in videos. At the lowest level in the data hierarchy, our approach is an extension of conventional BOV approaches. However, this is only at the bottom level of a more descriptive data hierarchy that is based on representing a video by compositional contextual data. The hierarchical structure consists of three main levels:
                        
                           •
                           Densely sampling and coding a video using spatio-temporal volumes to produce a low-level codebook. This codebook is similar to the one constructed in conventional BOV approaches.

Constructing an ensemble of video volumes and representing their structure using probabilistic modeling of the compositions of the spatio-temporal volumes. This is followed by the construction of a high-level codebook for the volume ensembles.

Analyzing the codewords assigned to each pixel as a function of time in order to determine salient regions.

Given a single query video (an example of a particular activity), the method computes the similarity of each pixel in each frame of the target videos to the query, and finds the subset of target videos that are similar to that query. This is accomplished by analyzing a relatively large contextual region around the pixel, while considering the compositional structure using a probabilistic framework. The algorithm was tested on three popular benchmarks, KTH, Weizmann, and MSR II. We showed that it is effective and robust for both action-matching and cross-dataset recognition. Moreover, the results are highly competitive with state-of-the-art methods. However, a major advantage of our approach is that it does not require background and foreground segmentation and tracking, and is susceptible to on-line real-time analysis. The proposed video method can easily be extended to multi-action retrieval and action localization by modifying the inference mechanism. Since the proposed method codes the video using spatio-temporal video volumes and their compositional information, it does not impose any constraints on the video contents and therefore, it can be extended to unconstrained video matching and content-based search engines. One of the major advantages of the proposed algorithm for event recognition in videos is that it does not require a model of the event. However, it does have some drawbacks that need to be addressed in future work. Clearly, such a video representation of activities in a scene cannot be applied for long-term behavior understanding, e.g., behaviors that consist of numbers of activities that occur sequentially. Some form of event segmentation might deal with this issue. Future research will extend the approach by adding another level of analysis to the hierarchical structure, which models the spatial and temporal connectivity of the learnt activities.

@&#REFERENCES@&#

