@&#MAIN-TITLE@&#On heuristic solutions for the stochastic flowshop scheduling problem

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We propose a method to estimate expected makespan in a stochastic flowshop.


                        
                        
                           
                           We conduct experiments that show that existing results may not be fully justified.


                        
                        
                           
                           We compare existing heuristics using the proposed method for makespan estimation.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Scheduling

Flowshop

Stochastic

Makespan objective

Heuristics

@&#ABSTRACT@&#


               
               
                  We address the problem of scheduling jobs in a permutation flowshop when their processing times adopt a given distribution (stochastic flowshop scheduling problem) with the objective of minimization of the expected makespan. For this problem, optimal solutions exist only for very specific cases. Consequently, some heuristics have been proposed in the literature, all of them with similar performance. In our paper, we first focus on the critical issue of estimating the expected makespan of a sequence and found that, for instances with a medium/large variability (expressed as the coefficient of variation of the processing times of the jobs), the number of samples or simulation runs usually employed in the literature may not be sufficient to derive robust conclusions with respect to the performance of the different heuristics. We thus propose a procedure with a variable number of iterations that ensures that the percentage error in the estimation of the expected makespan is bounded with a very high probability. Using this procedure, we test the main heuristics proposed in the literature and find significant differences in their performance, in contrast with existing studies. We also find that the deterministic counterpart of the most efficient heuristic for the stochastic problem performs extremely well for most settings, which indicates that, in some cases, solving the deterministic version of the problem may produce competitive solutions for the stochastic counterpart.
               
            

@&#INTRODUCTION@&#

The flowshop scheduling problem with makespan objective (usually denoted as Fm|prmu|Cmax
                     ) has been subject of research for more than 60 years, being one of the most comprehensively studied problems in Operations Research (see in this regard the reviews by Framinan, Gupta, & Leisten, 2004; Reza Hejazi & Saghafian, 2005 and Ruiz & Maroto, 2005). This decision problem consists of how to schedule jobs in a permutation flowshop in order to minimize the maximum completion time or makespan. A classical assumption is that the processing times of each job in each machine are considered different, but known in advance (deterministic). In contrast, our paper deals with the problem of scheduling n jobs in a permutation flowshop consisting of m machines where the processing times are not deterministic, but follow some known distribution. The objective considered is that of minimizing the expected makespan. This problem is considered to be more realistic that their deterministic counterpart, as it allows capturing part of the inherent variability present in many real-life manufacturing environments (see e.g. Hopp & Spearman, 2008). In the following, we will denote our problem as Fm|prmu|E[Cmax
                     ].

The Fm|prmu|E[Cmax
                     ] problem has been much less studied than its deterministic counterpart, and it is clearly much more complex. In fact, apart from a dominance rule obtained by Makino (1965) for the case of two jobs, no exact solution is available without assumptions on the distribution of the processing times. For m = 2 and exponential distribution of the processing times, Talwar (1967) conjectured an exact solution for the problem that was later proved to be optimal by Cunningham and Dutta (1973), and is currently known as Talwar’s rule.

Despite these advances, for the rest of the cases, no optimal procedure has been found. For the two-machine case, three approximate solutions have been proposed by Baker and Trietsch (2011) based both in Talwar’s rule and in Johnson’s rule (Johnson, 1954) for the deterministic flowshop, all of them with similar (near optimal) performance. For the general m machine case, (Baker & Altheimer, 2012) suggest three heuristics based on adaptations of the CDS (Campbell, Dudek, & Smith, 1970) and NEH (Nawaz, Enscore, & Ham, 1983) heuristics, again with similar and near optimal performance. Although it might seem that, from these results, the problem Fm|prmu|E[Cmax
                     ] is already solved, some issues have to be discussed:

                        
                           •
                           First of all, the evaluation of sequences in a stochastic flowshop is far from being a trivial task. Since the objective is to obtain the expected makespan of a given sequence, E[Cmax
                              ] has to be estimated by running N simulations using the sequence as a solution, from which a sample 
                                 
                                    C
                                    
                                       m
                                       a
                                       x
                                    
                                    i
                                 
                               (i = 1, …, N) is obtained. Then, E[Cmax
                              ] is estimated by averaging the sample, i.e. 
                                 
                                    E
                                    
                                       [
                                       
                                          C
                                          
                                             m
                                             a
                                             x
                                          
                                       
                                       ]
                                    
                                    ≈
                                    
                                       
                                          C
                                          ¯
                                       
                                       
                                          m
                                          a
                                          x
                                       
                                    
                                    =
                                    
                                       1
                                       N
                                    
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       N
                                    
                                    
                                       C
                                       
                                          m
                                          a
                                          x
                                       
                                       i
                                    
                                 
                              .

Up to now, there is no standardized procedure to determine N, although the authors of related contributions use a large number in order to ensure the significance of the estimation. Thus, Baker and Altheimer (2012) use N = 100,000 whereas Gourgand, Grangeon, and Norre (2003) employ N = 200,000 regardless the size and characteristics of each instance, while Portougal and Trietsch (2006) set N to 10,000 for the 2-machine case. In addition, there is no mechanism to establish the statistical significance of the so-obtained 
                                 
                                    
                                       C
                                       ¯
                                    
                                    
                                       m
                                       a
                                       x
                                    
                                 
                               and, consequently, to assess the differences in the performance among different heuristics.

Due to the computational complexity of the stochastic problem, the experiments in the literature have been limited to very small problem sizes (up to n = 10 and m = 6 in the most recent studies). This makes the conclusions obtained so far to be restricted to very small problem sizes.

Finally, to the best of our knowledge, the necessity of heuristics specifically designed for the stochastic problem has not been yet determined. In other words, one may try to solve the stochastic flowshop scheduling problem by transforming it into its deterministic counterpart, i.e. by obtaining a flowshop with the same number of jobs and machines but with deterministic processing times equal e.g. to the means of those from the stochastic problem. Then, heuristics for the Fm|prmu|Cmax
                               problem can be applied and a (possibly good) sequence for the deterministic problem can be obtained. If this sequence performs well when applied to the stochastic problem, then the need of specific stochastic heuristics can be questioned. However, such test has not been conducted so far. It is foreseeable that the so-obtained sequences perform worse that those specifically designed for the stochastic version, but maybe the differences in the quality of the results do not justify the much higher computation times required for the stochastic heuristics. Even if the deterministic heuristics are not valid for some cases, it would be interesting to quantify the degree of variability for which using them is still acceptable, as it is clear that a stochastic flowshop with low variability would resemble very much to a deterministic flowshop.

With these issues in mind, we first discuss and propose a procedure for estimating the expected makespan of a sequence in a stochastic flowshop, so the error in such estimation is bounded by a given percentage. In this way, the statistical significance of the results obtained by the different heuristics can be more clearly established. Interestingly, the results show that the sample sizes (N) obtained from our procedure proposed range from very small to very high values, thus supporting the conclusion that no predetermined value can be easily found regardless the variability of the instances and the error assumed in the estimation.

Next, we compare the main heuristics proposed in the literature as well as the expected makespan obtained from the application of purely deterministic procedures to the mean processing times of the stochastic problem. These heuristics are tested for problem sizes larger than those presented so far in the literature, so the conclusions from the results can be better supported. The results show that, in contrast to Baker and Altheimer (2012), there are significant differences in the performance of the heuristics, and that – perhaps not so surprisingly – the performance of the sequences obtained from purely deterministic methods in the stochastic flowshop do not differ greatly from that obtained from specific stochastic methods.

The remainder of the paper is organized as follows: First, the problem under consideration is formally described in Section 2, where the main contributions of the literature are discussed. Since our work is of computational nature, we devote Section 3 to discuss the key issue of the testbed in which the heuristics are to be compared, as we intend to capture different problem sizes and different degrees of variability of the flowshop. Next, we present in Section 4 the procedure to estimate the expected mean makespan of a given solution, and compare the number of iterations required with those employed in the literature. The comparison of the performance of different heuristics for the problem is done in Section 5, where the main results are also discussed. Finally, Section 6 present the conclusions and points out future research lines.

@&#BACKGROUND@&#

A flowshop consists of n jobs that must be processed on m machines in the same order, where job i requires pij
                      time units to be processed on machine j. The scheduling problem in flow shops is to find a sequence of jobs for each machine according to certain performance measure(s). Additionally, for many situations, it is assumed that the job sequences will be the same on every machine (permutation flowshops). Other hypotheses common in scheduling research include the simultaneous availability of all jobs and of all machines, deterministic processing times, etc. For a complete list of these assumptions, see e.g. Framinan, Leisten, and Ruiz-Usano (2005).

While the deterministic flowshop scheduling problem with makespan objective has been extensively studied (see the reviews mentioned above), the same cannot be said about its stochastic counterpart. For the two-jobs case and a general distribution of the processing times, a dominance rule is given by Makino (1965), but this result is extremely restrictive and with little applicability for most practical settings.

By making assumptions on the distribution of the processing times of the jobs, an important result is due to Talwar (1967). He conjectures that the expected makespan is minimized when the processing time of the jobs follows an exponential distribution by sequencing the jobs in non increasing order of 
                        
                           
                              1
                              
                                 μ
                                 
                                    i
                                    1
                                 
                              
                           
                           −
                           
                              1
                              
                                 μ
                                 
                                    i
                                    2
                                 
                              
                           
                           ,
                        
                      where μij
                      is the mean processing times of job i on machine j. This order is proved to be optimal by Cunningham and Dutta (1973), and it is currently known as Talwar’s rule. As an extension of this rule to other distributions, Kalczynski and Kamburowski (2006) heuristically adapt Talwar’s rule for the Weibull distribution. For a general family of distributions, Portougal and Trietsch (2006) develop a heuristic named PSH which starts with the solution given by Johnson’s rule (Johnson, 1954) for the deterministic flowshop, and applies an adjacent pairwise interchange (a reason why this heuristic is later renamed API by Baker and Trietsch, 2011). Finally, Baker and Trietsch (2011) test three different procedures for different families of distributions, i.e. Talwar’s rule, Johnson’s rule, and the API heuristic. They conclude that the (deterministic) Johnson’s rule could be better unless the coefficient of variation of the jobs is very high. For such cases, Talwar’s rule or API may perform better.

For the general flowshop problem with m machines, Baker and Altheimer (2012) propose different heuristics. The first heuristic is called CDS/Johnson and consists of obtaining a set of m − 1 2-machine flowshop subproblems with the addition of the processing times of the jobs in the manner of the CDS heuristic by Campbell et al. (1970). More specifically, 2-machine flowshop subproblem k (with k = 1, …, m − 1) is constructed by obtaining the processing times of job i in the first (second) machine of this subproblem as 
                        
                           
                              A
                              i
                           
                           =
                           
                              ∑
                              
                                 j
                                 =
                                 1
                              
                              
                                 j
                                 =
                                 k
                              
                           
                           
                              p
                              
                                 i
                                 j
                              
                           
                        
                      (
                        
                           
                              B
                              i
                           
                           =
                           
                              ∑
                              
                                 j
                                 =
                                 k
                                 +
                                 1
                              
                              
                                 j
                                 =
                                 m
                              
                           
                           
                              p
                              
                                 i
                                 j
                              
                           
                        
                     ). Then, Johnson’s procedure is applied to each of the resulting m − 1 subproblems, and m − 1 sequences are obtained. The estimation of the expected makespan of each of this sequences is obtained (in their case by running 100,000 simulations and taking the average makespan value), and the one yielding the lowest value is selected.

The second tested heuristic is the CDS/Talwar heuristic. In a similar manner to the previous one, a set of m − 1 2-machine flowshop subproblems are obtained, and a sequence is obtained for each one by applying Talwar’s rule. Out of these m − 1 sequences, the one with the minimum estimation of the expected makespan is selected.

The third heuristic tested is based on the famous NEH heuristic proposed by Nawaz et al. (1983) for the deterministic case. This heuristic consists of two phases: First the jobs are ranked according to the descending sum of their mean processing times. In a second phase, a solution is constructed as follows: Starting from a partial sequence constructed by taking the first job of the rank, then, for k = 2, …, n, k partial sequences are constructed by inserting the kth job of the rank in all k slots of the partial sequence. These k partial sequences are evaluated with respect to their expected makespan (estimated, as in previous cases, by means of obtaining a sample via simulation and taking the average value), and the one obtaining the lowest value of the estimation of the expected makespan is retained as partial sequence for step k + 1. The procedure is repeated until a full sequence is obtained. Note that the NEH heuristic has been widely applied to deterministic flowshop scheduling problems with objectives different than makespan minimization, as it is done in Framinan et al. (2005) for flowtime minimization.

Among the three tested heuristics, Baker and Altheimer (2012) find that the NEH adaptation is the best one, but none of the heuristic procedures dominates the others. Furthermore, their performance is compared against that of a genetic algorithm (assumed to find the optimal or near-optimal sequence for most instances), and the authors conclude that the three heuristics generate average suboptimalities of less than 1 percent, leaving little room for the development of new approximate algorithms.

Despite the advances reported, there are some issues already discussed in Section 1 that affect the existing results and deserve further research. A closer look on how to estimate the expected makespan is needed, in order to add statistical consistency to the results. Furthermore, the necessity of special stochastic heuristics for the problem has not been established, particularly when, for the two-machine case, Portougal and Trietsch (2006) state the excellent performance of the Johnson’s deterministic heuristic in the stochastic setting. To address all these issues, we first need a benchmark testbed to conduct the computational experience. The design of this testbed is presented in the next section.

Regarding the design of a testbed for flowshop scheduling, we first have to decide about the problem sizes, i.e. the number of jobs n and machines m of the different instances. Given the computational complexity of the stochastic model, problem sizes have to be much more reduced than those in the deterministic counterpart, however we want to substantially increase the existing problem sizes in order to gain generality on the results. With these premises, we choose n ∈ {5, 10, 15, 20} and m ∈ {2, 5, 10, 20}.

In order to ease the analysis, we assume that all distributions of the processing times belong to the same family, which is a common assumption almost universally made (see e.g. Gourgand et al., 2003, Baker & Trietsch, 2011 or Baker & Altheimer, 2012). Regarding to the family of distributions, there are several options, including the random distribution (Baker & Trietsch, 2011 or Baker & Altheimer, 2012), the normal distribution (Gourgand et al., 2003) the exponential distribution (Gourgand et al., 2003, Baker & Trietsch, 2011 or Baker & Altheimer, 2012), and the log normal distribution (Baker & Trietsch, 2011 or Baker & Altheimer, 2012).

After reviewing the different distributions, we will assume a log normal distribution for our testbed. The log normal distribution is characterized by two parameters (mean μ and standard deviation σ), and it has a considerable practical value for modelling real-life processing times. Additionally, in contrast e.g. to the exponential distribution, we can control its variability by means of the standard deviation and thus model different degrees of variability of the instances in the testbed.

Regarding the average processing times of the jobs (μij
                     ), each one is drawn from a discrete uniform [1,99] distribution. In the deterministic counterpart, these values are assumed to generate difficult instances (see e.g. Dannenbring, 1977, Campbell et al., 1970, Taillard, 1993, or Vallada, Ruiz, & Framinan, 2015), so we expect the same for the stochastic case. For each mean processing time, we want to model different degrees of variability. Hopp and Spearman (2008) classify the processing time variability according to the coefficient of variation 
                        
                           c
                           =
                           
                              σ
                              μ
                           
                           ,
                        
                      so processing times with low variability are those with c < 0.75, with moderate variability if 0.75 ≤ c < 1.33, and with high variability if c ≥ 1.33. Although this classification if rather arbitrary, we will use it to characterize the different instances to be generated. More specifically, we employ c ∈ {0.01, 0.25, 0.50, 0.75, 1.00, 1.25, 1.50}. When c = 0.01, the processing times are close to be deterministic, but for c = 1.50 the processing times have a high variability. Although bigger c values are naturally possible, we believe that these do not represent reasonably realistic environments, as very large coefficient of variations are not the norm in industry due to the substantial efforts done to reduce the variability of the processing times via process automation and standardization procedures. Note that the case c = 1.0 yields the same variance as in the exponential distribution, and, in order to make sure that we also cover this case, in Section 4 we also include results assuming an exponential distribution of the processing times.

In summary, we build each instance of the testbed in the following manner: For a given coefficient of variation c, we select a number of jobs and a number of machines (problem size). Next, for each job on each machine, we generate their mean processing time μij
                      by using a uniform [1,99] distribution. The standard deviation of each job on each machine σij
                      is then set to c · μij
                     . This procedure is repeated in order to generate 20 instances for each problem size and coefficient of variation. In total, 2240 instances (320 for each value of c) are generated. Nevertheless, we will see in Sections 4 and 5, results cannot be obtained for the combination of certain values of c with very small allowed percentage error, so these are not reported.

As mentioned before, a critical issue in stochastic scheduling is how to evaluate the solutions. Recall that here the objective function is the expected makespan, therefore, given a sequence, an expected makespan must be assigned to this sequence. However, such expected makespan has to be estimated via a sample mean of the makespans corresponding to this sequence. The usual way to obtain the sample mean is to conduct a very large number of simulations of the makespan. Gourgand et al. (2003) assessed the accuracy of such simulations by making comparisons of the simulation results against those obtained by a Markov chain, and found that sample sizes of 200,000 produced 95 percent confidence intervals of the order of 0.1 percent. In their experiments, Baker and Altheimer (2012) use a sample size of 100,000 for problem sizes where m ∈ {2, 3, 6} and n is up to 10 jobs whereas, for the 2-machine case, Portougal and Trietsch (2006) use a sample size of 10,000. Nevertheless, it is clear that the sample size should depend – at least – on two factors, namely the confidence interval of the results, and the stochasticity of the problem. Note that the way the solutions are evaluated may determine the significance of the results, here the criticality of this issue.

In this paper, we propose a method to estimate the expected makespan of a sequence based on the maximum percentage error accepted for the estimation of E[Cmax
                     ]. More specifically, the half-width of a confidence interval for E[Cmax
                     ] of 1 − α confidence level is given by 
                        
                           
                              t
                              
                                 α
                                 /
                                 2
                                 ,
                                 N
                                 −
                                 1
                              
                           
                           
                              s
                              
                                 N
                              
                           
                           ,
                        
                      where s is the sample standard deviation of the makespan, and α/2 is the area of a Student’s t-distribution with N − 1 degrees of freedom left in the interval (−∞, t
                     
                        α/2,N− 1] (see e.g. Vijay & Saleh, 2011). We intend that 
                        
                           
                              t
                              
                                 N
                                 −
                                 1
                                 ,
                                 α
                                 /
                                 2
                              
                           
                           
                              s
                              
                                 N
                              
                           
                           ≤
                           
                              
                                 C
                                 ¯
                              
                              
                                 m
                                 a
                                 x
                              
                           
                           ·
                           p
                           ,
                        
                      where p is a (small) percentage. By doing so, E[Cmax
                     ] is confined in the interval 
                        
                           [
                           
                              
                                 C
                                 ¯
                              
                              
                                 m
                                 a
                                 x
                              
                           
                           
                              (
                              1
                              −
                              p
                              )
                           
                           ,
                           
                              
                                 C
                                 ¯
                              
                              
                                 m
                                 a
                                 x
                              
                           
                           
                              (
                              1
                              +
                              p
                              )
                           
                           ]
                        
                      with a 1 − α confidence level. If we set α to a very low value (in our experiments, α = 0.001), we can be almost sure (statistically speaking) that p represents the maximum relative error of the estimation of E[Cmax
                     ] and thus use p to check the significance of different results obtained by several heuristics. Note that the normality assumption of the random variables required for this confidence interval can be dropped if the sample size is large due to the Central Limit Theorem (see e.g. Vijay & Saleh, 2011), which is our case in practice, and given the fact that the result of each run is independent from the others.

More specifically, our procedure for estimating the expected makespan of a sequence S consists of the following steps:

                        
                           1.
                           Set the simulations counter to zero, i.e. N ≔ 0

Set the sum of makespans to zero, i.e. SumCmax
                               ≔ 0

Set the sum of squares of makespans to zero, i.e. SumSCmax
                               ≔ 0

do:
                                 
                                    (a)
                                    Run a simulation to obtain a sample makespan Cmax
                                        of S.

Update the number of simulations, i.e. N ≔ N + 1.

Update the sum of makespans, i.e. SumCmax
                                        ≔ SumCmax
                                        + Cmax
                                       
                                    

Update the sum of squares of makespans, i.e. 
                                          
                                             S
                                             u
                                             m
                                             S
                                             
                                                C
                                                
                                                   m
                                                   a
                                                   x
                                                
                                             
                                             :
                                             =
                                             S
                                             u
                                             m
                                             S
                                             
                                                C
                                                
                                                   m
                                                   a
                                                   x
                                                
                                             
                                             +
                                             
                                                C
                                                
                                                   m
                                                   a
                                                   x
                                                
                                                2
                                             
                                          
                                       
                                    

Calculate 
                                          
                                             
                                                
                                                   C
                                                   ¯
                                                
                                                
                                                   m
                                                   a
                                                   x
                                                
                                             
                                             :
                                             =
                                             
                                                
                                                   S
                                                   u
                                                   m
                                                   
                                                      C
                                                      
                                                         m
                                                         a
                                                         x
                                                      
                                                   
                                                
                                                N
                                             
                                          
                                        and 
                                          
                                             s
                                             :
                                             =
                                             
                                                
                                                   
                                                      S
                                                      u
                                                      m
                                                      S
                                                      
                                                         C
                                                         
                                                            m
                                                            a
                                                            x
                                                         
                                                      
                                                      −
                                                      N
                                                      ·
                                                      
                                                         
                                                            (
                                                            
                                                               
                                                                  C
                                                                  ¯
                                                               
                                                               
                                                                  m
                                                                  a
                                                                  x
                                                               
                                                            
                                                            )
                                                         
                                                         2
                                                      
                                                   
                                                   
                                                      N
                                                      −
                                                      1
                                                   
                                                
                                             
                                          
                                       .

while 
                                 
                                    
                                       
                                          s
                                          ·
                                          
                                             t
                                             
                                                N
                                                −
                                                1
                                             
                                          
                                          ,
                                          α
                                          /
                                          2
                                       
                                       
                                          
                                             
                                                C
                                                ¯
                                             
                                             
                                                m
                                                a
                                                x
                                             
                                          
                                          
                                             N
                                          
                                       
                                    
                                    >
                                    p
                                 
                              
                           

Return 
                                 
                                    
                                       C
                                       ¯
                                    
                                    
                                       m
                                       a
                                       x
                                    
                                 
                              .

In order to check the number of simulation runs required by this procedure for different degrees of variability and percentage error p (more specifically, p ∈ {0.001, 0.005, 0.010, 0.050}), we obtain the estimations of the expected makespan of a random sequence for each instance in the testbed. The results are shown in Table 1
                      in terms of the average number of simulation runs for each number of jobs and machines. Note that, for some combinations of low values of p and high values of c, the results are not reported as the number of iterations for the procedure to converge was too high (typically around one hour of computing time), and the procedure was stopped before reaching the end.

From the results, it can be seen that, as foreseeable, the number of runs varies greatly depending on the percentage error accepted. Allowing a 5 percent error means that results can be obtained with less than 50,000 runs if the processing times have a low variability, but a 0.1 percent error requires around 1,000,000 runs even for instances with c = 0.25. When the variability turns to moderate (i.e. c around 1.00), even a 5 percent error requires 500,000 runs. For the cases of c = 1.00 and an error bounded to 1 percent, the number of runs exceeds the millions.

In general, the need of more runs decreases with the number of machines, but remains relatively stable with respect to the number of jobs. This may speak for a compensation of the processing times of a job across the machines.

A similar pattern can be seen for the exponential case in Table 2
                     . In this case, the number of iterations for the cases p = 0.005, p = 0.010, and p = 0.050 are reported, but not for p = 0.001, as the computational requirements were excessive. As it can be seen, in order to bound the error within 1 percent, more than 100,000 runs are required on average.

Additional experiments were carried out to establish the percentage error (p) induced when the number of simulation runs is considered fixed. To do so, we estimate 
                        
                           
                              C
                              ¯
                           
                           
                              m
                              a
                              x
                           
                        
                      for a random sequence using 100,000 simulation runs, and calculate a confidence interval for α = 0.001. By doing so, we are ‘almost’ sure that the makespan is contained in this interval, and then we can use the extreme values of this interval to obtain p. The results are shown in Table 3 for all values of c considered in the testbed, and make clear that, while 100,000 simulation runs are an acceptable number for low variability, for c = 0.75 (the frontier between low and moderate variability), this number of simulations leads to an average error over 3.7 percent, and that, in some instances, this error exceeds 15 percent. The error explodes for medium variability scenarios, being the average error around 10 percent for c = 1.0, 20 percent for c = 1.25. Finally, for c = 1.5 (high variability), the average error is almost 35 percent, and it may be up to 280 percent.

The general outcome of these experiments is that it is difficult to be confident in the results obtained for the number of simulation runs employed in the literature, as small estimation errors usually require more than 200,000 runs for scenarios with medium/high variability. In addition, our method allows to know the accepted error of the estimations (in percentage terms) and therefore to assert the significance of the differences in the performance of solution procedures.

In this section, we carry out a computational study to establish the performance of different heuristics for the problem according to the procedure for the estimation of the expected makespan presented in the previous section. The heuristics tested are the following:

                        
                           •
                           The stochastic version of the NEH heuristic as described in Baker and Altheimer (2012). This heuristic is labelled SNEH.

The stochastic version of the CDS/Talwar heuristic as described in Baker and Altheimer (2012). This heuristic is labelled SCDS/Talwar.

The deterministic NEH heuristic applied using as data the mean processing times of the instances.

The deterministic CDS/Talwar heuristic applied using as data the mean processing times of the instances.

The deterministic NEH heuristic applied using as data the mean processing times of the instances, but using as initial order that given by the deterministic CDS/Talwar heuristic. This heuristic is labelled NEH-Talwar.

Note that, although the procedure for SNEH and SCDS/Talwar are identical to that of Baker and Altheimer (2012), the estimation of the expected makespan of the subsequences and that of the final sequence is carried out according to the procedure presented in Section 4 for p = 0.01. Analogously, in order to estimate the expected makespan given by the deterministic heuristics (NEH, CDS/Talwar, and NEH-Talwar), the sequence obtained is evaluated following the aforementioned procedure.

The testbed presented in Section 3 is solved using the five heuristics presented above. Tables 4–8
                     
                     
                     
                     
                      show the results obtained for different values of c. Apart from the average values obtained by the estimated makespan for each one of the heuristic (labelled as Avg. in the tables), the average percentage increase of the makespan of each heuristic with respect to that of SNEH is presented (labelled as Δ in the tables).

In these tables, we do not give information on the time required for each heuristic. Note that the deterministic heuristic are nearly instantaneous for the problem sizes tested (e.g. NEH is less than 0.01 seconds for the biggest instances), although for our purposes, we have to evaluate the so-obtained sequence (something not required when applying it for a real problem). Regarding the times required for the stochastic heuristic, they depend obviously on the problem size and on the value of c, ranging from 1300 seconds for c = 0.01, n = 20, m = 20 to roughly 20 hours for c = 1.0, n = 20, m = 20. In total, the computational workload of the experiments contained in the tables can be measured in weeks of CPU time.

Several comments can be done on the results obtained:

                        
                           •
                           With respect to the heuristics specifically designed for the stochastic problem, there are significant differences in performance. This result contradicts those obtained by Baker and Altheimer (2012), who did not detect significant differences among them. It has to be noted that their way to estimate E[Cmax
                              ] the solution is based on a fixed number of simulation runs and that the number that they employed (100,000) has been proved to be insufficient to establish consistent results for medium/large coefficient of variations. In addition, our testbed is of bigger size, a fact that may also explain some differences.

The differences in performance between SNEH and SCDS/Talwar decrease with the variability of the testbed (from roughly 15 percent for c = 0.01 to about 2 percent for c = 1.0), but still are substantial for relatively large coefficient of variations. The explanation may lie in the fact that Talwar’s rule is optimal for the exponential distribution, whose coefficient of variation is 1, and therefore its performance improves for instances where c is closer to that value.

With respect to the deterministic heuristic tested, the best performance corresponds to the NEH, although these differences decrease as c increases. However, it is interesting to note that CDS/Talwar is supposed to incorporate some stochastic considerations in their ordering, however these do not pay off, at least if CDS/Talwar is used as a simple heuristic. When CDS/Talwar is employed as a starting order for the NEH heuristic (NEH-Talwar), their differences with the original (deterministic) NEH are very small, and it is reasonably to assume that both heuristics perform roughly the same. This fact, together with the relatively poor performance of CDS/Talwar, may imply that the initial ordering is not very relevant in the performance of the deterministic NEH.

When comparing SNEH and NEH it may be seen that the differences are very small. In many cases, these differences are below the error accepted for p (1 percent). The conclusion is that, for a realistic ranges of variability in the flowshop, the application of the NEH to the mean processing times gives an extremely good estimation of the performance of their stochastic counterpart. If we note that SNEH is highly CPU-intensive, requiring a high number of simulations of all subsequences for all steps, it has to be questioned whether the effort in running SNEH pays off.

Although there is no clear pattern, it seems that the differences in performance of all the heuristics decrease with the number of machines, a fact for which we do not have at the moment a fully convincing explanation. Nevertheless, we may conjecture that, since it is widely-known that the performance of the deterministic NEH worsens with the number of machines, its stochastic counterpart is even more sensitive to the number of machines, so the differences between the two versions of the NEH are smaller.

@&#CONCLUSIONS@&#

In this paper, we have addressed the problem of scheduling jobs in a flowshop when their processing times adopt a given distribution. Existing literature for the problem reveals that optimal solutions can be found only for very specific cases, so some heuristics with similar performance have been proposed for the general case. We first focus on the critical issue of estimating E[Cmax
                     ] the expected makespan of a sequence, and found that, for instances with a medium/large variability (expressed as the coefficient of variation of the processing times of the jobs), the number of samples (simulation runs) used in the literature to estimate E[Cmax
                     ] may not be sufficient to derive conclusive results. We propose a procedure with a variable number of iterations that ensures that the error in the estimation of the expected makespan is bounded (with a very high probability) within a small percentage. Using this procedure, we test the main heuristic proposed in the literature in a larger testbed and in many cases found significant differences in their performance, in contrast with existing studies. We also found that the deterministic counterpart of the most efficient heuristic for the stochastic problem performs extremely well for most settings, which indicates that (at least within the limitations of our study), a practical way to solve the Fm|prmu|E[Cmax
                     ] is to simplify it to its deterministic version.

Several issues lie ahead as future research lines. First of all, the computational analysis can be enhanced by introducing different families of distribution, higher coefficients of variation for the log normal distribution, and bigger problem instances. However, it is to note that some families of distributions widely applied in theory due to their good properties (such as e.g. the exponential) do not match well with the processing times distributions found in practice. Analogously, very large coefficient of variations are not the norm in industry, as discussed before. Finally, there are severe computational limitations for using bigger testbeds, as the experiments carried out in this paper already amount for weeks of CPU time.

A more fruitful research line may be to check whether the results obtained by the best stochastic heuristic (SNEH) are close to the optimal values, or not. Although this necessarily has to be done for small instances, it would be interesting to confirm the results by Baker and Altheimer (2012), who indicate that SNEH obtains close-to-optimal solutions. If this was not the case, ground for developing new approximate solutions for the problem may be re-opened.

@&#ACKNOWLEDGEMENTS@&#

The authors wish to thank the referees for their comments on the earlier versions of the manuscript. This research has been funded by the Spanish Ministry of Science and Innovation, under project “ADDRESS” with reference DPI2013-44461-P.

@&#REFERENCES@&#

