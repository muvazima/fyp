@&#MAIN-TITLE@&#Predicting treatment process steps from events

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Completely new approach for workflow-recognition from real-world patient data.


                        
                        
                           
                           Usage of Hidden Markov Models to create a stochastic model of the therapy process.


                        
                        
                           
                           Intelligent exception-handling techniques to improve HMM recognition rates.


                        
                        
                           
                           Application for the treatment process in oncological head and neck surgery.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Hidden Markov Model

Workflow recognition

Tumor therapy

Digital patient model

@&#ABSTRACT@&#


               
               
                  Motivation
                  The primary economy-driven documentation of patient-specific information in clinical information systems leads to drawbacks in the use of these systems in daily clinical routine. Missing meta-data regarding underlying clinical workflows within the stored information is crucial for intelligent support systems. Unfortunately, there is still a lack of primary clinical needs-driven electronic patient documentation. Hence, physicians and surgeons must search hundreds of documents to find necessary patient data rather than accessing relevant information directly from the current process step. In this work, a completely new approach has been developed to enrich the existing information in clinical information systems with additional meta-data, such as the actual treatment phase from which the information entity originates.
               
               
                  Methods
                  Stochastic models based on Hidden Markov Models (HMMs) are used to create a mathematical representation of the underlying clinical workflow. These models are created from real-world anonymized patient data and are tailored to therapy processes for patients with head and neck cancer. Additionally, two methodologies to extend the models to improve the workflow recognition rates are presented in this work.
               
               
                  Results
                  A leave-one-out cross validation study was performed and achieved promising recognition rates of up to 90% with a standard deviation of 6.4%.
               
               
                  Conclusions
                  The method presented in this paper demonstrates the feasibility of predicting clinical workflow steps from patient-specific information as the basis for clinical workflow support, as well as for the analysis and improvement of clinical pathways.
               
            

@&#INTRODUCTION@&#

@&#MOTIVATION@&#

The substantial amount of patient-specific information in clinical information systems helps physicians and surgeons obtain increasingly comprehensive overviews of patients. This information is available but is insufficiently structured and poorly searchable, particularly in the large Hospital Information System (HISs) such as SAP i.s.h.med. There are several reasons for the poor structuring of this information, but it is mainly due to the primary economy-driven documentation goals of hospitals. Mature IT solutions are still lacking for a primary focus on treatment decision making, for hierarchically organizing patient-specific results, and for summarizing the results of relevant pretreatment investigations. This situation leads to inefficient processes in daily clinical routine and to considerable time spent searching documents that are relevant in the current treatment phase. Unfortunately, physicians do not have the ability to simply and quickly obtain the most relevant documents for the specific therapy phase. Furthermore, scientific questions such as “Show me all of the clinical documents that are generated during surgical interventions for patients with laryngeal carcinoma” cannot readily be answered using the currently available clinical information systems.

To improve clinical workflows, to relieve physicians and surgeons from time-consuming activities, to improve patient safety and to reduce costs, developing user-friendly and intuitive capabilities for searching and accessing clinical data is of critical importance. A fundamental prerequisite is knowledge of treatment processes in general, as well as specific knowledge regarding the current treatment phase of the patient. Knowledge of the overall treatment process can be acquired through a workflow analysis in the specific clinical disciplines or departments. Statistical models such as Hidden Markov Models (HMMs) are good methods for subsequently transferring the workflow analysis results into a mathematical representation that can subsequently be used to infer workflow phases from given patient-specific information that originates from clinical information systems, i.e., the so-called observations [1].

In this work, a completely new approach for automatically inferring the current step of a clinical patient treatment process from clinical data is presented. First, patient-specific information from different clinical information systems is used as training data for creating a probabilistic model of the underlying treatment process. Subsequently, patient-specific information that was not a part of the training process can be provided to the model for predicting the most likely treatment step. Finally, each patient-specific information entity, such as disease codes, procedure codes or laboratory results, contains additional meta-information regarding the originating process step. This meta-information is very valuable for the development of sophisticated workflow assistance or clinical decision support systems.

Additionally, a mathematical representation of clinical workflows provides an important basis for a wide variety of analyses. The model can identify both the most likely course of treatment as well as outliers for patients with complex cases. Subsequently, this information provides the basis for improving clinical pathways or for serving as a metric for clinical quality management.

The primary objective of this work is to develop a new method for predicting single phases of the oncological treatment process based on patient-specific information entities originating from clinical information systems. The first step is to develop a probabilistic model based on HMM that is able to represent clinical workflows in different granularity levels. This model should then be able to predict the current treatment phase for a set of unknown clinical patient-specific information entities. We developed two types of HMM models with 3 and 7 therapy phases and with additional exception-handling approaches to address different types of variations in clinical workflows. The exception-handling approaches consider both the hierarchical properties of International Statistical Classification of Diseases and Related Health Problems (ICD10) and International Classification of Procedures in Medicine (ICPM) codes, as well as others, to improve the HMM prediction rates. These models are then trained with anonymized real-world clinical datasets from patients with primary head and neck tumor diagnoses. Finally, two types of HMM models are evaluated in terms of their recognition rates and the results of their exception-handling algorithms.

@&#BACKGROUND@&#

A stochastic model for workflows can only be realized with thorough knowledge of their underlying processes. Thus, the clinical workflow that provides the basis for this work is briefly described in this section and is illustrated in Fig. 1
                        .

The Ear, Nose & Throat (ENT) clinic at the Leipzig University Medical Center is an independent clinic that consists of ambulance, ward, phoniatry and operating rooms. Generally, 250 patients with a primary tumor diagnosis in the head and neck area are treated annually with full in-house services for cancer therapy, from pre-op consultation to post-op evaluation, check-up and therapy.

The clinical workflow for patients with head and neck cancer is divided into three major phases with a minimum duration of at least 5years. The therapy begins with a clinical diagnostics phase, during which important patient-specific information is acquired. A first consultation is performed to document the medical status of the patient and to clarify the actual disorders. The consultation consists of an anamnesis, in which the physician investigates the patients current medical condition, allergies, comedication, previous interventions and lifestyle, as well as a clinical examination of the patient’s ears, nose, oropharynx and larynx. During the diagnostics phase, morphological and functional medical imagings are performed, such as Computer Tomography (CT), Magnetic Resonance Imaging (MRI) or Positron Emission Tomography (PET) in combination with CT or MRI (PET/CT or PET/MRI). The results are recorded in the local Picture Archiving and Communication System (PACS). When a tumor is suspected, the patient receives a panendoscopy, which is an examination performed under full anesthesia that provides a better contouring of the tumor extension. During the panendoscopy, biopsy samples of potential tumor tissues are collected and immediately sent to the pathology department for a histopathological examination. Subsequently, the biopsy locations and clinical TNM Classification of Malignant Tumors (TNM) classifications are recorded in the Dornheim Tumor Therapy Manager (DTTM) [2]. In the case of positive histopathological findings, the local head and neck tumor board
                           2
                           The head and neck tumor board is an institution in which physicians from different medical disciplines, such as ENT and maxillofacial surgery, radiation therapy, radiology, nuclear medicine, oncology and pathology, meet to discuss patient cases and different therapy approaches.
                        
                        
                           2
                         decides on the most appropriate therapy for the patient based on all previously acquired information.

The following therapy phase consists of either surgical interventions, adjuvant and neoadjuvant radiotherapy or chemotherapy, or a combination of these steps, to improve the therapy outcome. In a post-therapeutic tumor board, the achieved therapy outcomes are discussed, and the patient either receives further therapies or is released into follow-up. During this last phase of the tumor treatment process, the patient attends regular follow-up consultations to ensure that recurring tumors are quickly identified.

Knowledge of workflow information from surgical interventions and from perioperative processes is important for the development of intelligent clinical workflow assistance systems. Therefore, statistical models such as HMMs [1] are appropriate for the recognition of workflow steps and are commonly used in the medical field.

The course of a surgical intervention can be represented as a sequence of 5-tuples, the individual Surgical Process Model (iSPM), which contain process step information such as activity, actor, surgical instrument, target structure and time [3]. Subsequently, multiple iSPMs can be merged into a generalized Surgical Process Model (gSPM) [4]. A gSPM represents an averaged course of the corresponding intervention type. Based on the aforementioned theoretical basis, Schumann et al. investigated mathematical approaches for comparing iSPMs and gSPMs and assessing the quality of surgical processes [5]. Franke et al. developed a representation of surgical interventions based on Markov theory [6–8]. Based on the process knowledge that can be acquired from gSPM, the surgical intervention can be partitioned into different phases. Subsequently, the gSPM can be used in combination with Markov theory to predict the actual high-level phase of an ongoing intervention with the help of low-level tasks such as sensor information. Based on these models, intelligent workflow assistance systems, such as systems that present surgical process information in the operating room, can be developed. This method has already been applied to surgical interventions in eye, neuro and ENT surgeries.

The following research studies investigated intraoperative workflow recognition during laparoscopic cholecystectomies and used HMMs for predicting surgical steps based on Operating Room (OR) sensor information. Padoy et al. used the process information to predict the remaining intervention time. The HMM in this project was developed from 12 recorded surgical interventions that consisted of 14 surgical phases. This model yielded a high prediction error (>14min) at the beginning of the intervention, but the error rate decreased significantly at the end (≈1min) [9]. Blum et al. developed a HMM from 11 laparoscopic interventions that consisted of 14 process phases for the recognition of surgical phases during cholecystectomy interventions [10]. The evaluation of this model with a leave-one-out cross validation yielded a positive phase detection rate of 93%. Bouarfa et al. developed a framework to clean noisy sensor information with a Bayesian network approach to infer the correct low-level task [11]. Subsequently, a HMM was used to infer the corresponding surgical process steps based on the corrected low-level tasks with a prediction accuracy of up to 90%.

Modeling clinical workflows other than the OR is an emerging field of research for describing and optimizing clinical pathways. Huang et al. used dynamic programming approaches to summarize a clinical pathway from clinical event logs [12]. Medical behaviors in real-world event logs are very diverse and heterogeneous, but the authors demonstrated the applicability of the presented approach for creating condensed clinical pathway summaries in polynomial time. Subsequently, Huang et al. investigated the use of process mining techniques to extract explicit clinical pathway patterns from medical behaviors recorded in clinical workflow logs [13]. This research group focused on specific diseases, e.g. bronchial lung cancer, colon cancer, gastric cancer, breast cancer and cerebral infarction. Based on classical sequence pattern mining algorithms, the authors developed an algorithm tailored to the specific scenario (SCP-Miner) and performed an evaluation against existing algorithms, such as CloSpan and BIDE. The results of the study demonstrated that the proposed approach provides better outcomes in terms of processing time, scalability and generated clinical pathway patterns. A later work of Huang et al. focused on improving the discovery of clinical pathway patterns from event logs through the use of probabilistic topic models [14]. The authors used a clinical event log from the cardiology department of the Chinese PLA General Hospital and successfully discovered the underlying clinical pathway patterns using the aforementioned method. The clinical pathway analysis can also be complemented by measuring the similarities between patient traces. Huang et al. employed latent Dirichlet allocation (LDA) to measure similarities between pairwise patient traces [15]. To evaluate the performance of the proposed approach, comparisons with the edit-distance-based similarity measure and a classical simple term vector-based method were performed. The precision of the LDA approach was 19% higher than those of the edit-distance and vector-based outcomes.

Statistical models such as HMMs provide the theoretical basis for a wide range of applications and are also appropriate for modeling clinical workflows [16–19]. Hence, this section will provide a brief introduction to HMMs and the algorithms used in this work.

We first wish to briefly discuss why we decided to use HMMs and to compare HMMs to neural networks and decision trees [20,21]. The HMMs is a well-established method for workflow recognition with proven mathematical foundations. Additionally, many studies have previously employed HMMs for the recognition of surgical workflows, as depicted in detail in Section 2.2, with promising results. In our case, HMMs provide several advantages compared to neural networks and decision trees. First, a substantial amount of training data is required to train neural networks. Furthermore, neural networks do not operate on transition and observation probability matrices, which is an important feature of HMMs. These matrices provide transition and observation probabilities in human-readable form, which provides a basis for further research, such as analyses of clinical workflows. Decision trees are a method for rule-based classifications: Each node in the tree represents one rule. Following the tree from the root node to the leaves, it provides exactly one answer to a specific question. The disadvantages of decision trees include their size when rules are very complex and their relatively low recognition rates compared to other algorithms [22,23]. Finally, we decided to use a HMM in this case, but we will eventually perform the study with other algorithms in the future to compare the results.

A HMM describes a process over time. This process is characterized by a sequence of states that change by chance. The state change is modeled with a random variable 
                              
                                 
                                    
                                       X
                                    
                                    
                                       t
                                    
                                 
                              
                           , where the transition probability between the states only depends on the current state, not on the past states. This property is called the “memorylessness” or Markov property. The states are not directly observable in a HMM because they are hidden, but each state yields so-called observations that are used for inferring the most probable state at time t. Observations can also be described with a random variable 
                              
                                 
                                    
                                       Y
                                    
                                    
                                       t
                                    
                                 
                              
                           . A HMM is a 5-tuple 
                              
                                 λ
                                 =
                                 (
                                 S
                                 ;
                                 M
                                 ;
                                 A
                                 ;
                                 B
                                 ;
                                 π
                                 )
                              
                           , where:
                              
                                 •
                                 
                                    
                                       
                                          S
                                          =
                                          {
                                          
                                             
                                                s
                                             
                                             
                                                1
                                             
                                          
                                          ;
                                          …
                                          ;
                                          
                                             
                                                s
                                             
                                             
                                                n
                                             
                                          
                                          }
                                       
                                    : Number of hidden states and values of 
                                       
                                          
                                             
                                                X
                                             
                                             
                                                t
                                             
                                          
                                       
                                    .


                                    
                                       
                                          M
                                          =
                                          {
                                          
                                             
                                                m
                                             
                                             
                                                1
                                             
                                          
                                          ;
                                          …
                                          ;
                                          
                                             
                                                m
                                             
                                             
                                                m
                                             
                                          
                                          }
                                       
                                    : Number of possible observations and values of 
                                       
                                          
                                             
                                                Y
                                             
                                             
                                                t
                                             
                                          
                                       
                                    .


                                    
                                       
                                          A
                                          ∈
                                          
                                             
                                                R
                                             
                                             
                                                n
                                                ×
                                                n
                                             
                                          
                                       
                                    : Transition matrix; 
                                       
                                          
                                             
                                                a
                                             
                                             
                                                i
                                                ,
                                                j
                                             
                                          
                                       
                                     denotes the probability of changing from state i to state j.


                                    
                                       
                                          B
                                          ∈
                                          
                                             
                                                R
                                             
                                             
                                                n
                                                ×
                                                m
                                             
                                          
                                       
                                    : Observation matrix; 
                                       
                                          
                                             
                                                b
                                             
                                             
                                                i
                                                ,
                                                j
                                             
                                          
                                       
                                     denotes the probability of observing 
                                       
                                          
                                             
                                                m
                                             
                                             
                                                j
                                             
                                          
                                       
                                     in state 
                                       
                                          
                                             
                                                s
                                             
                                             
                                                i
                                             
                                          
                                       
                                    .


                                    
                                       
                                          π
                                          ∈
                                          
                                             
                                                R
                                             
                                             
                                                n
                                             
                                          
                                       
                                    : Initial state distribution; 
                                       
                                          
                                             
                                                π
                                             
                                             
                                                i
                                             
                                          
                                          =
                                          P
                                          (
                                          
                                             
                                                X
                                             
                                             
                                                1
                                             
                                          
                                          =
                                          
                                             
                                                s
                                             
                                             
                                                i
                                             
                                          
                                          )
                                       
                                     denotes the probability that 
                                       
                                          
                                             
                                                s
                                             
                                             
                                                i
                                             
                                          
                                       
                                     is the initial state.

To find the most likely state sequence 
                              
                                 Q
                                 =
                                 {
                                 
                                    
                                       q
                                    
                                    
                                       1
                                    
                                 
                                 ;
                                 …
                                 ;
                                 
                                    
                                       q
                                    
                                    
                                       T
                                    
                                 
                                 }
                                 ∈
                                 S
                              
                            for a given HMM 
                              
                                 λ
                              
                            and observation sequence 
                              
                                 O
                                 =
                                 {
                                 
                                    
                                       o
                                    
                                    
                                       1
                                    
                                 
                                 ;
                                 …
                                 ;
                                 
                                    
                                       o
                                    
                                    
                                       T
                                    
                                 
                                 }
                                 ∈
                                 M
                              
                           , the Viterbi algorithm can be used [24]. The Viterbi algorithm incorporates the transition and observation probabilities of individual states as well as the probabilities previously calculated for all states 
                              
                                 t
                                 -
                                 1
                              
                           . This makes the Viterbi algorithm more robust to zero probabilities (
                              
                                 
                                    
                                       a
                                    
                                    
                                       i
                                       ,
                                       j
                                    
                                 
                                 =
                                 0
                              
                            for some i and j) [1]. However, lacking probabilities in the underlying model may lead to serious problems. Two approaches for solving these drawbacks are presented in Section 3.3.

Clinical information is not stored within a central database but is rather distributed across multiple information systems, such as HIS, Radiology Information System (RIS), PACS or research-driven department-internal solutions [25]. This leads to inefficient clinical workflows and hinders the execution of clinical studies due to the lack of patient-specific information and error-prone copy-paste procedures. To mitigate the aforementioned drawbacks, a web-based clinical information system, oncoflow, has been developed [26]. The oncoflow system has been tailored to support physicians and surgeons in daily clinical routine during the treatment of patients with head and neck carcinoma. Therefore, electronic communication interfaces have been developed to import therapy-related information into a central database (see Fig. 2
                           ). There are two main information sources, the HIS and the DTTM, that provide the majority of relevant patient-specific information. The first consultation reveals important therapy-related details, such as personal habits like alcohol or nicotine consumption, comedication or former surgical interventions. This information is acquired using a structured web-based documentation form. Finally, a broad set of information is acquired during follow-up consultations through the use of an Android tablet-based screening tool for functional disorders [27,28].

Physicians and surgeons at the University Medical Center Leipzig have used the oncoflow system in daily clinical routine as a scientific prototype since March 2013. To train the HMM, only an Electronic Patient Record (EPR) that contains at least a first consultation, a panendoscopy, a tumor board and a follow-up dataset is used to ensure that the patient already underwent the entire treatment process. Available tumor therapy-related ICD10 and ICPM datasets serve as important training data for the HMM.

The patient-specific information in the oncoflow database is not instantly usable as training data for the HMMs. Hence, an appropriate EPR must be identified, after which relevant information entities can be extracted. First, the patient datasets, which contain at least one entity of consultation, panendoscopy, tumor board and follow-up, were identified in the database. The resulting 40 available EPRs that met the required criteria provided the basis for creating the HMMs. Subsequently, the previously mentioned information entities and the tumor therapy-related ICD10 and ICPM codes from the specific patients were selected and exported in chronological order as a Comma-Separated Values (CSV) file, which serves as the input for the HMM. Detailed numbers are presented in Table 1
                           . An information entity is also referred to as an observation.

The ICD10 and ICPM codes provide a hierarchical mapping of health conditions or operation procedures to generic categories. To reduce the complexity of the HMMs and to increase the hidden state recognition rate, two different approaches are evaluated in this paper. The first approach is to construct the HMMs with full ICD10 and ICPM codes, which results in a more complex model with a larger number of observations. The second approach is to reduce the depth in the hierarchy by shortening the ICD10 and ICPM codes to a coarse granularity.

In the following examples, the ICD10 code C32 and the ICPM code 5-303 should clarify the procedure. C32 denotes a malignant tumor in the larynx and can be viewed as a parent node in the ICD10 classification. In the selected EPRs, more specific descriptions of this disease are used, such as C32.0, which denotes a malignant tumor in the glottis, or C32.3, which denotes a malignant tumor in the cartilage of the larynx. The same holds for the ICPM code 5-303, which encodes a laryngectomy. In more fine-granular representations, 5-303.0 denotes a simple laryngectomy, and 5-303.01 denotes a reconstruction with local mucosa.

To evaluate the influence of the aforementioned code lengths on the prediction results, a post-processing of the exported information was performed. Therefore, all of the ICD10 and ICPM codes were shortened to their hierarchical parents and stored in a second CSV file for later use. Using the shortened disease and procedure codes results in fewer observations and more robust transition probabilities in the HMM.

The oncological treatment process can be represented by different levels of granularity. At the finest-grained level, the process can be characterized by information entities in the EPR, such as ICD10 or ICPM codes. Unfortunately, this information is not readily usable by physicians or surgeons for obtaining a quick overview on the current medical status of the patient or for identifying the relevant information entities in the current treatment step. Hence, a presentation of the process in an adjustable granularity must be developed for the specific clinical or organizational needs.

In this study, two types of HMMs with different numbers of hidden states were developed in close cooperation with physicians and surgeons from the local ENT department (see Fig. 3
                        ). The 3-state model is a coarse representation of the workflow as described in Section 2.1 and allows existing patient data to be divided into three different categories. The finer-grained 7-state model is actually more helpful for physicians in daily clinical routine because the current treatment steps, such as panendoscopy or therapy, are clearly identifiable and existing patient data from the current step can be acquired instantly.

To train the HMM, each of the 2208 observations were manually tagged with the correct state for the 3-state model and for the 7-state model. The appropriate states for the specific observations were identified by studying the specific EPRs in the HIS and receiving support from physicians and surgeons to clarify observations in which the actual state was not clearly identifiable. Finally, four CSV files were created for generating the HMM:
                           
                              •
                              3-state model with full ICD10/ ICPM codes,

3-state model with short ICD10/ICPM codes,

7-state model with full ICD10/ICPM codes,

7-state model with short ICD10/ICPM codes.

Based on the tagged observations, the initial state distribution 
                           
                              π
                           
                        , the transition probability matrix A and the observation matrix B can be created. The length of the initial state distribution vector 
                           
                              π
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    n
                                 
                              
                           
                         is equal to the number of hidden states of the HMM. For each data set included in the model, the hidden state of the first observation is assigned to the vector element of this hidden state in 
                           
                              π
                           
                        . Finally, the initial state distribution is calculated. The transition matrix 
                           
                              A
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    n
                                    ×
                                    n
                                 
                              
                           
                        , where n denotes the number of states, is generated by assigning each state transition to the corresponding element in the matrix. If no state transition occurs from the observation at time t to the observation at time 
                           
                              t
                              +
                              1
                           
                        , then the matrix element 
                           
                              
                                 
                                    A
                                 
                                 
                                    j
                                    ,
                                    j
                                 
                              
                           
                        , where j denotes the current state, is increased by 1. Let n be the number of states and m be the number of observations; then, the observation matrix is given by 
                           
                              B
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    n
                                    ×
                                    m
                                 
                              
                           
                        . The matrix is generated by assigning each observation and its hidden state to the corresponding matrix element. The final probability distributions for A and B are row stochastic and calculated for each row in the matrix such that 
                           
                              
                                 
                                    ∑
                                 
                                 
                                    j
                                    =
                                    1
                                 
                                 
                                    n
                                 
                              
                              A
                              [
                              i
                              ,
                              j
                              ]
                              =
                              1
                              
                              
                                 
                                    ∀
                                 
                                 
                                    i
                                 
                              
                              ∈
                              {
                              1
                              …
                              n
                              }
                           
                         and 
                           
                              
                                 
                                    ∑
                                 
                                 
                                    j
                                    =
                                    1
                                 
                                 
                                    m
                                 
                              
                              B
                              [
                              i
                              ,
                              j
                              ]
                              =
                              1
                              
                              
                                 
                                    ∀
                                 
                                 
                                    i
                                 
                              
                              ∈
                              {
                              1
                              …
                              n
                              }
                           
                         hold.

Statistical models such as HMMs are based on probability distributions for predicting hidden states from given observations. Thus, specific observations may be misclassified or not classified at all due to missing transitions or observation probabilities. The exception handling presented in this section focuses on observations that are not classified by the HMMs and are thus tagged as unclassified (see Fig. 4
                           ). There are two reasons why observations may not be classified.

The first reason is that a specific observation is only present in one EPR. Hence, when this patient record serves as test object during the leave-one-out cross validation, the specific observations are not included in the HMM generation, the respective elements in the B matrix have zero probability, and the model fails during the study. This situation is mitigated beforehand, and the specific observations are tagged as unclassified. A detailed description is given in Section 4.

Unfortunately, the classification with the Viterbi algorithm also fails in the case of specific observation sequences in which transition and observation probabilities are given. This case occurs when the study observation sequence significantly differs from the observation sequences used to generate the model. The respective observations are also tagged as unclassified.

To improve the overall recognition rates of the HMMs, two algorithms for exception handling were developed. The algorithms are applied to the state sequence produced by the Viterbi algorithm and aim to identify and re-classify the existing unclassified observations.

The first exception-handling algorithm sequentially parses a given state sequence for unclassified states and remembers the last classified state. If an unclassified state is found, it is assumed that the model is still in the same state as predicted for the previous observation, and the previous state is assigned to the actual observation. This approach has two major drawbacks: the first is that this algorithm is not applicable if the first observation is already unclassified – in this case, the first and all immediately following unclassified observations remain unchanged. The second drawback is that this approach can only be applied to models in which consecutive states are equal for a certain number of observations. If the hidden states change frequently in consecutive observations, this approach may yield poor outcomes.

The Observation-specific State Estimation (OSE) approach is more sophisticated and aims to overcome the drawbacks of the previous recognized state algorithm described in Section 3.3.2. OSE incorporates knowledge about the hierarchical structure of ICD10 and ICPM codes in combination with modified parts of standard HMM algorithms to finally acquire hidden states for observations in which the standard HMM failed. The input parameters for OSE are the state sequence produced by the HMM’s Viterbi algorithm 
                              
                                 
                                    
                                       
                                          
                                             ν
                                          
                                          
                                             →
                                          
                                       
                                    
                                    
                                       Viterbi
                                    
                                 
                              
                           , the corresponding observation sequence 
                              
                                 
                                    
                                       ω
                                    
                                    
                                       →
                                    
                                 
                              
                            and the underlying HMM, including the 
                              
                                 A
                                 ,
                                 B
                              
                            and 
                              
                                 π
                              
                            matrices; the set of hidden states S; and the set of observations M (see Algorithm 1).
                              Algorithm 1
                              The Observation-specific State Estimation (OSE) 
                                    
                                       
                                          
                                          
                                             
                                                
                                                   input: observation sequence 
                                                      
                                                         
                                                            
                                                               ω
                                                            
                                                            
                                                               →
                                                            
                                                         
                                                      
                                                   , Viterbi result sequence 
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     ν
                                                                  
                                                                  
                                                                     →
                                                                  
                                                               
                                                            
                                                            
                                                               Viterbi
                                                            
                                                         
                                                      
                                                   , HMM 
                                                      
                                                         λ
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                   1: 
                                                   for each 
                                                   
                                                      
                                                         
                                                            
                                                               ν
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                         ∈
                                                         
                                                            
                                                               
                                                                  
                                                                     ν
                                                                  
                                                                  
                                                                     →
                                                                  
                                                               
                                                            
                                                            
                                                               Viterbi
                                                            
                                                         
                                                         
                                                         |
                                                         
                                                         
                                                            
                                                               ν
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                         =
                                                         unclassified
                                                      
                                                    
                                                   do
                                                
                                             
                                             
                                                
                                                   
                                                   2: highestProbability=0
                                             
                                             
                                                
                                                   
                                                   3: mostProbableState=
                                                   unclassified
                                                
                                             
                                             
                                                
                                                   
                                                   4: observation 
                                                      
                                                         
                                                            
                                                               ω
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                         =
                                                         
                                                            
                                                               ω
                                                            
                                                            
                                                               →
                                                            
                                                         
                                                         [
                                                         index
                                                         (
                                                         
                                                            
                                                               ν
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                         )
                                                         ]
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                   5: HiddenStates S
                                                   =
                                                   
                                                      
                                                         λ
                                                         .
                                                         S
                                                      
                                                    // get list of hidden states from HMM
                                             
                                             
                                                
                                                   
                                                   6:
                                             
                                             
                                                
                                                   
                                                   7: 
                                                   for each 
                                                   
                                                      
                                                         s
                                                         ∈
                                                         S
                                                      
                                                    
                                                   do
                                                
                                             
                                             
                                                
                                                   
                                                   8: 
                                                   if 
                                                   
                                                      
                                                         
                                                            
                                                               ω
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                      
                                                    
                                                   
                                                      
                                                         ∈
                                                         λ
                                                      
                                                    
                                                   then
                                                
                                             
                                             
                                                
                                                   
                                                   9: 
                                                   _prob=
                                                   calculateHighestStateProbability(
                                                      
                                                         
                                                            
                                                               ω
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                         ,
                                                         s
                                                         ,
                                                         λ
                                                      
                                                   )
                                             
                                             
                                                10: 
                                                   else
                                                
                                             
                                             
                                                11: 
                                                   _simObs=
                                                   findSimilarObservations(
                                                      
                                                         
                                                            
                                                               ω
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                      
                                                   )
                                             
                                             
                                                12: 
                                                   _prob=
                                                   calculateHighestStateProbability(_simObs, 
                                                      
                                                         s
                                                         ,
                                                         λ
                                                      
                                                   )
                                             
                                             
                                                13:
                                             
                                             
                                                14: 
                                                   if _prob>highestProbability then
                                                
                                             
                                             
                                                15: 
                                                   highestProbability=_prob
                                             
                                             
                                                16: 
                                                   mostProbableState=s
                                             
                                             
                                                17:
                                             
                                             
                                                18: 
                                                   // fallback solution
                                                
                                             
                                             
                                                19: 
                                                   if highestProbability=0 then
                                                
                                             
                                             
                                                20: mostProbableState=
                                                   usePreviousRecognizedState()
                                                
                                             
                                             
                                                21:
                                             
                                             
                                                22: 
                                                   
                                                      
                                                         
                                                            
                                                               ν
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                      
                                                   
                                                   =mostProbableState
                                             
                                          
                                       
                                    
                                 
                              

Initially, the given sequence of resulting states 
                              
                                 
                                    
                                       
                                          
                                             ν
                                          
                                          
                                             →
                                          
                                       
                                    
                                    
                                       Viterbi
                                    
                                 
                              
                            is sequentially parsed to locate unclassified states (see Algorithm 1; line 1). If an unclassified state 
                              
                                 
                                    
                                       ν
                                    
                                    
                                       i
                                    
                                 
                              
                            is found, the corresponding observation 
                              
                                 
                                    
                                       ω
                                    
                                    
                                       i
                                    
                                 
                              
                            is acquired from the observation sequence. To find the most probable state for 
                              
                                 
                                    
                                       ω
                                    
                                    
                                       i
                                    
                                 
                              
                           , the OSE algorithm is applied to all states s in the actual model (see Algorithm 1; line 7). Then, two different cases must be distinguished. In the first case, the current observation is part of the HMM, which means that this observation was part of the learning process of the model and has transition and state probabilities in the corresponding matrices. Hence, these probabilities can be immediately used to calculate the highest state probability (see Algorithm 3). In the second case, the observation is not part of the model training; hence, there are no transition and observation probabilities available. Thus, in the case that the observation is an ICD10 or ICPM code, similar observations are searched within all available observations M (see Algorithm 2), and the highest state probability for these observations can be subsequently computed (see Algorithm 1; lines 11, 12). Subsequently, the highest probability and the corresponding state are updated.

However, there is still a possibility that no appropriate similar observations can be found, and thus, the hidden state remains unclassified. In this case, the previous recognized state is selected as the fallback solution, as depicted in Section 3.3.2 (see Algorithm 1; lines 19, 20). Finally, the state of the current observation is updated.
                              Algorithm 2
                              
                                 findSimilarObservations 
                                 
                                    
                                       
                                          
                                          
                                             
                                                
                                                   input: observation 
                                                      
                                                         
                                                            
                                                               ω
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                      
                                                   , hidden state s, current HMM 
                                                      
                                                         λ
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   output: array of observations 
                                                      
                                                         Θ
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                   1: Observations M = 
                                                      
                                                         λ
                                                         .
                                                         M
                                                         ⧹
                                                         
                                                            
                                                               ω
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                      
                                                    // get list of observations from HMM w/o 
                                                   
                                                      
                                                         
                                                            
                                                               ω
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                   2:
                                             
                                             
                                                
                                                   
                                                   3: if 
                                                   
                                                      
                                                         (
                                                         ω
                                                         ∈
                                                         ICD
                                                         10
                                                         )
                                                      
                                                    
                                                   or 
                                                   
                                                      
                                                         (
                                                         ω
                                                         ∈
                                                         ICPM
                                                         )
                                                      
                                                    
                                                   then
                                                
                                             
                                             
                                                
                                                   
                                                   4:
                                             
                                             
                                                
                                                   
                                                   5: found=false
                                             
                                             
                                                
                                                   
                                                   6: searchString = 
                                                      
                                                         
                                                            
                                                               ω
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                   7: 
                                                   while searchString.length>1 and ! found do
                                                
                                             
                                             
                                                
                                                   
                                                   8: searchString.removeLastCharacter
                                             
                                             
                                                
                                                   
                                                   9:
                                             
                                             
                                                10: 
                                                   
                                                   for each 
                                                   
                                                      
                                                         m
                                                         ∈
                                                         M
                                                      
                                                    
                                                   do
                                                
                                             
                                             
                                                11: 
                                                   
                                                   
                                                   if 
                                                   m.startsWith(searchString) then
                                                
                                             
                                             
                                                12: 
                                                   
                                                   
                                                   
                                                   
                                                      
                                                         Θ
                                                      
                                                   .add(m)
                                             
                                             
                                                13: 
                                                   
                                                   
                                                   found=true
                                             
                                             
                                                14:
                                             
                                             
                                                15: return 
                                                   
                                                      
                                                         Θ
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              


                           Algorithm 2 shows the procedure for finding observations that are similar to a given observation. Here, the hierarchical structure of ICD10 and ICPM codes is utilized. First, a regular expression is used to ensure that the given observation 
                              
                                 
                                    
                                       ω
                                    
                                    
                                       i
                                    
                                 
                              
                            is an ICD10 or ICPM code (see Algorithm 2; line 3). Then, a search string is created from the observation. During each iteration in the while-loop, the search string is shortened by one character, which means that the resulting string matches a wider set of ICD10 or ICPM codes, as described in detail in Section 3.1.2. Subsequently, the current search string is compared to all observations in the given HMM. When the search string matches a given observation code for the first time (longest prefix match), the corresponding code is saved in an array 
                              
                                 Θ
                              
                           , and the while-loop is stopped for the next iteration (see Algorithm 2; lines 11–13). The for-loop processes the remaining observations in M such that the most similar observations may be found.
                              Algorithm 3
                              
                                 calculateHighestStateProbability 
                                 
                                    
                                       
                                          
                                          
                                             
                                                
                                                   input: array of observations 
                                                      
                                                         Θ
                                                      
                                                   , hidden state s, HMM 
                                                      
                                                         λ
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   output: probability for being in state s
                                                
                                             
                                             
                                                
                                                   
                                                   1: TransitionMatrix A
                                                   =
                                                   
                                                      
                                                         λ
                                                         .
                                                         A
                                                      
                                                    // get transition matrix from HMM
                                                
                                             
                                             
                                                
                                                   
                                                   2: ObservationMatrix B
                                                   =
                                                   
                                                      
                                                         λ
                                                         .
                                                         B
                                                      
                                                    // get observation matrix from HMM
                                                
                                             
                                             
                                                
                                                   
                                                   3: HiddenStates S
                                                   =
                                                   
                                                      
                                                         λ
                                                         .
                                                         S
                                                      
                                                    // get list of hidden states from HMM
                                                
                                             
                                             
                                                
                                                   
                                                   4: highestProbability=0
                                             
                                             
                                                
                                                   
                                                   5:
                                             
                                             
                                                
                                                   
                                                   6: for each 
                                                   
                                                      
                                                         θ
                                                         ∈
                                                         Θ
                                                      
                                                    
                                                   do
                                                
                                             
                                             
                                                
                                                   
                                                   7: observationProbability=
                                                   
                                                      
                                                         B
                                                         [
                                                         s
                                                         ]
                                                         [
                                                         θ
                                                         ]
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                   8:
                                             
                                             
                                                
                                                   
                                                   9: 
                                                   for each hidden state 
                                                      
                                                         k
                                                         ∈
                                                         S
                                                      
                                                    
                                                   do
                                                
                                             
                                             
                                                10: transitionProbability=
                                                   
                                                      
                                                         A
                                                         [
                                                         k
                                                         ]
                                                         [
                                                         s
                                                         ]
                                                      
                                                   
                                                
                                             
                                             
                                                11: currentProbability=observationProbability*transitionProbability
                                             
                                             
                                                12:
                                             
                                             
                                                13: 
                                                   
                                                   if currentProbability>highestProbability then
                                                
                                             
                                             
                                                14: 
                                                   
                                                   highestProbability=currentProbability
                                             
                                             
                                                15:
                                             
                                             
                                                16: return highestProbability
                                             
                                          
                                       
                                    
                                 
                              


                           Algorithm 3 aims to find the highest probability for observation 
                              
                                 
                                    
                                       ω
                                    
                                    
                                       i
                                    
                                 
                              
                            being in state s at time t. Therefore, the transition probabilities from all states 
                              
                                 k
                                 ∈
                                 S
                              
                            at time 
                              
                                 t
                                 -
                                 1
                              
                            into state s at time t are multiplied by the probability of the observation being in state s. This procedure is performed for all observations 
                              
                                 Θ
                              
                           , and finally, the highest probability is returned. Unlike the forward variables 
                              
                                 
                                    
                                       α
                                    
                                    
                                       t
                                    
                                 
                                 (
                                 i
                                 )
                              
                            in [1], the HMM model probabilities for being in state k at time 
                              
                                 t
                                 -
                                 1
                              
                            are not included in the actual calculation to avoid the influences of any incorrect calculations from the Viterbi algorithm.

To apply a HMM to a given observation sequence for predicting the corresponding sequence of hidden states, additional mathematical algorithms, such as the Viterbi algorithm (see Section 2.3.2), are necessary. Therefore, the open source software library jahmm (https://code.google.com/p/jahmm/) was used. This library implements the basic Forward and Viterbi HMM algorithms as well as the Baum-Welch and K-Means structure learning algorithms. A Java-based command line application was developed to implement the model generation process (see Section 3.2), to include the jahmm library and to perform the HMM exception handling (see Section 3.3). The software also performs the statistical calculations presented in Section 5.

The aforementioned theoretical approaches for HMM model generation, hidden state prediction and exception handling were conducted in an evaluation study. The study was performed with a command line Java application (see Section 4.1) and four CSV files containing the input observations and the corresponding hidden states (see Section 3.2). Each input file contained 
                           
                              N
                              =
                              40
                           
                         EPRs with the specific observations in exactly the same order but in a different granularity or as a different model type (3-state or 7-state).

To estimate the accuracy of the HMM and the presented exception-handling methods, the leave-one-out cross validation technique was used [29]. This approach uses 
                           
                              N
                              -
                              1
                           
                         observation sequences and the corresponding hidden states (known data) as a training data set. The 
                           
                              N
                           
                        th observation sequence serves as an independent validation sample (unknown data) against which the model is tested. Hence, the study must be performed N times for each CSV input file. Within each iteration, one HMM is generated with the training data and then tested against the left-out observation sequence.

However, some observations are only included in one EPR. These observations are presented in detail in Section 5.1. This leads to significant problems during the execution of the study if the specific EPR serves as the validation data set. In this case, the lacking probabilities in the transition and observation matrices cause the Viterbi algorithm to yield incorrect state prediction results. To mitigate these shortcomings, the validation observation sequence is checked prior to the Viterbi processing. Therefore, each observation of the validation sample is checked against all observations included in the HMM, which are processed during training of the model. If the validation sequence contains such singular observations, the corresponding index is stored and the observation is removed from the original test sequence. Subsequently, the test sequence serves as an input for the Viterbi algorithm such that the processing can be finished without failure. Then, the resulting state sequence is restored to the length of the original observation sequence by inserting the state unclassified at the index positions of the previously removed observations. The resulting state sequence then immediately serves as input data for exception handling or can be used directly for postprocessing.

The resulting state sequences of each iteration are stored for later postprocessing. After postprocessing, the results are compared to the states of the previously tagged observations, and both the correct and incorrect predicted states are counted in different granularities, including for each iteration, for each hidden state and for the entire cross validation. The recognition rates are subsequently calculated. The results are depicted in detail in Section 5.

@&#RESULTS@&#


                        Table 2
                         presents the recognition rates of the Viterbi algorithm without any exception handling. The median values of 
                           
                              86.3
                              %
                           
                         and 
                           
                              86.6
                              %
                           
                         for long ICD10 and ICPM codes (in contrast to 
                           
                              89.6
                              %
                           
                         for short codes) show that the pure HMM is more sensitive to the code length than to the number of hidden states. The minimum recognition rates for long codes are lower than those for short codes. One can also observe that the recognition of at least one observation sequence in the 7-state HMM completely failed because the minimum recognition rate is 
                           
                              0
                              %
                           
                        . This also leads to a significantly higher standard deviation in the 7-state model with long codes.

The first applied exception-handling algorithm that uses the last known state improves the recognition rates in all model types (see Table 3
                        ). In particular, the median value increased to 
                           
                              90
                              %
                           
                         for each type, which thus indicates that the dependency on the length of the underlying ICD10 and ICPM codes could be mitigated. The minimum recognition rates increased significantly from 
                           
                              7
                              %
                           
                         to 
                           
                              10
                              %
                           
                        . Unfortunately, this exception-handling approach is unable to correct observation sequences that are completely unclassified; therefore, the minimum recognition rate of the 7-state model with long codes is still 
                           
                              0
                              %
                           
                        . The average and maximum recognition rates are slightly increased, and the standard deviation shows a slight decrease, except for the 7-state model with long codes.

The results for the OSE approach are presented in Table 4
                        . The overall, median and maximum recognition rates are actually quite similar to those of the previous state exception-handling approach. A major improvement is the increase in the minimum recognition rate of the 7-state model with long codes from 
                           
                              0
                              %
                           
                         to 
                           
                              66.7
                              %
                           
                         and a decrease in the standard deviation from 
                           
                              16
                              %
                           
                         to 
                           
                              8.6
                              %
                           
                        . This result indicates that the OSE approach is able to correct hidden states of observation sequences even if the Viterbi algorithm completely fails.

Finally, in Table 5
                        , the number of states that were corrected by the previous state and the OSE exception-handling algorithm are presented. This table shows that both algorithms handled the same number of unclassified observations. Furthermore, it can be observed that the OSE approach failed for eight specific observations, which are characterized by the following six ICD10 codes: L02.1 (skin cheek abscess), K44.9 (Hernia diaphragmatica), C43.0 (malignant lip malignoma), C64 (malignant renal tumor), F33.2 (depressive dysfunction) and B37.81 (Candida oesophagitis). Within all 2208 observations, the observation L02.1 was present three times in one EPR, and the other observations were present only one time. For these observations, the OSE algorithm could not find similar observations, and thus, the fallback solution was chosen.

In this section, the results regarding the model accuracy are shown, which means that the recognition rates differed according to the hidden states in the HMM.


                        Table 6
                         presents the recognition rates for the 3-state model grouped by the different code lengths and exception-handling mechanisms. As shown, the recognition rates are equally distributed for the diagnostic and therapy phases. The recognition rates during the follow-up phase range between 
                           
                              3
                              %
                           
                         and 
                           
                              7
                              %
                           
                         lower. However, although there is no significant difference between the percentage values, there is no model-phase combination in which the recognition rate reaches 
                           
                              100
                              %
                           
                        .

The result from the therapy-phase based evaluation for the 7-state model is presented in Table 7
                        . The results for the anamnesis, panendoscopy and follow-up phases are homogeneous within the same ICD10 and ICPM code length but show better results for short codes. The tumor board and radiochemotherapy states show high recognition rates for each model type, in which the tumor board state yields recognition rates up to 
                           
                              100
                              %
                           
                        . Finally, the recognition rates for the intervention and the other care phases are less than those of the aforementioned phases. These findings are discussed in detail in Section 6, in which the connection to the specific observation in these phases is established.

The study results are presented in side-by-side boxplots in Fig. 5
                        . Each plot shows a single HMM type, the corresponding recognition rates for the plain Viterbi algorithm and the exception-handling approaches. In general, all of the plots show quite similar recognition rates for the different model types and exception-handling approaches (as previously discussed in detail in Section 5.1). Comparing the interquartile ranges reveals that the spread of the 7-state model results is considerably greater than that of the 3-state model. The median value in the 3-state model is situated in the center of the boxplot and exhibits a symmetric probability distribution. In contrast, the median value in the 7-state model is situated at the top of the box, which shows a high skewness or asymmetric probability distribution. Regarding the interquartile ranges between the long and short codes, plot A and plot B do not show significant differences, which indicates that the algorithms perform identically for the 3-state model regardless of the code length. The interquartile ranges in plot C and plot D show a considerably larger difference, which leads to the conclusion that the code length plays a larger role when the model consists of more states.

Finally, the outliers in the boxplot provide important information about the different algorithms. The 3-state model in plot A and plot B show the most outliers for the plain HMM. Plot A shows additional outliers for the exception-handling algorithms, whereas the 7-state model shows less outliers for the model with long codes in plot C. This plot also shows the two observation sequences that are completely misclassified by the plain HMM but corrected by the OSE approach. The 7-state model with short codes in plot D shows more outliers for all model types.

The recognition rates of the HMM significantly differ for different hidden states, particularly in the 7-state model (see Section 5.2). To explain the reasons for these differences, two overfitted HMMs with short observation codes, including all observation sequences, were created for the 3-state and 7-state models. In general, there are two different types of observations: the first type can be uniquely identified by the HMM; therefore, this observation only occurs within one hidden state. The second observation type consists of observations that occur along the entire treatment process and are present in multiple hidden states. To create an appropriate example, ten representative observations and their observation probabilities in the B matrix were chosen and transformed into levelplot diagrams. For comparison, the same observations for the 3-state model and the 7-state model were chosen and are explained in detail in Table 8
                        .


                        Fig. 6
                         shows the levelplot for the 3-state model observation matrix. The observations A–E are clearly assigned to the clinical diagnostics phase. These are the anamnesis, panendoscopy and tumor board observations that only appear in this phase. In addition to these clearly allocable observations, the following observations, F and G, were chosen. These observations are ICD10 codes, which are encoded every time a patient enters a clinical department throughout the entire treatment process. Thus, these observations are equally distributed across all treatment phases. Subsequently, observation H denotes a surgical intervention where neck lymph nodes are removed. This ICPM code primarily appears in the therapy phase, but it also appears in the diagnostics phase because lymph nodes are sometimes also removed during a panendoscopy. Observation I represents a radiation therapy that is only conducted during the therapy phase. Finally, observation J corresponds to a follow-up meeting, which is typically only performed in the follow-up phase; however, if recidivisms occur directly in the first meeting, the therapy phase is prolonged until the patient is tumor-free.

The levelplot in Fig. 7
                         shows both the states and corresponding observations for the 7-state model. Only the differences between this model and the 3-state model should be mentioned here. Observations A–E are separated into 3 different states but are only present in exactly one state. Observations F and G are present in all states but not in the tumor board state because physicians and surgeons do not encode ICD10 codes when the tumor board discusses a patient. Due to the more fine-grained tagging in the 7-state model, observations H and J are only present in one state. Observation I is again only present in one state, which in this case is a radiochemotherapy state.

@&#DISCUSSION@&#

This work presented a new methodology for predicting clinical workflow steps based on HMMs. The concepts and algorithms were developed and evaluated in a study with anonymized real-world patient data sets of 40 EPRs and a total number of 2208 observations. The performed study provided good results for the recognition of the patient’s current therapy phase based on given observation sequences. The achieved recognition rates ranged between 
                        
                           82
                           %
                        
                      and 
                        
                           90
                           %
                        
                     .


                     Pros: The prediction results of a pure HMM’s Viterbi algorithm primarily depend on the lengths of the used ICD10 and ICPM codes. The number of hidden states in the model does not exhibit a significant influence on the results. The developed exception-handling approaches improve the recognition rates and most notably minimize the influence of long and shortened ICD10 and ICPM codes. Both exception-handling results provide similar outcomes, as shown in Tables 3 and 4. The previous recognized state algorithm uses a type of educated guess in combination with the hypothesis that the actual state of the process does not frequently change over time. In contrast, the OSE algorithm uses actual transition and observation probabilities from the current HMM along with knowledge of the hierarchical structures of ICD10 and ICPM codes to estimate the correct hidden state. Hence, this approach provides higher reliability and robustness; therefore, the OSE algorithm should be preferred over the previous recognized state algorithm.

The influence of an increasing number of hidden states on the recognition rates is an important factor for use in cases where the underlying workflows have variable granularity. Fortunately, the results of this study show that the recognition rates between a model with three states and a model with seven states do not exhibit considerable differences. Hence, this approach can be used in scenarios or use cases where models with a highly variable number of hidden states are necessary. The lengths of ICD10 and ICPM codes were also a subject of the evaluation study. In fact, the results of the study demonstrated that the hierarchical depth of the underlying codes primarily influences the recognition rates of the Viterbi algorithm. This behavior is reasonable given the model complexity. The model with long codes consisted of 185 observations, whereas the model with short codes consisted of only 117 observations. A smaller number of states results in a smaller observation matrix, higher probabilities for each observation and thus better prediction results. The use of exception-handling algorithms improves the recognition rates and increases the robustness of the model. The OSE approach incorporates the previous findings and reduces the model complexity by shortening the codes from unclassified observations. Thus, OSE makes the model less complex to find appropriate transition and observation probabilities.


                     Cons: However, although the number of observations used in the study is quite large, specific aspects in the results show that a larger number of observations would have made the model more robust against complex treatment processes. The outliers in Fig. 5 plot C can be viewed as an example for such an observation sequence originating from a rare complex medical case. A larger set of training data with a wider variety of treatment processes would result in models that are better fitted to cluttered workflows and thus provide better recognition results and a decreased need for exception handling.

Some remarks on the OSE algorithm are also necessary. Observations that are not ICD10 or ICPM codes can only be corrected by OSE when transition and observation probabilities are available in the corresponding matrices. A search for similar observations is not currently possible. Hence, if no probabilities are given for these observations, the last known hidden state is assigned. The search for similar ICD10 and ICPM codes works very well for the oncological therapy process in ENT surgery. However, this approach cannot be immediately applied to treatment processes in other clinical disciplines. In different medical fields, a broader range of codes may be encoded such that supposedly similar observations actually yield an incorrect hidden state. Hence, future studies regarding therapy processes for different medical disciplines must be conducted.

@&#CONCLUSIONS@&#

The use of a stochastic model provides a reliable classification of patient-specific information entities with the corresponding workflow step in the treatment process. To the best of our knowledge, this is the first approach designed to infer clinical workflows directly from HIS information entities. There is only one similar approach available in the scientific literature, which was developed by Huang et al., who used clinical event logs for mining clinical pathway patterns [13,12].

Workflow recognition and the automatic classification of patient-specific information are important for the development of assistance functions for different clinical use cases. The knowledge of the patient’s current treatment step provides the basis for workflow assistance. With this knowledge, clinical information systems are able to provide physicians and surgeons with only the information necessary for a specific therapy phase such that less time is spent in searching the appropriate information entities. A second relevant use case in daily clinical routine is the acquisition of patient-specific documents. In existing information systems, physicians and surgeons browse through tens or hundreds of data sets and manually search appropriate documents. It is not possible to answer questions such as “Please give me all documents that have been created during radiation therapy”. When each information entity is tagged with its corresponding workflow step, access to information generated during a specific step in the treatment process can be significantly improved.

Future work will focus on improving the presented approach and integrating it into daily clinical routines. First, the capabilities of the model must be extended. The current state of development does not support breaks in the process in the case of treatment abortion or death, which is very important for the use in daily clinical routines. Additionally, the time elapsed between two observations should be incorporated into the model to support the detection of potential process phase transitions. This information can then be used to refine the transition and observation matrices of the model. Second, the ability to train the model for daily use is a crucial prerequisite. This could be realized using a feedback-based approach, in which experts correct failures of automatically tagged information entities. Furthermore, the HMM will be integrated into oncoflow (see Section 3.1.1) to enrich the patient-specific information with workflow information and to implement the aforementioned clinical workflow assistance functionalities.

@&#ACKNOWLEDGMENTS@&#

I would like to thank Stefan Franke for his kind advice during this project. His profound knowledge of stochastic models helped me to circumvent many problems and made the results even stronger. I also want to thank Ulrike Gerber for her patience in proofreading this paper, her constructive suggestions and her outstanding support in interpreting the results.

ICCAS is funded by the German Federal Ministry of Education and Research (BMBF), the Saxon Ministry of Science and Fine Arts (SMWK) in the scope of the Unternehmen Region with Grant No. 03Z1LN12, the European Regional Development Fund (ERDF) and the state of Saxony within the frame of measures to support the technology sector.

@&#REFERENCES@&#

