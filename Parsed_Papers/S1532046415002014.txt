@&#MAIN-TITLE@&#Textual inference for eligibility criteria resolution in clinical trials

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Manual chart review for clinical trial screening is a laborious process.


                        
                        
                           
                           We present the task of identifying criteria relevant sentences in clinical notes.


                        
                        
                           
                           This is formulated as a search-based textual entailment problem.


                        
                        
                           
                           We describe creation of a dataset and implementation of four baseline methods.


                        
                        
                           
                           Method using similarity between UMLS concept pairs performs the best.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Natural language processing

Electronic health records

Textual inference

Clinical trials

@&#ABSTRACT@&#


               
               
                  Clinical trials are essential for determining whether new interventions are effective. In order to determine the eligibility of patients to enroll into these trials, clinical trial coordinators often perform a manual review of clinical notes in the electronic health record of patients. This is a very time-consuming and exhausting task. Efforts in this process can be expedited if these coordinators are directed toward specific parts of the text that are relevant for eligibility determination. In this study, we describe the creation of a dataset that can be used to evaluate automated methods capable of identifying sentences in a note that are relevant for screening a patient’s eligibility in clinical trials. Using this dataset, we also present results for four simple methods in natural language processing that can be used to automate this task. We found that this is a challenging task (maximum F-score=26.25), but it is a promising direction for further research.
               
            

@&#INTRODUCTION@&#

Clinical trials play an important role in medical research. The successful completion of a trial is dependent on achieving a significant sample size of patients enrolled into the trial within a limited time period. However, the process of eligibility determination is extremely challenging and time-consuming, often mandating manual chart review [1,2]. Such reviews can involve repeated readings of the patients’ electronic health record (EHR) for multiple trials, across every visit. This limits the number of patients that can be evaluated. Although institutions participating in clinical trials spend significant resources in conducting eligibility screening, the cost of screening is generally not fully compensated through the contracts supporting the trials [3].

The eligibility requirements of a patient, for a clinical trial, are specified in the form of inclusion and exclusion criteria. These are detailed descriptions of the characteristics a patient must or must not have in order to participate in the trial. Patients can be pre-screened for eligibility by referring to either structured or unstructured data in the EHR, or a combination of both. While structured data such as diagnosis codes, laboratory results, medication orders, procedure information, and problem lists are useful for eligibility determination, clinical notes still remain the preferred means of documentation for physicians [4]. These notes frequently contain nuances of clinical presentation and care that are critical for making an eligibility screening determination that the structured data does not. Moreover, the criteria are specified in natural language. Hence, not all criteria can be translated into queries that leverage structured data. Köpcke et al. [5] found that there was a significant gap (65%) between the structured data documented for patient care and the data required for eligibility assessment.

Thus, the use of clinical notes from the EHR is imperative for eligibility determination and clinical trial coordinators often undertake lengthy reviews of the EHR, specifically clinical notes, to assist with determining patient eligibility. Since this process is expensive in terms of time and effort, it would be desirable to speed up this step in the eligibility screening workflow. The NLP community has developed techniques for problems such as question answering [6], textual entailment [7], textual inference [8] and information retrieval [9], which can be used for identifying text of interest from the entire document.

In this paper, we describe the creation of a dataset and evaluate baseline methods for identifying text in clinical notes that is pertinent to a given eligibility criterion for a trial. We compare simple methods in natural language processing (NLP) that can identify specific sentences in clinical notes that are relevant to a trial’s eligibility criteria. Such methods may be used to direct trial coordinators to specific parts of the notes, making the process of manual review easier, in the future. Recently, Köpcke et al. [10] reviewed 79 Clinical Trial Recruitment Support Systems (CTRSS) across 101 publications and found that success of a CTRSS depends on its successful workflow integration, rather than on sophisticated reasoning and processing algorithms. Although it needs formal validation, we believe that our proposed approach can be incorporated into the existing workflow seamlessly and will therefore help expedite the eligibility determination process.

In order to evaluate the performance of an automated system that can identify text relevant to eligibility criteria in clinical notes, we created a new gold standard dataset, as outlined below. The 2014 i2b2 challenge [11] had two tracks with a training set of 790 notes, with annotations for protected health information and risk of heart disease, respectively. The third track evaluated the usability of systems developed for the i2b2 challenges. The fourth track allowed participants to demonstrate novel use of the data made available through the challenge. This work was a submission to this fourth track, proposing the use of NLP for expediting clinical trial eligibility screening. Since a limited amount of time was available to attempt the challenge, 80 notes were annotated for four criteria (20 notes per criterion) by our team of annotators (described in Section 2.3). Every annotator marked all full sentences in a note that were relevant to an eligibility criterion. These annotations are similar to other NLP shared tasks [7,12] such as the Recognizing Textual Entailment (RTE) challenges in the open domain. Therefore, systems developed for these annotations can leverage NLP research from such shared tasks for a challenging domain-specific real world problem and identify interesting research questions. There is at present no other publicly available dataset in the clinical domain with such annotations.

The National Library of Medicine and the National Institutes of Health maintain the website www.clinicaltrials.gov, a registry and a database of publicly and privately supported clinical studies across the globe. The 2014 i2b2 challenge provided notes for coronary artery disease (CAD) patients from three cohorts: (1) patients with no CAD, (2) patients who developed CAD over the course of provided notes for that patient, and (3) patients who had CAD from beginning of their record. Therefore, we downloaded from clinicaltrials.gov study record data for all trials associated with the search term “coronary artery disease” as XML files (5054 trials in the May 2014 download). We extracted eligibility criteria for these trials and segmented them into individual sentences using a combination of the Ling-pipe [13] sentence chunking module trained on MEDLINE biomedical abstracts and user defined rules to identify sentence boundaries. Each sentence was assumed to be a single criterion. The first author (CS) and the physician (CH) in the team considered the following factors while selecting the criteria: (1) the resolution of the criterion would require a healthcare professional to refer to the notes of the patient, (2) it would be hard to assess patient eligibility for the criterion by referring only to the structured data associated with the patient’s EHR, and (3) the criterion is commonly used across clinical trials for CAD. The criteria were selected to reflect realistic situations where clinical trial coordinators spend substantial time reading notes to assess eligibility, and would benefit from a NLP system such as the one proposed in this study.

The segmented criteria sentences described above were analyzed using MetaMap [14] to identify concepts from the Unified Medical Language System (UMLS). Ignoring concepts such as “patient,” “study,” “trials,” and “subject,” we found that medical history (C0262926) and lesion (C0221198) were the most common concepts in these criteria. We chose one eligibility criterion associated with each of these two concepts (Criteria 1 and 2). We also found that angina was the most common concept in the semantic category ‘Sign or Symptom’ across the eligibility criteria for CAD trials. It is a common practice among physicians to specify angina using grades as specified by the Canadian Cardiovascular Society Angina Grading Scale [15]. We chose classification of patients into Class 1 (Criterion 3) and Class 3 angina (Criterion 4) as the other two eligibility criteria in our study. These criteria are shown in Table 1
                        .

Many studies have used physicians to create gold standard datasets in the clinical domain. However, Raghavan et al. [16] showed that individuals with varying level of clinical expertise could also generate high quality annotations. Our annotation team consisted of two senior undergraduate nursing students, a second-year graduate-entry nursing student, and a physician. We developed detailed annotation guidelines for the annotators for each criterion. The annotation process was carried out using CLINical TrIals CrIteria ANnotator (CLINICIAN), a tool developed at our institution. CLINICIAN is a secure web-based tool and has a simple user interface.

After having logged in, annotation for a single note has the following workflow: the user selects a criterion of choice (screenshot shown in Fig. 1a
                        ) to annotate notes for. This brings up a webpage (screenshot shown in Fig. 1b
                        ) that displays the criterion statement at the top and a note beneath it. The user first determines whether the note has any text that is relevant to the criterion and marks the note as “Not relevant” otherwise. If found to be relevant, the user highlights all complete sentences in the note that contribute toward determining, whether the patient meets the criterion, based on that particular note. After selecting all relevant sentences, the user clicks on one of the three buttons “Yes,” “No,” or “Maybe,” indicating an assessment of whether the patient meets the criterion. This brings up the next note and the user repeats the workflow. Table 2
                         summarizes the distribution of these annotations in terms of mean (μ) and standard deviation (σ) across 80 notes in the dataset and the appendix lists the details.

In order to ensure that these guidelines covered all cases, we carried out four training rounds. Each round involved annotations of ten notes per criterion (40 in total). All four annotators worked on the same set of 40 notes. This was followed by a discussion about the correctness of these annotations. Decisions associated with correctness of annotations were made through group discussions led by the physician. The annotation guidelines were updated after every training round to restrict differences amongst annotators. After completing the training, we calculated reliability of agreement amongst the four annotators based on annotations obtained for 20 notes per criterion (80 in total). We ensured that notes from the training rounds were not used in the agreement testing set. We used Fleiss’ kappa [17] as a metric of agreement, since more than two annotators were involved.

We calculated three metrics of agreement. As outlined in the previous section, if a note was found to be relevant, it was annotated for criterion eligibility. Thus, every note could be relevant (“Yes,” “No,” “Maybe”) or “Not Relevant.” The first metric calculated agreement on whether a note was relevant. The second metric calculated agreement for assessment of criterion eligibility criteria across all four answers, “Yes,” “No,” “Maybe,” and “Not Relevant.” All notes were segmented into constituent sentences using the Ling-pipe [13] sentence chunking module trained on MEDLINE biomedical abstracts. We calculated agreement on sentences found to be relevant by the annotators. A sentence was considered relevant only if the annotator highlighted it. We can observe in Table 3
                         that the annotators achieve high agreement considering overall performance across all three metrics.

@&#RELATED WORK@&#

There has been a large body of work investigating automated methods identifying eligible patients for clinical trials. Many articles investigate methods to standardize eligibility criteria for trials [18–20]. These methods facilitate the automation by representing natural language eligibility criteria into a computable format. The medical records tracks of the annual Text Retrieval Challenge (TREC) [21] for 2011 and 2012 aimed at identifying specific cohorts of patients based on the content of clinical notes. The dataset consisted of a variety of clinical notes (radiology reports, progress notes, discharge summaries, etc.) mapped to multiple patient visits. Participants of these challenges had to develop an information retrieval system to identify visits relevant to an eligibility criterion. However, these criteria were synthetically created and the goal was to identify relevant notes at a coarse level of granularity, namely patient visits (a single visit could constitute up to 50 notes). Unfortunately this dataset is no longer available [21]. Ni et al. [22] evaluate a system working on real-world trials with a goal similar to the TREC challenge and retrieve patient encounters relevant to trial. Their system uses a combination of NLP, information extraction and machine learning methods.

This work focuses on a task that involves retrieving information at a finer level of granularity. The goal is to identify specific sentences within a note that are relevant to a trial eligibility criterion. It is similar to the larger problem of question–answering in NLP. Many studies [6,23,24] have explored clinical question–answering in the past. However, all of these systems look into scientific articles for answers. McKeown et al. [25] describe a system that can customize keyword search results over biomedical literature by re-ranking them using concepts identified in a patient’s EHR. As stated in Patrick and Li [26], no question–answering system investigates use of clinical notes for finding answers. Athenikos and Han [27] discuss in their extensive survey of clinical question answering systems, that current systems are limited in the type of questions they can handle. Very few systems can handle questions in natural language, and those that can, are limited to handling a set of restricted types such as definitional questions. This limits the utility of such systems for trial recruitment where criteria are natural language statements, not even questions, with varying degree of specificity and complexity [28].

The Recognizing Textual Entailment (RTE) [7] challenges have also been very popular with the NLP community. The problem statement for these challenges is closest to our work: given two fragments of text (called hypothesis and text), decide whether the meaning of the hypothesis can be inferred from the text. In the first three RTE challenges, automated systems were developed for answering “Yes” or “No” to the question of entailment. For example: The hypothesis “A Filipino hostage was freed in Iraq” should be inferred to be true from the text “A Filipino hostage in Iraq was released.” Similarly, the hypothesis “Time Warner is the world’s largest company” should be inferred to be false from the text “Time Warner is the world’s largest media and Internet company.” The later challenges, RTE-4 [29] to RTE-7 [30] added a third category of “Unknown.” From RTE-1 [7] to RTE-5 [31], while the size of the hypothesis, in terms of number of words, remained almost the same (7–10 words), the average size of the text increased steadily from 24 words in RTE-1 to 99 words in RTE-5. The RTE-5 challenge had a pilot task that was corpus-based: it was a search task aimed at finding all sentences that entail a given hypothesis, in a given set of documents, about a topic. This is analogous to the task of finding all sentences that are relevant to a given eligibility criterion, in a given set of clinical notes, for a particular diagnosis.

The RTE tasks have been known to be hard problems in NLP. None of systems in RTE-1 could perform better than the baseline and accuracy was between 0.5 and 0.6 on a balanced dataset of positive and negative entailments, which steadily increased from 0.75 for RTE-2 to 0.8 for RTE-3. RTE-4 and RTE-5which had a 3-way task to determine entailment between text-hypothesis pairs as “Yes,” “No” and “Unknown,” noticed a fall in accuracy, with the best accuracy being 0.68 for both challenges. The RTE-5 pilot, RTE-6 and RTE-7 challenges had a different setting with a search-based task. The best F1 scores for these tasks were 0.455, 0.48 and 0.48 respectively.

There are a few important differences between the RTE challenges and our task. First, the hypotheses in the RTE challenges were manually curated based on the text against which entailment was to be ascertained. Care was taken to ensure that the hypotheses were explicit, thus limiting ambiguities, as well as concise and easy to interpret in terms of spatial and temporal descriptions. In our case, the criteria and the notes reflect data in the real world. The criteria therefore do not obey the above desirable properties. Second, the texts of interest in RTE challenges were newswire documents, which are fairly consistent and well formed in terms of English grammar, spellings and punctuations. Our text of interest originates from clinical notes, which are characterized by misspellings, abbreviations, incomplete sentences, inconsistent document structure and other undesirable properties of poor text quality for any NLP task. Third, there is a significant word overlap between the text and the hypothesis in these challenges and this increased steadily from 69.25% in RTE-1 to 77.14% in RTE-5 for the positive entailment relation. Thus, it was observed that systems relying on lexical overlap performed well [31]. For the search-based tasks, this overlap dropped to 47.59% for RTE-5 pilot and then increased from 54.66% for RTE-6 to 58.89% for RTE-7. Therefore, it was observed that systems taking word overlap into account had better chances of performing well. Our data collection shows a very low word-overlap between the criteria and the relevant sentences (μ
                     =17%) making the task much more challenging. Lastly, the RTE challenges aimed at open-domain textual inference, while our task is very domain-specific. Systems in the RTE challenges used external knowledge sources such as WordNet [32], Wikipedia extracts, and FrameNet [33]. to enrich their systems. Ablation tests of participating systems indicated that they had a significant impact on the system performance. However such resources are domain-independent and hence have very little coverage in the biomedical domain [34]. Given the quality of data and domain-specific nature of our task, these resources are likely to fall short.

Textual Entailment systems can be applied in three different modes [35]. In the recognition mode, given a text and hypothesis pair as input, the system needs to classify whether entailment holds for the pair or not. This was the focus of challenges RTE-1 to RTE-5. In the search mode, the system is given a hypothesis and a corpus, and needs to find all text fragments in the corpus that entail the hypothesis. RTE-5 had a pilot task and exploring this mode and was the main task for RTE-6 and RTE-7. In the generation mode, the system is given a text, and needs to generate statements which are entailed by the text. Although, we have collected annotations that can be used for either of these modes, the focus of this study is to evaluate a system in the search mode. Thus, given an eligibility criterion of interest, the goal is to automatically identify all sentences in a note that are relevant to that criterion. The system is evaluated using standard metrics of precision, recall and F1 which are computed by comparing the system output with the gold standard annotations.

@&#METHODS@&#

In order to develop an understanding of the task, we implemented two lexical methods considered as baselines in the RTE literature. We also implemented two semantic methods that are adaptations of these baselines to the clinical domain that are informed by specialized knowledge-sources. These implementations develop an understanding of the challenges associated with the task and serve as a direction for further research. These algorithms are applied at a sentence level in every clinical note to determine a relevance score of every sentence with a criterion statement. In terminologies used by the RTE community, these algorithms were applied to pairs of text and hypotheses, where text is a sentence in the note (denoted as N) and hypothesis is a sentence defining the criterion (denoted as C). It should be noted that these methods ignore document-level features such as co-references, negations, and document structure. All sentences in a note having a positive score are considered to be relevant to the criterion.

This is one of the simplest algorithms for textual inference that computes a score based on lexical overlap obtained by matching the lemma for each word in C with the lemma for some word in N, ignoring stop words.

Apache Lucene [36] is an open source text search engine; given a query of search terms, it returns a set of documents ranked by score of relevance. This system served as a pure information retrieval baseline in the search-based RTE challenges (RTE-5 to RTE-7), where every sentence is a document and the search terms in the query are all words in the hypothesis. The Lucene search engine can be configured using parameters to analyze the query terms and the text to be searched, in different ways. We followed parameters as described in the RTE challenge guidelines, namely, the StandardAnalyzer (tokenization, lowercase, stop-word filtering and basic clean-up of words), a Boolean “OR” query for all words in the criterion sentence and the default document scoring function. Unlike the RTE challenge, which uses an older version (2.9.1) of Lucene, we used version 4.4 for our experiments.

As in the RTE challenge, we evaluated this approach by considering a limited number of sentences top-ranked by Lucene, as the relevant sentences for a criterion, across all notes for that criterion. We found that, on average, there were between two to three sentences annotated as relevant to a criterion in our dataset. As mentioned earlier, since 20 notes were annotated for each criterion, we considered different thresholds in the range of 30–50 sentences (in increments of 5) for each criterion.

The UMLS brings together many biomedical vocabularies and standards. It is the largest thesaurus of biomedical concepts, mapping them to semantic types and associating them through hierarchical and non-hierarchical relations. We used MetaMap [14] to identify UMLS concepts in every pair of C and N. A score was computed for every sentence N, as the number of concepts in N that were exactly identical to a concept in C.

UMLS::Similarity is a freely available open source tool [37] that can be used to obtain similarity or relatedness between any two concepts from the UMLS. Given one or more ontologies and one or more hierarchical relations of interest, the tool treats the UMLS as a graph, with concepts as nodes and relations as edges. The tool has a library of measures to compute similarity between two concepts using different graph-based properties. As in the previous approach, we identified UMLS concepts in every pair of C and N using MetaMap. A similarity score was computed between every pair of concepts in a given pair of C and N. The score for a sentence N was the sum of similarity scores its constituent concepts share with the concepts in a criterion C.

Systematized Nomenclature of Medicine–Clinical Terms (SNOMED–CT) is the most comprehensive healthcare terminology in the world. Pedersen et al. [38] demonstrated that similarity measures between clinical concepts, computed using different measures, had high correlation with physicians and human coders. They used parent–child relationships between concepts in SNOMED–CT to define the graph and computed similarity scores. We used the same relations, but on the version of SNOMED–CT (2013_01_31) included in the 2013AA release of the UMLS.

The UMLS::Similarity tool provides implementations of a number of similarity measures capturing different relationships between two concepts. This includes path-based measures, information content-based measures and corpus-based measures. The simplest ones are based on the path information between two concepts in the UMLS graph. The path implementation is simply the inverse of path length between two concepts. The cdist is an implementation of the measure proposed by Rada et al. [39] that computes the number of edges along the shortest path between two concepts. Wu and Palmer [40] proposed a measure, wup incorporating depth of the Least Common Subsumer (LCS) of the two concepts into the similarity calculations. The lch measure proposed by Leacock and Chodorow extends the path measure by incorporating depth of the taxonomy. Finally, Nguyen and Al-Mubaid [41] incorporate both depth and LCS in their measure nam.

Information content (IC) of a concept is defined as the negative log of probability of the concept. Implementation of the simplest measure res proposed by Resnik [42] computes the IC of the LCS of two concepts. The jcn and lin implementations of measures proposed by Jiang and Conrath [43] and Lin [44] respectively propose using IC of the two concepts and their LCS in the similarity calculation.

Finally, the implementation of a corpus based method vector is also available. This method proposed by Patwardhan and Pedersen [45] computes word vectors for each word in the definition of a concept and further captures distributional similarity of words co-occurring with the definition word in the corpus. These vectors are averaged to create a single co-occurrence vector for the concept. Similarity between two concepts is the cosine of the two vectors. A detailed description for each of these metrics can be found in a study [37] published by the authors of the tool.

While some of the similarity measures discussed above are bounded between 0 and 1, some of them are not. We varied the threshold value for considering two concepts to be similar using 10-fold cross validation. Since each criterion was annotated for 20 notes, a fold consisted of two documents. Thus, training was carried out on 18 notes and testing was carried out on 2 notes. An optimal similarity threshold was determined in the training and used in the testing for evaluation.

As mentioned earlier, in order to determine if a note sentence N was relevant to criterion C, the score for a sentence was calculated as the sum of similarity scores its constituent concepts share with the concepts in a criterion C. This similarity score was calculated using the different metrics described above. Path-based measures rely only on the structure of the UMLS graph. However, similarity using the IC measures, res, lin and jcn, require a corpus for the IC calculation of a concept. Similarly the corpus-based measure vector requires a corpus for the distributional similarity calculations. We varied the amount of data used for computing these measures. The data for the i2b2 challenge was made available to the participants in increments. The first batch of training data consisted of 521 notes, followed by a second and final training batch of 269 notes. A test set of 514 notes was also made available. We varied the data used for computing similarity reflecting these increments. First, we used only the 80 notes in our dataset. This was followed by notes from the first training batch (521 notes), the full training set (790 notes) and finally the full data set (1304 notes) comprising both the training and testing sets. It should be noted that although the amount of data for computing IC and distributional similarity was varied in these experiments, the 10-fold cross validation experiments described above always used the same amount of data for training (18 notes) and testing (2 notes) in each fold.

@&#EVALUATION@&#

We evaluated the performance of all four approaches by using standard metrics of precision, recall and F1-score on a per sentence basis. A prediction was assessed as correct if a method identifies a sentence in the note to be relevant to the criterion, as per the gold standard annotations.

The lexical matching and concept matching methods do not involve any parameters. We simply evaluated them on all notes in our dataset. Table 9 summarizes the performance of these two methods across all four criteria in terms of the F1-score. The last column in the table reflects the overall performance of a method, which is the average F1-score across all four criteria.

As stated in Section 5.1, the Lucene-based method was evaluated for different thresholds of top scored sentences considered as relevant to the criterion. The performance remained more or less constant. The best performance was achieved at a threshold of 45 sentences being considered as relevant from the set of 20 notes for each criterion. Table 4
                      summarizes these results. Values in bold indicate the highest value in that column.

The evaluation for UMLS::Similarity-based matching methods was carried out using 10-fold cross validation. We calculated micro-averaged and macro metrics. The micro-averaged F1 was calculated by averaging the F1 score across the 10-folds. It should be noted that although a fold is defined at a note level, the F1 score is calculated at a sentence level. Thus, while calculating this metric, the total number of predictions for a fold equals the sum of number of sentences in the two test notes for that fold. In contrast, a macro-F1 is calculated by considering all sentence level predictions across the 20 notes together. In this calculation, the total number of predictions equals the total number of sentences across all 20 notes. Table 5
                      summarizes the micro-averaged performance of different similarity measures with their standard deviations (σ).

The standard deviation across the folds is very large. This is possibly because there is a large variation in the number of sentences occurring in a note (μ
                     =46.97, σ
                     =33.70). Similarly there is a large variation in the number of criterion-relevant sentences (μ
                     =2.46, σ
                     =1.84). A large number of notes (33=41.25%) in the dataset do not have any sentences that are relevant to a criterion. Therefore we also calculated a macro F1 score, which considers predictions for all sentences in the 20 notes during the test folds for performance calculations. These evaluations are summarized in Table 6
                     .

We also evaluated the IC-based measures and the corpus-based method by varying the amount of data they use in calculating IC and distributional similarity respectively. The results shown in Tables 5 and 6 use only 80 notes from our dataset. The variation in performance due to the amount of data was calculated both using micro-F1 and macro-F1. These results are summarized in Tables 7 and 8
                     
                      respectively. Contrary to our expectations, we observed that performance of most measures worsened with an increase in data. To understand the reason behind these observations, we compared the precision and recall of predictions in experiments with increasing amounts of data. On comparing the precision and recall values for predictions using only 80 notes with those using the full dataset, we found that a drop in the macro-F1 was associated with the decrease in precision of the predictions. It is possible that adding more data is contributing to adding noise than capturing similarity in the IC-based measures.

The res implementation of the information content-based similarity measure proposed by Resnick [42] gave the best macro-F1 score using the full training corpus for information content calculations. The best result from Lucene and UMLS::Similarity-based matching is summarized along with the other two methods in Table 9
                     .

While differences in overall (considering all four criteria together) predictions, between lexical matching and the Lucene based matching are not significant, all other differences are statistically significant (p
                     <0.05) as per McNemar’s test [46]. We investigated the results and found that semantic approaches performed better in criteria where the relevant sentences in a note described the condition in related, but different words. This was the primary reason why the method leveraging UMLS::Similarity performed well. For example, relevant sentences for criterion 2: “History of revascularization procedure” are framed using specific names of procedures such as “CABG (Coronary Artery Bypass Graft)” which are impossible to capture without use of an external knowledge base. This strengthens our intuition behind why textual inference is harder in the clinical domain than open domain due to a low lexical overlap as stated in Section 3. Instances where UMLS Similarity Matching falls short are those where concepts are identified as relevant by human annotators but, the similarity measure fails to capture them. For example: the concepts coronary lesion (C3272304) and NSTEMI (C3537184) are found to be related by the annotators, but the similarity metrics fail to capture this relation. This is most dominant in criterion4 where all methods perform the worse. Although, the largest number of criterion-relevant sentences (n
                     =35) are found for this criterion, there is variability in language that is not captured by the current methods. We believe this is problem can be addressed if more labeled data is available.

The primary limitation of this study is the size of data used in these experiments. We used only 20 notes per criterion (80 notes in total) in our experiments. A larger dataset with annotations for relevant sentences can possibly yield more accurate and robust results. Our annotation team consists of nursing students led by a physician. Having physicians or experts from the domain (in this case, cardiologists) as annotators could improve upon the quality of annotations. The UMLS::Similarity tool is used in conjunction with a reference terminology to compute similarity between UMLS concepts. In this work, we followed previous work [38] and used SNOMED–CT as the reference terminology. Recent studies [47,48] have investigated the problem of choosing the right UMLS terminology for NLP tasks clinical notes. Determining the right terminology or a combination of them to conclude similarity between concepts is an unexplored topic of research. The eligibility criteria used in this study were manually picked by the authors as explained in Section 2.1. Automating this step will be a useful direction of research for textual entailment in clinical text. Although we evaluate a search-based textual entailment task, the annotations described in this study are complete for recognition-based tasks too. Our future work will include investigating machine-learning methods for improving performance of the current system.

@&#CONCLUSION@&#

In this paper, we describe the problem of textual inference in the context of clinical trial recruitment. The objective of this work was to evaluate baseline methods that can identify sentences from clinical notes that are relevant to different clinical trial eligibility criteria. To do this, we constructed a dataset and showed that a high agreement was achieved between annotators with varying level of clinical expertise. This suggests that there is systematic reasoning employed by humans for performing this task. We evaluated baseline computational methods by considering previous work for similar tasks and found that there is significant room for improvement. We found that semantic methods gave better results than lexical methods and appear to be a promising direction for improvement. The biomedical domain has rich knowledge sources such as the UMLS, which can be leveraged to perform intelligent inferences in clinical text. In the future, we plan to explore such resources and linguistic properties of text that are specific to the clinical domain to achieve better results in this task.

The authors declare that they have no competing interest.

@&#ACKNOWLEDGMENTS@&#

We would like to thank our annotators Alissa Schultz, Jennifer Fox and Jessica Schellenbach for their help in the annotation effort. Research reported in this publication was supported by the National Library of Medicine of the National Institutes of Health under award number R01LM011116. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.

Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.jbi.2015.09.008.


                     
                        
                           Supplementary data 1
                           
                        
                     
                  

@&#REFERENCES@&#

