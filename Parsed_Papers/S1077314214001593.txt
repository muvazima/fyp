@&#MAIN-TITLE@&#Discovering salient regions on 3D photo-textured maps: Crowdsourcing interaction data from multitouch smartphones and tablets

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We model human interest on 3D models using multi-touch interactions from tablets.


                        
                        
                           
                           We create a crowdsourcing system to gather interaction data from thousands of users.


                        
                        
                           
                           We propose using the view frustum and a Hidden Markov model for calculating saliency.


                        
                        
                           
                           We present results comparing the proposed interest model to traditional visual saliency.


                        
                        
                           
                           We report results demonstrating the proposed techniques on over 500,000 interactions.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Crowdsourcing

Visual saliency

3D maps

Multitouch interaction

HMM

Gaze-tracking

Smartphones

Mobile devices

@&#ABSTRACT@&#


               
               
                  This paper presents a system for crowdsourcing saliency interest points for 3D photo-textured maps rendered on smartphones and tablets. An app was created that is capable of interactively rendering 3D reconstructions gathered with an Autonomous Underwater Vehicle. Through hundreds of thousands of logged user interactions with the models we attempt to data-mine salient interest points. To this end we propose two models for calculating saliency from human interaction with the data. The first uses the view frustum of the camera to track the amount of time points are on screen. The second uses the velocity of the camera as an indicator of saliency and uses a Hidden Markov model to learn the classification of salient and non-salient points. To provide a comparison to existing techniques several traditional visual saliency approaches are applied to orthographic views of the models’ photo-texturing. The results of all approaches are validated with human attention ground truth gathered using a remote gaze-tracking system that recorded the locations of the person’s attention while exploring the models.
               
            

@&#INTRODUCTION@&#

We have developed a smartphone/tablet app for the viewing and manipulation of 3D models gathered with an Autonomous Underwater Vehicle (AUV). This app is freely available and has been downloaded and used by a large number of users. The question this paper is attempting to answer is “Can we employ crowdsourcing to perform salient interest point detection from users not specifically tasked to find these points?” A diagram depicting the high-level system presented in this work is shown in Fig. 1
                     .

Saliency, particularly visual saliency is a popular construct from the field of biological vision and broadly describes an organisms ability to focus attention on a subset of its sensory input for further processing. In this work data subsetting is the most relevant part of the visual saliency process. While scientists and non-experts will have differing opinions on the high level top-down definitions of saliency, rapid bottom-up visual saliency is much less task and operator dependent [49]. This paper is focused on such processing in the context of a long-term environment-monitoring program using AUVs. At the Australian Centre for Field Robotics there is an ongoing program to perform benthic monitoring with an AUV [73]. This program deploys an AUV in unstructured natural environments where it gathers data for human review. One of the major bottlenecks in this process is the vast amount of data gathered by the AUV. The AUV is capable of gathering orders of magnitude more data than previous techniques. Traditionally divers used hand-held cameras to gather visual data in underwater environments and issues of decompression, airtime, and safety severely limited the quantity of data that scientists could gather. With the AUV in its current configuration, monitoring images can be gathered at up to 4Hz. A typical field campaign lasting two weeks can result in hundreds of thousands of images requiring review.

The challenge of how to deal with this massive image archive is being explored on several fronts. A large effort has gone into unsupervised clustering [64], human hand labeling [46], and supervised classification [4]. This work presents an alternative for gathering large amounts of human review data quickly and inexpensively. The assertion we present in this paper is that human visual saliency can be modeled by proxy through the exploratory motions of a large number of users in a 3-D environment.

Capturing human curiosity and exploration for robotic platforms is non-trivial. The well established approach is to use visual saliency measures but it is not necessarily clear that they can predict what people find interesting in a 3D scene and how they will choose to interact with it. This paper presents two alternative measures of human interest both based on the motion of the viewpoint used by the operator and compares them to traditional saliency measures. Through the crowdsourcing of many remote smart phone/tablet users we gather data to perform the identification of visual saliency on 3-D photo mosaic maps. Human experiments with ground truth from eye tracking are used to validate our results.

Crowdsourcing has emerged as a successful model for solving tasks by leveraging the human intelligence of large groups of remote users in a distributed fashion. The term crowdsourcing was coined in 2006 and first appeared in scientific literature in 2008 as “an online, distributed problem-solving and production model” [6]. The crowdsourcing model has since been adapted to outsource difficult steps in many computational tasks [32]. Recently the computer vision community has begun using crowdsourcing to solve challenging vision problems.

In parallel, researchers have started harnessing the power of data-mining over massive user bases to answer many new questions. Search engines and social networking use the interaction from millions of users to refine and improve advertising and site usability [17]. Researchers have used this data to learn about the demographics of users, social trends, and behavioral patterns [41,65]. With the rise of smart phones and ‘app stores’ mobile platforms have quickly become a practical means of gathering massive amounts of user data. App analytics is attempting to turn the millions of smart phones in use into a distributed network of data sources.

Traditional crowdsourcing of vision tasks relies upon motivating users through community good will [59], financial incentives (Mechanical Turk) or competitive/entertainment incentives [1,2] by turning a task into a game. The intended motivation for users of our app was education and entertainment. The app was advertised in the education section of the Apple iTunes app store and in its description and screenshots offered the promise of exploration of images from the deep sea. We attempted to capitalize on public interest in science, especially exploratory science, to motivate downloads. A novel aspect of our approach is that the motivation of users was somewhat more decoupled from the task than in a traditional crowdsourcing model. To work with such user data we propose the use of a novel paradigm from big data analytics where the answers to questions can be inferred from the data of many users. The power of our data-mining approach to crowdsourcing is that data is collected from a much larger pool of users. A full discussion of the motivations and demographics of users on various crowdsourcing platforms is beyond the scope of this paper, however Kaufmann et al. [30] present a review of the studies on Mechanical Turk. While these studies reflect a diverse user pool they also show that the Mechanical Turk user base is a fraction the size of the potential smart phone app user pool [31,56]. Using the smart phone platform gives us access to a much more general audience. To further the general appeal of the app we do not ask users to explicitly identify things they find interesting. Rather, we attempt to infer interest from patterns of interaction and in doing so free the user from an artificially constrained task. Without asking users to answer a specific question, their motivations for participating can be much more varied. This potentially gives access to a much larger ‘crowd’.

We will be presenting two novel metrics to calculate saliency from human user interaction data. One employs the use of the camera’s frustum to histogram observed points, while the other leverages a Hidden Markov model (HMM) to classify interaction data spatially into a saliency map. These techniques are compared to several state-of-the-art visual saliency techniques and validated using human gaze tracking data. The paper is laid out as follows. Section 2 discusses prior work. Section 3 presents the developed app as a platform for crowdsourcing. In Section 4 the two interaction-based formulations for saliency are laid out. The human trials for validation are discussed in Section 5. Results are presented in Section 6 and finally Section 7 concludes and presents future work.

Tools such as LabelMe, ImageNet, BUBL and other systems which leverage Amazon’s Mechanical Turk have provided solutions to the problem of image-labeling using human computation [18,14,33]. Mechanical Turk has become a particularly popular platform for crowdsourcing for vision. It offers flexibility and there has been research into assessing, processing, and rectifying image labelings from large groups of human sources [63,71,55]. All the aforementioned systems deal with image annotation with various types of semantic information ranging from object identification, object classification, and object segmentation.

In the field of gaze tracking Rudoy et al. propose a relevant model of crowdsourcing gaze tracking. They project a pattern over video or image data. Then a ‘crowd’ of remote users enter the subsection of the pattern viewed providing a proxy for direct gaze tracking [58].

Research on the human perception system has shown that it is selective in its attention [69]. Human perception focuses on salient regions of an image and there has been a great deal of literature published proposing models for that process [13,25,67]. Such work produces a saliency map for an image whereby the relative saliency of each pixel is expressed. The research is coarsely divided into techniques that exploit bottom-up cues and techniques that attempt to learn top-down high-level saliency measures [37,16,10]. These measures have then been applied across a range of applications including retargeting, tracking, compression, and object recognition [36,38,7].

Interaction-based saliency is the process of extracting saliency metrics from the way a user interacts with an image, video, or 3D model. Existing research on interaction-based saliency is primarily focused on 2D images and videos, not 3D models, however many relevant analogues exist. For 2D visual data Zoomable User Interfaces (ZUIs) or interfaces that allow users to zoom and pan around a large image or video have gained popularity in recent years. Utilizing these new interfaces several techniques have been developed to gather metadata for a piece of media being viewed in a ZUI. Carlier et al. propose a system where users watch a video and can retarget (zoom and center) it using the keyboard. Users input is then aggregated to produce a global retargeting for the video [8,9]. Similarly, Cricri et al. propose the use of implicit user region-of-interest data by detecting overlapping regions and concurrent events in multiple synced videos and attitude heading reference data streams [12]. In addition to these videos stream retargeting projects, a 3D distributed camera based saliency model was proposed by Park et al. where camera positions and viewpoints are correlated to determine regions of interest in 3D using intersecting camera rays [52]. In another video based technique Lee et al. learn a regressor over several high-level saliency features to predict salient people and objects from egocentric videos streams [34].

For static images, one of the most relevant works to this study is that of Xu et al. who propose the use of ‘Touch Saliency’ by capturing the center and size of zooms to produce saliency maps which are compared to traditional image saliency measures using eye-tracking data [74]. In a non-touch based interface Baccot et al. proposed using a smart phone’s picture browsing functionality to capture and store zoom and pan data. This data was then relayed to a central server to produce user interest maps [3].

In the 3D realm [48] propose a system for online shopping interfaces where users’ interactions with a 3D model are recorded and aggregated to propose a recommended view of the model to future users.

Mobile platforms like smart phones and tablets have advanced significantly in the last 10years. The incorporation of faster processors and mobile GPUs have greatly increased the power and range of applications that can be run on such platforms. The ubiquity of maps and mapping applications for smart phones has familiarized a broad base of users with the navigation and interpretation of 2-D maps. Recently 3-D maps have been introduced on the major smartphone platforms. This phenomenon is increasing users’ familiarity with navigating in 3-D textured environments.

For this work we have created an app that allows users to explore and navigate a 3-D photo-textured model of the seafloor. A screenshot of it running can be seen in Fig. 2
                     . Written in Objective-C and using OpenGL ES (OpenGL for Embedded Systems) it runs on both phones and tablets. The app is downloadable for free.
                        1
                        
                           http://www-personal.acfr.usyd.edu.au/mattjr/app.html.
                     
                     
                        1
                      Named SeafloorExplore the app was released into the Apple iTunes app store in 2012. The app itself uses virtual texturing [44,47] and a static Level-of-Detail (LOD) hierarchy [11] to be able to display models of up to 1000km2 with textures up to 128k2 pixels. It is only more recent phone models that have the GPU capability to be able to display 3-D models of such size and scale. It is these advances that are driving the slow but gradual adaptation of 3-D maps on modern phones, replacing their 2-D analogs.

The app gathers and logs interaction data from users. This data is periodically relayed back to a central server. The data is aggregated and compressed to minimize network traffic.

The models used in the app are created from data gathered in-situ beyond diver depths with an AUV. Once the AUV has completed an image gathering mission the vehicle is retrieved and the data downloaded. The AUV is equipped with a suite of navigation sensors: a 3-axis roll/tilt sensor, an acoustic Doppler Velocity Log (DVL), and a magnetic compass. These sensors along with the visual imagery are used to estimate poses for the vehicle.

The pose estimation process is performed in two steps. Firstly the data from cameras, DVL (with the AUV) and tilt sensor are fed to a Simultaneous Localization And Mapping (SLAM) filter. A sparse information-form pose-based SLAM algorithm [40] estimates the pose of the cameras for each pair of stereo images. Once this SLAM filter has completed we perform a second refinement step where the poses are optimized to further reduce the reprojection error. This second step takes the form of a sparse bundle-adjustment globally optimizing the poses given stereo matched scale-invariant feature transform (SIFT) features [66,39].

Once an optimized set of poses exists we employ the 3D reconstruction technique proposed by Johnson-Roberson et al. [28] and later extended to full 3D by Johnson-Roberson et al. [29] producing a high-resolution textured structural model. The approach breaks down the problem into manageable steps that allow for out-of-core processing. It first estimates local structure and then combines these estimates to recover a composite georeferenced scene using the SLAM-based vehicle pose estimates. Lighting correction and band-limited blending are applied to produce a texture-mapped surface at multiple scales which can be visualized efficiently. Through the use of a state-of-the-art model parameterization the distortion of mapping the 3D model to a 2D texture is minimized while attempting to maintain the resolution of the original source imagery [29,35,61]. A sample 3D model can be seen rendered on the screen of the iPad in Fig. 2.

For multitouch interfaces gestures for 3D interaction are more variable across platforms and applications than for 2D interaction. Several options appear in the literature. Hancock et al. propose shallow depth manipulation with one to three fingers [19]. Martinet et al. propose selecting an object with one finger and indirectly manipulating it with another [42]. We have selected a terrain centric interaction model which emphasizes the separation of degrees of freedom [43]. This model affords the user three gestures for three types of camera movement: pan, tilt/rotate, and zoom. Each type of motion is represented as a discrete value of the variable 
                           
                              
                                 
                                    m
                                 
                                 
                                    t
                                 
                              
                           
                        . All operations occur around a point on the model which we will refer to as 
                           
                              
                                 
                                    P
                                 
                                 
                                    0
                                 
                              
                           
                        . The camera is pointing at 
                           
                              
                                 
                                    P
                                 
                                 
                                    0
                                 
                              
                           
                         and is drawn back along a ray a distance d. The ray’s angle with respect to the model both vertically and horizontally are expressed as λ and ϕ respectively. This model is depicted in Fig. 3
                        .

Pan is performed using a single finger. When the user places their finger on the screen a ray is projected into the scene and intersected with the model. If the user’s first touch point does not hit the model the ray is intersected with a plane spanning the x and y axes at the average z-depth of the model. This first intersection point forms the starting camera location for a model translation. As the user drags their finger we perform a camera transformation such that the original point remains under the user’s finger, but the camera’s center 
                           
                              
                                 
                                    P
                                 
                                 
                                    0
                                 
                              
                           
                         in Fig. 3 is changed. This type of motion has an intuitive feel for an object like a terrain model.

Zoom is performed similarly to the two-dimensional case. A pinch gesture is used to change the distance d of the camera along the ray between the center of projection and the model in Fig. 3.

Tilt and rotate are both performed using a two-finger drag. We have chosen to separate each axis in x and y on the screen into tilt and rotate respectively. That is moving vertically on the screen modifies λ and moving horizontally modifies ϕ as shown in Fig. 3. The limits of tilt are stopped at 90 degrees to prevent going through the model.

One advantage of this camera model is that it is a compact representation of the camera position. The parameters to reconstruct the camera’s movement:
                           
                              
                                 {
                                 
                                    
                                       m
                                    
                                    
                                       t
                                    
                                 
                                 ,
                                 
                                    
                                       P
                                    
                                    
                                       0
                                    
                                 
                                 ,
                                 d
                                 ,
                                 λ
                                 ,
                                 ϕ
                                 ,
                                 t
                                 }
                              
                           
                        are stored continuously along with the time t. This data consists of seven 32-bit floats and one char 
                           
                              
                                 
                                    m
                                 
                                 
                                    t
                                 
                              
                           
                         (note 
                           
                              
                                 
                                    P
                                 
                                 
                                    0
                                 
                              
                           
                         is a vector in 
                           
                              
                                 
                                    R
                                 
                                 
                                    3
                                 
                              
                           
                        ). It is important to keep a compact representation as this data is transmitted (often over low-bandwidth mobile internet) back to a central server. Once aggregated on the central server it can be downloaded and used as the source data for the proposed saliency detection algorithm.

The app SeafloorExplore has been available in the Apple iTunes app store since June 2012. Without any additional advertising beyond what occurs automatically in the app store it has gathered over 5000 unique users from all over the world. Each month between 500 and 1000 sessions occur each gathering interaction data. The vast majority of users are casual and only play with the app once or twice. This can be seen in the pie chart in Fig. 4
                         which breaks down the number of users that have n sessions logged. Despite the casual nature of most of the use 15,000 total sessions have been logged. Furthermore there have been 491,232 total camera movement events recorded and transmitted to the central server. On average 32.27 events are logged per session. This has, without direct request for labeling, generated a massive data set for the saliency analysis.

Building upon the work in interaction-based saliency discussed in Section 2.3 this section will propose two novel frameworks that attempt to capture the notion of saliency using 3D camera motions (as described in Section 3.2) crowdsourced from the developed app. While traditional visual image saliency experiments rely on static images, this paper presents two key differences to that work. First, 3-D photo-textured models are used which means that not only do intensity and color play a role in saliency, but depth and relief contribute to what is or is not interesting. Second, the maps employed in this work can be thought of as multiscale. The resolution and extent of the maps is larger than could be captured by the eye or the screen in any single view. With such maps there is a relationship between a low-resolution overview and a high-resolution zoom that the user explores by navigating the model. Very simply we make the assumption that the user will attempt to see a higher resolution version of something they are interested in. To this end the multitouch interface (described in Section 3.2) along with the virtual texturing renderer (see Section 3) allow for the exploration of the model at arbitrary resolutions.

One key assumption of this work is that users are looking at what they find interesting. In a static single image set up, when the users gaze is tracked, this assumption holds very well. We assert that while this assumption may not hold for any single user selected at random from a pool of remote users it does hold across the group. We assume on average most people are looking at what they find interesting. Usage patterns that violate this assumption do exist. A user may be:
                        
                           •
                           attempting to learn the interface and simply performing actions to understand the mapping between touch and motion, moving randomly not understanding or interested in what they are viewing,

have put the device away while the app is still active (which is handled through a rejection of data temporally close to a device lock event),

have a gaze pattern that is very uncorrelated with motion.

In Section 4.1 we will attempt to show these outlier usage patterns are not the dominant trend in the data and that with a sufficiently large amount of users such patterns do not affect the outcome of the analysis.

The first proposed technique which we will refer to as the frustum method uses the camera’s view of the scene to model human interest. This idea is based on the simple principle that users will move the camera to cover areas of the scene they are interested in looking at. The more time a point appears in view the more ‘salient’ it is said to be. To begin the algorithm we initialize every 3D vertex in the scene to have a time counter of zero. Each interaction the user completes triggers the recording of the current camera parameters. These parameters are used to calculate the screen visibility of the vertices of the model through an inverse camera projection. The time between camera moves is recorded. This allows for the increment of a vertex’s counter by the number of seconds it appeared onscreen (i.e. the longer the vertex is in the camera’s view the greater the increment of the counter). After the camera motions from all users have been processed we have the screen time for each vertex in the 3D model. To create a 2D map for comparison to the baseline techniques we spatially histogram the vertices of the model under an orthographic projection. In the map the bins of the spatial histogram contain the sum of all the vertex time counters in that bin. We experimented with several weighting schemes that increased a vertex’s weight in proportion to its distance from the center of the camera. These other methods had a negligible impact on performance and ultimately a uniform time-based weighting proved most effective. This trivial algorithm produced quite compelling results, however a more complex model of the camera’s motion is presented in the following section.

Here we present a second technique to capture the notion of saliency from interaction. We propose the use of a Hidden Markov model to capture the motion of the camera through time. Hidden Markov models (HMMs) are probabilistic state machines that have been used extensively for the classification of time series data. Most notably, HMMs have been used for speech recognition and bioinformatics to great success [54,15]. We apply them here to give us a formulation that can learn the relationship between motions which indicate interest: such as spending a long time rotating around a point, zooming in and out of a point, or tilting about a point. To capture these more complex chaining of motions the camera’s path can be thought of as a time series. Through this time series the user is either interested in a single point or moving around looking for something new to explore. The amount of time spent on a point and the motions that occur near it provide a strong indicator of the user’s intentions with respect to that point.

In an HMM the Markov property is assumed for the modeled system. The model contains a sequence of observable data and a set of unobserved states. In this formulation the unobserved or hidden states take on binary values: salient or non-salient. The input, observable data, is in this case the camera motions gathered from a user. A HMM is a probabilistic model and three sets of probabilities determine the classification of the input data. The first are the prior probabilities of the states. Here we assume a uniform prior over the states. The second are the observational probabilities. These probabilities describe the likelihood that an observation corresponds to a certain state. Third are the transitional probabilities. These probabilities describe the likelihood of a transition between two states.

For the classification of saliency a two state HMM is proposed. A diagram of the proposed two state HMM is shown in Fig. 5
                        . A two-state HMM consists of two random variables O and Y which are both sequences. The variable Y consists of states 
                           
                              {
                              
                                 
                                    y
                                 
                                 
                                    0
                                 
                              
                              ,
                              
                                 
                                    y
                                 
                                 
                                    1
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    y
                                 
                                 
                                    n
                                 
                              
                              }
                           
                        . Y changes sequentially with the first order Markov property. This means the probability of a state change in Y only depends on its current state.

Formally:
                           
                              
                                 P
                                 (
                                 Y
                                 (
                                 t
                                 +
                                 1
                                 )
                                 =
                                 
                                    
                                       y
                                    
                                    
                                       i
                                    
                                 
                                 |
                                 Y
                                 (
                                 0
                                 )
                                 …
                                 Y
                                 (
                                 t
                                 )
                                 )
                                 =
                                 P
                                 (
                                 Y
                                 (
                                 t
                                 +
                                 1
                                 )
                                 =
                                 
                                    
                                       y
                                    
                                    
                                       i
                                    
                                 
                                 |
                                 Y
                                 (
                                 t
                                 )
                                 )
                              
                           
                        
                     

These states can take two values s (salient) and n (non-salient).

Formally the hidden state Y is:
                           
                              
                                 Y
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   s
                                                
                                                
                                                   salient
                                                
                                             
                                             
                                                
                                                   n
                                                
                                                
                                                   non
                                                   -
                                                   salient
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

The observable data is O a set of continuous value variables. Here it is the camera motion feature data as described in Section 4.4.

Three important questions can be asked of an HMM. What is the probability of an observed sequence O? What is the most likely series of states Y that would generate the observed data? And finally, can we learn the parameters for the HMM given a set of data?

Here we are interested in finding the single best state sequence Y, for the given observation sequence O. Formally this can be written as
                           
                              
                                 
                                    
                                       argmax
                                    
                                    
                                       Y
                                    
                                 
                                 P
                                 (
                                 Y
                                 |
                                 O
                                 )
                              
                           
                        Naively we could try every possible combination of states Y that maximize the joint probability of the model. Practically this is computationally expensive. However, the structure of the model can be exploited to formulate a more efficient solution. The probability 
                           
                              P
                              (
                              Y
                              |
                              O
                              )
                           
                         can be expanded out using Bayes’ rule:
                           
                              
                                 P
                                 (
                                 Y
                                 |
                                 O
                                 )
                                 =
                                 
                                    
                                       P
                                       (
                                       O
                                       |
                                       Y
                                       )
                                       
                                       P
                                       (
                                       Y
                                       )
                                    
                                    
                                       P
                                       (
                                       O
                                       )
                                    
                                 
                              
                           
                        
                     

In the maximization of 
                           
                              P
                              (
                              Y
                              |
                              O
                              )
                           
                         the constant 
                           
                              P
                              (
                              O
                              )
                           
                         can be eliminated and the optimal Y (denoted 
                           
                              
                                 
                                    Y
                                 
                                 
                                    ^
                                 
                              
                           
                        ) can be computed using:
                           
                              
                                 
                                    
                                       Y
                                    
                                    
                                       ^
                                    
                                 
                                 =
                                 
                                    
                                       argmax
                                    
                                    
                                       Y
                                    
                                 
                                 
                                    
                                       
                                          
                                             P
                                             (
                                             O
                                             |
                                             Y
                                             )
                                          
                                          
                                             ︸
                                          
                                       
                                    
                                    
                                       observation
                                    
                                 
                                 ·
                                 
                                    
                                       
                                          
                                             P
                                             (
                                             Y
                                             )
                                          
                                          
                                             ︸
                                          
                                       
                                    
                                    
                                       transition
                                    
                                 
                              
                           
                        
                     

The transition probabilities 
                           
                              P
                              (
                              Y
                              )
                           
                         are parameterized by a 
                           
                              2
                              ×
                              2
                           
                         matrix, A:
                           
                              
                                 A
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         a
                                                      
                                                      
                                                         ss
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         a
                                                      
                                                      
                                                         sn
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         a
                                                      
                                                      
                                                         ns
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         a
                                                      
                                                      
                                                         nn
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        as the following notation generalizes to an n state HMM we will refer to the s-state as state 1 and the n-state as state 2 therefore 
                           
                              
                                 
                                    a
                                 
                                 
                                    ij
                                 
                              
                           
                         in A refers to the probability of transitioning between state i and j where these indices correspond to the rows and columns of A.

The observation probabilities 
                           
                              P
                              (
                              O
                              |
                              Y
                              )
                           
                         are defined by a vector B composed of continuous probability density functions (pdf), where
                           
                              
                                 B
                                 =
                                 {
                                 
                                    
                                       b
                                    
                                    
                                       j
                                    
                                 
                                 (
                                 
                                    
                                       o
                                    
                                    
                                       t
                                    
                                 
                                 )
                                 }
                              
                           
                        
                     

Every state j has an associated probability distribution 
                           
                              
                                 
                                    b
                                 
                                 
                                    j
                                 
                              
                              (
                              
                                 
                                    o
                                 
                                 
                                    t
                                 
                              
                              )
                           
                         which is the probability of generating observation 
                           
                              
                                 
                                    o
                                 
                                 
                                    t
                                 
                              
                           
                         at time t.

The representation for the pdf that is used is a mixture of 3D Gaussian distributions:
                           
                              
                                 
                                    
                                       b
                                    
                                    
                                       j
                                    
                                 
                                 (
                                 
                                    
                                       o
                                    
                                    
                                       t
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          l
                                          =
                                          1
                                       
                                       
                                          L
                                       
                                    
                                 
                                 
                                    
                                       c
                                    
                                    
                                       jl
                                    
                                 
                                 ×
                                 N
                                 (
                                 
                                    
                                       o
                                    
                                    
                                       t
                                    
                                 
                                 ,
                                 
                                    
                                       
                                          
                                             μ
                                          
                                          
                                             →
                                          
                                       
                                    
                                    
                                       jl
                                    
                                 
                                 ,
                                 
                                    
                                       Σ
                                    
                                    
                                       jl
                                    
                                 
                                 )
                              
                           
                        where L is the number of mixtures, 
                           
                              
                                 
                                    c
                                 
                                 
                                    jl
                                 
                              
                           
                         is the mixture coefficient for the l-th mixture in state 
                           
                              j
                              ,
                              
                                 
                                    
                                       
                                          μ
                                       
                                       
                                          →
                                       
                                    
                                 
                                 
                                    jl
                                 
                              
                           
                         is the mean vector for the l-th mixture component in state 
                           
                              j
                              ,
                              
                                 
                                    Σ
                                 
                                 
                                    jl
                                 
                              
                           
                         is the covariance matrix for the l-th mixture component in state j, and 
                           
                              N
                              (
                              
                                 
                                    o
                                 
                                 
                                    t
                                 
                              
                              ,
                              
                                 
                                    
                                       
                                          μ
                                       
                                       
                                          →
                                       
                                    
                                 
                                 
                                    jl
                                 
                              
                              ,
                              
                                 
                                    Σ
                                 
                                 
                                    jl
                                 
                              
                              )
                           
                         is a multi-dimensional Gaussian (Normal) distribution.

The classification of 
                           
                              
                                 
                                    Y
                                 
                                 
                                    ^
                                 
                              
                           
                         is performed in standard fashion using the Viterbi algorithm [68].

To parametrize the camera data a model that represents the camera’s velocity is used. The intuition is that the user will slow the camera down with respect to a point on the surface of the model that they are interested in. The camera model (described in Section 3.2) has a parameter 
                           
                              
                                 
                                    P
                                 
                                 
                                    0
                                 
                              
                           
                         which is the point which the center projective ray of the camera passes through. Differencing this parameter over time the velocity vector v of the camera at time t in 
                           
                              m
                              /
                              s
                           
                         can be calculated and this is the observation 
                           
                              
                                 
                                    o
                                 
                                 
                                    t
                                 
                              
                           
                         used in the HMM.

In the HMM formulation described above the parameters require training. The parameters to be trained are λ:
                           
                              
                                 λ
                                 =
                                 (
                                 A
                                 ,
                                 B
                                 )
                              
                           
                        
                     

One option for training is to use labeled data to find a set of parameters λ that maximize the correct classification. This option was explored as there is correlated eye tracking and motion data from the human validation experiments described in Section 5. This data can be treated as a labeling by using fixation points as positive examples of saliency. However, techniques that do not require labeled data are regularly used to train HMMs. This option was selected as performance differences were negligible. Additionally, such techniques remove the need to conduct eye tracking experiments to implement the algorithm. It is important to note that the trained HMM still requires a post-processing step to assign appropriate output labels. At the conclusion of training the mean velocities of the two output classes are compared and the high-velocity data is automatically assigned to the non-salient class label while the low-velocity data is assigned to the salient class.

To train without labeled data the Baum-Welch algorithm maximizes 
                           
                              P
                              (
                              Y
                              |
                              λ
                              )
                           
                         with respect to λ 
                        [70]. The algorithm performs a local search and is therefore sensitive to initial parameters. The result is a set of parameters optimized to explain the input data. A full discussion of this process can be found in Rabiner [54].

@&#EXPERIMENTS@&#

To validate that the proposed technique is an effective proxy for human interest we set up a traditional visual saliency eye tracking experiment. Eye tracking has long been used in psychology, human computer interaction, and vision research to experimentally measure human attention [51,60,53]. Most commonly we see the results of these experiments represented as heat maps that capture various metrics about eye fixations on the screen. There are several competing metrics for what to aggregate in heat maps for human attention. A full discussion can be found in Meur and Baccino [45]. For the presented heat maps we use total fixations in a spatial area as the aggregating mechanism as this was used commonly across much of the literature referenced in this paper.

The eye tracking experiment had the following form. A near-infrared remote eye tracking system that offered sub degree resolution on the user’s gaze location was placed in a fixed position. The gaze tracker logs continually and transmits the current gaze location over TCP/IP. The user sits at a desk with an iPad permanently mounted above the gaze tracking equipment. The system is calibrated to return gaze locations on the iPad’s screen. The iPad reports all of its interaction and model rendering data directly to the same logging computer over User Datagram Protocol (UDP). The user is given a test model to help them familiarize themselves with the interface. The user has two minutes to learn to navigate the interface using the test model (a randomly generated fractal terrain). After the test period the user is presented with three models and has three minutes to explore each in sequence. The time period of three minutes was selected as this is the average approximate time users interact with any model as computed from statistics gathered from the app. Twelve users participated producing a total of 313,161 total fixation points across all the models tested. In the results plots that follow only gaze points with a high frequency of occurrence (More than 50 gazes in a cell given a 110 
                           
                              ×
                           
                         110 grid) are shown to prevent plotting clutter.

We compare our saliency maps generated from interaction to the following published saliency algorithms: the original Itti-Koch saliency model (denoted Itti) [26] an important early technique that measures saliency based upon local spatial discontinuities of color, saturation and orientation, Graph-Based visual saliency (denoted GBVS) where the authors present a technique that calculates saliency using the same features as the Itti-Koch model but a Markov chain interpretation of the relationships between image features [20], Rudinac’s salient object detection algorithm (denoted Rudinac) that calculates the spectral residuals of three color channels and then calculates interest points in that space [57], and Hou’s spectral saliency algorithm (denoted Hou) which also uses the spectral residuals to calculate the statistically redundant regions of an image [22]. These methods provide another means of generating a saliency map using an orthographic image of the 3D model’s visual texture as their input. Such methods are commonplace for calculating visual saliency and we feel they provide an important comparison to this new technique.

@&#RESULTS@&#

The results are generated across data gathered on missions from three separate field deployments.

The first dataset (denoted Geebanks in figures) was gathered at a site on the Western Australian coastline, the Geebank area of the Abrolhos Islands. This location has been identified as a key indicator region because of its ecological sensitivity and socio-economic importance [62]. The Abrolhos Islands, 80km off mainland Australia form a remote archipelago with only seasonal human habitation. At the Abrolhos, patches of hard corals and kelp beds co-exist to form mixed habitats. This data set presents a challenging array of colors and textures and is rich from a visual saliency standpoint. It is also challenging as arguably it is all ‘interesting’, however one very unique structure appears in inset of Fig. 6
                        (a) which both visually and structurally is unique in the scene.

The results of all the visual saliency techniques discussed in Section 5.2 are shown in Fig. 6(b)–(e). It should be noted that the Itti-Koch model of visual saliency (shown in Fig. 6(d)) does well at identifying the uniqueness of the hard coral among the rich background of other cover. When compared to the results of the eye-tracking experiment (described in Section 5.1) where human gaze is used to create a heat map, we can see that it predicts human attention well in this case. The heat maps from the first interaction-based technique, the Frustrum based technique (described in Section 4.1) appears in Fig. 6(f). This technique produces a noisy heat map with some peaks in correlation with the gaze tracking experiment but many without. This highlights the difficulty of using such a model on a very rich data set. When there are many interesting things in the scene, without a more complex temporal model of the camera’s motion, relative saliency becomes unclear. The HMM model (described in Section 4.2) is much more restrictive in saliency identification. It fires only on points that saw a great deal of motion in the form of zooms and tilts/rotations. That model fires heavily on the structurally unique hard coral and other objects in the surrounding area. As the coral stood off the ground plane and users rotated and tilted the camera to view it in its entirety we see high peaks in this region. The results of the generated heat map can be seen in Fig. 6(g).

The second dataset (denoted Ningaloo in figures) was gathered at the Ningaloo Marine Park located in the Ningaloo Reef on Australia’s northwest coast. It is unique as one of only two major reef systems to form on the westside of a continent, additionally it is one of the longest fringing reefs in the world. It lies in a transitional zone between tropical and temperate marine life. The data was gathered at a depth of around 80m where a rich variety of large sponges, gorgonians and sea whips live. Understanding these organisms plays an important role in assessing the health of the reef and the effectiveness of the Marine Park protected area [21]. This dataset is the most well suited to traditional visual saliency as it consists primarily of grey sand with rich bursts of color from living organisms. An example of a richly colored red Gorgonian appears in the inset in Fig. 7
                        (a).

The results of all the visual saliency comparison techniques discussed in Section 5.2 are shown in Fig. 7(b)–(e). In contrast to the results shown in Section 6.1 the Rudinac visual saliency technique works well here in predicting human gaze. We theorize as this method is designed as an object detector, this dataset which consists of ‘object like things’ on a relatively plain background is well suited to that paradigm. The Frustum based technique shown in Fig. 7(f) captures most of the salient regions with two caveats. First it lacks the localization of the visual saliency methods or the HMM algorithm. The impact of this issue does not affect the qualitative results presented later in Section 6.4. Additionally in situations where saliency is used as a guide for human review, localization is of less importance. In contrast in situations where saliency will be used for segmentation or machine processing such behavior is undesirable. Secondly the Frustum approach creates a much smaller numerical distance between salient and non-salient than either the visual methods or the HMM approach. This again had little impact on the qualitative results in Section 6.4 but may cause greater problems in larger datasets with more multi-modal distributions of saliency. The HMM approach results are displayed in Fig. 7(g). Here the technique is overly conservative with respect to the ground truth saliency map (shown in Fig. 7(h)). Fewer locations are determined to be salient but the centers of the salient clusters in both maps show good alignment.

Finally the third dataset was gathered in Tasmania, Australia off the coast of St. Helens. Here sea urchins have invaded the local habitat and formed barrens disrupting the indigenous ecosystem. Economically important macroalgal beds are being overgrazed by this invasive species [27]. The AUV was deployed to study and monitor the behavior of this species and help understand how to mitigate the harm being caused [72]. Structurally this data is quite rich. A large amount of relief exist in the boulder fields and as such camera navigation is much more complex than the previous two data sets. Note that in the imagery urchins and sparse macroalgal colonies appear, one of which can be seen in the inset in Fig. 8
                        (a).

Again the results for the visual saliency methods are shown in Fig. 8(b)–(e). Here there is a noticeable breakdown in the results of all the techniques due to the visual uniqueness of the sandy thin region at the base of dense grid section of the model. The algorithms all track on this region as most visually salient. This highlights one of the weaknesses of traditional visual saliency on a highly 3D scene. Vision-only algorithms are at a disadvantage as they know nothing about the 3D structure of the scene. A higher level process employed by viewers determined that sand is flat and uninteresting and they explored the highly structured boulder field instead. This is apparent from the much greater consistency between the interaction techniques shown in Fig. 8(f) and (g) and the ground truth shown in Fig. 8(h).

In addition to the visual results presented above, we present a quantitative comparison with the same bottom-up visual saliency measures again using human gaze tracking data as ground truth. Results are shown using the shuffled Area Under Curve (AUC). The shuffled AUC is a slight modification of the traditional AUC for Receiver Operating Characteristics (ROC) curves. The problem of visual saliency can be thought of as a binary classification task where the decision boundary is between salient and non-salient. In this formulation human fixations are considered the positive set while random points from the image are sampled to form the negative set. Perfect classification is a AUC of 1.0 while random chance is 0.5. Because our image mosaics are non-rectangular and only pixel data from the actual mosaic and not the black background is used. This prevents the border areas from biasing the results. As discussed by Zhang et al. border effects and center bias can significantly affect results [75]. The shuffled AUC selects the negative set from the union of all fixation points across the data set with the exception of the positive set. The shuffled AUC is gaining popularity as a metric to assess saliency measures [75,23,5].

As noted by Hou et al. one parameter that can have a significant effect on model accuracy is the smoothing or blurring of the saliency map [24]. Traditionally this smoothness is achieved by convolving the map with a Gaussian kernel. To assess each technique’s sensitivity to this parameter the σ or width of the Gaussian kernel is varied (from 
                           
                              5
                              ×
                              
                                 
                                    10
                                 
                                 
                                    -
                                    4
                                 
                              
                           
                         to 
                           
                              2
                              ×
                              
                                 
                                    10
                                 
                                 
                                    -
                                    2
                                 
                              
                           
                         times the image size in steps of 
                           
                              5
                              ×
                              
                                 
                                    10
                                 
                                 
                                    -
                                    3
                                 
                              
                           
                        ) [5]. All of the visual results in the previous section are displayed at the lowest σ used in the experiments 
                           
                              5
                              ×
                              
                                 
                                    10
                                 
                                 
                                    -
                                    4
                                 
                              
                              ×
                              
                                 
                                    I
                                 
                                 
                                    size
                                 
                              
                           
                         maximizing the visual interpretability of the results over best the numerical performance which occurred at higher sigmas. The result of this comparison run across the three sets of field data can be seen in Fig. 9. In the Geebanks results (Fig. 9
                        (a)) most of the visual saliency techniques perform fairly similarly when compared using this metric. However, the proposed HMM technique consistently exceeds all other techniques for low σ and has the highest peak shuffled ROC at 0.772. While less clear in the qualitative results (shown in Fig. 6) this graph shows that the proposed HMM, tested under a varying decision boundary and randomized sample locations, performs quite well. The frustum based technique sees a performance bump from increasing σ suggesting again that its localization of salient features is quite poor as blurring improves the results.

The result of the Ningaloo data set appears in Fig. 9(b) here we see the strength of the frustum based algorithm on a flat thin dataset. The frustum technique is not hampered by ‘incidental contact’ in this dataset. When looking obliquely at the model very few points in front or behind the object of interest are in view. This leads to an up-weighting of saliency for primarily the object not the background. Performance for the frustum technique starts and stays high (peaking at an AUC of 0.767) across all tested σ values. Again here we see the strength of the proposed HMM technique with the highest peak performance (almost 0.8 AUC). Additionally this score is for well localized maps with smaller σ. Several of the visual techniques perform well, notably the rudinac object detector and the Itti-Koch and Hou models for large σ.

Finally the St. Helens dataset (Fig. 9(c)) shows the HMM technique again leading the pack for small σ and again achieving the highest peak performance (0.769 AUC). However, its performance becomes comparable to the visual saliency techniques for larger σ. The mix of high relief and flat and thin structure is challenging and lead to the poor performance for the Itti-Koch model.

A single run 
                           
                              σ
                              =
                              (
                              0.006
                              ×
                              
                                 
                                    I
                                 
                                 
                                    size
                                 
                              
                              )
                           
                         is shown in Fig. 11 here the error bars express the standard deviation across the multiple random point shuffles runs. Note the similar performance between the two proposed techniques and the traditional visual saliency approaches across all three datasets. Note the superior performance of the proposed HMM technique across all datasets.

Finally, another important question that needs to be answered is: How much crowd data is required to achieve good performance? Fig. 10
                        
                         displays a graph of shuffled AUC performance, both mean and standard deviation across the multiple random point shuffles, as a function of the number of crowd interactions used. As the graph shows performance steadily increases as more data is added. Over 200,000 interactions for the Geebanks map were used and performance of the proposed technique was still consistently improving. However, at this time only 200,000 interactions are currently available for that map so it is still unclear when performance will asymptote as data is added further. This is an area of future work as more data is collected. Another conclusion from this data is that at least 10,000 interactions seem to be required to begin to model saliency in a reasonable fashion. This result also raises another important point about algorithm runtime. The computational cost of running analysis over thousands of users is certainly greater than the cost of computing static visual saliency for small regions. The proposed approaches took on an order of hours to run on a standard desktop machine and could be run in the cloud in a distributed fashion producing results in minutes. This runtime is roughly an order of magnitude greater than the visual saliency techniques. In the results presented here we note relatively comparable performance between the visual saliency techniques and the crowdsourced saliency. However, we feel the idea of crowdsourced saliency offers two unique advantages that warrant the additional computational effort. The first is the ability to process 3D saliency without the use of visual information. This has application to gathering saliency from non-visual 3D structures like untextured meshes and laser scanned 3D models. The second is the value of leveraging a larger set of user inputs allowing for the possibility of higher performance and the segmentation of data along user demographics. Expanded to the scale of enterprise commercial projects like smartphone map navigation the user input data and available server processing power can grow by several orders of magnitude. We feel the paradigm presented here offers a great deal of potential for exploration and growth on such data and provides a viable alternative to visual saliency for a broader range of applications.

@&#CONCLUSIONS AND FUTURE WORK@&#

In this paper we have presented a novel technique for extracting saliency from crowdsourced interaction data. We have developed two algorithms to extract interest metrics from camera motions. To our knowledge this system is the first of its kind in using a distributed smart phone app to gather saliency data for 3D maps. The proposed system provides an alternative to traditional visual saliency by harnessing the modality of touch to provide a proxy for interest. Our results show that comparable performance to visual saliency is achievable solely through interaction given a large enough user base. Through a gaze tracking experiment we have validated the techniques showing the proposed metrics are consistent with human attention.

The level of agreement between the human experiment and the automated techniques is good, however looking at the human subject data suggests the variability of ‘interest’ as a definition for any saliency metric. The process that governs what we as humans find interesting in a large 3D map is more complex than the traditional model of visual attention for a static image. Additionally because of the amount of time spent exploring the model this process is likely to evolve. People will become more or less interested in certain types of organisms or terrain structures over time. This means the path of a human through the model, especially a large model is rarely going to be similar for small trial sizes. This points to the challenge of applying any saliency technique on a large, visually and structurally rich model. Casual users will not explore a model with the rigor or depth of a scientist reviewing images for scoring. As such the proposed technique cannot replace the ongoing efforts to automate the segmentation and classification of such science data. However the technique offers a promising approach for discovery and data-mining in a vast archive of images (currently we hold an archive of over a million AUV images). Ultimately the ability to understand what users find interesting in 3D models is appealing for applications beyond the one presented here. Mapping companies, advertisers, and geo-statisticians are all interested in what people are looking at on a map. As 3D maps overtake their 2D counterpoints on smart phones, interest metadata could help businesses know what streets are explored commonly, or what terrain features people are drawn to in a region.

As we move forward a greater leveraging of the existing work in how to intelligently rectify the labels of multiple operators when crowdsourcing would strengthen the approach presented [50]. The use of more complex inter-operator relationships in terms of cross checking and aggregation of interest could further refine the saliency maps produced. Unsupervised clustering is another important avenue of exploration. Variational Dirichlet Processes (VDPs) have been applied to the clustering of 2D image data [64] and an adaptation of those techniques to saliency in 3D is a future direction of inquiry. An intelligent combination of the two presented techniques could help strengthen the overall results. A more complex HMM model could capture more states such as exploratory motion vs. directed search in addition to the salient/non-salient distinction. Also exploring the use of the depth information as a saliency channel could improve the performance of the visual-only techniques.

@&#ACKNOWLEDGMENTS@&#

This work is supported by the New South Wales State Government and the Integrated Marine Observing System (IMOS) through the DIISR National Collaborative Research Infrastructure Scheme. The authors of this work would like to thank the Australian Institute for Marine Science and the Tasmanian Aquaculture and Fisheries Institute (TAFI) for making ship time available to support this study. The crews of the R/V Solander and R/V Challenger were instrumental in facilitating successful deployment and recovery of the AUV. We also acknowledge the help of all those who have contributed to the development and operation of the AUV, Ian Mahon, Stephen Barkby, Ritesh Lal, Paul Rigby, Jeremy Randle, Bruce Crundwell and the late Alan Trinder, Duncan Mercer and George Powell. We also thank Bryce Barlow for his help with running experiments.

@&#REFERENCES@&#

