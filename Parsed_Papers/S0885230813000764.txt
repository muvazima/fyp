@&#MAIN-TITLE@&#Level of interest sensing in spoken dialog using decision-level fusion of acoustic and lexical evidence

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           This study investigates accurate recognition of a user's interest.


                        
                        
                           
                           We use two different classifiers for acoustic level prediction to reduce over-fitting problem.


                        
                        
                           
                           We design robust lexical features from the erroneous ASR output.


                        
                        
                           
                           We use a decision level fusion approach using acoustic and lexical model.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Level of interest

Decision-level fusion

Human–machine interaction

@&#ABSTRACT@&#


               
               
                  Automatic detection of a user's interest in spoken dialog plays an important role in many applications, such as tutoring systems and customer service systems. In this study, we propose a decision-level fusion approach using acoustic and lexical information to accurately sense a user's interest at the utterance level. Our system consists of three parts: acoustic/prosodic model, lexical model, and a model that combines their decisions for the final output. We use two different regression algorithms to complement each other for the acoustic model. For lexical information, in addition to the bag-of-words model, we propose new features including a level-of-interest value for each word, length information using the number of words, estimated speaking rate, silence in the utterance, and similarity with other utterances. We also investigate the effectiveness of using more automatic speech recognition (ASR) hypotheses (n-best lists) to extract lexical features. The outputs from the acoustic and lexical models are combined at the decision level. Our experiments show that combining acoustic evidence with lexical information improves level-of-interest detection performance, even when lexical features are extracted from ASR output with high word error rate.
               
            

@&#INTRODUCTION@&#

Automatic detection of a user's interest in spoken dialogs can benefit many applications, making the systems more adaptive and responsive to a user's behavior. For example, in a tutoring system, if a student is bored or shows no interest in a topic, the system may change the topic or modify the way of its presentation. Similarly for a custom service application, systems with abilities to detect user's level of interest are more likely to provide better service. There have been some previous studies on level of interest detection from speech. Schuller et al. (2009a) adopted a feature-level fusion of various information, including visual, acoustic, linguistic, and temporal evidences. They found that the system combining all evidences achieved better performance than only using single information. In the Paralinguistic Challenge level-of-interest detection subtask at Interspeech 2010 (Schuller et al., 2010), Gajsek et al. (2010) focused on acoustic information to design more robust features, and Jeon et al. (2010) developed a decision-level fusion method using acoustic, lexical, and contextual information. Wang and Hirschberg (2011) used a hierarchical fusion learning method to use the feedback from previous predictions.

Research into level-of-interest detection has inherited many of its techniques from the much more extensively investigated field of emotion recognition. Many studies focused on using single information, such as acoustic evidence from speech (Hoque et al., 2006; Batliner et al., 2008; Schuller et al., 2009b; Lee et al., 2011; Jeon et al., 2011), facial expression (Essa, 1997; Ioannou et al., 2005; Metallinou et al., 2010a), and linguistic features (Osherenko and André, 2007). The interacting pattern between partners in a dialog was also shown as a useful cue for emotion recognition (Lee et al., 2009). There also exist studies to combine various information for emotion recognition, for example, fusion of facial and acoustic cues (Busso et al., 2004; Zeng et al., 2007), facial expression, prosody, and head movement (Metallinou et al., 2010b), or body language and prosody (Metallinou et al., 2011). As shown in many studies (e.g., Busso et al., 2004; Zeng et al., 2007; Metallinou et al., 2011), multi-modal processing achieved better performance than relying on any single modality. In particular, combining acoustic and linguistic evidence achieved better performance of emotion recognition than using speech cues only (Lee and Narayanan, 2005; Schuller et al., 2005). The increasing interest in this field has also been manifested by the several challenge tasks recently, such as at Interspeech 2010, 2011, and 2012, and ICMI 2012.

This paper is an extension of our previous work (Jeon et al., 2010). We use a decision-level fusion approach to combine acoustic and lexical information. As shown in our previous study (Jeon et al., 2010), the lexical model was greatly affected by speech recognition errors, therefore the combined system has performance degradation on the ASR condition. Based on our previous decision-level fusion system, we propose more robust lexical features to address the performance loss due to ASR errors. We use the bag-of-words model as baseline features, and introduce new features including level-of-interest value for each word, different ways to represent length information, and similarity values. We also utilize multiple ASR candidates (n-best lists) to extract lexical features. In addition, we propose to use acoustic level combination of two different regressors to address a possible over-fitting issue. Our experimental results show that although there is performance loss for ASR conditions, our proposed lexical features can still provide valuable information, and the decision-level fusion system achieves significantly better performance (McNemar test, p
                     <0.05) than our previous approach (Jeon et al., 2010).

The remainder of this paper is organized as follows. In the next section, we describe the level-of-interest detection task and the data. We provide details of our approach in Section 3, including the acoustic and the lexical models, and their combination. Section 4 presents our experimental results and discussion. The last section gives a brief summary along with future directions.

The data used in this study is from the Interspeech 2010 Paralinguistic Challenge, which is part of the Audiovisual Interest Corpus created at the Technische Universität München (TUM AVIC) by Schuller et al. (2007). In the following we briefly describe how the data was collected and annotated. More information can be found in (Schuller et al., 2007). In the scenario setup of TUM AVIC data, an experimenter and a subject sit on the opposite sides of a desk. The experimenter plays the role of a product presenter and leads the subject through a commercial presentation. The subject's role is to listen to explanations and topic presentations of the experimenter, ask several questions of her/his interest, and actively interact with the experimenter considering his/her interest in the addressed topics. The subject was explicitly asked not to worry about being polite to the experimenter, for example, by always showing a certain level of ‘polite’ attention. Visual and voice data is recorded by a camera and two microphones (one headset and one far-field microphone in this situation). 21 subjects participated in the recordings, three of them Asian, the remaining European. The language used throughout experiments is English, and all subjects are very experienced English speakers. The database comprises 12,839 utterances.
                        1
                     
                     
                        1
                        Utterances as used in this work correspond to a speaker and sub-speaker-turns in the TUM AVIC data.
                      The level of interest (LOI) of the utterances was labeled by four male annotators, independently from each other. Five levels were used in the first place: Disinterest (−2), Indifference (−1), Neutrality (0), Interest (+1), Curiosity (+2). Ground truth is then established by averaging the annotators’ labels and then divided by 2. The final LOI values are thus in the range of [−1.1].

In the Interspeech 2010 Paralinguistic Challenge, the data comprised of three data sets: training, development, and blind test sets. Table 1
                      shows the information for these data sets, including the number of utterances, the average duration of utterances with standard deviation, the average correlation between the ground truth and 4 annotators with standard deviation,
                        2
                     
                     
                        2
                        We cannot estimate the average correlation for the test data since only ground truth data is released.
                      and speaker information. We use the training and development data to evaluate the regressors and tune the parameters, and the test data is used only for the final testing.

Note that the shortest utterance is about 0.1 second, while the longest one is longer than 11 seconds. In Fig. 1
                      we plot the histograms for the LOI values of the training and development data. As can be seen, the samples in the training and development data have a skewed distribution, not (roughly) uniformly distributed in the entire region of [−1, 1]. For example, most instances have a label of between 0.4 and 0.8. We will evaluate if this kind of distribution has an impact on the learning methods.

For the training and development data, the spoken content has been transcribed, and pause and non-linguistic vocalizations have been labeled. These vocalizations include breathing, consent, hesitation, laughter, coughing, and other human noise. In order to evaluate the performance using ASR output, which is the real testing scenario in human–machine interaction, we generated ASR output for the three data sets using a state-of-the-art SRI recognizer that was developed for broadcast news speech (Stolcke et al., 2006; Zheng et al., 2007). We did not perform any adaptation of acoustic and language models. Because the speakers are non-native English speakers in this data, and the domain is very different from the training data for the recognizer, the word error rate is quite high for this data: over 60% when excluding non-linguistic vocalization tokens, and over 70% when counting all the tokens. When n-best hypotheses from ASR output are used, the oracle word error rate decreases as expected. For example, we find that the oracle word error rate is reduced by more than 10% absolute when using 10-best hypotheses.

Since a listener provides feedback in a dialog, we expect that the listener's interest level is dependent on not only how the person says something (represented by acoustic features), but also what the person said (represented by lexical features). Therefore we use both information sources for this task. We adopt a decision-level combination strategy, which achieved better performance than a feature-level combination in our previous work (Jeon et al., 2010). As shown in Fig. 2
                     , our system consists of the acoustic model, lexical model, and a final decision model that combines their outputs. The acoustic and lexical models were independently optimized to achieve the best individual performance using different learning algorithms. Below we explain the details of each component.

The acoustic features are extracted using openSMILE (Eyben et al., 2009). This feature set is generated in three steps. First, 38 low-level descriptors (LLDs) as shown in Table 2
                         are extracted and smoothed by a simple moving average filter of three window length, which removes the high frequency components present in the signal. The 60ms Gaussian windows and 10ms step size were used for F0 related low-level descriptors, and the 25ms Hamming windows and 10ms step size were used for the others. Next, their first-order regression coefficients are added, and then 21 functionals (cf. Table 3
                        ) are applied. We discarded 16 features which contain no information (e.g., in the presence of unvoiced frames, openSMILE erroneously identifies minimum F0 as zero). Finally, two single features: F0 values for the onset and turn duration, are added, resulting in 1582 features in total. Details of the acoustic features are provided in (Eyben et al., 2009).

The training and development data are used to evaluate acoustic regressors. For the baseline regressor, we used the Random Subspace (RS) algorithm (Ho, 1998), which uses multiple trees on randomly chosen subsets of the features and then combines the output from multiple trees. RS was used as the baseline in the Interspeech 2010 Paralinguistic Challenge. We also explored various algorithms for the acoustic model, but did not find any regressor outperforming that baseline. However, we notice that RS has very little error on the training data, which may suggest overfitting, therefore we used the Gaussian Process (GP) algorithm (Seeger, 2004) as a complementary model, which achieved the second best performance and has less performance difference between the training and development sets in our preliminary experiments. Ensemble of multiple models has been known to often outperform individual models. In this study, we use a weighted linear interpolation of the output from the two regression models. We divided the development set into two parts and used them to estimate the combination weights for each other.

Given the amount of the training data, we suspect that the feature size might be too large. Therefore we tried to reduce the feature size using feature selection, which we expect may alleviate the potential over-fitting problem and improve the performance. We found when using Correlation-based Feature Selection (CFS) (Hall, 2000), the reduced feature set yielded better performance for the GP model, but not for RS (possibly because RS already uses subsets of features in each tree). Therefore for the acoustic model, we use the combination of GP with full feature space and RS with the reduced feature space. More details about parameters of each algorithm and analysis are described in Section 4.1.1.

The interest level cannot just be determined by how the person says something (represented by acoustic features), but it is also dependent on what the person says. Schuller and Rigoll (2009) showed the spoken content also carries cues with respect to a subject's interest level. In this study, we choose to use simple lexical features for level-of-interest detection, rather than more complex features based on syntactic and semantic analysis. This is because some responses include only a few words or non-linguistic vocalizations. In this case syntactic and semantic features are less useful. Furthermore, simple lexical features are less impacted by erroneous ASR output.

We first processed the training transcripts (they can be either from human transcripts or ASR output) to create the dictionary needed for the lexical features. Pauses in the middle of an utterance and non-linguistic vocalization tokens are considered to be a type of word in the dictionary. We kept the words that appear more than once, and the others are collapsed into one word (called INFREQUENT). This token will be used for unseen words in testing. The following describes the features we use for each utterance.
                           
                              •
                              Bag-of-words model (BOW): We use a bag-of-words model as the base lexical features, which has been widely used for various text classification tasks (Joachims, 1998). Each utterance is represented as a vector of word features. We build the BOW vector based on the dictionary described above. The size of the BOW vector varies depending on the data used to generate the dictionary, and will be described in more details in Section 4.1.2. The weight of each word 
                                    
                                       w
                                       i
                                    
                                  is:
                                    
                                       (1)
                                       
                                          log
                                          
                                          TF
                                          (
                                          
                                             w
                                             i
                                          
                                          )
                                          =
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            1
                                                            +
                                                            log
                                                            (
                                                            count
                                                            (
                                                            
                                                               w
                                                               i
                                                            
                                                            )
                                                            )
                                                         
                                                         
                                                            if
                                                            
                                                            count
                                                            (
                                                            
                                                               w
                                                               i
                                                            
                                                            )
                                                            >
                                                            0
                                                         
                                                      
                                                      
                                                         
                                                            0
                                                         
                                                         
                                                            otherwise
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 Various term weighting methods have been studied in previous work in language processing (for example, for tasks such as text categorization and information retrieval). We performed a preliminary evaluation for our task and found using logTF achieved reasonable performance. More thorough studies for term weighting are still needed.

Word level-of-interest (W-LOI) (3 features): The second set of features is regarding the LOI values of the words in the utterance. First we obtain statistics of LOI values for each word in the dictionary from the training set, including the average, minimum, and maximum LOI value. These are the averaged/min/max LOI values of all the utterances that contain this word. Based on these statistics, we then derive three W-LOI features for the utterance: average, minimum, and maximum LOI values. They are the average/minimum/maximum LOI value of all the words in an utterance. These features are in the range of [−1, +1].

Length information (LEN) (3 features): These features are used to represent the sentence length and speaking rate. We hypothesize that there might be a correlation between the length of a person's utterances and whether a person is bored or interested in something. There are three features in this category: log value of the total number of words in an utterance, the estimated speaking rate, and silence length.
                                    3
                                 
                                 
                                    3
                                    Note that the speaking rate and the silence length are not purely based on lexical information, but require acoustic information (duration). We include them in the lexical features since they are different from other acoustic/prosodic features whose extraction does not need word information.
                                  We decide to apply logarithm to the total number of words such that utterances with a large number of words have a smaller impact. The estimated speaking rate for each word 
                                    
                                       w
                                       i
                                    
                                  is 
                                    (
                                    
                                       μ
                                       
                                          
                                             w
                                             i
                                          
                                       
                                    
                                    −
                                    duration
                                    (
                                    
                                       w
                                       i
                                    
                                    )
                                    )
                                    /
                                    3
                                    
                                       σ
                                       
                                          
                                             w
                                             i
                                          
                                       
                                    
                                 , where 
                                    
                                       μ
                                       
                                          
                                             w
                                             i
                                          
                                       
                                    
                                  and 
                                    
                                       σ
                                       
                                          
                                             w
                                             i
                                          
                                       
                                    
                                  refer to the mean and standard deviation of the duration of word 
                                    
                                       w
                                       i
                                    
                                 . This formula maps most of the speaking rate values into [−1, 1]. Then the speaking rate of an utterance is the sum of the speaking rate for all the words in the utterance. The word duration information is obtained from the force-aligned data for human transcription or ASR output. The silence length refers to the sum of all the silence/pause in the utterance (excluding the start or the end part of an utterance).

Similar utterances (SIM) (20 features): These features leverage information from similar instances (this is similar to nearest-neighbor instance-based learning approach). They consider the LOI values of those utterances that are similar to the testing utterance in content. For an utterance, we first find 20 most similar utterances from the training data using the cosine similarity measure of the two utterances, each represented using the bag-of-words vector.
                                    4
                                 
                                 
                                    4
                                    
                                       
                                          cosine
                                          (
                                          
                                             θ
                                             AB
                                          
                                          )
                                          =
                                          
                                             
                                                
                                                   BOW
                                                   A
                                                
                                                ·
                                                
                                                   BOW
                                                   B
                                                
                                             
                                             
                                                |
                                                |
                                                
                                                   BOW
                                                   A
                                                
                                                |
                                                |
                                                ·
                                                |
                                                |
                                                
                                                   BOW
                                                   B
                                                
                                                |
                                                |
                                             
                                          
                                       .
                                  Then we use the LOI values from these 20 utterances as features.

We evaluate these lexical features on the human transcription as well as the output from the ASR system described in Section 2. Even though we use a state-of-the-art ASR system, the word error rate is quite high. Incorrect words will naturally have an impact on the lexical features, affecting the BOW vectors (missing or inserting wrong words) and a word's term weight and LOI information. In addition to just using the top ASR hypothesis, we also investigate if we can leverage multiple candidates, such as n-best lists, to reduce the performance loss caused by recognition errors. We use 10-best list in this study. To extract lexical features from 10-best list, we first concatenate the 10 hypotheses as a single utterance, generate the dictionary, and then extract lexical features as described above. When an utterance has fewer than 10 hypotheses, we boost the word term frequency by multiplying by (10/#of hypotheses). This boosting only affects the term weighting in the BOW vector and the value of the total number of the words.

Using the training and development data, we evaluated Support Vector Regression (SVR) and RS for the lexical model, which performed better than others such as Multilayer Perceptron, GP, or bagging approach in our preliminary experiments. The parameters of each regressor are optimized using the development data (details are in Section 4.1.2).

We use a weighted linear interpolation method to combine the hypotheses from the acoustic and lexical models to make the final decision. At the development stage, the development set was divided into two folds in order to estimate the combination weights for the acoustic and lexical models. This division was also used to optimize the weighting values for the two acoustic regressors, RS and GP. The optimal weights found on one fold are applied to the other for testing. For the final system, we combine the training and development data together to train the final models since the training data is rather small. The combination weights for the final system on the test set are estimated based on the entire development data and the models learned from the training data.

@&#EXPERIMENTAL RESULTS@&#

Since LOI detection is a regression task, we use correlation coefficient (CC) and mean linear error (MLE) as the performance measure, as used in the Interspeech 2010 Paralinguistic Challenge. We use the implementation in the WEKA data mining toolkit (Hall et al., 2009) for the regressor models. To test statistical significance of the results, we used a McNemar test – for every test instance we record whether one system is better/worse than the other one system, and evaluate the statistical significance according to this contingency table of the two systems.
                        5
                     
                     
                        5
                        There can be other significance tests, for example, a paired t-test on the MLE results to evaluate the absolute error difference.
                     
                  

We divided the development data (described in Table 1) into two parts (i.e., Dev1 and Dev2), corresponding to different speaker groups. This way there is no test speaker that is included to tune parameters, thus the evaluation is speaker independent. Dev1 includes 654 utterances from 2 male and 1 female speakers, while Dev2 has 507 utterances from 1 male and 2 female speakers. We use Dev1 to tune system parameters and test on Dev2, and vice versa. The results on Dev1+Dev2 are simply the concatenated results from two separate testing on Dev1 and Dev2.

We used the RS model as the baseline regressor. Using the development set, we tuned the parameters of RS with unpruned REPTrees.
                              6
                           
                           
                              6
                              The command-line parameters to the WEKA binary were: -P 0.13 -S 15 -I 100 -W weka.classifiers.trees.REPTree – -P.
                            Results are shown in Table 4
                           . The overall performance of RS was better than other models in our preliminary experiments. However, we noticed a large performance gap between the training data itself and the development data. The CC and MLE on the training data itself is 0.997 and 0.012, respectively. This indicates that there might be an overfitting problem using RS regressor.

In order to complement the RS model, we used a GP as the second regressor. The parameters of the GP
                              7
                           
                           
                              7
                              The command-line parameters to the WEKA binary were: -L 3.0 -N 0 -K “weka.classifiers.functions.supportVector.NormalizedPolyKernel -E 2.0”.
                            with NormalizedPolyKernel were optimized using the development data as well. We used Correlation-based Feature Selection (CFS) to reduce the feature size. On the training data, we performed 10-fold cross-validation using CFS. We removed the features which were not included in any selected feature set among the 10 folds. The number of selected feature was 217 in total, which is much smaller compared to the full feature size of 1582. The results using the GP regressor with the full feature set and the reduced feature set are shown in Table 5
                           . When using the full feature set, the CC on the training data of the GP is 0.668. The smaller gap between the training and development data suggests the over-fitting problem is not as serious as that in RS. When the reduced feature set was used, the improvement on Dev1 data was slight, but there is a significant improvement on Dev2 data. The performance on Dev2 using the GP is even better than the result using RS. Compared to the performance gap between Dev1 and Dev2 of RS, the gap of the GP is smaller. Again this shows that the GP model might reduce the over-fitting problem caused by RS.

The linearly combined results using RS and GP regressors are shown in Table 6
                           . The optimal weights from one fold are applied to the other for testing. We can see that the combined results are better than those of the individual regressors, especially for CC. From these results, we expect that the GP could complement the RS model.

In this experiment we evaluate the performance using our proposed lexical features, the impact of ASR errors, and the effectiveness of using more ASR hypotheses. First we use human transcripts to evaluate the contributions of different lexical features and two regressors: SVR and RS. The parameters of SVR
                              8
                           
                           
                              8
                              The command-line parameters to the WEKA binary were: -S 3 -K 2 -C 0.25.
                            with an RBF kernel and RS
                              9
                           
                           
                              9
                              The command-line parameters to the WEKA binary were: -P 0.3 -S 15 -I 100 -W weka.classifiers.trees.REPTree – -P.
                            with unpruned REPTrees are tuned using the development data. The size of the vocabulary in the training set is 1029, and reduced to 535 after we collapsed words that appeared only once.


                           Table 7
                            presents the results using different combinations of lexical features and regressors. Using BOW achieved significantly better performance (p
                           <0.05) than choosing a centroid value of the data set (which resulted in CC and MLE of 0.339 and 0.148, respectively). Incorporating additional feature sets yielded further improvement. When LEN features were added, the largest performance gain was obtained. This may be because we can differentiate the features using the timing information, even though some utterances have the same words. The second most contributing feature set was SIM. In the case that an utterance included one or two words, augmenting BOW with SIM features was effective. Combining all the feature sets together achieved the best performance. Overall using SVR slightly outperforms the RS regressor. These results are significantly better than those of the previous system (Jeon et al., 2010) (CC 0.507 and MLE 0.128). Table 8
                            shows the performance on the divided development data using all the feature sets. Similar to the acoustic model, the performance on Dev1 was better than that on Dev2. Overall we can see that the lexical model performs worse than the acoustic model.

Next, we evaluate using ASR output. Previous work (e.g., Metze et al., 2011; Jeon et al., 2010) showed that matched training and testing transcripts are needed when using lexical models for classification. For example, when the ASR output was used for testing, using ASR output for training performed better than using human transcripts. Therefore in this experiment, we considered matched training and testing conditions, that is, using 1-best output for both training and testing, or n-best output for both training and testing.


                           Table 9
                            shows the performance on ASR 1-best output using all the features. The dictionary was re-estimated using 1-best output. The size of BOW is 445 in this case, which is smaller than that on the human transcription. As we expected, there is a performance degradation using ASR output. Using all the proposed features achieves slightly better performance than our previous results (CC 0.406 and MLE 0.137) (Jeon et al., 2010).

The next question is if we can use multiple hypotheses (e.g., n-best lists) to reduce performance loss due to ASR errors. Table 10
                            shows the results on 10-best output with all the features. In this experiment, we train the models on the ASR 10-best lists. The size of BOW using 10-best lists is 863. Using 10-best list outperforms 1-best ASR output (CC 0.409, MLE 0.136) (McNemar test, p
                           <0.05). The better results of using 10-best output suggest that we can indeed explore information from other candidates than just using the top hypothesis. One benefit using multiple hypotheses is from the word coverage. When we considered the out-of-vocabulary (OOV) words based on the collapsed dictionary, the OOV rate of the development data was 9% on the 10-best output, while it was 14% on the 1-best output.

As shown in the previous experiment, the performance of SVR is better than RS for all the conditions. Therefore, we decided to use SVR alone for the lexical model; unlike for the acoustic model, where we use the combination of RS and GP since GP shows advantages or competitive performance in some conditions.

Our system uses weighted linear interpolation to combine the output from the acoustic and lexical models to generate the final decision. Note that the results from two different acoustic regressors were combined first before combining with lexical models. Table 11
                            shows the combined results when using different transcripts (human transcripts and ASR output) for the lexical model. For a comparison, we include the results based on the combined acoustic models. As shown in Table 11, using human transcripts (REF) achieves the best performance, much better than the acoustic results. Using the ASR output (i.e., ASR-1 and ASR-10) results in performance loss in the lexical models. However, even though the word error rate is quite high, adding lexical information can still improve the performance than only using acoustic information. For this condition, our system is statistically better than the baseline of using acoustic information (McNemar test, p
                           <0.05).

Furthermore, in order to understand the relationship of the system output and the reference labels, we show the scatter plot of the combined results using ASR 10-best hypotheses in Fig. 3
                           , where x-axis is the reference labels and the y-axis is the system output. We can see that most of the predicted values are located between 0.4 and 0.8. This will be further discussed in Section 4.3.1.

For the blind test set, we used the same regressor configurations as on the development data, but combined the training and development data together to train the acoustic and lexical models. For the acoustic model, the features are the same, and the only difference is in the increased training data. We trained two acoustic regressors with increased data and generated the output on the blind test data from each regressor. The output from both regressors were linearly combined. In the case of the lexical model, we used 10-best hypotheses to train the model. The LOI and duration information of each word in the dictionary are also re-estimated using the increased training data. The size of BOW is 1272. The linear combination weights for the RS and GP, and the acoustic and lexical models are based on the optimal weights obtained using the entire development data, which come from the results when only using the training data in the previous experiments.


                        Table 12
                         shows the results of our proposed system, along with the baseline result using the RS acoustic regressor as well as our previous approach in Jeon et al. (2010). The proposed system (RS+GP+SVR) performed significantly better than the baseline and our previous approach (McNemar test, p
                        <0.05). We also notice that the results on the test set are worse than those on the development, even though an increased training set (combined training and development sets) was used for model training. This suggests that the test data may not be similar to the development data, and the optimal weights obtained from the development data are not good for the test set. In fact, we verified this when using the test set to optimize the combination weights, rather than taking the weights obtained from the development data. When new weights are used, there is indeed a significant error reduction (MLE changing from 0.14 to 0.134). This means that if we could use similar data as the test data to optimize the weights, there might be more performance gain.

As shown in Fig. 1, the LOI of most of the training samples lies between 0.4 and 0.8. For an analysis, we want to know whether such a skewed distribution of the LOI values has an impact on system performance. Fig. 4
                            shows the number of instances for different LOI levels in the development data, and for the outputs using different systems. As shown in the figure, the predicted values are more saturated in that range, even more than that for the reference labels. This is more noticeable for the lexical model using 10-best output than the acoustic one, suggesting that the SVR regressor is more affected by the imbalanced distribution. The combination of acoustic and lexical models lies in between. This kind of concentration reduces errors in that range, but causes more errors in regions with higher and lower LOI values. Note that the similar distribution of the reference labels and predicted values might be a good property of a model, but does not guarantee better system performance.

To reduce this data skew, we tried up/down-sampling approaches on the development data to increase/decrease the number of instances in the regions with low/high distributions. We expect that reducing the data imbalance by sampling might improve the performance. For the up-sampling experiment, we duplicated some utterances in the low LOI frequency groups that are most similar to utterances in the high frequency area and added in the training data. Similarly for down-sampling, utterances in the high frequency area that are most similar to utterances in the low frequency area were removed from training data. For the similarity measure, we used a cosine similarity measure. We evaluated these sampling approaches using the RS acoustic regressor. We up/down-sampled about 25% of the original training samples. The RS acoustic regressor was trained with up/down-sampled training data and tested on the development data. We did not find any performance gain from these up/down-sampling experiments. It might be because we do not have enough training instances or the similarity measure is not appropriate to identify instances to add/remove in sampling. More research is needed to address this problem for better system performance.

It is expected that high ASR word error rate degrades the lexical model and therefore results in performance loss of the combined system. The ASR system we used in this study is a state-of-art system, but because of various factors (mismatched data, non-native speakers), the word error rate is quite high. If we re-compute the word error rate based on BOW representation, which does not consider the word sequences and only compares whether the words appear in the vector, this reduces the error rate by about 7%. Note that an OOV word on the development data is regarded as INFREQUENT in the dictionary. This shows that even when using this lenient measure, a lot of the words are still incorrectly recognized, which explains the low performance of the lexical model on the ASR output. Another problem with ASR is that it does not detect the non-linguistic vocalization tokens accurately. Schuller et al. (2009a) pointed out that non-linguistic vocalization tokens, especially coughing and laughter, are important cues for emotion or affective state recognition. Using human transcripts, when we disregard those non-linguistic vocalization tokens, the performance of the lexical model on development data drops to 0.443 and 0.134 for CC and MLE respectively, from 0.521 and 0.127 in Table 7, when SVR was used. This result is only slightly better than that using 10-best from ASR output (0.424 and 0.134 in Table 10). Therefore for LOI detection or similar emotion classification tasks, the ASR system needs to be developed to take into account the important non-linguistic vocalization tokens.

Often we can assume that there is no sudden change of interest level from the listener in two consecutive turns, and thus contextual information may help predict LOI for the current utterance. (Schuller et al., 2009a; Lee et al., 2009; Jeon et al., 2010) showed some performance gain when combining the previous listener's status to predict the current status. We did not include contextual information in this study since for the test set, time and context information is not available (i.e., the identity and sequence of speakers are blinded), and thus we cannot use it for the blind testing.

@&#CONCLUSIONS@&#

In this paper, we presented a study of using a decision-level fusion of acoustic and lexical information for level-of-interest sensing. We used two different regression algorithms, RS and GP, to complement each other for the acoustic model. Although RS regressor performs the best by itself, we notice that it has a potential over-fitting problem, and thus combine it with a second regressor, GP. We also develop lexical features that are less affected by erroneous ASR output. Beyond the BOW feature, we include estimated LOI values for the words, length-related features, and the LOI vector of most similar utterances. For ASR conditions, we propose to use n-best hypotheses. Incorporating these features yielded significantly better performance than our previous approach on human transcripts, but only slightly better on 1-best ASR output. When n-best hypotheses are used to extract the lexical features, significantly better performance gain is achieved. We demonstrate that a decision-level fusion from the acoustic and lexical models outperforms using one information source, even for the ASR conditions with high word error rate.

Although we tried to develop robust lexical features for erroneous ASR output, the contribution of the lexical model on ASR output is less than that using human transcripts because of the word errors. In the future, we will investigate other lexical features. Another important work is to detect non-linguistic vocalization tokens as explained in Section 4.3.2. Furthermore, for ASR conditions, we plan to incorporate recognition confidence information in the lexical model.

@&#ACKNOWLEDGMENTS@&#

The authors thank Wen Wang for helping generate the ASR output for the data sets used in this study. This work is supported by an award from the US Air Force Office of Science Research, FA9550-10-1-0388.

@&#REFERENCES@&#

