@&#MAIN-TITLE@&#Detecting conversational groups in images and sequences: A robust game-theoretic approach

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A game-theoretic approach for group detection in still images and video.


                        
                        
                           
                           Extended (game) theory to integrate temporal information and data continuity.


                        
                        
                           
                           A new model of frustum of visual attention which achieves better performance.


                        
                        
                           
                           A new annotated dataset for group detection including 10685 labeled frames.


                        
                        
                           
                           Performance evaluation on all public datasets outperforming the state of the art.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Group detection

F-formation detection

Conversational groups

Game-theory

Scene understanding

@&#ABSTRACT@&#


               Graphical abstract
               
                  
                     
                        
                           Image, graphical abstract
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

The visual analysis of groups is becoming more and more widespread in computer vision, after decades of research on the automated modeling of individuals (which still remains an open problem), the goal has moved from encoding simple actions performed by a single subject to capturing dyads or clusters of social interactions [1–10]. This is of extreme importance in many fields and applications, also addressing social and life sciences [11,12]. This seems to be a necessary step, since humans are essentially a social species, as demonstrated by the fact that in everyday life people continuously interact with each other to achieve goals or simply to exchange states of mind. In this paper, we exploit a recent taxonomy presented in [13], which indicates that many types of groups can be defined. In particular, we target standing conversational groups, also known as F-formations 
                     [14], that is, groups of people who spontaneously decide to be in each other’s immediate presence to converse with each and every member of that group.

Standing conversational groups are of primary importance in many contexts, such as video surveillance [7], social signal processing [1,2,4,6], multimedia [3], social robotics [15], and activity recognition [16], as we will discuss extensively in Section 2.

Many studies have been carried out by social psychologists to understand how people behave in public. By exploiting the theory behind these findings, we propose novel and more socio-psychologically principled ways of designing methods for automatically analyzing human behavior. For example, Hall [17] proposed that relationships and levels of interactions could be inferred by considering different physical distances.
                     
                  

Goffman [18] observed that group interactions can be categorized into those that are ‘focused’ and those that are ‘unfocused’. Focused interactions concern the gathering of people to participate in an activity where there is a common focus, such as playing and watching a football match, conversing, or marching in a band. Unfocused encounters involves light interactions such as avoiding people on a busy street, briefly greeting a colleague while passing them in the corridor, or indicating to let someone pass when boarding a train. This taxonomy has been exploited recently in [13] for addressing F-formations.

Within the class of focused encounters, the F-formation is a specific type of group interaction which requires more attention from our senses. Specifically, an F-formation arises “whenever two or more individuals in close proximity orient their bodies in such a way that each of them has an easy, direct and equal access to every other participant’s transactional segment, and when they maintain such an arrangement” [19, p. 243]. Some examples of F-formations in real-world situations are illustrated in Fig. 1a. There can be different F-formations as shown in Fig. 2a–e. In the case of two participants, typical F-formation arrangements are vis-a-vis, L-shape, and side-by-side.

Three social spaces emerge from an F-formation: the o-space, the p-space and the r-space. The most important part is the o-space (see Fig. 2), a convex empty space surrounded by the people involved in a social interaction, in which every participant looks inward, and no external people are allowed. The p-space is a narrow strip that surrounds the o-space, and that contains the bodies of the conversing people, while the r-space is the area outward the p-space.

Our goal in this paper is to develop a robust approach to automatically detect F-formations from images and videos employing a single monocular camera. As input, the approach requires the position of the persons in the scene on the ground plane as well as their body orientation, although in most cases, head orientation is more readily captured, even under heavy occlusions. These cues are easily obtainable nowadays, even if they are not estimated very accurately, and many approaches are aimed at extracting such information from raw images/videosequences [4,20,21]. Among the few approaches of F-formation detection, a recent experimental work of Setti et al. [22] shows that substantial improvement in the performance of F-formation detection algorithms can be achieved by combining a probabilistic approach (as [7]) and graph-based clustering methods [6]. Motivated by these studies, we develop a new sociologically-based approach which combines in a natural way the modeling of the uncertainty in the position and orientation of the subjects and a game-theoretic clustering approach , allowing one to extract coherent groups in edge-weighted graphs, digraphs and hypergraphs [23,24]. The game-theoretic setting provides a conceptual framework which allows us to integrate temporal information in a principled way, in an attempt to reliably extract groups in video sequences under severe noisy conditions. This is done by using a recent approach to integrate multiple payoff functions in an evolutionary game-theoretic setting [25].

This work represents a substantial contribution to group detection in real scenarios. To date in computer vision, grouping behaviors have been analyzed mainly in dynamic situation via tracking, exploiting the oriented velocity as a primary cue, for example by associating individuals’ tracklets [26–34]. In our case, F-formation are manifested primarily when people are still, so that a finer yet robust analysis is required. Our approach considers in fact the detection of groups in both still images and videos.

To test the effectiveness of the proposed approach, we performed extensive experiments over five different datasets, each one representing a particular scenario. In particular, we used a synthetic dataset [7], the Coffee Break dataset [7], the GDet dataset [7], the Idiap Poster data dataset [6], the Cocktail Party [5] dataset and two new dataset, one proposed by Choi et al. [35] and FriendsMeet2 that we propose in this work. We also carried out systematic noise resilience experiments to fully investigate the stability and robustness of our method. The results consistently show the superior or comparable performances of the proposed approach over the state of the art.

The rest of the paper is organized as follows. A detailed review of the literature on group detection approaches is presented in Section 2. Our approach is detailed in Section 3. In Section 4 we describe the game-theoretic clustering approach we use to extract F-formations and its extension to multiple affinity matrices. Finally, Section 5 presents the experimental results and Section 6
                      concludes the paper.

@&#LITERATURE REVIEW@&#

During multi-party activities, we expect that there is a different underlying structure that governs the behavior of groups compared to individuals acting independently. For example, there has been considerable prior work on estimating group activities by modeling behavior at the individual as well as group level [8–10,36]. However, unlike works that treat all group structures equivalently, our premise is that there are fundamental semantic differences in what this prior work has considered to be a ‘group’ and what we refer (from the social psychological literature) as an ‘F-formation’ [14]. These prior definitions of a group of people assume that they should be close together because they are for example, forming a queue, watching a football match, crossing the road together, or asked to mingle in a specific location. Some of these principles informed early socially-motivated methods of people tracking [37] by the social force model [38], that originated from pedestrian simulation research.

In more semantically meaningful social cases, one can attribute meaning to groupings based on some form of acquaintanceship, such as for detecting when people are traveling together [28,36] or when people are conversing in a lecture hall [2]. Such an interaction requires a focusing of the senses, compared to the other group behaviors which can rely more on peripheral and unfocused sensing [18]: an interesting taxonomy of diverse kinds of social groups in relation to the kind of acquaintanceship of their members, and especially suited to computational frameworks, can be found in [13]. The automated analysis of different forms of unfocused and unfocused encounters was investigated extensively by Choi et al. [35] who created a data set of 7 different categories of group types, relating categories such as queues, sitting in a row, standing in a row, standing facing each other, and others. However in free standing scenarios, when people come together physically in order to make conversation, a specific, unspoken, and mutual agreement is made between all those involved that they wish to converse for some extended but finite period of time. Such behavior goes beyond just a cooperation between people to behave in a socially acceptable manner (e.g. by staying in line in a queue) and really indicates someone’s willingness to be associated with someone else, and to actively exchange ideas and build social relations with a person.

Importantly, the region in front of the body in which limbs can reach easily, and hearing and sight is most effective was defined as the transactional segment
                        [19]. A necessary condition of the F-formation was that the transactional segments of all members of an F-formation should overlap. Such a region can be considered an individual’s frustum of social attention.

Considering this idea of frustum of attention, computer vision researchers have considered how the head pose can be used as a proxy for visual attention [39]. For visually led tasks such as looking at adverts [39], considering the visual attentional mechanisms is useful. However, when considering social contexts, the concept of social attention is a relatively new domain in the social sciences [40]. More specifically, head pose is actually equally if not more perceptually salient as a cue for gaze direction in humans [40, Chapter 6]. Moreover Kendon studied the role of gaze direction during conversational interactions suggesting that it functions as a cue for turn-taking, holding, or yielding [41]. Jovanovic and Op den Akker also found that addressees could be identified using gazing cues [42], while Duncan found that speakers attracted the gaze of listeners [43] during conversations. Finally, Ba and Odobez [44] exploited findings in primate social behavior by modeling plausible eye-in-head positions for gaze estimation to estimate the visual focus of attention of participants during meetings using only head pose.

For the specific task of detecting F-formations, different approaches have been proposed. Groh et al. [1] proposed to use the relative shoulder orientations and distances (using markers attached to the shoulders) between each pair of people as a feature vector for training a binary classification task. Cristani et al. [7] proposed to solve the task using a Hough voting strategy which accumulated a density estimating the location of the o-space. Concurrently, Hung and Kröse [6] proposed to consider an F-formation as a dominant-set cluster [45] of an edge-weighted graph where each node in the graph was a person, and the edges between them measures the affinity between pairs.

Later these two approaches were compared by Setti et al. [22] to investigate the strengths and weaknesses of both approaches for the F-formation task. They found that while the method of Cristani et al. [7] was more stable using head orientation information in the presence of noise, the method of Hung and Kröse [6] performed better when only position (and not orientation) information was available. Setti et al. [46] also proposed to handle the physical effect that different cardinalities of the F-formations sizes would have on the most plausible physical spatial layout of each member of the group. By taking this into account using separate accumulation spaces for each size, they were able to improve over the original Hough voting strategy proposed in [7]. A similar density-based approach has also been proposed by Gan et al. [3] where the final purpose of the task was to dynamically select camera angles for automated event recording. Tran et al. have subsequently analyzed temporal patterns of activities [10]. Choi et al. [35] have modeled different forms of group behavior discriminatively by projecting the body positions into 3d space and similar to our model, finding overlaps in a sampled density space. However, their approach was trying to distinguish differing group types and is not dedicated to conversational groups. Finally, Setti et al. [13] presented a graph-cut based minimization for detecting F-formations using proxemic data, that even if it shows strong performances, does not include temporal reasoning (it applies only to static images).

Given a dataset of frames with positions of the persons and head/body orientations, the pipeline of the algorithm (see Fig. 3) can be summarized in the following steps:
                     
                     
                        
                           1.
                           For each person pi
                               ∈ P in a frame/scene, generate a frustum fi
                               based on his position and orientation in world coordinates and modeled by a 2-dimensional histogram (see Section 3.1).

Compute a pairwise affinity matrix for each pi
                               ∈ P (see Section 3.3).

In case a smoothing across multiple frame is required, compute the weights of each frame based on the theory of multipayoff games (see Section 4.1).

Extract F-formation (clusters) using evolutionary stable strategy (ESS)-clusters (see Section 4).

Our frustum of social attention is inspired by Kendon’s definition of a transactional segment. This takes into account both the field of view of the person and also the locus of attention of all other senses for a given body orientation. Since it is typically easier to obtain the head pose rather than the body orientation in crowded environments (due to occlusions), the head pose provides an approximation of the direction of the social attention frustum. It is characterized by a direction θ (which is the person’s head orientation), an aperture γ (we used 
                           
                              γ
                              =
                              
                                 160
                                 ∘
                              
                           
                         which was reported by Ba and Odobez [44], who used the same measure for approximating the range of possible eye gaze directions given a specific head pose) and a length l specified in cm or in meters based on the data. These three elements determine the socio-attentional frustum of a person. In this work we propose a new frustum model based on sampling from two probability distributions. In our approach the sampling has a twofold impact in the whole pipeline:

                           
                              •
                              
                                 Application decoupling: It decouples the entire algorithm from a specific model because using samples and histograms makes the entire approach non-parametric and thus able to easily accommodate forthcoming models;


                                 Data smoothing: Sampling methods, in general, smooth noisy data by looking for a statistical consensus. It is quite common in our scenario to deal with unreliable data since tracking, detections, head orientations, etc. are all noisy and prone to errors.

The new model differ from [47] in two fundamental aspects:

                           
                              •
                              
                                 Sampling method: Previously, samples were drawn from a 2D Gaussian distribution which was chopped based on the field of view (see Fig. 5a). Each sample was subsequently marked as valid or not and the drawning process stops until the desired number of valid samples was reached. This approach is time consuming. The samples generated using our new method are all valid by default speeding up the entire process.


                                 Peripheral view: The new method is more expressive since it is able to capture the peripheral field of view (see Fig. 5b) emboding the natural lateral decay of the human view instead of the sharp boundaries of the previous approach.

These two modifications are reflected into an higher performances and an overall speedup of the algorithm. More precisely, the proposed new frustum is based on a combination of two probability distributions, a Gaussian distribution G and a Beta distribution B.

The G distribution (see Fig. 4a) is used to generate samples related to the aperture of the frustum so is centered in the head orientation θ of a person with a variance set such that the full width of the Gaussian distribution corresponds to the desired aperture of the frustum. In a Gaussian distribution the 99% of the samples are located in the range of 
                           
                              [
                              −
                              3
                              σ
                              ,
                              3
                              σ
                              ]
                           
                        ; this range will correspond to the full aperture of the frustum, so that setting the variance in a way that the aperture is fully covered becomes an easy task, 
                           
                              σ
                              =
                              
                                 1
                                 3
                              
                              *
                              
                                 γ
                                 2
                              
                           
                        .

The B distribution (see Fig. 4b) is used to generate samples that are dense in close proximity of the person while decades going far away, to achieve this shape we set the distribution parameters 
                           
                              α
                              =
                              0.8
                           
                         and 
                           
                              β
                              =
                              1.1
                           
                         (see Fig. 5a). The values returned by the B distribution are bounded in [0, 1] and need to be multiplied by the desired length of the frustum l. The samples obtained using these two distributions are in polar coordinates (an angle and a distance), to obtain samples in the 2D space is sufficient to apply a simple trigonometric rule to each of them. Given a pair of samples from G and B (Gi, Bi
                        ) and the position of a person (px, py
                        ) the 2D position of each sample is:
                        
                           
                              (1)
                              
                                 
                                    
                                       
                                          
                                             s
                                             x
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                p
                                                x
                                             
                                             +
                                             cos
                                             
                                                (
                                                
                                                   G
                                                   i
                                                
                                                )
                                             
                                             *
                                             
                                                B
                                                i
                                             
                                             *
                                             l
                                          
                                       
                                    
                                    
                                       
                                          
                                             s
                                             y
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                p
                                                y
                                             
                                             +
                                             sin
                                             
                                                (
                                                
                                                   G
                                                   i
                                                
                                                )
                                             
                                             *
                                             
                                                B
                                                i
                                             
                                             *
                                             l
                                          
                                       
                                    
                                 
                              
                           
                        Drawn independently n samples from both the distributions and applying the above equations we obtain a set of samples that falls in the human frustum of visual attention. With respect to the previous model there is no need to have a continuous sampling until the desired number of samples are reached, because all pairs of sample generated from the N and B distributions are already valid without the need of pruning the un-biological ones and making the approach faster.

Using the approach in [47] with the number of samples 
                           
                              n
                              =
                              5000
                           
                         the time to generate the feasible samples is ≃ 0.161s while with the new method is ≃ 0.008 s, speeding up the entire algorithm twenty time the previous approach. Each person in a scene is thus modeled using his/her frustum represented as 2-dimensional histogram hi
                         of size Nc
                         × Nr
                        , normalized by the number of samples (n), where Nc
                         and Nr
                         span over the area of the scene captured by the camera.

In order to decide the best binning of the 2D histogram we decided to carry out an extensive experimentation over all the publicly available datasets to see which size gives the better trade-off between required space to store the histograms and performances. We tested the performances using binning ranges from [5, 10, 15, 20, 30, 50, 100, 150, 200, 300, 400], obtaining the F1-score plotted in Fig. 6.

Summing the F1-score obtained on each dataset for a particular binning it is possible to rank the performance, obtaining the Table 1. Here we can see (Table 1
                        ) that the best performances are obtained using 
                           
                              
                                 N
                                 c
                              
                              =
                              
                                 N
                                 r
                              
                              =
                              50
                           
                         or 20 bins; since their performance difference is very low (0.002), we decided to use histograms of size 20 × 20 to keep the size as smaller as possible without losing the strengths of the method.

Two persons are more likely to be interactants if their social attention frustums overlap. By quantifying the pairwise interaction as a distance between distributions, we are able to encode the uncertainty about the true transactional segment of the persons given their head pose. Since we are dealing with histograms that represent discrete probability distributions, it is natural to consider information-theoretic measures to model the distance between them.

Given a pair of discrete probability distributions 
                           
                              P
                              =
                              {
                              
                                 p
                                 1
                              
                              ,
                              …
                              ,
                              
                                 p
                                 n
                              
                              }
                           
                         and 
                           
                              Q
                              =
                              {
                              
                                 q
                                 1
                              
                              ,
                              …
                              ,
                              
                                 q
                                 n
                              
                              }
                              ,
                           
                         the first natural choice to measure their distance is given by the well-known Kullback–Leibler (KL) divergence, which is defined as:

                           
                              (2)
                              
                                 
                                    
                                       
                                          
                                             D
                                             
                                                (
                                                P
                                                ∥
                                                Q
                                                )
                                             
                                             =
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                n
                                             
                                             log
                                             
                                                p
                                                i
                                             
                                             
                                                
                                                   p
                                                   i
                                                
                                                
                                                   q
                                                   i
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        The KL-divergence is known to be asymmetric. A symmetric version of the KL-divergence measure is the Jensen–Shannon (JS) divergence [48], which is defined as:

                           
                              (3)
                              
                                 
                                    
                                       
                                          
                                             J
                                             
                                                (
                                                P
                                                ,
                                                Q
                                                )
                                             
                                             =
                                             
                                                
                                                   D
                                                   (
                                                   P
                                                   ∥
                                                   M
                                                   )
                                                   +
                                                   D
                                                   (
                                                   Q
                                                   ∥
                                                   M
                                                   )
                                                
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              M
                              =
                              
                                 1
                                 2
                              
                              
                                 (
                                 P
                                 +
                                 Q
                                 )
                              
                           
                         is the mid-point between P and Q. Hence, given two persons i and j in a scene and their vectorized histograms hi
                         and hj
                        , the distance between i and j can be calculated either as D(hi
                        ||hj
                        ) or as JS(hi, hj
                        ).

To obtain a measure of affinity, rather than distance, between each pair of histograms we used the classical Gaussian kernel:

                           
                              (4)
                              
                                 
                                    
                                       
                                          
                                             γ
                                             (
                                             i
                                             ,
                                             j
                                             )
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             exp
                                             
                                                {
                                                
                                                   −
                                                   
                                                      
                                                         d
                                                         (
                                                         
                                                            h
                                                            i
                                                         
                                                         ,
                                                         
                                                            h
                                                            j
                                                         
                                                         )
                                                      
                                                      σ
                                                   
                                                
                                                }
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where the function “d” refers to either the KL- or the JS-divergence. The parameter σ in Eq. (4) allows intrinsic properties of the scene (e.g., how far people usually stand from each other when they are in an F-formation) to be taken into account. Once we calculate this measure, it becomes possible to find groups of persons that are interacting by exploiting a grouping game, as described in the next section.

In this work we cast the approach proposed in [23] in the problem of detecting F-formations in terms of a non-cooperative clustering game. We choose this clustering algorithm for a series of desirable properties:

                        
                           •
                           The similarity function is not required to be a metric, so it is usable with the Kullback–Leibler.

Setting an a-priori number of clusters, like in the k-means procedure, is not needed. This is useful, since the number of groups in a scene in unknown.

Game-theory domain provides us the theoretical foundation to integrate multiple payoff matrices, which is of valuable importance when dealing with different temporal instants (see Section 4.1).

Despite the above properties and for the sake of completeness, the performances of the game-theoretic clustering in this scenario has been compared with a more traditional method, the Spectral Clustering [49] algorithm, showing the superiority of the first method. The details of the experiment and the quantitative results has been reported respectively in Section 5.3.3 and in Table 3 (see “R-GTCG SC” rows).

Given a set of elements 
                        
                           O
                           =
                           {
                           1
                           …
                           n
                           }
                        
                      and an n × n (possibly asymmetric) affinity matrix 
                        
                           A
                           =
                           (
                           
                              a
                              
                                 i
                                 j
                              
                           
                           )
                        
                      which quantifies the pairwise similarities between the objects in O, we envisage a situation whereby two players play a game which consists of simultaneously selecting an element from O. After showing their choice the players get a reward which is proportional to the similarity of the chosen elements. In game-theoretic jargon the elements of set O are the “pure strategies” available to both players and the affinity matrix A represents the “payoff” function (specifically, aij
                      represents the payoff received by an individual playing strategy i against an opponent playing strategy j). In our application, the objects to be grouped (namely, the pure strategies of this grouping game) correspond to the persons detected in a scene, the payoff function being the similarity measure between subjects as described in the previous sections.

A central notion in game theory is that of a mixed strategy, which is simply a probability distribution 
                        
                           x
                           =
                           
                              
                                 (
                                 
                                    x
                                    1
                                 
                                 ,
                                 …
                                 ,
                                 
                                    x
                                    n
                                 
                                 )
                              
                              T
                           
                        
                      over the set of pure strategies O. Mixed strategies clearly belong to the 
                        
                           (
                           n
                           −
                           1
                           )
                        
                     -dimensional standard simplex:

                        
                           (5)
                           
                              
                                 Δ
                                 =
                                 
                                    {
                                    x
                                    ∈
                                    
                                       R
                                       n
                                    
                                    :
                                    
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       n
                                    
                                    
                                       x
                                       i
                                    
                                    =
                                    1
                                    
                                    and
                                    
                                    
                                       x
                                       i
                                    
                                    ≥
                                    0
                                    ,
                                    
                                    i
                                    =
                                    1
                                    ,
                                    …
                                    ,
                                    n
                                    }
                                 
                                 
                                 .
                              
                           
                        
                     Given a mixed strategy 
                        
                           x
                           ∈
                           Δ
                           ,
                        
                      we define its support as 
                        
                           σ
                           
                              (
                              x
                              )
                           
                           =
                           
                              {
                              i
                              ∈
                              O
                              :
                              
                                 x
                                 i
                              
                              >
                              0
                              }
                           
                        
                     .

The expected payoff received by an individual playing mixed strategy 
                        y
                      against an opponent playing mixed strategy 
                        x
                      is given by 
                        
                           
                              y
                              T
                           
                           A
                           x
                        
                     . The set of best replies against a mixed strategy 
                        x
                      is defined as 
                        
                           β
                           
                              (
                              x
                              )
                           
                           =
                           
                              {
                              y
                              ∈
                              Δ
                              :
                              
                                 y
                                 T
                              
                              A
                              x
                              =
                              
                                 max
                                 z
                              
                              
                                 
                                    z
                                    T
                                 
                                 A
                                 x
                              
                              }
                           
                        
                     . Finally, a mixed strategy 
                        
                           x
                           ∈
                           Δ
                        
                      is said to be a Nash equilibrium if it is a best reply to itself, namely if 
                        
                           x
                           ∈
                           β
                           (
                           x
                           )
                        
                      or, in other words, if

                        
                           (6)
                           
                              
                                 
                                    x
                                    T
                                 
                                 A
                                 x
                                 ≥
                                 
                                    y
                                    T
                                 
                                 A
                                 x
                              
                           
                        
                     for all 
                        
                           y
                           ∈
                           Δ
                        
                     . If inequality holds strictly, then 
                        x
                      is said to be strict Nash equilibrium. Intuitively, at a Nash equilibrium no player has an incentive to unilaterally deviate from it. The clustering game is supposed to be played within an evolutionary setting wherein the two players, each of which is assumed to play a pre-assigned strategy, are repeatedly drawn at random from a large population. Here, given a mixed strategy 
                        
                           x
                           ∈
                           Δ
                           ,
                        
                      
                     xj
                      (j ∈ O) is assumed to represent the proportion of players that is programmed to select pure strategy j. A dynamic evolutionary selection process will then make the population state 
                        x
                      evolve according to a survival-of-the-fittest principle in such a way that, eventually, the better-than-average (pure) strategies will survive while the others will get extinct. Within this context, a mixed strategy 
                        
                           x
                           ∈
                           Δ
                        
                      is said to be an evolutionary stable strategy (ESS) if it is a Nash equilibrium and if, for each best reply 
                        y
                      to 
                        
                           x
                           ,
                        
                      we have 
                        
                           
                              x
                              T
                           
                           A
                           y
                           >
                           
                              y
                              T
                           
                           A
                           y
                        
                     . Intuitively, ESS’s are strategies such that any small deviation from them will lead to an inferior payoff (see [50] for an excellent introduction to evolutionary game theory).

In [23,24] a combinatorial characterization of ESS’s is given which makes them plausible candidates for the notion of a cluster (which they call ESS-cluster). The motivation behind this claim resides in the property that ESS-clusters do incorporate the two basic features which characterize a cluster, i.e.,

                        
                           •
                           
                              Internal coherency: elements belonging to the cluster should have high mutual similarities;


                              External incoherency: the overall cluster internal coherency decreases by introducing external elements.

We refer to [23,24] for details. One of the distinguishing features of this approach is its generality as it allows one to deal in a unified framework with a variety of scenarios, including cases with asymmetric, negative, or high-order affinities. Note that, when the affinity matrix A is symmetric (that is, 
                        
                           A
                           =
                           
                              A
                              T
                           
                        
                     ) the notion of an ESS-cluster coincides with that of a dominant set [45], which amounts to finding a (local) maximizer of 
                        
                           
                              x
                              T
                           
                           A
                           x
                        
                      over the standard simplex Δ.

Algorithmically, to find an ESS-cluster one can use the classical replicator dynamics 
                     [50], a class of dynamical systems which mimic a Darwinian selection process over the set of pure strategies. The discrete-time version of these dynamics is given by the following update rule:

                        
                           (7)
                           
                              
                                 
                                    x
                                    i
                                 
                                 
                                    (
                                    t
                                    +
                                    1
                                    )
                                 
                                 =
                                 
                                    x
                                    i
                                 
                                 
                                    (
                                    t
                                    )
                                 
                                 
                                    
                                       
                                          (
                                          A
                                          x
                                          
                                             (
                                             t
                                             )
                                          
                                          )
                                       
                                       i
                                    
                                    
                                       x
                                       
                                          
                                             (
                                             t
                                             )
                                          
                                          T
                                       
                                       A
                                       x
                                       
                                          (
                                          t
                                          )
                                       
                                    
                                 
                              
                           
                        
                     for all i ∈ O. The process starts from a point 
                        
                           x
                           (
                           0
                           )
                        
                      usually close to the barycenter of the simplex Δ, and it is iterated until convergence (typically when distance between two successive states is smaller than a given threshold). It is clear that the whole dynamical process is driven by the payoff function which, in our case, is defined precisely to favor the evolution of highly coherent objects. Accordingly, the support 
                        
                           σ
                           (
                           x
                           )
                        
                      of the converged population state 
                        x
                      does represent a cluster, the non-null components of which providing a measure of the degree of membership of its elements.

The support of an ESS corresponds to the indices of the elements in the same group. To extract all the ESS-clusters we implemented a simple peel-off strategy: when an ESS-cluster is computed the corresponding elements are removed from the original and the replicator dynamics is executed again on the remaining elements.

When dealing with videos, the inter-frame smoothness between consecutive frames can be exploited to face cases of noisy data, such as wrong positions or head orientations. The idea is simply to consider a buffer of K frames: at time t, we will have knowledge of the frames at time 
                           
                              t
                              −
                              K
                              +
                              1
                              ,
                              …
                              ,
                              t
                              ,
                           
                         which can be used jointly for a more robust group estimation. This keeps the process of group modeling on-line (it can lie on top of the tracking algorithm), while permitting to prune out noise in an effective way. Assuming that the movement of the same person between frames is smooth, given a set of K consecutive frames, the problem is then to somehow integrate the corresponding affinity matrices to perform the grouping process.
                     

From our game-theoretic perspective this problem can be seen in the context of multiple-payoff (or multi-criteria) games, a topic which has been the subject of intensive studies by game theorists since the late 1950s [51–54]. Under this setting, payoffs are no longer scalar quantities but take the form of vectors whose components represent different commodities. Clearly, the main difficulty which arises here is that the players’ payoff spaces now can be given only a partial ordering. Although in “classical” game theory several solution concepts have been proposed during the years, the game theory community has typically given little attention to the evolutionary setting. Recently, a solution to this problem has been put forward by Somasundaram and Baras [25] who extended the notion of replicator dynamics and that of an ESS using the concept of Pareto–Nash equilibrium. Another recent attempt towards this direction, though more theoretical in nature, can be found in [55].

In the work reported in this paper, we follow the idea proposed in [25]. Using concepts from multi-criteria linear programming (MCLP) [56] they proposed a notion of Pareto reply and of Pareto–Nash equilibrium and showed the equivalence with “weighted sum scalarization”, a classical technique from multi-objective optimization (see, e.g., [56]). Basically, this means that a Pareto–Nash equilibrium can be achieved by integrating the K affinity matrices as follows:

                           
                              (8)
                              
                                 
                                    
                                       
                                          
                                             A
                                             ^
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                K
                                             
                                             
                                                
                                                   w
                                                   ^
                                                
                                                i
                                             
                                             
                                                A
                                                i
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where the 
                           
                              
                                 w
                                 ^
                              
                              i
                           
                        ’s (
                           
                              i
                              =
                              1
                              …
                              K
                           
                        ) represent appropriate non-negative trade-off weights associated to the different matrices, subject to the constraint 
                           
                              
                                 ∑
                                 i
                              
                              
                                 
                                    w
                                    ^
                                 
                                 i
                              
                              =
                              1
                           
                        . Formulated in this way, the problem of determining a Pareto–Nash equilibrium in a multi-payoff scenario is now reduced to the problem of determining the correct trade-off weights and this in turn can be done by solving a multi-objective linear programming problem (see Fig. 7). To this end, following [25], in our experiments we used the multi-objective simplex method described in [56, Chapter 7] (see also [25] for details).

The algorithm described above provides as output a set of weight vectors which allows one to obtain the whole Pareto front of the original multi-objective problem. However, in practice, in a decision-making context like ours one has to obtain somehow a single solution but, in general, it is not clear how to do it since it might depend on subjective or other extra-criterion preferences on the decision maker’s part. Below we provide some heuristics which are motivated by the following empirical observations.

                           
                              1.
                              If the matrices are all very similar to each other the weights generated are uniformly distributed meaning that the matrices are all equally important.

If a matrix is very different from the other ones (e.g. one noisy frame) it gets the highest weight.

At a first glance, the straightforward idea is to remove the matrix with the highest weight, unfortunately this could lead to very bad results because the most diverse could be the matrix without corruption if two frames are taken into account. Given a set of weights 
                           
                              W
                              =
                              {
                              
                                 
                                    w
                                    ¯
                                 
                                 1
                              
                              ,
                              …
                              ,
                              
                                 
                                    w
                                    ¯
                                 
                                 n
                              
                              }
                              ,
                           
                         in which each 
                           
                              
                                 w
                                 ¯
                              
                              i
                           
                         is an K-dimensional vector representing the weight for each payoff matrix 
                           
                              
                                 A
                                 
                                    1
                                    ⋯
                                    K
                                 
                              
                              ,
                           
                         we propose the following heuristics to select/generate the proper set 
                           
                              w
                              ^
                           
                        :

The set 
                              W
                            can be seen as a k × n matrix in which each column is a feasible solution from the Pareto front. The final weights are computed summing the rows of the matrix 
                              W
                            and normalizing the result by the sum, so that 
                              
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    n
                                 
                                 
                                    
                                       w
                                       ^
                                    
                                    i
                                 
                                 =
                                 1
                              
                           :

                              
                                 (9)
                                 
                                    
                                       
                                          
                                             
                                                
                                                   w
                                                   i
                                                
                                                ^
                                             
                                          
                                          
                                             =
                                          
                                          
                                             
                                                
                                                   ∑
                                                   
                                                      j
                                                      =
                                                      1
                                                   
                                                   n
                                                
                                                
                                                   W
                                                   
                                                      j
                                                      ,
                                                      i
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   w
                                                   i
                                                
                                                ^
                                             
                                          
                                          
                                             =
                                          
                                          
                                             
                                                
                                                   
                                                      w
                                                      ^
                                                   
                                                   i
                                                
                                                
                                                   
                                                      ∑
                                                      
                                                         i
                                                         =
                                                         1
                                                      
                                                      K
                                                   
                                                   
                                                      w
                                                      i
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           this heuristic has been used in [47] to solve the problem of highly unbalanced weights that occur when few frames in the window are available (in particular in case of two frames). Few frames, in general, produce a reduced set of feasible solutions. Moreover, in the case of noise, the algorithm assigns an higher weight to the more dissimilar matrix (the correct or the corrupted ones), ending up into a set of unbalanced weights with the limit case of weights in {0,1}. For example, given two consecutive frames (A,B) with B the corrupted ones, a possible set of weights can be:

                              
                                 
                              
                           
                        

If the 
                              
                                 
                                    w
                                    ¯
                                 
                                 1
                              
                            is taken the uncorrupted matrix is chosen while taking 
                              
                                 
                                    w
                                    ¯
                                 
                                 2
                              
                            the opposite happens. Choosing 
                              
                                 
                                    w
                                    ¯
                                 
                                 2
                              
                            lead to a poor performances or wrong grouping since the corrupted frames is chosen. As one can see, making a decision in this context is very complicated because we have no prior information on whether the noise is and the weights are not informative enough. To avoid to make this choice, a statistical consensus is searched by meaning the possible solution. This heuristics, in the previous example, will assign the same weights 
                              
                                 
                                    
                                       w
                                       A
                                    
                                    ^
                                 
                                 =
                                 
                                    
                                       w
                                       B
                                    
                                    ^
                                 
                                 =
                                 0.5
                              
                            to both the frames. Thus in the final matrix at least half of the correct data are kept from A and half of the corrupted data from B are left. In case that more frames are taken into account, the number of possible weights become larger and thus the true importance of each frame emerges naturally by meaning the weights.

The rationale of this approach is to weigh each solution 
                              
                                 w
                                 ∈
                                 W
                              
                            based on the similarity with respect to the other; this leads the most similar one to be chosen as the most representative, while the others participate less in the final solution. A way of deciding the weights is using the characteristic vector provided by the Dominant Set [45] approach. A graph G = (V,E,ω) is created in which 
                              
                                 V
                                 =
                                 W
                                 ,
                              
                            the set of weights, and the similarity between pairs of weights i, j is 
                              
                                 ω
                                 
                                    (
                                    
                                       w
                                       i
                                    
                                    ,
                                    
                                       w
                                       j
                                    
                                    )
                                 
                                 =
                                 
                                    e
                                    
                                       
                                          −
                                          ∥
                                       
                                       
                                          w
                                          i
                                       
                                       −
                                       
                                          w
                                          j
                                       
                                       
                                          
                                             ∥
                                          
                                          2
                                       
                                    
                                 
                              
                           . The first dominant set is extracted and the corresponding characteristic vector x is used to weigh each w. The final weight is:

                              
                                 (10)
                                 
                                    
                                       
                                          
                                             
                                                
                                                   w
                                                   i
                                                
                                                ^
                                             
                                          
                                          
                                             =
                                          
                                          
                                             
                                                
                                                   ∑
                                                   
                                                      j
                                                      =
                                                      1
                                                   
                                                   n
                                                
                                                
                                                   
                                                      x
                                                      j
                                                   
                                                   *
                                                   
                                                      W
                                                      
                                                         i
                                                         ,
                                                         j
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   w
                                                   i
                                                
                                                ^
                                             
                                          
                                          
                                             =
                                          
                                          
                                             
                                                
                                                   
                                                      w
                                                      ^
                                                   
                                                   i
                                                
                                                
                                                   
                                                      ∑
                                                      
                                                         i
                                                         =
                                                         1
                                                      
                                                      K
                                                   
                                                   
                                                      w
                                                      i
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

The rationale is to select the set of weights which are close to an uniform distribution, in order to prune trivial solutions (the ones having only one matrix with full weight) and keeping the peculiar characteristics of different payoff matrices in which each matrix is represented.

                              
                                 (11)
                                 
                                    
                                       
                                          
                                             
                                                
                                                   w
                                                   ^
                                                
                                                =
                                                
                                                   arg max
                                                   
                                                      w
                                                      ∈
                                                      W
                                                   
                                                
                                                
                                                   (
                                                   
                                                      −
                                                      
                                                         ∑
                                                         i
                                                      
                                                      
                                                         
                                                            w
                                                            i
                                                         
                                                         
                                                            log
                                                            2
                                                         
                                                         
                                                            w
                                                            i
                                                         
                                                      
                                                   
                                                   )
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

The rationale here is that, since each set of weights 
                              
                                 w
                                 ∈
                                 W
                              
                            represents a feasible theoretical solution, we use each set separately to extract the groups of people from an image obtaining n different possible groupings. The final solution is generated via “consensus” between the different groupings from the n solutions. Given a set of groupings, the consensus is found by an evidence accumulation matrix 
                              
                                 E
                                 =
                                 m
                                 ×
                                 m
                                 ,
                              
                            similar to [57], in which m is the maximum number of persons in the scenes and E
                           
                              i, j
                            counts the number of times that persons i and j are grouped together in n different solutions. The E matrix is then divided by the number of solutions n. The final grouping is then obtained through clustering over the matrix E using the Dominant Set approach.

@&#EXPERIMENTS AND RESULTS@&#

We carried out experiments considering both the single- (Section 5.3) and multiple-frame methods (Section 5.4) in ideal and noisy situations. In the former, F-formations are estimated on each single frame independently, while in the latter we perform integration over consecutive frames in order to filter out noisy detections. Moreover, the robustness of the method injecting increasing levels of noise (Fig. 9) has been tested.

The seven datasets used (see Table 2
                        
                        ) are the current publicly available benchmarks for detecting F-formations, where for each individual in a scene its x, y position and the head orientation are provided. Consecutive frames are available for three of them with a low frame rate. In four cases the annotation has been done via automatic tracking while other two were manually annotated by the respective authors as stated in Table 2. The datasets that are used in this work are all in world coordinates, with a top-view camera setting, except for the Idiap Poster data, which used an orthogonal projections of a 3-D bounding box overlayed on people in the scene to determine the position of each person in the image plane.

It consists of an extensions of the FriendsMeet proposed in [58]; this version is composed by the 15 original real sequences, in which additionally the head orientation has been manually annotated in each sequence. This results in 10685 annotated frames, the biggest dataset for group detection available to date. The head orientation annotation has been done in the image plane by pointing to the head of the person and drawing a line in the direction where she/he is looking. Through the available homography has been possible to convert the line from image plane to world coordinates obtaining the real head angle on the ground plane. The ground-truth for the groups is the same as in the original dataset.

This dataset has been recently proposed by Choi et al. [35] and is composed of 599 images of different real environments. Groups are divided into 7 categories based on the internal disposition of the persons: queuing (Q), standing facing each other (SF), sitting facing-each-other (OF), sitting on the ground facing-each-other (GF), standing side by side (SS), sitting side by side (OS), sitting on the ground side by side (GS). For each image, the annotation of the persons position, head orientation, the groups and the corresponding types are provided. The dataset has been not specifically designed for the task of conversational groups detection and, on the basis of the hypothesis that facing is mandatory to converse, we use only the groups of facing people (SF, OF, and GF) as our ground-truth.

It consists of 3 h of indoors video in the large atrium of an hotel building with over 50 people during a scientific meeting involving poster presentations and a coffee break. The cameras were mounted from above pointing downwards to record the scene. The 82 distinct image frames were selected based on maximizing differences between images, ambiguity in group membership and varying levels of crowdedness. 21 trained annotators were split into 7 groups (3 persons each) who annotated 10–11 images for F-formations, leading to a subjective representation of the ground-truth.

The CocktailParty dataset contains 16 min of video recordings of a cocktail party in a 30 m2 lab environment involving 7 subjects. This scenario was recorded using four synchronized angled-view cameras (15 Hz, 1024 × 768px, jpeg) installed in the corners of the room. The dataset is challenging for video analysis due to frequent and persistent occlusions given the highly cluttered scene. Subject’s positions and horizontal head orientations were logged using a particle filter-based body tracker with head pose estimation. Groups were annotated manually by a trained expert every 3 s, resulting in a total of 320 distinct frames for evaluation.

The dataset focuses on a coffee-break scenario of a social event, with max 14 individuals organized in groups of 2–3 people. People positions were estimated by exploiting multi-object tracking of the heads, and head orientation detection has been performed afterward, considering solely 4 possible orientations (front, back, left, right). The tracked positions were projected onto the ground plane. A trained expert annotated the videos indicating the groups present in the scenes (in combination with questionnaires that the subjects filled in about the number of people they spoke with) on two different coffee-break events, for a total of 45 frames for Seq1 and 75 frames for Seq2, acquired every 3 s.

A trained expert synthesized 10 different situations, with F-formation and singletons. Each situation is repeated 10 times, with slightly varying positions and head orientations of the subjects. Here, noise (in position and orientation) is absent.

The scenario consists in a vending machines area where people take coffee and other drinks, and chat. In this case, head orientation considers solely 4 possible alternatives and, since the frame rate is very low, the multiple frame approach cannot be applied.

As comparative approaches, we consider the Hough-based approach of [7] in its renewed version of [22] (HFF), the hierarchical extension of the Hough-based approach of [46] (MULTI), the dominant set-based technique of [6](DS), and the approach of Choi et al. [35]. Comparison with other baselines are not reported in Table 3 since they are already carried out and reported in [7,22].

In terms of evaluation, as in [22], we consider a group as correctly estimated if at least ⌈(T · |G|)⌉ of their members are correctly detected by the algorithm, and if no more than 
                           
                              ⌈
                              (
                              1
                              −
                              T
                              )
                              ·
                              |
                              G
                              |
                              ⌉
                           
                         false subjects are identified, where |G| is the cardinality of the labeled group G, and 
                           
                              T
                              =
                              2
                              /
                              3
                           
                        . The DGPI [35] dataset uses a different criterion to evaluate the performances: a group is correctly detected if at least half of the persons in a detected group matches a group in the ground-truth. In practice, it is the same criterion as above but with T parameter equal to 0.5. Based on this metrics, we compute precision, recall, and F1-score per frame; averaging these values over the frames gives the final scores.

Different combinations of parameters are explored and validated on each dataset. In particular, we examine the performance of our approach when using the similarity function in Eq. (4) with the distance function in Eq. (3) (as suggested by [47]), and by varying the value of σ in the range {0.1, 0.2, 0.4, 0.5, 0.7, 0.9}. To explore the effect of the length of the frustum we based our analysis on the studies conducted in [59,60] in which a focused encounter between two persons may occur between 45 cm to 2 m; correspondingly, the parameter l will range in the same interval.

In this experiment, we apply our method on several publicly available datasets and one new dataset proposed in this paper. The section is subdivided into two parts, the first in which we compare our method with consolidated public benchmarks, and the last one dedicated to the new datasets.


                           Table 3 shows the parameters used and the quantitative results obtained in the single-frame modality, and in Fig. 8
                           
                            qualitative results of our group detector are shown in comparison with the HFF method [7]. As done in the comparative approaches, we show here the performances obtained with the best parameter settings using the Jensen–Shannon (JS) divergence, and averaged over 10 runs to evaluate the stability. As shown, the only cases where our approach does not outperform the state-of-the-art [6] is on the Poster Data, with a difference of 1% in the precision with respect to HHF, and in the recall of the CoffeeBreak with a difference of 1% , a difference which is still below the variance found in the experiments. In the other cases, the results are definitely superior, saturating for example the synthetic benchmark, and outperforming by over 10% the F1-score on the GDet and the CocktailParty. In the IRPM approach, results are considerable low, since the F/formation modeling was one of the first being formalized in computer vision from the sociological literature. In particular, it stated that a group is formed by people whose view frustum intersect, without accounting for occlusions among people. This generated many false positives that, in turns, cause the approach to lose many good groups (in other words, a group is estimated accounting the wrong people, which in turns may have been involved in other formations). It is worth noting that the performances across the different runs of the algorithm have been quite stable, with a maximum variance about 0.6% for both the precision and recall values.

The results on the DGPI dataset are reported in Table 4
                           . The features on this dataset are automatically annotated and for this reason this dataset is represents a very challenging test bed. The comparison with [35] has been made by taking as our ground-truth the union of the ground-truths of the facing persons, and we compared our results with the average on the related ground-truth obtained by [35]. The reported values are calculated using the result from the full model in Table 2 of [35]. If all the seven ground-truth classes are considered as generic groups and used for evaluating our approach, we have very high figures with a precision reaching 0.99, a recall of 0.73, and an F1-score of 0.84. Unfortunately, in [35] only the average over the detections per type of groups is reported (precision 0.50, recall 0.44, and F1-score 0.47), so this comparison is not completely fair but it gives an indication of the goodness of our approach also in these conditions.

The results on the FM2 dataset are reported in Table 5
                           , the parameters used in this dataset are the same for all the sequences, since there is no change of the viewpoint, 
                              
                                 l
                                 =
                                 1.5
                              
                            and 
                              
                                 σ
                                 =
                                 0.5
                              
                           . As one can note, the performance is very low in three sequences (Seq 11–13): this is motivated by the fact that in these sequences there are no conversational groups but persons that are queuing or walking producing a lot of false positives. If we keep only the sequences in which persons are facing and interacting (Seq1–10, 14 and15), we obtain very good performances with a mean precision of 0.886 , recall equal to 0.862 , and F1-score equal to 0.873.

The rationale of this experiment is to compare the overall performances of our pipeline when changing the clustering method in the last step. To this end a more traditional clustering technique, the spectral clustering [49] algorithm,
                              1
                           
                           
                              1
                              Note that since the number of groups in a scene (number of clusters) are not known a-priori, the spectral gap heuristic [49] has been used to find the proper subdivision of the data.
                            has been chosen. In this experiment the last step (the fourth) of the pipeline has been changed, substituting the Game-Theoretic-clustering method with the Spectral Clustering and keeping the remaining steps fixed. The single-frame modality on all the state-of-the-art datasets has been explored since the interest is to prove the validity of our clustering choice rather than other steps of the algorithm. The evaluation criteria are exactly the same as the other experiments and the quantitative results have been reported in Table 3 (see “R-GTCG SC” rows) showing the superiority of the game-theoretic approach.

In this experiment we carried out the analysis in case of noisy unreliable data, faced using the multi-payoff game theory proposed in Section 4.1. This analysis involves the injection of noise in the data, and in particular in the head orientation rather than in the person position; this because the orientation is the most problematic feature to be automatically extracted and so the more noisy. Given a window of K frames, the noise is added to the head orientation on an increasing number frames and persons in the scene, and is expressed in percentage. In particular, the amount of frames and persons affected by noise was set by selecting from these values: 
                           
                              F
                              =
                              {
                              0
                              %
                              ,
                              25
                              %
                              ,
                              50
                              %
                              ,
                              75
                              %
                              }
                              ,
                           
                         where the percentages indicate both the number of corrupted frames (whose time indexes have been sampled uniformly without replacement from the entire sequence) and the number of people affected by the noise. For example, in a sequence with 100 frames and 8 persons, setting a noise of 25% means to have 25 random frames in which the head orientation of 2 (random) individuals is altered by noise. Considering the following size of the window 
                           
                              K
                              =
                              {
                              1
                              ,
                              2
                              ,
                              3
                              ,
                              4
                              ,
                              5
                              ,
                              7
                              ,
                              9
                              }
                           
                         of frames, we explore our approach applying the temporal integration. The JS divergence has been used to generate the similarity matrices because it produces the better results [47] in the single-frame experiments, outperforming the KL divergence in both the datasets.

In Fig. 9, we analyze the performance of the heuristics proposed in Section 4.1 under the highest level of noise in the head orientation 
                              
                                 γ
                                 =
                                 
                                    2
                                    3
                                 
                                 π
                                 ,
                              
                            and on an increasing number of corrupted frames and persons.

From the Fig. 9, we can draw the following conclusions for each heuristic:

                              
                                 
                                    Fig. 9a.
                                 
                                 The normalized sum of weight is the best performing heuristic over the other methods, in particular when a small number of frames is considered. This is because when few frames (2 or 3) are taken into account, the number of possible solutions is more or less in the same order, if one keeps at random one of these solution it may choose exactly the corrupted frame so averaging over the possible weights makes an uniform distribution, eventually avoiding or less weighting, the erroneous frames.

The weighted solution using the characteristic vector is an intriguing alternative to the normalized sum. The first dominant set extracted in fact captures the peculiarity of the entire graph assigning a high score to the most similar node. This means that the set of weights which share the larger similarity with respect to the other possible solutions, will have a higher score. Statistically, the noisy part should be less than the entire set, hence the set of weights with the highest score will have a good chance to be the right one.

The use of the maximal entropy is motivated by the fact that when the entropy is maximized a uniform distribution is obtained. In our case, we have an equal weight distribution only in the case when all the frames are exactly the same, but in most of the cases this does not happen. For this reason, searching for the maximal entropy means to find the set of weights giving a chance to all frames to participate in the final grouping process without suppressing the ability of the optimizer in assigning more weight to the most diverse matrices.

The performance of the ensemble of clusters starts from very low results that rapidly grow as the number of frames increases. This is motivated by the fact that when few frames are taken into account (2...4) the number of weights that the algorithm in Section 4.1 finds is very few. This leads to very different clustering results in which it is difficult to find a consensus. When the number of frames increases, so do the weights, and thus finding a consensus becomes an easier task. For example, with 9 frames the average number of possible weights are more than 150, this means that we will have more than 150 clustering results in which finding a consensus is quite an easy task.

As one can note, in general, there is a performance drop in correspondence of 
                              
                                 K
                                 =
                                 5
                              
                           : this is quite obvious because after 5 frames the changes in the scene starts being consistent. This occurs in particular in Seq 1 of the CocktailParty dataset, because in that sequence the groups change frequently. To achieve a good smoothing is thus suggested to use no more than 5 consecutive frames.

Compared with the single-frame approach, in a noiseless tracking situation (blue curve), this version gives comparable results. As shown in the Fig. 9, the temporal integration varies almost uniformly except a slight increase in the Seq1 of the CoffeeBreak dataset. In the case of noise (green,red and cyan curves) the single frame (first point on the curves) provides a low F-score and is in general completely dominated by the multi-frame version, irrespective of the number of frames considered in the buffer.

For the sake of curiosity we carried out a last experiment with the aim of establishing if the existing relationship between speed and visual field of view, the so-called tunneling effect affecting the visual field of view on high speed moving, could be confirmed in scenarios in which the velocity of persons are not as high as in cars. To this end, we set the following experiment: for the three datasets in the multi-frame experiment we changed the length and the aperture of the frustum (l and θ parameter) and we correlate them with the speed typical of the dataset used. We end up that there were no relationship at all.

@&#DISCUSSION@&#

After this empirical evidence we can provide an overall final analysis. The proposed approach is to be preferred over the others under a wide variety of different scenarios. In general, the performance is incredibly stable under both noisy (real) and ideal (synthetic) dataset. For example, we have the highest performance in the CoffeeBreak even if it is a very noisy dataset in terms of head orientation since only 4 orientations are possible. From the single-frame experiments, it is clear that the JS measure produces the highest and more stable performance. This seems to suggest that, while modeling a pairwise social interaction, it is reasonable to assume that both the individuals want to maintain a connection with the same strength, implying a symmetric affinity. Moreover, the comparison between the noisy multi- and the single-frame results reveals the meaningfulness of considering consecutive instants of the same scene to smooth out the noise effect. From the computational point of view, the multi-frame approach is to be preferred in case of noisy measures, using a window of no more than 5 frames. If more frames are considered, the found solutions could be inconsistent. If near real-time detection is required, the single-frame approach is to be preferred over the multiple frame, because it is able to perform group detection at 15/20 fps. Summarizing, the more significant processing modules that absolutely contributed the most in this work and that represent the main novelty, are the biologically inspired model of the frustum, which capture far better the sociological interaction between individuals with respect to the previous approaches, and the game-theoretic temporal integration which provides a principled way to efficiently prune noise by smoothing data across multiple frames.

As very final note, we analyze the timing requirements for the two approaches, the single- and multi-frame modalities. In the former case, the time requirement is very low, reaching the 15/20 fps (given the detections). Things get worse in the latter case, since the fastest heuristics represented by the weighted summation and the maximal entropy weights (Sections 4.1.1 and 4.1.3) reaches peaks of 2–5 fps (depending by the number of frames). This is quite obvious due to the fact that after having found the set of weights, the final re-weighting is based on a simple sum or sum of logarithms. In second position, we find the weighted solutions (Section 4.1.2), in which a Dominant Set extraction is performed over the set of weights to find the most representative ones. In the last position, we have the clustering ensemble (Section 4.1.4) in which we perform a separate clustering on each set of weights and the final solution is found again by clustering over the evidence accumulator matrix.

The code has been written in (non optimized) Matlab on a Core i7-3720QM2.60GHz with 8GB of RAM.

@&#CONCLUSIONS@&#

In this paper, we have proposed a method for detecting conversational groups (F-Formations) that can be included in a typical surveillance pipeline or on top of a person detector. The approach improves upon existing methods by building a stochastic model of social attention which captures pairwise scores between people, indicating their joint tendency in aggregating in a group. Pairwise scores fill an affinity matrix which encodes an edge weighted graph representing the entire scene under analysis. On this structure, a game-theoretic clustering strategy efficiently finds the groups. In addition, this game-theoretic perspective has allowed us to integrate in a principled way information coming from multiple consecutive frames in videos, in an attempt to deal with noisy situations resulting from the scene complexity (e.g., a crowded high density scenario) and the inaccuracy of the detection and orientation estimation algorithms. Our extensive experimental session on single-frame situation has shown a dramatic improvement over other methods in the literature on five different datasets, and competitive performances on other two benchmarks. Adding the integration with multiple-frames, where applicable, has allowed to augment the overall group detection accuracy, especially in the case of strong noise altering person positions and the related head orientations. In the future, we plan to address the problem of modeling F-formations by considering the instability points, that is, when a group is forming or disaggregating, with the challenge of guessing as soon as possible when a person will join or leave a group.

@&#ACKNOWLEDGMENTS@&#

Authors want to acknowledge Wongun Choi and Yu-Wei Chao to provides us the code, the data of their paper and the valuable support. Hayley Hung has been partially supported by the European Commission under contract number FP7-ICT-600877 (SPENCER) and is affiliated with the Delft Data Science consortium.

@&#REFERENCES@&#

