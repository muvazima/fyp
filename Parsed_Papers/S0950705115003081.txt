@&#MAIN-TITLE@&#Application of entropies for automated diagnosis of epilepsy using EEG signals: A review

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Epilepsy can be detected using EEG signals.


                        
                        
                           
                           The entropy indicates the complexity of the EEG signal.


                        
                        
                           
                           Various entropies are used to diagnose epilepsy.


                        
                        
                           
                           Unique ranges for various entropies are proposed.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Epilepsy

Interictal

EEG

Entropy

Fuzzy

HOS

@&#ABSTRACT@&#


               
               
                  Epilepsy is the neurological disorder of the brain which is difficult to diagnose visually using Electroencephalogram (EEG) signals. Hence, an automated detection of epilepsy using EEG signals will be a useful tool in medical field. The automation of epilepsy detection using signal processing techniques such as wavelet transform and entropies may optimise the performance of the system. Many algorithms have been developed to diagnose the presence of seizure in the EEG signals. The entropy is a nonlinear parameter that reflects the complexity of the EEG signal. Many entropies have been used to differentiate normal, interictal and ictal EEG signals. This paper discusses various entropies used for an automated diagnosis of epilepsy using EEG signals. We have presented unique ranges for various entropies used to differentiate normal, interictal, and ictal EEG signals and also ranked them depending on the ability to discrimination ability of three classes. These entropies can be used to classify the different stages of epilepsy and can also be used for other biomedical applications.
               
            

@&#INTRODUCTION@&#

Epilepsy is a common neurological disorder known to have affected about 3 million people in USA and an estimated 50 million people worldwide [103,104]. The incidence and prevalence of epilepsy is about 1–2% of the world’s population [67] and increases drastically in elderly population. It is estimated that adults aged 65years and above are affected by this condition reaching more than 570,000 cases [104]. Epilepsy is characterised by unexpected repeated and brief disruption of perception or behaviour arising from excessive synchronisation of cortical neural network. Epileptic person encounters constantly recurring abnormal outbreaks of electrical discharges in the brain. The marker of epilepsy is recurrent unprovoked seizures known as epileptic seizures [14]. According to World Health Organisation (WHO), epilepsy is marked by repeated seizures, which are physical responses to unexpected, usually short, uncontrolled electrical discharges in a group of brain cells [104]. Based on clinical manifestations or displays, epileptic seizures are grouped into partial or focal, generalised, unilateral and unclassified seizures [44,98,99]. Only a part of cerebral hemisphere is involved in focal epileptic seizures whereas the whole brain is involved for generalised epileptic seizures. Subjects of any age group can be affected by epileptic seizures.

Clinically to predict and diagnose epileptic seizures, the brain activities are to be monitored through EEG signals which contain the markers of epilepsy. EEG signals of epileptic patients exhibit two states of abnormal activities namely interictal or seizure free (in-between epileptic seizures) and ictal (in the course of an epileptic seizure). The interictal EEG signals are transient waveforms and exhibit spikes, sharp or spiky waves. The ictal EEG signals are continuous waveforms with spikes and sharp wave complexes [62]. Generally, a clinician relies on identifying interictal (seizures free) EEG signals for epilepsy prediction as the ictal segments are obtained rarely. Thus, longer durations of EEG signals are necessary to visually monitor and analyse in order to localise the normal, interictal and ictal episodes for a patient. Epilepsy can be detected by traditional methods by well-trained and experienced neurophysiologists by visual inspection of long durations of EEG signals. This is time – consuming, tedious and subjective. Hence, in order to overcome these limitations, a computer – aided detection of epileptic EEG signals can be used.

Selecting features that best describe the behaviour of EEG signals are important for automated seizure detection performance. Many time-domain [63,64], frequency-domain [26,38,93,1,77,21], time–frequency analysis [69], energy distribution in the time–frequency plane [35,98], wavelet features [38,2], and chaotic features such as entropies [106,64] are used for seizure detection. In many works, combination of two or more features and more than one classifier [66,100] are used. Various entropy based measures have been used to quantify and better understand EEG complexities which differ in-between and during seizures episodes [20]. Literatures show that the entropy has the ability to identify the complexity present in EEG signal using computerised techniques. Acharya et al. [5–7] have used different entropy features (Approximate entropy, Sample entropy, Phase entropy, Shannon entropy, Renyi’s entropy and HOS entropies) for the diagnosis of epilepsy using EEG signals. Sharma et al. [87] have classified the focal and non-focal epilepsy EEG signals using Shannon, Renyi, approximate and sample entropies extracted from intrinsic mode functions extracted from the EEG signals with an accuracy of 87%. However, to best of our knowledge, this is the first paper describing the large set of entropies used for the diagnosis of epilepsy using EEG signals. Therefore, in this review paper, we are proposing unique ranges for various entropies namely Approximate Entropy (ApEn), Fuzzy Entropy (FE), Sample Entropy (SampEn), Renyi’s Entropy (RE), Spectral Entropy (SEN), Permutation Entropy (PE), Wavelet Entropy (WE), Tsallis entropy (TE), Higher Order Spectra Entropies (S1, S2, PhEn), Kolmogorov–Sinai entropy (KSE) and Recurrence Quantification Analysis entropy (RQA En) applied on normal, interictal and ictal EEG signals.

The block diagram of proposed automated epilepsy detection is given in Fig. 1
                     . The EEG signals are divided into training dataset and testing dataset for building and evaluating the performance of a classifier. An offline training system describes the steps followed in building a classifier for the classification of EEG signals. After EEG signal pre-processing, different entropy algorithms are used for significant feature extraction from the EEG signals. Feature ranking helps to rank the features based on their clinical significance and also enhances the performance of the classifier. Significant features and ground truth of whether the EEG signals are normal, ictal or interictal as concluded by physicians are used to train the classifiers. In an online system, characteristic features are extracted from unclassified test samples and all the classifier parameters are applied to resolve its class label. Classifier performance is evaluated in terms of its accuracy, sensitivity, specificity and positive predictive value (PPV).

The EEG recordings used to test the efficiency of different entropies in epileptic seizure detection were collected from open database available at the University of Bonn [11]. All the EEG data taken were free of any artefacts. EEG data can be divided into five sub-groups. The first two sub-groups contain EEG signals obtained from normal subjects with eyes-closed and eyes-open respectively. The next two sub-groups define interictal states and the fifth sub-group contains seizure patients. Thus, two class problem deals with (i) normal and seizure patients and (ii) interictal and ictal patients. Three class problem is normal, interictal and seizure patients. Five class problem deals with normal with eyes-closed, normal with eyes-open, interictal (two classes) and seizure patients.

The data includes five sets of EEG segments (Z, O, N, F, and S) from five healthy subjects recorded during the awake condition with eyes closed (Set Z) and eyes open (Set O) and epileptic subjects undergoing pre-surgical evaluations (having both segments interictal and ictal) each having length of 23.6s. The depth electrodes implanted were used to record the interictal segments during seizure free intervals or in-between seizures (i) from the hippocampal formation of the opposite hemisphere (Set N) and (ii) from the epileptogenic zone (Set F). Both the depth electrode and strip electrode implanted into the neocortex were used to record ictal segments (Set E). Total of 100 files were selected from each normal and epileptic (interictal=100, ictal=100 files) subjects. Each EEG file of 4096 samples was considered as a separate EEG signal resulting in a total of 300 EEGs. In this study, datasets O, F and S are used for further analysis. Fig. 2
                         shows the three typical EEG signals.

In developing an automated epilepsy detection, various features which better describe the behaviour (either static or dynamical) of seizures [97] are to be extracted. In this review paper, entropy features based on various methods are evaluated to differentiate EEG signals into normal, interictal and ictal classes. Brief descriptions of entropies used in EEG signal classification are provided in this section.

Entropy is a measure of rate of information generation which can be used in the signal processing to separate the useful signal from an intrusive noise [54]. Usually a high entropy value corresponds to increased irregularity or unpredictability while a low value corresponds to high regularity. Entropy is a nonlinear index that reflects the degree of chaos within a system [54]. It is often used to analyse epileptic EEG signals to detect whether there is an epileptic attack or to inspect the state of epileptic seizures. The entropy parameter is broadly categorised into spectral and embedding entropies. The entropies computed from amplitude components of the power spectrum of the signal are referred as spectral entropies whereas entropies calculated using the time series directly are defined as the embedding entropies [45,46].

It evaluates the uncertainty of the signal with respect to time [32] and is computed from the embedded time series signal [47]. It is also defined as metric entropy which is zero for non-chaotic signals and is greater than zero for chaotic signals. Entropy is calculated by finding points that are closer to each other on the trajectory in a phase space and are uncorrelated with time. Rate of divergence of these point pairs produces the KSE value. Let tdiv
                            be the average time taken for the point pairs to move apart. Thus, KSE can be calculated from the following equation
                              
                                 (1)
                                 
                                    
                                       〈
                                       
                                          
                                             t
                                          
                                          
                                             div
                                          
                                       
                                       〉
                                       =
                                       
                                          
                                             2
                                          
                                          
                                             -
                                             KSt
                                          
                                       
                                    
                                 
                              
                           where KSE is expressed in bits per second. The reconstruction of a system’s trajectory from an embedding space leads to the computation of KSE as depicted in Eq. (2).
                              
                                 (2)
                                 
                                    
                                       KSE
                                       =
                                       
                                          
                                             
                                                lim
                                             
                                             
                                                r
                                                →
                                                0
                                             
                                          
                                       
                                       
                                       
                                          
                                             
                                                lim
                                             
                                             
                                                m
                                                →
                                                ∞
                                             
                                          
                                       
                                       
                                       
                                          
                                             1
                                          
                                          
                                             τ
                                          
                                       
                                       
                                          
                                             
                                                
                                                   C
                                                
                                                
                                                   m
                                                
                                             
                                             (
                                             r
                                             ,
                                             
                                                
                                                   N
                                                
                                                
                                                   m
                                                
                                             
                                             )
                                          
                                          
                                             
                                                
                                                   C
                                                
                                                
                                                   m
                                                   +
                                                   1
                                                
                                             
                                             (
                                             r
                                             ,
                                             
                                                
                                                   N
                                                
                                                
                                                   m
                                                   +
                                                   1
                                                
                                             
                                             )
                                          
                                       
                                    
                                 
                              
                           where 
                              
                                 
                                    
                                       C
                                    
                                    
                                       m
                                    
                                 
                                 (
                                 r
                                 ,
                                 
                                    
                                       N
                                    
                                    
                                       m
                                    
                                 
                                 )
                              
                            is called as a correlation function that gives the probability of two points being closer to each other than 
                              
                                 r
                              
                           . Higher KSE value indicates higher unpredictability. Thus, KSE does not provide accurate results for the signals with slightest noise.

The main advantage of this entropy is that it is useful in differentiating the regular and chaotic systems [65,31]. It underestimates and decays towards zero as the length of a given time series becomes large, which is the main limitation of this entropy [27].

It is used to evaluate the instability of variation in the signal [75,76]. It detects changes in the underlying episodic behaviour and compares the similarity of the samples by pattern length (m) and similarity coefficient (r). ApEn is a scale invariant measure because the similarity criterion is equivalent to the standard deviation of the data. Highly irregular time series gives a high ApEn value and a time series with more number of similar patterns gives a low ApEn value. Mathematically, ApEn is calculated by
                              
                                 (3)
                                 
                                    
                                       ApEn
                                       =
                                       ln
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            C
                                                         
                                                         
                                                            m
                                                         
                                                      
                                                      (
                                                      r
                                                      )
                                                   
                                                   
                                                      
                                                         
                                                            C
                                                         
                                                         
                                                            m
                                                            +
                                                            1
                                                         
                                                      
                                                      (
                                                      r
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where 
                              
                                 
                                    
                                       C
                                    
                                    
                                       m
                                    
                                 
                                 (
                                 r
                                 )
                              
                            is the pattern mean of length 
                              
                                 m
                              
                            and 
                              
                                 
                                    
                                       C
                                    
                                    
                                       m
                                       +
                                       1
                                    
                                 
                                 (
                                 r
                                 )
                              
                            is the pattern mean of length 
                              
                                 m
                                 +
                                 1
                              
                           . The pattern mean is calculated by computing the count of similar patterns of length, 
                              
                                 m
                              
                            and length, 
                              
                                 m
                                 +
                                 1
                              
                           . In this work we have chosen m
                           =2 and r
                           =0.2 times the standard deviation of the EEG signal [95,75]. Let 
                              
                                 
                                    
                                       C
                                    
                                    
                                       im
                                    
                                 
                                 (
                                 r
                                 )
                              
                            indicate the count of patterns of length 
                              
                                 m
                              
                            and is defined by the following equation.
                              
                                 (4)
                                 
                                    
                                       
                                          
                                             C
                                          
                                          
                                             im
                                          
                                       
                                       (
                                       r
                                       )
                                       =
                                       
                                          
                                             
                                                
                                                   n
                                                
                                                
                                                   im
                                                
                                             
                                             (
                                             r
                                             )
                                          
                                          
                                             N
                                             -
                                             m
                                             +
                                             1
                                          
                                       
                                    
                                 
                              
                           ApEn depends on pattern length, which is inconsistent and counts the sequence that matches itself to avoid the condition 
                              
                                 ln
                                 (
                                 0
                                 )
                              
                           . The advantages of this entropy are: (i) it can be calculated for a relatively short series of noisy data [85], (ii) it can potentially differentiate a variety of systems such as periodic and multiple periodic systems, chaotic systems, and stochastic systems [75], and (iii) it can give statistically accurate results compared to the KS entropy [75].

The drawbacks of approximate entropy are: (i) it is heavily dependent on the input signal length. Especially short signals lead to a lower value than expected [80], (ii) significant noise compromises meaningful interpretation of this entropy [85], (iii) this entropy is a biassed statistic as it depends on length of the time series, and counts self-matches [76], and (iv) consistency is absent and is largely dependent on length of the data.

It measures the regularity of a physiological signal and is independent of the pattern length. If SampEn value of one dataset is higher than the other for a given pattern length (m) and similarity criterion (r) then it remains higher for all the different values of m and r. Thus, SampEn is relatively consistent and reduces the bias of approximate entropy [80]. High value of SampEn implies that the signal is highly unpredictable and a low SampEn value implies the signal is predictable. SampEn can be computed by
                              
                                 (5)
                                 
                                    
                                       SampEn
                                       =
                                       -
                                       log
                                       
                                          
                                             
                                                
                                                   
                                                      A
                                                   
                                                   
                                                      B
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where A contains the total number of vector pairs of length 
                              
                                 m
                                 +
                                 1
                              
                            and B contains total number of vector pairs of length 
                              
                                 m
                              
                           . The value of m is taken as 2 in this work [89]. The advantages of this entropy are: (i) it can be used for shorter series of noisy data, (ii) it is able to differentiate large variety of systems, (iii) it performs much better than the approximate entropy with theory for random numbers [80], (iv) it maintains relative consistency [80], (v) self-matches are not counted, which reduces the bias, and (vi) the entropy values are more consistent across different pattern lengths. However, the main drawback of this entropy is its inconsistency for the sparse data [80].

Spectral Entropy, a normalised form of Shannon’s entropy, which uses the power spectrum amplitude components of the time series for entropy evaluation [86,34]. It quantifies the spectral complexity of the EEG signal. SEN is obtained by multiplying the power in each frequency 
                              
                                 
                                    
                                       p
                                    
                                    
                                       f
                                    
                                 
                              
                            by the logarithm of the same power, and the product is multiplied by −1. The SEN is given by
                              
                                 (6)
                                 
                                    
                                       SEN
                                       =
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                f
                                             
                                          
                                       
                                       
                                          
                                             p
                                          
                                          
                                             f
                                          
                                       
                                       log
                                       
                                          
                                             
                                                
                                                   
                                                      1
                                                   
                                                   
                                                      
                                                         
                                                            p
                                                         
                                                         
                                                            f
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

Shannon’s Entropy (ShEn) is the measure of set of relational parameters that vary linearly with the logarithm of the number of possibilities [54]. It is also a measure of data spread and is most commonly used to assess the dynamical order of a system [86].

The main advantage of ShEN is that, it is better adapted to normal distributions [94]. However, few drawbacks of this entropy are: (i) the possibility of losing more information due to aggregation, (ii) the possibility of over-estimation of entropy level if too many zones are used [41], and (iii) this method fails to explain temporal relationships between different values extracted from a time series signal [33].

It is a generalised form of ShEn and is used for estimating the spectral complexity of a time series signal [79]. Mathematically, REN can be calculated by
                              
                                 (7)
                                 
                                    
                                       REN
                                       (
                                       α
                                       )
                                       =
                                       -
                                       
                                          
                                             α
                                          
                                          
                                             1
                                             -
                                             α
                                          
                                       
                                       
                                          ∑
                                       
                                       
                                          
                                             logp
                                          
                                          
                                             i
                                          
                                          
                                             α
                                          
                                       
                                    
                                 
                              
                           where the order, 
                              
                                 α
                                 ≠
                                 1
                              
                           , 
                              
                                 
                                    
                                       p
                                    
                                    
                                       i
                                    
                                 
                              
                            defines the total spectral power. REN with order, 
                              
                                 α
                                 ⩾
                                 2
                              
                            provides a lower bound for its smooth entropy as REN with order 
                              
                                 α
                                 =
                                 1
                              
                            is analogous to ShEN and it produces superficial amount of smooth entropy [18]. In this work, we have chosen 
                              
                                 α
                                 =
                                 2
                              
                            for REN.

The significant advantages of REN are: (i) at the rescaling of the variables, this entropy changes by an additive constant, and (ii) it remains unchanged for the different density functions. In other words, it do not vary irrespective of the density functions used [91]. The main drawback is that it is not a sub-additive, recursive, nor it possess the branching and sum properties [8].

It is the complexity estimation of time series by identifying the couplings between the time series signal [13,19,101]. The dynamics associated with the EEG signal can be derived by assessing the presence and absence of permutation patterns of varying elements in the given time series signal. At high frequencies, permutation entropy elevates with asymmetry of the time series while at low frequencies, the permutations corresponding to peaks and troughs observed are seldom. PE is given by
                              
                                 (8)
                                 
                                    
                                       PE
                                       =
                                       -
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                j
                                                =
                                                1
                                             
                                             
                                                n
                                             
                                          
                                       
                                       
                                          
                                             p
                                          
                                          
                                             j
                                          
                                       
                                       
                                          
                                             log
                                          
                                          
                                             2
                                          
                                       
                                       
                                          
                                             p
                                          
                                          
                                             j
                                          
                                       
                                    
                                 
                              
                           where 
                              
                                 
                                    
                                       p
                                    
                                    
                                       j
                                    
                                 
                              
                            represents relative frequencies of the possible sequence patterns, n implies permutation order of 
                              
                                 n
                                 ⩾
                                 2
                              
                           . In this work, we have chosen the embedding dimension of 3 and delay of 1 [68,13,81].

Permutation entropy is a measure of chaotic and non-stationary time series signal in the presence of dynamical noise. This method is robust, efficient and produces fast results irrespective of noisy data. Thus, it can be used for processing of huge data sets without pre-processing of data and fine-tuning of complexity parameters [13].

The advantages of this entropy are: (i) it is simple, robust and less prone to computational complexity [55], (ii) it is applicable to real and noisy data, (iii) it does not require any model assumption and is suitable for the analysis of nonlinear processes [108], and (iv) it is useful to analyze huge data sets and requires less pre-processing time and fine-tuning of parameters [108]. The main limitation is its inability to distinguish well defined patterns of a particular design [30].

It helps in characterising the physical behaviour of a system. It describes systems with long-term memory effects, long-range interactions and multifractal space–time constraints [15]. Tsalli’s coefficients help in differentiating EEG spikes, bursts and continuous rhythms. Mathematically, TE is defined as
                              
                                 (9)
                                 
                                    
                                       TE
                                       =
                                       
                                          
                                             1
                                             -
                                             
                                                
                                                   ∑
                                                
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                
                                                   W
                                                
                                             
                                             
                                             
                                                
                                                   p
                                                
                                                
                                                   i
                                                
                                                
                                                   q
                                                
                                             
                                          
                                          
                                             q
                                             -
                                             1
                                          
                                       
                                    
                                 
                              
                           where 
                              
                                 q
                              
                            is measure of nonextensivity, 
                              
                                 
                                    
                                       p
                                    
                                    
                                       i
                                    
                                 
                              
                            describes the discrete set of probabilities with 
                              
                                 W
                              
                            microscopic configurations [96]. In this work, we have taken 
                              
                                 q
                                 =
                                 2
                              
                           . TE can be used to measure the abrupt changes and long-term memory effects in a time series signal unlike the ShEn. It is a constructive tool in characterising fractal stochastic processes.

The advantages of this entropy are: (i) this method helps in measuring the uncertainity of generalised systems (ii) it is better suited to deal with non-Gaussian measures [94], and (iii) it can provide more detailed information than the conventional Shannon, especially when used to evaluate different rhythms and burst EEG analysis [20].

The main drawback is the selection of q-index, which is done randomly without any automated method or theory for its computation [82].

It represents the information of randomness and is defined as the entropy of fuzzy set whose elements have different degrees of membership. Membership functions vary in the unit interval of 0 and 1. Fuzzy entropy is defined as
                              
                                 (10)
                                 
                                    
                                       H
                                       [
                                       A
                                       ]
                                       =
                                       
                                          ∫
                                          
                                             -
                                             ∞
                                          
                                          
                                             ∞
                                          
                                       
                                       S
                                       (
                                       
                                          
                                             C
                                          
                                          
                                             r
                                          
                                       
                                       {
                                       A
                                       ⩾
                                       t
                                       }
                                       )
                                       dt
                                    
                                 
                              
                           where A is the fuzzy variable, 
                              
                                 
                                    
                                       C
                                    
                                    
                                       r
                                    
                                 
                              
                            is the credibility measure [50]. Fuzzy entropy measures ambiguous uncertainties from the highly irregular signals whereas ShEn deals with the measure of probabilistic uncertainties. The advantage of this entropy is that, it is insensitive to noise; and is highly sensitive to changes in the information content [74].

The Orthogonal Discrete Wavelet Transform (ODWT) is a quantitative measure in the study of dynamical brain signals. The ODWT defines the relative wavelet entropy as the measure of relative energies in the different frequency bands of the EEG signals. It computes the degree of similarity between different fractions of signal whereas WE is a measure of the degree of disorder associated with the multi-frequency signal response [83]. Wavelet entropy is computed by
                              
                                 (11)
                                 
                                    
                                       WE
                                       =
                                       -
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                i
                                                <
                                                0
                                             
                                          
                                       
                                       
                                          
                                             p
                                          
                                          
                                             i
                                          
                                       
                                       
                                          
                                             lnp
                                          
                                          
                                             i
                                          
                                       
                                    
                                 
                              
                           where 
                              
                                 
                                    
                                       p
                                    
                                    
                                       i
                                    
                                 
                              
                            defines the probability distribution of a time series signal and 
                              
                                 i
                              
                            defines different resolution levels. Wavelet entropy can be used in identifying the underlying episodic behaviour of a signal and it can provide efficient result for a periodic mono-frequency signal [83].

The advantages of this entropy are it can detect the fine changes in a non-stationary signal [83], requires less computational time, noise contributions can be easily eliminated, and performance does not depend on any parameter.

Second order measures such as autocorrelation function or power spectrum provides a partial explanation of a random process. The principles of power spectrum are extended to higher orders called HOS to determine the subtle changes and the shape of a waveform. HOS does not contain additional information of a Gaussian process as it is completely defined by second order statistics. Hence, HOS of a Gaussian process is identically zero [71,70]
                        

HOS are the spectral representations of higher order moments of an irregular signal. It detects the nonlinearity, deviations from Gaussianity and preserves the phase characteristics of a signal [71,72]. Two approaches can be employed in the estimation of HOS. They are (a) Indirect method where the estimate of cumulant function is followed by its Fourier transform and (b) Direct method that deals with segment averaging.

In the direct approach, data is divided into segments with equal number of overlapping blocks. The Fast Fourier transform (FFT) is computed for each block and the product of spectral coefficients results in ‘raw’ HOS. The weighted average of raw HOS gives rise to the third order moment called the bispectrum. It contains two frequencies (
                              
                                 
                                    
                                       f
                                    
                                    
                                       1
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       f
                                    
                                    
                                       2
                                    
                                 
                              
                           ) and is given by
                              
                                 (12)
                                 
                                    
                                       B
                                       (
                                       
                                          
                                             f
                                          
                                          
                                             1
                                          
                                       
                                       ,
                                       
                                          
                                             f
                                          
                                          
                                             2
                                          
                                       
                                       )
                                       =
                                       E
                                       [
                                       X
                                       (
                                       
                                          
                                             f
                                          
                                          
                                             1
                                          
                                       
                                       )
                                       X
                                       (
                                       
                                          
                                             f
                                          
                                          
                                             2
                                          
                                       
                                       )
                                       X
                                       (
                                       
                                          
                                             f
                                          
                                          
                                             1
                                          
                                       
                                       +
                                       
                                          
                                             f
                                          
                                          
                                             2
                                          
                                       
                                       )
                                       ]
                                    
                                 
                              
                           where 
                              
                                 X
                                 (
                                 f
                                 )
                              
                            is the Fourier transform of a signal 
                              
                                 x
                                 (
                                 nT
                                 )
                              
                           . It is a complex quantity as it analyzes the interactions between three frequency components (
                              
                                 
                                    
                                       f
                                    
                                    
                                       1
                                    
                                 
                              
                           , 
                              
                                 
                                    
                                       f
                                    
                                    
                                       2
                                    
                                 
                              
                           , 
                              
                                 
                                    
                                       f
                                    
                                    
                                       1
                                    
                                 
                                 +
                                 
                                    
                                       f
                                    
                                    
                                       2
                                    
                                 
                              
                           ). The frequency 
                              
                                 f
                              
                            can be normalised to lie between 
                              
                                 0
                              
                            and 
                              
                                 1
                              
                            the Nyquist frequency.

Bispectral entropy have proposed [23] normalised bispectral entropies similar to spectral entropies [43],
                              
                                 (13)
                                 
                                    
                                       S
                                       1
                                       =
                                       -
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                n
                                                =
                                                0
                                             
                                             
                                                N
                                                -
                                                1
                                             
                                          
                                       
                                       
                                          
                                             p
                                          
                                          
                                             n
                                          
                                       
                                       log
                                       
                                          
                                             p
                                          
                                          
                                             n
                                          
                                       
                                    
                                 
                              
                           where 
                              
                                 
                                    
                                       p
                                    
                                    
                                       n
                                    
                                 
                                 =
                                 
                                    
                                       |
                                       B
                                       (
                                       
                                          
                                             f
                                          
                                          
                                             1
                                          
                                       
                                       ,
                                       
                                          
                                             f
                                          
                                          
                                             2
                                          
                                       
                                       )
                                       |
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             
                                                
                                                   f
                                                
                                                
                                                   1
                                                
                                             
                                             ,
                                             
                                                
                                                   f
                                                
                                                
                                                   2
                                                
                                             
                                             ∈
                                             Ω
                                          
                                       
                                       |
                                       B
                                       (
                                       
                                          
                                             f
                                          
                                          
                                             1
                                          
                                       
                                       ,
                                       
                                          
                                             f
                                          
                                          
                                             2
                                          
                                       
                                       )
                                       |
                                    
                                 
                              
                           , 
                              
                                 N
                              
                            is an integer.

Bispectral squared entropy,
                              
                                 (14)
                                 
                                    
                                       S
                                       2
                                       =
                                       -
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                n
                                                =
                                                0
                                             
                                             
                                                N
                                                -
                                                1
                                             
                                          
                                       
                                       
                                          
                                             q
                                          
                                          
                                             n
                                          
                                       
                                       log
                                       
                                          
                                             q
                                          
                                          
                                             n
                                          
                                       
                                    
                                 
                              
                           where 
                              
                                 
                                    
                                       q
                                    
                                    
                                       n
                                    
                                 
                                 =
                                 
                                    
                                       |
                                       B
                                       (
                                       
                                          
                                             f
                                          
                                          
                                             1
                                          
                                       
                                       ,
                                       
                                          
                                             f
                                          
                                          
                                             2
                                          
                                       
                                       )
                                       
                                          
                                             |
                                          
                                          
                                             2
                                          
                                       
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             
                                                
                                                   f
                                                
                                                
                                                   1
                                                
                                             
                                             ,
                                             
                                                
                                                   f
                                                
                                                
                                                   2
                                                
                                             
                                             ∈
                                             Ω
                                          
                                       
                                       |
                                       B
                                       (
                                       
                                          
                                             f
                                          
                                          
                                             1
                                          
                                       
                                       ,
                                       
                                          
                                             f
                                          
                                          
                                             2
                                          
                                       
                                       )
                                       
                                          
                                             |
                                          
                                          
                                             2
                                          
                                       
                                    
                                 
                              
                           , 
                              
                                 N
                              
                            is an integer.

Phase entropy (PhEn),
                              
                                 (15)
                                 
                                    
                                       PhEn
                                       =
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                n
                                                =
                                                0
                                             
                                             
                                                N
                                                -
                                                1
                                             
                                          
                                       
                                       p
                                       (
                                       
                                          
                                             y
                                          
                                          
                                             n
                                          
                                       
                                       )
                                       log
                                       p
                                       (
                                       
                                          
                                             y
                                          
                                          
                                             n
                                          
                                       
                                       )
                                    
                                 
                              
                           where 
                              
                                 p
                                 (
                                 
                                    
                                       y
                                    
                                    
                                       n
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       L
                                    
                                 
                                 
                                    
                                       ∑
                                    
                                    
                                       
                                          
                                             f
                                          
                                          
                                             1
                                          
                                       
                                       ,
                                       
                                          
                                             f
                                          
                                          
                                             2
                                          
                                       
                                       ∈
                                       Ω
                                    
                                 
                                 l
                                 (
                                 ϕ
                                 (
                                 B
                                 (
                                 
                                    
                                       f
                                    
                                    
                                       1
                                    
                                 
                                 ,
                                 
                                    
                                       f
                                    
                                    
                                       2
                                    
                                 
                                 )
                                 )
                                 ∈
                                 
                                    
                                       y
                                    
                                    
                                       n
                                    
                                 
                                 )
                              
                           ,
                              
                                 
                                    
                                       
                                          
                                             y
                                          
                                          
                                             n
                                          
                                       
                                       =
                                       
                                          
                                             
                                                φ
                                                |
                                                -
                                                π
                                                +
                                                
                                                   
                                                      2
                                                      π
                                                      n
                                                   
                                                   
                                                      N
                                                   
                                                
                                                ⩽
                                                φ
                                                <
                                                -
                                                π
                                                +
                                                
                                                   
                                                      2
                                                      π
                                                      (
                                                      n
                                                      +
                                                      1
                                                      )
                                                   
                                                   
                                                      N
                                                   
                                                
                                             
                                          
                                       
                                       ,
                                       
                                       n
                                       =
                                       0
                                       ,
                                       1
                                       ,
                                       …
                                       N
                                       -
                                       1
                                    
                                 
                              
                           where 
                              
                                 N
                              
                            = an integer, 
                              
                                 L
                              
                            = number of points within Ω, 
                              
                                 φ
                              
                            = the phase angle of the bispectrum, 
                              
                                 l
                              
                            = indicator function (
                              
                                 l
                                 =
                                 1
                              
                            when phase angle is within the 
                              
                                 
                                    
                                       y
                                    
                                    
                                       n
                                    
                                 
                              
                            range).

Some of the advantages of HOS entropies are (i) it is a third order statistical analysis and a nonlinear method with high signal to noise ratio (SNR) [71,72,70], (ii) it can capture interaction among its frequency components and phase coupling [72], and (iii) it accurately captures the subtle variation in the bio-signals [24,25].

It is a measure of the average information contained in the line-segment distribution. It is one of the Recurrence Quantification Analysis (RQA) method based on state-space trajectories. It helps in the quantification of number and duration of recurrences of a dynamical system [109]. RQA measures the hidden periodicities of a time series signal and is calculated to represent a non-stationary input signal in terms of its complexity and nonlinearity [109].

Recurrence entropy helps in the detection of chaos-chaos transitions, unstable periodic orbits, time delays and extracts useful information from short and non-stationary input data [61].

In this review paper, all the entropy features extracted are subjected to statistical analysis to identify the clinical significance between three classes (normal, interictal and ictal). Features are tested using Analysis Of Variance (ANOVA) statistical test [49]. The test determines F-value and p-value (probability-value) for three groups of features. F-value and p-value are inversely related. Thus, features with high F-value have p-value near to zero and vice versa. Other ranking methods such as Bhattacharyya distance [28,29], Wilcoxon signed rank test [40,107], Receiver Operating Characteristic Curve (ROC) [84], and fuzzy max-relevance and min redundancy (mRMR) 
                     [12] can also be used to rank the features.

Ten-fold cross validation approach can be used to develop the automated system. Around 90% of the entropy features are used for training and around 10% are used for testing the dataset. This is the standard approach that helps to develop robust classifiers and efficient diagnostic system [16].

A total of 13 entropy features such as – Approximate entropy (ApEn), Fuzzy entropy (FE), Sample entropy (SampEn), Renyi’s entropy (REN), Spectral entropy (SEN), Permutation entropy (PE), Wavelet entropy (WE), Tsallis entropy (TE), Phase entropy (PhEn), Bispectral entropy (S1), Bispectral squared entropy (S2), Kolmogorov–Sinai entropy (KSE) and Recurrence Quantification Analysis entropy (RQA En) are extracted from 300 EEGs (100 from each datasets O, F and S). Table 1
                      presents the mean and standard deviation (SD) values of these entropies for three different classes; normal, interictal and ictal.

Various features extracted from normal, interictal and ictal EEG signals provided sufficient discrimination between the three classes (p
                     <0.0001). The good discrimination capability of these features is evaluated by p-value and the ANOVA F value is used to rank all the entropic features. It is reported that the brain activity is more predictable during seizures than non-seizure phase and is reflected by a sudden drop in the entropy values [37,1,48,42]. From the results listed in Table 1, it can be observed that most of the entropy features are best suitable for the classification of epileptic EEG signals. Among these entropies, Renyi’s entropy, SampEn, SEN and PE bags top position with significantly higher discrimination F-values.

The F-values help in characterising the entropy performance and the features with highest F-values are ranked the highest as they provide significant differentiation between the normal, interictal and ictal classes. Thus, it is presumed that summation of various entropies can help in improving the rate of classification.

Various studies are reported for automated classification of EEG signals into two, three, and five epileptic classes. They are briefly described below.

Kannathal et al. [45,46] used various entropy features namely, Kolmogorov–Sinai entropy, approximate entropy, Renyi’s entropy and Shannon’s entropy to differentiate the epilepsy from normal EEG signals. Among four entropy features extracted, Kolmogorov–Sinai entropy outperformed the rest in differentiating the normal and epilepsy EEG signals (p-value<0.05). The study reported a classification accuracy of around 90% using Adaptive Neuro-Fuzzy Inference System (ANFIS) classifier. Polat et al. [77] developed epilepsy detection algorithm by combining Fast Fourier Transform (FFT) feature extraction technique and Decision Tree (DT) classifier. The study algorithm extracted power spectral density features using FFT based Welch method from EEG recordings. Classification accuracy of 98.72% is reported using DT classifier.

Srinivasan et al. [92] applied approximate entropy to develop an automated EEG classification tool for epilepsy detection based on neural network. The algorithm extracted ApEn from the EEG signal and used as input to the Probabilistic Neural Network (PNN) classifier. The experimental results reported an accuracy of 100% in classification of epileptic EEG signals from normal.

Ocak [73] separated normal and epileptic EEG signals using Discrete Wavelet Transform (DWT). Approximate entropy is applied on the DWT coefficients and a classification accuracy of 96% is obtained. Kumar et al. [51] applied DWT to the normal and epileptic EEG signals and used ApEn algorithm for feature extraction. They obtained an accuracy of 100% using feed-forward back propagation neural network classifier. In another twin study by Kumar et al. [52], 100% classification accuracy is reported using DWT, ApEn with Support Vector Machine (SVM) classifier. Liang et al. [57] computed approximate entropy on the spectral coefficients extracted from normal and epileptic EEG signals. SVM with radial basis kernel (RBF) is used to obtain an accuracy of 98.3% using 21 features. Shen et al. [88] used sample entropy with SVM classifier to classify EEG signals of normal and epileptic and marked an accuracy of 99.2%.

Few studies explored permutation entropy features [68], Wavelet entropy features [53,102] and multi-discrete wavelet transforms [10] to obtain a substantial classification accuracy of more than 92%. Liang et al. [57] computed approximate entropy on the spectral coefficients extracted from normal and epileptic EEG signals. SVM coupled with RBF is used and an accuracy of 98.3% is obtained using 21 features. Shen et al. [88] used sample entropy with SVM classifier to classify EEG signals of normal and epileptic and marked an accuracy of 99.2%. Ahammad et al. [9] applied energy, spectral entropy, standard deviation, maximum, minimum and mean-DWT-linear classifier to achieve an accuracy of 98.5% using data from CHB-MIT repository. Gandhi et al. [36] reported 100% accuracy in two-class (normal and epileptic) classification problem using DWT, spectra entropy and energy features coupled with PNN classifier.

Few studies have evaluated the entropy features from EEG signals to characterise two classes namely interictal and ictal EEGs. Yuan et al. [106] presented a new approach to differentiate interictal and ictal EEG signals using nonlinear dynamical features such as scaling exponent computed from Detrended Fluctuation Analysis (DFA), Hurst exponent and ApEn. They used two data sets containing EEG recordings of interictal and ictal. The study trained single hidden layer feedforward neural network using Extreme Learning Machine (ELM) algorithm for the classification of EEG signal features. They reported a classification accuracy of 96.5%. Song et al. [90] also used optimised sample entropy algorithm combined with ELM for differentiation of interictal and ictal EEG signals and reported an accuracy of 99%. Recently Zhang et al. [110] proposed automatic identification of interictal and ictal EEG signals using ApEn. The study collected EEG recordings (interictal and ictal stage) of four patients having partial epileptic seizures. The ApEn features evaluated from these four patients signals are subjected to SVM for automated differentiation. Their study reported an accuracy of 93.33% in identifying interictal and ictal states. Xiang et al. [105] proposed an algorithm using fuzzy entropy (FE) to inspect the seizures free and seizures states from EEG signals. The study used two datasets to carry out two classification experiments (i) interictal and ictal based on the N and S datasets (EEG signals recorded from the hippocampal formation of the opposite hemisphere and seizure activity) and (ii) interictal and ictal based on the F and S datasets (EEG signals recorded from the epileptogenic zone and seizure activity). The algorithm computed FE features at different epileptic states of EEG signals which are further subjected to SVM classifier. Their study demonstrated an accuracy of 100% accuracy, sensitivity of 100%, and specificity of 100% in classifying interictal and ictal EEG signals (N and S datasets). Also obtained an accuracy of 88.5%, sensitivity of 90.36%, and specificity of 87.63% in classifying interictal and ictal EEG signals (F and S datasets). Bruzzo et al. [17] used permutation entropy (PE) to extract significant features from interictal and epileptic EEG signals. They observed Area Under Curve (AUC) to be 0.85 using Receiver Operating Curve (ROC) plot. Aarabi et al. [1] studied entropy features using interictal and epileptic data collected from Freiburg Seizure Prediction EEG database. They marked a classification accuracy of 98.7% using frequency and time analysis-fuzzy rule based classification. Li et al. [56] used PE features for the classification of interictal and epileptic data and observed that entropy value gradually decreased from seizure-free to seizure-phase.

Few researchers developed the state-of-the-art methods to characterise the EEG signals into normal, interictal and ictal classes for better diagnosis, prediction and treatment. Recently Acharya et al. [7] applied Continuous Wavelet Transforms (CWT) to the EEG signals. Higher Order Spectra (HOS) and texture features are extracted from the CWT plot and characteristic features are fed to four different classifiers; DT, PNN, SVM and k-nearest neighbour (kNN) classifier. They observed that SVM classifier coupled with RBF yielded the highest classification accuracy of 96%, sensitivity of 96.9% and specificity of 97%. They concluded that their technique could be used as an automatic seizure monitoring software. Martis et al. [60] explored a novel nonlinear method with time–frequency representation called Intrinsic Time-scale Decomposition (ITD). ITD representation is used to identify the subtle variations in the EEG signals and significant energy, Fractal Dimension (FD), SampEn features are calculated from ITD. These features are fed to DT classifier and a classification accuracy, specificity, sensitivity of 95.6%, 99%, 99.5% is obtained respectively. Kumar et al. [52] used fuzzy approximate entropy to evaluate the complexity of EEG signals recorded from five healthy (both with eyes open and closed) and epileptic subjects. In their study the EEG are decomposed into different frequency sub-bands signals using DWT and computed fuzzy approximate entropy from these sub-bands. The study reported that the fuzzy approximate entropy features are significant in the classification of two classes (interictal and ictal EEG signals) and also three classes (normal, interictal and ictal EEG signals). Using SVM classifier, they reported an accuracy of 100% for normal versus ictal signals; 99.3% for normal (eyes open) data, and 99.65% for normal (eyes closed) data versus ictal signals. In addition accuracies of 99.6% (SVM-RBF) and 99.85% (SVM-linear) are reported for classification of interictal and ictal EEG signals.

Chua et al. [24] obtained an accuracy of 93% in classifying normal, interictal and ictal EEG signals using bispectrum entropies and phase entropies with Gaussian Mixture Model (GMM) classifier. In another consecutive study by Chua et al. [25], features from HOS and power spectrum were extracted to differentiate between normal, interictal and ictal EEG signals. Classification accuracy obtained using HOS features and power spectrum features are compared. It is observed that HOS features yielded higher accuracy of 93.1% than spectrum features yielding an accuracy of 88.7% using GMM classifier. Acharya et al. [5] reported an accuracy of 98.1% in the classification of normal, interictal and ictal EEG signals using ApEn, SampEn, phase entropies (S1 and S2) coupled with fuzzy classifier. In another study by the same group obtained a classification accuracy of 99.7% using entropies, HOS, FD and Hurst exponent features with fuzzy classifier [6]. Quiroga et al. [78] classified normal and epileptic EEG signals into three-classes using Kulback–Leibler and renormalized entropies. They concluded that Kulback–Leibler provided better classification results than renormalized entropies. Recurrence Plots (RP) and Recurrence Quantification Analysis (RQA) parameters are also explored in the classification of normal, ictal and interictal EEG signals. Acharya et al. [4] used ten RQA features in the classification of three types of EEG segments. They employed seven different classifiers such as DT, SVM, GMM, KNN, PNN, fuzzy Sugeno classifier (FSC) and naive Bayes classifer (NBC) classifier and evaluated their performance. They observed that SVM classifier yielded the highest accuracy of 95.6% with sensitivity and specificity of 98.9% and 97.8% respectively. Song and Lio [89] applied SampEn to the normal, interictal and ictal EEG segments combined with ELM and achieved an accuracy of 95.7%. Martis et al. [59] explored spectral entropy features with Empirical Mode Decomposition (EMD) and decision tree (C4.5) classifier. EEG signals are decomposed using EMD to obtain Intrinsic Mode Functions (IMF) represented as amplitude and frequency modulated waves. Spectral features such as spectral peaks, spectral entropy and energy are extracted from each IMF segments. These features are fed to C4.5 decision tree classifier and obtained an accuracy of 95.3%.

Researchers have even studied five sets of EEG signals normal (two classes – signals recorded with eyes open and closed or signals from data sets Z and O), interictal (two classes – signals from data sets N and F), and ictal segments using wavelet-based technique [38].

Guo et al. [39] applied multi-wavelet transform and ApEn to the EEG signals of normal and epilepsy coupled with Multi-Level Probabilistic Neural Network (MLPNN) classifier. They reported a classification accuracy of 98.2% for the classification of normal (signals of two data sets), interictal (two classes), and ictal classes.


                        Table 2
                         shows that the various researchers have experimented different entropies in EEG signal analysis and classification. Many researchers [106,90,110,105] have used one entropy feature for EEG signal analysis and reported an accuracy of more than 92% in classification of normal, interictal, ictal signals. Few other researchers [45,46,5–7,24,25] have used combination of entropies features for the detection of normal, interictal and ictal EEG signals. The combination of different entropies has been reported an accuracy of more than 92% in EEG signal classification. However, the combination of entropies has improved the classification efficiency. In other words, the combination of entropies should be such that the (main advantage of combining the entropies would be that the) limitation of one entropy will be overcome by the adding more entropies.

In this review paper, different entropies are used to classify into normal, interictal, and ictal EEG signals. The results demonstrate that the entropies can be used as one of the state-of-the-art-methods for the classification of EEG signals into different epileptic states.

The application of entropies for feature extraction from EEG signals is found to have the following advantages:
                           
                              (i)
                              It can be used in the signal processing to separate the useful signal from redundant information.

It usually has high value for highly irregular or unpredictable signal and low value for highly regular signal. Thus, it is easier to identify the abnormal activities in the signal.

It can be used to automatically identify ictal segments that are difficult to localise visually. Also, it can be used to differentiate focal and non-focal EEG signals.

The results shown in Table 1 shows unique ranges for various entropies and also their ranking using F-value.

These entropy features can be used for other biomedical application like cardiac abnormalities, eye diseases, early stage of diabetes, Alzheimer’s disease, autism, fatty liver disease, and stroke detection and treatment.

@&#CONCLUSION@&#

Epilepsy is a neurological disorder caused by recurring seizures and can be assessed by EEG signals. The EEG signal is nonlinear nature and has higher inter and intraobserver variability. Thus, automated detection of epileptic EEG signals may prove to be useful in the monitoring and treatment of epilepsy patients. The nonlinear and chaotic nature of the EEG signals can be evaluated efficiently using entropy features. It exhibits the degree of variability and complexity present in the EEG signals. This comprehensive review presents the application of various entropies to discriminate normal, interictal and ictal EEG signals. Table 1 summarizes the performance of all the entropies and it can be concluded that RE, SampEn, SEN and PE are highly discriminative features to classify normal, interictal and ictal EEG signals.

@&#REFERENCES@&#

