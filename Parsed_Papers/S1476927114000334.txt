@&#MAIN-TITLE@&#An ensemble method for prediction of conformational B-cell epitopes from antigen sequences

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A support vector machine-based ensemble method is proposed to predict conformational B-cell epitopes.


                        
                        
                           
                           Epitopes are more accessible than non-epitopes, and preferred in beta-turn.


                        
                        
                           
                           The flexibility and polarity of epitopes are higher than non-epitopes.


                        
                        
                           
                           In bound dataset, Asn, Glu, Gly, Lys, Ser, and Thr are preferred in epitope regions.


                        
                        
                           
                           In unbound dataset, Glu and Lys are preferred in epitope sites.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Bound dataset

Unbound dataset

Support vector machine

Beta-turn

Flexibility

@&#ABSTRACT@&#


               
               
                  Epitopes are immunogenic regions in antigen protein. Prediction of B-cell epitopes is critical for immunological applications. B-cell epitopes are categorized into linear and conformational. The majority of B-cell epitopes are conformational. Several machine learning methods have been proposed to identify conformational B-cell epitopes. However, the quality of these methods is not ideal. One question is whether or not the prediction of conformational B-cell epitopes can be improved by using ensemble methods. In this paper, we propose an ensemble method, which combined 12 support vector machine-based predictors, to predict the conformational B-cell epitopes, using an unbound dataset. AdaBoost and resampling methods are used to deal with an imbalanced labeled dataset. The proposed method achieves AUC of 0.642–0.672 on training dataset with 5-fold cross validation and AUC of 0.579–0.604 on test dataset. We also find some interesting results with the bound and unbound datasets. Epitopes are more accessible than non-epitopes, in bound and unbound datasets. Epitopes are also preferred in beta-turn, in bound and unbound datasets. The flexibility and polarity of epitopes are higher than non-epitopes. In a bound dataset, Asn (N), Glu (E), Gly (G), Lys (K), Ser (S), and Thr (T) are preferred in epitope regions, while Ala (A), Leu (L) and Val (V) are preferred in non-epitope regions. In the unbound dataset, Glu (E) and Lys (K) are preferred in epitope sites, while Leu (L) and Val (V) are preferred in non-epitiopes sites.
               
            

@&#INTRODUCTION@&#

B-cell epitopes are the sites on antigen proteins which are recognized or bounded by B-cells. The identification of B-cell epitopes is useful for the design of peptide-based vaccines and drugs (Walter, 1986; Van Regenmortel, 2004). Predicted peptides can be synthesized and can be used as reagents for detecting anti-protein antibodies (Van Regenmortel, 2006). Predicted peptides can also used in synthetic peptide vaccines (Yadav et al., 2011). B-cell epitopes can be divided into two types: linear epitopes and conformation epitopes. Linear epitopes are continuous amino acids. Conformational epitopes are closed in the antigen protein sequence, but discontinuous in the protein sequences. Most B-cell epitopes are conformational epitopes (Pellequer et al., 1991).

It is time consuming and expensive to identify B-cell epitopes experimentally. Some computational methods are proposed to identify the epitopes and non-epitopes. The first stage of using computational methods to predict B-cell epitopes is mainly concerned with linear B-cell epitopes. Simple models are proposed to predict these linear B-cell epitopes. These methods train on small datasets and are based on a single amino acid property (Hopp and Woods, 1981; Welling et al., 1985; Karplus and Schulz, 1985; Parker et al., 1986; Kolaskar and Tongaonkar, 1990; Pellequer et al., 1993; Pellequer and Westhof, 1993; Alix, 1999). Later, some researchers combined several properties, including hydrophilicity, solvent accessibility, flexibility, and secondary structure, in order to predict B-cell epitopes (Odorico and Pellequer, 2003).

The main feature of the secondary stage of B-cell epitope prediction is the use of a more advanced model for linear epitopes. BepiPred is a method based on a hidden Markov model (Larsen et al., 2007). ABCPred is based on neural networks (Saha and Raghava, 2006). Chen et al. (2007) used 20mer peptides as the training dataset and built the model using support vector machine. BCPred (El-Manzalawy et al., 2008) was built by a kernel-based support vector machine (SVM). COBEpro (Sweredoski and Baldi, 2009) used a two-stage design with SVM. BayesB (Wee et al., 2010) predicted epitopes using the position specific scoring matrix (PSSM), with the Bayes feature selection method.

With the development of linear epitopes, predictors of conformational epitopes are proposed. CBTOPE (Ansari and Raghava, 2010) is an SVM-based predictor for conformational epitopes. CEP (Kulkarni-Kale et al., 2005) is a structure-based method which uses solvent accessibility. DiscoTope (Haste Andersen et al., 2006) is a method which combines several properties, including solvent accessibility, contact numbers and amino acid propensity scores. SEPPA (Sun et al., 2009) combines propensity scores and the packing density of amino acids. PEPITO (Sweredoski and Baldi, 2008) determined the probabilities of epitopes by linear regression. EPSVR (Liang et al., 2010) assigned the probability via a support vector regression. Zhang et al. (2011) utilized a random forest model to predict epitopes. Epitopia (Rubinstein et al., 2009a, 2009b) utilized Naïve Bayes to improve the epitopes. BEST (B-cell Epitope prediction using Support vector machine Tool) (Gao et al., 2012) is a new method, using two-stage schemes to predict B-cell epitopes.

More recently, some ensemble methods have been applied to identify the epitopes in antigen proteins. Ensemble method is a method that aggregates multiple machine learning methods by weighting individual classifiers to get a classifier which outperforms each one of them (Polikar, 2006; Rokach, 2010). Ensemble method has advantages in dealing with higher-dimensional and complicated data. EPMeta (Liang et al., 2010) is an ensemble method which combines six predictors to predict conformation epitopes. Zhang et al. (2012) built an ensemble method using random forests to predict epitopes.

One motive of this paper is to examine whether an ensemble method can improve the prediction of B-cell epitopes. To this end, we have built an ensemble method which combined several support vector machines predictors to predict B-cell epitopes. We also noticed other ensemble method is proposed by Zhang et al. (2012). Different from Zhang's random forest-based ensemble method, multiple inputs including PSSM profile, amino acids pair indicators predicted disorder region and predicted secondary structure indicators are used. Here we compute the features for each residue, and analyze the selected features on a bound dataset and an unbound dataset. AdaBoostM1 and the re-sample method are used to deal with the imbalanced dataset. Some interesting results related on the bound dataset and the unbound dataset are investigated.

Four datasets were used in this paper: (1) Rubinstein’ bound structure dataset was first introduced by Rubinstein et al. (2009a). There are 66 antibody and antigen complex structures. Antigen sequences were abstracted from the structure dataset. Since some proteins contain more than two chains, 83 antigen sequences are obtained, consisting of 1076 epitopes and 16,744 non-epitopes, with a ratio between epitopes and non-epitopes of 1:15.6 (named Bound83); (2) Liang's unbound structure dataset was first introduced by Liang et al. (2009, 2010). Antigen sequences from the structure dataset were abstracted. The final dataset contains 48 antigen sequences which consist of 885 epitopes and 10,145 non-epitopes. The ratio between epitopes and non-epitopes is 1:11.5 (named Unbound48). Both the Bound83 and Unbound48 datasets, were used as the training datasets. The proposed method is trained on the Unbound48 dataset in this paper; (3) 19 antigen sequences were abstracted from Liang's independent unbound structure dataset (Liang et al., 2010). This dataset is used as the independent dataset (named TEST19); (4) the other dataset, which was used in Rubinstein et al., 2009a, 2009b and Gao et al. (2012), is also used to evaluate the predictor. This dataset is named TEST194.

We consider the following features to represent the residues.

Physiochemical propensities of amino acids were used in this paper. Hydrophilicity, hydrophobicity, accessibility, flexibility, polarity, turns, antigenicity and beta-turn were all downloaded from the AAindex database (Kawashima et al., 2008). These features are chosen based on the work (Zhang et al., 2012; Su et al., 2012). We computed the minimum, maximum, and average of each of these properties within sliding windows, with window width from 5, 7, 9, …, 21. If the residue is in the head or terminus of sequence, we assigned zero for the special properties in the window.

Each amino acid type was represented by a 20-bit binary string. For example, amino acid Ala (A) is denoted as (1, 0, 0, 0, …, 0), (19 zeroes, 1 one).

Composition of the 20 amino acids (AA) in 21mer windows, i.e., the count residue number divided by the sequence length, where AA
                              i
                            stands for one of 20 amino acids (20 features).

If the residue is in ASN–Tyr (N–Y), His–Tyr (H–Y) and His–Met (H–M), we denoted this as 1, otherwise 0. This is motivated by the work from Sun et al. (2011).

If the residue is within the top 10 or end 10 residues, we denoted this as 1, otherwise it was denoted as 0. These features are motivated by Chen et al. (2012).

PSSMs were obtained by the PSI-BLAST program (Altschul et al., 1997), which searched the nr data set downloaded in 2012-09. The elements of PSSM for an amino acid were scaled by 1/(1+exp(x)). A residue is represented by its corresponding 20-dimensional row vector in the PSSM matrix. We computed the self-entropy score for each residue. We also used the sliding windows of PSSM profile with 21mer window width to represent the center residue. If the residue is in the head or terminus of sequence, we assigned zero for the PSSM profile in the 21mer window.

We used PSIPRED (Jones, 1999) to predict the protein sequence secondary structure. Each type of secondary structure is encoded by {0,1,2}; 0 for helix, 1 for strand and 2 for coils. The binary profile of secondary structure gives, H (helix) for (1,0,0), E (strand) for (0,1,0), C (coil) for (0,0,1). The predicted probabilities for the three types of secondary structures were also used in this paper. For the secondary structure segment indicator, each 3mer secondary structure is mapped 1–27 numbers (3*3*3=27). For example CCC, is mapped 1, the secondary structure segment indicator of CCC is (1, 0, 0, 0, …, 0), CCH is mapped 2, the secondary structure segment indicator of CCH is (0, 1, 0, 0, …, 0).

Relative solvent accessibility (RSA) was predicted by SpineX (Faraggi et al., 2012). We used sliding windows of size 5, 7, …, 21, and computed the minimal, maximal and average of the RSA values in each of the sliding windows. Buried/exposed residue notation was used. RSA values above 0.25, give an exposed residue (encoded as 1), otherwise we have buried residues (encoded as 0). The dihedral angles predicted by SpineX were also used here. If the residue is “X”, we assigned zero. We also computed the minimal, maximal and average of the Phi/Psi values in one-window using window sizes of 5, 7, …, 21.

The disorder region was predicted by Disopred2 (Ward et al., 2004). Disorder residues were encoded as {1}, non-disordered residues are encoded as {0}.

We noted that our training datasets (Bound83 and Unbound48) were imbalanced. We used two ways to deal with these imbalanced datasets: (a) AdaboostM1 (Freund and Schapire, 1996). If the sample training goes wrong, it will be picked out, and the sample will be trained again in the later classifier; (b) resample method. The ratio of non-epitopes to epitopes is 12:1 and 16:1 for Bound83 and Unbound48, respectively. If we used the original data set to build the model, the predictor would be biased, predicting all residues as non-epitopes. In this paper, we divided the non-epitopes (majority class) into N equal parts, and combined the epitopes (minority class), respectively, where N is the ratio of non-epitopes to epitopes. This scheme is shown in Fig. 1
                        . N classifiers predict results with N probabilities. A vote algorithm is used to decide the results. If the number of votes for epitope exceeds the number of votes for non-epitope, we declare it to be an epitope with maximal probability, otherwise, it is declared a non-epitope, but with minimal probability.

The predictor will output binary outputs, whether or not the residue is a part of an epitope. To evaluate the binary predictor, accuracy (ACC), sensitivity (Sn), specificity (Sp), and Matthews correlation coefficient (MCC) are used:
                           
                              
                                 
                                    Accuracy
                                    =
                                    
                                       
                                          TP
                                          +
                                          TN
                                       
                                       
                                          TP
                                          +
                                          FP
                                          +
                                          TN
                                          +
                                          FN
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              
                                 
                                    Sensitivity
                                    =
                                    
                                       
                                          TN
                                       
                                       
                                          TN
                                          +
                                          FP
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              
                                 
                                    Specificity
                                    =
                                    
                                       
                                          TN
                                       
                                       
                                          TN
                                          +
                                          FP
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              
                                 
                                    MCC
                                    =
                                    
                                       
                                          TP
                                          *
                                          TN
                                          +
                                          FP
                                          *
                                          FN
                                       
                                       
                                          
                                             
                                                {
                                                (
                                                TP
                                                +
                                                FP
                                                )
                                                *
                                                (
                                                TP
                                                +
                                                FN
                                                )
                                                *
                                                (
                                                TN
                                                +
                                                FP
                                                )
                                                *
                                                (
                                                TN
                                                +
                                                FN
                                                )
                                                }
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where TP and TN are the number of correctly predicted epitope, and non-epitope residues, respectively, FP is the number of non-epitope residues that were predicted to be in epitopes, and FN is the number of epitope residues that were predicted not to be in epitopes. Higher values of MCC and ACC indicate better quality predictions.

For some predictors, which output the probability that the residue is an epitope, we calculate the area under the ROC curve (AUC) to evaluate the real-value predictions. A perfect classifier will have an AUC equal to 1. A random predictor will have an AUC of 0.5.

We optimized the parameters of the RBF kernel in support vector machine, C from 2−2, 2−1, …, 25, gamma from 2−3, 2−2, …, 22. We selected the parameters which achieved the highest average AUC, with 5-fold cross validation. Take the Bound83 for example, we divided the non-epitopes (majority class) into 12 equal parts, and combined the epitopes (minority class), respectively. There are 12 subsets. We calculate the AUC for 12 subsets with 5-fold cross validation, and the AUC for the training dataset Bound83 is computed by the average of 12 AUCs of subsets. The optimized parameters for the unbound dataset are C
                        =8, gamma
                        =0.000488. The SVM is implemented in libsvm (version 2.19) (Chang and Lin, 2011). The AdaBoostM1 is implemented in Weka (version 3-7-7) (Hall et al., 2009).

@&#RESULTS AND DISCUSSION@&#

We compared the different models on the training dataset with 5-fold cross validation (Table 1
                        ). On the Bound83 data set, Ensemble
                           svm
                         achieved the highest MCC 0.243, and the highest AUC 0.672. The second highest MCC is 0.140, achieved by AdaBoostM1
                           NaiveBayes
                        , while its AUC is 0.666. This indicated that the ensemble method, based on SVM predictors, obtained the highest prediction power. On the Unbound48 dataset, Ensemble
                           svm
                         achieved the highest MCC, 0.207. We noted that AdaBoostM1
                           svm
                         achieved the highest AUC, 0.654 on the Unbound48 dataset. However, AdaboostM1
                           svm
                         achieved the lower MCC, 0.058. This indicates that the ensemble algorithm achieves better MCC than the AdaBoostM1 algorithm, on both bound and unbound datasets. We also noted that the ensemble method, using the bound dataset, achieves lower sensitivity (0.642) than using the unbound dataset (0.651). In practice, we always need to predict the epitopes in antigen sequence in unbound, so the ensemble algorithm, which trains on the Unbound48 dataset, is used to build the predictor to implement our method in this paper.

In order to compare with Zhang's method, which is the newer sequence-based method, results are computed for TEST19. Table 2
                         shows the results. Using the Unbound48 dataset as the training dataset, our method (Ensemble
                           unbound
                        ) achieved an AUC of 0.604. For the structure-based method, EPSVR achieved the highest AUC, 0.606, while for the sequence-based method, CBTOPE achieved 0.607. In order to assess statistical significance of improvements, we randomly selected 10 sequences from TEST19, 10 times. We used the paired t-test to evaluate the significance. It is motivated by Chen et al. (2011), Gao et al. (2012) and Peng et al. (2013). Ensemble
                           unbound
                         is better than EPCES, Ensemble
                           bound
                         and Zhang
                           unbound
                        , and achieves the same quality as CBTOPE and EPSVR. This indicates that the ensemble algorithm performed with the same quality as some structure-based method in terms of AUC, and better than sequence-based methods in terms of AUC on TEST19.

We also notice that the model achieves better performance on unbound dataset (TEST19) using the unbound training dataset (Unbound48). The AUC of 0.604, achieved by Ensemble
                           unbound
                        , is better than the AUC of 0.579 achieved by Ensemble
                           bound
                        . This observation is consistent with the Zhang's method (Zhang et al., 2012).


                        Fig. 2
                         shows the comparison between our method and Zhang's method (Zhang et al., 2012), which is a newer method for B-cell epitopes. The protein is defined as a discrimination, if the protein's AUC≤0.5. Using the Unbound48 dataset, we found that there were four proteins that were incorrectly discriminated by both Zhang
                           unbound
                         and Ensemble
                           unbound
                        . Zhang
                           unbound
                         incorrectly discriminated three proteins (PDB ID: 1mbn, 1av1, 2gib) while two proteins were discriminated incorrectly by Ensemble
                           unbound
                        . Using the Bound83 dataset, we found that there are three proteins (PDB ID: 1eku, 1og5, 1rec) incorrectly identified by both methods. Besides these three proteins, CBTOPE incorrectly identified one protein (PDB ID: 1mbn), while Ensemble
                           bound
                         incorrectly identified three proteins (PDB ID: 2gmf, 1y8o, 2b5i). This indicates that our method is complementary to existing methods.

Our method was further compared with Epitopia, ABCPred, COBEpro, and BEST on the TEST194 dataset (Table 3
                        ). The significance of differences between our predictor and the other methods were evaluated. 100 chains were selected randomly from the TEST194 dataset, and the evaluation was repeated 10 times. A paired t-test was used to compare the AUC values, and the differences are considered significant if the p-value<0.05. The structure-based Epitopia outperformed other methods, and obtained an AUC of 0.57–0.59. Compared with the sequence-based methods, our ensemble method achieved the same quality as BEST (B-cell Epitope prediction using Support vector machine Tool) with parameter 16 (BEST16), with AUC of 0.57. We noted that Ensemble
                           unbound
                         achieved the same quality as BEST with parameter 10 (BEST10). The corresponding average AUCs and the standard deviation are shown in Fig. 3
                        .

We computed the biserial correlation with our feature and the labels. The correlation coefficient is close to 1, which means that this is more efficient for the discrimination of epitopes and non-epitopes. We analyzed all the features which are above 0.1. The highest biserial correlation feature is the RSA value. Fig. 4A and B shows the difference in RSA between the bound and unbound datasets. It means that the average RSA for epitopes is higher than the average RSA for non-epitopes on both bound and unbound datasets. The following features are the RSA values in windows with sizes of 5, 7, 9, and with buried/exposed residue indicators. It shows that solvent accessibility plays an important role in the classification of epitopes and non-epitopes. Epitopes are more accessible than non-epitopes.


                        Fig. 5
                         shows the accumulated Sparse_AA, which indicates the account of the amino acids in the epitopes and the non-epitopes. In the Bound83 training dataset (in Fig. 5A), Asn (N), Glu (E), Gly (G), Lys (K), Ser (S), and Thr (T) are more likely to appear in epitope sites, while Ala (A), Leu (L) and Val (V) are more likely to be in non-epitope sites. In the Unbound48 dataset(in Fig. 5B), Glu (E) and Lys (K) are preferred at epitipe sites, while Leu (L) and Val (V) are preferred at non-epitiope sites.

We analyze the properties of epitopes and non-epitopes in the unbound and bound datasets. Self-entropy and the dihedral angle of amino acids (predicted phi and psi by SpineX) do not show a difference between the epitopes and non-epitopes on the training datasets. We find that the epitopes are preferred for the beta-turn in both bound and unbound datasets. The flexibility of epitopes is higher than that of non-epitopes in both bound and unbound datasets. The polarity of epitopes is higher than that of non-epitopes in both bound and unbound data sets. Fig. 6
                         shows the distribution of these properties within sliding windows size of 7 in the unbound dataset (Unbound48). Take the beta-turn (Fig. 6A) for example, the distribution of beta-turn of epitopes is similar as the distribution of beta-turn of non-epitopes. However, the average of beta-turn of epitopes is larger than the average of beta-turn of non-epitopes. The same conclusion can be found for flexibility (Fig. 6B) and polarity (Fig. 6C). It indicates that single amino acid propensity profiles can help to predict the epitopes and non-epitopes, but results are not reliable. Multiple propensities should be combined to improve the prediction of epitopes. It is consistent with Blythe and Flower (2005). These hints may provide useful information for the prediction of B-cell epitopes.

@&#CONCLUSION@&#

In this paper, we proposed an ensemble predictor based on the support vector machine. The model considers the composition of amino acids, predicted secondary structure and predicted the relative solvent accessibility. The proposed method achieves AUC of 0.642–0.672 on training dataset with 5-fold cross validation and AUC of 0.579–0.604 on test dataset. In TEST19, our method performed with the same quality as some structure-based method in terms of AUC, and the same quality as sequence-based method CBTOPE, and better than other sequence-based methods in terms of AUC (p-value=0.05). In TEST194, our method achieved a better AUC than other sequence-based methods, and of the same quality as the structure-based methods.

We find that the ensemble method achieves a higher MCC, but it does not improve AUC by much. This may due to the difficulties raised by the B-cell epitope problem. One consideration for epitopes, maybe the need to develop more accuracy in annotation, and larger datasets. In this paper, we found some interesting conclusions regarding the differences between the bound and unbound datasets. Epitopes in both unbound and bound datasets are preferred for beta-turn. The flexibility and polarity of epitopes are higher than that of non-epitopes. In the bound dataset, Asn (N), Glu (E), Gly (G), Lys (K), Ser (S), and Thr (T) are more likely to appear in epitope sites, while Ala (A), Leu (L) and Val (V) are more likely to be in non-epitopes regions. In the unbound dataset, Glu (E) and Lys (K) are preferred in epitope sites, while Leu (L) and Val (V) are preferred in non-epitiope sites. These findings may give some clues about the prediction of B-cell epitopes.

We declare that we have no conflict of interest.

@&#ACKNOWLEDGEMENTS@&#

JG was supported by Fundamental Research Funds for the Central Universities (grant numbers 65011491), Specialized Research Fund for the Doctoral Program of Higher Education (SRFDP). JR was supported by the International Development Research Center, Ottawa, Canada (no. 104519-010); NSFC (grant numbers 31050110432, 31150110577).

@&#REFERENCES@&#

