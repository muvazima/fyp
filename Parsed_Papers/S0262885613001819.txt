@&#MAIN-TITLE@&#Unsupervised manifold learning using Reciprocal kNN Graphs in image re-ranking and rank aggregation tasks

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Presentation of an unsupervised manifold learning algorithm using Reciprocal kNN Graphs
                           


                        
                        
                           
                           Presentation of the Reciprocal kNN Graph ReRanking for improving the effectiveness of CBIR systems


                        
                        
                           
                           Description of how Reciprocal kNN Graph algorithm can be used for rank aggregation tasks


                        
                        
                           
                           Discussion about the computational complexity and the convergence of proposed algorithm


                        
                        
                           
                           Experimental evaluation considering different datasets, descriptors, and baselines


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Content-based image retrieval

Re-ranking

Rank aggregation

@&#ABSTRACT@&#


               
               
                  In this paper, we present an unsupervised distance learning approach for improving the effectiveness of image retrieval tasks. We propose a Reciprocal kNN Graph algorithm that considers the relationships among ranked lists in the context of a k-reciprocal neighborhood. The similarity is propagated among neighbors considering the geometry of the dataset manifold. The proposed method can be used both for re-ranking and rank aggregation tasks. Unlike traditional diffusion process methods, which require matrix multiplication operations, our algorithm takes only a subset of ranked lists as input, presenting linear complexity in terms of computational and storage requirements. We conducted a large evaluation protocol involving shape, color, and texture descriptors, various datasets, and comparisons with other post-processing approaches. The re-ranking and rank aggregation algorithms yield better results in terms of effectiveness performance than various state-of-the-art algorithms recently proposed in the literature, achieving bull's eye and MAP scores of 100% on the well-known MPEG-7 shape dataset.
               
            

@&#INTRODUCTION@&#

The development of multimedia technologies for creating and sharing digital contents has triggered an exponential increase of image collections. Traditional search approaches based on image metadata can be unfeasible for large collections, since much human intervention is required for image annotation. Content-Based Image Retrieval (CBIR) systems have emerged as a promising alternative, aiming at retrieving the images that are the most similar to a given query.

The effectiveness of CBIR systems is very dependent on the distance measure adopted. Images are often modelled as high dimensional points in an Euclidean space, and the distances among them are usually measured by Euclidean distances. In this scenario, CBIR systems often consider only pairwise image analysis, that is, compute similarity measures considering only pairs of images, ignoring the information encoded in the relations among several images. On the contrary, the user perception considers the query specification and responses in a given context. In view of that, there has been significant research [45,44,13,26,14] on improving the distance measures in CBIR systems, replacing pairwise similarities by more global affinity measures that consider the relationships among images. The overall goal of these methods is to mimic the human behavior on judging the similarity among objects by taking into account the context of the search process. As previously observed [43,41], an effective distance measure should describe the relationship between the query and retrieved objects in the context of the whole collection.

Therefore, how to capture and utilize the intrinsic manifold structure of a collection becomes a central problem in the vision and learning community [14]. A common recent approach is manifold learning, mainly based on non-linear dimensionality reduction techniques. The idea is to explicitly construct a new embedding space with a corresponding metric which is more faithful to the manifold structure and hence induces a better distance/similarity measure. The manifold learning algorithms are able to learn distances between data points that correspond to geodesic distances on the data manifold [44]. In other words, the new distances are estimated considering a walk along the geometric structure of the dataset.

In this paper, we propose an unsupervised learning algorithm based on Reciprocal kNN Graph. The proposed algorithm improves the effectiveness of image retrieval through re-ranking and rank aggregation tasks by taking into account the instrinsic the geometry of the dataset manifold. The capacity of considering the geometry of the dataset manifold is illustrated in Figs. 1, 2, and 3
                     
                     
                     . We illustrate the Two-Moon dataset, comparing the Euclidean distance with the proposed Reciprocal kNN Graph. One point is selected as a labeled point (marked with a triangle) in each moon. In the following, all other data points are assigned to the closest labeled point, determining their color. Fig. 1 illustrates the classification computed by the Euclidean distance. Fig. 2 illustrates the ideal classification (with points in red and blue) considering the dataset manifold. The Euclidean distance does not consider the geometry structure of the dataset. As it can be observed, the extremities of the moons are misclassified. Fig. 3 illustrates the distances learned by the Reciprocal kNN Graph, after only one iteration. We can observe that several points were corrected compared with the Euclidean distance. The arrows in Fig. 3 illustrates how the Reciprocal kNN Graph algorithm iteratively propagates the similarity along the dataset structure considering the connectivity of the data set: (i) the red points in the left and; (ii) the blue points in the right.

The Reciprocal kNN Graph is mainly based on the information encoded in the top positions of the ranked lists. Given a query image, the ranked lists represent a relevant source of contextual information, since they define relationships not only between pairs of images (as distance functions), but also among all the images in the ranked list. The modelling of the similarity information consists in the essential difference between the Reciprocal kNN Graph approach and existing diffusion-based algorithms: the Reciprocal kNN Graph is based only on the ranked lists, and therefore independent of any distance (or similarity) scores.

By analyzing the ranked lists, it is expected, for example, that similar images present reciprocal references at the beginning of their ranked lists. It is also expected that images ranked at the top positions of ranked lists are similar to each other. In this way, aiming at redefining the distance between two images, the Reciprocal kNN Graph uses both the reciprocal nearest neighbor references and the graph structure considering all references among images at top positions of ranked lists. This approach represents the main contribution of our method, since it enables exploiting the maximum contextual information available in the ranked lists with low computational efforts. Another contribution relies on the efficiency of the Reciprocal kNN Graph algorithm. Unlike other diffusion approaches based on matrices multiplication [3,42,44], which presents complexity of O(n
                     3), our algorithm recomputes only the beginning of ranked lists with a constant size of elements, which presents computational and storage requirements of only O(n), where n represents the number of images in the collection.

We conducted a large evaluation protocol involving shape, color, and texture descriptors, different datasets and comparisons with other post-processing approaches. Experimental results demonstrate the effectiveness of our method. The re-ranking and rank aggregation algorithm yield better results in terms of effectiveness performance than various state-of-the-art algorithms.

This paper is organized as follows: Section 2 discusses related work; Section 3 discusses the definition of the image re-ranking problem; in Section 4, we present our Reciprocal kNN Graph algorithm. Section 5 presents the experimental evaluation and, finally, Section 6 draws on conclusions and presents future work.

@&#RELATED WORK@&#

Defining an effective distance measures consists in a key role in many multimedia applications, including classification and retrieval tasks. For example, choosing a good distance measure is often critical for building a content-based image retrieval (CBIR) system. In general, aiming at retrieving the most similar images to a given query image, CBIR systems compute a predefined distance measure between the query image and each collection image. Traditional distance measures that consider only the pairwise similarity between two images, as Euclidean distance, are often adopted. These approaches fail to return correct results in many scenarios, mainly due to the well-known semantic gap problem [11].

Recently, there has been considerable research on improving the distance measures in CBIR systems [42,43,15,13,41,45,3,26]. The main idea of various algorithms [44] is inspired by the success of Google PageRank [24] algorithm. Basically, the data manifold is represented as a graph with edge weights determined by similarity scores. Then, the similarities are propagated through weighted connections in the context of other dataset objects. In [42], a graph-based transductive learning algorithm is proposed for shape retrieval tasks. Inspired by semi-supervised label propagation algorithm, the shape retrieval was treated as an unsupervised problem. In [43], a locally constrained diffusion process is proposed where the influence of other shapes is propagated as a diffusion process on a graph formed by a given set of shapes. A shortest-path propagation algorithm was proposed [40] for explicitly finding the shortest path between them in the distance manifold of the dataset objects. However, a disadvantage of these methods is the large computational efforts required in the diffusion process, which is usually O(n
                     3).

Other methods analyze the k-neighborhood relationships for learning new distance measures and performing re-ranking tasks. In [13], a contextual dissimilarity measure was introduced aiming at improving the symmetry of the k-neighborhood relationship by iteratively regularizing the average distance of each vector to its neighborhood. The reciprocal kNN relationships are considered in [29] to construct a close-set and a far-set, used for performing a re-ranking task. Another kNN re-ranking [34] uses the top-k retrieved object to refine retrieval results by using information of ranked lists of each k neighbors. In [26], a recommendation process is simulated aiming at exploiting information encoded in ranked lists and collaboratively redefining pairwise distances.

The Reciprocal kNN Graph proposed in this paper combines characteristics of different methods. It uses the reciprocal kNN information, but also considers the graph structure constructed based on top positions of ranked lists. The distances among images are collaboratively redefined taking into account all references contained in top-k lists. Unlike other diffusion approaches [3,42,44], our algorithm redefines only the images placed at first positions of ranked lists. This procedure presents computational and storage requirements of only O(n). In addition, the Reciprocal kNN Graph can be used for re-ranking and rank aggregation tasks, aiming at combining different descriptors.

Let 
                        
                           C
                           =
                           
                              
                                 im
                                 
                                    g
                                    1
                                 
                                 ,
                                 im
                                 
                                    g
                                    2
                                 
                                 ,
                                 …
                                 ,
                                 im
                                 
                                    g
                                    n
                                 
                              
                           
                        
                      be an image collection, where n is the number of images in the collection. Let 
                        D
                      be an image descriptor which defines a distance function between two images imgi
                      and imgj
                      as ρ(imgi
                     , imgj
                     ). For simplicity and readability purposes, we use the notation ρ(i,j) for denoting the distance between images imgi
                      and imgj
                     .

Based on the distance function ρ, a ranked list τq
                      can be computed in response to a query image imgq
                     . Although the ranked lists contain distance information from the entire collection, the top positions of ranked lists are expected to contain the most relevant images related to the query image. Therefore, it can be very desirable that the ranked list τq
                      considers only a subset of the ns
                      most similar images, such that ns
                     
                     ≪
                     n and ns
                      is a constant value. That is even more crucial especially for large collections, where n is very high, and therefore τq
                      is very expensive to compute. In addition, various index structures [1,31] that have been proposed to speed up similarity queries can be used for computing the ranked lists.

The ranked list τq
                     
                     =(img
                     1, img
                     2, …, 
                        
                           im
                           
                              g
                              
                                 n
                                 s
                              
                           
                        
                     ) can be defined as a permutation of the subset 
                        
                           
                              C
                              s
                           
                           ⊂
                           C
                        
                     , which contains the most similar images to query image imgq
                     , such that and 
                        
                           
                              
                                 C
                                 s
                              
                           
                           =
                           
                              n
                              s
                           
                        
                     . A permutation τq
                      is a bijection from the set 
                        
                           C
                           s
                        
                      onto the set [n
                     
                        s
                     ]={1,2,…,n
                     
                        s
                     }. For a permutation τq
                     , we interpret τq
                     (i) as the position (or rank) of image imgi
                      in the ranked list τq
                     .

We can say that, if imgi
                      is ranked before imgj
                      in the ranked list of imgq
                     , that is, τq
                     (i)<
                     τq
                     (j), then ρ(q, i)≤
                     ρ(q, j). Note that, if the position of imgi
                      in the ranked list of imgq
                      is higher than the constant ns
                     , then τq
                     (i)=
                     ns
                     .

We also can take every image 
                        
                           im
                           
                              g
                              i
                           
                           ∈
                           C
                        
                      as a query image imgq
                     , in order to obtain a set 
                        R
                     ={τ
                     1,
                     τ
                     2,…,
                     τ
                     
                        n
                     } of ranked lists for each image of the collection 
                        C
                     . The storage requirements for handling the set 
                        R
                      are O(n), since the size of ranked lists is given by the constant ns
                     .

Our objective is to define a function fr
                      which takes a set of ranked lists 
                        R
                      as the input and computes a new and more effective set of ranked lists 
                        
                           R
                           ^
                        
                      by taking into account contextual information available on ranked lists:
                        
                           (1)
                           
                              
                                 
                                    R
                                    ^
                                 
                                 =
                                 
                                    f
                                    r
                                 
                                 
                                    R
                                 
                                 .
                              
                           
                        
                     
                  

The Reciprocal kNN Graph algorithm, presented in next section, represents a definition of the function fr
                     . For rank aggregation tasks, which combine ranked lists computed by different descriptors, the input of function fr
                      is given by a set of sets {
                        R
                     
                     1,
                        R
                     
                     2,…,
                        R
                     
                     
                        m
                     }, where m is the number of descriptors used.

In this section, we present the Reciprocal kNN Graph algorithm and its application in re-ranking and rank aggregation tasks. We also discuss convergence and efficiency aspects.

Ranked lists represent a relevant source of contextual information, since they establish relationships not only between pairs of images (as distance functions), but also among all images in the ranked list. The objective of Reciprocal kNN Graph is to exploit all available contextual information in ranked lists using three main strategies:
                           
                              •
                              
                                 Reciprocal neighborhood: the k-reciprocal nearest neighborhood mitigates the risk of false positives at top positions of ranked lists.


                                 Collaborative ranking: a ranked list can provide useful information for improving the effectiveness of other ranked lists. If two images appear at the top position of any ranked list, it indicates that they are probably similar to each other.


                                 Authority of ranked lists: we propose a score to estimate the authority of a given ranked list for collaborating with other ranked lists. The score is based on density of the graph that represents the reciprocal references among images at top positions of the ranked list.

The new distance score between two images is computed based on the reciprocal reference found in their ranked lists and collaboratively by other ranked lists, considering their respective authority. The process is iteratively repeated until a convergence criterion is reached.

The main concepts of the algorithm are formally defined in terms of three scores: (i) Reciprocal kNN Score; (ii) Authority Score; and (iii) Collaborative Score. Based on these scores, a new distance measure (nominated Reciprocal kNN Distance Measure) is computed. Fig. 4
                         illustrates the overall organization of the algorithm considering the proposed scores, which are discussed in details in the next sections.

Given a query image imgq
                        , we can define a neighborhood set that contains the k most similar images to imgq
                         as 
                           
                              N
                              
                                 q
                                 k
                              
                           
                        . For the k-nearest neighbors query, we obviously have 
                           
                              
                                 
                                    N
                                    
                                       q
                                       k
                                    
                                 
                              
                              =
                              k
                           
                        . The nearest neighbor relationships are not symmetric [13,29]. This means that 
                           
                              im
                              
                                 g
                                 i
                              
                              ∈
                              N
                              
                                 j
                                 k
                              
                           
                         does not imply 
                           
                              im
                              
                                 g
                                 j
                              
                              ∈
                              N
                              
                                 i
                                 k
                              
                           
                        . The set of k-reciprocal nearest neighbors of image imgq
                         can be defined [29] as:
                           
                              (2)
                              
                                 
                                    
                                       N
                                       r
                                    
                                    
                                       q
                                       k
                                    
                                    =
                                    
                                       
                                          im
                                          
                                             g
                                             i
                                          
                                          ∈
                                          N
                                          
                                             q
                                             k
                                          
                                          ∧
                                          im
                                          
                                             g
                                             q
                                          
                                          ∈
                                          N
                                          
                                             i
                                             k
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

We can easily verify if a given image imgi
                         is a k-reciprocal nearest neighbor of an image imgq
                         by checking if 
                           
                              im
                              
                                 g
                                 i
                              
                              ∈
                              
                                 N
                                 r
                              
                              
                                 q
                                 k
                              
                           
                        . However, beyond knowing if two images are reciprocal neighbors, we are also interested in the position from which on images became reciprocal neighbors. Therefore, we propose a Reciprocal kNN Score Rs
                         that consider this position:
                           
                              (3)
                              
                                 
                                    
                                       R
                                       s
                                    
                                    
                                       q
                                       i
                                    
                                    =
                                    
                                       
                                          max
                                          
                                             
                                                
                                                   τ
                                                   q
                                                
                                                
                                                   i
                                                
                                                ,
                                                
                                                   τ
                                                   i
                                                
                                                
                                                   q
                                                
                                             
                                          
                                       
                                       
                                          n
                                          s
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

This score reduces the probability of false positives at top positions of ranked lists. Therefore, a low value represents a strong indication of the similarity between images.

We consider that a ranked list encodes contextual information that can be used for improving effectiveness of other ranked lists. In this scenario, it is important to estimate the authority of a given ranked list. Our approach is analogous to the PageRank [24] algorithm and inspired by the cohesion measure [26]. We consider that an accurate ranked list has their top images referencing to each other at the top positions of their ranked lists. This conjecture is somehow close to the cluster hypothesis [30], which states that “closely associated items tend to be relevant to the same requests”.

Another important role of the authority score in the proposed algorithm is to estimate the quality of a ranked list. In other words, the score is an unsupervised predictor of the effectiveness of a given ranked list. The computation of the authority score is equivalent to measure the density of the graph that represents the reciprocal references among images at top-k positions. The Reciprocal kNN Authority Score of the ranked list τq
                         is defined as follows:
                           
                              (4)
                              
                                 
                                    
                                       A
                                       s
                                    
                                    
                                       q
                                       k
                                    
                                    =
                                    
                                       
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   ∈
                                                   N
                                                   
                                                      q
                                                      k
                                                   
                                                
                                             
                                             
                                          
                                          
                                          
                                             
                                                ∑
                                                
                                                   j
                                                   ∈
                                                   N
                                                   
                                                      i
                                                      k
                                                   
                                                
                                             
                                             
                                          
                                          
                                          
                                             f
                                             in
                                          
                                          
                                             j
                                             q
                                          
                                       
                                       
                                          k
                                          2
                                       
                                    
                                    ,
                                 
                              
                           
                        where fin
                         returns 1 if 
                           
                              im
                              
                                 g
                                 j
                              
                              ∈
                              N
                              
                                 q
                                 k
                              
                           
                         and 0 otherwise.

The score As
                         is defined in the interval [0,1]. Note that, for a complete reciprocal graph (where all k images references each other at top-k positions) this score returns a perfect score As
                        (q,k)=1.

Beyond the Reciprocal kNN Score, the distance between two images is defined using a collaborative score that considers information encoded in other ranked lists and their respective authority score. This means that, when an image imgi
                         appears at top positions of a ranked list τq
                        , the distance among imgi
                         and neighbors of image imgq
                         
                        
                           
                              
                                 im
                                 
                                    g
                                    j
                                 
                                 ∈
                                 N
                                 
                                    q
                                    k
                                 
                              
                           
                         are reduced proportionally to the authority of τq
                        .

In this way, given two images imgq
                         and imgi
                        , we define a Reciprocal kNN Collaborative Score, that accumulates the authority scores of all ranked lists in which images imgq
                         and imgi
                         appear. The collaborative score considers different values of k (varying from 1,2,…,k) with the purpose of giving greater weights to references at top positions. The collaborative score Cs
                         between two images imgq
                         and imgi
                         is defined as follows:
                           
                              (5)
                              
                                 
                                    
                                       C
                                       s
                                    
                                    
                                       q
                                       i
                                       k
                                    
                                    =
                                    
                                       
                                          ∑
                                          
                                             c
                                             =
                                             1
                                          
                                          k
                                       
                                       
                                    
                                    
                                    
                                       
                                          ∑
                                          
                                             j
                                             ∈
                                             C
                                          
                                       
                                       
                                    
                                    
                                    
                                       A
                                       s
                                    
                                    
                                       
                                          j
                                          c
                                       
                                       2
                                    
                                    ×
                                    
                                       f
                                       in
                                    
                                    
                                       q
                                       i
                                       j
                                    
                                    ,
                                 
                              
                           
                        where fin
                         returns 1 if 
                           
                              im
                              
                                 g
                                 q
                              
                              ,
                              im
                              
                                 g
                                 i
                              
                              ∈
                              N
                              
                                 j
                                 k
                              
                           
                         and 0 otherwise. A squared value is used for the authority score aiming at penalizing low scores. Notice that, although by definition j iterates for each 
                           
                              j
                              ∈
                              C
                           
                         (what requires linear computation efforts on the size of the collection), the value As
                        (j,c)2 is added only when 
                           
                              im
                              
                                 g
                                 q
                              
                              ,
                              im
                              
                                 g
                                 i
                              
                              ∈
                              N
                              
                                 j
                                 k
                              
                           
                        , that is, considering only top-k positions. Therefore, the collaborative score Cs
                        (q,i,k) can be computed requiring only O(k
                        2).

In this section, we define an iteratively distance measure, which is the basis of the proposed re-ranking and rank aggregation algorithms. Using this distance, a new set of ranked lists can be computed.

Based on Reciprocal kNN Score (Rs
                        ) and Reciprocal kNN Collaborative Score (Cs
                        ), we define the distance measure ρr
                        . All images imgq
                        , imgi
                         
                        
                           
                              ∈
                              C
                           
                         that present collaborative score Cs
                        (q,i,k)>0 have the distance between them updated as:
                           
                              (6)
                              
                                 
                                    
                                       ρ
                                       r
                                    
                                    
                                       q
                                       i
                                    
                                    =
                                    
                                       
                                          
                                             R
                                             s
                                          
                                          
                                             q
                                             i
                                          
                                       
                                       
                                          1
                                          +
                                          
                                             C
                                             s
                                          
                                          
                                             q
                                             i
                                             k
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

The remaining images with zero collaborative score (Cs
                        (q,j,k)=0) keep the distance between them as their current ranking, i. e., ρr
                        (q,j)=
                        τr
                        (j). No more computation efforts are required for those images. The main motivation of Eq. (6) is to consider information from both: (i) reciprocal reference between images being compared (dividend) and; (ii) information given by ranked list of other images, according to their authority (divisor). While the Reciprocal kNN Score (Rs
                        ) avoids false positives at top positions of ranked lists, the Reciprocal kNN Collaborative Score (Cs
                        ) propagates the similarity among different ranked lists, considering their quality estimation given by authority score.

Based on the distance ρr
                        , the set of ranked lists 
                           R
                         is updated, for ensuring that, if τq
                        (i)<
                        τq
                        (j), then ρ(q, i)≤
                        ρ(q, j). Finally, the process can be iteratively repeated. Let the superscript (t) denotes the iteration and let ρ
                        
                           r
                        
                        (0) be the distance at first iteration, we can define an iterative distance measure as follows:
                           
                              (7)
                              
                                 
                                    
                                       ρ
                                       r
                                       
                                          
                                             t
                                             +
                                             1
                                          
                                       
                                    
                                    
                                       q
                                       i
                                    
                                    =
                                    
                                       
                                          
                                             R
                                             s
                                             
                                                t
                                             
                                          
                                          
                                             q
                                             i
                                          
                                       
                                       
                                          1
                                          +
                                          
                                             C
                                             s
                                             
                                                t
                                             
                                          
                                          
                                             
                                                q
                                                ,
                                                i
                                                ,
                                                k
                                                +
                                                t
                                             
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

Note the value of k grows along iterations. It is expected that non-relevant images are moved out from the first positions of the ranked lists and, therefore, k can be increased for considering more images.

@&#DISCUSSION@&#

This section presents a detailed discussion on how the method works as well as its main equations. As previously mentioned, the ranked lists define relationships not only between pairs of images (as distance functions), but also among all the images in the ranked list. In this sense, if an image is well ranked for a given query, other images, similar to this image, are also expected to be well ranked for the same query. This observation is consistent with the “cluster hypothesis” [30], which states that closely related items tend to be relevant to the same requests.

The Reciprocal kNN Graph aims at exploiting the cluster hypothesis by analyzing the reciprocal references among ranked lists at their top positions. The three scores which define the algorithm are based on this principle: (i) Reciprocal kNN Score; (ii) Reciprocal kNN Authority Score; (iii) Reciprocal kNN Collaborative Score. We discuss the cluster hypothesis for each score in the following.

Given two similar images, a content-based descriptor is expected to produce ranked lists which present reciprocal references at the beginning of their ranked lists. When an image does not refer to the other image at the top positions of its ranked list, this behavior indicates a low confidence in the similarity between them. The Reciprocal kNN Score (Eq. (3)) represents the position in which this confidence is reached. Therefore, this score can be used to provide a more accurate distance measure than that used to compute the initial ranked lists.

However, the Reciprocal kNN Score considers the reciprocal references only between pairs of images. In addition, it is also expected that images ranked at the top positions of ranked lists present reciprocal references among each other (similar images tend to be relevant to the same queries). Therefore, the Reciprocal kNN Authority Score (Eq. (4)) measures the amount of reciprocal references by computing the density of the graph that represents the references among images at top-k positions of a given ranked list. Therefore, an effective ranked list which presents similar images at its top positions will also present a high authority score. In this way, this score can be used to estimate the effectiveness of a ranked list. Fig. 5
                         illustrates the computation of authority score, which is proportional to the number of edges on the Reciprocal kNN Graph.

Once we have an estimation of the effectiveness of ranked lists given by the authority score, images which appear at top positions of effective ranked lists are very likely to be similar. This assumption consists in the basis of the Reciprocal kNN Collaborative Score. The collaborative score between two images is given by the sum of authority scores of ranked lists in which these two images appear at top positions.

Finally, the Reciprocal kNN Distance Measure is computed by combining the Reciprocal kNN Score (that considers reciprocal references between two images), and the Reciprocal kNN Collaborative Score (that considers the top-k positions of all ranked lists using the Reciprocal kNN Authority Score).

Basically, an iterative method is said to converge, if the difference between results obtained along iterations decreases, tending to reach an ultimate result. In our case, a new distance measure should be iteratively executed while the quality of ranked lists is improved. We define a convergence criterion that determines the number of iterations using the proposed authority score. While the authority of ranked lists is increasing more than a threshold ϵ per iteration, the algorithm should continue executing. Given that the authority score estimates the quality of ranked lists, the proposed criterion is equivalent to keep executing the algorithm while the quality of ranked lists is improving.

Aiming at verifying the convergence criterion, an average authority score between all collection images is computed as follows:
                           
                              (8)
                              
                                 
                                    
                                       G
                                       s
                                    
                                    =
                                    
                                       
                                          
                                             
                                                ∑
                                                
                                                   c
                                                   =
                                                   1
                                                
                                                k
                                             
                                             
                                          
                                          
                                          
                                             
                                                ∑
                                                
                                                   j
                                                   ∈
                                                   C
                                                
                                             
                                             
                                          
                                          
                                          
                                             A
                                             s
                                          
                                          
                                             j
                                             c
                                          
                                       
                                       
                                          k
                                          ×
                                          n
                                       
                                    
                                 
                              
                           
                        
                     

In the following, the difference between iterations is compared with the threshold ϵ for defining the convergence criterion. The re-ranking is executed while (G
                        
                           s
                        
                        (t
                           +1)
                        −
                        G
                        
                           s
                        
                        (t))>ϵ.

In general, different types of measures may focus on different aspects of the images and are often complementary to each other [3]. Our goal is to use the Reciprocal kNN Graph algorithm to combine different distance measures in rank aggregation tasks. We propose a multiplicative approach inspired by [26] for combining the Reciprocal kNN Graph Distance Measures of different descriptors. The combination is computed only at the first iteration and subsequent iterations are computed over the already combined ranked lists. Let 
                           
                              ρ
                              
                                 r
                                 d
                              
                              
                                 1
                              
                           
                         be the kNN reciprocal distance measure at first iteration of a given descriptor d, with d defined in the interval [1.m], where m is the number of descriptors considered. The fused distance measure can be defined as:
                           
                              (9)
                              
                                 
                                    
                                       ρ
                                       r
                                       
                                          1
                                       
                                    
                                    
                                       q
                                       i
                                    
                                    =
                                    
                                       
                                          ∏
                                          
                                             d
                                             =
                                             1
                                          
                                          m
                                       
                                       
                                    
                                    
                                    
                                       ρ
                                       
                                          r
                                          d
                                       
                                       
                                          1
                                       
                                    
                                    
                                       q
                                       i
                                    
                                 
                              
                           
                        
                     

By multiplying the distance measures between the same images considering different descriptors, high distances obtained by one descriptor will be propagated to the others, leading to high aggregate values. Note that our algorithm multiplies distance measures computed by the Reciprocal kNN Graph algorithm and it does not depend on the original descriptors distance measures. It only considers the ranked lists. Therefore, our approach does not present a common problem in late fusion tasks of combining heterogeneous scores at different numeric scales.

This section briefly discusses some aspects of efficiency, computational complexity and storage requirements. The proposed algorithm takes as input only the beginning of ranked lists (with a constant size ns
                         of elements), which becomes the storage requirements of O(n). It represents a significant advantage in comparison with other methods that requires the complete similarity/distance matrix [26,44].

The asymptotic computational complexity of the algorithm is also O(n), since only the ns
                         top positions of ranked lists are redefined, independent on the size n of the dataset. The computation of the authority and collaborative scores, which presents the high computational effort needed, is proportional to (nTK
                        2), where T denotes the number of iterations and k denotes the number of neighbors considered when algorithm starts. Other steps of the algorithm have diverse computation cost, but all limited to the asymptotic cost of O(n). State-of-the art methods based on random walks on graphs [3,42,44] use matrices multiplication which presents complexity of O(n
                        3). Beyond that, the Reciprocal kNN Graph algorithm has potential to be massively parallelized.

This section demonstrates the effectiveness of the proposed re-ranking and rank aggregation methods in image retrieval tasks. A large set of experiments was conducted considering four datasets and nineteen CBIR descriptors, aiming at analyzing and comparing our method under several aspects.

The computation of Reciprocal kNN Graph algorithm considers only two parameters: (i) k: number of neighbors considered when algorithm starts; and (ii) ϵ: convergence threshold parameter.

To evaluate the influence of different parameter settings on the retrieval scores and for determining the best parameters values, we conducted a set of experiments considering the MPEG-7 [17] dataset. The MPEG-7 [17] dataset is a well-known shape dataset, composed of 1400 shapes divided in 70 classes. For evaluation, the so-called bull's eye score was considered, which counts all matching objects within the 40 most similar candidates. Since each class consists of 20 objects, the retrieved score is normalized with the highest possible number of hits. For distance computation, we used the Contour Features Descriptor (CFD) [25] shape descriptor.

We varied the parameter k in the interval [1,20] and iterations in the interval [1,15]. Fig. 6
                         illustrates the results of precision scores for different values of k and iterations.

We observed that best retrieval scores increased along iterations yielding the best precision score (96.49%) for k
                        =15 and final iteration T
                        =7. At this iteration, the difference value of convergence score to previous iteration is 0.0125. Therefore, we set the parameter values as k
                        =15 and ϵ=0.0125. Fig. 7
                         illustrates the evolution of convergence score along iterations. As we can observe, the algorithm converges very quickly, requiring a very small number of iterations for reaching the threshold.

As previously discussed, our algorithm considers only a subset of ranked lists. The size of ranked lists considered in experiments is ns
                        
                        =200. These values are used in all experiments, both for re-ranking and rank aggregation tasks, considering different descriptors and datasets, what demonstrates the robustness of our method.

In this section, we present the set of conducted experiments for evaluating our method in the task of re-ranking images considering shape, color, and texture descriptors.

We evaluate the use of our method with six shape descriptors: Segment Saliences (SS) [32], Beam Angle Statistics (BAS) [2], Inner Distance Shape Context (IDSC) [18], Contour Features Descriptor (CFD) [25], Aspect Shape Context (ASC) [19], and Articulation-Invariant Representation (AIR) [10]. We consider the MPEG-7 [17] dataset, described in Section 5.1.


                           Table 1
                            presents results considering the bull's eye score (Recall@40) and accuracy (Precision@20) for shape descriptors on the MPEG-7 [17] dataset. We can observe very significant gains in relation to the results observed for each descriptor initially, ranging from +6.69% to +29.48% for the bull's eye score and ranging from +6.10% to +40.50% for the accuracy measure.

The iterative behavior of the Reciprocal kNN Graph algorithm can be observed in the results shown in Fig. 8
                           . The figure shows the evolution of rankings along iterations. The first row presents a query image (first column with green border) and 20 image results, according to the CFD [25] shape descriptor. Note that wrong results (with red border) contain images from different classes. The remaining rows present the results for each iteration of Reciprocal kNN Graph algorithm. We can observe the significant improvement in terms of precision, ranging from 20% (on the ranking computed by the CFD [25] descriptor) to 100% at the 7th iteration of the proposed re-ranking algorithm.

Results for shape descriptor considering the MAP (Mean Average Precision) score are presented in Table 2
                           . We can observe positive gains for all shape descriptors ranging from +8.54% to +37.99%.

We evaluate our method with three color descriptors: Border/Interior Pixel Classification (BIC) [35], Auto Color Correlograms (ACC) [12], and Global Color Histogram (GCH) [36]. The experiments were conducted on a dataset used in [41] and composed of images from 7 soccer teams, containing 40 images per class. The size of images ranges from (198×148) to (537×672) pixels.


                           Table 2 presents the experimental results considering MAP as score. We can observe a positive gain for all color descriptors ranging from +4.50% to +15.33%.

The experiments consider three well-known texture descriptors: Local Binary Patterns (LBP) [22], Color Co-Occurrence Matrix (CCOM) [16], and Local Activity Spectrum (LAS) [37]. We used the Brodatz [6] dataset, a popular dataset for texture descriptors evaluation. The Brodatz dataset is composed of 111 different textures of size (512×512) pixels. Each texture is divided into 16 blocks (128×128) pixels of non-overlapping sub images, such that 1776 images are considered.


                           Table 2 presents the experimental results considering MAP as score. We can observe a positive gain for all texture descriptors ranging from +3.85% to +15.16%.

We evaluate the use of Reciprocal kNN Graph method to combine different CBIR descriptors. We selected three shape descriptors with highest retrieval scores in re-ranking tasks and evaluated the different combinations between them. Table 3
                         presents the rank aggregation results. Besides MAP scores, we also present the accuracy and the bull's eye score on the MPEG-7 dataset. Notice that the combination of CFD [25]
                        +AIR [10] presents retrieval scores of 100% for the three considered measures, which means perfect retrieval results.

We also selected two color and texture descriptors, with the highest MAP scores in re-ranking tasks. Table 4
                         presents results of MAP score of these descriptors. We can observe that significant gains are obtained when compared with the results of descriptors in isolation.


                        Fig. 9
                         illustrates the Precision
                        ×
                        Recall curves of two shape descriptors in different situations: before and after applying the Reciprocal kNN Graph algorithm, and after using it for rank aggregation. We can observe that significant gains in terms of precision have been achieved, in special for the rank aggregation task.

Finally, we also evaluate our method in comparison with other state-of-the-art post-processing methods. We use the MPEG-7 [17] dataset, with the bull's eye score, commonly used for post-processing methods evaluation and comparison. Table 5
                         presents results of the proposed Reciprocal kNN Graph algorithm (in bold) in comparison with several other post-processing methods recently proposed in the literature. Note that the results of Reciprocal kNN Graph algorithm presents better effectiveness performance when compared to various recently proposed methods in re-ranking tasks. We report the results of CFD [25] and AIR [10] descriptors, respectively the descriptors that presented the highest gain and the highest bull'eyes score in re-ranking tasks.

We also present results of Reciprocal kNN Graph in rank aggregation tasks in comparison to other recently proposed methods. Note that the Reciprocal kNN Graph applied to the combination of only two descriptors CFD [25]
                        +AIR [10] reached a perfect retrieval scores (considering MAP, accuracy and the bull's eye score), obtained by other state-of-the-art method only combining three descriptors.

The University of Kentucky Recognition Benchmark [21] (“UKBench”), also referred as Nister and Stewenius (N–S) Dataset, has a total of 10,200 images. The dataset is composed of 2550 objects or scenes, where each object/scene is captured 4 times from different viewpoints, distances and illumination conditions. Thus, it consists of 2550 image classes, and each class has only 4 images. For evaluation purposes, each image is used as query and the N–S retrieval score between 1 and 4 is computed. The score corresponds to the number of relevant images among the first four image returned (the highest achievable score is 4, indicating that all similar images are retrieved at top positions). Due to the small number of images in each class, the N–S dataset is a very challenging dataset especially for unsupervised learning approaches.

In the following, we describe several experiments conducted aiming at evaluating the Reciprocal kNN Graph on N–S Dataset. For all experiments, we use the same parameters used for other datasets, except for k
                        =5, due to the small number of similar images. The experiments consider nine descriptors
                           1
                        
                        
                           1
                           We used the LIRE implementation (http://www.semanticmetadata.net/lire/) of descriptors ACC [12], CEED [7], FCTH [8], and JCD [45].
                         exploiting several features as color, texture and local features. In the following, we briefly describe the descriptors based on considered features:
                           
                              •
                              
                                 Color: Auto Color Correlograms (ACC) [12], Border/Interior Pixel Classification (BIC) [35], Local Color Histogram (LCH) [36].


                                 Color and texture: Color and Edge Directivity Descriptor (CEED) [7], Fuzzy Color and Texture Histogram (FCTH) [8], Joint Composite Descriptor (JCD) [45].


                                 Local: Scale-Invariant Feature Transform (SIFT) [20], considering the number of matches between images as a similarity score
                                    2
                                 
                                 
                                    2
                                    We considered resized dataset images of 160×120 pixels and a public available implementation at http://www.cs.ubc.ca/lowe/keypoints.
                                 
                              


                                 Bag of visual words: Several configurations of descriptors based on visual words were evaluated
                                    3
                                 
                                 
                                    3
                                    We varied the low-level feature extraction using sparse and dense sampling, and using SIFT [20] and OpponentSIFT [33]. Different codebook sizes were evaluated, varying from 100 to 50,000 visual words. Hard and soft assignments [9] were also used, and we tested average and max pooling [5] and a spatial pooling approach called WSA (word spatial arrangement) [27].
                                  The results presented in this section consider only the best configurations, which are: sparse sampling (Harris-Laplace detector), OpponentSIFT [33], codebook of 1000 visual words, soft assignment (σ
                                 =150) and spatial pooling WSA (Word Spatial Arrangement) [27,28], using threshold for soft assignment as 0.01 and the distance function presented in [27].


                                 Holistic: GIST [23], a scene recognition descriptor based on the Spatial Envelope low dimensional representation
                                    4
                                 
                                 
                                    4
                                    We considered resized dataset images of 256×256 pixels and a public available C implementation at http://lear.inrialpes.fr/software.
                                 
                              


                        Table 6
                         presents the experimental results considering the N–S score for all considered descriptors on re-ranking tasks. We can observe positive gains for all descriptors ranging from +3.30% to +15.35%. Table 6 also presents the experimental results for Reciprocal kNN Graph considering rank aggregation tasks. We consider for the combination the three descriptors which have obtained the best scores in the re-ranking tasks (ACC [12], BIC [35], SIFT [20]). We compute the gain considering the highest score obtained by the descriptors used in the combination. We can observe (in bold) that the combination of ACC [12]
                        +SIFT [20] present a very high score (N–S=3.79) and BIC [35]
                        +SIFT [20] a very high gain (+18.42%).

We also compare the Reciprocal kNN Graph with state-of-the-art methods recently proposed on the N–S dataset. Table 7
                         presents the best N–S scores obtained by Reciprocal kNN Graph and reported by the recent retrieval methods and fusion approaches. We can observe that the Reciprocal kNN Graph presents the highest N–S score (in bold) when compared with other approaches.

In this Section, we present the run time for the Reciprocal kNN Graph on the N–S dataset, considering the four descriptors which have presented the highest retrieval scores (ACC [12], BIC [35], SIFT ePaperSIFT-1999, JCD [45]). We use a serial C implementation running over Linux on an Intel Xeon 2.40GHz processor. Fig. 10
                         presents the total run time and the run time per iteration for the four considered descriptors. The ACC [12] descriptor, for example, converged after only 3 iterations, with a total run time of 8.3s. The average time required for the re-ranking of each image in this case is only 8.1ms.

@&#CONCLUSIONS@&#

In this work, we have presented a novel re-ranking and rank aggregation approach that exploits the Reciprocal kNN Graph for improving image retrieval tasks.

The main idea consists in analyzing the reciprocal references at top positions of ranked lists for performing re-ranking and rank aggregation tasks. The Reciprocal kNN Graph algorithm iteratively propagates the similarity along the dataset structure by taking into account intrinsic geometry of the dataset manifold.

We conducted a large set of experiments considering different descriptors and datasets. Experimental results demonstrated the applicability of our method to several image retrieval tasks based on shape, color, and texture descriptors. In re-ranking tasks, for example, the Reciprocal kNN Graph algorithm achieves gains up to +37.99% considering MAP scores. Our proposed approach also achieves very high effectiveness performance when compared with recent state-of-the-art methods on well-known datasets.

Future work focuses on optimizing the proposed re-ranking and rank aggregation methods by considering parallel architectures.

@&#ACKNOWLEDGMENTS@&#

Authors thank AMD, FAEPEX, CAPES, FAPESP, and CNPq for financial support.

@&#REFERENCES@&#

