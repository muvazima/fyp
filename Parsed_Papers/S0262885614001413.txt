@&#MAIN-TITLE@&#Model-based graph-cut method for automatic flower segmentation with spatial constraints

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           An improvement of the classical energy function (graph-cut) is proposed.


                        
                        
                           
                           Integration of spatial priori and gradient information improves segmentation result.


                        
                        
                           
                           A new “coarse-to-fine” flower segmentation method is presented and implemented.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Automatic flower image segmentation

Graph-cut

Spatial prior

@&#ABSTRACT@&#


               Graphical abstract
               
                  
                     
                        
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

Automatic flower classification systems are important for a wide range of application including pharmacy research, environment protection and perfume production. Thanks to computer vision, image processing and pattern recognition techniques, automatic recognition systems make the identification of the flower category easier by analyzing color images. Image segmentation is generally considered an important component of the recognition or classification processes, and affects the quality of the image analysis. Automatic flower segmentation allows the extraction of the object of interest (foreground) from the rest of the image (background) without any manual interaction.

The majority of flower images present natural scenes with complex background. The areas surrounding the flowers have generally large variety of colors and textures. It can contain several entities distributed separately or together such as stones, leaves, turf grass, green foliage and soil. Fig. 1
                      illustrates different types of elements that can be contained in the area surrounding the flower. As the flowers from different species may look very similar both in shape and color, the use of the background information to generate the image features can increase this similarity and consequently reduce the classification accuracy.

Therefore, we believe that the extraction of features only from the object of interest provides more meaningful and accurate information than the one obtained from the whole image. Although many flower image segmentation methods have been proposed in the literature, it remains difficult to find a general solution that is applicable to all types of flowers and gives accurate results. In the next paragraph, we present the state of the art on flower image segmentation.

Das et al. [2] proposed an iterative segmentation algorithm using color and spatial domain knowledge-driven feedback. Their method mapped the RGB color space to commonly used color names in order to delete pixels which belong to background color classes like black, brown, green or gray. The foreground region represented by the remaining colors is accepted if it is included in the flower area. In order to define the flower region, some hypotheses were made such as the flower centroid should fall within the “central region” of the image. Saitoh et al. [3] presented the Normalized Cost (NC) method to extract flower regions. It is based on a Dynamic Programming method known as the intelligent scissors [4] for extracting the boundary of the object of interest. The image is represented as a directed weighted graph where nodes are pixels and arcs between neighboring pixels represent the 8-connectivity information. This method consists in computing the local minimum cost given by a path between two seeds. The obtained cost is normalized by the length of this path. The shortest path in the graph gives the object edges. In this work, the authors assume that the flower is at the center of the image and the background occupies the peripheral area. Based on this hypothesis, the authors determine some local minimum points of each local cost profile along the straight line from the starting point to all the middle points of four sides. Then, they extract the boundary for each local minimum point based on the NC and they select the one that has the smallest normalized cost and contains the center point. Another interesting automatic algorithm can be found in [1]. The first step of this algorithm aims to obtain an initial flower segmentation using the contrast dependent prior MRF (Markov Random Field) cost function, which is optimized by using graph-cut [5] based on general foreground and background color distributions. These distributions are learned by labeling pixels in few training images for each flower class as foreground or background. Then, these distributions are averaged over all classes. A generic flower-shaped model is then fitted to this initial segmentation in order to detect petals which have a loose geometric consistency using an affine invariant Hough-like procedure. The image region corresponding to the petals is used to obtain image-specific foreground and background color distributions which will be employed to obtain new color distributions by blending them with general ones. A new segmentation using MRF-based graph-cut is repeated using these new color distributions. The shape-model fitting and the segmentation will be iterated until convergence. In [6], Aydin and Ugur proposed a flower image segmentation algorithm based on ant colony optimization (ACO) [7]. First, RGB color space is converted to CIE-LAB color space in order to provide accuracy and perceptual approach in color difference determination. Second, the histogram of each color component is built (n bins for each component) to determine the center points of each bin and three “bin-centers” are obtained to generate all possible combinations of center points (n
                     3) which constitute the potential candidate cluster's center points. Then, ant colony optimization algorithm is applied to select optimum cluster center points. Finally, each pixel is classified in the nearest cluster. Recently, Fukuda et al. [8] proposed an automatic flower segmentation model based on graph-cut. They set the region penalty in graph-cut based on posterior probability, and not on likelihood as in the classical graph-cut. The posterior probabilities are determined by using AdaBoost [9] and saliency map [10]. In fact, the AdaBoost classifier provides a rectangular window representing the approximate flower location used to compute foreground and background color distributions (likelihoods). Then, the saliency map is used to provide prior probabilities. Using the Bayes' theorem, posterior probabilities are computed and integrated in graph-cut. More recently, Chai et al. in [11] addressed the co-segmentation of flowers which consists of segmenting a similar object (flower) from a pair of images. Their algorithm iterates at two levels: segmenting each image independently with GrabCut [12] at the pixel level and classifying the obtained superpixels into foreground and background using SVM learning algorithm.

To conclude, all of these approaches aim the automatic segmentation of flower images but there are some differences between them in terms of quality and time-consuming. In fact, the approach proposed by [2] doesn't extract all flower regions in the image, but isolates a region as the best description of the flower color. Besides, the fact of eliminating pixels belonging to non-flower color classes will fail with the images containing brown or gray flowers which will be considered in the evaluation of our algorithm. All these factors can reduce the segmentation performance of this method [2]. As for the method proposed by [3], the main disadvantage is the high computational time due to the research of the local cost paths. As demonstrated in [1], the method of Nilsback et al. achieves good performances (93% in quality measure [13]) but it is very slow to be run in real-time applications. In this method [1], fitting a generic shape model needs the determination of the corners, the petals and the center of the flowers which requires high computational time. The method proposed by [6] yields accurate segmentation (87% in quality measure) but we have no information about the processing time of the algorithm. The method reviewed by [11] achieves about 94% segmentation accuracy not on the original data split of the Oxford-17 flower dataset. As cited in [14], the algorithm proposed by iteChai11 needs more than 30s as run-time for segmentation. Finally, in [8], it is demonstrated that the proposed method can reduce the error rate in segmenting flower images with no mention of the consuming time. In terms of evaluation settings, all aforementioned flower segmentation methods have been performed on different datasets and evaluated with different accuracy measure as shown in Table 1
                     .

After this study, one can say that the model proposed by Nilsback et al. [1] yields the best performance of automatic flower image segmentation even if it is very time-consuming. That is why we take it as a state-of-the-art in order to improve it or to propose a new fast algorithm offering similar accuracy.

Recently, it has been an important interest in image segmentation approaches based on graph-cut [15–18]. Many works use graph-cut technique since it gives positive results on both medical and natural images [19,12]. Unfortunately, the standard graph-cut algorithm suffers from some limits: (i) it fails to give a smooth segmentation result by labeling some object pixels as background, or vice versa. (ii) It fails to define the desired boundary of the object. In fact, if object pixels have color distribution similar to the image background then these pixels will be labeled as background, and they won't be considered as part of the object. This failure can be explained by the fact that the traditional energy function to be minimized uses only the color information. In order to alleviate these problems, some solutions have been proposed to integrate a priori information other than color information. It has been demonstrated that the segmentation results of graph-cut methods can be improved by introducing novel constraints in the segmentation process such as shape constraints [20,21] and spatial constraints [22]. For example, authors in [21] proposed to incorporate a generic star-shaped prior in the energy function minimized by graph-cut. The shape prior is represented as an object mask form using the distance transform of the star shape. Although this method [21] gives encouraging results with the assumption that the center of the shape is given by the user, the extracted object shape tends to be star-aligned. While shape-based graph-cut works can be robust, they have a shortcoming to segment object with high shape variability [23].

In this paper, our aim is to modify the standard graph energy function to enhance the segmentation result. Therefore, we introduce spatial constraints through an additional term in the formulation of the graph-cut energy function. Thus, we obtain a better classification of pixels as foreground despite their high probabilities of belonging to the background if we consider only their colors and vice versa. We integrate also gradient information in the traditional graph energy function to better express boundary constraints. Consequently, true boundaries can be determined based on both gradient and color information. The rest of the paper is organized as follows: In Section 2, first, we present the mathematical theory of graph-cut technique and its use for energy minimization. Then, we introduce our proposed new energy function to be minimized by graph-cut. Finally, our automatic flower segmentation algorithm is described. In Section 3, we present the experimental results and the evaluation of the proposed segmentation method.

The graph-cut optimization technique proposed by [5] is one of the energy minimization algorithms which solve the object segmentation problem [24]. The algorithm defines a non-oriented graph G
                        =(V,
                        E) where V is a set of nodes corresponding to the image pixels and E is a set of non-oriented edges that connect those nodes. There are two terminal nodes called source s and sink t added to V in order to represent “foreground” and “background”, respectively. Each pixel node is connected to each one of the two terminal nodes by a link called t-link and each pair of neighboring pixels is connected by an edge called n-link. A cut is a partition of V into disjoint subsets S and T such that the source s is in S and the sink t is in T as shown in Fig. 2
                        .

The cost of the cut is defined as the sum of weights of all edges that are severed by the cut. The optimal segmentation is obtained by finding the cut that has the minimum cost among all cuts [25,26]. This graph-cut technique is used to minimize the standard energy function formulated as shown in Eq. (1).
                           
                              (1)
                              
                                 E
                                 
                                    f
                                 
                                 =
                                 
                                    
                                       ∑
                                       
                                          p
                                          ∈
                                          V
                                       
                                    
                                    
                                 
                                 
                                 
                                    R
                                    p
                                 
                                 
                                    
                                       f
                                       p
                                    
                                 
                                 +
                                 λ
                                 
                                    
                                       ∑
                                       
                                          p
                                          ,
                                          q
                                          ∈
                                          C
                                       
                                    
                                    
                                 
                                 
                                 
                                    B
                                    
                                       p
                                       ,
                                       q
                                    
                                 
                                 
                                    
                                       f
                                       p
                                    
                                    
                                       f
                                       q
                                    
                                 
                              
                           
                        where C is the set of pairs of adjacent pixels representing the 4- (or 8-) neighborhood system, f
                        =
                        f
                        
                           p
                           ∈
                           V
                         is the labeling function which associates each pixel p with a label f
                        p (f
                        p
                        =0 if it belongs to the foreground and f
                        p
                        =1 otherwise). The first term R
                        p is called the region or data dependant term which evaluates the penalty for assigning any label to a pixel p and represents the weight of t-link edges. The second term B
                        p,q is the boundary or smoothness term that measures the cost for two neighboring pixels p and q being different and represents the weight of n-link edges. The constant λ in Eq. (1) controls the relative importance of the boundary term versus the region term. Since I
                        p is the intensity of pixel p, the terms R
                        p(“
                        Obj
                        ”) and R
                        p(“
                        Bkg
                        ”) are equal to negative log-likelihood of foreground and background intensity models, respectively (Eqs. (2) and (3)).
                           
                              (2)
                              
                                 
                                    R
                                    p
                                 
                                 
                                    
                                       “
                                       O
                                       b
                                       j
                                       ”
                                    
                                 
                                 =
                                 
                                    R
                                    p
                                 
                                 
                                    
                                       
                                          f
                                          p
                                       
                                       =
                                       0
                                    
                                 
                                 =
                                 −
                                 lnPr
                                 
                                    
                                       
                                          I
                                          p
                                       
                                       |
                                       “
                                       O
                                       b
                                       j
                                       ”
                                    
                                 
                              
                           
                        
                        
                           
                              (3)
                              
                                 
                                    R
                                    p
                                 
                                 
                                    
                                       “
                                       B
                                       k
                                       g
                                       ”
                                    
                                 
                                 =
                                 
                                    R
                                    p
                                 
                                 
                                    
                                       
                                          f
                                          p
                                       
                                       =
                                       1
                                    
                                 
                                 =
                                 −
                                 ln
                                 
                                 Pr
                                 
                                    
                                       
                                          I
                                          p
                                       
                                       |
                                       “
                                       B
                                       k
                                       g
                                       ”
                                    
                                 
                                 .
                              
                           
                        
                     

The smoothness term B
                        p,q is commonly expressed by the Eq. (4).
                           
                              (4)
                              
                                 
                                    B
                                    
                                       p
                                       ,
                                       q
                                    
                                 
                                 =
                                 
                                    1
                                    
                                       dist
                                       
                                          p
                                          q
                                       
                                    
                                 
                                 exp
                                 
                                    
                                       
                                          −
                                          
                                             
                                                
                                                   
                                                      I
                                                      p
                                                   
                                                   −
                                                   
                                                      I
                                                      q
                                                   
                                                
                                             
                                             2
                                          
                                       
                                       
                                          2
                                          
                                             σ
                                             2
                                          
                                       
                                    
                                 
                              
                           
                        where σ is the standard deviation of the norm of the image gradient [27]. For grayscale images, I
                        p and I
                        q are the intensities of pixels p and q. For color images, they can be the RGB color vectors of pixels p and q. The dist(p,q) is the Euclidean spatial distance between the pixels p and q.

We propose a new modeling of the energy function to be optimized by graph-cut. As we search to consider not only color information, but also the spatial one, we add a new term to the energy function. We also improved the boundary constraints by modifying the standard smoothness term. The proposed energy function consists of two new terms, namely, data-consistent term D
                        p and modified boundary term 
                           
                              
                                 
                                    B
                                    ˜
                                 
                                 
                                    p
                                    ,
                                    q
                                 
                              
                           
                         as illustrated in Eq. (5).
                           
                              (5)
                              
                                 E
                                 
                                    f
                                 
                                 =
                                 
                                    
                                       ∑
                                       
                                          p
                                          ∈
                                          P
                                       
                                    
                                    
                                 
                                 
                                 
                                    D
                                    p
                                 
                                 
                                    
                                       f
                                       p
                                    
                                 
                                 +
                                 λ
                                 
                                    
                                       ∑
                                       
                                          p
                                          ,
                                          q
                                          ∈
                                          C
                                       
                                    
                                    
                                 
                                 
                                 
                                    
                                       
                                          B
                                          ˜
                                       
                                       
                                          p
                                          ,
                                          q
                                       
                                    
                                 
                                 
                                    
                                       f
                                       p
                                    
                                    
                                       f
                                       q
                                    
                                 
                              
                           
                        where
                           
                              (6)
                              
                                 
                                    D
                                    p
                                 
                                 
                                    
                                       f
                                       p
                                    
                                 
                                 =
                                 β
                                 
                                    R
                                    p
                                 
                                 
                                    
                                       f
                                       p
                                    
                                 
                                 +
                                 
                                    
                                       1
                                       −
                                       β
                                    
                                 
                                 
                                    S
                                    p
                                 
                                 
                                    
                                       f
                                       p
                                    
                                 
                                 .
                              
                           
                        
                     

The first term D
                        p is a barycentric combination (Eq. (6)) of a priori color-dependent term R
                        p (Eqs. (2) and (3)) and a priori spatial-dependant term S
                        p (Eqs. (7) and (8)). In fact, we add the spatial distribution to the color distribution in the segmentation process in order to improve the quality. The spatial terms S
                        p(“
                        Obj
                        ”) and S
                        p(“
                        Bkg
                        ”) represent respectively the penalities for assigning a pixel p to the classes “foreground” and “background” according to spatial distributions (Pr
                        
                           s
                        (p|“
                        Obj
                        ”) and Pr
                        
                           s
                        (p|“
                        Bkg
                        ”)). Thus, the spatial term S
                        p is defined as the negative log-likelihood of foreground and background spatial distributions such as described in Eqs. (7) and (8).
                           
                              (7)
                              
                                 
                                    S
                                    p
                                 
                                 
                                    
                                       “
                                       O
                                       b
                                       j
                                       ”
                                    
                                 
                                 =
                                 −
                                 ln
                                 
                                 
                                    
                                       Pr
                                       s
                                    
                                 
                                 
                                    
                                       p
                                       |
                                       “
                                       O
                                       b
                                       j
                                       ”
                                    
                                 
                              
                           
                        
                        
                           
                              (8)
                              
                                 
                                    S
                                    p
                                 
                                 
                                    
                                       “
                                       B
                                       k
                                       g
                                       ”
                                    
                                 
                                 =
                                 −
                                 ln
                                 
                                    
                                       1
                                       −
                                       
                                          
                                             Pr
                                             s
                                          
                                       
                                       
                                          
                                             p
                                             |
                                             “
                                             O
                                             b
                                             j
                                             ”
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

The spatial distribution of foreground is determined by the approximate spatial location R
                        
                           s
                         of the flower zone in the image. Inside the region R
                        
                           s
                        , the probability to consider a pixel as an object is maximum. Whereas, for every pixel outside the region R
                        
                           s
                        , this probability decreases according to a Gaussian of the distances dist(p,
                        R
                        
                           s
                        ) that separate the pixel p to the boundary of the region R
                        
                           s
                        . The formulation of these probabilities is given by Eq. (9).
                           
                              (9)
                              
                                 
                                    
                                       Pr
                                       s
                                    
                                 
                                 
                                    
                                       p
                                       |
                                       “
                                       O
                                       b
                                       j
                                       ”
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             1
                                          
                                          
                                             if
                                             
                                             p
                                             ∈
                                             
                                                R
                                                s
                                             
                                             .
                                          
                                       
                                       
                                          
                                             exp
                                             
                                                
                                                   −
                                                   
                                                      
                                                         dist
                                                         
                                                            
                                                               p
                                                               
                                                                  R
                                                                  s
                                                               
                                                            
                                                            2
                                                         
                                                      
                                                      
                                                         2
                                                         
                                                            σ
                                                            
                                                               R
                                                               S
                                                            
                                                            2
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             if
                                             
                                             p
                                             ∉
                                             
                                                R
                                                s
                                             
                                             .
                                          
                                       
                                    
                                 
                              
                           
                        where σ
                        
                           RS
                         is the standard deviation of the distances dist(p,
                        R
                        
                           s
                        ).

In standard graph-cut, the expression of the smoothness term is not enough to describe the boundary constraints. In fact, the area surrounding the flower is characterized by a large variety of colors and texture, but it is generally a repetitive texture pattern or elements. Using only the intensity difference in the boundary term (Eq. (4)) penalizes the discontinuity between two adjacent pixels within the same region. So, it is likely that the two neighboring pixels will not have the same label not only when they belong to different regions but also when they belong to the same textured area. Since adding the gradient norm difference will provide the intensity variation within a window of pixels, both of the two pixels will have a high value of gradient norm which will reduce the difference and increase the smoothness term in order to avoid the cut. The modified boundary term 
                           
                              
                                 
                                    B
                                    ˜
                                 
                                 
                                    p
                                    ,
                                    q
                                 
                              
                           
                         is then given by Eq. (10).
                           
                              (10)
                              
                                 
                                    
                                       
                                          B
                                          ˜
                                       
                                       
                                          p
                                          ,
                                          q
                                       
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                1
                                                
                                                   dist
                                                   
                                                      p
                                                      q
                                                   
                                                
                                             
                                             
                                                
                                                   B
                                                   
                                                      I
                                                      
                                                         p
                                                         ,
                                                         q
                                                      
                                                   
                                                   +
                                                   B
                                                   
                                                      G
                                                      
                                                         p
                                                         ,
                                                         q
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             if
                                             
                                             
                                                f
                                                p
                                             
                                             ≠
                                             
                                                f
                                                q
                                             
                                             .
                                          
                                       
                                       
                                          
                                             0
                                          
                                          
                                             otherwise
                                          
                                       
                                    
                                 
                              
                           
                        where BI
                        p,q and BG
                        p,q are two neighborhood interaction functions that can penalize intensity difference and gradient norm difference between two neighboring pixels p and q, respectively. By performing an optimization of a parameter combining BI
                        p,q and BG
                        p,q, we have not obtained an efficient estimation. So we chose to minimize the proposed energy function with no parameter between BI
                        p,q and BG
                        p,q. The term BI
                        p,q is obtained using the old boundary term (Eq. (4)) and is expressed by Eq. (11).
                           
                              (11)
                              
                                 B
                                 
                                    I
                                    
                                       p
                                       ,
                                       q
                                    
                                 
                                 =
                                 exp
                                 
                                    
                                       
                                          −
                                          
                                             
                                                
                                                   
                                                      I
                                                      p
                                                   
                                                   −
                                                   
                                                      I
                                                      q
                                                   
                                                
                                             
                                             2
                                          
                                       
                                       
                                          2
                                          
                                             σ
                                             I
                                             2
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

The additional term BG
                        p,q used to improve boundary regularity is defined by Eq. (12).
                           
                              (12)
                              
                                 B
                                 
                                    G
                                    
                                       p
                                       ,
                                       q
                                    
                                 
                                 =
                                 exp
                                 
                                    
                                       
                                          −
                                          
                                             
                                                
                                                   
                                                      
                                                         ∇
                                                         p
                                                      
                                                   
                                                   −
                                                   
                                                      
                                                         ∇
                                                         q
                                                      
                                                   
                                                
                                             
                                             2
                                          
                                       
                                       
                                          2
                                          
                                             
                                                
                                                   σ
                                                   I
                                                
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where ‖∇
                           p
                        ‖ and ‖∇
                           q
                        ‖ are gradient norms of the image at neighboring pixels p and q.Since we have optimized this new energy function using graph-cut, we have called it “Extended graph-cut”.

We propose a new coarse-to-fine method which devises a two-level scheme to segment flower image. In our work, we use general foreground and background color distributions (G
                        
                           fg
                         and G
                        
                           bg
                        ) which are learned as in [1] using the Gaussian mixture model [28]. At the first level, our goal is to perform a coarse segmentation based on the minimization of the proposed energy function by the graph-cut. At the second level, we enhance the obtained result by refining color distributions in order to apply the standard graph-cut. In the next section, we will detail the components of the proposed algorithm followed by the complexity analysis. The proposed automatic segmentation framework is depicted in Fig. 3
                        .

At this first level, a huge part of the background is eliminated in order to approximate the location of the flower zone. The idea is to select automatically several background seeds: two seeds are selected in each window corner of the image as mentioned in Section 2.3.1.2, and then we gather all pixels similar to those seeds in terms of color. The obtained area will correspond to a large part of the background. The remaining area of the image will contain necessary the flower region and will be served to determine the spatial model for applying extended graph-cut. The process of this level is described by Algorithm 1.
                              Algorithm 1 Coarse segmentation ()
                              
                                 
                                    
                                       Input: general background G
                                          
                                             bg
                                           and foreground G
                                          
                                             fg
                                           color models

-Input image I
                                          
                                             img
                                          
                                       

Output: coarse segmentation result.
                                             
                                                1.
                                                Estimate background probability density function PDF
                                                   
                                                      bg
                                                    of the border of I
                                                   
                                                      img
                                                   .

For each corner={pixel(x, y)} in I
                                                   
                                                      img
                                                    do

set 
                                                      
                                                         
                                                            seed
                                                            1
                                                            
                                                               x
                                                               y
                                                            
                                                         
                                                         ^
                                                      
                                                      =
                                                      
                                                         
                                                            arg
                                                            
                                                            m
                                                            a
                                                            
                                                               x
                                                               
                                                                  pixel
                                                                  
                                                                     x
                                                                     y
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                      
                                                      P
                                                      D
                                                      
                                                         F
                                                         
                                                            b
                                                            g
                                                         
                                                      
                                                      
                                                         
                                                            pixel
                                                            
                                                               x
                                                               y
                                                            
                                                         
                                                      
                                                   
                                                

set 
                                                      
                                                         
                                                            seed
                                                            2
                                                            
                                                               x
                                                               y
                                                            
                                                         
                                                         ^
                                                      
                                                      =
                                                      
                                                         
                                                            
                                                               arg
                                                               
                                                               max
                                                            
                                                         
                                                         
                                                            pixel
                                                            
                                                               x
                                                               y
                                                            
                                                         
                                                      
                                                      
                                                         G
                                                         
                                                            b
                                                            g
                                                         
                                                      
                                                      
                                                         
                                                            pixel
                                                            
                                                               x
                                                               y
                                                            
                                                         
                                                      
                                                   
                                                

End for

Set I
                                                   q
                                                   =Quantization(I
                                                   
                                                      img
                                                   , 12).

Label the pixels from I
                                                   q having the same color as background seeds to 0 and the other pixels to 1.

Compute Euclidean distance map EDM of the previous result (step 7).

Build object and background spatial models using EDM.

Use graph-cut algorithm to segment I
                                                   
                                                      img
                                                    using spatial and color models.

Return the coarse segmentation result Res
                                                   coarse.

We have estimated a specific background distribution calculated from the image to be segmented. So, as illustrated in Fig. 4
                              , we divide our input image into two areas, an internal area (IA) representing the kernel of the image and an external one (EA) representing the border. We can estimate the specific background distribution by computing the multivariate Gaussian distribution in EA because we made the assumption that the internal area will contain the flower region and the external one will correspond to the background.

In each corner of the input image, we choose two pixel seeds within 20×20 window, as shown in Fig. 4. The first and the second seeds are chosen among the pixels contained in the window corner as they have the maximum background likelihood value based on background color distributions, respectively, learned by GMM (G
                              
                                 bg
                              ) and computed on the external area EA (PDF
                              
                                 bg
                              ).

Due to complex mixtures of colors in natural scenes and the fact that texture features tend to be ambiguous and not discriminative enough [29], image color quantization is performed in order to reduce the color number. Consequently, we can easily reduce the visual difference between pixels having similar colors as shown in Fig. 5
                              .

In order to perform the color image quantization, we compute the class-map of the image by changing each original pixel color to his corresponding quantized value. Typically, the number of levels needed for the quantization of natural scenes is between 10 and 20 [30]. We have used an empirical value of quantization level which is fixed to 12. Fig. 5 shows some results of 12-level color quantization on four images from “Daffodil”, “Iris”, “Pansy” and “Fritillary” classes.

This step consists in gathering the pixels having the same color level as the background seeds. As shown in (Fig. 6b), the gathered pixels represent a background area Z
                              
                                 bg
                               while the remaining part Z
                              
                                 o
                               contains the flower and it is considered its spatial location.

A distance map [31] of the previous obtained result is computed (Fig. 6c) in order to provide a prior probability of the spatial object model which will be incorporated in the energy function minimized by graph-cut. This map labels each background pixel with the shortest Euclidean distance to the nearest foreground pixel, unlike the foreground pixels which will take the value zero as a label. Fig. 6c illustrates the Euclidean distance map of the background pixels gathering result.

The final step at the coarse segmentation is the application of the extended graph-cut. Fig. 6d shows an example of the obtained result on two flower images, namely “crocus” and “iris”. The result may not be perfect, but it is sufficient to detect a large part of the object with some parts of the background and a few object pixels misclassified as background. In the next section, we will explain in simple steps how to refine this result. The modified boundary term of the energy function optimized by the extended graph-cut enhances the boundary smoothness.


                              Fig. 7
                               shows how the integration of the gradient information improves the coarse segmentation result. Using the new boundary term, we obtain better coarse segmentation (Fig. 7c) than the result obtained using the standard boundary term (Fig. 7b).

At this level, we try to refine the color distributions used at the coarse segmentation level. Then, we apply standard graph-cut using these updated color models. The pseudocode of the fine segmentation is shown in Algorithm 2.
                              Algorithm 2 Fine segmentation()
                              
                                 
                                    
                                       Input: coarse segmentation result Res
                                          coarse
                                       

Output: fine segmentation result
                                             
                                                1:
                                                Compute color histogram BCH of the pixels classified as background in Res
                                                   coarse.

Threshold BCH to identify dominant background colors.

Find the set of object pixels OP from Res
                                                   coarse that has a dominant background colors.

Modify the labels of all pixels in OP in order to be assigned as background.

Reestimate the object h
                                                   object and background h
                                                   background color models according to the labeling result obtained in step 4.

Use graph-cut algorithm to segment I
                                                   
                                                      img
                                                    using the updated color models h
                                                   object and h
                                                   background.

Return the fine segmentation result Res
                                                   final.

Considering the result of the coarse segmentation, we identify the background dominant colors by computing the background color histogram BCH and thresholding it. These colors will be used to rectify the classification of background pixels labeled as foreground in the coarse segmentation result. So, we modify the labels of these pixels classified as foreground and having a dominant background color. The obtained result is denoted by “image
                              
                                 cleaned
                              ” where the foreground pixels form a region denoted by “object
                              
                                 cleaned
                              ”.

In order to apply standard graph-cut segmentation, we have to re-estimate color distributions. These distributions should be finer than those used at the coarse segmentation level. We can rectify the color likelihoods used at the coarse segmentation level by updating general color models. The new foreground color model h
                              
                                 object
                               is a barycentric linear combination of the color distribution h
                              
                                 OC
                               of the object
                              
                                 cleaned
                               and the general foreground distribution G
                              
                                 fg
                               as mentioned in Eq. (13).
                                 
                                    (13)
                                    
                                       
                                          h
                                          object
                                       
                                       =
                                       γ
                                       ∗
                                       
                                          h
                                          
                                             O
                                             C
                                          
                                       
                                       +
                                       
                                          
                                             1
                                             −
                                             γ
                                          
                                       
                                       ∗
                                       
                                          G
                                          fg
                                       
                                       .
                                    
                                 
                              
                           

In some cases, the flower is not accurately cut at the coarse segmentation level. In fact, the foreground pixels marked as background inhibit the fine segmentation to extract the flower object accurately if we include them in the background color model. To avoid this, we consider an uncertainty zone around the object
                              
                                 cleaned
                               by applying morphological dilation using a 5×5 disk-shaped structuring element. We called object
                              
                                 cleaned
                                 −
                                 dilated
                               the new object region. The background model h
                              background is, therefore, a barycentric linear combination of the general background distribution G
                              
                                 bg
                               and the color distribution h
                              
                                 NOC
                               of the pixels which are not part of the region object
                              
                                 cleaned
                                 −
                                 dilated
                               (Eq. (14)).
                                 
                                    (14)
                                    
                                       
                                          h
                                          background
                                       
                                       =
                                       γ
                                       ∗
                                       
                                          h
                                          
                                             N
                                             O
                                             C
                                          
                                       
                                       +
                                       
                                          
                                             1
                                             −
                                             γ
                                          
                                       
                                       ∗
                                       
                                          G
                                          
                                             b
                                             g
                                          
                                       
                                       .
                                    
                                 
                              
                           

Since the new color distributions provide more accurate modeling of both background and foreground than those used at the first level, we give them more confidence with a high weight. In our experiments, the γ empirical value used is 0.8.

The standard graph-cut is then performed using the updated color models. At this step, the use of the extended graph-cut makes hard to precisely segment the flower even if the object and background models have been updated. In fact, the background pixels contained in the flower zone will be used to compute the object spatial model and they will be classified again as foreground due to their high object probabilities. Therefore, we use the standard graph-cut algorithm at the fine segmentation level. Fig. 8
                               shows how the fine segmentation accurately cleans the coarse segmentation result.

In fact, it is difficult to segment the center pixels of some flower (e.g. Sunflower), having a dark background color (e.g. brown and black), as foreground. That's why we fill the center hole into the foreground.

Our method applies, at each segmentation level, the graph-cut technique. Since we have two levels, the computational complexity of our algorithm will be 2×
                           O(m×n2
                           ×|c|). It is twice the running time complexity of the max-flow min-cut algorithm in the worst case; where n is the number of nodes, m is the number of edges and |c| is the cost of the minimum cut in the graph. However, the complexity of the algorithm in [1] is NB
                           ∗
                           O(m×n2
                           ×|c|) where NB is the number of iterations required until convergence of the algorithm. In [1], it was mentioned that the algorithm can need five iterations to converge. This means that NB is greater than or equal to 5 in the worst case. Since the flower images in the Oxford database vary in size, at least 500×500pixels, the constant NB which is relatively high (≥5) increases the computational time for large images. Thus, our proposed algorithm is faster than that proposed in [1].

@&#EXPERIMENTAL RESULTS@&#

We have implemented the segmentation algorithm using MATLAB software. We used maxflow library
                        1
                     
                     
                        1
                        
                           The library is available online at 
                           
                              http://vision.csd.uwo.ca/code/.
                      for computing the mincut/maxflow of a graph. Our approach was evaluated on the Oxford flower dataset
                        2
                     
                     
                        2
                        
                           The dataset is available online at 
                           [32].
                      as it was used in [1]. This dataset contains 17 species of flower with 80 images per category having large variations in viewpoint, scale and illumination. There is a ground truth segmentation provided with this dataset. As in [1], we will remove four classes, i.e. “snowdrops”, “lilies of the valley”, “cowslips” and “bluebells”, because they either haven't sufficient images or haven't segmentation ground truth. Therefore, it remains 753 images in the dataset representing 13 classes which are split into a training set containing 260 images (20 images per class) and a test set containing 493 images. We have also tested our method on the alternative data split used in [11] having 15 training and 65 test images per class.

The evaluation protocol proceeds as follows: first, we automatically estimate optimum values of our method's parameters using the training set and the ground truth segmentations. Next, we present flower segmentation results on the test set and evaluate our proposed method. Finally, our method is confronted with other existing methods. We first compare our method to the co-segmentation method of Chai et al. [11] for the alternative data split (Table 1). Then, we provide a comparative study in terms of performance and computational complexity of our segmentation results with those obtained by our reimplementation of Nilsback's method [1] for the original data split. In fact, since authors [1] did not provide the source code of their method, we reimplemented the corresponding version, denoted as Nilsback's reimplemented method and, in doing so, we choose the different techniques that they did not mention in the paper [1]. For example we have used the Gaussian Mixture Models (GMMs) [28] for learning the general foreground and background distributions. Furthermore, in order to detect corners in [1], it is mentioned that two parameters, namely the worm length and the minimum distance from a potential boundary point to the straight line between the worm's head and tail, are required. However, only an interval of values for the first parameter (worm length) is provided as a variable value to ensure optimal performance. That's why, in our reimplementation of the Nilsback's method we have fixed the worm length parameter to 25 empirically and we have used the Phillips–Rosenfeld algorithm [33] to set the value of the second parameter (minimum distance) which is calculated based on the fixed worm length. Indeed, we have not many other values of other several optimized parameters to execute the reimplemented algorithm.

Our method depends on some parameters that must be set to perform segmentation. The parameters λ and β are estimated to minimize the energy function via graph-cut (Eq. (5)) and the background threshold parameter bg
                        
                           th
                         is used to determine the background dominant colors. The best values of the parameters are chosen so that they maximize a segmentation quality measure called the overlap score OS, between the segmentation and the ground truths. The performance measure OS is defined as indicated in Eq. (15).
                           
                              (15)
                              
                                 OS
                                 =
                                 
                                    
                                       True
                                       
                                       foreground
                                       ∩
                                       Segmented
                                       
                                       foreground
                                    
                                    
                                       True
                                       
                                       foreground
                                       ∪
                                       Segmented
                                       
                                       foreground
                                    
                                 
                                 .
                              
                           
                        
                     

It is known [26] that a low λ value leads to an over-segmentation and a high value of λ gives an under-segmentation. Fig. 9
                            shows the impact of λ on segmentation results. For this reason we have to be careful about the choice of the parameter λ.

In order to find the optimal λ value, we performed segmentation of the 260 training images using different values of λ in the range from 1 to 10 by step 1 and also in the range from 10 to 100 by step 10 as shown in Fig. 10
                           .

For each fixed λ, we compute the average overlap score over all classes. Fig. 10 illustrates that the average overlap score starts at 78.2% for λ
                           =1, then goes up to 79.3% for λ
                           =4 and finally decreases until it reaches 67.2% for λ
                           =100. So, we fixed the value 4 for the parameter λ as the best value that corresponds to the highest average overlap score over all classes.

Because the parameter β is the weight of the spatial model which was used only at the first level (coarse segmentation), it is estimated according to this level result. Fig. 11
                            shows coarse segmentation results of two different images under different values of β.

The parameter β have to ensure a trade-off between the color model and the spatial one in order to minimize the number of misclassified background pixels considered foreground as illustrated in Fig. 11. To choose the best β, we performed segmentation of 260 training images using different values of β in the range from 0.1 to 0.92. Fig. 12
                            shows the evolution of the segmentation quality according to the values of the parameter β.

As illustrated in Fig. 12, when the value of β is less than 0.9, the segmentation quality has increased sharply. Then, it falls slightly from β equal to 0.9. Thus, we fix the parameter β to 0.9, which corresponds to the first value with which the segmentation quality has decreased.

Finally, we have estimated the best background threshold used to find the dominant background colors from the coarse segmentation result as shown in Fig. 13
                           .

If bg
                           
                              th
                            is small, foreground colors will be considered dominant background colors; on the other hand, if bg
                           
                              th
                            is large, we cannot obtain a satisfactory list of dominant background colors. By varying this parameter, we note that the best segmentation is obtained with bg
                           
                              th
                            fixed at 80 as illustrated in Fig. 13.

After setting the parameters, we applied our method on the test set. Fig. 14
                         shows some flower images and their segmentation results at each level.

Our method works under an assumption that the coarse segmentation result should rectify the distributions which will be used at the fine segmentation level. Yet, this assumption may not be detained in some cases when a large part of the foreground is similar in color to the background pixels. As shown in Fig. 14, our method fails to separate the flower from the background under such cases.

Our proposed segmentation algorithm is executed on a machine with an AMD Athlon processor. Table 2
                         shows execution times of the algorithm on different images (presented in Fig. 14). The running time required to perform our algorithm is around 40–55s, while the reimplemented Nilsback's algorithm takes far more than 1min.

In order to objectively evaluate the accuracy of our segmentation method, we compute the average overlap score for each flower class as indicated in Fig. 15
                        .

The red bars represent the confidence intervals. Upper bounds are the overlap scores obtained in the best case and lower bounds are the overlap scores obtained in the worst case. The best and the worst overlap scores are achieved when we choose the 20 best and the 20 worst segmentation results from each class, respectively. In the best case, the average of our segmentation score is 85.28%. As Fig. 15 indicates, the average overlap score (OS) can reach 92% and it is never below 55%. This score indicates that our method offers encouraging and good results for automatic flower segmentation. Our method gives the worst results for segmenting “Crocus” class images due to having flower colors similar to those of the background.

In order to illustrate the contributions of our proposed flower segmentation system, we performed these four tests at the coarse segmentation level using obviously the classical graph-cut at the fine segmentation level:
                           
                              •
                              Test using the standard graph-cut;

Test using the standard graph-cut with the modified boundary term;

Test using the standard graph-cut by incorporating the spatial term;

Test using the standard graph-cut with the additional spatial term and the modified boundary term.

The results of these experimental tests are shown in Fig. 16
                        . We note that the quality of segmentation is improved thanks to the addition of spatial constraints and the modification of the boundary term in the formulation of the energy function minimized by graph-cut. For example, the fact that flower leaves for “Fritillary” and “Pansy” classes containing an obvious difference in color makes the fail of the segmentation using only the classical graph-cut (24% for “Fritillary” and 56% for “Pansy”). It can be seen (Fig. 16) that the integration of the gradient information in the boundary term improves the overlap score for “Fritillary” and “Pansy” classes to 67% and 73%, respectively. Furthermore, adding the spatial term improves the overlap score for each flower class. Thus, the segmentation performance is improved over most flower classes using our proposed algorithm.

In order to place our segmentation results, we compare experimentally our work to those of Chai et al. [11]. Indeed, we obtain an average OS equal to 77% whereas the average OS for Chai et al. method is about 94%. Although it can be noticed that our method achieves 91.96% segmentation accuracy with the alternative datasplit used in [11] in the best case. We compare also our method to Nilsback's method [1] in terms of segmentation quality and running time. We note that the unavailability of the parameter values and the adopted techniques in the implementation does not allow us to have exactly the same results obtained in [1]. So, we compare our proposed method segmentation results with those obtained by our Nilsback's reimplemented method (Fig. 17
                        ). As shown, the proposed method can give more accurate segmentation results for a variety of flower images. In Fig. 18
                         we also compare our segmentation results with those obtained by the method originally implemented by Nilsback [1] available online at [32]. We note that our method delivers again accurate segmentation results with images that fail using Nilsback's method [1].

We compute the average overlap score for each flower class by the two methods as indicated in Fig. 19
                           . Comparing our values with those obtained by the reimplemented method of Nilsback et al., we note that our results are better for eight classes, very similar for one class (“Dandelion”) and lower for four classes (“Crocus”, “Iris”, “Daisy” and “Windflower”). So, our method can achieve better results in most cases.

We have also evaluated the performance of our method in terms of computational time in comparison with the reimplemented method of Nilsback et al. [1]. We notice that the running time of our algorithm is faster than that of Nilsback as shown in Fig. 20
                           . Our running time is obviously justified by our time complexity previously calculated. So, we consider that the method presented in [1] is not well suited to such real-time application because it requires extraprocessing time to achieve accurate segmentation (over one minute). Our method is then more effective in practical.

@&#CONCLUSION@&#

In this work, we present a method for flower segmentation based on the minimization of MRF energy by graph-cut. We have made two important contributions. At first, we have added a spatial term in the energy function. Then, we have incorporated the gradient information in the boundary term of the energy function. In summary, we have described a two-level segmentation scheme. At its first level, the scheme extracts a large part of the object from the input image and at the second one refines the result obtained above. The experimental study shows that our proposed method has a high accuracy and overcomes the limitations of the standard graph-cut. While our accuracy is not greater than that of the studied method of Nilsback et al. [1], our method is more efficient and more faster. Future work will concentrate on generalizing our method by testing it on another benchmark database of flower images.

@&#ACKNOWLEDGMENTS@&#

We thank Vladimir Kolmogorov for graph-cut software which we used in our work.

@&#REFERENCES@&#

