@&#MAIN-TITLE@&#Purely vision-based segmentation of web pages for assistive technology

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We use a novel vision-based method to analyze the layout of a web page.


                        
                        
                           
                           Our method produces a hierarchical segmentation of the page reflecting its structure.


                        
                        
                           
                           Vision-based methods are not sensitive to implementation language or complexity.


                        
                        
                           
                           The visual presentation of a page provides rich information about semantic structure.


                        
                        
                           
                           This structure can help create modified presentations for users with assistive needs.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Webpage segmentation

Edge detection

Assistive technology for the visually impaired

the ageing population and those with cognitive deficit

@&#ABSTRACT@&#


               
               
                  We propose a system for analyzing the structure of a web page based on purely visual information, rather than on implementation details. This is advantageous because regardless of the complexity of the underlying implementation, the web page is designed to be easily interpreted visually. Our method produces a hierarchical segmentation reflecting the visual structure of the rendered page. This rich information about the presentation of the web page can be used by other systems which produce alternate presentations more suitable for users with visual or cognitive disabilities.
               
            

@&#INTRODUCTION@&#

We begin with a general overview of our proposed research into vision-based segmentation of web pages, clarifying the anticipated benefit for users with assistive needs. We then focus more specifically on the computer vision research, highlighting the main elements of what we are proposing and their intended contribution.

@&#OVERVIEW@&#

Today there are an increasing number of users on the Internet with specific assistive needs. Visually complex webpages with dense, information-rich structures can be difficult for these users to navigate. In this paper, we present research in support of providing systems that facilitate access to web content for users with visual or cognitive impairments. At the core of our solution are computer vision algorithms, applied to produce an effective segmentation of webpage content, which can then be employed to deliver alternative, useful depictions of those webpages to users with assistive needs.

Our approach is one that aims to provide semantically-rich representations of web page content structure by treating web pages as images to be interpreted using computer vision techniques. In developing this framework, we reflected upon its potential value for a wide range of users with challenges requiring assistive technology. Our initial motivation was to support improved audio screen readers for users who are visually impaired [1]. But our system as designed could also support selective presentation of full content for users, of particular benefit for reducing extraneous elements and emphasizing central elements instead. This may be of particular use for users such as the elderly.

We first present the proposed algorithms for segmentation, clarifying the novelty of the computer vision techniques and presenting a validation of the methodology as sound and effective in capturing webpage content. We then examine a host of user communities who may be well served by a system depicting web content that is guided by our algorithms. We also outline some directions for future research, both in extending the technical solution that is offered and also with respect to conducting user studies to demonstrate usability. In all, we emphasize the value of providing a solution that is not tied to the implementation and underlying code of the webpages, discussing how approaching the challenge from a computer vision standpoint offers important contributions for assistive technology.

The objective of our vision-based method is to determine the hierarchical structure of a web page layout using visual cues, without reference to the implementation of the web page. Our intention is for this system to serve as a back-end system, supporting front-end systems that reformat the web page for presentation to the user. Many such front-end systems, such as screen readers, exist today. Existing back-end systems for depicting web pages may use visual cues, but extract them from visual attributes defined in the code. As code-based analysis is brittle, we want to instead leverage the image of the rendered page. We believe that this approach has three principal advantages:

                           
                              1.
                              It does not depend on the quality or implementation language of the underlying code (provided that the browser’s rendering engine can handle it).

It allows for semantically significant divisions within images, Flash objects, and other entities that are treated monolithically in HTML or CSS code.

Perhaps most importantly, it analyzes the web page’s structure using as evidence the page designer’s view of the page (the appearance of the rendered web page)
                                    1
                                 
                                 
                                    1
                                    This is similar to the view of Saund and Moran, who argued (in the context of an image editor) that is desirable to consider visual objects that reflect what users are likely to perceive and want to manipulate [2].
                                 .

Essentially, the advantage of an image-based analysis is that it depends not on the details of how the visual structure of the page is produced, but rather on what the visual appearance is. It uses exactly the information seen by users who do not require assistive technology to make the same type of inference about the structure of the page contents. In this paper we present a robust, extensible Bayesian framework, grounded in a formal model of web page appearance, for performing image-based segmentation of a web page, together with a comparison between the results of such an analysis and more traditional code-based techniques. As we shall see, assistive technology systems that rely on source code-based segmentation algorithms face challenges when there are, for example, images or Flash objects in the page. These algorithms would only be able to treat these objects atomically, and would be unable to detect their internal structure. As a result, users who require distracting content to be suppressed would not be able to select only parts of these objects for display.

@&#RELATED WORK@&#

Although relatively few researchers have attempted to use vision-based segmentation of web pages to support screen reader technology, there has been considerable work on using vision-based page segmentation in information retrieval and optical character recognition systems. This section examines some prominent or otherwise interesting techniques used in these and other fields (which could constitute the foundation of a back-end system designed to support effective depiction of web pages for users with assistive needs), with a focus on showing the range of techniques used rather than describing every detail of each approach. We also review HCI and accessibility research on assistive web transformation.

Our use of computer vision falls into two principal categories: edge detection and image segmentation. Our specific techniques are chosen to suit our model of the appearance of a web page. Edge detection and image segmentation are both very broad fields, primarily focused on natural images; here we discuss only those aspects of these problems related to our project, rather than trying to provide a comprehensive account. We discuss image segmentation as applied to documents in Section 2.2; edge detection and other fundamental techniques from computer vision related to our system are discussed in Section 5 (as these are better appreciated after the presentation of the details of our system, for comparisons).

One common case of document structure analysis occurs in optical character recognition (OCR) technology. It is necessary for OCR to find the regions of text in the image. In this case, the objective is to find blocks of texts to be read by other parts of the system. Note that our objective is to produce semantically significant groups, whether of text, photographs, icons, or other items. Segmentation into very fine units, such as lines of text in a paragraph, is neither necessary nor desirable for our purposes.

Early OCR segmentation work generally used binary images with rules for building up blocks of text or image from groups of pixels. Shih et al., for example, use a method based on bridging small horizontal and vertical gaps, and then finding connected components [3]. Zlatopolsky described a more complex segmentation system which used different heuristics for different types of merging and different types of object (e.g. line segments and text blocks) [4].

A recent survey of document segmentation (for OCR and other applications) [5] classifies work in this area into methods based on projection, on smearing, on connected components, and on analysis of the background. In projection, pixels are summed along a line of sight (generally horizontally or vertically relative to the document axes). Anomalous peaks or troughs in the resulting one-dimensional projections are presumed to be caused by components in the document [5]. Smearing methods attempt to expand the foreground to fill each component. Methods based on morphological operations are a generalization of smearing methods [5]. One notable method using connected components is the construction of a graph of connected components with edges weighted by the Euclidean distance between component centroids; subtrees of the minimum spanning tree of this graph are considered to be page regions [5]. Methods using the background to define components rather than the foreground include the white tiles method, which finds areas of whitespace, the edges of which are used as separators [6]. All of these methods are distinct from our segmentation method, which is based on probabilistically finding edges and alignments of edges.

Visual features have been used to segment web pages for over a decade. A useful overview of some of the different efforts in doing so is offered in [7], which clarifies that vision-based approaches segment the web page from the browserside perspective as it is rendered. Most of the existing web oriented segmentation methods use different combinations of code and visual features. A large group of methods use some kind of vector representation of the rendered page, although they work with the rendered content elements and their visual features, they abstract from the actual DOM (document object model) implementation rather than from a bitmap image of the resulting page. In 2003, Cai et al. proposed the Visual Page Segmentation (VIPS) algorithm [8]. VIPS uses the visual properties defined for nodes in the DOM tree to determine if they should be further divided in the visual segmentation or not. The properties include HTML tags, colors, and bounding boxes sizes for the node and any potential child nodes. Many other projects have used the VIPS algorithm as a starting point, including work by Petasis et al. [9] and by Song et al. [10]. This paper recognizes the importance of the visual properties of page regions in determining their position in the page structure, but it relies entirely on the DOM tree to produce a segmentation (along with heuristics in order to decide which elements actually create a visual segment). As such, it is not independent of implementation details and cannot extract structure at any level finer than the DOM tree (within an image, for example).

Cesarini et al. described a tree structure designed to represent the series of horizontal and vertical divisions found in a typical document; they call this structure the modified X–Y tree [11]. A standard X–Y tree is built from the top down by recursively dividing detected regions; regions are divided through whitespace or lines, and the direction of division (horizontal or vertical) alternates at successive levels in the tree (a node with exactly one child can be used to represent consecutive divisions along the same axis). Thresholds on the dimensions of lines and whitespace regions are used to avoid dividing at meaningless positions. In a modified X–Y tree, each node is associated with features describing its type and coordinates in the document image, and additional edges between nodes in the tree are used to represent adjacency relationships. This is a richer model, although like the standard X–Y tree it requires that separators extend across an entire node.

Chen et al. describe a method for using a generalized hidden Markov model (GHMM) for classifying regions in a web page [12]. An ordinary HMM is a probabilistic graphical model in which there is an underlying state and an observation at each discrete time step in an evolving system; the observation and next state depend probabilistically on the current state. In a GHMM, the single observation of an HMM is replaced by a set of observations, allowing a set of features to be used rather than a single feature. The states represent the types of regions in the web page, and the observations represent the relevant features of the regions. The problem of classifying these regions can be viewed as the problem of finding the sequence of states which is most likely, given the observations of visual properties of regions and the dependency of each state on the previous one. A significant problem that Chen et al. needed to overcome in creating a GHMM of web page organization is that a web page has a two-dimensional layout whereas a GHMM assumes a one-dimensional sequence of states. The authors used a depth-first traversal of the tree of regions to create such a sequence of states. Our system is focused on the generation of a segmentation tree based on visual information, rather than on region classification (although we discuss extending our model to include classification in Section 7).

It is also worth mentioning the “Prefab” GUI analysis system [13]. This system uses visual detection of interface elements to alter the interface. As in the case of our work, the authors make stronger assumptions than are used in general machine vision algorithms to reflect their model of the image of an interface. Prefab takes advantage of the natural tendency toward repetition of GUI elements to learn prototype-based representations of elements, whereas our system uses a model of the appearance of boundaries between regions to detect their layout.

Some work has been done in using visual properties of web page elements (obtained from the source code of the page) to improve screenreader performance. Krüpl-Sypien et al. describe a bottom-up, rule-based system which represents a web page using a layered model, similar to the layered model of a network stack [14]. At the lowest level is the page source code; progressively higher levels contain discrete objects such as words or images, collections of objects, and logical divisions of the page. The top layer provides a model for navigating through the page on multiple “axes” of logical relationships between objects. This model has been applied to screen readers as a part of the Vienna University of Technology’s Project ABBA (Advanced Barrier-free Browser Accessibility) [15]. Our proposed approach differs in that it is intended to be independent of the implementation of the web page, using only the rendered web page and not the source code, and to avoid the use of rigid, hand-coded rules in favor of learned characteristics and classifiers.

Computer vision has also been used in other assistive contexts. A sample use is to aid the elderly in detecting obstacles in the environment and in navigation; this may be a fully general system [16] or a specialized system detecting a class of objects (such as steps down [17]), and may be designed specifically for users in a specialized group such as wheelchair users with dementia [18]. Computer vision may also support the blind and users with other visual impairments, for example in a large system such as RegionSpeak, which crowdsources visual descriptions of objects in an image, but uses image stitching to provide suitable context for the annotators [19]. Other research focused on effective audio representation of environments for blind users, to allow them to ”see” with sound, is offered in [20]. Although interesting and important, these methods operate on natural scenes, and so are quite distinct from our method, which works with generated images of web pages.

The complexity of modern webpages creates substantial accessibility barriers across a number of physical, sensory, and cognitive disabilities. For example, screen reader users can find it difficult and laborious to reach desired content in a page, while individuals with cognitive, reading, or learning disabilities can find it difficult to process cluttered and content-heavy pages. In general, individuals with disabilities encounter many of the same barriers to accessing the web as non-disabled users historically faced when using small mobile devices [21,22]. While guidelines exist for making webpages more accessible, research has shown that these best practices are often ignored [23,24]. In response to these challenges, researchers have explored a number of approaches to segmenting pages and reducing page complexity.

Most work to date has focused on improving accessibility for visually impaired users accessing the web via screen readers, although there is much potential for leveraging these same core ideas in other domains. Asakawa and Takagi [25] developed a system that transcodes existing Web pages with manual annotations to add structure that can be used for reordering visually fragmented grouping according to performance. As manually annotating pages is labor intensive, follow-on work focused on automatically annotating similarly structured pages across multi-page websites [26]. Yesilada et al. [27] use a structured ontology to identify visual segments in a web page and re-engineer pages to facilitate navigation using a screen reader. Mahmud, Borodin, and Ramakrishnan [28] introduced CSurf, a system that uses web page partitioning techniques from natural language processing and machine learning to capture the context of a link to enable the screen reader to start reading the next page starting from the most relevant section. HearSay [29] extends this approach to additionally facilitate movement between the segments of a page.

Where work has targeted cognitive accessibility, it has focused more on the viability of potential supports than on the technical challenges associated with implementing them. Lee explored the general acceptability of a number of scaffolds for managing visual complexity, including visually highlighting core content, reading content aloud, and rendering page segments as buttons [30]. While these features received positive support from two groups of informants (people working with seniors and working with individuals who have developmental disabilities), the paper did not address how these supports could be implemented at large. IBM developed an enhanced open-source browser that provided a wide range of accessibility transformations, including—through manipulations of the webpageâÇÖs Document Object Model (DOM)—clutter-reducing features such as adjusting the font-size and line-spacing, stopping animations, hiding backgrounds, and reformatting pages for a single-column layout [31]. While these enhancements can help users to manage page complexity, they do not directly address the problem of excess content. Kurniawan et al. compared five different personalized web page presentations [32]. While the tools included in their evaluation were quite different from the approach we consider here, two important lessons emerge: (1) personalization places extra demands on the user which must be balanced against the benefit gained from the personalization, and (2) approaches that remove or filter content create a trade-off between simplification and faithful representation. Thus, approaches that can ease reading and navigation while preserving layout and content will generally be preferable.

We acknowledge as well an established field of research on depiction of webpages for mobile devices (e.g. 
                        [33–35]). While many of the challenges encountered by early mobile web users have been mitigated by improved hardware and a heightened sensitivity to the need for mobile-friendly web layouts, this paper nonetheless operates in a context similar to ones we are also trying to address, namely supporting the presentation of subsets of the full webpage.

Our system takes as input an image of a rendered web page and produces a hierarchical segmentation of the image. The original image is identical to the output of the browser’s rendering engine, as intended by the page’s designer. The image is segmented by first detecting edges in the image, then searching for the segmentation which is best supported by the edge structure. The system is best viewed at three levels, as follows. At the high-level, it takes as input the image of a rendered page (from a browser) and outputs the hierarchical segmentation of the image. At the mid-level, we examine regions, edges, alignments and tiling. At the lowest level, we perform probabilistic, context-sensitive edge detection, through image partial derivatives. The sections below provide a detailed explanation of our proposed system.

In our model, a web page is represented by a tree of regions. The root of the tree is a region consisting of the entire page, leaf nodes are “simple” regions which need not be divided further, and internal nodes represent “complex” regions to be subdivided or used as a whole, depending on the requirements of the application. The locations of the divisions between sibling regions in the tree are determined by evidence provided by the locations of edge in the image of the parent region. Fig. 1 shows an example of this type of segmentation tree for a simplified, abstract web page.

An edge may be caused by a line across a background (a ridge or trough edge), by a difference in two colors (a step edge), by an alignment of objects (an alignment edge), or by the boundaries of the page image (a boundary edge). The first three of these types are shown in Fig. 2. Alignment edges are in fact composed of aligned ridge or step edge segments corresponding to the edges of the objects in alignment. Ridge or trough edges may be caused by the use of a line to divide sections with the same background colour, as in the case of the <hr> (horizontal rule) HTML tag. Step edges may be caused by the use of different background colors to set regions apart. An alignment edge may be caused by the organization of objects into regions, as in the case of aligned lines of text. Each of these types may, of course, have a less-significant cause. Finally, the edges of the image of the page must be considered significant—outside of the page is clearly semantically different from inside the page.

Edges in the page image provide evidence for the boundaries of regions. The mere presence of an edge does not, however, guarantee the presence of a boundary. Region boundaries must also satisfy an additional set of criteria for validity:

                           
                              1.
                              A region boundary must be closed

A region boundary must be rectangular

A region boundary must be axis-aligned

While these criteria would be too limiting for image segmentation in general, they are reasonable in the context of web pages. The elements of a web page are defined on an axis-aligned grid, and it is unusual to encounter web page elements for which a rectangular bounding box would be a poor representation. Fig. 3
                         depicts regions that follow and violate these rules.

The entire page is built up as a tiling of regions. This tiling must obey the following four rules:

                           
                              1.
                              Each region in the tiling must be a valid region as described above

Each region must have at most one parent region

Every region but the root (which occupies the entire page image) has exactly one parent region

Every pixel in a parent region must be a member of exactly one child region

Examples of valid and invalid tilings are shown in Fig. 4
                        . These rules enforce a tree structure on the segmentation of the page; thus, it is a hierarchical segmentation of the page image.

The first step in segmenting the page is edge detection. We use a Bayesian method to determine the probability of an edge in a specified direction passing through an each pixel. Our method for edge detection finds both ridge and step edges directly; alignment edges are not detected directly, but alignments are considered as evidence in the construction of region boundaries. Our method is based on finding possible edges that “stand out” from their surroundings; this is intended to account for the presence in web pages of both highly textured regions such as text blocks where strong edges are common but do not divide semantically significant regions, and subtle but semantically significant edges such as a small change in background colour. Fig. 5
                         shows the edge detection process, as described in this section.

To determine if a potential edge pixel stands out from its surroundings, we must first define what those surroundings are. The simplest neighborhood (i.e. immediate surroundings) would include all of the pixels within a specified distance of the point in question. This would, however, include points along the proposed line; if the line exists, these points would paradoxically make the evidence for the existence of the line seem weaker. Excluding these points prevents this problem. For an edge between two textures, where one contains many short edges and the other contains very few, the edge “stands out” more relative to the texture with few edges than relative to the texture with many. It is therefore worth examining pixels on each side of the proposed line separately (i.e. as two distinct neighborhoods), and considering the degree to which the edge “stands out” from either side to be the true degree to which it stands out from its surroundings. To limit the degree to which the pixel in question can stand out from the neighborhood, we include each pixel in its own neighborhood; this has been found to give good results in practice and avoids numerical issues later. Fig. 6
                         shows our neighborhood structure.

Having defined the appropriate neighborhoods around a possible edge, we must define what it means for an edge to stand out from it. For a page image I with three colour channels, (IR, IG
                        , and IB
                        ), we define a map of the preliminary edge strength for each direction. The map for horizontal edge strength is defined, in the continuous case, by:

                           
                              (1)
                              
                                 
                                    
                                       S
                                       h
                                    
                                    
                                       (
                                       x
                                       ,
                                       y
                                       )
                                    
                                    =
                                    
                                       
                                          
                                             
                                                ∂
                                                
                                                   I
                                                   R
                                                
                                             
                                             
                                                ∂
                                                y
                                             
                                          
                                          
                                             
                                                (
                                                x
                                                ,
                                                y
                                                )
                                             
                                             2
                                          
                                          +
                                          
                                             
                                                ∂
                                                
                                                   I
                                                   G
                                                
                                             
                                             
                                                ∂
                                                y
                                             
                                          
                                          
                                             
                                                (
                                                x
                                                ,
                                                y
                                                )
                                             
                                             2
                                          
                                          +
                                          
                                             
                                                ∂
                                                
                                                   I
                                                   B
                                                
                                             
                                             
                                                ∂
                                                y
                                             
                                          
                                          
                                             
                                                (
                                                x
                                                ,
                                                y
                                                )
                                             
                                             2
                                          
                                       
                                    
                                 
                              
                           
                        The vertical preliminary edge strength Sv
                         is defined analogously. When the same equation applies to both the horizontal case and the vertical case, we use S(x, y) to indicate that either edge strength may be used. In the discrete case, we use the vertical and horizontal Sobel partial derivative kernels [36].

To determine if a possible edge “stands out” we must have a description of the surroundings to compare it to. Since the objective is to determine if a potential edge pixel is in fact on an edge, we compare its strength to the edge strengths at all neighboring pixels. We describe these strengths using a probability distribution P(e) over edge strengths e in a neighborhood N(x, y). The simplest approach would be to assume a normal distribution for P(e) and determine its parameters from the neighborhood edge strengths, but the assumption of normality produces poor results in practice. Instead, we apply kernel density estimation (using a Gaussian kernel) to the observed distribution of edge strengths in the neighborhood, resulting in a probability density function

                           
                              (2)
                              
                                 
                                    P
                                    
                                       (
                                       s
                                       )
                                    
                                    =
                                    
                                       1
                                       
                                          |
                                          N
                                          |
                                       
                                    
                                    
                                       ∑
                                       
                                          (
                                          x
                                          ,
                                          y
                                          )
                                          ∈
                                          N
                                       
                                    
                                    
                                       Norm
                                    
                                    
                                       (
                                       s
                                       ;
                                       S
                                       
                                          (
                                          x
                                          ,
                                          y
                                          )
                                       
                                       ,
                                       σ
                                       )
                                    
                                 
                              
                           
                        where Norm(x; μ, σ) represents the probability density at x for a normal distribution with a mean of μ and a standard deviation of σ. With this probability distribution, we can define a probability of observing a pixel with an edge strength of at least 
                           
                              s
                              =
                              S
                              (
                              x
                              ,
                              y
                              )
                           
                         when no edge is present given the neighborhood in which it occurs. For brevity, let S
                        
                           x, y, s
                         represent the event S(x, y) ≥ s, E
                        
                           x, y
                         the event “the pixel at (x, y) is an edge pixel”, and P represent the neighborhood edge strength distribution defined in Eq. (2). Then

                           
                              (3)
                              
                                 
                                    
                                       Pr
                                    
                                    
                                       (
                                       
                                          S
                                          
                                             x
                                             ,
                                             y
                                             ,
                                             s
                                          
                                       
                                       
                                          |
                                          
                                             
                                                E
                                                
                                                   x
                                                   ,
                                                   y
                                                
                                             
                                             ¯
                                          
                                          ,
                                          P
                                       
                                       )
                                    
                                    =
                                    1
                                    −
                                    
                                       ∫
                                       
                                          0
                                       
                                       s
                                    
                                    P
                                    
                                       (
                                       t
                                       )
                                    
                                    
                                       d
                                    
                                    t
                                 
                              
                           
                        By Bayes’ Theorem,

                           
                              (4)
                              
                                 
                                    
                                       
                                          
                                             
                                                Pr
                                             
                                             
                                                (
                                                
                                                   
                                                      E
                                                      
                                                         x
                                                         ,
                                                         y
                                                      
                                                   
                                                   ¯
                                                
                                                
                                                   |
                                                   
                                                      S
                                                      
                                                         x
                                                         ,
                                                         y
                                                         ,
                                                         s
                                                      
                                                   
                                                   ,
                                                   P
                                                
                                                )
                                             
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                
                                                   Pr
                                                
                                                
                                                   (
                                                   
                                                      S
                                                      
                                                         x
                                                         ,
                                                         y
                                                         ,
                                                         s
                                                      
                                                   
                                                   
                                                      |
                                                      
                                                         
                                                            E
                                                            
                                                               x
                                                               ,
                                                               y
                                                            
                                                         
                                                         ¯
                                                      
                                                      ,
                                                      P
                                                   
                                                   )
                                                
                                                
                                                   Pr
                                                
                                                
                                                   (
                                                   
                                                      
                                                         
                                                            E
                                                            
                                                               x
                                                               ,
                                                               y
                                                            
                                                         
                                                         ¯
                                                      
                                                      |
                                                   
                                                   P
                                                   )
                                                
                                             
                                             
                                                
                                                   Pr
                                                
                                                
                                                   (
                                                   
                                                      S
                                                      
                                                         x
                                                         ,
                                                         y
                                                         ,
                                                         s
                                                      
                                                   
                                                   
                                                      |
                                                      P
                                                   
                                                   )
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (5)
                              
                                 
                                    
                                       
                                          
                                             
                                             =
                                             
                                                
                                                   
                                                      (
                                                      1
                                                      −
                                                      
                                                         ∫
                                                         
                                                            0
                                                         
                                                         s
                                                      
                                                      P
                                                      
                                                         (
                                                         t
                                                         )
                                                      
                                                      
                                                         d
                                                      
                                                      t
                                                      )
                                                   
                                                   
                                                      Pr
                                                   
                                                   
                                                      (
                                                      
                                                         
                                                            
                                                               E
                                                               
                                                                  x
                                                                  ,
                                                                  y
                                                               
                                                            
                                                            ¯
                                                         
                                                         |
                                                      
                                                      P
                                                      )
                                                   
                                                
                                                
                                                   
                                                      Pr
                                                   
                                                   
                                                      (
                                                      
                                                         S
                                                         
                                                            x
                                                            ,
                                                            y
                                                            ,
                                                            s
                                                         
                                                      
                                                      
                                                         |
                                                         P
                                                      
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (6)
                              
                                 
                                    
                                       
                                          
                                             
                                             =
                                             
                                                
                                                   
                                                      Pr
                                                   
                                                   
                                                      (
                                                      
                                                         
                                                            
                                                               E
                                                               
                                                                  x
                                                                  ,
                                                                  y
                                                               
                                                            
                                                            ¯
                                                         
                                                         |
                                                      
                                                      P
                                                      )
                                                   
                                                
                                                
                                                   
                                                      Pr
                                                   
                                                   
                                                      (
                                                      
                                                         S
                                                         
                                                            x
                                                            ,
                                                            y
                                                            ,
                                                            s
                                                         
                                                      
                                                      
                                                         |
                                                         P
                                                      
                                                      )
                                                   
                                                
                                             
                                             −
                                             
                                                
                                                   
                                                      (
                                                      
                                                         ∫
                                                         
                                                            0
                                                         
                                                         s
                                                      
                                                      P
                                                      
                                                         (
                                                         t
                                                         )
                                                      
                                                      
                                                         d
                                                      
                                                      t
                                                      )
                                                   
                                                   
                                                      Pr
                                                   
                                                   
                                                      (
                                                      
                                                         
                                                            
                                                               E
                                                               
                                                                  x
                                                                  ,
                                                                  y
                                                               
                                                            
                                                            ¯
                                                         
                                                         |
                                                      
                                                      P
                                                      )
                                                   
                                                
                                                
                                                   
                                                      Pr
                                                   
                                                   
                                                      (
                                                      
                                                         S
                                                         
                                                            x
                                                            ,
                                                            y
                                                            ,
                                                            s
                                                         
                                                      
                                                      
                                                         |
                                                         P
                                                      
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        Every region has edges. Considering an arbitrary point in a region, without considering its own preliminary edge strength, the distribution of edge strengths in its neighborhood does not inherently affect the probability that the point in question is an edge. We therefore assume that 
                           
                              
                                 Pr
                              
                              
                                 (
                                 
                                    
                                       
                                          E
                                          
                                             x
                                             ,
                                             y
                                          
                                       
                                       ¯
                                    
                                    |
                                 
                                 P
                                 )
                              
                              =
                              
                                 Pr
                              
                              
                                 (
                                 
                                    
                                       E
                                       
                                          x
                                          ,
                                          y
                                       
                                    
                                    ¯
                                 
                                 )
                              
                           
                         (i.e., the probability that a given pixel in a region is an edge pixel does not depend on the distribution of preliminary edge strengths in the region, in the absence of evidence about the relationship between this distribution and the preliminary edge strength of the pixel in question). Thus

                           
                              (7)
                              
                                 
                                    
                                       
                                          
                                             
                                                Pr
                                             
                                             
                                                (
                                                
                                                   
                                                      E
                                                      
                                                         x
                                                         ,
                                                         y
                                                      
                                                   
                                                   ¯
                                                
                                                
                                                   |
                                                   
                                                      S
                                                      
                                                         x
                                                         ,
                                                         y
                                                         ,
                                                         s
                                                      
                                                   
                                                   ,
                                                   P
                                                
                                                )
                                             
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      Pr
                                                   
                                                   
                                                      (
                                                      
                                                         
                                                            E
                                                            
                                                               x
                                                               ,
                                                               y
                                                            
                                                         
                                                         ¯
                                                      
                                                      )
                                                   
                                                
                                                
                                                   
                                                      Pr
                                                   
                                                   
                                                      (
                                                      
                                                         S
                                                         
                                                            x
                                                            ,
                                                            y
                                                            ,
                                                            s
                                                         
                                                      
                                                      
                                                         |
                                                         P
                                                      
                                                      )
                                                   
                                                
                                             
                                             −
                                             
                                                
                                                   
                                                      (
                                                      
                                                         ∫
                                                         
                                                            0
                                                         
                                                         s
                                                      
                                                      P
                                                      
                                                         (
                                                         t
                                                         )
                                                      
                                                      
                                                         d
                                                      
                                                      t
                                                      )
                                                   
                                                   
                                                      Pr
                                                   
                                                   
                                                      (
                                                      
                                                         
                                                            E
                                                            
                                                               x
                                                               ,
                                                               y
                                                            
                                                         
                                                         ¯
                                                      
                                                      )
                                                   
                                                
                                                
                                                   
                                                      Pr
                                                   
                                                   
                                                      (
                                                      
                                                         S
                                                         
                                                            x
                                                            ,
                                                            y
                                                            ,
                                                            s
                                                         
                                                      
                                                      
                                                         |
                                                         P
                                                      
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (8)
                              
                                 
                                    
                                       
                                          
                                             
                                             =
                                             
                                                
                                                   
                                                      Pr
                                                   
                                                   
                                                      (
                                                      
                                                         
                                                            E
                                                            
                                                               x
                                                               ,
                                                               y
                                                            
                                                         
                                                         ¯
                                                      
                                                      )
                                                   
                                                
                                                
                                                   
                                                      Pr
                                                   
                                                   
                                                      (
                                                      
                                                         S
                                                         
                                                            x
                                                            ,
                                                            y
                                                            ,
                                                            s
                                                         
                                                      
                                                      
                                                         |
                                                         P
                                                      
                                                      )
                                                   
                                                
                                             
                                             
                                                (
                                                1
                                                −
                                                
                                                   ∫
                                                   
                                                      0
                                                   
                                                   s
                                                
                                                P
                                                
                                                   (
                                                   t
                                                   )
                                                
                                                
                                                   d
                                                
                                                t
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        Further algebraic manipulation yields

                           
                              (9)
                              
                                 
                                    
                                       
                                       
                                       
                                          
                                             
                                             
                                             
                                             
                                             Pr
                                             (
                                             
                                                E
                                                
                                                   x
                                                   ,
                                                   y
                                                
                                             
                                             |
                                             
                                                S
                                                
                                                   x
                                                   ,
                                                   y
                                                   ,
                                                   s
                                                
                                             
                                             ,
                                             P
                                             )
                                          
                                       
                                    
                                    
                                       
                                       
                                       
                                          
                                             =
                                             
                                                
                                                   Pr
                                                   
                                                      (
                                                      
                                                         E
                                                         
                                                            x
                                                            ,
                                                            y
                                                         
                                                      
                                                      )
                                                   
                                                   
                                                      (
                                                      1
                                                      +
                                                      Pr
                                                      
                                                         (
                                                         
                                                            S
                                                            
                                                               x
                                                               ,
                                                               y
                                                               ,
                                                               s
                                                            
                                                         
                                                         |
                                                         
                                                            
                                                               E
                                                               
                                                                  x
                                                                  ,
                                                                  y
                                                               
                                                            
                                                            ¯
                                                         
                                                         ,
                                                         P
                                                         )
                                                      
                                                      −
                                                      Pr
                                                      
                                                         (
                                                         
                                                            E
                                                            
                                                               x
                                                               ,
                                                               y
                                                            
                                                         
                                                         )
                                                      
                                                      Pr
                                                      
                                                         (
                                                         
                                                            S
                                                            
                                                               x
                                                               ,
                                                               y
                                                               ,
                                                               s
                                                            
                                                         
                                                         |
                                                         
                                                            
                                                               E
                                                               
                                                                  x
                                                                  ,
                                                                  y
                                                               
                                                            
                                                            ¯
                                                         
                                                         ,
                                                         P
                                                         )
                                                      
                                                      )
                                                   
                                                
                                                
                                                   Pr
                                                   
                                                      (
                                                      
                                                         E
                                                         
                                                            x
                                                            ,
                                                            y
                                                         
                                                      
                                                      )
                                                   
                                                   +
                                                   Pr
                                                   
                                                      (
                                                      
                                                         S
                                                         
                                                            x
                                                            ,
                                                            y
                                                            ,
                                                            s
                                                         
                                                      
                                                      |
                                                      
                                                         
                                                            E
                                                            
                                                               x
                                                               ,
                                                               y
                                                            
                                                         
                                                         ¯
                                                      
                                                      ,
                                                      P
                                                      )
                                                   
                                                   −
                                                   Pr
                                                   
                                                      (
                                                      
                                                         E
                                                         
                                                            x
                                                            ,
                                                            y
                                                         
                                                      
                                                      )
                                                   
                                                   Pr
                                                   
                                                      (
                                                      
                                                         S
                                                         
                                                            x
                                                            ,
                                                            y
                                                         
                                                      
                                                      |
                                                      
                                                         
                                                            E
                                                            
                                                               x
                                                               ,
                                                               y
                                                            
                                                         
                                                         ¯
                                                      
                                                      ,
                                                      P
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        This represents the probability of an edge at the image position (x, y), given the preliminary edge strength and the statistical distribution of edge strengths in the immediate context of (x, y). In this equation, the prior probability of an edge P(E
                        
                           x, y
                        ) is an important parameter.

We evaluate segmentation quality by the probability that the segmentation is valid. There are two significant factors in the quality of a segmentation:

                           
                              •
                              There should be evidence supporting all divisions that are made

There should be no evidence of additional divisions across the entire width or height of the region

We define the probability that a horizontal line exists between (x, y) and (x′, y) to be

                           
                              (10)
                              
                                 
                                    
                                       ∏
                                       
                                          t
                                          =
                                          x
                                       
                                       
                                          x
                                          ′
                                       
                                    
                                    
                                       (
                                       1
                                       −
                                       
                                          Pr
                                       
                                       
                                          (
                                          
                                             
                                                
                                                   E
                                                   
                                                      t
                                                      ,
                                                      y
                                                   
                                                
                                                ¯
                                             
                                             |
                                          
                                          
                                             S
                                             
                                                t
                                                ,
                                                y
                                                ,
                                                s
                                             
                                          
                                          ,
                                          N
                                          
                                             (
                                             t
                                             ,
                                             y
                                             )
                                          
                                          )
                                       
                                       )
                                    
                                 
                              
                           
                        where N(t, y) represents the neighborhood around the pixel (t, y). Vertical line probabilities are defined analogously. Then the overall probability that the evidence supports a given level of a segmentation is the probability that lines exist along all divisions present, and that no lines cross entire regions. This does not fully account for the support given to the presence of a line by the presence of other high-probability segments along the line, but it appears to be a useful heuristic in practice. The algorithm for calculating segmentation probabilities is shown in Algorithm 1.
                     

For each region that is to be divided into child regions, we optimize over the segmentation quality q found by Algorithm 1. Starting from a region consisting of the entire image, we optimize recursively until either a predefined maximum tree depth is reached or a region has no logical division points. The current implementation of the system uses a simple but effective optimization method. First, a grid of “candidate boundaries” is found, corresponding to rows or columns in the image with particularly strong evidence for a division. Second, the optimization algorithm searches for the segmentation best supported by the evidence from the image, subject to the constraints that region boundaries must lie on candidate boundaries and the number of child regions is not excessively large.

Candidate boundaries are detected by measuring the strength of the evidence for a line crossing the image horizontally or vertically at each position. For a horizontal candidate boundary, 
                           
                              C
                              B
                              
                                 (
                                 y
                                 )
                              
                              =
                              
                                 ∏
                                 
                                    t
                                    =
                                    0
                                 
                                 
                                    image
                                    
                                    width
                                 
                              
                              
                                 (
                                 1
                                 −
                                 
                                    Pr
                                 
                                 
                                    (
                                    
                                       
                                          
                                             E
                                             
                                                t
                                                ,
                                                y
                                             
                                          
                                          ¯
                                       
                                       |
                                    
                                    
                                       S
                                       
                                          t
                                          ,
                                          y
                                          ,
                                          s
                                       
                                    
                                    ,
                                    N
                                    
                                       (
                                       t
                                       ,
                                       y
                                       )
                                    
                                    )
                                 
                                 )
                              
                              ,
                           
                         as described in Section 3.3. To account for near-alignments of edges, we convolve the sequence CB(y) with a discrete Gaussian kernel with a standard deviation of 1 pixel, truncated at ± 3σ. We use outlier detection to find candidate boundaries. If CB(y) is sufficiently large, it is an outlier, but the threshold for “sufficiently large” must depend on the distribution of candidate boundary strengths in CB. To account for variation in the average candidate boundary strength, we subtract the mean of CB, and to account for variation in the variance of CB we divide this difference by the standard deviation to find the distance of CB(y) from the mean in units equal to the standard deviation of CB; that is, if 
                           
                              
                                 
                                    C
                                    B
                                    (
                                    y
                                    )
                                    −
                                    
                                       mean
                                    
                                    (
                                    C
                                    B
                                    )
                                 
                                 
                                    
                                       stddev
                                    
                                    (
                                    C
                                    B
                                    )
                                 
                              
                              ≥
                              
                                 t
                                 
                                    C
                                    B
                                 
                              
                           
                         for some threshold of significance tCB
                        , we assume that there is a candidate boundary at position y. We use an initial value of 
                           
                              
                                 t
                                 
                                    C
                                    B
                                 
                              
                              =
                              3
                              ,
                           
                         set as a reasonable value that performed well on an older dataset of 30 images. Although we use the evidence for a line across the full width or height of the image to find candidate boundaries, it is reasonable to expect that lines that do not fully cross the region but have strong evidence for a significant length will appear in this set as well.

The sets of vertical and horizontal candidate boundaries taken together form a candidate boundary grid, which is much coarser than the resolution of the image, dramatically reducing the search space. If the candidate boundary grid is still too large to search (i.e., too many lines qualify as candidate boundaries), the threshold tCB
                         used to detect peaks in the strength of evidence for an edge is increased by δt
                         (in our implementation a moderate value of 0.5), progressively, until the candidate boundary grid is sufficiently small. Even for candidate boundary grids at a reasonable size, an exhaustive search of all segmentations is not feasible. Since it is also undesirable for a region to have a large number of child regions, only solutions with a reasonable number of child regions are checked; this heuristic encourages solutions that correspond to a reasonable intuition about page structure
                           2
                        
                        
                           2
                           By avoiding a segmentation with a flattened hierarchy with little structure, which may be more difficult for users to navigate.
                         and reduces the search space still further. Note that if the true number of semantic children is larger than this child region threshold, some will simply be grouped together in the next layer; they could then be separated in the next layer if there is sufficient evidence. Since long list-like structures are common in web pages, we increase the permissible number of child regions for candidate boundary grids that have a single row or column of cells. This results in two sets of lists of possible segmentations:

                           
                              •
                              For x × y rectangular grids of cells such that x, y ≥ 2 and xy ≤ 35, we find all segmentations with a maximum of 8 child regions

For 1 × n or n × 1 grids for n ≤ 23, we find all segmentations with a maximum of n child regions

Since the sets of tilings depend only on the size of the candidate boundary grid and not on the contents or proportions of the cells, the sets can be pre-computed. This improves performance, although the current testbed implementation is not, in general, highly optimized.

Our segmentation algorithm, including optimization, is shown in full in Appendix A.

We present both qualitative and quantitative results of our algorithm. Qualitative results are shown first, followed by a quantitative comparison between segmentations produced by our algorithm and segmentations produced by taking the bounding boxes of the nodes of the DOM tree of the page. We were inspired to design our algorithm to address a variety of challenges faced by source code-based solutions; Appendix B discusses a several of these challenges, with practical examples.

Our test dataset consists of images of the top 50 web sites in Canada (as ranked by Alexa
                        3
                     
                     
                        3
                        
                           www.alexa.com
                        
                     ). The images were taken of the main site, possibly after clicking through an introductory page (such as selecting the English version of Wikipedia) but not bypassing any login pages (because active accounts were not available for most web sites that requested the user log in to continue). The images were collected on October 9, 2015. The full list of web sites is shown below. The dataset is diverse, including news, shopping, social networking, and business pages. Each page was rendered in Firefox 18.0.1 for Ubuntu, and the page image captured using the extension Screengrab (fix version) 0.98.09c. A window width of 1024 pixels is used for all images. They were segmented using our method implemented as an offline testbed system running in MATLAB. The page images are approximately 1000 pixels wide and range from approximately 600 pixels to over 10000 pixels in height.

                        
                           1.
                           
                              google.ca
                           


                              google.com
                           


                              facebook.com
                           


                              youtube.com
                           


                              yahoo.com
                           


                              amazon.ca
                           


                              wikipedia.org
                           


                              live.com
                           


                              twitter.com
                           


                              amazon.com
                           


                              kijiji.ca
                           


                              linkedin.com
                           


                              reddit.com
                           


                              netflix.com
                           


                              imgur.com
                           


                              msn.com
                           


                              cbc.ca
                           


                              instagram.com
                           


                              royalbank.com
                           


                              pinterest.com
                           


                              ebay.ca
                           


                              apple.com
                           


                              bing.com
                           


                              td.com
                           


                              paypal.com
                           


                              imdb.com
                           


                              ebay.ca
                           


                              tumblr.com
                           


                              craigslist.ca
                           


                              wordpress.com
                           


                              diply.com
                           


                              theweathernetwork.com
                           


                              rbcroyalbank.com
                           


                              cnn.com
                           


                              microsoft.com
                           


                              bmo.com
                           


                              cibc.com
                           


                              kat.cr
                           


                              bestbuy.ca
                           


                              theglobeandmail.com
                           


                              scotiabank.com
                           


                              go.com
                           


                              vice.com
                           


                              indeed.com
                           


                              gfycat.com
                           


                              stackoverflow.com
                           


                              aliexpress.com
                           


                              dropbox.com
                           


                              walmart.ca
                           


                              wikia.com
                           

For the following tests, our algorithm was run with a threshold for candidate edge significance 
                        
                           
                              t
                              
                                 C
                                 B
                              
                           
                           =
                           3
                        
                      (for its initial value, with an update increment 
                        
                           
                              δ
                              t
                           
                           =
                           0.5
                        
                     ), an a priori probability of an edge 
                        
                           
                              Pr
                           
                           (
                           
                              E
                              
                                 x
                                 ,
                                 y
                              
                           
                           )
                           =
                           0.01
                           ,
                        
                      a neighborhood size of 
                        
                           w
                           =
                           15
                           ,
                        
                      and a kernel width for kernel density estimation of edge strength distributions of 
                        
                           σ
                           =
                           0.1
                        
                     . The maximum tree depth is set to 6 in the following experiments, except where otherwise noted
                        4
                     
                     
                        4
                        The neighborhood size w should be set to a moderate value, larger than the gap between two lines of text in the same paragraph, but not so large as to cross major divisions; we chose 15 pixels for this value. The maximum tree depth is set to a reasonable value to avoid excessively detailed segmentations. Good values for other parameters were determined experimentally on an earlier dataset of 30 diverse web pages, to avoid overfitting.
                     .

The qualitative results we present show selected examples of full and partial segmentations. We show examples illustrating the advantages of our segmentation method and examples showing cases that are difficult for our algorithm to handle.

Some example segmentations are shown in Fig. 7, one
                         from a news web site, one from an entertainment website, and one from a banking website. It is clear that the divisions are largely reasonable, especially for column divisions; even a grid structure is detected fairly well despite the large number of nodes in it. These examples were all generated using entire web pages, not just the area visible on the screen at any given time. As width is typically fixed by the width of the browser window, but length is variable due to vertical scrolling, the full web pages tend to be long and narrow. Fig. 8 shows details from the two images on the left at a higher resolution.


                        Fig. 9a and b shows successful detections of two difficult edges. In Fig. 9a, the edge is a faint transition in background colour between light grey and white, but it is detected correctly because it stands out from its immediate surroundings, which contain no other edges. In Fig. 9b, the edge is formed by the vertical alignment of several regions of text. Although the text produces strong edges on one side of the edge, the other side has no texture nearby, resulting in a correct detection despite gaps between the blocks.


                        Fig. 10
                         shows an example of a spurious division caused by a coincidental alignment of the vertical line in the letter “K” and the vertical edge of a thumbnail. It is useful to segment the thumbnail separately, but the coincidental alignment causes the division to extend too far.


                        Fig. 11
                         shows a flawed detection of an overlay on an image. Perfect detection of overlays is not realistic, since they constitute a violation of the assumption that the regions in the page are rectangular and non-overlapping. Extensions to our model may allow it to handle such overlays correctly by admitting segmentations in which the child node or nodes of a region do not fully cover their parent region.

While spurious edges can occur in our segmentations, a segmentation with boundaries corresponding to the bounding boxes in the DOM tree can also produce edges without visual support. An example is shown in Fig. 12
                        . The leftmost edge is not supported by image data; it is produced by a functional region whose edges are not visually distinct from its immediate surroundings.

Our intention is to compare our segmentation tree to competitor algorithms. It is especially desirable to compare to a method based purely on the DOM tree, to contrast with our image-based approach. One obvious choice for such a competitor is the DOM tree itself. We performed an initial study using the segmentations produced by all bounding boxes of elements in the DOM tree. Since the DOM tree is generally not used in its “raw” state for page segmentation in practice, we also compared our segmentation method quantitatively to two existing algorithms: a “control” version of our algorithm which uses evidence from the DOM tree rather than the image of the page, and the well-known VIPS algorithm [8] (described in Section 2.2). The comparison with the control algorithm is useful for comparing segmentations produced using evidence from the image to those produced using evidence from the DOM tree, using the same optimization methods and tiling-based segmentation model. The comparison with VIPS shows the relationship between our method and one of the most familiar web page segmentation algorithms.

If our algorithm produces better results, according to some reasonable metric, or even comparable results (given the greater generality of our method), then we can demonstrate that our proposed approach is valuable and thus offer an initial validation. One approach—the one we take for our quantitative evaluation—is to compare segmentation trees for agreement, using the “flat” segmentations produced by their leaf nodes. Alternatively, a de-cluttering front end could be tested using a measurement of clutter such as feature congestion [37]. This would be a very tidy, self-contained evaluation method, although it does not evaluate the structure of the segmentation. We instead introduce qualitative experimental results showcasing our effect on clutter within Section 6 (in conjunction with a discussion of those user groups requiring assistive technology).

Our control algorithm is a variation on our segmentation algorithm which uses information from the DOM tree (collected using a purpose-built Firefox extension) to perform the segmentation, rather than information from the image. Specifically, we replace the edge map with a “bounding box map”, where the probability that pixel x is a horizontal edge is defined to be pbb
                         if it is on a horizontal edge of a bounding box, and pnonbb
                         otherwise (pbb
                         and pnonbb
                         are parameters set to reflect uncertainty about the relationship between the presence of a bounding box edge and the presence of a semantic boundary); vertical edge probabilities are defined analogously. For our experiments, we use parameter values 
                           
                              
                                 p
                                 
                                    b
                                    b
                                 
                              
                              =
                              0.9
                           
                         and 
                           
                              
                                 p
                                 
                                    n
                                    o
                                    n
                                    b
                                    b
                                 
                              
                              =
                              0.01
                           
                        . The use of this algorithm provides better experimental control than using an existing competitor would; since the optimization method and segmentation model are identical, any difference in the results must derive from the use of the DOM tree bounding boxes in place of image data.

For our comparison with VIPS, we use the reference implementation found at http://www.cad.zju.edu.cn/home/dengcai/VIPS/VIPS.html, and specifically the “Using VIPS” application (slightly modified to ensure that the rendering window is the same size as used in Firefox to collect screen images). For compatibility reasons, we ran this algorithm in a Windows XP Service Pack 3 virtual machine. The reference implementation uses Internet Explorer to render the page and obtain the style and placement information used by the VIPS algorithm; we use Internet Explorer 8, as it is the most recent version compatible with Windows XP. Using this version, three web pages are not correctly rendered; these are omitted from the comparison.

To generate data suitable for comparison across segmentation techniques, we create edge maps including all edges across all levels of the segmentation trees. This flattens the hierarchy, allowing us to avoid the need to compare the structure of trees with differing numbers of nodes and other structural properties, while still preserving all edges between regions.

To compare our segmentations with the segmentations based other methods, we use the earth mover’s distance (EMD) between the two edge maps [38,39]. The EMD describes the total “work” or cost needed to transform one function to another, where work is defined to be the amount of “mass” moved multiplied by the distance moved. Fig. 13
                            shows examples of functions that are “close” and “far” according to the EMD. Formally, given two discrete functions f(x) and f′(x), the EMD between f and f′ is defined to be

                              
                                 (11)
                                 
                                    
                                       d
                                       =
                                       
                                          min
                                          
                                             m
                                             
                                                (
                                                x
                                                ,
                                                
                                                   x
                                                   ′
                                                
                                                )
                                             
                                             ,
                                             a
                                             
                                                (
                                                x
                                                )
                                             
                                          
                                       
                                       
                                          (
                                          
                                             ∑
                                             
                                                x
                                                ,
                                                
                                                   x
                                                   ′
                                                
                                             
                                          
                                          
                                             c
                                             
                                                move
                                             
                                          
                                          
                                             (
                                             x
                                             ,
                                             
                                                x
                                                ′
                                             
                                             )
                                          
                                          m
                                          
                                             (
                                             x
                                             ,
                                             
                                                x
                                                ′
                                             
                                             )
                                          
                                          +
                                          
                                             ∑
                                             x
                                          
                                          
                                             c
                                             
                                                new
                                             
                                          
                                          
                                             |
                                             a
                                             
                                                (
                                                x
                                                )
                                             
                                             |
                                          
                                          )
                                       
                                    
                                 
                              
                           where c
                           move represents the cost of moving one unit of mass from x to x′, c
                           new represents the cost of adding or removing one unit of mass at any given position, m(x, x′) the quantity of mass moved from x to x′, and a(x) the amount of mass added or removed at position x. This is subject to the constraint that

                              
                                 (12)
                                 
                                    
                                       
                                          f
                                          ′
                                       
                                       
                                          (
                                          x
                                          )
                                       
                                       =
                                       f
                                       
                                          (
                                          x
                                          )
                                       
                                       +
                                       a
                                       
                                          (
                                          x
                                          )
                                       
                                       +
                                       
                                          ∑
                                          
                                             
                                                x
                                             
                                             ′
                                          
                                       
                                       m
                                       
                                          (
                                          
                                             x
                                             ′
                                          
                                          ,
                                          x
                                          )
                                       
                                       −
                                       
                                          ∑
                                          
                                             x
                                             ′
                                          
                                       
                                       m
                                       
                                          (
                                          x
                                          ,
                                          
                                             
                                                x
                                             
                                             ′
                                          
                                          )
                                       
                                       .
                                    
                                 
                              
                           
                        

In this equation, f and f′ again represent the original and target functions. We define m(x, x′) to be the Euclidean distance between x and x′. We use a cost of adding or removing mass c
                           new equal to the maximum Euclidean distance between two points in the image, although a lesser cost is possible (we use a cost of 63, half of the patch width, for a richer demonstration of the EMD in Fig. 14
                           , since this ensures that mass is both added and moved).

Because the EMD is expensive to compute and the full edge maps are very large, we perform two sets of tests. In the first, randomly-chosen 127 × 127-pixel patches
                              5
                           
                           
                              5
                              This size was chosen because it is small enough to be practical for large numbers of tests but large enough to encompass interesting features.
                            of the edge maps are compared to each other at full resolution; this is useful for comparison of the small-scale structure of segmentations. We use 10 samples from each page for our experiments. Fig. 14 shows examples of two such pairs of patches, with different EMD distances, including the components used to obtain the EMD. In the second comparison, the full page edge maps are downsampled (by a factor of 50, so that the areas of large pages are comparable to the area of the patches) while still maintaining the overall structure of the edge maps, and compared to each other directly; this is used for a large-scale comparison of segmentations. Because our visual segmentation algorithm and the DOM tree bounding boxes may produce different numbers of edge pixels, we normalize the edge maps to have the same total mass for each comparison. Table 1
                            compares the two uses of the EMD.

For our purposes, we normalize the EMD by dividing by the maximum cost possible for the given distance function and new-mass cost; this allows easy comparison when, for example, the image size or aspect ratio varies between two cases. We also normalize the sum of mass in each edge image to 1 (or 0 if the image is uniformly zero) prior to calculating the EMD. The range of the normalized EMD is [0, 1]. If the normalized EMD is 0, the two patches or images are identical; if it is small, they are similar; if it is large, they are dissimilar; and if it is 1, they are entirely different (e.g. one patch has nonzero mass and the other does not, or all of the mass is concentrated at opposite points in each patch). These extremes often occur for small patches, especially for sparse segmentations.

The control algorithm provides a useful point of comparison for our algorithm, since it uses the same optimization methods with the same parameters, and only differs from our method in the use of evidence from the DOM tree bounding boxes in place of edges in the image of the rendered page. For this experiment, we use a maximum tree depth (including the root node) of 6. Fig. 15
                            shows the differences (expressed in terms of the normalized EMD) between the segmentations produced by our algorithm and the control algorithm.

For downsampled full pages, the mean EMD (with a 95% confidence interval) between the two solutions is 0.167 ± 0.036, assuming a normal distribution, which is reasonable for this data; for patches at full resolution, the mean EMD is 0.478 ± 0.032. This demonstrates that the two approaches give results that are distinct from each other, and more similar at a large scale than at a small scale. This would be expected if both methods capture broad themes of organization, but treat the details of the structure differently. Furthermore, even at a large scale the two are significantly different. Note also the relatively high occurrence of the extreme values of EMD for small patches, due largely to perfect agreement that an edge does not exist or disagreement about whether or not an edge exists in the patch.

Our comparison with VIPS uses a slightly reduced dataset of 47 pages. Due to being incompatible with more modern web technologies, VIPS was unable to correctly render three of the webpages; to make the most conservative comparison, we omitted what couldn’t be handled at all in VIPS. We used the default PDoC (permitted degree of coherence) value of 5 to generate the VIPS segmentations; this resulted in a slightly coarser segmentation than the 6-level segmentation trees used above, so we show comparisons with the upper 3 levels and upper 5 levels of our segmentation trees rather than the full depth of the tree. As will be seen, these two tree depths give broadly similar results.

We show results for full webpages in Fig. 16
                            and for patches in Fig. 17
                           . With a tree depth of 3, the mean EMD between downsampled full pages is 0.303 ± 0.032, and the mean EMD between 127 × 127 patches is 0.449 ± 0.043. With a tree depth of 5, the mean EMD between full pages is 0.308 ± 0.030, and the mean EMD between patches is 0.633 ± 0.040. The smaller tree depth results in better performance in terms of small patches, but is not significantly different at a large scale. Both are significantly more different from our segmentations than the control algorithm for full pages.

Comparing Fig. 17 with Fig. 15b, we see that the mean EMD is significantly different from the control algorithm for a tree depth of 5, but not for a tree depth of 3. Qualitatively, an EMD of 0 or 1 occurs much more frequently in the comparison of patches with VIPS than in a comparison of patches with the control algorithm. This is likely to be due at least in part to the relative sparsity of the VIPS segmentation; when no edge appears in either patch, the normalized EMD is zero, and if one patch contains an edge but the other doesn’t, the normalized EMD is one. Because of the prevalence of extreme values in Fig. 17a and b, we show histograms of the central region alone in Fig. 17c and d, for better viewing of the results. These figures show that while there is partial agreement between the two segmentations, our algorithm still tends to produce results significantly different from VIPS.

We also performed preliminary tests by comparing our segmentations to segmentations performed by the raw DOM tree (using all bounding boxes of DOM tree nodes as the boundaries of regions) on the same dataset of 50 webpages. As expected, we found that our results are substantially different, especially at a small scale. Fig. 18
                            displays these results.

To evaluate the simplicity of segmentations produced by our method, we compare our method to the segmentation consisting of the bounding boxes of the DOM tree nodes and also to the control algorithm. The former comparison demonstrates that we produce a simpler segmentation than the raw DOM tree does, while the latter demonstrates that our segmentation method does not sacrifice simplicity for versatility. We omit a comparison with VIPS because, with the default PDoC value, it produces a coarser segmentation than either our method or the control algorithm, and thus the judgment of relative simplicity is less well defined.

We use two measures of simplicity for our comparison: the total number of edge pixels and the total number of basic or bottom-level regions. For our segmentation tree, the bottom-level regions are simply the leaf nodes. The bounding boxes of regions in the DOM tree, however, can overlap and are not necessarily covered by their child nodes. Accordingly, we define basic regions in the segmentation based on the DOM tree to be connected regions of non-edge pixels contained by the same set of bounding boxes.

Our segmentation algorithm produces simpler segmentations than the raw DOM tree. In terms of both number of edge pixels and number of bottom-level regions, our method’s segmentation is simpler in 49 of 50 cases. The null hypothesis—that the two segmentations are equally likely to be the simpler—can be rejected at a significance threshold of p < 0.001. Thus we can conclude that our segmentation algorithm usually produces a simpler segmentation than the raw DOM tree.

Our segmentation and the control segmentation are approximately equally likely to produce the simpler segmentation as measured by both the number of edge pixels and the number of basic regions; there is no statistically significant difference in probability for either measure. By the number of edge pixels, our segmentation is simpler than the control algorithm’s for 24 of 50 web pages; by the number of basic regions, our segmentation is simpler in 25 out of 50 web pages. The null hypothesis cannot be rejected in either case. This result indicates that our method does not sacrifice the simplicity of the segmentation for independence from implementation details.

Taken together, the results described above indicate that our segmentations are related to the competing segmentations in the following ways:

                              
                                 1.
                                 At a small scale, our segmentation produces distinct results from the control algorithm and VIPS (as well as from the raw DOM tree competitor).

Our segmentations are more similar to the competing segmentations at a large scale than at a small scale.

Our segmentation is simpler than the raw DOM tree, in that it presents a coarser display of regions, and is comparable in simplicity to the DOM-based control algorithm.

The first two points establish that our segmentations are distinct from but related to the segmentations based on the DOM tree; it does not simply serve as a slower version of these algorithms in practice. The third point suggests advantages in terms of usability; a simpler tree should be easier for users to navigate through or select regions from than a more complex one (e.g. one with extraneous edges, per Fig. 12). First, our model is based on a principled derivation from intuitively plausible assumptions. Second, our qualitative results indicate that our model places region boundaries in intuitively reasonable positions, and, combined with the quantitative validation, offers evidence to support the usefulness of our model. Table 2
                            summarizes the primary performance metrics that we have used in our validation, when judging the effectiveness of our approach. We note as well that the simplicity offered with our particular approach aligns with key desirable attributes outlined in [40], suggesting that having fewer nodes overall, fewer paths to explore, and structure that minimizes disorientation, is of benefit when users are confronted with hierarchical organization online. In summary, we have shown that our segmentation method has implementation independence, that it produces distinct results from its competitors in practice, that it produces qualitatively plausible segmentations, and that it achieves these desirable properties without sacrificing simplicity to use the image of the rendered page.

@&#DISCUSSION@&#

Although our primary focus is on the application of our method to web pages, there are other, similar domains to which it could be applied, perhaps in a slightly modified forms. Our method is designed for artificial, designed images (as opposed to the more common use of computer vision for natural images) which convey information about their semantic structure through their visual organization. Other cases of this include images of a desktop windowing system, infographics, and academic papers in PDF format. These would all be interesting cases to examine using our approach.

In analyzing artificial images formed by rendering from an underlying source code, we can take advantage of useful properties of these images that do not typically occur in natural images. First, the image axes generally correspond to the natural axes of the document; in a web page, for example, the vertical and horizontal axes of the page are the same as the vertical and horizontal axes of the image. This type of image is also often generated from rectangular elements, and perspective projection is not usually involved. These properties allow strong assumptions to be made about the properties of the resulting images, and eliminate the need to handle complex effects that are common in natural images.

Our edge detection method defines an edge as an outlier with respect to the local gradient. This idea occurs in a variety of contexts; Black and Sapiro, for example, use this definition of edges to improve anisotropic diffusion methods for smoothing images while preserving edges [41]. The same idea can be expressed as adapting the edge detection threshold such that stronger evidence for an edge is required when the local texture features strong gradients. A variety of techniques have been proposed for this formulation of the problem; Marimont and Rubner describe a particularly relevant method, as their method, like ours, calculates the probability of an edge at a given pixel given its surroundings [42]. Other statistical methods for edge detection compare the distribution of intensity on each side of a proposed line. These techniques are related to our edge detection method both in that they use an analysis of distributions of image features to detect edges and in that they use neighborhoods on either side of a proposed line. Williams et al. have produced a survey and evaluation of a number of such methods [43].

Assembly of edges and region boundaries from the map of edge probabilities is done simply but effectively in our current system. There are, however, more sophisticated approaches to constructing and combining edges which may be relevant in the future. Lowe, for example, researched perceptual grouping, developing Bayesian methods for determining whether features in the image occur accidentally or are significant in the real scene [44]. Lowe’s methods allow tentative conclusions about small structures to be reinforced by observations about the embedding of small structures in larger ones. Although Lowe was focused on images of three-dimensional scenes, accidental alignments can also occur in a web page (e.g. 
                     Fig. 10). Systematic grouping of areas where edges are highly probable could be a useful intermediate stage between calculating the probability of an edge at an individual pixel and the construction of region boundaries, especially for bridging gaps in an alignment edge. Related work by Jepson and Mann analyzed similar types of features in terms of qualitative probability [45].

In this section, we first discuss sample users with assistive needs in the context of web use. We then describe examples of assistive systems that could produce alternative depictions of web pages based on a segmentation of the page, and show mock-ups of these interface front-ends. Each of these very different interfaces depends on a high-quality segmentation to provide information about the page structure; segmentation algorithms such as ours are versatile back-end components that can be incorporated into a large range of assistive technology systems. While our method is one of many page segmentation algorithms, it does have the advantage of versatility and independence from implementation details (as discussed in Section 1.2 and elaborated upon in Appendix B).

The kinds of users we are most interested in supporting are as follows. Visually impaired users (ranging from those who would rely on audio output to replace the visual depiction, to those with some limited vision, who can benefit from enhanced presentation of subsets of the page) constitute the first class of users for whom we would like to offer improvements; this user base formed the initial motivation for our research. The second primary class of users that we aim to assist are those who have some form of cognitive deficit. Here, the range of scenarios is broad, including those who could benefit from decluttering as an aid to either comprehension (e.g., users with aphasia [46]) or focus (e.g. those with attention deficit [47], or neurological symptoms associated with Lyme disease [48]); and older adults who may be overwhelmed by busy presentations [49]. Indeed, concentration on the ageing population is a distinct project of the international initiative to enhance web accessibility [50]. Below, we elect to organize the discussion in terms of the front end functions that we can support with our proposed back end system for capturing webpage segmentation based on computer vision, and proceed on that basis.

A major component of browsing the web is locating content areas within each page and assessing that content against current information needs. While sighted web users can quickly locate main content areas on the page and skim them for relevance, users with severe visual impairment relying on screen readers are often forced to process content sequentially. Because this can be a slow process, screen readers typical provide mechanisms for skipping forward. For example, users can often jump ahead by using structural markers (such as <h> tags) to move between content areas [51]. Unfortunately, such approaches rely heavily on the quality of the underlying code. The code structure also tends to be complex, resulting in page segmentations that consist of many small segments of text. In general, as the number of segments increases and the size of each decreases, the benefit of being able to stop-short the reading of a segment and jumping ahead diminishes. Our approach provides a simple segmentation tree based on the visual cues in the page, which can be used to improve the efficiency of navigating between regions of a page when using a screen reader program. Fig. 19
                         shows a comparison of all edges in the bounding boxes of DOM tree nodes with all edges in our segmentation tree
                           6
                        
                        
                           6
                           This is generated by finding all bounding boxes for all nodes in each tree and plotting their outlines on the original image, using the Web Developer 1.2.5.1 Firefox extension.
                        . Our segmentation tree is clearly much simpler than a segmentation tree defined purely by the code hierarchy. This simplicity results in shorter paths through the tree from one region to another; for example, there are fewer leaf nodes, fewer nodes with only a single child, and fewer levels in the tree.

A wide range of individuals could potentially benefit from support to reduce distractions and simplify the information processing of webpages. Individuals with attention deficits, including age-related declines in divided attention, can find it difficult to filter out distractions and to maintain focus. Such individuals can benefit from adaptations such as highlighting or content isolation to help them to maintain activation on their area of focus [52]. Those with visuospatial deficits can find it harder to process and keep track of the spatial relationships among elements of a page [52], and can benefit from adaptations that reinforce the layout and relationships among items on the page. As aging is associated with a diminished ability to learn and retain new information, and some individuals may benefit from tools that provide cues on the structure or organization of the page [32]. Some work has also suggested that older adults browse differently from their younger counterparts [53], although this distinction would seem to have more to do with age-associated differences in fluid intelligence than age itself [54]. Eye tracking research suggests older adults attend longer to areas of the page than do younger users [55]. While this might be considered a strength if that time is spend synthesizing important content [56], it can also be a barrier if it is wasted on unimportant or irrelevant elements of the page. Our proposed segmentation could be used to present appropriate regions to users who are in need of clutter reduction; the following subsection clarifies what we can provide for these users.

In this section we present mockups of assistive interfaces based on our segmentation system. Although not produced by complete standalone programs, all of the examples shown were generated using real segmentation trees produced by our method from real web pages. These mockups are intended to show examples of possible interfaces based on our system; the details of the appearance of each could be adjusted to match aesthetic preferences or to accommodate the assistive needs of individual users.


                        Fig. 20a shows one possible adaptation that could be achieved with our segmentation. In this design mock up, all segments of the webpage are greyed out except the one currently under the user’s cursor. Although additional work is needed to validate this approach, it could potentially help users to maintain control over their area of focus. Other methods, such as blurring, may be used instead of or in combination with greying out non-focus regions, depending on what method is found to be most effective for users in general or on the preferences of individual users.

An extension to this idea would be to leverage the hierarchical nature of our segmentation, and enable the user to control the size of the focus areas. This support might be particular useful to those who have difficulty identifying the structures within a page, as they could be used to reveal and reinforce those relationships. Fig. 20b shows the same visualization as Fig. 20a but where a region at a higher level in the tree is considered to be the region of focus. In this case, an entire column is highlighted, but other columns of the page remain greyed out to reduce distractions. Further work is needed to determine appropriate interaction mechanisms for controlling the granularity of the region of focus.

Not all users who are visually impaired require a screen reader program. For some, it may be sufficient to simply present regions at a larger scale. Individuals with certain types of cognitive impairment can also benefit from larger text [57]. Simply enlarging the entire page may obscure the large-scale organization of the page, however. Similarly, accessibility features such as Windows Magnifier or the Picture-In-Picture Zoom on Mac OS X, which use a fixed magnification boundary, may not allow the user to read an entire visually coherent region at once. We propose an interface which selects an entire visually-coherent region to enlarge as an overlay, as shown in Fig. 21
                        . Allowing the user to reposition the overlay would assist in maintaining context in the page, as the users could uncover nearby regions at will to select the next region they would like to enlarge. Another avenue to explore would be to combine this approach with a “fisheye” distortion around the enlarged areas (as discussed by Furnas [58]) so that the enlarged area does not obscure any of the content; this, however, may prove confusing or distracting for some users.

Some users, including those with working memory deficits might find it helpful to view multiple segments at once, or even move segments around to facilitate cross comparison. This could be accomplished by detaching regions from the page, much as in the selective enlargement method described above. Detached regions such as the one shown in the Fig. 21 could be rearranged, possibly even combining regions from multiple pages in a single view.

@&#FUTURE WORK@&#

There are two primary directions for future work. The first is to extend our existing model, to make it broader or deeper. This is research focused on computer vision. The second is to explore in greater detail the user community requiring assistive technology based on our proposed model. We outline both of these directions for future research below.

One important extension to our method will be classification of regions. Basically, the idea is that the segmentation tree (including the image data for each region) will be the input to the classification algorithm. The classification algorithm will extract its own features, and use the structure of the segmentation tree to generate a probabilistic graphical model with states corresponding to the labels of each region. This is similar to the classification framework proposed in [1].

The structure of a web page is useful in the absence of classifications, but it would be still more useful to have labels describing the semantic roles of each region. Useful types of label may include labels describing the role of a region (e.g. “main article” or “navigation toolbar”) or its position (e.g. “header” or “left sidebar”). It is likely to be simpler to classify regions according to the latter system, but the former would probably be more informative. A combination of the two, with position-based classification used as a fallback option in case of the failure of the more informative system, is an attractive option. Classification should respect the tree structure of the segmentation rather than labeling each region in isolation. This suggests the use of a tree-structured probabilistic graphical model. The classification would be trained using labeled segmentations as training data.

Other significant vision-based extensions would include refinement and generalization of the model of the page, and integration with OCR technology. Improvements to the model may include allowing non-rectangular (but probably still orthogonal) segments and learning prior probabilities of observing different arrangements of subregions. Integration with OCR would be useful for many applications of our system, and may include making use of plain text found in the page source where it is available.

In Section 5, we outlined potential front-end applications of our back-end segmentation technique. An important next step for this research would be to evaluate these front-end approaches with respect to their ability to address user needs. In the near-term, informal user tests are needed to gather feedback on our proposed designs, with the goal of identifying the most promising alternatives and best-matching them to specific accessibility needs. This paper would also aim to extend and refine the support provided and the interaction design needed to manipulate our proposed techniques. At this stage, iterative testing with target users is needed to ensure the resulting design matches the needs of and is acceptable to the target demographic. In the longer-run, evaluation is needed to establish the efficacy of our approach. In this stage, controlled experiments are needed to compare our refined designs to appropriate baseline conditions. In many cases the appropriate baseline may be no support (i.e., the unaltered webpage); however, in some cases, competing approaches exist and could be used as a baseline. For example, in the case of supporting screen reader navigation, an effective comparison would be to examine navigation via our segments versus navigation via traversal of DOM elements (e.g. heading tags). Performance measures including speed and accuracy, and Likert scale preference data could be compared via parametric and non-parametric inferential statistics. Further studies could aim to establish the extent of support provided; for example, studies comparing performance with and without support across users with and without accessibility needs could be conducted to establish the extent to which support reduces performance gaps. While it would be useful to gauge the value of our proposed approach for distinct user bases who may require assistive technology, as mentioned earlier, each user arrives with distinct, personal needs that suggest caution in reaching generalizations about what we offer, for an entire class of impairment.

We can also imagine a more targeted exploration of how the model outlined in this paper can be viewed as an advance in enabling users of screenreaders (i.e. those with visual assistive needs) to better navigate to desired content within webpages. For example, existing proposals such as those surveyed in [51], in order to zero in on content of interest, face challenges when the webpages are poorly designed, due to reliance on the underlying implementation. Our approach, which is not tied to this depiction should provide quantifiable benefits which can be revealed through specific user studies.

We are also aware of efforts to encourage web designers to create their web-pages in a manner that is inherently more favorable to users who have accessibility needs. As one case in point, [59] offers a scripting framework that can augment current webpage-designing efforts, for example to enhance visual depictions with alternative text (if aligned with the WebInSight framework [59,60]; the framework can also be employed to support context-centred web browsing, zeroing in on relevant DOM tree elements, for users. This latter usage may first of all help to inform some of our selection of content for specific groups of users, when introducing our segmentation algorithm. Future work can also examine how our model provides additional benefits, in contexts where designers have already incorporated adjustments to the webpages, in an effort to be supportive of users with assistive needs. This could form the basis for another targeted user study.

Capturing the visual depiction of webpages has been the focus of discussion of this paper, to showcase the value of our particular segmentation methods. While our current experimentation was not operating explicitly in an environment of a small, mobile device, the current ubiquity of mobile web interaction has forced designers to address mobile needs. It is interesting to note that many of the challenges faced by early users of the mobile web are in fact similar to those encountered by users with accessibility needs [21]. In fact, a consistent theme in accessibility research has been that support often transcends initial accessibility goals to address broader mainstream needs, which are commonly referred to as situational impairments. We believe that other sources of situational impairments likely remain, for the usage of technology in the future, and identifying them would also be a useful avenue for subsequent research.

@&#CONCLUSION@&#

In this paper, we have developed a computer-vision model for determining the segmentation of webpages, which can then be leveraged to offer improved depictions of these pages for users with a variety of assistive needs. Our proposed system is a back-end for use in assistive technology systems. This system supplies the front-end with rich, semantically significant information. We have also explained how our system can be readily extended to provide higher-level information such as segment classifications. We have included discussion of the specific techniques that we use in our approach and offered contrast with existing computer vision research. Our overall aim is to improve the quality of life for such users as those with disabilities and the elderly, for a task that is endemic to everyday life: interacting with online webpages. This research relates more specifically to several of the items in focus for this special issue, most notably: applications for the ageing society, applications for the visually impaired and augmented communication. There is even a potentially useful connection for the application of home healthcare: indeed as users increasingly look for online information towards the management of their health, including participation in online social networks which may have very cluttered presentations, a system such as ours may prove to be very beneficial in enabling the success of patient-led care in the twenty-first century. This is of course just one example of where we are motivated to be making a difference through our particular effort that combines a collective expertise in computer vision and human-computer interaction. With our detailed system design in hand, together with important validation of the proposed methods as effective (in comparison with existing methods), we are now well positioned to proceed with studies that engage our user population, driving our future efforts with this research.

@&#ACKNOWLEDGMENTS@&#

Thanks to NSERC (Natural Sciences and Engineering Research Council of Canada) for financial support. We also wish to acknowledge the contributions of Shari Trewin from IBM TJ Watson during initial brainstorming of ideas on a desiderata for reducing clutter in webpages, as HCI assistive technology; and John A. Doucette, for feedback on an earlier version of the paper. We are grateful as well to the anonymous reviewers for their very helpful comments.


                     Algorithm 2 shows our segmentation algorithm, including the optimization process. It produces a tree, with the key of each node providing the necessary information about the division into child regions; this is represented by a tuple of the form (K, C), where K is the key and C is the set of children. Parameters of the segmentation algorithm which are not generally altered are the maximum segmentation depth dmax
                     , the maximum number of candidate boundaries (either horizontal or vertical) nmax
                     , the threshold of significance update increment δt
                     , and the initial threshold of significance tinit
                     . These parameters are set based on performance considerations 
                        7
                     
                     
                        7
                        In our implementation, 
                              
                                 
                                    d
                                    
                                       m
                                       a
                                       x
                                    
                                 
                                 =
                                 6
                                 ,
                              
                           
                           
                              
                                 
                                    δ
                                    t
                                 
                                 =
                                 0.5
                                 ,
                              
                            and 
                              
                                 
                                    t
                                    
                                       i
                                       n
                                       i
                                       t
                                    
                                 
                                 =
                                 3
                              
                           . We pre-computed sets of tilings for candidate boundary grids of different sizes for efficiency reasons; rather than a fixed nmax
                           , the maximum number of grid cells formed by the candidate boundaries was 35, except for single columns or rows, where it was somewhat lower at 23. The algorithm shown is slightly idealized, using the simpler (but possibly slower) fixed maximum.
                     . The “current depth” parameter d represents the depth of the tree if generation stops at the current iteration; in calling the function, it is set to 1, and it is updated automatically for recursive calls.

In this appendix, we show several examples of common cases in which our purely image-based page segmentation method has an advantage
                        8
                     
                     
                        8
                        Any image-based segmentation method would have these kinds of advantages.
                      over a DOM tree-based segmentation method because it is image-based. These scenarios include changes in the implementation language, information embedded in an image, and navigation using a screenreader.

It is common for page segmentation systems that use the source code or DOM tree to segment the page to use HTML tags to guide the segmentation—by, for example, treating different tags differently according to their typical role in a page. This practice introduces a dependence on the implementation language which can be problematic over time. A good example of this is the transition from HTML 4.0.1 to HTML5. Many new tags were introduced in HTML5, and some older tags were deprecated. Segmentation algorithms that were designed before the introduction of these new tags obviously cannot include them for consideration.

VIPS [8], as one of the most prominent page segmentation methods in the literature, is a good example of this phenomenon. The following tags and DOM node types are given special treatment in terms of segmentation rules:

                           
                              •
                              Inline text nodes


                                 TABLE
                              


                                 TR
                              


                                 TD
                              


                                 P
                              

Additionally, HTML tags are used to determine the degree of coherence for a given block in the segmentation, which plays an important role in guiding the segmentation process. New tags introduced in HTML5 would, at best, simply use the default set of rules and a default degree of coherence value in a given implementation; to include the new tags would require a extending the algorithm to recognize them and determining the appropriate way to process them—a significant amount of work.

Our purely image-based algorithm, on the other hand, has no interaction with the page source code; only the appearance of the rendered web page affects its performance, so it is not sensitive to implementation details. It would, therefore, require much less (if any) modification to support new technology. As browsers are updated to properly render new versions of HTML, CSS, or other implementation languages, the assistive interface could make use of this work by simply capturing and analyzing the rendered page, thus reducing maintenance requirements for the corresponding assistive interfaces. This is an important advantage, since assistive technology systems have a smaller user community than, say, a major web browser, and the developer community is likely to be correspondingly smaller. This problem is discussed in [61], which touches on users’ impressions of the issue.

It is often the case that there is structured information embedded in images in a web page. While it is often possible to obtain the same layout and presentation using HTML and CSS (especially with HTML5), images are still very commonly used for graphs, infographics, and other types of structured content. Fig. B.22 shows the detection of edges within a screenshot from a cell phone interface. Our image-based segmentation method seamlessly handles the transition to the image, in much the same way that a user would perceive the internal structure. The DOM tree-based control algorithm, on the other hand, is unable to detect this internal structure. The ability to detect boundaries within images is a significant advantage of our method over methods based on the page source code, for which the internal structure of an image is entirely inaccessible.

If one examines sample transcripts from screenreaders
                           9
                        
                        
                           9
                           
                              e.g. NVDA, found at nvaccess.org
                           
                        , there are examples of rather complicated navigation methods presented to users. Our particular approach for visual-based depiction of webpages has been discussed in Section 4.1 as one that offers a more versatile and no less simple solution for users, than competing approaches tied to the DOM-tree. We note that what we offer to users with assistive needs is in line with HCI literature confirming that in the context of hierarchy complexity, those with cognitive impairments (specifically visuospatial disabilities) need support for discrimination, encoding and recognizing landmarks. Arguably, empty or extraneous containers are poor landmarks (and these may arise if relying simply on a raw DOM tree solution). Findings about the value of creating simpler hierarchies for users to navigate are also included in [62]; this paper reinforces the conclusions of [40] for the context of non-visual navigation, demonstrating that these principles also apply for blind users using screenreaders. Recent surveys of screenreader users
                           10
                        
                        
                           10
                           See webaim.org/projects/screenreadersurvey and webaim.org/projects/screenreadersurvey2
                           
                         find that a vast majority of users have significant issues regarding accessing Flash content (over 70% find it to be difficult). This provides additional evidence for the advantages that we would offer, compared to solutions tied to the implementation code, such as VIPS.

@&#REFERENCES@&#

