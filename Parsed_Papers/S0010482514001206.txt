@&#MAIN-TITLE@&#A prediction model of drug-induced ototoxicity developed by an optimal support vector machine (SVM) method

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A classification model of drug-induced ototoxicity is established by GA-CG-SVM.


                        
                        
                           
                           The naïve Bayesian method is used to develop prediction models of ototoxicity.


                        
                        
                           
                           The RP method is used to develop prediction models of ototoxicity.


                        
                        
                           
                           The established GA-CG-SVM model II outperforms models developed by other methods.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Drug-induced ototoxicity

Support vector machine

Naïve Bayesian

Recursive partitioning

Classification

@&#ABSTRACT@&#


               
               
                  Drug-induced ototoxicity, as a toxic side effect, is an important issue needed to be considered in drug discovery. Nevertheless, current experimental methods used to evaluate drug-induced ototoxicity are often time-consuming and expensive, indicating that they are not suitable for a large-scale evaluation of drug-induced ototoxicity in the early stage of drug discovery. We thus, in this investigation, established an effective computational prediction model of drug-induced ototoxicity using an optimal support vector machine (SVM) method, GA-CG-SVM. Three GA-CG-SVM models were developed based on three training sets containing agents bearing different risk levels of drug-induced ototoxicity. For comparison, models based on naïve Bayesian (NB) and recursive partitioning (RP) methods were also used on the same training sets. Among all the prediction models, the GA-CG-SVM model II showed the best performance, which offered prediction accuracies of 85.33% and 83.05% for two independent test sets, respectively. Overall, the good performance of the GA-CG-SVM model II indicates that it could be used for the prediction of drug-induced ototoxicity in the early stage of drug discovery.
               
            

@&#INTRODUCTION@&#

Clinical studies have shown that some drugs, such as amikacin, gentamicin and neomycin, could lead damage to the ear, specifically the cochlea or auditory nerve and sometimes the vestibular system [1–3]. Symptoms of the drug-induced ototoxicity vary considerably from drug to drug and person to person, ranging from mild imbalance to total incapacitation, and tinnitus to total hearing loss. The drug-induced ototoxicity may be either reversible and temporary, or irreversible and permanent. The bad thing is that currently there is no effective treatment to reverse the effects of ototoxicity if permanent damage happens to the ear [1,2]. Therefore, drug-induced ototoxicity, like other drug toxicities, is also an important issue needed to be considered in drug discovery.

Currently, several methods based on chemical biology have been used to evaluate drug-induced ototoxicity [4]. However, these methods require a number of experiments, most of which are time-consuming and expensive, indicating that they are not suitable for a large-scale evaluation of drug-induced ototoxicity in the early stage of drug discovery. Lately emerging zebrafish-based methods seemed faster and cheaper [5,6]. However, due to the relatively large species difference between zebrafish and humans, these methods are prone to misjudge the drug-induced ototoxicity [4,7]. Therefore, it is highly needed to develop more efficient and fast methods for a large-scale evaluation of drug-induced ototoxicity in drug discovery.

Computational methods have been thought as a faster and cheaper strategy and have been successfully used in the prediction of various pharmacokinetic and toxic properties of drugs. For example, Wang et al. developed prediction models of human ether-a-go-go related gene (hERG) potassium channel blockage using the naïve Bayesian (NB) and recursive partitioning (RP) methods [8]. Burton et al. used the RP method to develop prediction models of Cytochromes P450 2D6 and 1A2 inhibitors [9]. Ma et al. developed a prediction model of drug oral bioavailability by the support vector machine (SVM) method [10]. More computational prediction models of pharmacokinetic and toxic properties could be found in the literature [11–14]. However, as far as we know, there is no report of computational model for the prediction of drug-induced ototoxicity. Therefore, we shall, in this investigation, develop a computational classification prediction model of drug-induced ototoxicity using the SVM method [15,16]. Here we chose the SVM method because SVM has been demonstrated to be one of the best statistical learning methods and has shown better performances in the development of prediction models of pharmacokinetic and toxic properties than many other methods [17–21]. Specifically, we used in this study a modified version of SVM, namely GA-CG-SVM [22], in which the genetic algorithm (GA) [23]is used for the feature selection and the conjugate gradient (CG) [24] method for the parameter optimization. A main advantage of GA-CG-SVM is that it can concurrently optimize the features and SVM parameters. GA-CG-SVM has been demonstrated to outperform the common SVM method [10,17]. For comparison, NB and RP methods will also be used to build prediction models of drug-induced ototoxicity; the two methods were chosen since they are also very good and popular methods for classification modeling.

SVM is a supervised machine learning method, which has been widely used for classification and regression analysis. It has shown a good performance in solving a number of biological classification problems [25–27]. Nevertheless, some issues that may have important influence on the quality of established models are often not or insufficiently considered in SVM modeling, such as feature selection and parameter optimization. Our group recently developed an optimal SVM method, termed GA-CG-SVM, in which feature selection and parameter optimization are efficiently handled in SVM modeling. Detailed algorithms for GA-CG-SVM have already been described in a previous paper [22]. Here we just make a brief summary for the basic idea of GA-CG-SVM.

Supposing a given training set 
                           {
                           (
                           
                              
                                 x
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 y
                              
                              
                                 1
                              
                           
                           )
                           ,
                           (
                           
                              
                                 x
                              
                              
                                 2
                              
                           
                           ,
                           
                              
                                 y
                              
                              
                                 2
                              
                           
                           )
                           ,
                           ..
                           .
                           (
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                           ,
                           
                              
                                 y
                              
                              
                                 i
                              
                           
                           )
                           }
                         where 
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                         represents a vector of n real numbers (features or descriptors) and 
                           
                              
                                 y
                              
                              
                                 i
                              
                           
                         is the class that vector 
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                         belongs to. The purpose of SVM classification is to find an optimal hyperplane to separate these classes with the maximum margin, which needs to solve the following optimization problem:
                           
                              (1)
                              
                                 
                                    
                                       Max
                                    
                                    
                                       w
                                       ,
                                       b
                                    
                                 
                                 
                                    2
                                    
                                       |
                                       |
                                       w
                                       |
                                       |
                                    
                                 
                                 
                                 subject
                                 
                                 to
                                 
                                 
                                    
                                       y
                                    
                                    
                                       i
                                    
                                 
                                 (
                                 w
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 +
                                 b
                                 )
                                 −
                                 1
                                 ≥
                                 0
                              
                           
                        where 
                           2
                           /
                           |
                           |
                           w
                           |
                           |
                         is the margin.

For linearly separable cases, it is easy to find an optimal separating hyperplane by a classifying determination function. For linearly non-separable cases, there is no hyperplane that can be used to perfectly separate two sets of points (See Supplementary Figure S1). Therefore, non-negative slack variables 
                           
                              
                                 ξ
                              
                              
                                 i
                              
                           
                           ≥
                           0
                           ,
                           
                           i
                           =
                           1
                           ,
                           ...
                           .
                           ,
                           m
                         were introduced. The equation to be solved becomes:
                           
                              (2)
                              
                                 
                                    
                                       Max
                                    
                                    
                                       w
                                       ,
                                       b
                                    
                                 
                                 
                                    2
                                    
                                       |
                                       |
                                       w
                                       |
                                       |
                                    
                                 
                                 +
                                 C
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    m
                                 
                                 
                                    
                                       
                                          ξ
                                       
                                       
                                          i
                                       
                                    
                                 
                                 
                                 subject
                                 
                                 to
                                 
                                 
                                    
                                       y
                                    
                                    
                                       i
                                    
                                 
                                 (
                                 w
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 +
                                 b
                                 )
                                 −
                                 1
                                 +
                                 
                                    
                                       ξ
                                    
                                    
                                       i
                                    
                                 
                                 ≥
                                 0
                              
                           
                        where C is a user predetermined penalty parameter.

For nonlinear (non-) separable cases, the basic idea is to project the input data set 
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                         into a high-dimensional feature space via a nonlinear manner using a kernel function [28]. Until now, many kernel functions have been suggested for this purpose. Among them, the radial basis function (RBF) is widely used and performed very well in most cases [29]. Thus, the RBF kernel function was also selected in this study.
                           
                              (3)
                              
                                 k
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       x
                                    
                                    
                                       j
                                    
                                 
                                 )
                                 =
                                 e
                                 x
                                 p
                                 (
                                 −
                                 γ
                                 |
                                 |
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 −
                                 
                                    
                                       x
                                    
                                    
                                       j
                                    
                                 
                                 |
                                 
                                    
                                       |
                                    
                                    2
                                 
                                 )
                              
                           
                        where the parameter 
                           γ
                         denotes the width of Gaussian kernel.

As mentioned previously, the selection of penalty parameter C and kernel parameter 
                           γ
                         in SVM modeling has significant influence on the predictive accuracy of an SVM model. Thus, we used the conjugate gradient method to optimize the two parameters. Additionally, the selection of features is also of importance to the prediction ability of an SVM model [30]. The GA is a very popular optimization algorithm, which is based on the Darwinian evolutionary idea of natural selection and genetics in biological systems. GA has been widely used to solve a range of diverse problems such as data mining and optimization [31,32]. Here, we applied GA for the feature selection in SVM modeling. Finally, it is worth mentioning that an integrated scheme for the simultaneous treatment of both feature selection (by GA) and parameter optimization (by CG) was adopted. This is important since it has been shown that the feature selection and parameter setting influence each other in SVM modeling [33].

A naive Bayesian classifier is a simple probabilistic classifier based on applying Bayes׳ theorem with strong (naive) independence assumptions [34]. In NB, each object is described by an n-dimensional vector 
                           F
                           =
                           (
                           
                              
                                 f
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 f
                              
                              
                                 2
                              
                           
                           ,
                           ...
                           .
                           ,
                           
                              
                                 f
                              
                              
                                 n
                              
                           
                           )
                           ,
                         where 
                           (
                           
                              
                                 f
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 f
                              
                              
                                 2
                              
                           
                           ,
                           ...
                           .
                           ,
                           
                              
                                 f
                              
                              
                                 n
                              
                           
                           )
                         represents n features. Objects belonging to the first class are each labeled a value of 
                           
                              
                                 CL
                              
                              
                                 1
                              
                           
                           =
                           1
                        , while those in the second class are assigned 
                           
                              
                                 CL
                              
                              
                                 2
                              
                           
                           =
                           −
                           1
                        . Based on the Bayes׳ theorem, we got
                           
                              (4)
                              
                                 p
                                 (
                                 
                                    
                                       CL
                                    
                                    
                                       i
                                    
                                 
                                 |
                                 F
                                 )
                                 =
                                 
                                    
                                       p
                                       (
                                       F
                                       |
                                       
                                          
                                             CL
                                          
                                          
                                             i
                                          
                                       
                                       )
                                       p
                                       (
                                       
                                          
                                             CL
                                          
                                          
                                             i
                                          
                                       
                                       )
                                    
                                    
                                       p
                                       (
                                       F
                                       )
                                    
                                 
                              
                           
                        where 
                           p
                           (
                           
                              
                                 CL
                              
                              
                                 i
                              
                           
                           |
                           F
                           )
                         denotes the posterior probability, and 
                           p
                           (
                           
                              
                                 CL
                              
                              
                                 i
                              
                           
                           )
                         represents the prior probability. 
                           p
                           (
                           F
                           |
                           
                              
                                 CL
                              
                              
                                 i
                              
                           
                           )
                         and 
                           p
                           (
                           F
                           )
                         are conditional probability and marginal probability, respectively. 
                           p
                           (
                           F
                           )
                         is constant for all classes. 
                           p
                           (
                           F
                           |
                           
                              
                                 CL
                              
                              
                                 i
                              
                           
                           )
                         and 
                           p
                           (
                           F
                           )
                         can be learned from a training set.

In this study, NB was used for the development of prediction models of drug-induced ototoxicity. 
                           
                              
                                 CL
                              
                              
                                 1
                              
                           
                         and 
                           
                              
                                 CL
                              
                              
                                 2
                              
                           
                         represent ototoxic drug class and non-ototoxic drug class, respectively. The naive Bayesian classifier was constructed using the Discovery Studio 3.1 software package.

RP is a statistical method for multivariable analysis, which is also a popular classification method [35,36]. RP produces a decision tree that strives to correctly classify members of the population based on several dichotomous dependent variables. At each node of the decision tree, the data are split into two subsets based on a particular descriptor and corresponding splitting value, which are decided by an automated statistical analysis of the entire data set. The splitting process continues until no more significant nodes are obtained or a threshold value for stopping is reached. Let X stand for a set of independent properties (molecular properties) and Y represent a dependent property (ototoxicity class). With RP method, created decision trees can recursively partition data according to the relationship between X and Y values. In this study, 5-fold cross validation was used, splits were scored using Gini index and the minimum number of samples at each node was set to 10 to avoid excessive partitioning. Furthermore, the maximum tree depth was changed from 2 to 10 systematically in order to find a better RP model. Our recursive partitioning classifiers were built using the Discovery Studio 3.1 software package.

A total of 572 small molecule compounds (positive) that have been reported to bear ototoxicity were collected from reference [37], which contains the most updated collections of ototoxic drugs. 347 drug molecules (negative) for which there is no report to bear ototoxicity were collected from DrugBank [38]. We used the “Generate Training and Test Data” module in Discovery Studio to randomly select 20% compounds (121 positives and 63 negatives) to form an independent test set (called TS1) in advance. The remaining positives and negatives were taken to construct three training sets, namely dataset I, II and III; these compounds could occur in different sets. The biggest difference of these datasets is the risk or strength of ototoxicity of positive compounds. According to Bauman, the ototoxic drugs were divided into five classes (class 1, 2, 3, 4 and 5) based on their risk or strength of ototoxicity [37]. Compounds in class 1 have the lowest risk; compounds in class 5 have the highest risk; compounds in class 2–4 have an increasing risk. Dataset I includes positive compounds of all of the risk classes (class 1, 2, 3, 4 and 5). Dataset II contains positive compounds of risk class 2, 3, 4 and 5. Dataset III includes positive compounds of higher risk classes (class 3, 4 and 5). The finally formed dataset I consists of 451 positives and 284 negatives (see Supplementary Table S1). Dataset II contains 252 positives and 284 negatives (see Supplementary Table S2). Dataset III includes 64 positives and 284 negatives (see Supplementary Table S3). Test set TS1 (121 positives and 63 negatives) is presented in Supplementary Table S4.

To further evaluate the established models, we constructed another independent test set, called TS2. TS2 contains 19 positives and 40 negatives (see Supplementary Table S5). Positives in TS2 were collected from Hazardous Substances Data Bank (HSDB) of TOXNET toxicology data network [39]. Negatives in TS2 were again collected from DrugBank [37]. There is no overlap between TS2 and any one of training set I, II, III and TS1.

Molecular descriptors used in this study were calculated by Discovery Studio 3.1 software package. Initially, a total of 237 molecular descriptors for each molecule were calculated; these descriptors cover a variety of molecular properties, including topological descriptors, element counts, AlogP, surface area and volume, and so on. After that, a preprocessing procedure was conducted to these calculated descriptors. The preprocessing procedure includes two steps. In the first step, some “bad” descriptors were removed. For example, descriptors with too many zero values, bearing a very small standard deviation value with others (<0.5%), or having a high correlation coefficient with others (>95%), were deleted from the descriptor list. In the second step, all the descriptor values for the remaining descriptors were scaled to a range of −1 to 1. These preprocessed descriptors were further optimized by GA (for GA-CG-SVM) or a Monte Carlo (MC) method (for NB and RP), a feature selection method similar to that used in reference [40]. Finally, these selected descriptors were used for the development of prediction models of drug-induced ototoxicity.

To assess the performance of the established prediction models, the following quantities were calculated: true positives (TP, true ototoxic drugs), true negatives (TN, true non-ototoxic drugs), false positives (FP, false ototoxic drugs) and false negatives (FN, false non-ototoxic drugs). Sensitivity SE=TP/(TP+FN) and specificity SP=TN/(TN+FP) are the prediction accuracy for ototoxic drugs and non-ototoxic drugs, respectively. The overall accuracy (Q) was calculated by the equation: Q=(TP+TN)/(TP+TN+FP+FN). The Matthew׳s correlation (MCC) was calculated by the following equation:
                           
                              
                                 MCC
                                 =
                                 (
                                 TP
                                 ×
                                 TN
                                 −
                                 FN
                                 ×
                                 FP
                                 )
                                 /
                                 
                                    
                                       (
                                       TP
                                       +
                                       FN
                                       )
                                       (
                                       TP
                                       +
                                       FP
                                       )
                                       (
                                       TN
                                       +
                                       FN
                                       )
                                       (
                                       TN
                                       +
                                       FP
                                       )
                                    
                                 
                              
                           
                        
                     

@&#RESULTS@&#

Three prediction models, namely GA-CG-SVM model I, II and III, were developed using GA-CG-SVM based on dataset I, II and III, respectively. The compositions of these three datasets are shown in Supplementary Table S6. Numbers of descriptors, which were optimized by GA, are 27, 32 and 23 for GA-CG-SVM model I, II and III, respectively. 
                        Table 1 shows the 5-fold cross validation results for the established models. Among the three models, the GA-CG-SVM model III, which was developed based on dataset III, had the highest overall prediction accuracy (91.38%). Nevertheless, the prediction accuracy for positives was just 59.38%. Though GA-CG-SVM model I, which was developed based on dataset I, gave the highest prediction accuracy for positives (84.70%), it offered the lowest overall prediction accuracy (82.31%). Comparatively speaking, the GA-CG-SVM model II, which was developed based on dataset II, is relatively superior; it gave an overall prediction accuracy of 86.75% and a prediction accuracy of 82.94% for positives.

A good SVM model is not only able to classify training set correctly but also capable of categorizing external agents that are outside of the training set. Thus, an independent validation set, TS1, was further used to assess the predictability of these SVM models just built. The predicted results for TS1 are also shown in Table 1. For GA-CG-SVM model I, the calculated SE and SP were 63.64% and 88.89%, respectively, and the overall accuracy was 72.28%. For GA-CG-SVM model II, the calculated SE, SP and the overall accuracy were 81.82%, 92.06% and 85.33%, respectively. The calculated SE, SP and the overall accuracy for GA-CG-SVM model III were 40.50%, 100% and 60.87%, respectively. Clearly, among the three models, the GA-CG-SVM model II has a superior performance in terms of the overall accuracy and the accuracy for positives (SE).

For comparison, we also developed prediction models of drug-induced ototoxicity using naïve Bayesian and recursive partitioning methods. The same datasets and initial descriptors as those used in the development of GA-CG-SVM models were used. The initial descriptors were preprocessed in advance, followed by an optimization by the MC method. The predicted results for the RP models based on the three datasets with different tree depths are shown in Supplementary Table S7, S8 and S9, respectively. For dataset I, the RP-9 model (tree depth: 10) had the highest overall accuracy (Supplementary Table S7 and 
                        Table 2, 76.63%). For dataset II, the highest overall accuracy (Supplementary Table S8 and Table 2, 79.89%) corresponds to the RP-6 model (tree depth: 7). For dataset III, the RP-4 model (tree depth: 5) offered the highest overall prediction accuracy (Supplementary Table S9 and Table 2, 72.83%). For the naïve Bayesian models, the overall prediction accuracy was 73.37% for NB model I (developed based on dataset I), 76.09% for NB model II (developed based on dataset II) and 71.20% for NB model III (developed based on dataset III). For comparison, all the predicted results of models developed by GA-CG-SVM, RP and NB are summarized in Table 2. From Table 2, we can see that the overall prediction accuracy of GA-CG-SVM model II was still the highest one among those of all the models established here.

To further evaluate the GA-CG-SVM model II, we re-constructed a new independent test set, TS2, which contains 19 positives and 40 negatives. Different from TS1, the positive compounds in TS2 were collected from HSDB of TOXNET toxicology data network [37]. 
                        Table 3 shows the predicted results of GA-CG-SVM model II to TS2. The SE and SP are 78.95% and 85.00%, respectively. The overall prediction accuracy is 83.05%. Again, for comparison, the prediction results of three RP models (RP model I, II, and III, details see Supplementary Tables S10, S11, and S12) and three NB models (NB model I, II, and III, details see Supplementary Table S13) are also summarized in Table 3. Clearly, the GA-CG-SVM model II still outperforms all the RP and NB models established here in terms of the prediction accuracy.

@&#DISCUSSION@&#

Drug-induced ototoxicity is a severe adverse effect of drugs. However, the molecular mechanisms of drug-induced ototoxicity are extremely complicated and far from being established. Thus it is difficult to predict the ototoxic potential of new drugs using traditional statistical methods or structure-toxicity methods. Dealing with those systems with complex mechanisms is the strong point of SVM approach. The SVM method has shown promising capability for solving a number of biological classification problems [25–27]. However, some problems still exist in SVM modeling, i.e., feature selection and parameters optimization. GA-CG-SVM is a modified SVM method that can handle simultaneously the feature selection and parameters optimization [22]. Previous studies have also shown that the GA-CG-SVM method could give higher prediction accuracy than traditional methods [10,17,25]. The good performance of SVM model established here once again demonstrates the capability of GA-CG-SVM method in dealing with complicated systems. In the established GA-CG-SVM model II, 32 different molecular descriptors (see 
                     Table 4) were selected by the GA algorithm, which covers diverse molecular properties including molecular structural information, lipophilicity, hydrogen bonding feature, molecular electronic properties, molecular aromatic functions and molecular polar surface area. The involvement of so many molecular descriptors reflects at least to some extent that the drug-induced ototoxicity is affected by many complicated factors.

In the SVM modeling, we elaborately constructed the training sets. The ototoxic drugs were firstly assigned into three training datasets (I, II and III) according to their risk or strength. Of these datasets, dataset III contains a relatively stronger condition, which means that positive compounds in dataset III are definitely able to induce ototoxicity and have a relatively higher potency. On the contrary, dataset I contains a weaker condition, which means that positive compounds in dataset I also include those that are just suspected to be able to induce ototoxicity or have a lower potency, in addition to compounds that are definitely able to induce ototoxicity and have a relatively higher potency. Dataset II contains a medium condition. Based on the constructed datasets, three prediction models of drug-induced ototoxicity were then established. The GA-CG-SVM model II, which was developed based on dataset II, was superior compared with other models. These results reflect the fact that the training set is critical to the quality of generated SVM models, although we cannot conclude that a training set containing a medium condition must be better than that containing a stronger or weaker condition.

Finally, GA-CG-SVM displayed a better performance in prediction of drug-induced ototoxicity than NB and RP. In prediction of some other pharmacokinetic and toxic properties, this method also showed excellent performance. Even so, we still cannot guarantee that GA-CG-SVM must perform well and outperform other methods in various application fields. In fact, many factors may have influence on the quality of models established by a specific modeling method. These factors include the size, diversity, and representativeness of training sets, the number of descriptors, and the link between descriptor values and properties of samples. Among all the mentioned factors, what is worth mentioning is the size of training set. For majority of the modeling methods, a large training set benefits to the generation of models with a high quality. Nevertheless, in SVM, the size of training set is not decisively important on the model quality if samples constituting the supporting vectors have already been included in the training set [41].

In summary, we have established an effective prediction model of drug-induced ototoxicity using GA-CG-SVM. The GA-CG-SVM model II, which is the best GA-CG-SVM model among the three GA-CG-SVM models developed based on different training sets, gave an overall prediction accuracy of 85.33% and 83.05% for the independent test set TS1 and TS2, respectively. A comparison analysis showed that the GA-CG-SVM model II outperformed the RP and NB models, which were developed by the recursive partitioning and naïve Bayesian methods, respectively. All of these results indicate that the GA-CG-SVM model II is a good prediction model of drug-induced ototoxicity. It is expected that the GA-CG-SVM model established here can be used as an effective screening tool for identifying the ototoxic drugs and non-ototoxic drugs in the early stage of drug discovery.

None declared.

@&#ACKNOWLEDGMENTS@&#

This work was supported by the 863 Hi-Tech Programs (2012AA020301, 2012AA0203), National Natural Science Funds for Distinguished Young Scholar (81325021) and National Science and Technology Major Project (2012ZX09501001–003).

Supplementary data associated with this article can be found in the online version at doi:10.1016/j.compbiomed.2014.05.005.


                     
                        
                           
                              Supplementary Data
                           
                           
                        
                     
                  

@&#REFERENCES@&#

