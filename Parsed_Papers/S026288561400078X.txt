@&#MAIN-TITLE@&#Automatic expression spotting in videos

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Temporally segments macro- and micro-facial expressions from video


                        
                        
                           
                           Does not rely on trained model of particular expression(s)


                        
                        
                           
                           Measures the strain (deformation) impacted on facial skin tissue


                        
                        
                           
                           The method successfully detects both spontaneous and feigned expressions.


                        
                        
                           
                           The method works at several pixel resolutions.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Expression spotting

Macro-expressions

Micro-expressions

@&#ABSTRACT@&#


               
               
                  In this paper, we propose a novel solution for the problem of segmenting macro- and micro-expression frames (or retrieving the expression intervals) in video sequences, which is a prior step for many expression recognition algorithms. The proposed method exploits the non-rigid facial motion that occurs during facial expressions by capturing the optical strain corresponding to the elastic deformation of facial skin tissue. The method is capable of spotting both macro-expressions which are typically associated with expressed emotions and rapid micro- expressions which are typically associated with semi-suppressed macro-expressions. We test our algorithm on several datasets, including a newly released hour-long video with two subjects recorded in a natural setting that includes spontaneous facial expressions. We also report results on a dataset that contains 75 feigned macro-expressions and 37 feigned micro-expressions. We achieve over a 75% true positive rate with a 1% false positive rate for macro-expressions, and a nearly 80% true positive rate for spotting micro-expressions with a .3% false positive rate.
               
            

@&#INTRODUCTION@&#

Accurately and automatically spotting frames containing facial expressions in videos is often a prior step required for high-level facial analysis, including identifying emotional response, gestures, and human identification. In many papers, this is a manual pre-processing step, or it is assumed that the data consists of a single facial expression sequence. We address this problem using an optical strain based method that is capable of automatically spotting expressions in video sequences.

In this paper we do not address the problem of identifying expressions, but only solve the expression segmentation problem (see Fig. 1
                     ). Since our method is based on the non-rigid motion of the face, and not on pre-defined expression models, we are able to capture a large variety of facial motion corresponding to facial expressions. In other words, while some traditional techniques are capable of recognizing pre-defined expressions (such as recognizing when a person smiles or shows surprise in a video sequence), it is not possible for these types of methods to recognize expressions for which the algorithm has not been trained. We propose a novel expression spotting method that can be used to locate and distinguish between two broad classes of expressions. First are macro-expressions, which are generally characterized as occurring several seconds over several regions of the face. The second class of expressions typically occurs rapidly and in a single region of the face, or micro-expressions.

The method presented in this paper represents our complete work on expression spotting. Earlier ideas related to this method were documented in [1], with some further results reported in [2]. Some of our early micro-expression work was reported in [3] and also in a medical application [4]. We have developed a completely new algorithm compared with our prior work. Specifically, we have included more robust face tracking, a new masking technique, and a new peak detection algorithm. We show the performance of the algorithm at several scaled resolutions. We give results on more challenging datasets, including longer videos that contain a mixture of both macro- and micro-expressions during a single sequence, as well as spontaneous (genuine) facial expressions.

The method consists of the following steps: (i) the face and eyes are detected in all frames of the video sequence. These coordinates are then used to segment the face in to several regions; (ii) the non-rigid motion is estimated using an optical flow based method over several frames, for each region; (iii) a masking technique is used that removes erroneous flow estimation caused by blinking the eyes or the opening and closing of the mouth; (iv) optical strain maps are calculated over each pair of frames to generate strain maps, which are then summed to generate strain magnitude for four different regions on the face; (v) lastly, a peak detection method is used to locate intervals that correspond with macro-expressions [5], and the remaining intervals are then analyzed for micro-expressions.

It is important to note that while many papers address the problem of expression spotting, the definition of spotting is not always consistent. It may also be referred to as expression detection, or in the case of genuine expressions, spontaneous [6] or authentic expression analysis [7]. In some papers, this refers to determining if a pre-segmented group of frames does or does not contain an expression. For example Zeng et al. [6] propose a single classification method for detecting spontaneous expressions (emotion or non-emotion) by training a Support Vector Data Description (SVDD) on several examples of emotion data. Then, test segments containing roughly an equal number of frames are classified with a single binary decision (hence chance is 50%). The same type of experimental setup can be found in [8] for macro-expressions, and in Pfister et al. [9] and Polikovsky et al. [10] for micro-expressions. In [9], local spatio-temporal features are extracted from a high-speed video sequence (100frame/s) and then performance is measured using several classifiers. Similarly in [10], a high speed camera (300frames/s) is used to capture rapid micro-expressions. In their work, they use 3-D gradient histograms and the Facial Action Coding System (FACS) to spot the 4 states of micro-expressions: onset, apex, apex offset, and apex neutral.

A single frame approach using Gabor filters and GentleSVM is used by Wu et al. [11] Their work is based on the assumption that the appearance of micro-expressions completely resemble macro-expressions, and thus they reduce the entire micro-expression problem to the temporal duration of macro-expressions. While this definition may fit a subset of micro-expressions, we instead use the general categorization found in [12] that defines them as a suppression of macro-expressions. Hence, they are often distorted or only fractional representations of macro-expressions [13].

Some papers that address the problem of automatic expression analysis do so only for pre-trained examples, or in other words, they are capable of recognizing a subset of expressions in uncut videos. For instance, a dynamic approach can be found in the work by Sung et al. [14], the authors use generalized discriminant analysis for recognizing a subject randomly expressing four different facial expressions (neutral, happy, surprised, angry) in roughly 30second videos. Another example can be found in [15] where seven expressions (neutral, sadness, anger, joy, fear, disgust, surprise) are automatically recognized in videos. A method that can detect several more affective states can be found in [16].

Static (single frame) approaches that model a subset of macro-expressions are also found extensively in the literature, with perhaps local binary patterns performing among the best [17]. In the work by Ruiz-Hernandez and Pietikainen [18], a novel LBP encoding technique is proposed that uses a re-parametrization of the second order Gaussian jet. Similar to [9] they do not perform spotting, but test on sequences that each contains a single expression.

A popular method for describing several types of expressions on the face is the Facial Action Coding Systems (FACSs). In this system, an action unit label is given to different types of facial motion, and the activation of one or more action units can be used to describe a facial expression. There are several works in the literature that automatically detect action units, as well as provide a measure of intensity [19,20]. While there is a similarity between detecting the activation of an action unit and detecting expressions, the two are not equivalent. For instance, in the DISFA dataset [21], every action unit is a measure of several parts of expressions, but not fully reduced; in other words, there are still types of expressions, especially micro-expressions, that are not represented by the labeled action units. Of course, it is possible to find an action unit to describe every type of motion on the face, but then training would be required on each of them. Hence, the main contribution of our work is that individual training of for each type of motion, or action unit, is not needed. The goal of our approach is to successfully detect any type of motion that causes strain, or deformation on the face.

In the work by Zhou et al. [22], the authors propose to use Aligned Cluster Analysis on points obtained using FACS. In their work, they are able to identify differing types of spontaneous facial expressions, although it is dependent on a manually defined number of expression clusters, and performance on micro-expression detection is not given. Another unsupervised approach is given by Liwicki et al. [23]. In this approach, an online temporal video segmentation technique is described that uses a subspace learning method. While it has been used to successfully segment several macro-expressions from a video sequence, it assumes that changes in a scene (or signal) occur slowly, thus it does not appear to be suited for rapidmicro-expressions which can occur in a little as 2–3frames.

Similarly to the unsupervised methods, we do not rely on previously trained models of expressions. However, we want to emphasize a few key highlights of our method: (i) we rely on the fundamental dynamics of facial expressions, in that they cause the non-rigid deformation of the facial skin tissue. Hence, our method is naturally suited to spot all expressions that cause facial skin deformation; (ii) we detect facial expression over the entire video sequence, without any manual temporal pre-segmentation (however we do provide results for an experiment that is formatted similarly to [9] and [18] who both test on the SMIC corpus, so performance can be compared); and (iii) we are not aware of any other method that has yet been proposed that detects both macro- and micro-expressions. Finally, for clarity, we propose that expression spotting refer to the temporal segmentation of an entire video into segments that only contain the frames of each expression.

To further place our method into perspective, we find the categorization of motion-based methods useful for expression spotting. In general, they have been organized [24] into three types, namely: point-model, holistic, or some mixture of these two. Point-model approaches track several key points on the face over time. The interplay of these points can then be used to recognize the expression [25]. However, while these algorithms may be sufficient for large macro-expressions, the average 2–3pixel “jittering” in frame to frame tracking often suppresses the nearly equally small movement observed in micro-expressions. Alternatively, holistic approaches track all points on the face [26], and hence become more suitable for detecting a larger variety of possible facial expressions, including small motion.

Our method fits into the last category, i.e., it uses both a point-model and holistic approach. Our approach segments the face into several regions based on several detected landmarks. Then, within these segmented regions we use a holistic optical flow method that densely tracks each point on the face. Hence, we hope to have minimized the drawback of approaches in the first category, while taking advantage of the potential robustness associated with methods in the second category.

@&#BACKGROUND@&#

Expressions are generally believed to be the physiological response to an internal emotional state. While there does appear to be a universality for some expressions (such as happiness, sadness, surprise, disgust, and anger) there are a much larger number of possible expressions possible, as well as large inter- and intra-variability between subjects for the same expression. For instance in Fig. 2
                      there some expressions we may immediately recognize (such as anger in column d), however some other expressions may be harder to recognize, or in fact may not be recognized without context. Hence, the goal is of this paper is not to present a method which only spots pre-defined expressions, but to spot segments containing any possible type of facial expression that involves the strain (or deformation) of facial skin tissue.

Macro-expressions typically last 3/4th of a second to 2s (roughly 24–60frames) [13]. There are 6 universal expressions: happiness, sadness, fear, surprise, anger, and disgust. Spatially, macro-expressions can occur over multiple or single regions of the face, depending on the expressions. For instance, the surprise expressions generally cause motion around the eyes, forehead, cheeks, and mouth, whereas the expression for fear typically generates motion only near the eyes.

In general, a micro-expression is described as an involuntary pattern of the human body that is significant enough to be observable, but may not fully convey the triggering emotion [13,3]. Micro-expressions occurring on the face are rapid and are often missed during casual observation. Lasting from 1/25th to 1/3rd of a second (roughly 2–10frames) [12], micro-expressions can be classified, based on how an expression is modified, into three types [13]:
                              
                                 •
                                 Type 1. Simulated Expressions: When a micro-expressions is not accompanied by a genuine expression.

Type 2. Neutralized expressions: When a genuine expression is suppressed and the face remains neutral.

Type 3. Masked Expressions: When a genuine expression is completely masked by a falsified expression.

Type 2 micro-expressions are not observable and type 3 micro-expressions may be completely eclipsed by a falsified expression. In this paper, we focus on type 1 micro-expressions, i.e., micro-expressions that correspond to rapid, but observable and non-suppressed motion on the face.

There are two main approaches for calculating optical strain [2]: (i) integrate the strain definition into the optical flow equations, or (ii) derive strain directly from the flow vectors. The first approach requires the calculation of high order derivatives, hence is sensitive to image noise. The second approach allows us to post-process the flow vectors before calculating strain, possibly reducing the effects any errors incurred during the optical flow estimation.

Optical flow is a well-known motion estimation technique that is based on the brightness conservation principle [27]. In general, it assumes (i) constant intensity at each point over a pair (sequence) of frames, and (ii) smooth pixel displacement within a small image region. It is typically represented by the following equation:
                           
                              (1)
                              
                                 
                                    
                                       
                                          
                                             ∇
                                             I
                                          
                                       
                                       T
                                    
                                    p
                                    +
                                    
                                       I
                                       t
                                    
                                    =
                                    0
                                 
                              
                           
                        where I(x,y,t) represents the temporal image intensity function at point x and y at time t, and ∇I represents the spatial and temporal gradient. The horizontal and vertical motion vectors are represented by p
                        =[p
                        =
                        dx/dt,q
                        =
                        dy/dt]
                           T
                        .

Since large intervals over a single expression can often cause failure in tracking (due to the smoothness constraint), we implemented a vector linking (or stitching) process that combines small, local pairs of small intervals (1–3frames) into larger pairs to expand over the entire sequence of frames. This works by matching the optical flow from one pair of frames p(F
                        
                           n
                        ,F
                        
                           n
                           +1) to a consecutive pair of frames p(F
                        
                           n
                           +1,F
                        
                           n
                           +2) by summing the (p,q) components from each to generate the larger displacement p(F
                        
                           n
                        ,F
                        
                           n
                           +2) (see Fig. 3
                        ).

The projected 2-D displacement of any deformable object can be expressed by a vector u
                        =[u,v]
                           T
                        . If the motion is small enough, then the corresponding finite strain tensor is defined as [2,4]:
                           
                              (2)
                              
                                 
                                    ε
                                    =
                                    
                                       1
                                       2
                                    
                                    
                                       
                                          ∇
                                          u
                                          +
                                          
                                             
                                                
                                                   ∇
                                                   u
                                                
                                             
                                             T
                                          
                                       
                                    
                                    ,
                                 
                              
                           
                        which can be expanded to the form:
                           
                              (3)
                              
                                 
                                    ε
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   ε
                                                   xx
                                                
                                                =
                                                
                                                   
                                                      ∂
                                                      u
                                                   
                                                   
                                                      ∂
                                                      x
                                                   
                                                
                                             
                                             
                                                
                                                   ε
                                                   xy
                                                
                                                =
                                                
                                                   1
                                                   2
                                                
                                                
                                                   
                                                      
                                                         
                                                            ∂
                                                            u
                                                         
                                                         
                                                            ∂
                                                            y
                                                         
                                                      
                                                      +
                                                      
                                                         
                                                            ∂
                                                            v
                                                         
                                                         
                                                            ∂
                                                            x
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             
                                                
                                                   ε
                                                   yx
                                                
                                                =
                                                
                                                   1
                                                   2
                                                
                                                
                                                   
                                                      
                                                         
                                                            ∂
                                                            v
                                                         
                                                         
                                                            ∂
                                                            x
                                                         
                                                      
                                                      +
                                                      
                                                         
                                                            ∂
                                                            u
                                                         
                                                         
                                                            ∂
                                                            y
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   ε
                                                   yy
                                                
                                                =
                                                
                                                   
                                                      ∂
                                                      v
                                                   
                                                   
                                                      ∂
                                                      y
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where (ε
                        
                           xx
                        ,ε
                        
                           yy
                        ) are normal strain components and (ε
                        
                           xy
                        ,ε
                        
                           yx
                        ) are shear strain components.

Since each of these strain components are a function of displacement vectors (u,v) over a continuous space, each strain component is approximated using the discrete optical flow data (p,q):
                           
                              (4)
                              
                                 
                                    p
                                    =
                                    
                                       δx
                                       δt
                                    
                                    =
                                    
                                       
                                          Δ
                                          x
                                       
                                       
                                          Δ
                                          t
                                       
                                    
                                    =
                                    
                                       u
                                       
                                          Δ
                                          t
                                       
                                    
                                    ,
                                    u
                                    =
                                    p
                                    Δ
                                    t
                                    ,
                                 
                              
                           
                        
                        
                           
                              (5)
                              
                                 
                                    q
                                    =
                                    
                                       δy
                                       δt
                                    
                                    =
                                    
                                       
                                          Δ
                                          y
                                       
                                       
                                          Δ
                                          t
                                       
                                    
                                    =
                                    
                                       v
                                       
                                          Δ
                                          t
                                       
                                    
                                    ,
                                    v
                                    =
                                    q
                                    Δ
                                    t
                                 
                              
                           
                        where Δt is the change in time between two image frames. Setting Δt to a fixed interval length (Δt
                        =1 in our experiments in order to maximize sensitivity to micro-expressions which can last as little as 2–3 frames), we can estimate the partial derivatives of Eqs. (4) and (5):
                           
                              (6)
                              
                                 
                                    
                                       
                                          ∂
                                          u
                                       
                                       
                                          ∂
                                          x
                                       
                                    
                                    =
                                    
                                       
                                          ∂
                                          p
                                       
                                       
                                          ∂
                                          x
                                       
                                    
                                    Δ
                                    t
                                    ,
                                    
                                       
                                          ∂
                                          u
                                       
                                       
                                          ∂
                                          y
                                       
                                    
                                    =
                                    
                                       
                                          ∂
                                          p
                                       
                                       
                                          ∂
                                          y
                                       
                                    
                                    Δ
                                    t
                                    ,
                                 
                              
                           
                        
                        
                           
                              (7)
                              
                                 
                                    
                                       
                                          ∂
                                          v
                                       
                                       
                                          ∂
                                          x
                                       
                                    
                                    =
                                    
                                       
                                          ∂
                                          q
                                       
                                       
                                          ∂
                                          x
                                       
                                    
                                    Δ
                                    t
                                    ,
                                    
                                       
                                          ∂
                                          v
                                       
                                       
                                          ∂
                                          y
                                       
                                    
                                    =
                                    
                                       
                                          ∂
                                          q
                                       
                                       
                                          ∂
                                          y
                                       
                                    
                                    Δ
                                    t
                                    .
                                 
                              
                           
                        
                     

The second order derivatives are calculated using the central difference method. Hence,
                           
                              (8)
                              
                                 
                                    
                                       
                                          ∂
                                          u
                                       
                                       
                                          ∂
                                          x
                                       
                                    
                                    =
                                    
                                       
                                          u
                                          
                                             
                                                x
                                                +
                                                Δ
                                                x
                                             
                                          
                                          −
                                          u
                                          
                                             
                                                x
                                                −
                                                Δ
                                                x
                                             
                                          
                                       
                                       
                                          2
                                          Δ
                                          x
                                       
                                    
                                    =
                                    
                                       
                                          p
                                          
                                             
                                                x
                                                +
                                                Δ
                                                x
                                             
                                          
                                          −
                                          p
                                          
                                             
                                                x
                                                −
                                                Δ
                                                x
                                             
                                          
                                       
                                       
                                          2
                                          Δ
                                          x
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (9)
                              
                                 
                                    
                                       
                                          ∂
                                          v
                                       
                                       
                                          ∂
                                          y
                                       
                                    
                                    =
                                    
                                       
                                          v
                                          
                                             
                                                y
                                                +
                                                Δ
                                                y
                                             
                                          
                                          −
                                          v
                                          
                                             
                                                y
                                                −
                                                Δ
                                                y
                                             
                                          
                                       
                                       
                                          2
                                          Δ
                                          y
                                       
                                    
                                    =
                                    
                                       
                                          q
                                          
                                             
                                                y
                                                +
                                                Δ
                                                y
                                             
                                          
                                          −
                                          q
                                          
                                             
                                                y
                                                −
                                                Δ
                                                y
                                             
                                          
                                       
                                       
                                          2
                                          Δ
                                          y
                                       
                                    
                                 
                              
                           
                        where (Δx,Δy) is 1pixel.

Finally, each of these values corresponding to low and large elastic moduli are summed to generate the strain magnitude. Each value can also be normalized to 0–255 for a visual representation (strain map). Fig. 4
                         shows the visualization of the strain values (strain pattern) obtained during both a macro- and micro-expression.

The algorithm for spotting both macro- and micro-expressions can be seen in Fig. 5
                     . It consists of several steps, each of which will now be described.

Facial tracking was performed using the subspace constrained mean shift algorithm [28], which tracks several points on the face over a video sequence. These points are then used for two purposes: i) we are able to align faces in the video sequence by transforming each image to match the original location of the face, hence reducing the amount of motion between consecutive frames (such as a person rigidly moving his/her head back and forth); ii) we can then use these points for segmenting and masking the face (in the masking step). See Fig. 6(a) for an example of a face with the tracked points.

Optical flow calculation can be computationally expensive, so it is beneficial to reduce the size image before estimating motion. To do this, we crop the face from the image using the 66 points tracked on the face. Since all future images are aligned to this coordinate system, all future face images will have the same dimensions. Optical flow is estimated using the MATLAB implementation of the Horn–Schunck method. Next, optical strain is then estimated using the central difference method by convolving a 23×3 Sobel kernels (S
                        
                           x
                        ,S
                        
                           y
                        ) over each of the (p,q) flow fields to generate each of the four strain components of the strain tensor 
                           
                              ε
                              =
                              
                                 
                                    1
                                    2
                                 
                              
                              
                                 
                                    ∇
                                    p
                                    +
                                    
                                       
                                          
                                             ∇
                                             p
                                          
                                       
                                       T
                                    
                                 
                              
                           
                         (see Section 3.2).

By localizing several features of the face in the tracking stage, we are able to segment the reliable strain values on the face from those that are noise. Noisy values are due to inaccurate flow computations caused by the violation of the smoothness constraints and self-occlusions. Occlusions can be found at the boundary of the face, where the background is lost due to the rigid head motion. The mouth is problematic because opening/closing the mouth are rigid motions that contain self occlusions thus causing tracking failures. The eyes are also masked because they do not contain non-rigid motion and blinking can cause noisy motion estimations.

The strain magnitude S
                        
                           R
                         is calculated by summing all values generated separately for each region of the face. Each region then generates a sequence corresponding to the amount of strain observed over time. These values are processed using two passes, the first of which spots macro-expressions and the second micro-expressions.

Before a peak detector is used to find the points of maximum strain, the values are first pre-processed using the following steps:
                           
                              •
                              Fit a 2nd degree polynomial to the sequence of total strain magnitude using the least squares method.

At each point, subtract this curve from the sequence.

Perform Gaussian smoothing on the entire sequence.

Normalize sequence between [0,1] using min–max normalization.

The first step minimizes error in optical flow that is accumulated when stitching the flow values over the entire video sequence. By fitting a polynomial curve to the sequence and then subtracted from it, we effectively get the approximate mean values at each frame over time (see blue line in Fig. 7
                        ). This step could also use an adaptive mean filter at each frame if the user wanted to run this on live video streams with an unknown duration. Next, to remove false positives due to noisy spikes in the strain calculation, all values are smoothed using a Gaussian filter. Lastly, min–max normalization allows us to define the search space for the parameters needed when detecting the peaks which correspond to macro-expressions. Part b in Fig. 7 shows the final normalized strain signal.

Since macro-expressions can occur over multiple regions of the face simultaneously, all strain values contained in all regions are summed to generate an overall strain magnitude (i.e., the mask in Fig. 6(b) is used).

Next, peaks corresponding to macro-expressions are found. Refer to Fig. 9
                        
                         for an illustration. The peak detection [5] uses a parameter α which is a threshold on the minimal strain magnitude allowed to be a peak, and β which determines the amount that the peak must be above the surrounding areas. First, for each value S
                        
                           i
                        
                        ∈
                        S (where S is the set of all strain magnitudes for all frames), its forward derivative is taken, or
                           
                              (10)
                              
                                 
                                    
                                       S
                                       ′
                                    
                                    =
                                    
                                       
                                          
                                             S
                                             i
                                          
                                          −
                                          
                                             S
                                             
                                                i
                                                +
                                                1
                                             
                                          
                                       
                                    
                                    ,
                                    i
                                    =
                                    1
                                    …
                                    |
                                    S
                                    |
                                    −
                                    1
                                 
                              
                           
                        Next, all extrema (potential peaks and valleys) are indexed in E where sign changes occur, or
                           
                              (11)
                              
                                 
                                    k
                                    ∈
                                    E
                                    
                                    if
                                    
                                    
                                       S
                                       k
                                       ′
                                    
                                    ×
                                    
                                       S
                                       
                                          k
                                          +
                                          1
                                       
                                       ′
                                    
                                    <
                                    0
                                    ,
                                    ∀
                                    
                                       S
                                       k
                                       ′
                                    
                                    ∈
                                    
                                       S
                                       ′
                                    
                                    .
                                 
                              
                           
                        Then, ∀j
                        ∈
                        E, if
                           
                              (12)
                              
                                 
                                    
                                       S
                                       j
                                    
                                    −
                                    
                                       S
                                       
                                          j
                                          −
                                          1
                                       
                                    
                                    >
                                    β
                                    
                                    and
                                    
                                    
                                       S
                                       j
                                    
                                    >
                                    
                                       S
                                       
                                          j
                                          +
                                          1
                                       
                                    
                                 
                              
                           
                        or
                           
                              (13)
                              
                                 
                                    
                                       S
                                       j
                                    
                                    >
                                    
                                       S
                                       
                                          j
                                          −
                                          1
                                       
                                    
                                    
                                    and
                                    
                                    
                                       S
                                       j
                                    
                                    −
                                    
                                       S
                                       
                                          j
                                          +
                                          1
                                       
                                    
                                    >
                                    β
                                 
                              
                           
                        and
                           
                              (14)
                              
                                 
                                    
                                       S
                                       j
                                    
                                    >
                                    α
                                 
                              
                           
                        then S
                        
                           j
                         is a peak corresponding to a macro-expression.

Lastly, S
                        
                           j
                           +1 marks the potential end of the expression while the potential beginning of the expression is given by S
                        
                           j
                           −1. The final boundary is determined when the strain magnitude matches at both the beginning and end of the expressions, for the larger of the two strain magnitudes. Hence, given E
                        
                           max
                        
                        =max(S
                        
                           j
                           −1,S
                        
                           j
                           +1), then the boundary is determined at the points on each side of the peak that are approximately equal:
                           
                              (15)
                              
                                 
                                    
                                       S
                                       l
                                    
                                    ≈
                                    
                                       S
                                       r
                                    
                                    ≈
                                    T
                                    ×
                                    
                                       
                                          
                                             S
                                             j
                                          
                                          −
                                          max
                                          
                                             
                                                S
                                                
                                                   j
                                                   −
                                                   1
                                                
                                             
                                             
                                                S
                                                
                                                   j
                                                   +
                                                   1
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where T
                        =.1 (i.e., at 10% of the peak height) and (S
                        
                           l
                        ,S
                        
                           r
                        ) are the nearest points found in S, with the corresponding frame numbers (l,r) used to denote the expression beginning and end. Note that if the peak is located near the boundary, this value will be the boundary itself if no intersection is found. The last check is to ensure the expression is long enough to be a macro-expression, i.e., the expression boundary must be greater than 1/3rd of a second, or roughly 10 frames in duration. See Fig. 8 for spotted macro-expressions on an example sequence.

After the intervals for macro-expressions are found, the subintervals are then searched for any micro-expressions. The thresholding technique is nearly identical to that of macro-expressions. First, there is a constraint on locality, hence micro-expression are restricted to occurring in at most two bordering regions (see Fig. 6) of the face (I–II, II–III, III–IV, I–IV). Second, micro-expressions are very rapid (lasting as few as 3 frames) and often occur in just one region of the face. Hence, to ensure that the width of the peak is consistent (and not noisy spike in flow error), we use a threshold value of T
                        =.5 (Eq. (14)), or half the peak height. See Fig. 8 for spotted micro-expressions on an example sequence.

For segmenting very long videos, it is necessary to restart the optical flow calculation. This is an important step primarily because error in the flow linking and estimation can accumulate over many frames. Based on visual inspection of the optical flow fields in our experiments, we found that after 30s of video the flow error is too noisy for accurate strain estimation. Because the optical flow needs to be calculated from the beginning of an expression, we restart the algorithm at the end of the last found expression. If there have been no expressions found in the last N frames, we restart the tracking at the current frame.

Experiments were performed on several datasets. The ground-truth labeling were hand-marked by an analyst that specified the beginning and ending frame of each expression. Before ground-truthing, the analyst was first shown several examples of macro- and micro-expressions. We now discuss each dataset, and then report the results of macro- and micro-expression spotting.

This is a collection of 10 videos that contain 75 feigned macro-expressions and 37 feigned micro-expressions interspersed within each video sequence. Each video was recorded using a Panasonic AG-HMC40 camcorder at a resolution of 720×1280, with an average video length of 1min.

This dataset [9] is publically available and includes 77 sequences of micro-expressions, with one micro-expression per sequence. The faces are cropped in these images at an approximate resolution of 140×175.

The EXP-spontaneous dataset was collected from a 66minute HD (1920×1080pixel) video clip from a Panasonic HD camera. The video contains two subjects playing a video game on a large monitor. Both subjects were present and frontward facing in the video for 97% of the recording time. The camera was positioned between the subjects and the television, aimed and centered between the subjects. The dataset contains 68 expressions by Subject 1 and 104 expressions by Subject 2. The average expression duration reported by Subject 1 is 127 frames which is approx. 4.2s. The average expression duration reported by Subject 2 is 112 frames, which is approx. 3.7s. The average time between expressions reported by Subject 1 is 1653 frames which is approx. 55s. The average time between expressions reported by Subject 2 is 1057 frames, or approx. 35s. Talking and head turned away from camera movements (don't care regions) account for .9% of the video were labeled in the ground-truth and were not considered as an expression.

This is a publicly available dataset that consists of 27 adult subjects (12 female and 15 male). The subjects were recorded with a stereo camera (the left camera is used in our experiments) while watching a four minute video that invoked expressions. Per-frame annotation for 14 action units are provided, each with an intensity measure (0–5). Each action unit corresponds to a type of motion on the face, for example, “inner brow raiser”, “outer brow raiser”, “lip corner puller”, etc. Since we do not detect distinct types of expressions, we used these annotations to generate a generalized set of expressions. A macro-expression is defined by the activation of one or more action units for more than 10frames. It is worth noting that in the annotation, a single action unit may be activated for hundreds, and sometimes thousands of frames (for instance, if the person is concentrating on the video, they may slightly furrow their eyebrows for a long period of time). Often, within the intervals of these long expressions, there are several sub-expressions that have higher intensities than the larger expression (this common occurrence is also reported in [12]). Therefore, in order to properly represent these sub-expressions in the ground-truth, we generate and report results on several expression ground-truth labels for each sequence based on thresholding the intensity measures at two points (T
                           =1, T
                           =3), i.e., in order to be considered an expression, the action unit must have an intensity value greater than or equal to T. Overall, when thresholding at these two values, we obtain two sets of ground-truth that contain 409 and 259 expressions, respectively. It is worth noting that generating a set of micro-expression is not as clear, and using a strict definition such as “exactly one action unit active for less than 10 frames” only generates a few examples in the dataset.

@&#RESULTS@&#

Results will now be given for each dataset. The performance is shown using an ROC comparing the true positive rate (TPR) and the false positive rate, by varying the peak magnitude threshold β. We also report the area under the curve (AUC) for each ROC plot.

The true positive rate was calculated by the number of successfully detected expressions out of the number of total expressions, or
                           
                              (16)
                              
                                 
                                    TPR
                                    =
                                    
                                       
                                          N
                                          
                                             detected
                                             −
                                             Exp
                                          
                                       
                                       
                                          N
                                          
                                             total
                                             −
                                             Exp
                                          
                                       
                                    
                                 
                              
                           
                        while the false positive rate,
                           
                              (17)
                              
                                 
                                    FPR
                                    =
                                    
                                       
                                          N
                                          detectedExpFrames
                                       
                                       
                                          
                                             N
                                             Frames
                                          
                                          −
                                          
                                             N
                                             totalExpFrames
                                          
                                       
                                    
                                 
                              
                           
                        where for each subject, N
                        
                           detectedExp
                         and N
                        
                           detectedExpFrames
                         are the number of correctly detected expressions and the summed total number of frames they contain and similarly, N
                        
                           totalExp
                         and N
                        
                           totalExpFrames
                         are the number ground-truth expressions and the frames they contain. Lastly, N
                        
                           Frames
                         is the total number of frames in the dataset. For estimating the F1 score in Section 6, we define a false negative as
                           
                              (18)
                              
                                 
                                    FN
                                    =
                                    
                                       N
                                       totalExp
                                    
                                    −
                                    
                                       N
                                       detectedExp
                                    
                                    .
                                 
                              
                           
                        
                     

We now report the results of finding macro-expressions in the mixed dataset USF-combination in Fig. 10
                        . The ROC generated by varying β is promising, achieving 81% TPR with less than a .1% false positive rate, and at its peak successfully spots 94% of the macro-expressions with less than a .4% FPR. Alternatively, at these two datapoints, this translates 58 macro-expressions being spotted successfully at the expense of 4 false positives, and 68 macro-expressions being successfully spotted with 14 false positives. False positives can be mainly attributed to the subject moving his/her face too rapidly, resulting in failure in optical flow and hence strain estimation. On this same dataset, the ROC of micro-expression spotting is given in Fig. 11
                        . While micro-expression spotting was less precise than macro-expression spotting, most of the micro-expressions were found. For example, the algorithm is able to successfully spot 18 (roughly 50%) of micro-expressions with 4 false positives, and at most 29 (78%) micro-expressions at a cost of 21 false positives. We also report the precision of locating the start of the expression, as well as the expression length, in Tables 2 and 3
                        
                        
                         (See Table 1 for the statistics of the corresponding ground-truth intervals).

We also report the results on 77 sequences of micro-expressions from the SMIC-micro dataset in Fig. 12
                        . It is worth noting that due to the lower resolution of the faces, we used the facial points included with this dataset to generate the mask rather than using mean-shift tracking. Overall, the algorithm performed well on this dataset, with 56 (76%) of the micro-expressions detected with 10 false positives (.5% FPR), and at its peak we successfully detect 64 (83%) of the expressions, at the expense of 19 (.9%) false positives. For direct comparison to the results reported by [9] and [18], we used all 77 samples containing an expression along with another 77 other samples that did not. However, we test on all examples simultaneously since we do not rely on training. In total, 154 sequences were classified as either containing an expression or not. The best results reported by [9] was using a Multiple Kernel Learning (MKL) at a detection rate of 71.4%. The best results for [18] on this same dataset is 77.59%. We would like to stress that neither of the authors provide a FPR at this rate. At a FPR of 10%, we achieve a detection rate of 72% and reach a maximum TPR of 84% (see Fig. 13
                        ).

To observe the effect of resolution on the performance of our algorithm, we tested the USF-combination dataset at the following scales 100%, 50%, and 25%. On average this directly corresponds to facial dimensions of roughly 300×310, 150×160, and 77×80. The results of this experiment are given in Fig. 14
                        . A few things can be concluded from this experiment: (i) the algorithm is robust to re-scaled face dimension of 150×60 pixels on our dataset with no significant difference in accuracy, when detecting macro-expressions and micro-expressions. In fact at some points in the ROC curve, using a lower resolution increased accuracy at the same FPR (mainly because of smoother/less noisy flow fields); (ii) resolution is more important when detecting micro-expressions, since an immediate overall reduction in TPR can be observed at the 25% scale (77×80 pixels).

We now report results on the USF Spontaneous Expression dataset (see Fig. 15
                        ). It is worth noting that although the FPR is significantly higher than the feigned expression database, spotting spontaneous facial expressions is a much more difficult problem. The large number of false positives can be primarily attributed to the large variations and rapid head motions from each subject. We also observed that the intensity of the expressions were much lower than when they are feigned. We also tracked the mean and standard deviation of percent error for finding the start and length of all expressions (see Tables 4 and 5
                        
                        ). In general, the results are promising. Approximately 65% and 70% of the expressions are found on average for subject 1 and 2, respectively. Moreover, the start of the expression is on typically found within 9frames.

Finally, we give results for the DISFA dataset that consists of spontaneous macro-expression (see Fig. 16
                        ). Overall, the performance is positive, however, in the case of T=1 (see Section 1 for an explanation of T) there is a significant loss in performance for expression localization (see Tables 6 and 7
                        
                        ). This is mainly due to the nature of the expressions found when thresholding the action unit intensities at this point, which results in very long expression lengths that often last up to 1000frames. Since these longer expressions often contain many sub-expressions, their detections are considered successful, however, they are penalized with the bounds of the greater expression interval leading to larger localization error. By contrast, when setting T=3 and only detecting expressions with larger intensities, we achieve much better performance with respect to localization that is more consistent with the results achieved on the USF-Spontaneous dataset.

To summarize the performance of the method we provide the AUC for all experiments in Table 8
                     . Overall, the results are positive, and demonstrate that optical strain is a viable feature for capturing both rapid micro-expressions and the larger macro-expressions. The algorithms robustness to resolution is reflected in the USF-combination experiments where scaled-down facial resolutions performed equally as well or only slighter worse than full HD resolution for macro-expression spotting. Results on the SMIC dataset are promising. A decrease in overall performance can be seen for spontaneous expressions, although on average for both subjects, the AUC remains above 80%.

While we have tied performance to a choice of a single threshold β, a potential concern with the algorithm is making a good selection of this parameter. In general, a higher choice of β decreases the sensitivity of the approach, requiring large amounts of motion/deformation to be present before an expression is detected. In contrast, a lower choice of β increases the sensitivity.

In Table 9
                     , we provide the F1 scores at several selections of β. The F1 score is a measure of accuracy that uses the weighted average of both precision and recall. A few conclusions may be drawn from this table. First, on the USF-combination dataset, an optimal choice of β is less than or equal to .3 for both macro- and micro-expression spotting. For macro-expressions in particular, a peak size of .1 tends to be optimal, and deviating to nearby values can cause as much as a 9–14% decrease in accuracy. For micro-expressions on this dataset, it does not appear to be as constrained, where it deviates only 1–3% in this same range. However, for the high-speed spontaneous SMIC dataset, it is critical to choose the highest sensitivity, or lowest β=.01. For the USF-Spontaneous dataset, a good choice of β is greater than or equal to .5. This implicates that the amount of motion on the face in general is greater. We have also observed in this dataset that subjects often move their head during expressions, or move their head in general, leading to an increased optical strain magnitude signal. The same is true for the DISFA dataset. Moreover, many subjects in this dataset have very long expressions with low action unit intensities. In fact, 12 out of 27 subjects had expressions lasting longer than 600 frames, with each expression containing multiple sub-expressions. This led to large localization error when thresholding the action unit intensities at T=1. When thresholding at T=3, the smaller expressions were localized with greater precision.

@&#CONCLUSIONS@&#

In this paper, we proposed a method for the automatic spotting of facial expressions in videos comprised of numerous facial expressions without the need to train a model. The method relies on the increases and decreases in the magnitude of the strain observed on the facial skin as a subject performs an expression. This approach is able to successfully detect and distinguish between regular, universal macro-expressions and rapid micro-expressions. Results are positive on several datasets, including an hour long video that contains spontaneous expressions. The method has also been shown to work at several different resolutions, with face sizes of approximately 300×310, 150×160, and 77×80. A few points can be concluded from this work. First, to the knowledge of the authors, this is the first work that addresses the problem of automatically segmenting videos into sequences that contain a single macro- or micro-expression. Moreover, we are not aware of any method that can segment an unknown number of expression types. Second, we find that detecting spontaneous expressions is a much more difficult problem than feigned expression spotting, mainly because of two factors occurring simultaneously: (1) spontaneous expressions are often obtained while attempting to minimize the subjects knowledge or fixation on the camera. Hence large and semi-random rigid head motion is often present, which is typically missing in the ideal laboratory controlled settings, where often a subject's pose is meant to be stationary; (2) based on our observation, the expressions themselves are much more subdued when spontaneous. This observation is also supported in [29] where the spontaneous smile expression was of less amplitude than its feigned counterpart. Future work includes addressing the subjectivity of ground-truth labeling in the spontaneous dataset. We will also be providing USF-Spontaneous, along with the full annotation, to the community.

@&#REFERENCES@&#

