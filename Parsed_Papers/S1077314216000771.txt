@&#MAIN-TITLE@&#Human running detection: Benchmark and baseline

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           To construct one unified benchmark dataset with various scenes and challenges.


                        
                        
                           
                           To standardize uniform evaluation criteria for comparing different kinds of algorithms.


                        
                        
                           
                           To carefully design a baseline method for human running detection.


                        
                        
                           
                           To open up a new research direction for the specific human motion.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Running detection

Benchmark database

Baseline algorithm

@&#ABSTRACT@&#


               
               
                  Detection of running behavior, the specific anomaly from common walking, has been playing a critical rule in practical surveillance systems. However, only a few works focus on this particular field and the lack of a consistent benchmark with reasonable size limits the persuasive evaluation and comparison. In this paper, for the first time, we propose a standard benchmark database with diversity of scenes and groundtruth for human running detection, and introduce several criteria for performance evaluation in the meanwhile. In addition, a baseline running detection algorithm is presented and extensively evaluated on the proposed benchmark qualitatively and quantitatively. The main purpose of this paper is to lay the foundation for further research in the human running detection domain, by making experimental evaluation more standardized and easily accessible. All the benchmark videos with groundtruth and source codes will be made publicly available online.
               
            

@&#INTRODUCTION@&#

Human motion and behavior play an important role in the human visual system and video surveillance, which has drawn many researchers’ attention (e.g., [17–19]). Based on the motion information processing techniques, one can analyze and recognize different objects and even their behaviors for further high-level vision task. Thus, motion analysis has got more research attention and funding in recent years, in order to satisfy the increased global security concerns and the ever demanding requirements for effective monitoring of public places.

Over the last few decades, a considerable amount of literature has been dedicated to anomaly detection and abnormal behavior recognition, such as [1,7–11,13] and so on. However, few works focus on the specific domain of human running detection, although this topic is of extreme interest in surveillance applications. In motion perception field, human running behavior is a simple yet attractive activity due to its abnormality from customary and common walking event. That is, in public places such as campus, street, shopping mall, etc., most people usually walk around slowly, which can be regarded as normal activity. Then running can be identified as abnormal event, which means that unsafe event may happen in the crowd. Especially, abnormal running behaviors frequently happen in robbery cases and other criminal situations in airports and stations. In addition, in many sports games, the running detection technique is able to facilitate analyzing and evaluating the performance of players after the training or match. In consequence, it is of great interest to develop an effective and efficient system detecting human running behavior for a lot of realistic applications (such as video surveillance, security control, human–computer interaction, etc.). The running detection system can not only detect unusual activities and inform the illegal events in time, but also save a lot of storage space and avoid finding and collecting massively evidence and investigation after the events have happened.

To the best of our knowledge, it is a very challenging problem to automatically detect highly semantic events from cameras in a public place, due to the high complexity of the original video data, such as isolated and in group, sparse and crowded, horizontal and vertical motion, etc. It will get even worse if the surveillance system is equipped with low-resolution analog cameras. In addition, the storage and computational complexity should be low enough to ensure the realization of real-time performance. Therefore, human running detection is a challenging yet promising task in the vision field which should attract more research attention.

Since human running is a specific behavior, both rule-based and statistical-based methods can be exploited to design a running detection algorithm. Rule-based systems adopt predefined rules (such as velocity, amplitude, gait) to determine normal or abnormal activities, which exploit the prior knowledge and perform well in specific types. While statistical-based probabilistic methods build the activity models in a data-driven fashion, which learns the particular patterns from the statistical properties of the observed data. In the existing literature, there are merely a small number of works [1,3,16,20], focusing on the study of human running detection. In [3], Cheng et al. propose a method to detect and describe periodic motions like running and walking. Then running behavior is classified by checking the energy distribution of the periodic signal in its power spectrum. Zhu et al. [20] detect the abnormal running on the basis of spatio-temporal parameters, with the assistance of the multi-target tracking mechanism. Foucher et al. [5] propose a non-parametric approach to detect person running, where statistics about the tracked object’s velocities are accumulated over a long period using a Gaussian model, and the running events are detected by assessing the divergence of the current velocity from the learned model.

However, these algorithms have not brought the breakthrough in dealing with the running detection problem, due to the following three reasons. First, these approaches are evaluated on a small set of videos collected and annotated in different dataset or by their own (usually less than 5 video clips). Thus, a unified collection and annotation protocol is difficult to guarantee. In addition, the small size of the dataset induces significant bias, which limits the development of the human running detection field. Moreover, the lack of consistent evaluation criteria and unavailable implementations prevent persuasive comparisons.

This paper aims to address these questions by, for the first time, proposing a benchmark dataset and several evaluation criteria for human running detection, and introducing a simple yet effective algorithm as the baseline method for fair comparisons. Although some existing action recognition databases (such as KTH [14] and UCF [12] are of running classes, their purposes are to classify different human sports actions (e.g., walking, hand waving, running, jogging, hand clapping, diving, kicking) in some constrained conditions with small numbers of humans (usually one person) in each scenario. To establish a standard benchmark, we construct a surveillance camera dataset with 20 videos, which includes diversity of scenes like campus, mall, street and square (see Fig. 1
                     ), and challenges such as sparse and crowded, horizontal and vertical, close shots and distant views. To build a baseline algorithm, we design an effective method incorporating optical flow to determine the amplitude of movement and then distinguish the running behavior from walking. By using the proposed benchmark dataset and evaluation criteria, we evaluate the baseline method extensively and report the overall performance.

Overall, the goals and contributions are mainly four folds:

                        
                           1.
                           To construct one unified benchmark dataset with various scenes and challenges;

To standardize uniform evaluation criteria for comparing different kinds of algorithms;

To carefully design a baseline method for human running detection;

To open up a new research direction for the specific human motion, facilitating the future studies of researchers interested in this field.

All the benchmark videos with groundtruth and source codes will be made available to the public.

As a standard benchmark dataset, it is very important to be of high diversity and low bias. This dataset is acquired with a stationary camera that is mounted three to ten meters above the ground, overlooking pedestrian walkways. In this dataset, there exist totally 20 video clips collected from different scenarios. The video footage recorded for each clip lasts two minutes, the frame rate is 25 frame per second (fps), and the resolution is 960 × 540. Thus, the total frames of each clips are 3, 000 and of this dataset are 60, 000. There exist totally 152 running events in our dataset, where the numbers of running events in each clip can be found in the experiment section (Table 1
                     ). Fig. 2
                      summarizes the statistics of our dataset, which presents varieties in the following aspects:


                     Scene type: We collect our surveillance videos in various scenes, such as square, parking lot, sidewalk, shopping mall, street, school gate and so on. These places appear very common in our daily life, and the running behaviors are more likely to take place and are usually acted as abnormal events in these scenes. The square and shopping mall have a relative simple and clear background; while the contexts of street and parking lot are often cluttered with distractors like bicycles and cars.


                     Motion direction: There exist different motion directions for people in the recorded video clips. In general, we divide the main direction of movement into three types: vertical (people walk towards and away from the camera), horizontal (movement of pedestrian is parallel to the camera plane) and arbitrary (people wander around freely). However, the main direction only indicates the movement of most people, and there are still persons walking in opposite or perpendicular direction. Also in squares and malls, people can walk and run from every direction, which further increases the difficulty of running detection.


                     Human density: In our dataset, the crowd density within the frame is variable, ranging from sparse to very crowded. Take the school gate for an instance, there are few students at ordinary times, while when the class is beginning, the human density is very high, with some students rushing to the classroom.


                     Shooting angle and distance: To increase the scene diversity, we mount our camera in different heights with diverse focal distances. In some videos the captured image is limited to a small area, while in other ones, the focus is extended to views quite far away. In addition, we observe the persons from various perspectives and angles. Different angle will induce different concerned human part for the algorithm, for example, high camera leads to the focus on head and shoulders, while low angle to the whole human body. Thus, varied shooting angles and distances further pose great challenges for human running detection.


                     Annotation: In order to conduct fair evaluation, we firstly manually annotate the groundtruth with a binary flag per frame to indicate whether a running behavior is present in that frame (Although moving people are usually of various speeds, it is quite easy for human to identify the running event and provide an accurate label). This is used to evaluate the performance with respect to the ability to identify the abnormal frame (with running people). Besides, we define the whole running process of one person from beginning to end as an event, and provide the starting and finishing frame indices and the locations (denoted by bounding box) of this person in this period of time. This is intended to evaluate the performance with respect to the ability to localize running persons. We note that the videos and groundtruth annotations are made available to the public online.

The task of running detection evaluation is to determine whether a running behavior is present or not in each frame, and to examine how many running persons can be successfully detected in a video clip. To achieve this purpose, we introduce two criteria similar with anomaly detection [1,8] to address the above-mentioned issues.


                     Precision (frame level): If a frame contains at least one output running behavior, it is considered as a detection and labeled as 1. These detections are compared to the frame level groundtruth annotation of each frame. In this work, we define Precision as the ratio of the number of true detections to that of total detections, i.e.,

                        
                           (1)
                           
                              
                                 P
                                 r
                                 e
                                 c
                                 i
                                 s
                                 i
                                 o
                                 n
                                 =
                                 
                                    
                                       
                                          ∑
                                          i
                                       
                                       L
                                       a
                                       b
                                       e
                                       
                                          l
                                          O
                                          i
                                       
                                       
                                       &
                                       
                                       L
                                       a
                                       b
                                       e
                                       
                                          l
                                          G
                                          i
                                       
                                    
                                    
                                       
                                          ∑
                                          i
                                       
                                       L
                                       a
                                       b
                                       e
                                       
                                          l
                                          O
                                          i
                                       
                                    
                                 
                                 ×
                                 100
                                 %
                                 ,
                              
                           
                        
                     where 
                        
                           L
                           a
                           b
                           e
                           
                              l
                              O
                              i
                           
                        
                      and 
                        
                           L
                           a
                           b
                           e
                           
                              l
                              G
                              i
                           
                        
                      denote the label of output and the groundtruth respectively in ith frame and 
                        &
                      is the AND operation. We note that this evaluation does not ensure the detection coincides with the actual location of the running person. It is therefore possible for some true positives to be lucky co-occurrences of erroneous detections.


                     Recall (event level): To test whether the output region contains the running person, detections are compared to the groundtruth bounding box in that frame. If at least 40% of the truly running person is detected, the frame is considered to be detected correctly [8]. Then if at least one frame is correctly detected during the running process of one person, we deem that it is a successful event. The metric Recall is defined as the ratio of the actual number of successful events to the number of events that occur, i.e.,

                        
                           (2)
                           
                              
                                 R
                                 e
                                 c
                                 a
                                 l
                                 l
                                 =
                                 
                                    
                                       #
                                       
                                       s
                                       u
                                       c
                                       c
                                       e
                                       s
                                       s
                                       f
                                       u
                                       l
                                       
                                       e
                                       v
                                       e
                                       n
                                       t
                                    
                                    
                                       #
                                       
                                       t
                                       o
                                       t
                                       a
                                       l
                                       
                                       e
                                       v
                                       e
                                       n
                                       t
                                    
                                 
                                 ×
                                 100
                                 %
                                 .
                              
                           
                        
                     This criterion assesses the performance on detection rate in the individual level.

To facilitate a fair comparison and evaluation, we propose a baseline method based on the optical flow method for human running detection. The flowchart of the proposed algorithm is illustrated in Fig. 3
                      for a holistic view, which mainly involves three fundamental steps: human upper-body detection, optical flow computation and running detection. We note in our implementation we merely detect the human upper-body rather than the whole body, the underlying reason of which is that the walking and running behaviors can be better distinguished by the movement of shoulders and arms rather than legs which produce large optical flow in both behaviors. In addition, the upper-body is relative stable in the cluttered scenarios since the lower-body is of high chance to be occluded by obstacles or moving objects.

Since the subject of running behaviors is human, the detection of human upper-body is able to rapidly locate the regions where running may occur, thus avoiding the processing of the whole image. The detection mechanism shares the same idea with the classical “Viola–Jones” face detector [15]. The detection procedure is based on raw image gray values and utilizes three kinds of rectangle features. With the help of integral image, rectangular features can be evaluated in constant time, which gives a considerable speed advantage over other more sophisticated relatives [15]. Moreover, the features all rely on large amount of rectangular areas, therefore providing a rich image representation which supports effective learning.

Then given a training set of positive and negative images, a variant of Adaboost is used to both select a small set of features and train the classifier [6]. For feature selection, one can draw an analogy between weak classifiers and features and regard Adaboost as an effective method to weight different features. In support of this goal, the weak learning algorithm is designed to select the single rectangle feature which gives the best classification. For each feature, the weak learner determines the optimal threshold such that the minimum number of examples are misclassified. Finally, classifiers are combined successively in a cascade structure which dramatically increases the speed of detection by focusing on promising regions. Stages in the cascade framework are constructed by training classifiers using Adaboost and then adjusting the threshold to minimize false negatives, in which subsequent classifiers are trained using those examples which pass through all previous stages. For detailed procedure, please refer to [15]. To be specific, we adopt the implementation in OpenCV [2] to detect the human upper-body in each frame, and all the detected regions are re-scaled to share the same size. The sampled detection results are shown in Fig. 4
                        .

In this paper, we adopt the two-frame motion estimation method [4] based on polynomial expansion to compute the dense optical flow of each point in the given region. This method uses quadratic polynomials to estimate translations of a local neighborhood and determines motion vectors from polynomial expansion coefficients, which achieves good compromise between accuracy and speed. Specifically, given the neighborhood x of one pixel, the quadratic polynomial is formulated as

                           
                              (3)
                              
                                 
                                    f
                                    
                                       (
                                       x
                                       )
                                    
                                    ∼
                                    
                                       x
                                       ⊤
                                    
                                    A
                                    x
                                    +
                                    
                                       b
                                       ⊤
                                    
                                    x
                                    +
                                    c
                                    .
                                 
                              
                           
                        For patch f
                        1 (
                           
                              
                                 f
                                 1
                              
                              
                                 (
                                 x
                                 )
                              
                              ∼
                              
                                 x
                                 ⊤
                              
                              
                                 A
                                 1
                              
                              x
                              +
                              
                                 
                                    
                                       b
                                       1
                                    
                                 
                                 ⊤
                              
                              x
                              +
                              
                                 c
                                 1
                              
                              .
                           
                        ) and a new patch f
                        2 (
                           
                              
                                 f
                                 2
                              
                              
                                 (
                                 x
                                 )
                              
                              ∼
                              
                                 x
                                 ⊤
                              
                              
                                 A
                                 2
                              
                              x
                              +
                              
                                 
                                    
                                       b
                                       2
                                    
                                 
                                 ⊤
                              
                              x
                              +
                              
                                 c
                                 2
                              
                              .
                           
                        ) that is obtained by a global displacement d, equating the coefficients yields

                           
                              (4)
                              
                                 
                                    d
                                    =
                                    −
                                    
                                       1
                                       2
                                    
                                    
                                       A
                                       1
                                       
                                          −
                                          1
                                       
                                    
                                    
                                       (
                                       
                                          b
                                          2
                                       
                                       −
                                       
                                          b
                                          1
                                       
                                       )
                                    
                                    ,
                                 
                              
                           
                        where it usually assumes that 
                           
                              
                                 A
                                 1
                              
                              =
                              
                                 A
                                 2
                              
                           
                         (the detailed derivations can be found in reference [4]). Moreover, we can replace global offset with a local one and integrate information over a neighborhood, to remove the unexpected noise [4]. Also we apply the algorithm as implemented in the OpenCV library, and Fig. 5
                         shows several examples of the resulting optical fields for the detected upper-body regions.

It can be seen from Fig. 5 that the optical flow of walking behaviors seems smooth and uniform with small values and few directions (such as Fig. 5(a)–(c)). While the running activity enjoys messy optical flow fields, admitting variant directions and large amplitude (such as Fig. 5(d)–(f)). Therefore, an effective way is exploiting the amplitude of generated optical flow to distinguish the running person from walking pedestrians.

Generally, the optical flow in each point is denoted as a vector which can be decomposed into x and y directions. Given the calculated amplitudes Ax
                         and Ay
                         in both directions, the amplitude of this optical flow is calculated by

                           
                              (5)
                              
                                 
                                    A
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   A
                                                   x
                                                
                                             
                                             2
                                          
                                          +
                                          
                                             
                                                
                                                   A
                                                   y
                                                
                                             
                                             2
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        Usually, both the maximum and average amplitude values can be used as the measurement. However, the maximum considers only one location, which may cause the information loss of other important positions, and the unexpected impulsive noise will also affect the results. Although the average value takes all the locations into account, this kind of evaluation is also not a wise choice. The reason is that the bounding box inevitably includes some background pixels, which produce slight optical flow and have almost no positive response to the feature of the region,

In this paper, to achieve robust optical flow statistics, we adopt a least-median-squares type estimator to exploit the information of optical flow. We sort the amplitudes of optical flows at all locations in descending way, pick out a portion of values and average them as the final score s. In order to remove the useless background pixels, we select the top 45% values, and then take out the top 5% data to further avoid the effect of impulse noise. It is not difficult to understand features in this interval are more important and useful.

After we obtain the score s for each output region, complicated methods like SVM classifiers or statistical models can be introduced to differentiate the running behavior. In this paper, we simply set a global threshold T to distinguish walking and running behaviors, which is obtained based on our empirical observations. If the score is larger than the predefined threshold, i.e., s > T, the corresponding region is identified as a running person.

@&#EXPERIMENTS@&#

We note that the proposed baseline algorithm is implemented in C++ with the OpenCV library, which runs at real-time (25 fps) with 2.7GHz CPU and 2GB memory. In this section, we evaluate the baseline method on the proposed running detection benchmark, and report the precision and recall results in this section. In addition, since the threshold T is a critical parameter for our method, we also evaluate the performance with different values of T and discuss the effect of the parameter.


                        Fig. 6
                         demonstrates the qualitative results (i.e., representative screenshots) of successful detections and failure cases. We can see from the figure that, people run in the scene at various speeds and directions, but our method is capable of capturing the unusual speeds and locating the running persons accurately. The false detections are due to two aspects: (1) bicyclists or motorcyclists who also possess large optical flow; (2) incorrect detections like legs or hands whose movements are relatively drastic. On the whole, our algorithm performs well in these challenging videos and can detect most of running behaviors successfully.

We evaluate the quantitative performance in terms of precision and recall criteria. Table 1 summarizes the amount of activity and recall performance for each video that we obtain. Note that in some scenes we have hundred percent detection rate, implying that the optical flow information used is desirable for running classification, and the human upper-body is a suitable region for determining whether running behavior occurs. Fig. 7
                         illustrates the precision results in all the 20 videos. We can see that although good recall performance is achieved, there are lots of false positives and the precision result is not satisfying. However, as a baseline method, the performance is sufficiently comparative to promote the further research in this field. To assess the overall performance, we also report that the average precision and recall are 67.8% and 90.1%, respectively.

The parameter T is important and crucial for our method. False positives may increase with a lower T value, while a larger one will lead to the drop of the recall performance. Fig. 8
                         shows the average precision and recall results for different values of T. When T is too small, most walking behaviors will be regarded as running, and thus the precision performance will decrease. On the other hand, given a large value of T, some running persons cannot be detected, and the recall property slightly reduces. We can see that 
                           
                              T
                              =
                              10
                           
                         achieves the most favorable performance, and therefore we choose this value and fix it in all our experiments. In addition, we also investigate the effects of two thresholds for noise suppression in Fig. 9
                        , from which we can see that the proposed algorithm achieves stable performance if the thresholds are around 5% and 45%.

@&#CONCLUSION@&#

In this paper, we propose a benchmark dataset with diversity of challenging scenes and factors for human running detection, and introduce the evaluation criteria to assess the detection performance. In addition, we present a baseline method based on the Farnebäck optical flow method to distinguish running behaviors from walking. Extensive experiments on the proposed benchmark demonstrate the effectiveness of our algorithm. We hope that our running detection benchmark provides new insights to this specific field, by making experimental evaluation more standardized and easily accessible. In the future, we will expand our database to collect more video clips on security scenes (including airport terminal, subway station, bank and so on).

@&#ACKNOWLEDGEMENT@&#

This work was supported in part by the Natural Science Foundation of China under Grant 61502070, and in part by Omron Corporation Japan.

@&#REFERENCES@&#

