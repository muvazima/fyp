@&#MAIN-TITLE@&#Fuzzy-based discriminative feature representation for children's speech recognition

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A novel fuzzy-based discriminative feature selection was proposed.


                        
                        
                           
                           The proposed method enhanced discriminative ability of the cepstral features.


                        
                        
                           
                           HMM and MLP classifiers were adopted based on parameter selection.


                        
                        
                           
                           New arrangement of static, delta and delta–delta cepstral coefficient was obtained.


                        
                        
                           
                           The features outperformed the conventional MFCCs in children's speech recognition.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Children's speech recognition

Mel-frequency cepstral coefficient

Multi-layer perceptron

Hidden Markov model

Fuzzy-based feature selection

@&#ABSTRACT@&#


               
               
                  Automatic recognition of the speech of children is a challenging topic in computer-based speech recognition systems. Conventional feature extraction method namely Mel-frequency cepstral coefficient (MFCC) is not efficient for children's speech recognition. This paper proposes a novel fuzzy-based discriminative feature representation to address the recognition of Malay vowels uttered by children. Considering the age-dependent variational acoustical speech parameters, performance of the automatic speech recognition (ASR) systems degrades in recognition of children's speech. To solve this problem, this study addresses representation of relevant and discriminative features for children's speech recognition. The addressed methods include extraction of MFCC with narrower filter bank followed by a fuzzy-based feature selection method. The proposed feature selection provides relevant, discriminative, and complementary features. For this purpose, conflicting objective functions for measuring the goodness of the features have to be fulfilled. To this end, fuzzy formulation of the problem and fuzzy aggregation of the objectives are used to address uncertainties involved with the problem.
                  The proposed method can diminish the dimensionality without compromising the speech recognition rate. To assess the capability of the proposed method, the study analyzed six Malay vowels from the recording of 360 children, ages 7 to 12. Upon extracting the features, two well-known classification methods, namely, MLP and HMM, were employed for the speech recognition task. Optimal parameter adjustment was performed for each classifier to adapt them for the experiments. The experiments were conducted based on a speaker-independent manner. The proposed method performed better than the conventional MFCC and a number of conventional feature selection methods in the children speech recognition task. The fuzzy-based feature selection allowed the flexible selection of the MFCCs with the best discriminative ability to enhance the difference between the vowel classes.
               
            

@&#INTRODUCTION@&#

Industries and regular people are adopting the application of automatic speech recognition (ASR), such as voice-controlled systems for mobility-impaired people, voice dialing, simplified systems based on speech communication, content-based spoken audio search, speech therapy [1], and others. Despite the versatility of the English language, efforts on developing ASR systems are not limited to English. In several languages, such as Chinese [2,3], Japanese [4,5], Thai [6,7], Portuguese [8], and Arabic [9], research in this area continues to develop efficient ASR systems. While most speech recognition systems focus more on adults than children, the speech recognition of children has numerous applications, such as in educational games for children [10–12] aids for pronunciation [13,14], and in reading based on interactive platforms [10,15]. The speech sounds of children have higher spectral variations and more dynamic characteristics than that of adults due to the extension of the vocal tract size in growing children [16,17]. The performance of ASR systems adapted by adult speech degrades under the context of children speech. To deal with this effect in speech recognition, some approaches used a larger number of samples [18]. Variation of acoustic parameters is larger in children compared to that of adults [19,20], and thus, a frequency warping algorithm is used to reduce the variation of acoustic parameters [21]. In this study, a discriminative feature extraction method is proposed to deal with recognition of children's speech.

Conventionally, the extraction of helpful information about speech signals utilizes various signal-processing techniques to investigate relevant signal characteristics such as energy and spectrum. Mel-frequency cepstral coefficients (MFCCs) and perceptual linear prediction (PLP) parameters are the commonly used feature extraction methods. However, the MFCC does not provide an optimal feature space for the purpose of discrimination. Optimization of speech features such as MFCC is studied in some approaches. Occasionally, the extraction and the optimization of the features are simultaneously accomplished [22–25,29]. The reason for developing such structures is that the degradation of ASR systems can be the result of a mismatch between the acoustic conditions of training and application environments including additive noise, channel distortion, different speaker characteristics, and so on. To develop speaker independent ASR algorithms, speaker normalization is accomplished to produce a pitch-independent representation of speech [26–28]. In this way, several speaker normalization approaches utilize the formant-ratio theory. According to this theory, the quality of the spoken vowel depends on the log frequency intervals between formants (defined as a ratio). Consequently, invariant representation of vowels can be realized by shifting activations along a log frequency axis [29–35]. Computing MFCC involves a somewhat spectral smoothing carried out by triangular filter banks on the spectrum of the speeches. Consequently, filters with higher bandwidth result in higher spectral-smoothing and fewer filters as well as fewer coefficients (MFCCs). This effect is responsible for the degradation of the ASR approaches in the context of children's speech. To cope with this effect we employed narrower filters in obtaining MFCCs thus loss of spectral information can be prevented. However, this method results in many redundant and non-informative features.

In various intricate application tasks, such as phoneme recognition where the systems are developed based on real data, processing a large number of features is frequent. However, many of the features are not relevant to the problem of interest. In addition, numerous features demand a significant amount of computations, which slow down the overall process. Under these circumstances, automatically dismissing irrelevant features is necessary to achieve a model that is accurate and reliable in solving the problem at hand [36,37]. Additionally, using the feature selection technique reduces the computational cost, which enhances the response time of the process. Feature selection (FS) is one of the most productive research areas and has attracted a considerable amount of attention over the past three decades. Proposed FS methods in the literature vary in terms of evaluation criteria for the selected subsets. Frequent criteria recommended in the literature include relevance [38], gain entropy [39], and contingency table analysis [40]. These methods offer no intrinsic order of features. For the FS task, two well-known methods for feature evaluation are used. The first one uses distance metrics to measure the overlap between different classes [41]. Under this strategy, probability density functions of the sample distribution can also be considered. Consequently, the subset for which the average overlap is minimal is considered as a solution. Meanwhile, intra- as well as inter-class distances can be measured by considering the fuzziness and entropy of the features [42]. Through the second method, classification errors based on the feature subset candidates are evaluated. Consequently, the subset with minimal misclassification is selected as a solution. However, given that 
                        
                           
                              2
                           
                           
                              n
                           
                        
                      feature subsets can be generated by n features, we have to conduct a number of computations to obtain the optimal subset. Consequently, numerous related techniques have been proposed in the literature. Several methods in [43] have been compared and a Genetic algorithm (GA) method has been implemented for the variable selection. As recommended in this study, GA is a good choice for large-scale optimization in which the number of variables exceeds 50. In some feature selection approaches, particle swarm optimization [44,45] and ant colony optimization [46–48] have been employed. In different areas of researches multi-objective optimization based on GA have been performed [49] and dealing with conflicting objectives has been studied. Toward the problem of feature subset selection, multiple objectives are defined in several cases. Consequently, compromising between these objectives is necessary to solve the problem. Meanwhile, using flexibilities to define the optimization problem is helpful. For this purpose, fuzzy set theory is used to codify the flexibilities for the objective functions. This technique leads to achieving extra trade-off to solve this problem. Fuzzy optimization techniques have been frequently used in the optimization of conflicting objectives [48,50–52]. In most cases, having a priori knowledge about the priority of the aggregating objectives ensures incorporation of these techniques in an accumulative structure. Thus, combining the multiple objectives in such approaches is unnecessary. An effective idea for weighing the importance of conflicting objectives is using fuzzy decision making. Through fuzzy logic, uncertainties associated with different objective functions can be effectively formulated to combine using fuzzy aggregation functions [53]. In this study, specific fuzzy codification is proposed based on the data structure in the feature space, which considers the statistical dependence of the selecting features. Therefore, complementary discriminative features can be properly selected to enhance the performance of the proposed children's speech recognition method.

This paper is organized as follows. Section 2 provides a brief explanation of the MFCC as the speech feature extraction method. Additionally, a proposed fuzzy-based feature selection method is presented in Section 2. MLP-based speech recognizer as well as HMM-based speech recognizer are explained in Section 3. Experiments are presented in Section 4 and results are discussed in Section 5, while the conclusion is discussed in Section 6.

Providing a rational representation of the speech information, known as speech feature extraction, is the first step to accomplish any recognition procedure. One of the most popular acoustic feature extraction methods widely used in various ASR systems is the MFCC. Although its name seems incomprehensible, it conveys its nonlinear characteristic based on Mel distances. Mel distances or Mel scales are obtained from the resolution of human hearing. The Mel scales have a larger spread from higher frequencies to lower frequencies. In other words, its frequency scale has linear frequency spacing below 1 kHz and logarithmic spacing for more frequencies. Therefore, people can discriminate bass sounds better than treble, and MFCCs can be obtained by a Fourier transformation of the log–log warped frequency spectrum.

The approximate formula to obtain the Mels for a given frequency f in Hz is given as follows:
                           
                              (1)
                              
                                 
                                    
                                       F
                                    
                                    
                                       mel
                                    
                                 
                                 =
                                 2595
                                 .
                                 
                                    
                                       log
                                    
                                    
                                       10
                                    
                                 
                                 
                                 
                                    (
                                    1
                                    +
                                    
                                       (
                                       
                                          f
                                          100
                                       
                                       )
                                    
                                    )
                                 
                              
                           
                         To realize the MFCC in the given DFT of the input signal in Eq. (2), the Mel-frequency filterbank is defined with p triangular filters 
                           
                              
                                 m
                              
                              
                                 j
                              
                           
                           
                           (
                           j
                           =
                           1
                           ,
                           2
                           ,
                           …
                           ,
                           p
                           )
                        , as shown in Fig. 1
                        .
                           
                              (2)
                              
                                 
                                    
                                       X
                                    
                                    
                                       a
                                    
                                 
                                 [
                                 k
                                 ]
                                 =
                                 
                                    ∑
                                    
                                       n
                                       =
                                       0
                                    
                                    
                                       N
                                       −
                                       1
                                    
                                 
                                 [
                                 n
                                 ]
                                 
                                    
                                       e
                                    
                                    
                                       −
                                       j
                                       2
                                       π
                                       k
                                       /
                                       N
                                    
                                 
                                 ,
                                 
                                 0
                                 ≤
                                 k
                                 ≤
                                 N
                              
                           
                         The filter bank is applied to the FFT by multiplying each FFT magnitude coefficient to its corresponding filter gain from the Mel filter bank followed by a summation of the result. This operation is performed by using Eq. (3):
                           
                              (3)
                              
                                 
                                    
                                       m
                                    
                                    
                                       j
                                    
                                 
                                 =
                                 
                                    ∑
                                    
                                       k
                                       =
                                       0
                                    
                                    
                                       N
                                       −
                                       1
                                    
                                 
                                 
                                    
                                       |
                                       
                                          
                                             X
                                          
                                          
                                             a
                                          
                                       
                                       [
                                       k
                                       ]
                                       |
                                    
                                    
                                       2
                                    
                                 
                                 
                                    
                                       H
                                    
                                    
                                       j
                                    
                                 
                                 [
                                 k
                                 ]
                                 ,
                                 
                                 0
                                 ≤
                                 j
                                 ≤
                                 p
                              
                           
                         where 
                           
                              
                                 H
                              
                              
                                 j
                              
                           
                           [
                           k
                           ]
                         denotes the transfer function of filter j. The Mel-frequency cepstrum as described by Eq. (4) represents the discrete cosine transform of the p filter outputs.
                           
                              (4)
                              
                                 
                                    
                                       MFCC
                                    
                                    
                                       i
                                    
                                 
                                 =
                                 
                                    
                                       2
                                       N
                                    
                                 
                                 
                                    ∑
                                    
                                       j
                                       =
                                       1
                                    
                                    p
                                 
                                 
                                    
                                       m
                                    
                                    
                                       j
                                    
                                 
                                 cos
                                 
                                 
                                    (
                                    
                                       
                                          π
                                          i
                                       
                                       N
                                    
                                    (
                                    j
                                    −
                                    0.5
                                    )
                                    )
                                 
                              
                           
                         In standard MFCC, frequently proposed in literature, the number of Mel filters is 13. As a result, 13 cepstral coefficients are produced. Afterward, 13 deltas and 13 delta–delta coefficient are computed from the cepstral coefficients to provide 39 MFCC features. In the present study, to allow higher flexibility to select relevant features, 40 filters are used. Subsequently, relevant features are selected based on the proposed feature selection method for children's speech recognition.

We suppose that one is assigned a labeled data set X, which consists of n number of labeled patterns 
                              
                                 
                                    x
                                 
                                 
                                    k
                                 
                              
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    p
                                 
                              
                           , where p denotes the number of features. If we denote 
                              
                                 
                                    x
                                 
                                 
                                    k
                                 
                                 
                                    m
                                 
                              
                            as the value of the mth feature in 
                              
                                 
                                    F
                                 
                                 
                                    m
                                 
                              
                            of pattern 
                              
                                 
                                    X
                                 
                                 
                                    k
                                 
                              
                           , then we can represent each pattern 
                              
                                 
                                    x
                                 
                                 
                                    k
                                 
                              
                            of set X by a vector 
                              
                                 
                                    X
                                 
                                 
                                    k
                                 
                              
                              =
                              [
                              
                                 
                                    x
                                 
                                 
                                    k
                                 
                                 
                                    1
                                 
                              
                              ,
                              
                                 
                                    x
                                 
                                 
                                    k
                                 
                                 
                                    2
                                 
                              
                              ,
                              .
                              .
                              .
                              ,
                              
                                 
                                    x
                                 
                                 
                                    k
                                 
                                 
                                    p
                                 
                              
                              ]
                           .

Where all features are assumed to be represented by set F, we obtain
                              
                                 (5)
                                 
                                    F
                                    =
                                    {
                                    
                                       
                                          f
                                       
                                       
                                          1
                                       
                                    
                                    ,
                                    
                                       
                                          f
                                       
                                       
                                          2
                                       
                                    
                                    ,
                                    .
                                    .
                                    .
                                    ,
                                    
                                       
                                          f
                                       
                                       
                                          p
                                       
                                    
                                    }
                                 
                              
                            On this occasion, classification problem is defined as mapping of the 
                              
                                 
                                    X
                                 
                                 
                                    k
                                 
                              
                            to the class space C characterized by a set of possible class labels expressed as follows:
                              
                                 (6)
                                 
                                    C
                                    =
                                    [
                                    
                                       
                                          c
                                       
                                       
                                          1
                                       
                                    
                                    ,
                                    
                                       
                                          c
                                       
                                       
                                          2
                                       
                                    
                                    ,
                                    .
                                    .
                                    .
                                    ,
                                    
                                       
                                          c
                                       
                                       
                                          w
                                       
                                    
                                    ]
                                 
                              
                            Dealing with the problem involves developing a hypothesis, known as a classifier, to predict the labels of new instance data. Meanwhile, developing such a hypothesis requires a machine learning process in which the feature value of the instances is employed. An essential concern in feature selection is describing the relevance of the features to the problem of interest. The feature selection method attributes the relevance of a given subset to the processing task. Despite the belief that a higher number of features provides more discriminating power to the classification, in practice, as a limited amount of training data is accessible, more features slow down the process and classifiers are prone to overfitting [54–56]. Therefore, using irrelevant features degrades the learning performance. One of the key aspects of the feature selection research area is evaluating the goodness of the features. Consequently, various types of measuring criteria have been recommended in the literature [57] and are categorized as follows:

Filter method: This technique evaluates the quality of the features through feature subset measurement [58]. Feature selection is accomplished as an independent process to the selected predictor.

Wrapper method: This method evaluates feature subsets using the error rate of the classifiers [59]. Wrapper method does not feature what is happening inside the classifiers but only scores features based on the classification results.

Embedded method: This method includes the learning process and selects variables based on the employed modeling method [60]. This method can also achieve more functionality for a couple of objective functions including the addition of a number of variables and optimization of the fitness quality.

Hybrid method: An emerging method in feature selection [61,62], this method is a mixture of the wrapper and filter methods where the optimization procedure used in the wrapper method is guided by the scoring of feature subsets obtained from the filter method.

In fact, for classifiers with large computational costs, using the wrapper method is a time-consuming strategy. This condition is worse when the classifier is applied to large databases. In contrast, a filter method that uses a discriminant measurement for feature subsets can face a feature selection problem without addressing the classification issue. However, because the filter method itself cannot predict the classification error in a real classification problem, this study uses the hybrid strategy with the help of the wrapper method. First, a filter method is used to sort the features based on their power of prediction as well as linear independence of the features. In the proposed hierarchical feature-sorting algorithm, a fuzzy criterion is used to measure the predictive power of the features. The criterion is analogous to Fisher's discriminant criterion but is associated with a fuzzy codification that considers the independent measurement of the features. In the next step, the classifiers are adapted to the sorted features to produce the optimal feature subsets that still have higher discrimination ability for the classification. In this step, to prevent overfitting while the classifier is applied to the data, cross-validation method is used. Note that the samples used for the feature selection process are randomly selected from the training set and an equal number of samples from each class should be used.

We suppose that 
                              
                                 
                                    x
                                 
                                 
                                    i
                                    ,
                                    n
                                 
                                 
                                    m
                                 
                              
                            is the mth feature of the nth sample from class i. Inter-class distance between two different classes of i and j based on the feature m is expressed as follows:
                              
                                 (7)
                                 
                                    
                                       
                                          d
                                       
                                       
                                          i
                                          ,
                                          j
                                          ,
                                          m
                                       
                                    
                                    =
                                    
                                       1
                                       
                                          2
                                          
                                             
                                                c
                                             
                                             
                                                n
                                             
                                          
                                       
                                    
                                    
                                       ∑
                                       
                                          l
                                          =
                                          1
                                       
                                       
                                          2
                                          
                                             
                                                c
                                             
                                             
                                                n
                                             
                                          
                                       
                                    
                                    
                                       
                                          (
                                          
                                             
                                                U
                                             
                                             
                                                l
                                             
                                          
                                          −
                                          
                                             
                                                U
                                             
                                             
                                                ¯
                                             
                                          
                                          )
                                       
                                       
                                          2
                                       
                                    
                                 
                              
                           
                           
                              
                                 (8)
                                 
                                    
                                       
                                          U
                                       
                                       
                                          ¯
                                       
                                    
                                    =
                                    
                                       1
                                       
                                          2
                                          
                                             
                                                c
                                             
                                             
                                                n
                                             
                                          
                                       
                                    
                                    
                                       ∑
                                       
                                          l
                                          =
                                          1
                                       
                                       
                                          2
                                          
                                             
                                                c
                                             
                                             
                                                n
                                             
                                          
                                       
                                    
                                    
                                       
                                          U
                                       
                                       
                                          l
                                       
                                    
                                 
                              
                           
                           
                              
                                 (9)
                                 
                                    U
                                    =
                                    
                                       [
                                       
                                          
                                             x
                                          
                                          
                                             i
                                             ,
                                             1
                                          
                                          
                                             m
                                          
                                       
                                       ,
                                       
                                          
                                             x
                                          
                                          
                                             i
                                             ,
                                             2
                                          
                                          
                                             m
                                          
                                       
                                       ,
                                       .
                                       .
                                       .
                                       ,
                                       
                                          
                                             x
                                          
                                          
                                             i
                                             ,
                                             
                                                
                                                   c
                                                
                                                
                                                   n
                                                
                                             
                                          
                                          
                                             m
                                          
                                       
                                       ,
                                       
                                          
                                             x
                                          
                                          
                                             j
                                             ,
                                             1
                                          
                                          
                                             m
                                          
                                       
                                       ,
                                       
                                          
                                             x
                                          
                                          
                                             j
                                             ,
                                             2
                                          
                                          
                                             m
                                          
                                       
                                       ,
                                       .
                                       .
                                       .
                                       ,
                                       
                                          
                                             x
                                          
                                          
                                             j
                                             ,
                                             
                                                
                                                   c
                                                
                                                
                                                   n
                                                
                                             
                                          
                                          
                                             m
                                          
                                       
                                       ]
                                    
                                 
                              
                            where 
                              
                                 
                                    c
                                 
                                 
                                    n
                                 
                              
                            is the number of samples from class i or j.

To evaluate the discriminability of the classes based on a feature in addition to the intra-class distance, we have to consider the inter-class distance. Consequently, the discriminative criterion definable for a specific class, i, based on the feature m, can be expressed as follows:
                              
                                 (10)
                                 
                                    
                                       
                                          D
                                       
                                       
                                          i
                                          ,
                                          m
                                       
                                    
                                    =
                                    
                                       ∑
                                       j
                                    
                                    
                                       (
                                       
                                          
                                             
                                                d
                                             
                                             
                                                i
                                                ,
                                                j
                                                ,
                                                m
                                             
                                          
                                          
                                             
                                                
                                                   λ
                                                
                                                
                                                   m
                                                
                                             
                                             +
                                             
                                                
                                                   d
                                                
                                                
                                                   j
                                                   ,
                                                   j
                                                   ,
                                                   m
                                                
                                             
                                          
                                       
                                       )
                                    
                                 
                              
                            where 
                              
                                 
                                    λ
                                 
                                 
                                    m
                                 
                              
                            is a tuning coefficient to adjust the relative effect of the inter- and intra-class distance on the criterion. Here, 
                              
                                 
                                    λ
                                 
                                 
                                    m
                                 
                              
                            is equal to 0.25 of the average of the intra-class distances (
                              
                                 
                                    λ
                                 
                                 
                                    m
                                 
                              
                              =
                              
                                 0.25
                                 N
                              
                              
                                 
                                    ∑
                                 
                                 
                                    j
                                    =
                                    1
                                 
                                 
                                    N
                                 
                              
                              
                                 
                                    d
                                 
                                 
                                    j
                                    ,
                                    j
                                    ,
                                    m
                                 
                              
                           ). 
                              
                                 
                                    λ
                                 
                                 
                                    m
                                 
                              
                            guarantees limited effect of the intra-class distance on D. As Eq. (10) indicates, a higher value of D represents a higher discriminability of class i against the other classes based on the feature m. In reality, some features cannot provide similar discrimination scoring for all of the classes. Consequently, scoring a feature should address uncertainties about the goodness of the feature. To generalize the criterion for all of the classes and, at the same time, codify the uncertainties in this study, we use fuzzy aggregation operation. Thus, fuzzy memberships are employed to fuzzify the criteria and aggregate them using fuzzy aggregation operations.

To introduce fuzzy goals into the feature selection problem, mapping functions, namely, fuzzy memberships are used. Let 
                              
                                 
                                    Cr
                                 
                                 
                                    k
                                 
                              
                            with 
                              k
                              =
                              1
                              ,
                              .
                              .
                              .
                              ,
                              M
                            be a fuzzy criterion introducing an objective function, defined by a membership function 
                              
                                 
                                    μ
                                 
                                 
                                    
                                       
                                          Cr
                                       
                                       
                                          k
                                       
                                    
                                 
                              
                           , which maps the range of criteria into the interval between zero and one. The objectives are determined based on the corresponding optimization criteria.

Different fuzzy membership functions, namely, triangular, Gaussian, and trapezoidal, are commonly used in the literature to map the crisp data into the fuzzy space. An example of such membership functions has been shown in Fig. 2
                           . In a trapezoidal membership function, 
                              
                                 
                                    x
                                 
                                 
                                    1
                                 
                              
                            and 
                              
                                 
                                    x
                                 
                                 
                                    2
                                 
                              
                           , and in a Gaussian membership, σ determine the dropping rate of the membership to zero based on the difference of the fuzzifying value from the central value of the membership. In a triangular membership function, 
                              
                                 
                                    x
                                 
                                 
                                    1
                                 
                              
                            determines this effect. Fig. 2(d) presents a comparison of the membership functions used for this study. For the experiments, the values 
                              
                                 
                                    x
                                 
                                 
                                    2
                                 
                              
                            in trapezoidal function, 
                              
                                 
                                    x
                                 
                                 
                                    1
                                 
                              
                            in triangular function and σ in Gaussian function are set in a manner that corresponds to a high amount of criteria for the feature with the highest discriminative criteria for all of the classes. The membership value for these criteria is 0.6. This relation can be expressed as follow:
                              
                                 (11)
                                 
                                    
                                       
                                          μ
                                       
                                       
                                          i
                                          ,
                                          m
                                       
                                    
                                    (
                                    a
                                    )
                                    =
                                    0.6
                                    ,
                                    
                                    a
                                    =
                                    
                                       
                                          a
                                          r
                                          g
                                          
                                          m
                                          a
                                          x
                                       
                                       i
                                    
                                    (
                                    
                                       
                                          D
                                       
                                       
                                          i
                                          ,
                                          m
                                       
                                    
                                    )
                                 
                              
                            The amounts of 
                              
                                 
                                    x
                                 
                                 
                                    1
                                 
                              
                           , 
                              
                                 
                                    x
                                 
                                 
                                    2
                                 
                              
                           , and σ are sensitive to the number of samples used in the test set. The reason to use different shapes of membership functions is to study the effect of their shape on the result. Note that three experiments were conducted and in each, only one of the mentioned membership functions was employed.

The fuzzy criteria must be aggregated to be used in the feature selection problem. To achieve the optimal solution of a problem, the constraints and objectives must be fulfilled at the same time to a maximum degree [51]. For this purpose, an optimal feature subset is definable by aggregating the fuzzy memberships, 
                              
                                 
                                    μ
                                 
                                 
                                    
                                       
                                          Cr
                                       
                                       
                                          k
                                       
                                    
                                 
                              
                           , to produce 
                              
                                 
                                    μ
                                 
                                 
                                    s
                                 
                              
                            as follows:
                              
                                 (12)
                                 
                                    
                                       
                                          μ
                                       
                                       
                                          s
                                       
                                    
                                    =
                                    
                                       
                                          μ
                                       
                                       
                                          
                                             
                                                Cr
                                             
                                             
                                                1
                                             
                                          
                                       
                                    
                                    ∘
                                    
                                       
                                          μ
                                       
                                       
                                          
                                             
                                                Cr
                                             
                                             
                                                2
                                             
                                          
                                       
                                    
                                    ∘
                                    .
                                    .
                                    .
                                    ∘
                                    
                                       
                                          μ
                                       
                                       
                                          
                                             
                                                Cr
                                             
                                             
                                                k
                                             
                                          
                                       
                                    
                                 
                              
                            Through the aggregation operations, numerous fuzzy sets can be combined whereby each operator has its own properties that can be useful depending on the in-hand problem [63]. This operation is conducted to generate a single representation of the fuzzy and crisp dataset. Czogała and Zimmermann [64] have used a number of operations mostly based on triangular norms including t- and s-norms to conduct decision making based on fuzzy aggregation operations. A widespread survey has been presented by Dubois and Prade [65], which focused on the aggregation operations used in fuzzy sets for decision making and information processing. Aggregation of different criteria using ordered weighted average (OWA) operator has been introduced by Yager [66]. This criteria showed cumulative properties of fuzzy “or” as well as fuzzy “and” operators, behaving somehow between the two operators. The authors in [67] employed a neural network structure to conduct an information fusion for computer vision system. Their method was based on the fuzzy-set theory. Several properties of the aggregation scores including self-identity of fuzzy sets were investigated by Yager and Rybalov [68]. The authors of [69] proposed a method to aggregate the decisions using generalized mixture operators. The performance of their method was examined by comparing OWA and weighted average (WA) methods. Generalizing the study conducted by Pasi, in [70] the author used fuzzy sets to represent individual preferences through values placed between 0 and 1. Subsequently, the authors computed the collective preferences using OWA. In a group decision-making context, Pasi [71] investigated the problem of considering the individual opinion in decision making. The decline of the individual value into a representative value, known as majority opinion, is regularly accomplished via the aggregation procedure. They described this concept based on the fuzzy set theory using a linguistic quantifier (such as most), which is formally defined as a fuzzy subset. A generalization of certain well-known aggregation operators were presented by Vaníček et al. [72], who discussed certain conditions for which an operator can be considered as an aggregation operator.

In this study, “fuzzy-and” operator proposed by Werners [73] is used to aggregate the fuzzy memberships. The fuzzy set decision is defined by the membership function
                              
                                 (13)
                                 
                                    
                                       
                                          μ
                                       
                                       
                                          D
                                       
                                    
                                    (
                                    x
                                    )
                                    =
                                    γ
                                    
                                       min
                                       
                                          i
                                          =
                                          1
                                       
                                       N
                                    
                                    
                                    
                                       
                                          μ
                                       
                                       
                                          i
                                       
                                    
                                    (
                                    x
                                    )
                                    +
                                    (
                                    1
                                    −
                                    γ
                                    )
                                    
                                       1
                                       N
                                    
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       N
                                    
                                    
                                       
                                          μ
                                       
                                       
                                          i
                                       
                                    
                                    (
                                    x
                                    )
                                 
                              
                            where 
                              
                                 
                                    μ
                                 
                                 
                                    i
                                 
                              
                              (
                              x
                              )
                            denotes the membership function of the fuzzy sets added using the fuzzy-and operator, γ is the compensation degree, and i is the ith fuzzy membership. For 
                              γ
                              =
                              1
                           , the fuzzy-and operator behaves as min-operator, and behavior of 
                              γ
                              =
                              0
                            is similar to the arithmetic average of the fuzzy memberships. Fig. 3
                            presents the “fuzzy and” operator based on different value of compensation degree γ and aggregating memberships.

Plugging Eq. (10) into (13), we obtain the fuzzy discriminative criteria that present the discrimination power of the features.
                              
                                 (14)
                                 
                                    
                                       
                                          μ
                                       
                                       
                                          m
                                       
                                    
                                    (
                                    d
                                    )
                                    =
                                    γ
                                    
                                       min
                                       
                                          i
                                          =
                                          1
                                       
                                       N
                                    
                                    
                                    
                                       
                                          μ
                                       
                                       
                                          i
                                          ,
                                          m
                                       
                                    
                                    (
                                    d
                                    )
                                    +
                                    (
                                    1
                                    −
                                    γ
                                    )
                                    
                                       1
                                       N
                                    
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       N
                                    
                                    
                                       
                                          μ
                                       
                                       
                                          i
                                          ,
                                          m
                                       
                                    
                                    (
                                    d
                                    )
                                 
                              
                            where 
                              
                                 
                                    μ
                                 
                                 
                                    m
                                 
                              
                            is the criterion that we will use to sort the features from the highest to the lowest level of importance, 
                              
                                 
                                    μ
                                 
                                 
                                    i
                                    ,
                                    m
                                 
                              
                            denotes the fuzzified version of 
                              
                                 
                                    D
                                 
                                 
                                    i
                                    ,
                                    m
                                 
                              
                           , and N is the number of classes. For the experiments, the aggregation parameter, γ was set to 0.35.

In fuzzy feature selection approaches, dependence of the features is not embedded in the equations. In the sorting framework for feature selection, we consider the statistical independence of the features in addition to their discriminatory power in determining the feature arrangement. Independent features with discriminative power contain valuable discriminative information for classification. In this study, a measurement of linear dependence between the features is considered to resort them. For this purpose, Pearson's distance is used [74]. Having n samples of two variables 
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                            and 
                              
                                 
                                    y
                                 
                                 
                                    i
                                 
                              
                           , the Pearson distance for these variables is expressed as
                              
                                 (15)
                                 
                                    
                                       
                                          D
                                       
                                       
                                          p
                                       
                                    
                                    (
                                    x
                                    ,
                                    y
                                    )
                                    =
                                    1
                                    −
                                    
                                       
                                          r
                                       
                                       
                                          x
                                          ,
                                          y
                                       
                                    
                                 
                              
                           
                           
                              
                                 (16)
                                 
                                    
                                       
                                          r
                                       
                                       
                                          x
                                          y
                                       
                                    
                                    =
                                    
                                       
                                          n
                                          ∑
                                          
                                             
                                                x
                                             
                                             
                                                i
                                             
                                          
                                          
                                             
                                                y
                                             
                                             
                                                j
                                             
                                          
                                          −
                                          ∑
                                          
                                             
                                                x
                                             
                                             
                                                i
                                             
                                          
                                          ∑
                                          
                                             
                                                y
                                             
                                             
                                                j
                                             
                                          
                                       
                                       
                                          
                                             
                                                (
                                                n
                                                −
                                                1
                                                )
                                                ∑
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      i
                                                   
                                                   
                                                      2
                                                   
                                                
                                                −
                                                
                                                   
                                                      (
                                                      ∑
                                                      
                                                         
                                                            x
                                                         
                                                         
                                                            i
                                                         
                                                      
                                                      )
                                                   
                                                   
                                                      2
                                                   
                                                
                                             
                                          
                                          
                                             
                                                (
                                                n
                                                −
                                                1
                                                )
                                                ∑
                                                
                                                   
                                                      y
                                                   
                                                   
                                                      i
                                                   
                                                   
                                                      2
                                                   
                                                
                                                −
                                                
                                                   
                                                      (
                                                      ∑
                                                      
                                                         
                                                            y
                                                         
                                                         
                                                            i
                                                         
                                                      
                                                      )
                                                   
                                                   
                                                      2
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                            where 
                              
                                 
                                    r
                                 
                                 
                                    x
                                    ,
                                    y
                                 
                              
                            and 
                              
                                 
                                    D
                                 
                                 
                                    p
                                 
                              
                            are the correlation and the Pearson distance of the variables, respectively.

Eq. (15) indicates that for a given couple of features, lower linear dependence results in higher Pearson distance. Note that the value of the correlation as well as the Pearson distance is between zero and one. Considering the uncertainty of the linear independence of the features for selection, we should fuzzify the distance to be included in feature sorting through fuzzy aggregation. A triangular membership function is used to fuzzify the distance. The value of the parameter 
                              
                                 
                                    x
                                 
                                 
                                    1
                                 
                              
                            in this membership function is set in a manner that the highest distance between the feature with highest discriminative power and other features corresponds to 0.45 in the membership scale. Note that lower Pearson distances correspond to lower membership values.

To map the fuzzy aggregation to the crisp data, numerous methods have been proposed in the literature. These methods vary in terms of required computational cost and precision. The most frequently used defuzzification methods are center of gravity and mean of maxima [75]. This study uses the mean of maxima method. To explain this method, we use the notations in [76] and [75].

Let A be a mapping from X (an ordinary nonvoid set) into the interval 
                              [
                              0
                              ,
                              1
                              ]
                           . The value 
                              A
                              (
                              x
                              )
                            of A in 
                              x
                              ∈
                              X
                            indicates the degree of membership of x in A. Eq. (17) provides defuzzification of A.
                              
                                 (17)
                                 
                                    
                                       MeOM
                                    
                                    (
                                    A
                                    )
                                    =
                                    
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                x
                                                ∈
                                                
                                                   core
                                                
                                                (
                                                A
                                                )
                                             
                                          
                                          x
                                       
                                       
                                          |
                                          
                                             core
                                          
                                          (
                                          A
                                          )
                                          |
                                       
                                    
                                 
                              
                            where the core of A is the set of elements with the largest degree of membership in A.
                              
                                 (18)
                                 
                                    
                                       core
                                    
                                    (
                                    A
                                    )
                                    =
                                    
                                       {
                                       x
                                       |
                                       x
                                       ∈
                                       X
                                        and 
                                       ¬
                                       (
                                       ∃
                                       y
                                       ∈
                                       X
                                       )
                                       
                                          (
                                          A
                                          (
                                          y
                                          )
                                          >
                                          A
                                          (
                                          x
                                          )
                                          )
                                       
                                       }
                                    
                                 
                              
                            In fact, this method estimates the mean of all elements of the core of a fuzzy set.

As a primitive step for the proposed feature selection method, the feature arrangement from higher to lower importance is determined based on the previously discussed criteria. The feature sorting algorithm is enumerated as follows:
                              
                                 1.
                                 Make a list of features sorted based on their fuzzy discriminative power from the highest to the lowest and select the feature with the highest discriminative power as the first feature in the sorting arrangement.

Consider the first P consecutive features in the feature list, provided in step 1, and compute the distance (
                                       
                                          
                                             D
                                          
                                          
                                             p
                                          
                                       
                                    ) of the P features from the feature selected in the previous step.

Compute the membership of each distance.

Compute the fuzzy aggregation of the memberships including the discriminative and linear independence criteria (
                                       
                                          
                                             D
                                          
                                          
                                             p
                                          
                                       
                                    ) using the “fuzzy-and” operator as follows:
                                       
                                          (19)
                                          
                                             
                                                
                                                   μ
                                                
                                                
                                                   a
                                                
                                             
                                             =
                                             γ
                                             
                                                min
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                2
                                             
                                             
                                             
                                                
                                                   μ
                                                
                                                
                                                   m
                                                
                                             
                                             +
                                             (
                                             1
                                             −
                                             γ
                                             )
                                             
                                                1
                                                2
                                             
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                2
                                             
                                             
                                                
                                                   μ
                                                
                                                
                                                   
                                                      
                                                         D
                                                      
                                                      
                                                         p
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 

Defuzzify the fuzzy scores to obtain the crisp scores using Eq. (17) and select the feature with the highest score. Remove the selected feature from the list provided in step 1.

Repeat steps 2 to 5 for all of the features in the feature list provided in step 1.

The measure of linear independence formulated in the sorting algorithm aims to enhance the complementarity between the consecutive features. Consequently, features provided in this method are complementary and discriminative. Two parameters that adjust these conflicting effects at the same time are provided. First is the number of consecutive features, P, used in step 2, and second is the fuzzy aggregation parameter, γ, used in step 4. Value of the parameters γ and P in this study were 0.25 and 6, respectively. The value (0.25) results in higher effect of discriminative criteria on the sorting results.

The final step of the proposed feature selection method includes evaluating the number of features in the feature subset for which optimal classification accuracy is achieved. For this purpose, the classifiers are applied to the randomly selected data from the training database. After sorting the processing data using the hierarchical sorting method explained in the previous section, we adapt two well-known classifiers: MLP and HMM. Through this step, beside the optimal number of features, the optimal parameters of the classifiers are obtained for the experiments. Adjusting the classifier parameters and the number of features is conducted based on the leave-one-out method.

Proposed ASR approaches in literature, in the aspect of statistical modeling, are categorized in two main groups, which are generative and discriminative.

Phone recognition involves finding the best possible sequence of phones (Ph) that should be proper for a given input speech signal, X. Consequently, optimal phone sequence, 
                        
                           
                              Ph
                           
                           
                              ⁎
                           
                        
                     , should be found such that:
                        
                           (20)
                           
                              
                                 
                                    Ph
                                 
                                 
                                    ⁎
                                 
                              
                              =
                              
                                 
                                    a
                                    r
                                    g
                                    
                                    m
                                    a
                                    x
                                 
                                 
                                    Ph
                                 
                              
                              
                              P
                              (
                              
                                 Ph
                              
                              |
                              X
                              )
                           
                        
                      Based on the Bayes rule, Eq. (20) can be rewritten as 
                        
                           
                              Ph
                           
                           
                              ⁎
                           
                        
                        =
                        
                           a
                           r
                           g
                           
                           m
                           a
                           x
                        
                        
                        P
                        (
                        X
                        |
                        
                           Ph
                        
                        )
                        P
                        (
                        
                           Ph
                        
                        )
                     . According to this expression, the phone sequence can be determined based on the corresponding phone class membership and the learned model that indicates the conditional probability distribution of the observed acoustic features X.

During the learning process in generative approaches, the learned model generates the input observations to fit the model. HMMs, hidden trajectory models, Gaussian mixture models (GMMs), stochastic segment models, Bayesian networks, and Markov random fields are examples of such models.

In contrast, the discriminative approaches aim at maximizing the discriminability of the different classes while modeling the posterior class distribution. Examples of this approach are methods based on maximum entropy models, logistic regression, neural networks (multi-layer perceptron (MLP), time-delay neural networks (TDNN) or Boltzmann machines), support vector machines (SVMs), and conditional random fields (CRFs).

To benchmark the proposed feature extraction method, this study tests the phone recognition methods from both categories. HMMs with Gaussian mixtures are used in the generative modeling approach, while the MLP is used as the discriminative modeling approach.

A common tool dealing with classification problems, the use of ANNs is proposed frequently in literature. The name “artificial neural network” bears a resemblance to the structure of its computing units and the nervous system. However, mostly because of historical success of Hidden Markov Model (HMM) based ASR systems, commercial sectors prefer to employ these kinds of systems without considering any proof of their superiority to other contemporary methods. Through a neural network, phonemic, lexical, syntactic, pragmatic, and semantic knowledge of speech can be effectively integrated for speech recognition procedures such as segmentation and labeling. Hence, knowledge and constraints are distributed across the neurons in the training procedure.

Quite complex nonlinear classifiers and mapping functions can be effectively generated through ANN training [77,78]. As a result, assumptions about the underlying probability distributions are no longer necessary. In addition, given the simplicity and uniformity of the underlying computing components, the development of such systems is interesting for hardware implementation.

Recent neural network structures have been shown to be capable of time warping the phonemes observed in continuous speech [79]. Dynamic feed-forward networks have demonstrated their efficiency for this purpose as much as the best HMM based systems could. However, the availability of good training data and good strategy is critical for developing an efficient ANN-based ASR system. The capability of the ANNs motivated us to employ the MLP to solve the classification problem. ANN-based speech recognition system requires an optimal number of hidden neurons and input features. If the recognition task involves continuous speech, specific knowledge needs to be introduced to the ASR system for adjustments in the acoustic parameters. Several researchers used articulatory features for better phoneme discrimination [80]. As discussed before, based on discriminative feature selection [81,82], the present study proposes a method to obtain the optimal selection of MFCCs. In addition, the optimal MLP parameters are computed to improve the accuracy of the speech recognition. For training the MLP back-propagation algorithm proposed by Rumelhart et al. in 1986 [83] is frequently employed in literature. In this algorithm some parameters involve the learning process including learning rate, momentum, back-propagation error, number of epochs and stopping criterion.

Normally, the stopping criterion is applied based on a predefined maximum epoch number or a predefined minimum back-propagation error rate. In the proposed neural network, in addition to the mentioned stopping criteria, a heuristic cost function is defined based on the error gradient:
                           
                              (21)
                              
                                 
                                    
                                       |
                                       
                                          Eg
                                       
                                       (
                                       n
                                       )
                                       −
                                       
                                          Eg
                                       
                                       (
                                       n
                                       −
                                       100
                                       )
                                       |
                                    
                                    
                                       |
                                       
                                          Eg
                                       
                                       (
                                       n
                                       −
                                       100
                                       )
                                       −
                                       
                                          Eg
                                       
                                       (
                                       n
                                       −
                                       200
                                       )
                                       |
                                    
                                 
                                 <
                                 
                                    
                                       th
                                    
                                    
                                       Eg
                                    
                                 
                                 
                                 or
                                 
                                 Epoches
                                 >
                                 
                                    
                                       th
                                    
                                    
                                       Ep
                                    
                                 
                                 
                                 or
                                 
                                 E
                                 (
                                 n
                                 )
                                 <
                                 
                                    
                                       th
                                    
                                    
                                       E
                                    
                                 
                              
                           
                         where 
                           
                              
                                 E
                              
                              
                                 g
                              
                           
                        , 
                           
                              
                                 th
                              
                              
                                 Eg
                              
                           
                        , 
                           
                              
                                 th
                              
                              
                                 Ep
                              
                           
                        , 
                           
                              
                                 th
                              
                              
                                 E
                              
                           
                        , 
                           E
                           (
                           n
                           )
                         denote the error gradient (
                           
                              Eg
                           
                           (
                           n
                           )
                           =
                           E
                           (
                           n
                           )
                           −
                           E
                           (
                           n
                           −
                           1
                           )
                        ), its threshold, the maximum epoch number, minimum error value for stopping the training process and back-propagation error in nth epoch, respectively.

A three-layer MLP is used for the speech recognition of the Malay vowels. Log-sigmoid is used as the activation function. The output layer has six output neurons corresponding to six Malay vowels. Fig. 4
                         presents the block diagram of the proposed speech recognition approach.

One of the most widely used statistical modeling methods for time series and speech sequences is HMM, which is commonly employed in ASR systems. An infinite number of possible sequences is properly modeled by HMM via defining some probability distributions.

According to the famous description as stated by Rabiner [84], an HMM consists of a number of states that correspond to situations in columns of a multiple alignment. Each state produces a symbol based on symbol-release probabilities. An interconnection between states exists based on state transition probabilities. Consequently, a sequence of states is obtainable by starting from an input state and transitioning to other states until arriving at an ending state. Entering each state, a symbol is produced based on the release probability distribution that results in a sequence of symbols.

In dealing with speech recognition problems, we need to train the HMMs. With the large amount of speech data, finding the optimal structure of HMM including appropriate parameters properly describing the current speech data is significant. In ASR systems, the Baum–Welch training algorithm is frequently employed because it is an extensively studied technique for speech recognition. The algorithm randomly initializes a set of HMM parameters and alternatively updates the parameters. This process continues until it reaches a local maximum of sample likelihood (for more details see [85] and [86]). After finding the HMM parameters, the Viterbi algorithm is commonly used for recognition of a particular speech sequence. Based on the observed event, the Viterbi finds the most probable state sequence [87]. Both algorithms are suboptimal because their processes result in finding the local optima. The HMM model used in this study consists of six states of left to right HMM with each state containing eight Gaussian mixtures. The covariance matrix in all the states was diagonal. Each speech sample was associated with an appropriate label, which included one of the six Malay vowels to be used for the training process.

@&#EXPERIMENTS@&#

In this section, several experiments have been conducted to answer the following questions:
                        
                           1.
                           How does the new number of filters (40 filters) employed in MFCC affect the children's speech recognition?

What is the effect of the proposed fuzzy feature selection on the recognition performance of the classifiers?

What is the performance of the proposed method in comparison to the other state-of-the-art feature selection methods?

How robust are the proposed features in terms of age and gender of the speakers?

What is the comparative performance of the speech recognition based on the proposed features and the conventional MFCCs?

In order to perform the experiments in the context of different classifiers four main steps have been followed including preparation of speech database, parameter adjustment for the feature selections and the classifiers, training of the classifiers and validation.

Three hundred sixty normal Malaysian children aged between 7 and 12 participated in this study. Each age group (grouped by calendar) consisted of 30 males and 30 females. All subjects were selected from primary schools in Malaysia. None of them had vocal pathology or voice disorder, symptoms of cold or flu, allergies, history of smoking, neurologic disease, or respiratory dysfunction. The subjects were asked to pronounce sustained Malay vowels of /a/, /e/, /ə/, /i/, /o/, and /u/ for 5 s each at a comfortable pitch and loudness level. The speech sounds were recorded using a Shure SM58 microphone in a regular room environment. The mouth-to-microphone distance was fixed at 2–3 cm. Gold-Wave digital audio editor software was used to record the speech sounds at a sampling rate of 20 kHz with 16-bit resolution. MATLAB codes were used to convert the speech samples into MFCC arrays.

Speech signals with longer frame length contain higher variations of acoustic parameters, known as intrinsic variation of speech [88–90], which can affect human and machine speech perception. Considering this effect, feature extraction can be performed with different processing frame length. In the feature selection method, optimal shape of the membership functions as well as optimal number of the selected features needs to be estimated. Moreover, the number of hidden neurons in MLP can have important effects on speech recognition.

Hence, parameter selection for feature selection method and classifiers is performed.

In this part of the experiment, parameter selection was performed to search for optimal frame length, number of selected features and the hidden neuron number of MLP. In order to provide the features for classification, some processes need to be accomplished including extraction of standard MFCC features (with 13 Mel filters), extraction of MFCC with 40 Mel filters, performing fuzzy-based feature selection and other feature selection methods for comparison. For every vowel class, some samples were randomly selected for the parameter selection, and the remaining samples were used for the performance evaluation based on the selected parameters.

The leave-one-out evaluation was adopted to find the best parameter for each of the feature extraction methods. From 2160 samples, 1956 samples were used for the parameter selection (326 samples from each vowel), and the remaining 204 samples were utilized for the evaluation of feature extraction methods with their optimal parameters. The MLP was trained with 1955 samples and 1 sample at one time was used to test the MLP. The training and testing of MLP were repeated 1956 times. The performance was averaged over the 1956 samples to obtain the maximum recognition rate for each feature extraction method.

Based on this method, for every specific frame length, number of selected features, shape of membership function in feature selection method and so on, an average performance is obtained. Consequently, optimal parameters can be found according to the computed performances.

Because most ASR approaches use common values for some parameters, such as the analysis frame length and the step (shift), this study specifies the parameter for obtaining the optimal performance to be the number of the features selected by the fuzzy method. The parameter selection for HMM is performed based on a three-fold cross validation method. In this method two thirds of the same database (1440 samples) was used to train the HMM, while the remaining one third of the database (720 samples) was used for the validation. This experiment was repeated three times based on three different training and test sets. The training set and the test set were not in common. The recognition rates obtained from the three test sets were averaged. Analysis frame length and the shift parameters were 25 ms and 10 ms, respectively.

After selecting the optimal parameters for the feature selection methods and the classifiers, the classifiers are trained based on selected features.

The MFCCs were fed into the MLP. Simultaneously, corresponding target vectors were given to the output neurons. The MLP was trained by using an error back propagation algorithm.

Once the optimal parameters were selected for each of the feature extraction methods, the MLP was trained with the selected feature and 1956 samples out of 2160 samples. For training the MLP, the learning rate and momentum were set at 0.01 and 0.9, respectively. The training of the MLP was terminated when the 
                              
                                 
                                    th
                                 
                                 
                                    Eg
                                 
                              
                           , 
                              
                                 
                                    th
                                 
                                 
                                    Ep
                                 
                              
                           , and 
                              
                                 
                                    th
                                 
                                 
                                    E
                                 
                              
                            were achieved at 0.2, 2000, and 0.01, respectively.

The selected features and appropriate labels were used for the HMM training. The HMM toolkit (HTK) [91] was used for making and manipulating the HMMs for this study. Hinit, HRest, and Hvite were employed for the initialization of the HMMs, the training of HMMs based on Baum–Welch method, and the recognition based on the Viterbi algorithm, respectively. The HMM model used in this study consists of six states of left to right HMM with each state containing eight Gaussian mixtures. The covariance matrix in all of the states was diagonal. The HTK provides a straightforward framework as well as parameter specifications for speech recognition, which are commonly used by the HMM-based ASR approaches in literature. Consequently, the conducted simulations in this study were based on the common framework.

As previously mentioned, training of MLP has been done with 1956 samples out of 2160 samples. Consequently, the 204 remaining samples were used for the validation. By applying the test samples to the MLP, the value of output neurons were computed. The neuron with highest value represents the winner class.

Based on three-fold cross validation method, after training the HMMs with two thirds of the samples, the test samples were fed to the HMMs for validation using Viterbi algorithm. It is notable that this experiment was repeated 3 times to include all of the samples for validation. However, in each experiment the training set and the test set were not in common.

@&#RESULTS AND DISCUSSIONS@&#

After repeating the leave-one-out experiment in the parameter selection for the different numbers of selected features (15 to 29, in a step of 1), different numbers of hidden units in each MLP (20 to 200, in a step of 20), and the different frame lengths of analyzed speech (10 to 75 ms, in a step of 5 ms), the best parameters for each method were obtained. Subsequently, each method was tested using the remaining 204 samples. The proposed method achieved a recognition rate of 93.14%. This recognition rate was measured based on 29 MFCCs, frame length of 15 ms, and 180 hidden neurons. The shape of membership used for the feature selection method was Gaussian. Other membership functions including triangular and trapezoidal functions obtained optimal recognition rates of 92.65% and 92.16%, respectively.


                        Table 1
                         presents the confusion matrix of the proposed method. Vowel /a/ was recognized with the highest accuracy, whereas, /e/ was recognized with the lowest accuracy. About three /u/ samples were mistaken as /o/.

The fuzzy-based selected features included the MFCC numbers ordered 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 16, 19, 21, 23, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39 and 40. Each of the MFCC orders had its own value of discriminative criterion. These MFCC orders were sorted from the maximum to the minimum value in terms of discriminative criterion: 4, 38, 40, 2, 3, 39, 6, 36, 37, 5, 21, 35, 7, 31, 11, 34, 9, 8, 16, 33, 32, 29, 13, 10, 25, 19, 12, 23, and 30.

Hence, we conclude the following: (1) discriminative features can improve the learning of MLP; (2) higher orders of MFCC could have equal discriminative ability compared to the lower orders, and (3) improvement of the recognition rate for children's speech requires a higher number of MFCC (higher number of Mel filters) and a narrower band for each of the filters. However, this discriminative ability is based on the available database, or in other words, this ability is dependent on the database.

The proposed fuzzy-based feature selection method was compared with other methods, such as sequential forward feature selection (SFFS) [41], sequential backward feature selection (SBFS), consistency-based search feature selection [58], Neighborhood based feature selection [92], standard Mel-frequency cepstral coefficients (MFCC), and method without feature selection.

The SFFS [41] methods start from an empty feature set and gradually add features chosen by some goodness estimate methods, whereas the SBFS [41] method starts with the complete set and removes features to find the optimal feature subset. Statistical toolbox of MATLAB 2010 was used to conduct SBFS and SFFS. Objective function used for the methods was the accuracy of Mahalanobis distance-based classifier. SFFS and SBFS methods were adapted to the extracted features and the selected features were used to train and test the classifiers. The results from these methods are summarized in Table 3, which shows that the proposed feature selection outperformed the SFFS and SBFS methods.

Based on the method proposed in [58], consistency measurement of the features is considered in evaluating the goodness of the feature subset. The extracted MFCCs were selected for the classification. The results presented in Table 3 show that the proposed feature selection outperformed the consistency-based feature selection method.

This method [92], originally proposed for support vector machine (SVM) classifiers, searches the samples positioned on the decision boundary of the classes in n-dimensional feature space. In most classification methods including MLP and HMM, classification of the samples on the boundary of different classes is a challenging task. Consequently, taking these samples and removing the samples far away from the decision boundary can help learning performance of the classifiers. This method firstly divides the space into 3 parts including positive region, boundary and noise. Subsequently, it partitions the features into four subsets including strongly relevant features, weakly relevant and indispensable features, weakly relevant and superfluous features, and irrelevant features. Then only the boundary samples in the relevant and indispensable features subspaces are selected for training of the classifiers. Based on this method, we performed feature selection for comparison to the proposed method. The parameters δ and β, employed to search the neighborhood and find the boundary decision, respectively, was 0.4 and 0.9. The results based on this method are presented in Table 3.

The MLP was trained without any feature selection. All the 40 MFCCs were fed into the MLP, and the MLP was trained by using different signal frame lengths and number of hidden neurons. The optimal recognition rate was achieved at 90.19% with 200 hidden neurons and signal length of 35 ms. Table 3 presents the results of the experiments.

Given that all the 40 MFCCs were fed into the MLP for training, the network size of MLP was greater, and the training of the MLP took longer. Some of the MFCCs with no discriminative information among the vowel classes involved training of the classifier. As a consequence, the performance of the classifier degraded.

Standard 39 MFCCs were fed into the MLP, which included 13 static cepstral coefficients, 13 delta, and 13 acceleration. The MLP was trained at different numbers of hidden neurons and different frame lengths. The optimal recognition rate using the standard MFCCs was achieved at 90.69% with 180 hidden neurons and signal length of 10 ms. The confusion matrix of MLP with standard MFCC is shown in Table 2
                           .

In the standard MFCCs, all 13 static cepstral coefficients were used. The discriminative ability of each of the 13 coefficients was unknown. There was no flexibility in reducing the number of MFCCs according to their discriminative ability. In fuzzy-based feature selection, the discriminative ability of every coefficient of 40 MFCCs was known, and thus, the number of MFCCs could be reduced by eliminating those coefficients with less discriminative ability. Hence, fuzzy-based feature selection selectively opted for MFCCs with the best discriminative ability to enhance the differences between the vowel classes.

The performance of the feature extraction methods is summarized in Table 3
                           . The results showed that MLP with fuzzy-based feature selection performed better than other feature selection methods, standard MFCC, and feature extraction without feature selection.

The delta and delta–delta coefficients are important in multiple frame based feature extraction, hence, the HMM classifier with fuzzy feature selection was performed not only on the static cepstral coefficient, but also on the delta and delta–delta coefficients. Consequently, the optimal number of features was selected from 120 coefficients including 40 static, 40 delta, and 40 delta–delta coefficients. After repeating the three-fold cross validation experiment in the parameter selection for different numbers of MFCCs, the best number of MFCCs for which the optimal result is achievable was obtained.

The proposed feature extraction method with HMM classifier achieved a recognition rate of 95.67%. This recognition rate was measured based on 40 MFCCs, which included 16 static cepstral coefficients, 15 delta coefficients and 9 delta–delta, and frame length of 25 ms and frame rate of 10 ms. The fuzzy-based selected features included the MFCC order sorted from the maximum to the minimum value in terms discriminative criterion as 4, 38, 40, 2, 3, 39, 6, 36, 37, 5, 21, 35, 7, 31, 11, 34, 43, 41, 42, 45, 75, 44, 60, 46, 50, 67, 65, 61, 62, 57, 82, 80, 81, 84, 83, 85, 93, 90, 94, and 98.


                        Table 4
                         presents the confusion matrix of the proposed method. Vowel /o/ was recognized with the highest accuracy. Note that this result has been obtained using the Gaussian membership function. Best recognition rates were provided by trapezoidal and triangular membership functions at 94.72% and 94.17%, respectively. A possible reason for these results is that the Gaussian membership function could effectively formulate uncertainties involved with the measurement of goodness of the features. In the multiple-frame feature extraction framework used for HMM based speech recognition, the accuracy is computed as follows:
                           
                              (22)
                              
                                 
                                    Accuracy
                                 
                                 (
                                 %
                                 )
                                 =
                                 
                                    
                                       D
                                       −
                                       I
                                    
                                    N
                                 
                                 ×
                                 100
                              
                           
                         where D, I, and N stand for the number of correctly detected phonemes, insertions, and total number of phonemes in the analyzing speech, respectively. According to the formula, HMM based speech recognition is more challenging because here, in contrast with single-frame feature extraction method used for MLP classifier, the insertion and deletion of phonemes are possible.

The HMMs were trained and tested with the MFCCs without using the feature selection method. Consequently, 120 MFCCs were used for this part of the experiment. This experiment achieved a recognition rate of 89.12%. The results are summarized in Table 7.

Similar to most ASR approaches based on HMM, standard 39 MFCCs were fed into the HMM, which included 13 static cepstral coefficients, 13 delta, and 13 acceleration. The analysis frame length of 25 ms and frame rate of 10 ms were used for the extraction of the standard features. The recognition rate using the standard MFCCs was achieved at 92.08%. The confusion matrix of this method is shown in Table 5
                           .

The performance of the feature extraction methods is summarized in Table 6
                           . The results showed that HMM with fuzzy-based feature selection performed better than other feature selection methods used in this study, the standard MFCC and feature extraction without feature selection.

Due to the age-dependent variational acoustical speech parameters, the recognition of children's speech is more challenging than that of adult speech. Dealing with effects in children's speech is a significant issue in developing methods for recognizing children's speech. Thus, robust feature representation in this context should be highly independent of the age of the speaker. A number of experiments were conducted to investigate the age dependency of the proposed feature representation. In each experiment, training and testing data were isolated according to the speaker's age. As a result, recognition performance indicates the ability of the proposed method to represent acoustic features of the speakers of different ages. Similar experiments were conducted based on standard MFCCs. Table 7
                         summarizes the experimental results. Table 7 shows that the highest recognition performance according to both methods is that for speakers aged nine. In all experiments except the fourth and fifth ones, the proposed features outperformed the standard MFCCs. Thus, the proposed features are more age-independent than standard MFCCs. In other words, these features are robust to the age of the speaker.

Similar to the age dependency analysis experiments, the robustness of the proposed method to the gender of the speakers was investigated. For this purpose, the training and testing data were isolated according to the gender of the speakers. Subsequently, the recognition performance of the classifiers based on the selected features and standard MFCC was compared. Table 8
                         summarizes the result of the experiments. Table 8 indicates that the proposed method outperformed standard MFCCs in terms of accuracy. In conclusion, the proposed feature representation is more robust to the gender of the speakers than the conventional MFCCs.

@&#CONCLUSION@&#

The study has proposed a fuzzy-based feature representation method to improve the speaker-independent vowel recognition uttered by Malay children. The selected features have to fulfill conflicting objectives including relevant, discriminative, and complementary features. Fuzzy formulation combines objectives that consider the uncertainties of the feature goodness. For the classification of the speeches, MLP and HMM were used. The proposed method was compared with other feature selection methods, standard MFCCs, and method without feature selection. In each of the methods, the optimal performance for the MLP based classifier was determined based on leave-one-out experiments in terms of the hidden neuron number of MLP, MFCC number, and processing frame length. For the HMM-based vowel recognition method, the three-fold cross validation method was used to determine the optimal parameter adjustment. The proposed fuzzy-based feature extraction performed better than standard MFCCs and other feature selection methods including consistency search, Neighborhood based feature selection, SFFS and SBFS methods. The fuzzy-based feature selection allowed the flexible selection of the MFCCs with the best discriminative ability to enhance the difference between the vowel classes. These effects made the expert system appropriate for children's speech recognition task.

@&#ACKNOWLEDGEMENTS@&#

The authors would like to thank the University of Malaya for funding this study under UMRG grant (RP016A-13AET).

@&#REFERENCES@&#

