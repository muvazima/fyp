@&#MAIN-TITLE@&#A generative restricted Boltzmann machine based method for high-dimensional motion data modeling

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Extended RBM to model spatio-temporal patterns among high-dimensional motion data.


                        
                        
                           
                           Generative approach to perform classification using RBM, for both binary and multi-class classification.


                        
                        
                           
                           High classification accuracy in two computer vision applications: facial expression recognition and human action recognition.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Restricted Boltzmann machine

Generative model

High-dimensional motion data

Facial expression recognition

Human action recognition

@&#ABSTRACT@&#


               
               
                  Many computer vision applications involve modeling complex spatio-temporal patterns in high-dimensional motion data. Recently, restricted Boltzmann machines (RBMs) have been widely used to capture and represent spatial patterns in a single image or temporal patterns in several time slices. To model global dynamics and local spatial interactions, we propose to theoretically extend the conventional RBMs by introducing another term in the energy function to explicitly model the local spatial interactions in the input data. A learning method is then proposed to perform efficient learning for the proposed model. We further introduce a new method for multi-class classification that can effectively estimate the infeasible partition functions of different RBMs such that RBM is treated as a generative model for classification purpose. The improved RBM model is evaluated on two computer vision applications: facial expression recognition and human action recognition. Experimental results on benchmark databases demonstrate the effectiveness of the proposed algorithm.
               
            

@&#INTRODUCTION@&#

Spatio-temporal patterns in high-dimensional motion data are crucial in many recognition applications. For example, human action is the combination of the body joint movements over a time interval. Facial expression is the result of the facial landmark movements (Fig. 1
                     ). Understanding the movement trajectories and modeling the underlying spatio-temporal patterns play an important role in recognizing these actions, especially with the recent emergences of reliable algorithms [1,2] to estimate the positions of body joints and facial landmarks.

In this work, we are interested in developing a probabilistic model to capture the spatio-temporal patterns in high-dimensional time series for classification purpose. Many recent works develop novel models to capture the spatio-temporal dynamics [3–6]. However, most models, such as hidden Markov model (HMM) [5], dynamic Bayesian network (DBN) [7] and conditional random field (CRF) [8] are time-slice local models, which assume Markov property and stationary transitions and hence can only capture local dynamics. The local models suffer from two limitations. First, local dynamics may not represent a sequence well because it fails to model the overall dynamics. Second, the stationary transition assumption may not hold for many real-world applications.

Compared with time-sliced dynamic models, restricted Boltzmann machines (RBMs) has shown strong capability of modeling joint distributions and therefore can capture the global patterns. In literature, RBMs have been successfully applied to separately capture the spatial [9] or temporal [10] patterns in different types of data. In this work, we propose a variant of RBM that can capture spatial and global temporal patterns simultaneously to comprehensively model the high-dimensional motion data.

In a typical RBM, since there are no lateral connections among nodes in each layer, input data are independent of each other given the states of hidden layer. This assumption limits RBM’s representation power, since there exist direct dependencies among input data. There are generally two types of data interactions: interactions through latent variable and direct interactions independent of latent variable. For example, as stated in [11], soldiers on a parade follow the commander’s order to march in some direction, but they “form a neat rectangle by interacting with their neighbors”. This example illustrates that soldiers’ behavior are determined by both the commander’s order (latent) and the interactions with their neighbors. In this case, RBM is not effective in modeling the local interaction that is independent of the latent variable. The use of RBM for data representation and classification is further hindered by the difficulty in comparing different RBMs due to the intractable computation of the partition functions.

Allowing interaction among visible units can overcome this shortcoming. We introduce restricted Boltzmann machine with local interactions (LRBM) to capture both the global temporal patterns and local spatial interactions in the input data. Specifically, we add a new pairwise potential term in the learning objective function of RBM to capture the local spatial interactions among components of an input vector. To perform efficient learning for the model, we replace the reconstruction procedure in the conventional Contrastive Divergence [13] algorithm with a mean field approach.

For classification task, RBM is typically used for learning features, as the input to a second stage classifier (e.g., SVM [14]). Typically one model is trained for all classes. To obtain good features, deep structure is built and back propagation is employed to carefully tune the parameters. In this work, we use RBM as a generative model to capture the spatio-temporal patterns in the data. RBM is used for data representation instead of feature learning. Given an observation, the only output we get from an RBM model is the likelihood of the model. For the recognition purpose, one model is trained for one class of input data. To compare among different models, we employ a method to estimate the relative partition function of a pair of RBMs for binary classification, and a label ranking method is used to extend the binary classification to multi-class classification.

To evaluate the performance of LRBM, we apply it to two areas related to complex spatial and temporal patterns: facial expression recognition and human action recognition. Experimental results on benchmark databases demonstrate the effectiveness of the proposed model.

The rest of the paper is structured as follows. Section 2 presents an overview of the related work. Section 3 introduces the LRBM to model the global temporal dynamics and spatial patterns of motion data, as well as the classification method. We will then give the experimental results in Section 4. The paper is concluded in Section 5.

@&#RELATED WORK@&#

Capturing and representing spatio-temporal structure in data is important for many recognition and classification tasks. Research for capturing such patterns can be categorized into feature-based and model-based methods. The most widely used spatio-temporal features include spatio-temporal interest point (STIP) based features [3,4] and optical flow based features [15]. These features capture local appearance or motion patterns near the interest points or optical flows. Although having been successfully applied to many applications, these features generally focus more on local patterns.

Model-based methods include probabilistic graphical models such as hidden Markov models [5], dynamic Bayesian networks [16], conditional random fields [17], and their variants. While capable of simultaneously capturing both spatial and temporal interactions, they can only capture the local spatial and temporal interactions due to the underlying Markov assumption.

Restricted Boltzmann machines (RBMs) have been separately used for modeling spatial correlation or temporal correlation in the data in the last decade. RBM was firstly introduced to learn deep features from handwritings to recognize digits [18]. In [9], Eslami et al. propose a Deep Belief Network to model the shapes of horses and motorbikes. The samples from the model look realistic and have a good generalization. A more complicated model, proposed by Nair and Hinton [19], considers the spatial correlation among visible layer using a factored 3-way RBM, in which triple potentials are used to model the correlations among pixels in natural images. The intuition is that in natural images, the intensity of each pixel is approximately the average of its neighbors. Wu et al. [20] apply the 3-way RBM to facial landmark tracking, and model the relationship between posed faces and frontal faces, under varying facial expressions.

For dynamic data modeling, Taylor et al. [21] use a Conditional RBM (CRBM) to model the temporal transitions in human body movements, and reconstruct body movements. Nevertheless, like HMM, CRBM still models local dynamics by assuming n’th order Markov property.

The idea of using RBM to model global pattern is not new. In [22,23], RBM and CRF are combined for face labeling problem in a single image or video sequences. Compared with these works, our work is different for several reasons. First, our goal is to use RBM for data representation for multi-class classification, while the goal of [22,23] is MAP inference, which is to recover the label for each superpixel. Second, we extend the conventional RBM to capture the local shape and global temporal patterns in a unified model, while in [22,23] RBM is built on top of the hidden layer of the CRF, as the prior for the labels. Third, our method learns all the parameters simultaneously, while their method performs learning separately.

RBM and its variants have also been used for modeling motion data. For example, Sutskever and Hinton [24] introduce a temporal RBM to model high-dimensional sequences. Wang et al. [10] use RBM to capture the global dynamics of finger trace. However, it is limited to model the global dynamics for 1-D data only.

To improve the representation power of RBM, semi-restricted Boltzmann machine (SRBM) [11] is introduced to model the lateral interactions between visible variables. The main property of SRBM is that given the hidden variables, the visible layer forms a Markov random field. However, for high-dimensional motion data, there will be too many parameters if every pair of visible units has an interaction. In this work, we model the dynamic nature of data with fewer parameters than a SRBM, which makes the learning process more efficient.

Besides feature extraction and shape modeling, RBMs have also been used for classification. Larochelle and Bengio [25] introduce a discriminative RBM as a classifier by including the labels in visible layer, and make predictions by comparing the likelihood of each label vector. In [26] discriminate RBM is introduced to model vector inputs by duplicating the discriminative RBM and adding constraints on the hidden layer. In all RBM related models discussed above, one RBM is trained for all classes. For this work, in contrast, we build one RBM for each class, and perform a multi-class classification task.

In this research, we propose to extend the standard RBM to simultaneously capture the spatial and global temporal patterns in high-dimensional sequential data and employ such RBM model to different classification tasks in computer vision.

In this section, we will firstly give a brief introduction of restricted Boltzmann machine, and then introduce the proposed RBM with Local Interaction (LRBM) to model multi-dimensional motion data and its learning method. Finally we present the method to use LRBMs as a set of pairwise classifier to perform multi-class recognition.

A standard RBM is a generative model with two densely connected layers, one visible layer to represent data and one latent layer to extract stochastic binary features from data (Fig. 2
                        ). Hidden units are connected to visible nodes using symmetrically weighted connections to model their joint distribution. In our work, we use Gaussian-Binary RBM, where the hidden units are binary and the visible variables are assumed to follow normal distribution.

The energy function 
                           
                              E
                              (
                              v
                              ,
                              h
                              )
                           
                         for each pair 
                           
                              (
                              v
                              ,
                              h
                              )
                           
                         is parameterized in Eq. (1).
                           
                              (1)
                              
                                 E
                                 (
                                 v
                                 ,
                                 h
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         v
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   -
                                                   
                                                      
                                                         a
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             2
                                          
                                       
                                    
                                    
                                       2
                                       
                                          
                                             σ
                                          
                                          
                                             i
                                          
                                          
                                             2
                                          
                                       
                                    
                                 
                                 -
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          ,
                                          j
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             v
                                          
                                          
                                             i
                                          
                                       
                                    
                                    
                                       
                                          
                                             σ
                                          
                                          
                                             j
                                          
                                       
                                    
                                 
                                 
                                    
                                       w
                                    
                                    
                                       ij
                                    
                                 
                                 
                                    
                                       h
                                    
                                    
                                       j
                                    
                                 
                                 -
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          j
                                       
                                    
                                 
                                 
                                    
                                       b
                                    
                                    
                                       j
                                    
                                 
                                 
                                    
                                       h
                                    
                                    
                                       j
                                    
                                 
                                 ,
                              
                           
                        where 
                           
                              
                                 
                                    a
                                 
                                 
                                    i
                                 
                              
                           
                         is the bias for visible unit 
                           
                              
                                 
                                    v
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    σ
                                 
                                 
                                    i
                                 
                              
                           
                         is the standard deviation of the Gaussian distribution, which is typically 1 if we normalize the data, 
                           
                              
                                 
                                    b
                                 
                                 
                                    j
                                 
                              
                           
                         is the bias for the hidden unit 
                           
                              
                                 
                                    h
                                 
                                 
                                    j
                                 
                              
                              ,
                              
                                 
                                    w
                                 
                                 
                                    ij
                                 
                              
                           
                         is the weight of the link connecting 
                           
                              
                                 
                                    v
                                 
                                 
                                    i
                                 
                              
                           
                         and 
                           
                              
                                 
                                    h
                                 
                                 
                                    j
                                 
                              
                           
                        .

For every possible pair of visible and hidden vector, the network assigns a probability:
                           
                              (2)
                              
                                 p
                                 (
                                 v
                                 ,
                                 h
                                 )
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       Z
                                    
                                 
                                 exp
                                 (
                                 -
                                 E
                                 (
                                 v
                                 ,
                                 h
                                 )
                                 )
                                 ,
                              
                           
                        where Z is the partition function. With continuous inputs, the Z is calculated by integrating over all visible nodes and summing over all hidden unit configurations.

Using standard RBM to model high-dimensional motion data has its limitations. First, the interaction among the data is represented through latent variables, which can be easily represented in direct connections. Second, if a single RBM models the whole sequence, the spatial information in a time slice is treated the same as the temporal information of one dimension. If one RBM models only one time slice (as in [21]), the temporal information remains local.

In this work, we propose the restricted Boltzmann machine with local interaction (LRBM, Fig. 3
                        ) to overcome such limitations. For 
                           
                              d
                              ×
                              
                                 
                                    n
                                 
                                 
                                    t
                                 
                              
                           
                         sequential data 
                           
                              V
                              =
                              [
                              
                                 
                                    v
                                 
                                 
                                    1
                                 
                              
                              ,
                              
                                 
                                    v
                                 
                                 
                                    2
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    v
                                 
                                 
                                    
                                       
                                          n
                                       
                                       
                                          t
                                       
                                    
                                 
                              
                              ]
                           
                        , by allowing local interaction, we have the following energy function,
                           
                              (3)
                              
                                 E
                                 (
                                 V
                                 ,
                                 h
                                 )
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       2
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          
                                             
                                                n
                                             
                                             
                                                t
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       (
                                       
                                          
                                             v
                                          
                                          
                                             i
                                          
                                       
                                       -
                                       
                                          
                                             a
                                          
                                          
                                             i
                                          
                                       
                                       )
                                    
                                    
                                       T
                                    
                                 
                                 (
                                 
                                    
                                       v
                                    
                                    
                                       i
                                    
                                 
                                 -
                                 
                                    
                                       a
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 -
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          j
                                          =
                                          1
                                       
                                       
                                          
                                             
                                                n
                                             
                                             
                                                h
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       b
                                    
                                    
                                       j
                                    
                                 
                                 
                                    
                                       h
                                    
                                    
                                       j
                                    
                                 
                                 -
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          
                                             
                                                n
                                             
                                             
                                                t
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          j
                                          =
                                          1
                                       
                                       
                                          
                                             
                                                n
                                             
                                             
                                                h
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       v
                                    
                                    
                                       i
                                    
                                    
                                       T
                                    
                                 
                                 
                                    
                                       w
                                    
                                    
                                       ij
                                       ·
                                    
                                 
                                 
                                    
                                       h
                                    
                                    
                                       j
                                    
                                 
                                 -
                                 
                                    
                                       1
                                    
                                    
                                       2
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          
                                             
                                                n
                                             
                                             
                                                t
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       v
                                    
                                    
                                       i
                                    
                                    
                                       T
                                    
                                 
                                 
                                    
                                       Uv
                                    
                                    
                                       i
                                    
                                 
                                 ,
                              
                           
                        where 
                           
                              
                                 
                                    v
                                 
                                 
                                    i
                                 
                              
                           
                         is a d-dimension vector representing input vector at time slice 
                           
                              i
                              ,
                              w
                           
                         has the dimension of 
                           
                              
                                 
                                    n
                                 
                                 
                                    t
                                 
                              
                              ×
                              
                                 
                                    n
                                 
                                 
                                    h
                                 
                              
                              ×
                              d
                              ,
                              
                                 
                                    w
                                 
                                 
                                    ij
                                    ·
                                 
                              
                           
                         is the weight vector connecting 
                           
                              
                                 
                                    v
                                 
                                 
                                    i
                                 
                              
                           
                         to a hidden node 
                           
                              
                                 
                                    h
                                 
                                 
                                    j
                                 
                              
                           
                        . 
                           
                              
                                 
                                    a
                                 
                                 
                                    i
                                 
                              
                           
                         and 
                           
                              
                                 
                                    b
                                 
                                 
                                    j
                                 
                              
                           
                         have the same meaning as in Eq. (1). 
                           
                              U
                           
                         is a 
                           
                              d
                              ×
                              d
                           
                         symmetric matrix with zeros on diagonal, modeling the correlation of each vector 
                           
                              
                                 
                                    v
                                 
                                 
                                    i
                                 
                              
                           
                        .

The proposed energy function models two kinds of data interactions: interaction through latent variables and interaction directly among data. In high-dimensional motion data, components in a single time slice (spatial information) is better to be considered differently from components along the timeline (temporal information). Interactions between input data and hidden variables are through weight 
                           
                              w
                           
                        , which models global patterns, because every visible unit is connected to every latent unit. Matrix 
                           
                              U
                           
                         models direct interactions among input data, as in Fig. 3, representing local spatial patterns. This kind of interaction directly affects visible layer without going through latent layer, so it is more effective to model some local spatial patterns. The elements in 
                           
                              U
                           
                         are pairwise potentials of features in one time slice. Different shapes (spatial pattern) of input will have different contributions to the energy function. Thus, 
                           
                              U
                           
                         captures spatial patterns among elements of input vector. With temporal information captured via the hidden nodes, our work can capture global spatio-temporal dynamics.

We assume the spatial relationship are constant throughout the whole sequence, so the parameters in 
                           
                              U
                           
                         are shared in different frames, which means 
                           
                              U
                           
                         is invariant of i. Under this invariance assumption of 
                           
                              U
                           
                        , the number of parameters is significantly reduced.

From the energy function (Eq. (3)) and likelihood function (Eq. (2)), we can derive the probability of an hidden unit to be activated, given an visible layer:
                           
                              (4)
                              
                                 p
                                 (
                                 
                                    
                                       h
                                    
                                    
                                       j
                                    
                                 
                                 =
                                 1
                                 |
                                 V
                                 )
                                 =
                                 sigmoid
                                 
                                    
                                       
                                          
                                             
                                                b
                                             
                                             
                                                j
                                             
                                          
                                          +
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   i
                                                
                                             
                                          
                                          
                                             
                                                v
                                             
                                             
                                                i
                                             
                                             
                                                T
                                             
                                          
                                          
                                             
                                                w
                                             
                                             
                                                ij
                                                ·
                                             
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

As one hidden unit is connected to all visible units, whether it is activated or not depends stochastically on the visible layer. A hidden unit 
                           
                              
                                 
                                    h
                                 
                                 
                                    j
                                 
                              
                           
                         is activated through Eq. (4) when it detects some specific pattern in the visible layer. The pattern is captured by the weights connecting each element in 
                           
                              V
                           
                         to 
                           
                              
                                 
                                    h
                                 
                                 
                                    j
                                 
                              
                           
                        . Therefore the hidden layer 
                           
                              h
                           
                         is able to capture important global patterns of 
                           
                              V
                           
                        . Hence, the proposed model can simultaneously capture the global temporal patterns (through 
                           
                              w
                           
                        ) and local shape patterns (through 
                           
                              U
                           
                        ).

LRBM is similar to the factored 3-way RBM [19] and SRBM [11] with respect to modeling the interactions in the visible layer. However, there exist some significant differences. First, in factored 3-way RBM, every pair of visible units has a potential to represent the interaction, while in LRBM, units only interact with neighbors in the time slice. Second, if the number of factors in 3-way RBM or the data dimension in SRBM is high, the overall parameters are much more than LRBM. In LRBM, the additional parameters are from matrix 
                           
                              U
                           
                        , with the 
                           
                              d
                              (
                              d
                              -
                              1
                              )
                              /
                              2
                           
                         parameters, where d is the dimension of the vector 
                           
                              
                                 
                                    v
                                 
                                 
                                    i
                                 
                              
                           
                        . This would greatly reduce the computational load. Finally, both 3-way RBM and semi-RBM are proposed to model the spatial patterns of a single natural image, while LRBM models spatio-temporal patterns of sequential data.

The joint probability of 
                           
                              V
                           
                         and 
                           
                              h
                           
                         are the same as in Eq. (2). The likelihood of input data is computed by summing over all configurations of hidden units:
                           
                              (5)
                              
                                 p
                                 (
                                 V
                                 )
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       Z
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          h
                                       
                                    
                                 
                                 
                                    
                                       e
                                    
                                    
                                       -
                                       E
                                       (
                                       V
                                       ,
                                       h
                                       )
                                    
                                 
                                 .
                              
                           
                        
                     

In our work, one LRBM is trained for one class. The parameters include bias for visible and hidden units 
                           
                              
                                 
                                    a
                                 
                                 
                                    i
                                 
                              
                           
                         and 
                           
                              
                                 
                                    b
                                 
                                 
                                    j
                                 
                              
                           
                        , weight between two layers 
                           
                              w
                           
                        , and local interaction matrix 
                           
                              U
                           
                        . To learn the parameters, we seek to maximize the joint probability of all training data D of a class, 
                           
                              P
                              (
                              D
                              )
                              =
                              
                                 
                                    ∏
                                 
                                 
                                    V
                                    ∈
                                    D
                                 
                              
                              p
                              (
                              V
                              )
                           
                        . Assume the data has been normalized, so 
                           
                              
                                 
                                    a
                                 
                                 
                                    i
                                 
                              
                           
                         can be removed from the energy function. The derivative of the log-likelihood of a training instance with respect to a parameter is given below:
                           
                              (6)
                              
                                 
                                    
                                       ∂
                                       log
                                       p
                                       (
                                       V
                                       )
                                    
                                    
                                       ∂
                                       
                                          
                                             w
                                          
                                          
                                             ij
                                          
                                       
                                    
                                 
                                 =
                                 〈
                                 
                                    
                                       v
                                    
                                    
                                       i
                                    
                                 
                                 
                                    
                                       h
                                    
                                    
                                       j
                                    
                                 
                                 
                                    
                                       〉
                                    
                                    
                                       data
                                    
                                 
                                 -
                                 〈
                                 
                                    
                                       v
                                    
                                    
                                       i
                                    
                                 
                                 
                                    
                                       h
                                    
                                    
                                       j
                                    
                                 
                                 
                                    
                                       〉
                                    
                                    
                                       model
                                    
                                 
                                 ,
                              
                           
                        
                        
                           
                              (7)
                              
                                 
                                    
                                       ∂
                                       log
                                       p
                                       (
                                       V
                                       )
                                    
                                    
                                       ∂
                                       
                                          
                                             b
                                          
                                          
                                             j
                                          
                                       
                                    
                                 
                                 =
                                 〈
                                 
                                    
                                       h
                                    
                                    
                                       j
                                    
                                 
                                 
                                    
                                       〉
                                    
                                    
                                       data
                                    
                                 
                                 -
                                 〈
                                 
                                    
                                       h
                                    
                                    
                                       j
                                    
                                 
                                 
                                    
                                       〉
                                    
                                    
                                       model
                                    
                                 
                                 ,
                              
                           
                        
                        
                           
                              (8)
                              
                                 
                                    
                                       ∂
                                       log
                                       p
                                       (
                                       V
                                       )
                                    
                                    
                                       ∂
                                       
                                          
                                             u
                                          
                                          
                                             rs
                                          
                                       
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      i
                                                   
                                                   
                                                      
                                                         
                                                            n
                                                         
                                                         
                                                            t
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   v
                                                
                                                
                                                   i
                                                
                                                
                                                   (
                                                   r
                                                   )
                                                
                                             
                                             
                                                
                                                   v
                                                
                                                
                                                   i
                                                
                                                
                                                   (
                                                   s
                                                   )
                                                
                                             
                                          
                                       
                                    
                                    
                                       data
                                    
                                 
                                 -
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      i
                                                   
                                                   
                                                      
                                                         
                                                            n
                                                         
                                                         
                                                            t
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   v
                                                
                                                
                                                   i
                                                
                                                
                                                   (
                                                   r
                                                   )
                                                
                                             
                                             
                                                
                                                   v
                                                
                                                
                                                   i
                                                
                                                
                                                   (
                                                   s
                                                   )
                                                
                                             
                                          
                                       
                                    
                                    
                                       model
                                    
                                 
                                 ,
                              
                           
                        where the angle brackets are used to denote expectations under the distribution specified by the subscript that follows. 
                           
                              
                                 
                                    u
                                 
                                 
                                    rs
                                 
                              
                           
                         is the element of 
                           
                              U
                           
                         at position 
                           
                              (
                              r
                              ,
                              s
                              )
                           
                        . 
                           
                              
                                 
                                    v
                                 
                                 
                                    i
                                 
                                 
                                    (
                                    r
                                    )
                                 
                              
                              ,
                              
                                 
                                    v
                                 
                                 
                                    i
                                 
                                 
                                    (
                                    s
                                    )
                                 
                              
                           
                         are the rth and sth components of vector 
                           
                              
                                 
                                    v
                                 
                                 
                                    i
                                 
                              
                           
                        . This leads to a simple learning rule: stochastic steepest ascent in the log-likelihood of training data. Take 
                           
                              
                                 
                                    w
                                 
                                 
                                    ij
                                 
                              
                           
                         as an example:
                           
                              (9)
                              
                                 Δ
                                 
                                    
                                       w
                                    
                                    
                                       ij
                                    
                                 
                                 =
                                 ∊
                                 (
                                 〈
                                 
                                    
                                       v
                                    
                                    
                                       i
                                    
                                 
                                 
                                    
                                       h
                                    
                                    
                                       j
                                    
                                 
                                 
                                    
                                       〉
                                    
                                    
                                       data
                                    
                                 
                                 -
                                 〈
                                 
                                    
                                       v
                                    
                                    
                                       i
                                    
                                 
                                 
                                    
                                       h
                                    
                                    
                                       j
                                    
                                 
                                 
                                    
                                       〉
                                    
                                    
                                       model
                                    
                                 
                                 )
                                 ,
                              
                           
                        where ∊ is the learning rate. 
                           
                              〈
                              
                                 
                                    v
                                 
                                 
                                    i
                                 
                              
                              
                                 
                                    h
                                 
                                 
                                    j
                                 
                              
                              
                                 
                                    〉
                                 
                                 
                                    data
                                 
                              
                           
                         is easy to get from the data. 
                           
                              〈
                              
                                 
                                    v
                                 
                                 
                                    i
                                 
                              
                              
                                 
                                    h
                                 
                                 
                                    j
                                 
                              
                              
                                 
                                    〉
                                 
                                 
                                    model
                                 
                              
                           
                         is much more difficult to compute due to the large dimension of hidden and visible layers. Hinton [27] proposes the CD algorithm to approximate 
                           
                              〈
                              
                                 
                                    v
                                 
                                 
                                    i
                                 
                              
                              
                                 
                                    h
                                 
                                 
                                    j
                                 
                              
                              
                                 
                                    〉
                                 
                                 
                                    model
                                 
                              
                           
                         by using a reconstructed sample, which gives the following weight change:
                           
                              (10)
                              
                                 Δ
                                 
                                    
                                       w
                                    
                                    
                                       ij
                                    
                                 
                                 =
                                 ∊
                                 (
                                 〈
                                 
                                    
                                       v
                                    
                                    
                                       i
                                    
                                 
                                 
                                    
                                       h
                                    
                                    
                                       j
                                    
                                 
                                 
                                    
                                       〉
                                    
                                    
                                       data
                                    
                                 
                                 -
                                 〈
                                 
                                    
                                       v
                                    
                                    
                                       i
                                    
                                 
                                 
                                    
                                       h
                                    
                                    
                                       j
                                    
                                 
                                 
                                    
                                       〉
                                    
                                    
                                       recon
                                    
                                 
                                 )
                                 .
                              
                           
                        
                     

Given the visible layer, the hidden units are independent with each other, so sampling the states of hidden units can be performed in parallel using Eq. (4).

Given the states of the hidden units, the visible units form a Markov random field in which the pairwise interaction weight between 
                           
                              
                                 
                                    v
                                 
                                 
                                    i
                                 
                                 
                                    (
                                    r
                                    )
                                 
                              
                           
                         and 
                           
                              
                                 
                                    v
                                 
                                 
                                    i
                                 
                                 
                                    (
                                    s
                                    )
                                 
                              
                           
                         is 
                           
                              
                                 
                                    u
                                 
                                 
                                    rs
                                 
                              
                           
                        . They are no longer independent, so sampling cannot be done in parallel. However, we can obtain the conditional probability of each node by fixing its neighbors. The mean field algorithm is used to sample data from the model, and to reconstruct visible layer when learning the parameters of LRBM. Specifically, for each vector 
                           
                              
                                 
                                    v
                                 
                                 
                                    i
                                 
                              
                           
                        , each component 
                           
                              
                                 
                                    v
                                 
                                 
                                    i
                                 
                                 
                                    (
                                    s
                                    )
                                 
                              
                           
                         is sampled by fixing all its neighbors. Once a component is sampled, the vector is updated with the newly sampled component. This procedure is repeated until all components have been updated, and thus we get a reconstructed vector. The conditional probability of one visible node is given in Eq. (11):
                           
                              (11)
                              
                                 p
                                 (
                                 
                                    
                                       v
                                    
                                    
                                       i
                                    
                                    
                                       (
                                       s
                                       )
                                    
                                 
                                 |
                                 N
                                 (
                                 
                                    
                                       v
                                    
                                    
                                       i
                                    
                                    
                                       (
                                       s
                                       )
                                    
                                 
                                 )
                                 ,
                                 h
                                 )
                                 =
                                 N
                                 
                                    
                                       
                                          
                                             
                                                a
                                             
                                             
                                                i
                                             
                                             
                                                (
                                                s
                                                )
                                             
                                          
                                          +
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   j
                                                
                                             
                                          
                                          
                                             
                                                h
                                             
                                             
                                                j
                                             
                                          
                                          
                                             
                                                w
                                             
                                             
                                                ij
                                                ·
                                             
                                             
                                                (
                                                s
                                                )
                                             
                                          
                                          +
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   
                                                      
                                                         v
                                                      
                                                      
                                                         k
                                                      
                                                   
                                                   ∈
                                                   N
                                                   (
                                                   
                                                      
                                                         v
                                                      
                                                      
                                                         i
                                                      
                                                      
                                                         (
                                                         s
                                                         )
                                                      
                                                   
                                                   )
                                                
                                             
                                          
                                          
                                             
                                                v
                                             
                                             
                                                k
                                             
                                          
                                          
                                             
                                                u
                                             
                                             
                                                ks
                                             
                                          
                                          ,
                                          1
                                       
                                    
                                 
                                 ,
                              
                           
                        where 
                           
                              N
                              (
                              
                                 
                                    v
                                 
                                 
                                    i
                                 
                                 
                                    (
                                    s
                                    )
                                 
                              
                              )
                           
                         are the neighbors of 
                           
                              
                                 
                                    v
                                 
                                 
                                    i
                                 
                                 
                                    (
                                    s
                                    )
                                 
                              
                              ,
                              N
                              (
                              μ
                              ,
                              
                                 
                                    σ
                                 
                                 
                                    2
                                 
                              
                              )
                           
                         denotes the Gaussian probability density function with mean μ and variance 
                           
                              
                                 
                                    σ
                                 
                                 
                                    2
                                 
                              
                           
                        . The superscript (s) means the sth component of a vector.

Compared with distribution of visible units in standard RBM:
                           
                              (12)
                              
                                 p
                                 (
                                 
                                    
                                       v
                                    
                                    
                                       i
                                    
                                 
                                 |
                                 h
                                 )
                                 =
                                 N
                                 
                                    
                                       
                                          
                                             
                                                a
                                             
                                             
                                                i
                                             
                                          
                                          +
                                          
                                             
                                                m
                                             
                                             
                                                j
                                             
                                          
                                          
                                             
                                                h
                                             
                                             
                                                j
                                             
                                          
                                          
                                             
                                                w
                                             
                                             
                                                ij
                                             
                                          
                                          ,
                                          1
                                       
                                    
                                 
                                 ,
                              
                           
                        the mean of the distribution of a visible node is similar in the first two terms, except that in LRBM, it is modified by the pairwise potentials relating one node to all its neighbors.

With the reconstructed data, we are able to calculate the approximate gradient of each parameter, and perform the CD learning procedure. As mentioned in [28], the RBMs learn better if more steps of reconstruction are used before collecting the statistics. In practice, we sample 10 times using the mean field method before collecting the reconstructed data in each epoch.

Due to the non-convex property of the objective function, different parameter initializations in RBM learning can end up with different models. For each class, several candidate models are learned from different initializations, and we select the one that can best discriminate current class from the others.

For example, after learning model 
                           
                              
                                 
                                    M
                                 
                                 
                                    1
                                 
                              
                           
                         for class 
                           
                              
                                 
                                    C
                                 
                                 
                                    1
                                 
                              
                           
                        , given instances from all classes 
                           
                              {
                              
                                 
                                    I
                                 
                                 
                                    1
                                 
                              
                              ,
                              
                                 
                                    I
                                 
                                 
                                    2
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    I
                                 
                                 
                                    N
                                 
                              
                              }
                           
                        , we expect instance 
                           
                              
                                 
                                    I
                                 
                                 
                                    1
                                 
                              
                           
                         to have greater likelihood on model 
                           
                              
                                 
                                    M
                                 
                                 
                                    1
                                 
                              
                           
                         than all other instances (
                           
                              {
                              
                                 
                                    I
                                 
                                 
                                    2
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    I
                                 
                                 
                                    N
                                 
                              
                              }
                           
                        ). In practice, we can sort the likelihoods of instances from all classes, and choose the candidate model that minimizes the rank of the instances from class 
                           
                              
                                 
                                    C
                                 
                                 
                                    1
                                 
                              
                           
                        .

Notice that the likelihood is calculated according to Eq. (5), given a single LRBM model, so for the comparison of the likelihoods, we do not need to estimate the intractable partition function, since it is merely a constant.

For this work, RBM is used as a generative model for classification. A common strategy for training generative model for classification is to train multiple models for different classes and then evaluate the likelihood of each model during testing. In particular, given a set of properly learned LRBM’s 
                           
                              
                                 
                                    {
                                    
                                       
                                          M
                                       
                                       
                                          i
                                       
                                    
                                    }
                                 
                                 
                                    i
                                    =
                                    1
                                 
                                 
                                    N
                                 
                              
                           
                         for N classes, the basic idea for classification is to find the model that generates the largest likelihood given an instance of data 
                           
                              V
                           
                        .
                           
                              (13)
                              
                                 
                                    
                                       i
                                    
                                    
                                       ∗
                                    
                                 
                                 =
                                 arg
                                 
                                    
                                       
                                          max
                                       
                                       
                                          i
                                       
                                    
                                 
                                 p
                                 (
                                 V
                                 |
                                 
                                    
                                       M
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 ,
                              
                           
                        where 
                           
                              
                                 
                                    i
                                 
                                 
                                    ∗
                                 
                              
                           
                         is the prediction of our classifier. The likelihood of a data instance is
                           
                              (14)
                              
                                 log
                                 p
                                 (
                                 V
                                 )
                                 =
                                 -
                                 
                                    
                                       1
                                    
                                    
                                       2
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          
                                             
                                                n
                                             
                                             
                                                t
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       (
                                       
                                          
                                             v
                                          
                                          
                                             i
                                          
                                       
                                       -
                                       
                                          
                                             a
                                          
                                          
                                             i
                                          
                                       
                                       )
                                    
                                    
                                       T
                                    
                                 
                                 (
                                 
                                    
                                       v
                                    
                                    
                                       i
                                    
                                 
                                 -
                                 
                                    
                                       a
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 +
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          
                                             
                                                n
                                             
                                             
                                                t
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       v
                                    
                                    
                                       i
                                    
                                    
                                       T
                                    
                                 
                                 
                                    
                                       Uv
                                    
                                    
                                       i
                                    
                                 
                                 +
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          j
                                          =
                                          1
                                       
                                       
                                          
                                             
                                                n
                                             
                                             
                                                h
                                             
                                          
                                       
                                    
                                 
                                 log
                                 
                                    
                                       
                                          1
                                          +
                                          exp
                                          
                                             
                                                
                                                   
                                                      
                                                         b
                                                      
                                                      
                                                         j
                                                      
                                                   
                                                   +
                                                   
                                                      
                                                         
                                                            ∑
                                                         
                                                         
                                                            i
                                                            =
                                                            1
                                                         
                                                         
                                                            
                                                               
                                                                  n
                                                               
                                                               
                                                                  t
                                                               
                                                            
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         v
                                                      
                                                      
                                                         i
                                                      
                                                      
                                                         T
                                                      
                                                   
                                                   
                                                      
                                                         w
                                                      
                                                      
                                                         ij
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                                 -
                                 log
                                 
                                 Z
                                 .
                              
                           
                        
                     

For brevity, we denote 
                           
                              p
                              (
                              V
                              )
                           
                         as
                           
                              (15)
                              
                                 p
                                 (
                                 V
                                 )
                                 =
                                 g
                                 (
                                 V
                                 )
                                 -
                                 log
                                 
                                 Z
                                 .
                              
                           
                        
                     


                        
                           
                              g
                              (
                              V
                              )
                           
                         can be computed directly. However the partition functions Z are intractable for RBMs with large numbers of hidden and visible units. One possible solution is the Annealed Importance Sampling [29] to estimate the partition functions, and directly compare the likelihood. However, such estimation needs too many samples to obtain a good estimation of the partition function. Instead, for binary classification, Schmah et al. [30] propose a method to discriminatively estimate the difference of log-partition functions of two RBMs:
                           
                              (16)
                              
                                 
                                    
                                       c
                                    
                                    
                                       ij
                                    
                                 
                                 =
                                 log
                                 
                                 
                                    
                                       Z
                                    
                                    
                                       i
                                    
                                 
                                 -
                                 log
                                 
                                 
                                    
                                       Z
                                    
                                    
                                       j
                                    
                                 
                                 .
                              
                           
                        
                     

We extend this method to multi-class classification. Ideally, 
                           
                              
                                 
                                    c
                                 
                                 
                                    ij
                                 
                              
                              =
                              
                                 
                                    c
                                 
                                 
                                    ik
                                 
                              
                              +
                              
                                 
                                    c
                                 
                                 
                                    kj
                                 
                              
                           
                        . But since the partition function is not directly computed, there is a slight error in the estimation. To address this issue, we perform a label ranking process [31] to make the final decision using a set of pairwise classifiers.

In the simplest case, with all the pairwise classifiers 
                           
                              
                                 
                                    f
                                 
                                 
                                    ij
                                 
                              
                           
                        , each prediction is interpreted as a vote for a class, and the class with the highest votes is proposed as a prediction. Alternatively, in confidence estimation, instead of a binary result {0, 1}, a “soft” classifier is employed to map the difference in likelihood into the unit interval [0, 1]:
                           
                              (17)
                              
                                 
                                    
                                       F
                                    
                                    
                                       ij
                                    
                                 
                                 (
                                 V
                                 )
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       1
                                       +
                                       exp
                                       (
                                       -
                                       α
                                       (
                                       g
                                       (
                                       V
                                       |
                                       
                                          
                                             M
                                          
                                          
                                             i
                                          
                                       
                                       )
                                       -
                                       g
                                       (
                                       V
                                       |
                                       
                                          
                                             M
                                          
                                          
                                             j
                                          
                                       
                                       )
                                       -
                                       
                                          
                                             c
                                          
                                          
                                             ij
                                          
                                       
                                       )
                                       )
                                    
                                 
                                 .
                              
                           
                        
                     

Parameter α is searched in a range of 
                           
                              [
                              0.01
                              ,
                              100
                              ]
                           
                         to maximize the training accuracy.

The output of such “soft” binary classifier can be interpreted as a confidence value in the classification: the closer the output 
                           
                              
                                 
                                    F
                                 
                                 
                                    ij
                                 
                              
                           
                         to 1, the stronger the decision of choosing class 
                           
                              
                                 
                                    C
                                 
                                 
                                    i
                                 
                              
                           
                         is supported. Notice that 
                           
                              
                                 
                                    F
                                 
                                 
                                    ij
                                 
                              
                           
                         is not symmetrical. It only gives the preference toward i between i and j. The preference toward j is 
                           
                              1
                              -
                              
                                 
                                    F
                                 
                                 
                                    ij
                                 
                              
                           
                        . A valued preference relation matrix 
                           
                              
                                 
                                    R
                                 
                                 
                                    V
                                 
                              
                           
                         is defined for any query instance 
                           
                              V
                           
                        :
                           
                              (18)
                              
                                 
                                    
                                       R
                                    
                                    
                                       V
                                    
                                 
                                 (
                                 i
                                 ,
                                 j
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         F
                                                      
                                                      
                                                         ij
                                                      
                                                   
                                                   (
                                                   V
                                                   )
                                                
                                                
                                                   if
                                                   
                                                   i
                                                   <
                                                   j
                                                
                                             
                                             
                                                
                                                   1
                                                   -
                                                   
                                                      
                                                         F
                                                      
                                                      
                                                         ij
                                                      
                                                   
                                                   (
                                                   V
                                                   )
                                                
                                                
                                                   if
                                                   
                                                   i
                                                   >
                                                   j
                                                
                                             
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

In our approach, we evaluate the confidence score by summing up all the confidence value
                           
                              (19)
                              
                                 
                                    
                                       S
                                    
                                    
                                       V
                                    
                                 
                                 (
                                 i
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          j
                                          ≠
                                          i
                                       
                                    
                                 
                                 
                                    
                                       R
                                    
                                    
                                       v
                                    
                                 
                                 (
                                 i
                                 ,
                                 j
                                 )
                                 ,
                              
                           
                        where the index i goes through 1 to N, meaning the confidence score for one instance on different classes. The label of the model that associates with the highest score is proposed as the final decision.
                           
                              (20)
                              
                                 
                                    
                                       i
                                    
                                    
                                       ∗
                                    
                                 
                                 =
                                 arg
                                 
                                    
                                       
                                          max
                                       
                                       
                                          i
                                       
                                    
                                 
                                 
                                    
                                       S
                                    
                                    
                                       V
                                    
                                 
                                 (
                                 i
                                 )
                                 .
                              
                           
                        
                     

In general, if we have a N-class classification problem, 
                           
                              
                                 
                                    
                                       
                                          
                                             
                                                n
                                             
                                          
                                          
                                             
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                              =
                              n
                              (
                              n
                              -
                              1
                              )
                              /
                              2
                           
                         pairwise classifiers are used to compute the confidence score.

@&#EXPERIMENTS@&#

We evaluate our algorithm on two different areas that involve high-dimensional motion data: facial expression recognition and human action recognition. Two benchmark data sets are used in our experiment: the extended Cohn-Kanade data set (CK+) [2] and G3D data set [12]. We will also compare the proposed LRBM model with other related methods.

The extended Cohn-Kanade data set (CK+) is a complete data set for action units and emotion-specified expressions. In this work, based on a sequence of landmarks on the human face, our goal is to classify it into one of seven expressions: angry, disgust, fear, happy, sadness, surprise and contempt. CK+ data set includes 593 sequences from 123 subjects. From all the sequences, 327 are associated with expression labels. In our experiment, these 327 sequences are used. Landmarks are the positions of 68 facial points from a detection and tracking procedure, which are provided by the database.

As the pose of each subject varies slightly over time, we apply a pose rectification procedure to make each face a frontal one. Then, using the detected eyes and the interocular distance, we perform a geometric normalization by making the size of facial area the same throughout all subjects, and moving the centers of eyes to the same position. A smoothing filter is also used to reduce the noise in the trajectory. To reduce the feature dimension while keeping the useful information, as shown in Fig. 4
                        , we omit the outline landmarks and some other points, and select the more informative points, resulting in a 15-point feature for each frame, which is a 30d vector. The features selected are reasonable due to the property of facial symmetry.

Intuitively, the spatial patterns are crucial in recognizing an expression. From a single face, human can identify the expression without any difficulty. But in order to explore the underlying facial dynamics as additional features for our system, we choose 10 frames from each sequence, representing the neutral look, intermediate expressions and the peak expression. Since our method captures global temporal patterns, a good alignment of different sequences is important to ensure recognition performance. Linear interpolation is used to get sequences with fixed length. The size of hidden layer is set as 400. To compare with baseline method, we use a leave-one-subject-out cross-validation configuration. Each time we generate the testing data from sequences of one subject. 25 other subjects form the validation set, and all the other subject form the training set.

The performance of the algorithm is given in Table 1
                           . As we only use the shape features (i.e. facial landmark positions), the comparison is only between methods using the same features. Our method’s hit rates for each expression are: Angry – 97.8%, Disgust – 89.8%, Fear – 84.0%, Happy – 100.0%, Sadness 78.6%, Surprise – 97.6%, Contempt – 72.2%. The average accuracy of all the expressions is 88.6%, which is more than 2% better than state-of-the-art method, as reported in [32]. In Table 2
                           , we list the comparison of our model with some recent approaches. Our approach reaches the best accuracy on five expressions (angry, fear, happy, sadness and surprise) among all the methods listed. Although our model does not achieve the best accuracy for every expression, it does not fall behind very much on Disgust, but it fails sometimes on Contempt, because this is a subtle expression that our method needs more features to represent it accurately. In CK+ data set, contempt expression has only 18 samples. However, the average performance of the proposed method has been improved a lot. Another thing to mention is that the method in [2] uses both shape features and appearance features, while we only use the shape features.

If we do not consider the spatial interaction within each frame, the data is simply aligned as a long vector to feed in the conventional RBM. Then the performance is 86.3%, which is less than the LRBM. This also proves the improvement of LRBM over RBM.

We also compare our method with 4 best available results to date, including Time-series Kernels [33], spatio-temporal independent component analysis [34], boosted dynamic features [35], and non-negative matrix factorization techniques [36]. These methods are different from the baseline method in [2], because contempt emotion is removed from the data set, and binary classification is performed for each expression, so for the classification of one expression, the other five expressions are negative samples. Following the same experiment settings, the result of classification is shown in Fig. 5
                           , and detailed comparison is in Table 3
                           . The classification performance of our algorithm is close to, or even better than state-of-the-art method, with a better average accuracy. This demonstrate the effectiveness of the proposed LRBM model as both a binary classifier and a multi-class classifier.

To comprehensively evaluate our method, we compare with a feature learning method, since RBMs are typically used for feature learning. A single RBM is trained on all classes of sequences using Contrastive Divergence. The size of hidden layer is the same as in the generative model. In the test process, given each observation sequence 
                              
                                 V
                              
                           , we compute the posterior probability 
                              
                                 P
                                 (
                                 
                                    
                                       h
                                    
                                    
                                       j
                                    
                                 
                                 |
                                 V
                                 )
                              
                            for each hidden variable 
                              
                                 
                                    
                                       h
                                    
                                    
                                       j
                                    
                                 
                              
                            according to Eq. (4), as the feature representation. This is reasonable because 
                              
                                 P
                                 (
                                 
                                    
                                       h
                                    
                                    
                                       j
                                    
                                 
                                 =
                                 1
                                 |
                                 V
                                 )
                              
                            is the probability of a pattern being activated. A linear multi-class SVM is trained on such features for classification.

The confusion matrix of this method is given in Table 4
                           . For most expressions, the feature-based method is not as good as the generative model. The average recognition accuracy is 83.8%, which is approximately 5% below the performance of the generative model. This is reasonable, because the features learned from the model cannot capture the spatial relationship in each frame, which is specifically modeled in our model using the matrix U. If we add another latent layer, the classification performance only increases marginally by about 1%.

G3D data set is an action data set containing a range of gaming actions captured by Microsoft Kinect. The data set contains 10 subjects performing 20 gaming actions: punch right, punch left, kick right, kick left, defend, golf swing, tennis swing forehand, tennis swing backhand, tennis serve, throw bowling ball, aim and fire gun, walk, run, jump, climb, crouch, steer a car, wave, flap and clap. Synchronized video, depth and skeleton data are available in this data set. We only pick the skeleton data. To reduce the data dimension but keep the useful information, we use the 3D location information of four dominant joints (i.e. two hands and two feet). However the proposed approach can be applied to modeling more joints if we have enough training data.

Before abstracting the features, we perform a normalization procedure to minimize the effect brought by difference in subjects’ body shapes. Specifically, we obtain the average bone lengths from all subjects in the training data, and then change the skeleton tracking results in every frame for each subject with the average shape. Therefore, every subject has the same body shape, and the cross subject distinction is alleviated.

As the size of the visible layer of an RBM is fixed, linear interpolation is performed to convert all sequences into the same length (20 frames for each sequence). The 3D positions of the body joints along all three dimensions (x, y and z) form the 240-dimension input for the LRBM model. In the training phase, we set the size of hidden layer as 80.

We use the action segmentation that is provided by the data and the same experiment configuration as in [12]. The data set is split by subjects where the first 4 subjects were used for training, 1 for validation and the remaining 5 subjects for testing.

The confusion matrix is given in Fig. 6
                           . The overall accuracy of LRBM is 90.5%. Our model encounters some failures for the actions of ThrowBowlingBall in Bowling category and Jump. In these actions, occasionally the body parts are occluded from the Kinect sensor, which will lead to estimated positions of joints. Then the positions of the two feet are significantly corrupted. As our method depends fully on the trajectories of joints, corrupted tracking results have a huge influence on the performance of LRBM. If we remove the pairwise potential in the visible layer and make the model a standard RBM, the accuracy drops to 84%, which proves the better performance of LRBM than RBM, as expected, due to modeling the spatial interactions. Comparison with the baseline model [12] is given in Table 5
                           . Other than Bowling, performance on golf action is quite close to the baseline method, and on all the other actions, our approach outperforms the baseline method. The overall accuracy is increased by 15.5% in terms of F1 score.

One reason of the improvement is that the method in [12] is based on 3 frames, hence can only capture local dynamics, while the proposed method is sequence based that can handle global dynamics.

To further demonstrate the effectiveness of global dynamic model over local dynamic model, we implement a conditional RBM [21] and a hidden Markov model for action recognition. For the conditional RBM model, each frame depends on 3 previous frames. 20 models are trained for 20 actions. Classification is based on the likelihood of a target sequence on different models. Basically the local dynamic model (CRBM or HMM) models one frame at a time and the global model (RBM or LRBM) models all frames at the same time.

Similar to the expression recognition case, the posterior probability of the hidden variables 
                              
                                 P
                                 (
                                 
                                    
                                       h
                                    
                                    
                                       j
                                    
                                 
                                 |
                                 V
                                 )
                              
                            can be used as the features for classification. We implement the feature-based method using linear SVM as the classifier. Again, it does not perform well compare with other dynamic methods, mainly due to the reason that it cannot capture the local spatial patterns in each frame. Details are given in Table 6
                           .

One advantage of generative model is that they can handle noisy or missing data in the input. To demonstrate this point, we design two scenarios with randomly selected noisy or missing data:

(a) Noisy data. For a randomly selected portion of data, we multiply the ground truth data by a Gaussian noise 
                              
                                 1
                                 +
                                 N
                                 (
                                 0
                                 ,
                                 1
                                 )
                              
                           , where 
                              
                                 N
                                 (
                                 0
                                 ,
                                 1
                                 )
                              
                            is the normal distribution;

(b) Missing data. For a randomly selected portion of data, we assume they are missing, and use the average of their neighbors as the approximation when computing the likelihood.

The data under these scenarios are then fed into our generative model for classification purpose. We vary the portion of noisy or missing data from 0 to 50%, and observe the change of the classification accuracy. The details are given in Fig. 7
                           .

In scenario (a), with the portion of noisy data increase, the performance almost decays linearly. In scenario (b), if the percentage of missing data is small (less than 30%), the average of its neighborhood is a decent estimation of the missing value, but with more data missing, the performance decays quite significantly. This is because several consecutive frames are missing at the same time, and the temporal information cannot be recovered effectively. One thing to notice is that with as much as 30% noisy or missing data, our algorithm can achieve 86% accuracy, only 4% below the noiseless situation, which demonstrates the effectiveness of our algorithm to handle noisy or missing data.

In the learning procedure of LRBM, we compute the data-dependent expectation and model-dependent expectation using matrix multiplication, so the computational complexity is linear with the size of the weight matrix (
                           
                              
                                 
                                    dn
                                 
                                 
                                    t
                                 
                              
                              ×
                              
                                 
                                    n
                                 
                                 
                                    h
                                 
                              
                           
                        ) and iteration times (
                           
                              
                                 
                                    n
                                 
                                 
                                    iter
                                 
                              
                           
                        ). Thus, the computation complexity is 
                           
                              O
                              (
                              
                                 
                                    dn
                                 
                                 
                                    t
                                 
                              
                              
                                 
                                    n
                                 
                                 
                                    h
                                 
                              
                              
                                 
                                    n
                                 
                                 
                                    iter
                                 
                              
                              )
                           
                        . For facial expression recognition task, we set the epoch to 250, and the average running time for training one RBM is 1.77s, compared with 1.30s for standard RBM without local interaction. With 10 candidate models, it takes less than 5min to train all the models. Experiments are performed on a desktop computer with an Intel i7 3.4GHz CPU and 8GB RAM. For testing, given a target sequence, we need to compute the unnormalized likelihood on each LRBM model using Eq. (14), with computational cost 
                           
                              O
                              (
                              
                                 
                                    n
                                 
                                 
                                    t
                                 
                              
                              
                                 
                                    d
                                 
                                 
                                    2
                                 
                              
                              +
                              
                                 
                                    n
                                 
                                 
                                    h
                                 
                              
                              
                                 
                                    n
                                 
                                 
                                    t
                                 
                              
                              d
                              )
                           
                        . Given the unnormalized likelihood and the relative partition functions, the classification needs 
                           
                              
                                 
                                    n
                                 
                                 
                                    2
                                 
                              
                           
                         comparisons, therefore the overall classification complexity is 
                           
                              O
                              (
                              n
                              (
                              
                                 
                                    n
                                 
                                 
                                    t
                                 
                              
                              
                                 
                                    d
                                 
                                 
                                    2
                                 
                              
                              +
                              
                                 
                                    n
                                 
                                 
                                    h
                                 
                              
                              
                                 
                                    n
                                 
                                 
                                    t
                                 
                              
                              d
                              )
                              +
                              
                                 
                                    n
                                 
                                 
                                    2
                                 
                              
                              )
                           
                        , which is linear in the dimension of the input sequence. If we linearly increase the length of the sequence by increasing the frame rate, the complexity is also increased linearly.

For multi-class classification with n classes, the confidence value matrix 
                           
                              
                                 
                                    S
                                 
                                 
                                    V
                                 
                              
                           
                         is built with complexity 
                           
                              O
                              (
                              
                                 
                                    n
                                 
                                 
                                    2
                                 
                              
                              )
                           
                        . In typical computer vision applications, the number of classes cannot exceed the dimension of the data, which means 
                           
                              
                                 
                                    n
                                 
                                 
                                    2
                                 
                              
                           
                         does not contribute much in the complexity, compared to the first term. To reduce the quadratic term 
                           
                              O
                              (
                              
                                 
                                    n
                                 
                                 
                                    2
                                 
                              
                              )
                           
                         to a linear term 
                           
                              O
                              (
                              n
                              )
                           
                        , Annealed Importance Sampling [29] can be used to estimate the partition function, thus the likelihood can be directly compared. However, the sampling method requires many samples to get a good estimation of the partition function, and the overall classification complexity is not reduced significantly.

Theoretically, the proposed method can be scaled up to data with thousands of dimensions, but much more training data will be needed since the number of parameters increases linearly with the size of input.

@&#CONCLUSION@&#

In this paper, we study classification problem with multi-dimensional sequence data. To capture both the global dynamics and the local spatial interactions, we extend the conventional restricted Boltzmann machine by adding pairwise potentials in the energy function. An efficient mean field learning method is proposed to replace the reconstruction procedure in typical Contrastive Divergence learning, to estimate the additional parameters. Also, a novel label ranking approach of estimating the partition function of different LRBMs is presented to perform multi-class classification task. Experiments on two areas that involve multi-dimensional sequence data are performed to evaluate our approach. Results on two benchmark data sets prove the effectiveness of our algorithm.

Using the generative model for data representation, we do not need to deal with a deep and complicated structure, as in feature learning, and do not need to carefully tune the parameters. This is a major advantage of our method compared with feature learning methods. However, one shortcoming is that the data sequences have to be aligned for our global model to function properly, and the classification complexity is quadratic in the number of classes. We will address these issues in future work.

@&#ACKNOWLEDGMENTS@&#

The work described in this paper is supported in part by the grant N00014-12-1-0868 from the Office of Navy Research and by the grant IIS 1145152 from the National Science Foundation.

@&#REFERENCES@&#

