@&#MAIN-TITLE@&#Text classification for assisting moderators in online health communities

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We propose a method for improving the quality of online health communities.


                        
                        
                           
                           We present a solution to reduce the workload of the online community moderators.


                        
                        
                           
                           We explore low-cost text classification methods to a new social media domain.


                        
                        
                           
                           We uncover social, ethical, and legal issues in creating a gold standard.


                        
                        
                           
                           We discuss using precision versus recall in supporting the community moderators.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Online health communities

Consumer health

Human–computer interaction

Text mining

Health information seeking

@&#ABSTRACT@&#


               
               
                  Objectives
                  Patients increasingly visit online health communities to get help on managing health. The large scale of these online communities makes it impossible for the moderators to engage in all conversations; yet, some conversations need their expertise. Our work explores low-cost text classification methods to this new domain of determining whether a thread in an online health forum needs moderators’ help.
               
               
                  Methods
                  We employed a binary classifier on WebMD’s online diabetes community data. To train the classifier, we considered three feature types: (1) word unigram, (2) sentiment analysis features, and (3) thread length. We applied feature selection methods based on χ
                     2 statistics and under sampling to account for unbalanced data. We then performed a qualitative error analysis to investigate the appropriateness of the gold standard.
               
               
                  Results
                  Using sentiment analysis features, feature selection methods, and balanced training data increased the AUC value up to 0.75 and the F1-score up to 0.54 compared to the baseline of using word unigrams with no feature selection methods on unbalanced data (0.65 AUC and 0.40 F1-score). The error analysis uncovered additional reasons for why moderators respond to patients’ posts.
               
               
                  Discussion
                  We showed how feature selection methods and balanced training data can improve the overall classification performance. We present implications of weighing precision versus recall for assisting moderators of online health communities. Our error analysis uncovered social, legal, and ethical issues around addressing community members’ needs. We also note challenges in producing a gold standard, and discuss potential solutions for addressing these challenges.
               
               
                  Conclusion
                  Social media environments provide popular venues in which patients gain health-related information. Our work contributes to understanding scalable solutions for providing moderators’ expertise in these large-scale, social media environments.
               
            

@&#INTRODUCTION@&#

In clinic-sponsored, face-to-face support groups, moderators bring in clinical expertise when appropriate, such as when patients need clarification of clinical concepts or medical consultation. In these settings, patients not only benefit from peer-patients’ knowledge but also from the clinical knowledge of the moderators.

Although online health communities facilitate large-scale information exchange among peer-patients, few communities employ moderators [1]. For those few communities who do provide moderators, time and resource limitations make it challenging for the moderators to answer the hundreds of posts uploaded each day.

Not all posts in online community forums require moderators’ response. Some posts discuss everyday strategies, such as recipes for healthier diet, which could be supported purely by patients’ responses. However, other posts asking for medical consultation or clarification of medical concepts need moderators’ help. Moderators redirect patients to their health care providers, point to related health professionals’ blog articles, or provide medical information. Given the large amount of information exchanged online, the challenge lies in helping moderators efficiently select the posts that need their expertise.

In this paper, we present our exploration on low-cost text classification methods—namely the approach not requiring manual annotation for the training data and using readily available features—for prioritizing posts that need moderators’ help. Our method can help moderators of online health communities to more efficiently prioritize which patients’ posts they need to respond to, which will also help fulfill patients’ information needs. We built a binary classifier and explored various feature types, feature selection methods, and under sampling of the training data for performance optimization. We then performed an error analysis on a subset of the data to find ways to achieve a better gold standard and to further understand the remaining social and technical challenges.

@&#BACKGROUND@&#

Social media environments are common venues for health information seeking. Especially for patients with chronic conditions, where the majority of disease management occurs at home, these environments provide a primary resource for people to receive help from their peer patients. The PEW Internet Research found that 23% of patients who have a chronic illness and who use the Internet have gone online to find others with similar health conditions.
                        1
                        
                           http://pewinternet.org/~/media//Files/Reports/2011/Pew_P2PHealthcare_2011.pdf.
                     
                     
                        1
                      The survey also showed that, for information on accurate medical diagnosis, prescription drugs, and alternative treatment options, more than 80% preferred to ask professional sources like doctors and nurses rather than fellow patients, friends, and family. On the other hand, the participants preferred to get emotional support and a quick remedy for daily issues from fellow patients. Hartzler and Pratt [2] discussed such distinct expertise roles between (clinical) professionals and peer patients in social media environments. Accordingly, getting balanced help on both emotional and informational support would require both peer patients and professionals as available sources of information. Yet, few researchers have explored ways to weave together peer patients’ and professional moderators’ knowledge.

In contrast, researchers have long examined novel ways to augment question and answering systems in the medical domain. Lee et al. [3] developed MedQA which provides paragraphs of text for definitional questions in the medical domain, and Cao et al. [4] developed AskHERMES, an online question answering systems for complex clinical questions. Jacquemart and Zweigenbaum [5] explored feasibility of fully automated medical question and answering system and discussed limitations in retrieving specialized knowledge. Liu et al. [6] presented text classification methods for automatically distinguishing consumer questions from health professionals’ questions in the health domain to help systems provide appropriate information appropriate for askers’ expertise. Cruchet et al. [7] developed supervised approach to recognizing question types in question and answer systems in the health domain. These studies in medical question and answering systems have focused on clinicians’ use of the systems, rather than consumers.

From the consumer health perspective, researchers investigated text classification and natural language processing techniques for detecting personal health safety information or health-related expression of consumers in social media. For instance, Himmel et al. [8] developed automatic categorization of medical information requests by users in an online health forum. The authors used manual annotation for generating training data and used word n-grams as features, while refining features using text mining techniques such as principal component analysis [9]. Konovalov et al. [10] developed a classifier using manually selected relevant word unigrams as features to extract combat exposure descriptions from Weblogs. Yang et al. [11] reported best performing information retrieval metrics for detecting adverse drug events from user-contributed content in social media. Qiu et al. [12] showed combining Name and Slang features can improve emotional expression detection in online health communities. While we observe that increasing number of researchers investigate social media for consumer health as a place to incorporate text classification research, little study has examined low-cost approach without having to generate manually annotated training data, and no specific solutions have been proposed for helping to deliver clinical expertise in online health communities.

The increasing number of biomedical information research using text classification in social media and people visiting social media environments seeking health information push us to find scalable solutions for providing clinical expertise to consumers. In this paper, we explore low-cost solutions to assisting moderators of online health communities in delivering clinical expertise to the patient members in the online community.

@&#METHODS@&#

WebMD.com hosts one of the few online communities that offer moderators in patient forums. Their diabetes community shows the most active participation of both patients and moderators among other WebMD communities. Table 1
                      shows activity level characteristics of the diabetes community. The WebMD diabetes community has 18 moderators and 4383 patients
                        2
                        We do not know whether the community members we refer to as “patients” are patients, caregivers, or individuals without diabetes participating in the forum. However, for simplicity, we will use the term “patients” to refer to the community members who are not the WebMD moderators.
                     
                     
                        2
                      who posted at least once in the online forum by May of 2012. Thread conversations in WebMD online communities are publicly available. We applied for approval from the University of Washington’s Institutional Review Board (IRB) and received a letter stating that our project is unregulated by IRB because the data is publicly available. We wrote a script that crawled WebMD online diabetes community to download all thread conversations.

The dataset consists of 8239 initiated posts made by 2902 unique WebMD diabetes community’s patient members from July, 2007 to May, 2012. We tagged each post as positive if the question has been responded by a moderator and as negative if only patients responded to the post.

As a result, 2499 posts belong to the positive, moderated class—posts answered by moderators, and the remaining 5740 posts belong to the negative, non-moderated class—posts only answered by peer patients. Table A in the Appendices shows example questions answered by moderators and questions that only patients responded to.

In Fig. 1
                        , we illustrate the main components of our system architecture. We used three feature sets. Previous research pointed out the reliability and effectiveness of word unigrams over the knowledge engineering approach [13,14]. Thus, as the baseline, we used word unigrams (BOW
                           3
                           BOW is an abbreviation of Bag-of-Words. In another words, BOW refers to word unigrams, the occurrence of single words used as features for training classifiers.
                        
                        
                           3
                        ), where the occurrence of a single word is used as a feature for training our classifier. From BOW, we filtered stop words except pronouns. As we will show later, in our study, pronouns ranked high among other features (please see Appendix Table B). Campbell and Pennenbaker [15] showed that the use of pronouns in writing traumatic memories related to positive health outcomes. Another study found pronouns to be a critical feature for predicting people’s rating of community posts [16]. In our own previous work, we found pronouns to be one of the important predicting features for distinguishing health professionals’ writing from those of patients’ [17]. Accordingly, we included pronouns as part of our BOW feature type. Second, we employed features derived from a sentiment analysis tool called Linguistic Inquiry Word Count (LIWC
                           4
                           LIWC is an abbreviation of Linguistic Inquiry Word Count, the sentimental analysis tool we use as one of our feature types.
                        
                        
                           4
                        ) [18]. Researchers have provided evidence to suggest that people’s mental and physical health can be predicted by the words they use [19–21]. Based on this idea, Pennebaker et al. developed LIWC2007 dictionary over many years, which has been validated through series of experiments [22]. LIWC identifies words that pertain to categories such as social, health, bio, negative emotion, positive emotion, for instance. Each of these categories contains related words that would help identify sentiment of each post. For instance, ‘hurt,’ ‘ugly,’ and ‘nasty’ belong to the negative emotion category and ‘love,’ ‘nice,’ and ‘sweet’ belong to the positive emotion category. We followed LIWC’s approach and counted frequencies of words that fall under each category and used the frequency as values for each category. A complete list of LIWC features and further description of each feature can be found at LIWC’s website (http://www.liwc.net/descriptiontable1.php). Lastly, we recorded the total number of replies that followed the post as another feature. The rationale for using this feature comes from our preliminary work where short threads showed strong association with having medical topics requiring moderators’ help.

Researchers found that balancing training data can improve the overall performance of intelligence techniques [23,24]. Chen et al. [25] further found that under sampling the majority class to match with the minority class produced better sensitivity and specificity than bootstrapping additional samples from the minority class. Accordingly, to generate balanced training data, we randomly selected the same number of non-moderated class as moderated class for each fold. We used this under sampled data as the training dataset consistently throughout our experiments on balanced data. For the test data, we used the raw unbalanced data for both experiments with unbalanced and balanced training data. For feature selection methods, we used χ
                        2 statistics to rank features.

We then ranked features in each feature set based on χ
                        2 statistics (Table B in the Appendices shows an example list of ranked feature sets) and ran experiments to understand performance changes according to ignoring lower ranked features [9]. We trained the model using a Naïve Bayes classifier [26] using the Weka platform [27]. We also studied performance changes after using balanced data for the training set. We used 10-fold cross validation to measure the overall performance.

What is considered an ideal performance level differs greatly depending on the nature of text, the discipline, the quality of annotations, and the size of the dataset. Accordingly, reported ‘good performance’ can widely range from AUC of 0.45 [28], 0.59–0.68 [29], 0.71 [30], 0.82 [31], to 0.95 [32,33]. Our data consist of extremely informal, unstructured language, similar to studies in social media, using automatically generated annotations [28–31]. We expect our performance to be on the lower end, similar to studies working with social media data (AUC of 0.45–0.82).

@&#RESULTS@&#

In this section, we present findings on the classification experiments and the error analysis.

The baseline performance of using no feature selection methods, LIWC features, and thread length on unbalanced data produced 0.5 precision, 0.33 recall, 0.4 F1-score, and 0.65 AUC
                           5
                           AUC (Area Under Curve) represents the area under the ROC curve, which plots the true positives (x-axis) versus false positives (y-axis). This measure takes into account the downsides of binary classification systems with a varying discrimination threshold. A random output will produce 0.5 AUC, so classifiers producing AUC away from 0.5 and closer to 1 show better performance. Both F1-score and AUC will be used throughout the paper as relative overall success measures across different experiments.
                        
                        
                           5
                         (TP: 830, FP: 826, FN: 1669, TN: 4914). To the moderators who will be using the system, this result means that, out of the total messages the system suggested moderators to read and respond to, only 50% (0.5 precision) were the ones moderators would have responded to, and only 33% (0.33 recall) of the messages moderators would have responded to were retrieved. F1-score shows a harmonic mean of precision and recall. Our baseline performance shows slightly higher performance than a complete random method (which would have been 0.5 AUC).

As for the selected features we used, Table B in the Appendix shows the top 20 word unigrams and LIWC features after feature selection based on χ
                        2 statistics. The average thread length was 11.24 replies for the positive class and 5.5 replies for the non-moderated messages.

For the unbalanced data, we found that combining all feature types—BOW with the top 400 features, all the LIWC features, and thread length—yielded the best F1-score (0.48) and AUC (0.75) (TP: 2457, FP: 5328, FN: 42, TN: 412) (see Fig. 2
                           , left, for the ROC curve). For experiments using only the BOW feature type, when using fewer than the top 300 BOW features, the overall performance began to decrease (down to 0.29 F1-score and 0.60 AUC from 0.45 and 0.69 AUC), showing when is best to stop reducing the number of BOW to be used as a feature set. For experiments using only the LIWC feature type, selecting fewer features did not change the overall performance in AUC. Thus, the number of LIWC features used does not matter, but using LIWC features in combination with others will improve the performance. Using only the thread length feature type showed poor overall performance of 0.25 F1-score (TP: 380, FP: 210, FN: 2119, TN: 5530). Although thread length on its own did not increase performance, when all three features were used together the performance increased.

For the balanced data, we again found that combining all features types—BOW with the top 400 features, all the LIWC features, and thread length—yielded the best 0.54 F1-score and AUC (0.74) (TP: 1805, FP: 2402, FN: 694, TN: 3338) (see Fig. 2, right, for the ROC curve). Using only BOW features with balanced data showed an improvement in the F1-score over unbalanced data up to 0.5 F1-score. For experiments using only the LIWC features, the balanced data resulted in even performance between precision and recall compared to the unbalanced data (see Table C in the Appendix for further details).

In both the unbalanced and balanced data experiments, using all feature combinations—BOW 400, all LIWC, and thread length—produced the best F1-score and AUC. Compared to the unbalanced data, using the balanced data showed a higher F1-score (0.48 versus 0.54 F1-score) but similar AUC (0.75 versus 0.74). From the moderators’ perspective, although using unbalanced data gave around 600 more correct suggestions—TP: 2457 messages (unbalanced) versus 1767 messages (balanced)—unbalanced data would give 3000 more incorrect suggestions to read compared to using balanced data. Thus, using balanced data helps moderators spend less time reading unnecessary posts than they would using the unbalanced data. Table C in the Appendix provides full results on all experiments.

In summary, feature selection based on χ
                           2 statistics helped to improve performance in using BOW features, but feature selection did not improve performance in LIWC features. Thread length feature alone did not produce better performance than others, but combining all three feature types—top 400 BOW based on χ
                           2 statistics, all LIWC features, and thread length—produced the best overall performance. Under-sampling training data (balanced data) produced higher F1-score, less false positives and more true negatives than the unbalanced, raw data at a great degree (around 3000 posts differences for both). This finding shows that using balanced training data will better assist moderators’ work efficiency in responding to patients’ posts in online communities.

From the result of the best performing classifier model, we randomly selected 20 false negatives (falsely classified as non-moderated class by our gold standard) and another 20 false positives (falsely classified as moderated class by our gold standard) totaling in 40 falsely classified posts. We stratified the sample so that half of them have strong scores and the other half have weak scores toward the incorrect class. We then used open coding to understand the posters’ intended purpose of the post—whether they asked for peers’ help or answers from moderators (see Table 2
                        ).

As shown in Table 2, four categories emerged for the content of the posts: Asking for medical information, asking for patients’ experience, general chatting, and miscellaneous. The asking for medical consultation covers posts that need a response from a moderator—the moderated class. Examples include asking diagnosis questions, requesting clarification of medical concepts, and soliciting a second opinion about their health care providers’ suggestions. In posts falling under the category, asking for patients’ experience, the posters explicitly asked for peer-patients’ opinions on exercise regimens, drugs, or everyday topics, such as recipes. General chatting includes updating one’s status, chatting about news, or any other rapport building activities such as alerting the members about daylight saving time. We categorized posts not falling under any of the above three categories into the miscellaneous category. Examples include asking technical problems about the website to the moderators.

In the error analysis, we found that 7 out of 10 strong false positives (posts that the classifier strongly said should be moderated, but no moderators replied) asked for medical information. Similarly, 8 out of 10 weak false positives (posts weakly classified as should be moderated, but no moderators replied) did not ask for medical information. 7 out of 10 posts both strong and weak false negatives (posts the classifier said should be non-moderated, but moderators replied) did not ask for medical information.

Below, we discuss implications of our findings for providing moderators’ expertise in large-scale online health communities, more specifically in building interfaces to help moderators choose which posts to respond to.

@&#DISCUSSION@&#

We first discuss challenges in creating a gold standard based on our error analysis and present social, legal, and ethical issues that emerged from our error analysis. We then discuss implications of precision and recall for assisting moderators, connecting back to our results, followed by additional implications for designing interfaces for moderators in online health communities. Lastly, we end with summarizing our contributions to the biomedical informatics community and future work.

Our method for finding a gold standard—using existing activity information on whether a post has been responded by moderators—provides a low-cost solution without the need of manually annotated training data. However, we suspected that this gold standard could be responsible for the errors produced from our classification approach. Thus, we performed an error analysis to further investigate the quality of the gold standard. We found several social and technical challenges in using our gold standard. We also discovered challenges in classifying each post into binary classes. Posts often show ambiguity in who should respond to the post and automatic classification could pose additional legal and ethical challenges.

In our error analysis, we found that classification scores inform relevance strength. Among the ten posts weakly classified as should be moderated (false positives with weak scores in Table 2), eight posts asked for peer-patients’ opinions and general chatting. This finding shows that scores can potentially inform the strength of the classification results. Thus when building an interface for moderators to choose which posts to respond to, the scores can inform recommended posts’ listing order so that moderators see higher scored posts first.

On the other hand, among the ten posts strongly classified as should be moderated that our gold standard considered to be errors, 7 posts explicitly asked for medical information, showing flaws in the gold standard (see Table 2 on false positives with strong scores). For example, a member asked, “If I don’t take Iron pills can that be the reason I’m cold all the time?” No moderators answered his question. Rather, members who take iron pills shared similar experiences. One member brought up Raynaud’s syndrome, a possible cause of the symptom, suggesting the poster to talk to his doctor about it. In this case, if moderators in the forum could clarify how much Raynaud’s syndrome relates to the coldness symptom, the content of the thread could be enriched with both patients’ experience and moderators’ expertise. In the other 3 posts, members updated their health status to the community or asked for an opinion on using an avatar image in online forums. Thus, the gold standard was correct in this case.

Moderators may engage in community posts to build rapport, rather than exclusively to provide medical expertise. Among the twenty posts that our gold standard considered as should be moderated (see Table 2’s false negatives with high and low scores), only six posts contained content asking for medical information. This finding shows flaws in the gold standard—for posts that ought to be tagged as non-moderated class, just because moderators responded, the system tagged the posts as should be moderated. For instance, the other seven posts included asking peer patients to share recipes, updating one’s status, asking for personal opinions on a particular exercise regimen, or asking about a technical problem in using the online forum. Moderators responded to these posts as if to build rapport with the community members:
                              A community member: “After reading Rupa’s post I cooked cabbage but I don’t like the way mine taste. Very bland. Of course the only way I know to cook it is to use water, butter and some salt. Any recipes to make it taste good?”
                              A moderator: “I generally use soy crumbles for it. Shrug. Easier for me, healthier and no one in the family notices. I think the recipe I use actually calls for ground turkey.”
                           
                        

In another example, a member asked for opinions from peer members on choosing food. To this, a moderator responded:
                              “Elk and other wild game are excellent sources of healthy protein, and the fat content of wild game is much healthier and much lower, than other red meats. Can you grind some of it up, for burgers etc.? That would solve the long cooking time issue. A crockpot with lots of veggies and a flavorful liquid would be my approach. Elk stew. That way you can cook it overnight or all day long without effort. Incidentally, I was once served “moose balls” at a potluck in Alaska. “What part of the moose is that?” I asked without thinking, before it occured to me that these are meatballs made from ground moose!”
                           
                        

In this post, the moderator provided both medical expertise around diet as well as fun jokes that would help him build rapport with the community. This case shows posts asking for recipe information may not always mean that the thread should be non-moderated.

The WebMD’s policy states that the website will not give medical consultation but only medical information will be provided by the moderators. Thus, for the posts asking for a medical consultation, moderators responded that they cannot diagnose or provide such a consultation. Accordingly, although by the gold standard the post should be moderated, moderators cannot provide consultation. For instance, a member posted with a title, “Please help, I’m type 1 and pregnant”:
                              “I’m 35 weeks pregnant. I’m a brittle diabetic, I checked at around 12 pm it was 87 which is great, then I ate a whole can of spaghettios. I didn’t cover it, and at 2 pm my blood sugar was 45! I was sleeping during that time, so it’s not like I was exercising or running around. I called/emailed my endo, but he just said lower my basal levels a little bit, not even how I would have lowered them and said to still cover my meals. But when you are pregnant and your sugar goes to 30 in the middle of the night, that’s scary, and I’m tempted to not listen to him and just lower it how I think it should be lowered. Could this be a sign of something else wrong. I wish he would have listened to me better.”
                           
                        

The poster was asking for a second opinion from the forum since her doctor’s suggestion did not address her concerns. However, a moderator clarified that the community do not give medical consultation and redirected the member to a relevant past thread conversation with appropriate medical information. Another example below shows a member asking how much insulin to inject, and a moderator responding that they cannot help the member with the problem:
                              “am on a sliding scale of novolog and i take lantus at night, the problem is i am in the prosses of moving and lost the written out of the scale so i am lost with out it and if i call the dr and set up appt that will cost me and right now i dont have it to spare, its either appt or buying meds so i would like to know if you can help me please it would be greatly appreacated
                                 
                                 …
                                 
                                 thanks”
                           
                        

To this, a moderator responded, gently declining the request to give personalized medical consultation:
                              “I am afraid that we can’t help you. The sliding scale is personalized and decided upon a number of test results, your weight, any disabilities, other health issues, and activity level. Can you call your doctor and just ask for it to be faxed/mailed to you or call your pharmacy and see if they have a copy. Our health professionals also can not provide that information because it is not safe to diagnose/treat over the internet.”
                           
                        

The boundary defining moderators’ expertise, medical information to medical consultation, personalized consultation shows ambiguity.

@&#SUMMARY@&#

Our methods for constructing the gold standard can be efficient and scalable. At the same time, our error analysis result shows challenges in using this gold standard. If we were to construct a gold standard using manually annotated documents, we would still run into challenges because of the ambiguity and situational factors that exist in determining whether a post needs moderators’ expertise. Because we rely on moderators’ current activities on the forum, we inevitably encounter posts classified into either class for reasons other than the fact that the post needs moderators’ expertise. Additionally, moderators cannot respond to all posts that need their expertise, thus we find that, in practice, not all posts classified as non-moderated needs should be non-moderated.

Our classification approach together with varying results on precision versus recall provide implications for building a system that assists moderators to skim through a selected number of posts classified as needing their expertise, instead of making moderators navigate through thousands of posts in the forum. For instance, our system will suggest moderators to read only about a half (4207 messages) of the whole corpus (8239 messages) and still get 72% of all posts that require moderators’ attention. As shown in this case, methods producing high recall will serve the purpose of making sure that moderators respond to the posts needing their attention as many as possible, while still reducing the burden of reading all messages. On the other hand, methods producing high precision will serve the purpose of moderators who only have limited time to read a small number of posts. In this case, using only the top LIWC feature will result in suggesting moderators to only read 446 messages, and 93% of the time moderators would be responding to posts that need moderators’ attention.

Similarly, the TP, TN, FP, and FN numbers are useful in weighing the different options of setting up features to help with moderators’ engagement in online forums. From the results between balanced and unbalanced data in BOW400-LIWC-all-Length, we could see that, compared to using unbalanced data, using balanced data produced 3000 more correct judgments about which posts the moderators should ignore (TN) and 3000 fewer incorrect judgments about which posts they should read (FP). We also see that, using the balanced data, moderators would miss about 600 more posts needing their attention (FN) compared to using the unbalanced data, but moderators would have fewer incorrect suggestions to wade through. Thus, choosing balanced or unbalanced data allows us to explicitly choose whether to prioritize precision or recall, depending on whether moderators want to reduce their time spent reading unnecessary posts or reduce the number of missed posts.

Our experiment results suggest a number of methods to increase recall or precision depending on the preference of the moderators. BOW generally increased precision while LIWC increased recall. Using feature selection methods and balanced data, we can adjust precision and recall. For instance, for moderators wanting to see all possible posts that need their help, using methods such as BOW that produce high precision with low recall would be inappropriate. As we discussed in the error analysis, scores can further play a role in ranking results so that moderators can see the results in the order of stronger scores.

We could also allow moderators to further filter results based on topics of posts. For this task, we can highlight certain features—such as ‘social’ or ‘religious’ of LIWC features and suppress ‘bio’ and ‘health’ features if users wanted. Simple search mechanisms will help filter the results as well. We can also categorize posts by total replies so that moderators can choose to respond to posts that have not had sufficient responses. Moderators can also deliberately choose popular threads to see if they can clarify medical concepts exchanged in the thread.

As discussed in the error analysis, generating a gold standard by solely using existing forum activity information of moderators poses big challenges. To address the shortcomings of the current approach, we can allow moderators to correct the system’s results. Also, community members can vote which posts need moderators’ attention. The feedback information from users can help the system further learn and improve the gold standard over time. Furthermore, we can use moderators’ response in the training data to improve the gold standard.

The ultimate value of our research lies in helping both patients and moderators exchange fruitful, useful information in online health communities. To provide this value, we developed an automated approach to help moderators prioritize which posts they need to respond to among thousands of messages being posted each day. Furthermore, our work contributes in a number of ways to the biomedical informatics research in the area of social media, specifically in building highly supportive online health communities.

First, we show the potential of a low-cost text classification method that can easily be applied in other online health community settings without producing manually annotated training data. We show using automatically generated gold standard coming from the community’s past activity data and canned feature sets, such as a sentiment analysis tool called LIWC, as a low-cost text classification approach. We explore a statistics based approach to feature selection, which provides again a low-cost approach that one can easily apply to other community settings.

Second, we provide insights into potential social and technical challenges in assisting moderators in online health communities. Even with manually annotated training data, our results show that discriminating posts that need moderators’ attention will be challenging. Using our error analysis results showing various topics to which moderators respond to, future research can generate topic specific classification tasks.

Lastly, we provide trade offs and implications between strong precision versus recall in assisting moderators in online health communities. We show how moderators’ preferences for the number of messages they are willing to read and the posts needing moderators’ help can be addressed through the choice of feature selection methods. We further discuss possibilities of addressing moderators’ preferences for selecting topics of messages using LIWC.

Our work explored applying existing text classification methods to a new domain of determining whether a thread in an online health forum needs input from a moderator. To allow for low-cost solutions, we investigated using a gold standard based on existing activity data of moderators. Through the error analysis, we uncovered additional reasons for why moderators respond to patients’ posts. We found shortcomings in the current gold standard and suggested feedback mechanisms as one potential solution. Our arguments coming out of the error analysis, such as scores informing the result strength come out of small sample. In addition, we only investigated the diabetes community at WebMD.com. The diabetes community at WebMD.com has higher member activity compared to other communities, likely because of the high prevalence of diabetes in the overall population [34]. The natural next step would be to see how our findings apply to other chronic illness communities. We also infer much of moderators’ and patients’ intention based on what they have posted in the forum. Conducting interviews with moderators to investigate their needs in choosing posts to respond to as well as with patients to study their needs in wanting who to respond to their posts will serve as an informative future step.

Building on our findings, we will explore applying keywords or medical terms, using built-in tools such as Metamap [36], or consumer health vocabularies [37] as additional features to discriminate posts with medically-oriented topics.

@&#CONCLUSION@&#

Our findings show that it is possible to assist moderators’ providing their expertise with patients in online communities. We applied low-cost text classification methods to an underexplored domain of assisting moderators in large-scale online health communities. We reported results on classifying posts that need moderators’ help, using a low-cost approach—with the use of automatically generated gold standard of the community’s activity data and canned feature sets. We discussed implications for how our findings apply to building interfaces to facilitate exchanging moderators’ expertise in large-scale online health communities. Through the error analysis, we uncovered social, legal, and ethical challenges in providing moderators’ expertise. We showed implications of our results around precision versus recall towards building systems that assist moderators. Future work can compare different classifier algorithms and feature types to further improve the performance. Social media environments increasingly offer unique venues through which patients can gain health-related information. Our work contributes to developing and understanding scalable solutions for providing moderators’ expertise in these large-scale social media environments. The value of our methodology to online health communities lies in helping both patients and moderators in online health communities by assisting moderators prioritize which patients’ posts they need to respond to.

@&#ACKNOWLEDGMENTS@&#

This work has been partially funded by NSF SHB 1117187 and NIH-NLM # 5T15LM007442-10 BHI Training Program. We also thank the iMed group, David McDonald, Andrea Hartzler, Mark S. Ackerman, and Moon-Yul Huh for support.

Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.jbi.2013.08.011.


                     
                        
                           Supplementary data 1
                           
                              Table A. Post examples of threads where moderators do or do not participate; Table B. Top 20 list of word unigrams (BOW) and sentimental analysis (LIWC) features based on χ
                                 2 statistics; Table C. Classification experiment results.
                           
                           
                        
                     
                  

@&#REFERENCES@&#

