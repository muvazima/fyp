@&#MAIN-TITLE@&#Design and simulation of a parallel adaptive arbiter for maximum CPU utilization using multi-core processors

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           If we understand Moore's law and Amdahl's law then we can conclude that increasing number of CPU cores in a computing system is of no use just to attain high computation rate.


                        
                        
                           
                           We have designed a new arbitration technique which can use the CPU cores in a most optimised manner and can achieve high degree of task parallelism.


                        
                        
                           
                           The designed arbitration technique is superior to other existing arbitration technique in terms of CPU usage, bandwidth optimization and latency.


                        
                        
                           
                           The designed arbitration technique has been tested using high performance benchmark program to analyse its efficiency.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Parallel adaptive arbiter

Multi-core

CPU utilization

System-on-Chip

Bandwidth allocation

@&#ABSTRACT@&#


               Graphical abstract
               
                  
                     
                        
                           Image, graphical abstract
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

An arbiter is considered to be an electronic device which allocates access to the shared resources. In an environment of multi-core systems, the common bus of the System-on-Chip (SoC) is the sharing resource which is shared by multiple cores of the master. An arbiter plays a crucial role when it comes to granting an authority to utilise the shared resource efficiently. It ensures that at a time, at least one master gets access to the bus by observing the number of request issued by different number of masters during any cycle. It samples the multiple requests and decides which master should be given access to the shared bus. The main aim of an arbitration process is to assign processes to be implemented by the processor in such a manner that it meets the objectives such as efficient processor utilization, bandwidth optimization and low latency. As the technology is scaling towards the deep submicron, the feasibility for the integration of multiple processors on a chip is becoming possible. Every year, more amount of transistors are made compatible to fit on a single die, which adverts Moore's law.

An arbiter ensures compatibility between on-core speed with the off-core speed because a kind of memory wall starts building up if an enhancement in the on-core speed is not compatible with the off-core and I/O subsystem [1]. A lower frequency bus matched with the higher frequency core will stall the system frequently as the core waits for the data. These mismatches have been compensated till some extent by implementing large and fast, on chip caches, but this cannot be a permanent solution to the problem as by enhancing the size and increasing the on-chip caches, increases both power consumption and the silicon size. The multi-core designs are used as a standard design across the computing spectrum that consists of high-end systems, such as huge servers, telecom infrastructure and supercomputers. Multi-core devices have been in use for many years but in different forms, for instance in the form of uni or the dual Reduced Instruction Set Computing (RISC) cores inside Quad Integrated Communications Controller (QUICC) Engine communication unit [2]. A device that has a multiple cores with various types of instruction sets is known as heterogeneous device, whereas homogenous multi-core devices has multiple identical cores in it. In today's scenario, the main focus is to create multi-core homogenous devices, but a significant amount of advantage can only be gained by using accelerators and specialised cores to shed the load from the main cores [3]. The central challenge considered for multi-core environment is the task of parallelization [4]. In order to attain high degree of task parallelization, an arbitration technique plays a major role by synchronizing the execution of multiple cores. However, the concept of parallel computation is not new for the industry but in order to implement a system which becomes compatible to run in a parallel computing environment is quite arduous task. Multi-core computation emphasises more on data and task parallelism using fine arbitration technique as it focuses on the sector where the software and the system design matters a lot. The design of an arbiter should be kept as simple as possible but it is most important for an arbiter to ensure that it handles the critical path in an efficient manner. Increasing the number of cores on the processor is of no use to gain the system speed in long run. An arbiter is required which can exploit the multiple cores of the processor with a moderate bandwidth allocation. Therefore, a new arbitration technique is proposed which is called a Parallel Adaptive Arbitration (PAA) technique.

The performance of the new arbitration design is compared with other well-known arbitration policies for a bus based environment. Various arbitration techniques implemented earlier lacks efficiency in terms of CPU utilization and moderate bus bandwidth allocation therefore, it is essential to come up with an advanced arbitration technique. The final design is modelled using SystemC and OpenMP tools which makes this arbitration technique different. A research study designed to implement the proposed arbitration technique has an objective to develop an arbitration technique which can exploit the multiple cores of the processor using the method of task parallelization and to ensure a moderate bus bandwidth allocation with low latency.

In rest of the manuscript, Section 2 discusses related works, which elaborates earlier implemented arbitration techniques in detail. Section 3 introduces the PAA, which describes the implementation of multiple master cores. It also discusses the way SystemC and OpenMP threads interact with each other. Section 4 contains a detailed description of the obtained results, which consists of CPU utilization rate, bandwidth allocation values and latency rate. Finally Section 5 presents the conclusion.

@&#RELATED WORKS@&#

In recent years many researchers focused on developing multi-level arbitration scheme in order to reduce the system latency and to achieve fair bandwidth allocation. Xu et al. [5] proposed an arbiter called an adaptive dynamic arbiter in which they proposed a lottery-bus algorithm approach where an arbiter can adjust the bandwidth proportion assigned to every processor automatically due to the situations of bus transactions aiming to reduce total task execution time. Compared with conventional architectures, their architecture reduces the system latency but it does not allocate fair bandwidth to the processors and neither it maximises the CPU utilization as it does not gets implemented using parallel programming method. Research is also going on for arbitration based on quantum computing which involves enormous amount of parallel processing so as to utilise the cores to its maximum potential [6]. Aravind [7] presented an algorithm which is a fully- distributed software solution to the arbitration problem in multi-port memory systems. Their algorithm is purely based on First in First Out (FIFO) and Least Recently Used (LRU) fairness criteria but the algorithm does not deal with fair bandwidth allotment to the different masters which may become a barrier to get a better performance. Moreover, their arbitration technique follows sequential style of programming and therefore does not make use of the multiple CPU cores. Macii and Poncino [8] proposed a novel method of automatic synthesis of easily-scalable bus arbiters with dynamic priority assignment strategies. They emphasised more on those arbitration mechanisms which can be implemented on silicon as a digital circuit, rather than getting concerned about how the selected arbitration policies can affect the performance of a multiprocessor system. Their arbitration technique was fair in terms of bandwidth and latency but were least concerned regarding CPU utilization as it did not get implemented using parallel programming method. The major disadvantage of common-bus multiprocessor system is the reduction of throughput caused by conflict between processors requiring access to the shared memory. Ideally, throughput should increase directly with the number of processors but the bus contention diminishes this increase [9]. There is a critical number above which the processors show no improvement and this critical number depends naturally on the extent of bus used by the processors [10]. Zitouni and Tourki [11] proposed an arbiter synthesis approach that allows a high performance Multi Processor System-on-Chip (MPSoC) communication using Asynchronous Transfer Mode (ATM) switch for multi-bus and Network-on-Chip (NoC) architecture. Their result demonstrates that MPSoC offers an attractive alternative to conventional communication architectures by providing low communication latency using sequential style of programming.

Chen et al. [12] designed a real-time and bandwidth-guaranteed arbitration algorithm for SoC bus communication in which RT_Lottery algorithm has been used to meet both hard real-time and bandwidth requirements. Their work demonstrated a two-level arbitration scheme which comprised of Time Division Multiple Access (TDMA) algorithm and lottery based arbitration algorithm. They developed master cores according to the traffic behaviour of the data flow which consists of both heavy traffic masters and light traffic masters. On the other hand, their masters did not show synchronization among them and were implemented using sequential programming method. Therefore, the masters were unable to maximise the CPU core usage. However, in terms of diverse bandwidth allocation, their arbitration technique was superior and were able to handle hard real-time bandwidth requirement. In order to support stress testing, Shiva et al. [13] analysed the CPU usage in safety- critical embedded systems. Their main goal was to investigate whether their technique could help engineers in deriving test cases for CPU usage. So as to achieve their goal, two objective functions were used to capture high usage of CPU time. The first one calculates average CPU usage and is denoted by fCPUusage
                     . The second objective function computes the total length of the schedule, and is denoted by fmakespan
                     . This function computes the time required for all activities in an application to terminate upon the arrival of the first thread in the same application. Their overall analysis for safety critical system was impressive, however it lacks equal bandwidth distribution and synchronization of higher degree of heterogeneous masters to implement application-specific task.

Che et al. [14] introduced a new benchmark known as Rodina designed for heterogeneous computing systems to facilitate computer architects study engineering platforms such as multi-core CPU and GPU systems. Various algorithms like K-means, Needleman–Wunsch, Hotspot, Backpropagation, SRAD, Leukocyte Tracking, Breadth-First Search, STREAM Cluster and Similarity Scores have been tested using their developed benchmark. Their work characterise the diversity of the Rodina benchmark to prove that its each component represents unique behaviour [14]. However, bandwidth distribution remained unexplored in their experiment.

A unique algorithm was proposed by Li et al. [15], called Adaptive Arbitration (AA) algorithm in which an arbiter can adjust priority automatically to provide the best bandwidth for different master according to their real-time bus bandwidth needs. They showed that, it is possible to allocate fair bandwidth to a given set of processors with a very high degree of fairness. In their case, an arbiter records the number of time each master has requested for the bus and the total time that all master have requested for the bus access. Using these two values the arbiter can calculate the bus access probability of the corresponding master by the division operation method. The priority weight of the master is decided by its probability of getting the bus access. A master who has the bigger weight owns the higher priority. It is unnecessary for an arbiter to recalculate all the probabilities and weights and to reorder the priority of masters when a new bus access request appears. The solution to this problem is to reduce the frequency of weight calculation and priority reordering [15]. Their arbiter worked well in terms of fair bus bandwidth allocation but on the other hand it did not exploit the multi-core parallelism. In order to enhance multi-core parallelism, Akhtar and Saleh [16] proposed a novel arbitration technique called Smart Adaptive Arbitration (SAA) which works on the principle of parallel computation and offers moderate bandwidth allocation to its masters along with low latency. SAA is promising arbiter in a sense that it solves the problem of task parallelization by treating each master with moderate bandwidth. However, there is a limitation at the synchronization level of its masters which still remains unexplored.

According to Buttazo [17] , the decision mode of an arbitration algorithm can be generally categorised in two manner that is non-preemptive and preemptive. In the case of non-preemptive arbitration, if the process is in the running state, then it continues to execute until it terminates or it blocks itself for input/output operation or it requests for some other operating system service. Whereas in the case of preemptive arbitration, the process which is in the current running state can be interrupted and moved to the ready state. The decision to preempt any process is made if any new process arrives with a higher priority. When an interrupt occurs, then the arbiter puts a blocked process in the ready state. This kind of policy incurs greater overhead if compared with non preemptive policy. This policy prevents monopolization of processor for a very long time. There are several arbitration algorithms which have been developed and is mentioned as follows:

The most commonly used arbitration scheme is the SFP scheme, in which masters on a bus are assigned fixed priority values. The master with highest priority always gets access to the bus with a fixed bandwidth. This scheme can be implemented in a non-preemptive manner. In a preemptive implementation, an ongoing lower priority data transfer from a master is terminated immediately without being completed if a request for bus access is received from a higher priority master. In a non-preemptive implementation, the ongoing lower priority data transfer from a master is allowed to complete before the bus is handed over to the higher priority master. SFP scheme is simple to implement and can provide high performance by ensuring critical data transfers, such as between processor but this scheme should be implemented carefully as it can lead to starvation of lower priority masters, which might never be able to get access to the bus if there are frequent bus accesses by masters with higher priority.

TDMA arbitration scheme can guarantee a fixed, higher bus bandwidth to masters with higher data transfer requirements and also ensures that lower priority masters do not starve. In this scheme, each master is assigned time slots of varying lengths, depending on the bandwidth requirements of the master. The choice of number of time slots to assign to each master is extremely important. The slots allocated to higher-priority masters should not be in such a way that the master with lower priority starts starving. The length of each time frame should be long enough to complete at least a single data transfer. TDMA scheduling diagram mentioned by Xu et al. [5] divides execution time on the bus into time slots and allocates time slots to masters in a specific way using preemption.

In Fig. 1
                           , the first level (bottom block) uses a time wheel where each slot is statically reserved for a unique master. If a master possessing the current time slot does not issue request, the time slot would be wasted. To overcome this inefficiency the second level arbiter scheme has to issue the bus to the other master contending for the bus. Moreover, this arbitration technique cannot implement the requesting masters in parallel as the degree of granularity between them is very less. Therefore, it is unable to exploit the multiple cores of the processor.

Round-robin arbitration scheme ensures there is no starvation in the system. In this scheme, access to the bus is granted in a circular manner, to every master on the bus and makes it certain that every master will eventually get access to the bus. A master abandons control over the bus when it no longer has any data to send and passes the ownership to the next master in queue. RR scheme is simple to implement, and can ensure equal bandwidth distribution on a bus, but suffers from a drawback compared to the static priority scheme, that critical data transfers may have to wait a long time before they can proceed. According to Wiseman and Feitelson [18], RR scheme can be implemented using both preemptive (only applicable for clusters) and non-preemptive manner. A modified version of RR algorithm has been proposed by Yaashuwanth and Ramesh [19] which modifies all the drawbacks of a simple round robin algorithm by reducing the high context switch rate, large waiting time and larger response time using non-preemptive method. Shin et al. [20] came up with a new algorithm known as Round-Robin Arbiter Generator (RAG) tool. This RAG tool can generate a design for a bus arbiter which is able to handle the exact number of bus master for both on-chip and off-chip buses using non-preemptive method. Li et al. [15] implemented the RR scheme using non-preemptive method with four masters and achieved fair bandwidth allocation for each master using sequential programming. As per the experiment of Wiseman and Feitelson [18], the CPU utilization rate for RR scheme was 45% in the parpar cluster of 8 computing nodes. These days RR algorithm is also being used for comparison analysis related to job scheduling in the field of swarm intelligence [21]. The following Fig. 2
                            shows the layout of RR scheme.

This is another complex, but highly efficient arbitration scheme that can dynamically vary the priority of the master during the run time [22]. To analyse the data traffic at the run time, additional logic is used, and the priorities are dynamically adapted to the changing traffic profiles of an application. This type of scheme is very useful when the master needs to send large amount of data with low latency. The cost of implementing such type of scheme can be high as it requires several registers to keep track of priorities and data traffic profiles at various point of execution.

This is a simpler variant of the dynamic priority scheme, which allows application to write into the arbiters programmable registers and set the priority for masters on the bus dynamically [22]. Nitnaware [23] used Programmable Priority Encoder in their arbiter and it supposed to be the most time-critical component of the scheduler design. The programmable priority encoder chosen for their design is the hybrid design which combines two simple priority encoders. Thermometer encoding is used to mask the input of one priority encoder based on the programmed priority level. Their architecture is a combination of RR and 8×8 switch. As the context switching of the processors increases using RR policy, the latency of the system will increase. Moreover, RR scheme does not uses parallel method of programming.

Let M1,M2,M3
                            and M4
                            be the set of masters and t1,t2,t3
                            and t4
                            be the number of tickets held by each master. Suppose during any bus cycle, the group of pending bus-access requests be represented by set of boolean variables ri
                            (i=1,2…,n) where ri
                            = 1 if component M
                           
                              i
                            has got a pending request, otherwise ri
                            = 0. Master associated with the component which bears the largest number of tickets finally gets access to the bus with a fair bandwidth. Probability of granting component Mi
                            is given by:
                              
                                 (2.1)
                                 
                                    
                                       P
                                       
                                          (
                                          
                                             M
                                             
                                                i
                                                
                                             
                                          
                                          )
                                       
                                       =
                                       
                                       
                                          
                                             
                                                r
                                                j
                                             
                                             *
                                             
                                             
                                                t
                                                j
                                             
                                          
                                          
                                             
                                                ∑
                                                
                                                   j
                                                   −
                                                   1
                                                
                                                n
                                             
                                             
                                                r
                                                j
                                             
                                             *
                                             
                                             
                                                t
                                                j
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

In order to make a decision, the lottery manager analyses the number of active tickets (j=1…n) which the lottery system has got [24]. This is given by the formula:
                              
                                 (2.2)
                                 
                                    
                                       
                                          ∑
                                          
                                             j
                                             =
                                             1
                                          
                                          n
                                       
                                       
                                          r
                                          j
                                       
                                       *
                                       
                                       
                                          t
                                          j
                                       
                                    
                                 
                              
                           
                        

These tickets are generated sequentially to each master. A pseudo-random number from the range [0, 
                              
                                 
                                    ∑
                                    
                                       j
                                       =
                                       1
                                    
                                    n
                                 
                                 
                                    r
                                    j
                                 
                                 *
                                 
                                 
                                    t
                                    j
                                 
                              
                           ] is used to determine the component which is to be granted the bus. If the number is in the range [0, r1 * t1], then the bus is granted to the component M1
                           , or if it is in the range [r1 * t1, r1 * t1 + r2 * t2
                           ], then the component M2
                            is granted the bus. In general, if the number lies in the range [
                              
                                 
                                    ∑
                                    
                                       k
                                       =
                                       1
                                    
                                    i
                                 
                                 
                                    r
                                    k
                                 
                                 *
                                 
                                 
                                    t
                                    k
                                 
                              
                            , 
                              
                                 
                                    ∑
                                    
                                       k
                                       =
                                       1
                                    
                                    
                                       i
                                       +
                                       1
                                    
                                 
                                 
                                    r
                                    k
                                 
                                 *
                                 
                                 
                                    t
                                    k
                                 
                              
                           ], then the component M(i+1)
                            is granted the bus. Since there is a sequential process involved in generating tickets to the masters, therefore this arbitration technique cannot be implemented parallely.

In this architecture, the inputs are in the form of group of request lines i.e. r0, r1, r2, r3 as shown in the Fig. 3
                           . There is a specific ticket generator module, which generates tickets to the master.

As the range of the ticket values are dynamic, therefore a partial sum needs to be calculated for each component at every lottery which is given by:
                              
                                 (2.3)
                                 
                                    
                                       
                                          ∑
                                          
                                             j
                                             =
                                             1
                                          
                                          n
                                       
                                       
                                          r
                                          j
                                       
                                       *
                                       
                                          t
                                          j
                                       
                                    
                                 
                              
                           
                        

The above equation is implemented using bit-wise AND and tree of adder. The final result is obtained using the range (T = r0t0 + r1t1 + r2t2 + r3t3) in which the random number lies. The distribution of random numbers in this architecture is non-uniform which is a slight limitation of this architecture [25]. It is advantageous in a way that none of the masters suffers from starvation, as each master gains access to the bus with a fixed and fair bandwidth. It is disadvantageous in a way that it implements using sequential programming and if the value of the pseudo-random number is greater than the value of the total sum of the ticket, then none of the master gets access to the bus.

In a nut shell, Table 1
                            summarises the above discussed arbitration techniques.

@&#METHODOLOGY@&#

It can be inferred from the literature presented in previous section that earlier implemented arbitration techniques lacks efficiency in terms CPU utilization and latency. However, some of the arbitration techniques are fair in terms of bandwidth allotment. In spite of fair bandwidth allotment, the earlier arbitration techniques does not get implemented using the method of parallel programming, so they do not exploit the multiple core of the processor. This dominant characteristic of parallel processing has to be implemented using an arbiter to enhance the arbitration policy of the system including fair bandwidth, maximum CPU utilization and low latency.

Three types of masters were designed according to the traffic behaviour to implement real-time and bandwidth-guaranteed arbitration using non-preemptive RT_Lottery algorithm [12,16]. More emphasis was given on variable bandwidth allotment to handle real-time requirements rather than parallel implementation of the masters. Moreover, in terms of critical bandwidth requirements, RT_Lottery algorithm holds good. For the RT_Lottery algorithm, the weight function was used to decide the bus access priority by each master.

These three types of masters are appropriate to be utilised for real-time bandwidth-guaranteed algorithms. Recently, these masters were used by Akhtar and Saleh [16] for their proposed arbitration technique called SAA which was implemented on Amazon EC2 instance for hard real-time computing. The results were appropriate and satisfying in terms of bandwidth fluctuation. The average bandwidth fluctuation in case of SAA was ± 0.25% which was better than other conventional arbitration techniques [16]. For the proposed arbitration technique, similar type of masters would be used with different arbitration strategy which will be discussed in detail in the following subsections.

In D_Type master, D stands for dependent. This type of master has got no real-time requirements and the upcoming request is totally dependent on the finish time of the current request. Interval time between two continuous requests is the time, from the issued point of time of the previous request to the finish time of the latter. Fig. 4
                            shows an example of this type of master. A burst is generated of 4 beat at cycle 13, the request is not granted till cycle 16 gets finished at cycle 20. If interval time is 10, then the next request is issued at cycle 30 only, therefore no request can be issued between cycle 20 and cycle 30 which is represented by uni-directional line. Chen et al. [12] used this master to implement RT_Lottery algorithm. This master worked well in terms of processing high beat burst data. However, there was a huge difference between the required bandwidth of master and the maximum allotted bandwidth. D_Type master was also used to implement SAA developed by Akhtar and Saleh [16]. For SAA, this master acquired fair and moderate bandwidth to enhance the overall bandwidth optimization [16].

This is same as D_Type master, the only difference is that they have extra real-time requirements (R). Rcycle
                            is the real-time requirement for master which is set to 10 cycles. It is to be noted that the value of Rcycle
                            could be adjusted by the programmer according to the real-time requirements of the application or it could be set by the arbiter automatically [12]. It is shown in the Fig. 5
                            that the request issued at cycle 5 has to be finished before cycle 15 represented by dotted lines. A real-time violation occurs if the request is unable to finish before cycle 15. The bi-directional line represents first request which has to finish before cycle 15. The uni-directional line represents second request which starts after the completion of first request at cycle 12. The second request was continuously being issued, however the permission was granted only after the completion of first request at cycle 12. For RT_Lottery algorithm implemented by Chen et al. [12], it was observed that for DR_Type master, the difference between the required bandwidth of master and the maximum allotted bandwidth was less if compared to D_Type master. Moreover, this master was also used by Akhtar and Saleh [16] to implement SAA. For SAA, it was observed that DR_Type master acquired moderate and fair bandwidth and it showed high degree of synchronization with its associated masters [16].

The issued time of a request from a NDR_Type of master does not depend on the finish time of its previous request, and the interval time is the clock cycles between two successive requests. In Fig. 6
                           , time interval assumed is 15. At cycle 21 second request is granted permission and is executed which is represented by uni-directional line. This request directly depends on cycle 7 of the first request but not its completion time at cycle 14 which is represented by bi-directional line. In this case Rcycle
                            is supposed to be smaller than minimum possible interval time because the current request must be finished before the issue of next request. It means that, it is possible for the designer to include tight time constraints. NDR_Type master worked well for RT_Lottery algorithm developed by Chen et al. [12], as the required bandwidth of the master was equal to the maximum allotted bandwidth. Moreover, for SAA developed by Akhtar and Saleh [16], this master acquired optimum bandwidth. However for SAA, NDR_Type master showed tight time constraints due to the synchronization limitation [16]. Hence, an improvement over SAA is deemed necessary.

The proposed PAA technique has been designed in a manner that it can synchronise the implementation of the aforesaid masters to maximise the CPU cores utilization along with moderate bus bandwidth allocation. It is to be noted that, there is no evident proof regarding the maximum CPU core utilization for any benchmark program as it depends upon the degree of parallel and sequential codes. However, a certain gradient could be set for the maximum CPU core usage depending upon the benchmark application. If we observe Amdahl's law closely, then it could be figured out that the degree of sequential code has a fixed particular limit which cannot be changed to parallel form. Therefore, Amdahl's law is not taken into consideration by the programmers while partitioning the code into parallel and sequential form as the challenge lies in maximizing the parallel code by keep on changing the sequential code to minimalistic possible form. PAA is designed by taking into consideration both user oriented issues and system oriented issues, which ensures its high degree of parallelism along with fair and moderate bus bandwidth allotment. Some of the recent arbitration technique achieves fair bus bandwidth allocation up to some extent but fails to achieve maximum CPU utilization as the processor spends 95–96 % of their time idle and waits for cache misses to be satisfied [26, 27]. If we take Amdahl's law into reference, then it is assumed that total time taken to solve a problem is divided into Serial (S) component and parallel (P) component in such a way that with nt
                         threads the time to solution is S+ P /nt
                        . If the problem size has majority of the parallel component, then S can effectively be ignored and the time to solution could be approximated as P/nt
                        
                        . Thus, whenever a thread worker grows as O(n2) runtime complexity, the communication cost only grows as O(n), where O stands for order function. This is due to the fact that the communication cost mainly depends upon the algorithm itself which might require simple but time consuming machine instruction. Moreover, communication cost also depends upon the quality of the compiler, interpreter and the programming language used. Moreover, if we take mathematical norms into consideration, then O(n2) represents all those set of functions which in the long run do not grow faster than the function n2
                        . Therefore, n2 is considered to be an upper bound. If this situation prevails, then let f be an element of set O(n2), let C be the factor and n0
                         be an integer i.e. n, n0
                         now the following equation holds true:
                           
                              (3.1)
                              
                                 
                                    f
                                    
                                       (
                                       n
                                       )
                                    
                                    =
                                    C
                                    .
                                    
                                       n
                                       2
                                    
                                 
                              
                           
                        
                     

If C is the communication time, then the total time (T) can be expressed as:
                           
                              (3.2)
                              
                                 
                                    T
                                    =
                                    
                                       P
                                       
                                          n
                                          t
                                       
                                    
                                    +
                                    C
                                 
                              
                           
                        Where P stands for parallel component and nt
                         stands for number of threads.

For the above equation, as runtime complexity of P approaches O(n2) while C grows as O(n). Therefore, the new equation for the total time (T) could be expressed as [27]:
                           
                              (3.3)
                              
                                 
                                    T
                                    =
                                    
                                       
                                          
                                             t
                                             p
                                          
                                          
                                             n
                                             2
                                          
                                       
                                       
                                          n
                                          t
                                       
                                    
                                    +
                                    
                                       t
                                       c
                                    
                                    n
                                 
                              
                           
                        
                     

Where tp
                         is constant processing time and tc
                         is constant communication time.

Let us analyse the task distribution to multiple masters with the help of serial and parallel threading. For PAA, the scheduler manages the task distribution to its associated masters. Let t be the total number of task and m be the total number of masters to implement the task. Let b denote the branching factor or the ratio of threads generated during one iteration of PAA to the number of threads generated during the previous iteration of PAA. Let us assume that b remains constant throughout the task implementation and d denotes the maximum depth or the least-cost goal at which the task is achieved. Let x denote the maximum expansion of master nodes and n denote the number of threads generated by the total number of masters m. Now, it is to be noted that n = bx
                         and n ≥ m. As, ‘x’ denote the total expansion of the masters ‘m’, therefore x > logbP. Let us also assume that bx
                         ≈ P to perform task distribution to the respective masters. Now, the speedup of PAA could be computed as the ratio of the number of serial threads generated to the number of serial expansion of masters performed by PAA algorithm. According to Kumar and Rao [28], the speedup factor could be expressed as:
                           
                              (3.4)
                              
                                 
                                    S
                                    =
                                    M
                                    
                                    
                                       
                                          
                                             b
                                             d
                                          
                                          +
                                          
                                             b
                                             
                                                d
                                                −
                                                1
                                             
                                          
                                          +
                                          
                                             b
                                             
                                                d
                                                −
                                                2
                                             
                                          
                                          +
                                          
                                          …
                                          …
                                          .
                                          +
                                          
                                             b
                                             2
                                          
                                          +
                                          b
                                       
                                       
                                          
                                             b
                                             d
                                          
                                          +
                                          
                                             b
                                             
                                                d
                                                −
                                                1
                                             
                                          
                                          +
                                          
                                             b
                                             
                                                d
                                                −
                                                2
                                             
                                          
                                          +
                                          
                                          …
                                          …
                                          +
                                          
                                             b
                                             
                                                x
                                                +
                                                1
                                             
                                          
                                       
                                    
                                    *
                                    
                                       1
                                       
                                          2
                                          
                                             b
                                             x
                                          
                                       
                                    
                                 
                              
                           
                        
                     

It could be observed that as d increases, the left most fractional component of the above equation approaches 1 and thus can be ignored.

Now, let us analyse the other way round to implement PAA. Let us assume that arbiter distributes one task t to each processor P, so that each processor could perform at least one task. Since the thresholds of each processor to implement the task are not explored sequentially, therefore the first solution may not be an optimal solution. In order to ensure optimal solution, all processor with lower threshold must complete the task under first cycle of PAA. In this analysis, an assumption is made that a sufficient number of masters exists to achieve the task target without delay. Speedup of parallel task distribution could be computed using ratio of the number of non-target goal plus desired target goal threads to the number of threads generated by the masters to achieve the target. Powely and Korf [29] generate the above said ratio through the notion of left-to-right goal position a, which is considered as the fraction of the total number of master nodes which must be expanded. However, in order to reach the first node, a must be expanded to a compatible size [29]. Thus, speedup of parallel task distribution could be computed as:
                           
                              (3.5)
                              
                                 
                                    S
                                    =
                                    
                                       
                                          
                                             b
                                             
                                                d
                                                −
                                                1
                                             
                                          
                                          
                                             
                                                
                                                   (
                                                   
                                                      b
                                                      
                                                         b
                                                         −
                                                         1
                                                      
                                                   
                                                   )
                                                
                                             
                                             2
                                          
                                          +
                                          a
                                          
                                             b
                                             d
                                          
                                          
                                             b
                                             
                                                b
                                                −
                                                1
                                             
                                          
                                       
                                       
                                          a
                                          
                                             b
                                             d
                                          
                                          
                                             b
                                             
                                                b
                                                −
                                                1
                                             
                                          
                                       
                                    
                                    =
                                    1
                                    +
                                    
                                       1
                                       
                                          a
                                          
                                             (
                                             
                                                b
                                                −
                                                1
                                             
                                             )
                                          
                                       
                                    
                                 
                              
                           
                        
                     

To maintain moderate bus bandwidth, masters designed according to traffic behaviour have been used in the proposed work. In order to exploit the multiple cores of the processor, these masters have been synchronised to run parallel implementation of STREAM (Sustainable Memory Bandwidth in High Performance Computers) tasks which is synthetic benchmark to measure the sustainable bandwidth which in turn reduces latency up to a large margin [30]. Fig. 7
                         shows the flowchart of the proposed PAA technique.

On the basis of flowchart in Fig. 7 and the description in the Sections 3.1 and 3.2, the algorithm of the proposed PAA algorithm is outlined as follows:
                           
                              1.
                              Initially, the parent master/ D_Type master makes a request to the arbiter with the highest priority.

Arbiter acknowledges high priority master and grants permission to D_Type master, then D_Type master starts copying STREAM data to the cache using STREAM_copy.

Similarly, DR_Type master sends request to the arbiter to implement another set of operation of STREAM_scale parallely with STREAM_copy.

If arbiter grants permission, then DR_Type master starts fetching data from cache and starts implementing STREAM_scale in parallel with STREAM_copy. If arbiter does not grant permission, then DR_Type master sends the request again to the arbiter.

Similarly, NDR_Type master sends request to the arbiter to implement STREAM_sum and STREAM_triad operation.

If arbiter grants permission, then NDR_Type master synchronises with DR_Type master to increase the degree of parallelization and implements STREAM_sum and STREAM_triad operation. If arbiter does not grant permission, then NDR_Type master sends the request again to the arbiter.

The arbiter keeps high priority for D_Type master, as this master is considered as the parent master. Therefore when D_Type master request for arbiter access, then arbiter has to grant permission to it or else a real-time violation occurs. on the second hand, arbiter gives second priority to DR_Type master. However, DR_Type master synchronises with D_Type master and executes parallely with D_Type master. Finally, arbiter gives third priority to NDR_Type master, which synchronises with DR_Type master and executes with DR_Type master parallely. Following pseudo code in Fig. 8
                         shows the priority set by the arbiter.

The way how the masters according to their traffic behaviour gets implemented using PAA will be discussed in detail. Fig. 9
                         shows the prototype of the proposed architecture.

The three masters are required to synchronise with each other using the arbiter, so as to enable parallel implementation. Here, each master initiates independent request to the arbiter to access the bus. The arbiter ensures that CPU cores are utilised to its maximum with moderate bus bandwidth allocation for its masters. Following subsections describe the method using which the three masters i.e. D_Type, DR_Type and NDR_Type are implemented in synchronised mode to enable parallel implementation of STREAM modules.

The D_Type master is considered as the blocking master and is placed in such a way that it cannot be interrupted by any masters. In order to facilitate D_Type master with all its necessary features, this master is placed in the SystemC thread region, so that it can utilise the SystemC threads. This master has got no extra real-time requirements, as the next request is issued at the time depending on the finish time of the current request. This master is designed according to the typical SystemC thread process, as any method is invoked, it executes completely until it returns a value. The data sharing attribute of this SystemC thread is kept private, which means that each thread possess a local copy and uses it as a temporary variable. The code lying within the SystemC thread region is designed according to the clocked thread process to get a better synthesis result. This method has got an edge over thread process in terms of the speed of implementation. The task of D_Type master is to copy data from primary array to the cache array without being interrupted by other masters. However, other masters can synchronise with D_Type master whilst the copying of data from primary array to the cache array. The pseudo code in Fig. 10
                            shows the working of D_Type master. N is the total STREAM data size, a[N] is the STREAM data in the primary array, whereas c[] is the empty cache array. Since count1 is the clocked SystemC counter, therefore SystemC thread is enabled to implement the for loop, so that the STREAM data could be copied to the cache array.

This master is also considered as the blocking master and is similar if compared with D_Type master except that they have an extra real-time requirement. In order to function this module in the most appropriate manner, the data within the parallel region is shared, as the code is being executed parallely in the SystemC thread region of D_Type master as well as the OpenMP region of DR_Type master. In order to satisfy the extra real-time requirement, this master has two kind of real-time parameter. One is the independent real-time parameter and the other is dependent real-time parameter. The independent real-time parameter has to finish its execution within a specific count value whereas the dependent real-time value has to synchronise with NDR_Type master and has to finish its execution until the next request is issued by NDR_Type master. The synchronization for the code (dependent real-time parameter) lying in the OpenMP region is done using the parallel mode, which means that the code lying within the OpenMP region will be executed parallely with other SystemC threads. DR_Type master synchronises with D_Type master to implement STREAM_scale parallely with STREAM_copy. Moreover, DR_Type master also synchronises with NDR_Type master to implement its dependent real-time parameter using which STREAM_scale implements parallely with NDR_Type master. The pseudo code in Fig. 11
                            shows the working of DR_Type master. Here, midval is the threshold value to initiate NDR_Type master so that DR_Type master could implement dependent real-time parameter and scalar is a constant integer value. Count2 is the OpenMP counter which implements for loop to scale the STREAM data. However, if count2 equals midval, then dependent real-time parameter of DR_Type master starts its execution.

The initiation of the NDR_Type master does not depend upon the finish time of the DR_Type master. However, request to initiate NDR_Type master is issued by DR_Type master to implement its dependent real-time parameter. The function of NDR_Type master is to implement STREAM_sum and STREAM_triad parallely with DR_Type master by synchronizing with each other. This implementation is done in the OpenMP region and high degree of parallelization is maintained using omp_parallel function. The reason it implements two type of STREAM task is due to its high granularity and less number of synchronization with other masters. Therefore the bus bandwidth would also be optimised. The pseudo code in Fig. 12
                            shows the working of NDR_Type master. Here, a single OpenMP counter (Count3) is maintained to implement two parallel for loops shown in line 10 and 15 so that STREAM_sum and STREAM_triad could be implemented parallely.

@&#RESULTS AND DISCUSSION@&#

In order to analyse the impact of multi-threading on masters according to their traffic behaviour using PAA, an experiment has been conducted using SystemC whose libraries were ported in an integrated development environment composed of suitable profiler tools to measure the CPU usage and the bandwidth using SystemC buffers. It is to be noted that, SystemC is used as a tool in the proposed experiment to manage and analyse the bus bandwidth using SystemC threads. However, researchers are free to use alternative tools and its associated libraries to analyse the bus bandwidth. For the proposed arbitration technique, visual studio 2010 (EE) has been used to implement SystemC (version 2.2.0 from OSCI standards) using AMD Athlon ™ II X2 260 processor with 2048 KB dedicated L2 cache.


                        Fig. 13
                         shows an average CPU utilization for the threads running for the multiple functions of STREAM [30] without using PAA technique in a standalone mode using SFP technique. In this case the race conditions occur because the programmer does not anticipates the fact that a thread should be preempted at any position due to sequential processing of all the four functions of STREAM (STREAM_copy, STREAM_scale, STREAM_sum and STREAM_triad). This might allow another thread to read the block of code first. However the use of threads requires some precaution in communication libraries in order to avoid the race condition when the threads access the library concurrently [12]. This type of process does not utilise the CPU cores to the maximum and as the number of threads increases the bandwidth becomes the major barrier for the performance. Processor architects have to trade-off the speed versus a lot of features it may offer. In order to maximise the instruction per clock cycle, the instruction, operands and destination must be accessible at the same time rather than sequentially. As the CPU are getting faster much more quickly, so the speed of the systems main memory may also become a limiting factor in terms of throughput after a certain threshold point. Moreover, while implementing this sequential process, CPU suffers a lot of cache miss which results in high latency. Table 2 shows a complete analysis of maximum CPU utilization in case of non-preemptive environment of STREAM bench mark in SFP mode where a set of 25 readings have been taken to get the accurate values with precision. It can be clearly observed that, the second logical core of the CPU has not been used at all by the stand-alone STREAM benchmark code. This shows high degree of sequential programming implementation. In these 25 readings, 48 % was the best value record for CPU utilization, and this best value was obtained 15 number of times out of 25 times, thus the average value for 25 readings was recorded.


                        Fig. 14
                        
                         shows the implementation of COMET benchmark by Shiva et al. [13] in safety critical embedded system. The two objective functions fCPUusage
                         and fmakespan
                         calculates the average percentage of CPU usage and scheduling length. The two objective functions are implemented in two mode i.e. non-parallel and parallel version. Since it is a safety critical embedded system, therefore the scheduling length and maximum CPU usage is fixed.

From the above graph in Fig. 14, if we notice the termination time of the two process in both non-parallel and parallel version, then it could be well understood that the level of synchronization between fCPUusage
                         and fmakespan
                         is high if compared to synchronization between fCPUusage
                         and fmakespan
                         in non-parallel version. However, the drawback of their developed algorithm is that the programmer does not get much room to parallelise the codes.

The following graph in Fig. 15
                         shows the CPU usage and the execution time for various algorithm implemented using Rodina benchmark by Che et al. [14] as discussed in literature review.

It is to be noted that for Rodina benchmark, some algorithms worked well in terms of CPU usage and execution time. Since Rodina application have a large range of problem size, therefore it suffers cache misses due to the lack of thread synchronization and is not suitable for big scale data application [14].

Essence of high performance computing lies within the concept of parallel programming. Fig. 16
                         shows an average CPU utilization for the threads running the multiple modules of STREAM using the proposed arbitration technique. In this case all the three master of D_Type, DR_Type and NDR_Type are synchronised with each other so as to enable parallel implementation. These masters implement the four functions of STREAM (STREAM_copy, STREAM_scale, STREAM_sum and STREAM_triad), where STREAM_copy is implemented by D_Type master, STREAM_scale is implemented by DR_Type master and STREAM_sum along with STREAM_triad is implemented together by NDR_Type master. Therefore, virtually NDR_Type master acts as two master i.e. one which implements STREAM_sum and the other which implements STREAM_triad. Thread safety is one of the major criteria of multi-threading support for these masters. This means that communication in a multi-threaded application can be performed in multiple threads. Appropriate techniques should be used to utilise the multiple cores in order to make non-blocking communication primitives to progress in the background. Various thread-scheduling policies try to achieve optimal utilization of the CPU as well as the bus bandwidth during each quantum. If Fig. 16 is compared with Fig. 13, then it can be seen that, not only the CPU usage is utilised to its maximum using optimal bandwidth utilization as the threads are running in a preemptive environment but it also shows a decrement in the system latency. Moreover, if Fig. 16 is compared with Figs. 14 and15 in terms of CPU utilization, then also PAA holds good than existing arbitration techniques implemented using Rodina and COMET benchmark. Table 3
                         shows a complete analysis of maximum CPU utilization for the case of PAA technique, where 25 readings have been taken to find the best value for the percentage of CPU core usage. It can be clearly observed that, the second logical core of the CPU has been used up to a maximum possible level by the PAA technique to implement the STREAM bench mark. This shows high degree of task parallelism. In this 25 set of readings, 77 % was the best value recorded for CPU utilization, and this best value was obtained 17 number of times out of 25 times, thus the average value of 25 readings was taken.

Another experiment was made using PAA technique by keeping a high context core-switching rate. Table 4
                         shows a complete analysis of maximum CPU utilization for the case of PAA technique using high context core switching rate, where 25 readings have been taken to examine, so as to get accurate and precise values. In this 25 set of readings, 81 % was the best value recorded for CPU utilization, and this best value was obtained 16 number of times out of 25 times, thus the average value for the 25 readings was recorded. Fig. 17
                         shows the average CPU utilization for PAA in high context core-switching mode. In this case, it is clear that latency is higher if compared with Fig. 16 due to high rate of core-switching.

An experimental study has been conducted on the bus bandwidth requirements which is set automatically for the masters according to their traffic behaviour, where M1,M2,M3 and M4 are the set of masters taken. D_Type master is the first type of master (M1) which implements STREAM_copy, DR_Type is the second master (M2) which implements STREAM_scale and NDR_Type is the third master which implements STREAM_sum along with STREAM_triad. Therefore, virtually NDR_Type master acts as M3 and M4 i.e. one which implements STREAM_sum and the other which implements STREAM_triad. The masters are designed in such a way that they require bandwidth according to the data traffic for the four functions of STREAM. STREAM benchmark computes bandwidth for each master using its inbuilt functions by the following equation [30]:
                           
                              (4.1)
                              
                                 
                                    A
                                    l
                                    l
                                    o
                                    t
                                    t
                                    e
                                    d
                                    
                                    b
                                    a
                                    n
                                    d
                                    w
                                    i
                                    d
                                    t
                                    h
                                    =
                                    R
                                    e
                                    q
                                    u
                                    i
                                    r
                                    e
                                    d
                                    
                                    b
                                    u
                                    s
                                    
                                    b
                                    a
                                    n
                                    d
                                    w
                                    i
                                    d
                                    t
                                    h
                                    /
                                    T
                                    o
                                    t
                                    a
                                    l
                                    
                                    b
                                    u
                                    s
                                    
                                    b
                                    a
                                    n
                                    d
                                    w
                                    i
                                    d
                                    t
                                    h
                                 
                              
                           
                        
                     

Since the masters are designed according to the traffic behaviour, therefore a comparison in the bandwidth analysis is done between the proposed PAA technique and RT_Lottery algorithm. For the RT_Lottery algorithm proposed by Chen et al. [12], the weight function (W) was used to decide the bus access priority by each master. The weight function was calculated using the following equation:
                           
                              (4.2)
                              
                                 
                                    W
                                    =
                                    R
                                    e
                                    q
                                    u
                                    i
                                    r
                                    e
                                    d
                                    
                                    b
                                    a
                                    n
                                    d
                                    w
                                    i
                                    d
                                    t
                                    h
                                    /
                                    M
                                    a
                                    x
                                    i
                                    m
                                    u
                                    m
                                    
                                    b
                                    a
                                    n
                                    d
                                    w
                                    i
                                    d
                                    t
                                    h
                                    
                                    
                                       (
                                       
                                          S
                                          e
                                          t
                                          
                                          b
                                          y
                                          
                                          u
                                          s
                                          e
                                          r
                                       
                                       )
                                    
                                 
                              
                           
                        
                     

This weight function was further analysed or tuned to check the extra bandwidth requirement for each master. To implement RT_Lottery algorithm, 6 masters were used to process high beat burst data [12]. The bandwidth requirement for each master in the case of RT_Lottery algorithm was set manually. However, manual bandwidth allotment is unfavourable for parallel implementation of masters, as conflicts may arise among the masters in terms of bandwidth sharing [12].Table 5
                         shows the allotted bandwidth values for the masters implemented using RT_Lottery algorithm [12].


                        Table 6
                         shows the allotted bandwidth values for the masters implemented using PAA technique. The bandwidth requirement for PAA was set automatically by the arbiter. It can be clearly observed from both the tables that variation in the bandwidth values are less for the proposed PAA technique, if compared to RT_Lottery algorithm and each master is given a fair share of bandwidth. Moreover, since the different masters are set for moderate bandwidth allocation automatically, therefore it is assumed by the STREAM benchmark that the distributed bandwidth value among 4 masters should be near the range of 25 % (25 * 4=100 % total bandwidth). However, in the real-time scenario, this bandwidth value for each master varies from the range of 25%. The most probable reason for this is due to the background system process execution.


                        Table 7
                         shows the automatic allotted bandwidth values for the homogenous masters (not designed according to traffic behaviour) implemented by Li et al. [15] by the method of AA algorithm to process high beat burst of data generated using verilog code. Bandwidth distribution analysis were done for various arbitration technique like SFP [Implemented by Li et al. [15]], RR [Implemented by Li et al. [15]] and AA algorithm [Implemented by Li et al. [15]]. In this case also, it is assumed that each master should require 25 % of the bus bandwidth, however the actual value varies.


                        Fig. 18
                         shows the graphical form for the distributed bandwidth values in Tables 6 and 7. Average bandwidth fluctuation (B.Avg) for different masters (M) is calculated using the following equation [30]:
                           
                              (4.3)
                              
                                 
                                    B
                                    .
                                    A
                                    v
                                    g
                                    =
                                    
                                       [
                                       
                                          B
                                          F
                                          
                                             (
                                             
                                                25
                                                %
                                                −
                                                
                                                   M
                                                   1
                                                
                                                %
                                             
                                             )
                                          
                                          +
                                          B
                                          F
                                          
                                             (
                                             
                                                25
                                                %
                                                −
                                                
                                                   M
                                                   2
                                                
                                                %
                                             
                                             )
                                          
                                          +
                                          …
                                          …
                                          .
                                          B
                                          F
                                          
                                             (
                                             
                                                25
                                                %
                                                −
                                                
                                                   M
                                                   n
                                                
                                                %
                                             
                                             )
                                          
                                       
                                       ]
                                    
                                    /
                                    n
                                 
                              
                           
                        where n = number of masters and BF = bandwidth fluctuation.

From the Eq. 4.3 and the graph in Fig. 18, it is observed that in case of AA algorithm the average bandwidth fluctuation is ± 1.9%, whereas in the case of SFP algorithm its ±7.99% [15], in case of PAA its ± 1.35% and the minimum is in the case of RR algorithm that is ± 0.15% due to its non-preemptive manner of execution [15]. Therefore, in case of RR algorithm, there is no chance for the masters to suffer starvation, and it ensures that each master gets most fair bus bandwidth by following a sequential method of programming. However, the factor of latency has not been taken into consideration [15].

If seen in terms of latency which is considered as the time delay occurred in the system, the proposed PAA technique comes out to be superior if compared with implementation of STREAM-Standalone mode using SFP technique. Moreover, the proposed technique lies under preemptive execution category, whereas the STREAM-Standalone mode lies under non-preemptive execution category. Graph in the Fig. 19
                         shows latency variation during the execution of masters for the proposed PAA technique using moderate and high context switching which is compared with STREAM-Standalone mode for four quarters of total CPU execution.

@&#CONCLUSION@&#

An attempt has been made on maximizing the CPU utilization cores to achieve high degree of task parallelization by keeping moderate bandwidth allocation factor using the proposed PAA. Compared with other arbitration techniques, PAA has higher average CPU core usage of 77%. In terms of bandwidth fluctuation, PAA shows minimalistic fluctuation of just ±1.35%. PAA could be used in high performance applications where ever task parallelization is essential. In a nutshell, PAA technique helps in achieving a fair bandwidth allocation for the critical requirements by utilizing the CPU cores to its maximum to achieve high degree of task parallelization. It also reduces the system latency up to an adequate margin through synchronization of masters. If analysed in terms of latency level, then it is observed that the proposed arbitration technique shows low level of latency if compared with STREAM-Standalone mode using SFP technique. Nonetheless, PAA technique has to be tested on various intense image processing application for enhanced performance evaluation.

@&#ACKNOWLEDGEMENT@&#

This research is supported by the School of Electrical and Electronic Engineering of Universiti Sains Malaysia. The authors would like to acknowledge the Institute of Postgraduate Studies (IPS), Universiti Sains Malaysia for the Global Fellowship [USM.IPS/USMGF(06/14)] financial support to carry out this research.

@&#REFERENCES@&#

