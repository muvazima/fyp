@&#MAIN-TITLE@&#Automatic classification of personal video recordings based on audiovisual features

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A system for automatic classification of personal videos is presented.


                        
                        
                           
                           Personal video recordings are classified into 24 categories frame by frame.


                        
                        
                           
                           Features derived from both audio and image data are used to classify.


                        
                        
                           
                           The system learns the most appropriate parameters and classifiers for the task.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Feature extraction

Personal video recording

Video classification

Meta-classification

@&#ABSTRACT@&#


               
               
                  The aim of the present work is to design a system for automatic classification of personal video recordings based on simple audiovisual features that can be easily implemented in different devices. Specifically, the main objective is to classify frame by frame personal video recordings into 24 semantically meaningful categories. Such categories include information about the environment like indoor or outdoor, the presence or absence of people and their activity, ranging from sports to partying. In order to achieve a robust classification, features derived from both audio and image data will be used and combined with state of the art classifiers such as Gaussian Mixture Models or Support Vector Machines. In the process, several combination schemes of features and classifiers are defined and evaluated over a real data set of personal video recordings. The system learns which parameters and classifiers are most appropriate for this task.
                  The experiments show that the approach using specific classifiers for audio features (Mel-Frequency Cepstral Coefficients (MFCCs)) and image features (color, edge histograms), and using a meta-classification combination schema attains significant performance. The best performance obtained over the different approaches evaluated gave a promising 
                        
                           f
                           -
                           measure
                        
                      larger than 57% in average for all the categories and larger than 73% over diverse categories.
               
            

@&#INTRODUCTION@&#

Nowadays, due to the development of new and low-cost audiovisual technologies (such as video cameras, mobile phones, photo cameras, etc.) and the wide use of Internet, the audiovisual content available is huge. Furthermore, the amount of personal audiovisual content is often large enough to demand an automatic system to help classify personal recordings. Thus, diverse methods of indexing, classification and retrieval of audiovisual information have emerged. Most of the works in this topic have focused on the classification of high quality TV videos, being restricted to a small number of categories (sport, cartoon, news, commercial and music) [1,2].

Audiovisual information contains audio features as well as image features. The audio features normally used are the well known Mel-Frequency Cepstral Coefficients (MFCCs) [3]. Regarding video features, image features can be used [4,5], but also motion features [6] or a combination of both [2,3].

On the other hand, there exist several classification schemes that are used to combine the audiovisual features like Gaussian Mixture Models (GMMs) [2,3], Hidden Markov Models (HMMs) [2], Support Vector Machines (SVMs) [7,8] and combination schemes [9].

At the present time, modern personal computers allow the development of complex systems to classify audiovisual content as well as new cloud computing techniques. Personal video recordings have several particular features that make the classification task much more complex than with TV Videos or movies. The videos are often of low quality and they present unclear scenes. In addition, in order to perform a useful classification of the personal video recordings, a wide range of categories is required. In this way, Lee et al. [10] investigate a method for automatic classification of consumer video clips based on their soundtracks, using a set of 25 overlapping semantic classes (e.g. dancing, sports, sunset, music, wedding, etc.). Chang et al., in [11], deal with the same problem but using visual information solely, whereas Sugano et al., in [12], present a genre classification system for home video recordings that considers 5 categories based on audiovisual features.

In this paper, we will deal with the combination of audio and visual features, as well as with the combination of different kinds of classifiers to achieve an efficient frame by frame classification of personal video recordings into semantically meaningful categories. Such categories include information about the environment like indoor or outdoor, the presence or absence of people, and their activity, like sports or partying. The designed system learns which parameters and classifiers are most appropriate for this task.

Summing up, we will use 24 categories, in contrast to most works in which the classification considers a set of around 5 categories [1,3,7,13]. Also, we will work with a real data set of personal video recordings composed of 366 videos manually labeled at frame level and the classification will be done frame by frame.

The usage of frame level annotation and classification and the utilization of 24 different semantic categories constitute a main novelty of the proposed scheme. Other approaches commonly label the full videos in the database as a whole [5,12,14,15] and the number of different classes employed is smaller. For example, only 5 categories are considered in [3,7,12,16].

Also, unlike other works, we will be working on a data set of real personal video recordings instead of professional videos [1,2]. A comparative performance analysis is included to assess the difference in performance between several combinations of features and classifiers for the specific task described: frame by frame classification of personal video recordings. Another important objective of the designed system is its feasibility to work in as many devices as possible, so the selected audiovisual features should be kept as simple as possible.

This paper is organized as follows: in Section 2, we present the audio and video features selected. In Section 3, we describe the types of classifiers selected for the study, as well as several combination methods for features and classifiers. In Section 4, the evaluation results over a real data set are shown. Finally, Section 5 draws some conclusions on the work presented.

One of the main stages for the video classification task is the selection and extraction of numerical features from video sources that can be used to characterize a frame or an audio segment.

In this section, we review the features to use for video classification. Then, we select some of the more appropriate features, for classification into the 24 selected categories. The features will be taken from both audio an image domains and their definition will be based on the performance showed in previous works [17,18] and their easiness to be obtained and used. Since one of the objectives of the proposed scheme is that it can be easily implemented in different devices, we have focused on the simplicity and usability of the features. Because of this, we have avoided the selection of a large number of features, local features [5,19–21] and the implementation of feature selection algorithms [22,23]. However, we will deal with audio and image features, as well as with combination.

There is a huge variety of audio features [17,18,24,25] but some of them give poor separability among different classes or they are depending on the language, and the inclusion of these features will not improve performance. We could use the exhaustive combination method or a feature selection algorithm [22,23] in order to find the best feature vector, but the computational burden of this is huge.

Finally, the decision on the audio features was done according to previous works in Sony Corporation and taking into account the features more widely used in automatic speech recognition (ASR) [1,26]. The aim is to select a set of good audio features with low computational complexity to recognize the presence of humans. So, we decided to use the Mel-Frequency Cepstral Coefficients (MFCCs) due to their proved excellent performance (see [27,28]). Furthermore, MFCCs are widely used features as audio features in video analysis and classification [17,18].

We also use the first order derivative of the MFCCs (delta coefficients) and their second order derivatives (acceleration coefficients) in order to incorporate the ongoing changes over multiple frames [29]. The frames are defined like in [30]. In order to discard irrelevant information, we use telephone quality audio. This choice also helps to bound the computational load.

The classifier will receive a concatenation of the static (MFCCs) and dynamic features (delta and acceleration coefficients).

There exist a vast variety of image features that can be used for image and video classification [17–19,31], being more computationally complex than audio features. So it is important to carefully select the more significant ones for the classification of personal video recordings. Among the different types of image features, color based features are widely used [17,18].

Therefore in this paper, among the different color based features [17], color and edge histograms are used to gather the visual information (Sections 2.2.1 and 2.2.2). The use of texture was discarded since this type of feature shows very poor performance because of the huge variety of textures in individual frames in personal video recordings and the computational burden involved. The use of local features, such as the scale invariant feature transform (SIFT) [5,19], and the use of human action recognition features, such as the spatio-temporal steerable pyramid (STSP) [21], have also been dismissed because of their computational complexity. Also, note that these last features presented must be dismissed because the 24 video classification categories selected in this paper correspond to generic scenes which do not agree with the categories designed for the classification of human actions like walk, stand, sit, situp, etc [21].

Color histograms are used to compare images in many applications [32,33]. Their main advantages are related to their efficiency and insensitivity to small changes in the camera viewpoint.

Among the several color representation models available, the HSV (Hue, Saturation and Value) color space [34] has been selected. The HSV color space needs to be quantized into several bins. We decided to use the quantization scheme proposed in [4] where the color space is quantized into 36 non-uniform colors. After the analysis of the features processed, we observed that very few pixels were classified in the bins representing high saturation. This means that colors with high saturation are quite unusual in the personal video recording analyzed. In order to avoid the presence of empty bins we combined them with samples with lower saturation. The result was a descriptor formed by 20 bins.

In addition, personal recordings often show very different aspect in the upper and the lower part (i.e. sky-nature, sky-sea, etc.). With the aim of taking into account this kind of spatial information, the image was split vertically into an upper and a lower part. Consequently the HSV feature used in this work has 40 bins: 20 bins from the upper part of the frame and 20 bins from the lower part of the frame.

Edges constitute and important feature representing image content. One way to use such important feature is by means of the edge histogram. The edge histogram of an image represents the frequency and the directionality of its changes of brightness [35].

Specifically, we will use the edge distribution used in MPEG-7 [35]: the Edge Histogram Descriptor (EHD). Since it is important to keep the size of the descriptor as compact as possible, for the efficient storage of the metadata, the MPEG-7 edge histogram is designed to contain only 80 bins that describe the local edge distribution. These 80 histogram bins are the only standardized semantics for the MPEG-7 EHD. Basically, the EHD represents the distribution of 5 types of edges (vertical, horizontal, 45 degrees diagonal, 135 degrees diagonal and non-directional edges) in an area named sub-image. The sub-image is defined by the division of the image into 4×4 non-overlaping blocks. The details of MPEG-7 EHD can be found in [35].

In this section, we deal with the design of the classifiers in order to get the best results from the vector of features. First, we start with the general design cycle of classifiers based on machine learning (Section 3.1). After presenting the basic schema of the classifiers, we continue our study, in Section 3.2, combining different kinds of features in order to find complementary information between the different sources of information to improve the performance of the classifier. In Section 3.3, instead of combining the features, we develop several schemes of classifier combinations aimed at using specific classifiers for the different kinds of features and combining their results in an optimum way. Finally, in Section 3.4, an innovative technique of classifier combination is developed by means of meta-classifiers.

First, we define the general design cycle of every individual classifier. The type of classification used is binary classification based on supervised learning. This choice is due to the facility of implementation of both probabilistic and discriminative methods and the easily achieved multi-label classification by using a binary classifier per class [36].


                        Fig. 1
                         shows the different elements and phases of the design cycle that will be presented in the following sub-sections. First, in Section 3.1.1, the set of categories considered to label the personal video recordings is defined. In Section 3.1.2, the set of personal video recordings used for training and testing is presented, together with some characteristics related to the feature extraction process. Next, the statistical model based on machine learning methods for classification will be chosen. This model will be trained to get adapted to our video data. In Section 3.1.3, we analyze two of the state of the art machine learning models, Gaussian Mixture Models (GMMs) and Support Vector Machines (SVMs). In Section 3.1.4, we observe the probabilities or scores GMMs and SVMs attain. In Section 3.1.5, the usage of the probabilities or scores provided by the model, in order to get the best classification results, is analyzed. Finally, in Section 3.1.6, the metrics used for the evaluation of the different approaches are described.

The set of categories used for video classification is very variable across different works [11,37,38]. The set of 24 distinct classes, selected to categorize the personal video recordings, is based on the human presence, the geographic situation and the observed activity. Therefore, all the considered categories are objective, that is subjective categories like happiness, sadness, anger, disgust, fear and surprise [18], bright, dark, warm, cold, sensations, etc. [39], are not included. The complete list of categories follows:

Sea (scenes with great water presence), Sand (v.gr. beach or deserts), Snow (v.gr. people skying, snow boarding, etc.), Mountains, Green nature, Outdoor (scenes that are not indoor), Natural daylight, Indoor (note that this category is actually the opposite of “Outdoor”, so it is kind of redundant have both categories, outdoor and indoor, in our list. However, we decide to let the classifiers perform the classification into these two specific categories in order to be able to evaluate the behavior of the classifiers with these categories. Thus, this choice allows, for example, to know if it is easier to identify an outdoor or an indoor scene.), Night, Sunset (it also includes the sunrise), Artificial light, Party (people gathering together at an event, including grill parties with neighbors, birthday parties, etc.), Urban, Individual buildings (v.gr. recordings of cathedrals), Sports, Children, Small groups (small groups of people or individual persons. v.gr. filming friends and family.), Crowd, Voice only (when you can hear one or more persons speaking or screaming, but you do not see them or you see only their hands or back of the head, v.gr. driving or seminars), In public buildings (v.gr. train station, airport, museum, gym, etc.), At home, Performance (including in between shots of the audience. v.gr. on a stage, theater, dance, etc.), Restaurants (including bar, pub, both outdoors and indoors), In vehicle (only when the camera is in the motorized vehicle. v.gr. car, ship, airplane, train, bus, etc.).

These categories are used to label frame by frame all the videos. It must be noted that a frame can be classified into more than one category (v.gr. Party and Artificial light).

The training and the evaluation will be done using the personal video recordings repository provided by Sony Stuttgart Technology Center [40]. This repository contains 366 videos for a total of 8h 40min of recording time.

Since the set of videos is limited, cross-validation 
                           [41] was used in training-evaluation in order to develop models as general as possible. The S-fold cross-validation scheme divides the available data into S groups. In our approach, we chose S=3. Then, S-1 groups are used for training and one group for testing. This procedure is repeated for all the S possible selections of the testing group.

Most of the personal video recordings are in .avi format but there is also a set of video files in .mpg format. The image features are extracted at a sampling frequency of 5 fps. This choice is low enough so as not to increase the computational cost unnecessarily and, at the same time, it is high enough so as not to miss information for categorization; note that it is not realistic to consider a scene, v.gr. sports, shorter than 0.2s.

GMM is a probabilistic method that has been widely used in classification tasks with success [1]. In the Gaussian Mixture Model classifier the probability density function (pdf) of each class is assumed to be a mixture of a specific number K of multidimensional Gaussian distributions. The iterative Expectation Maximization algorithm (EM)[41] is commonly used to estimate the parameters of each Gaussian component and the mixture weight.

SVM is a discriminative method successfully used in a variety of pattern recognition tasks [42,43]. SVMs are built by mapping the input patterns into a higher dimensional feature space by using a nonlinear transformation (kernel function). Then optimal hyperplanes are built in the feature space as decision surfaces between classes.

An SVM classifier finds the hyperplanes that separates the classes in the training data with a maximal margin.

In our implementations, we make use of the LIBSVM library with MATLAB [44] and the parameter selection is done according to the recommendations in [44].

The way in which the probabilities attained by the models are calculated differently depending on its type.
                              
                                 •
                                 Gaussian Mixture Model: The model directly obtains the probabilities in the log-domain.

Support Vector Machines: LIBSVM provides with the possibility of directly getting the probabilities [44].

The inputs to this block are the probabilities of the positive and negative class for every frame, v.gr. the probability of the classifications or not, of a frame in a certain class. The classification method will use these probabilities in order to attain the best possible classification results.

After evaluating several classification methods, the Viterbi path based smoothing scheme has been selected because it is the one that provides the best results. Viterbi path based smoothing finds the best state sequence for the sequence of observations by using a Viterbi state trellis [45].

The evaluation is done by using the 
                              
                                 f
                                 -
                                 measure
                              
                           : 
                              
                                 (1)
                                 
                                    f
                                    -
                                    measure
                                    =
                                    2
                                    ·
                                    
                                       
                                          Precision
                                          ·
                                          Recall
                                       
                                       
                                          Precision
                                          +
                                          Recall
                                       
                                    
                                 
                              
                           where:
                              
                                 •
                                 Precision is the probability of classification of a sample in a given class if it actually belongs to that class.
                                       
                                          (2)
                                          
                                             Precision
                                             =
                                             
                                                
                                                   
                                                      
                                                         t
                                                      
                                                      
                                                         p
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         t
                                                      
                                                      
                                                         p
                                                      
                                                   
                                                   +
                                                   
                                                      
                                                         f
                                                      
                                                      
                                                         p
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    where 
                                       
                                          
                                             
                                                t
                                             
                                             
                                                p
                                             
                                          
                                       
                                     stands for the number of true positives and 
                                       
                                          
                                             
                                                f
                                             
                                             
                                                p
                                             
                                          
                                       
                                     for the false positives.

Recall is the probability of positively classifying a sample:
                                       
                                          (3)
                                          
                                             Recall
                                             =
                                             
                                                
                                                   
                                                      
                                                         t
                                                      
                                                      
                                                         p
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         t
                                                      
                                                      
                                                         p
                                                      
                                                   
                                                   +
                                                   
                                                      
                                                         f
                                                      
                                                      
                                                         n
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    where 
                                       
                                          
                                             
                                                f
                                             
                                             
                                                n
                                             
                                          
                                       
                                     stands for the number of false negatives.

Therefore, 
                              
                                 f
                                 -
                                 measure
                              
                            is the harmonic mean of precision and recall it represents the trade-off between precision and recall [46].

The previous section showed the basic design of the classifier using a single kind of image or audio features. Now, we address the usage of the combination of both types of sources of information in order to extract complementary useful information from both of them for our classification task. Thus, we will combine the features from the audio and image sources. In this way, during the training phase, the model will extract cross-domain information that will help in the evaluation phase.

For example, in a video scene representing a group of persons in a restaurant, the audio will provide with the necessary information to know that there are persons in the scene and the image will be able to provide with complementary information to know that the scene takes place in an indoor room (artificial light, walls with parallel edges, etc.). However, this approach has some potential problems that need to be observed: the model is not optimized for every specific source of information. The dimension of the feature vector can grow too large, leading to problems related to the curse of dimensionality [41]. The curse of dimensionality problem makes reference to the necessity of a large amount of training data for feature vectors with high dimensionality, since the number of variables increase. This relationship between the dimension of the feature vector and the observation space to be covered with the training data is exponential.

Among the different options to combine the feature vectors studied in Section 2, the best results were attained using the concatenation of the color histogram, edge histogram and MFCCs.

These three different features were accordingly synchronized and scaled. The dimension of the combined feature vector is 195. SVM was selected for the classification because of the very high dimensionality of the feature vector. The results are analyzed in Section 4.

In the feature combination schema, a single model is used to train and perform the classification using both audio and image information sources. In our case, we chose SVM [42,43] for that task. This implies that we lose the capability of optimizing the classifier for every different type of information (v.gr. to use GMM [1] for audio and SVM [42,43] for image features). Precisely, the classifier combination technique is strong in this sense.

By using this approach, we will select the best classifier for every type of information source and combine their results in order to perform the classification with the complete set of information sources.

Therefore, the advantage of this approach is that it considers the optimum classifier per type of information source, additionally, it makes use of feature vectors of reduced dimensionality, diminishing the problem of the curse of dimensionality.

Nevertheless, this approach presents some clear drawbacks because it is not possible to get the cross-domain information from the different information sources in the training phase. Different types of models imply different types of parameter optimization steps increasing the complexity of both the overall process and the algorithm used to combine the decisions from the different classifiers.


                        Fig. 2
                         shows the schema used in this paper for score combination.

As shown in Fig. 2, the SVM model deals with the image features and the GMM employs audio features. The scores provided by both models are post-processed: the scores are normalized (since the outputs are in different formats, as we saw in Section 3.1.4) and and combined using weights optimized per class. So, the combination of scores is calculated by using the following expresion:
                           
                              (4)
                              
                                 
                                    
                                       P
                                    
                                    
                                       C
                                    
                                 
                                 =
                                 α
                                 ·
                                 
                                    
                                       P
                                    
                                    
                                       SVM
                                    
                                 
                                 +
                                 β
                                 ·
                                 
                                    
                                       P
                                    
                                    
                                       GMM
                                    
                                 
                              
                           
                        where α and β are the weights per class. In order to simplify the scheme, only one variable W is used, such that: 
                           
                              α
                              =
                              W
                           
                         and 
                           
                              β
                              =
                              (
                              1
                              -
                              W
                              )
                           
                        . In this way, 
                           
                              W
                              =
                              1
                           
                         means only image features are used in the classification (with a classifier based on SVM), whereas 
                           
                              W
                              =
                              0
                           
                         means that only the audio features are used (with a classifier based on GMM). 
                           
                              
                                 
                                    P
                                 
                                 
                                    SVM
                                 
                              
                           
                         stands for the score vector, coming from the classifier based on SVM. These scores are normalized in mean and variance. Similarly, 
                           
                              
                                 
                                    P
                                 
                                 
                                    GMM
                                 
                              
                           
                         is the score vector for the classifier based on GMM.

The calculation of the weigh, W, was done by evaluating the results with the available data using 3-fold cross-validation. We observed the performance with respect to the value of W from 0 to 1 in steps of 0.1. The best result was found with 
                           
                              W
                              =
                              0.3
                           
                        .

This approach shows better performance than using feature combination. The results found are analyzed in Section 4.

Meta-Classification or Stacked Generalization [47] is a different way of combining multiple models. Although developed some years ago, it is less widely used than probability combining, majority voting or other combining schemes. This is so partly, because this approach is difficult to assess analytically and, also, because it is not generally accepted as the best choice.

The main idea behind meta-classification is to represent the decision of every individual classifier as a feature vector and, then, perform a subsequent classification in the new feature space. The final decision is made by the meta-classifier of by linearly combining the judgments of each classifier.

However, the advantage of meta-classification is two-fold. First, when combining multiple classifiers, the similarity score or probability obtained by each classifier does not necessarily convey all the information. The distribution of the scores of each class judged by the classifier reveals its degree of confidence in the decision, which is a characteristic that can be captured by a classification feature vector, unlike conventional combining schemes such as linear mixtures. Second, there may be some patterns across several classification vectors, which can be learned by a meta-classifier. For example, consider a scene in a very noisy environment, v.gr. outdoor, the audio classification stage can easily fail but the image classification step should perform well. Meta-classification can learn this pattern from synthesized feature vectors. Thus, the normal linear combination strategy will act unstably in this circumstance, whereas the meta-classifier, can make a better decision by observing the pattern of the results from multi-modal classifiers.

So, we adopted this method for our problem making use of the decisions of the specialized classifiers described in Section 3.3. The general schema can be seen in Fig. 3
                        . Our meta-classification scheme has two levels: Level 0 and Level 1. The levels will require independent and sequential training and evaluation. In Level 0, the specialized classifiers perform with the image and audio features, providing their scores. These scores are gathered and concatenated to form the “decision vector” which is used as input to the meta-classifier.

The training schema becomes of special relevance because of the complexity of training the two classifier levels. Fig. 4
                         shows the training and evaluation schema in our 3-fold-cross-validation selection.The notation used is as follows:
                           
                              •
                              F1, F2, F3: Since we are using of 3-fold-cross-validation, the sample data was split into 3 sets of videos: F1, F2, F3.

In Level 0, the features used in the classifier based on SVM are always the color histogram and the edge histogram. The features used in the classifier based on GMM are the MFCC+Δ+
                                    
                                       Δ
                                       Δ
                                    
                                  (see Fig. 3).


                                 
                                    
                                       
                                          
                                             P
                                          
                                          
                                             xy
                                             -
                                             z
                                          
                                          
                                             M
                                          
                                       
                                    
                                  represents the score matrices given by the classifier identified by M, with 
                                    
                                       M
                                       ∈
                                       
                                          
                                             
                                                SVM
                                                ,
                                                GMM
                                             
                                          
                                       
                                    
                                 . These are trained with the set of videos 
                                    
                                       
                                          
                                             F
                                          
                                          
                                             x
                                          
                                       
                                       +
                                       
                                          
                                             F
                                          
                                          
                                             y
                                          
                                       
                                    
                                  and evaluated using 
                                    
                                       
                                          
                                             F
                                          
                                          
                                             z
                                          
                                       
                                    
                                 .


                                 
                                    
                                       [
                                       
                                          
                                             F
                                          
                                          
                                             z
                                          
                                       
                                       
                                       
                                          
                                             P
                                          
                                          
                                             xy
                                             -
                                             z
                                          
                                          
                                             SVM
                                          
                                       
                                       
                                       
                                          
                                             P
                                          
                                          
                                             xy
                                             -
                                             z
                                          
                                          
                                             GMM
                                          
                                       
                                       ]
                                    
                                 , is the decision vector used by the meta-classifier (input to Level 1). It is obtained by the concatenation of the score vectors 
                                    
                                       
                                          
                                             P
                                          
                                          
                                             xy
                                             -
                                             z
                                          
                                          
                                             SVM
                                          
                                       
                                    
                                  and 
                                    
                                       
                                          
                                             P
                                          
                                          
                                             xy
                                             -
                                             z
                                          
                                          
                                             GMM
                                          
                                       
                                    
                                 , corresponding to the evaluation of the set of videos 
                                    
                                       
                                          
                                             F
                                          
                                          
                                             z
                                          
                                       
                                    
                                 .

By using this combination method and the selected training approach, we have obtained interesting results that will be shown in Section 4. The good performance attained is held empirically by the advantages derived from the usage of the meta-classifiers: Possibility of using the specialized classifier for every source of information (SVM for the image features and GMM for the audio features), capacity to learn the goodness and badness of the specialized classifiers (the meta-classifier learns, from the training data, the cases in which every specialized classifier is more reliable) and capacity to learn cross-patterns built upon different types of information (image and audio) and use them together in order to attain the best classification performance.

@&#EVALUATION@&#

In this section we analyze and compare, from different points of view, the different results attained by the different approaches developed. In Section 4.1, we will analyze the dependency of results on the type of information, audio or image used, and the type of classifier, GMM or SVM. In Section 4.2, we will analyze the performance of the combination of classifiers.

Training and evaluation will be done using the personal video recordings repository provided by Sony Stuttgart Technology Center [40]. This data set contains 366 videos of average duration of 85s and a total of 8h 40min of recording time. The videos were manually annotated frame by frame. The total number of labels is over 600,000, with all the categories being assigned a significant number of labeled frames.Recall that every frame can be classified into more than one category (v.gr. Party and Artificial light).

In this section, we analyze the behavior of the classifiers based on GMM and SVM with the different types of information considered. First, we compare the results obtained using the image features (HSV and EDGE histograms) with the SVM and GMM classifiers. Next, we perform the same comparison using the audio features (MFCCs and their velocity and acceleration). These two comparisons are used to decide which type of information is more suitable for video classification. Finally, we compare the results obtained when feature combination is used.

In Fig. 5
                        , the results (
                           
                              f
                              -
                              measure
                           
                        ) of the classifiers based on GMM and the SVM with image features (HSV and EDGE histograms) are presented. Both classifiers were trained with the same number of samples and 3-fold-cross-validation.

The figure shows that the SVM classifier attains better average results than the GMM classifier (micro average of 54.83 versus 42.62). This observation confirms the hypothesis that SVM has better performance with vectors of high dimensionality (our image feature vector has 120 components).

In Fig. 6
                        , the comparison (
                           
                              f
                              -
                              measure
                           
                         per class and averages) between the GMM and SVM classifiers when using audio features (MFCCs and their velocity and acceleration) is presented. For this comparison, the set of classes was reduced, discarding the categories in which the audio is by no means meaningful, v.gr. “Sand”.

The results show that the GMM classifier attains better performance than the SVM classifier using this type of features (micro-average of 48.5 versus 41.83, 
                           
                              f
                              -
                              measure
                           
                        ).

The reason for this behavior is the relative low dimensionality of the audio feature vector (75 components).

If we compare the results illustrated in Figs. 5 and 6, it should be observed the general better classification performance attained by using image features than by using audio features. However, it should also be noted that the audio classification stage works very well in classes with very characteristic sounds like “In vehicle”, “Children” and “Small groups”.

In general, the SVM classifier with image features attains best average result. But it also observed that for certain specific classes, the GMM classifier outperforms the SVM classifier (v.gr. “Sea”). So, the proper combination of features and classifiers must improve the classification results.

Now, the results obtained by feature combination are presented. In this comparison we use the classifier described in Section 3.2. In Fig. 7
                        , the 
                           
                              f
                              -
                              measure
                           
                         across all the 14 classes attained by means of audio classification (MFCC+velocity+acceleration feature), image classification (HSV+edge histogram) and concatenated vector classification (audio+image) are presented. These three approaches make use of the SVM classifier. It must be highlighted that use the SVM model. It should be noticed that the results for the GMM classifier are not shown for the concatenated vector (audio+image), because of the poor results attained by this classifier with vectors of high dimensionality.

The results in Fig. 7, show that the classification results improve by using the combination of audio and image features. The macro-average improved over a 2 % of 
                           
                              f
                              -
                              measure
                           
                        . This global improvement is not the most significant conclusion. The observation of the results per class shows that image and audio worked together to provide complementary information. For example, the classification performance of the classes “In vehicle”, “Crowed”, “Only voice” and “Party” have improved significantly and in very few cases the reduction of the 
                           
                              f
                              -
                              measure
                           
                         was important.

At the sight of these results, we can draw some interesting conclusions:
                           
                              •
                              Audio and image provide complementary pieces of information that can collaborate to get better video classification performance.

The complementary information provided by audio and image is only present in some specific categories. Furthermore, if the combination is not effective the performance can be diminished.


                        Fig. 8
                         shows the results obtained by using the different classifier combination techniques developed: score combination, meta-classification and majority voting. The majority voting classifier [48] has been implemented by using the three votes that achieve the best results for these classifiers: SVM classifier with the HSV color space (image features), SVM classifier with the EDGE histogram (image features) and GMM classifier with the MFCCS (audio features).

The results show that majority voting gives the worst performance. However, this schema shows very positive values for some specific categories like “Performance”, “At home”, “Indoor” or “Small groups”. This behavior can be explained by the fact that the voting stage makes use of two decisions from classifiers using image information and only one classifier using audio features. Thus, the categories for which the audio features perform better than the image features are inherently penalized (v.gr. “In vehicle” or “Only voice”), and vice versa (v.gr. “At home”).

The techniques “Score combination” and “Meta-Classification” show very good and similar performance (only 1% of difference in macro-average 
                           
                              f
                              -
                              measure
                           
                        ). Regarding the results per class, it is noticeable the excellent performance obtained by the meta-classifier for the categories “At home”, “Indoor” and “Sports”. The explanation of this behavior relies on the fact that the meta-classifier has the ability to find the correlation between classes; for example, in our context, the classes “At home”, “Indoor” and “Outdoor” are strongly related. Nevertheless, probability matrix combination shows better classification results for the categories in which the audio features contributes more effiently to the classification (v.gr. “In vehicle”, “Small groups” or “Only voice”). This behavior is due to the fact that this technique allows to reduce the negative effect of any piece of information (and/or a classifier) that produces noisy results, giving more importance to the ones with better performance.

These results illustrate the fact that although both techniques, score combination and meta-classification, provide very good and similar results, each of them presents specific characteristics that can be particularly useful, depending on the specific problem. The meta-classification allows to learn the correlation between classes and helps to choose the most suitable classifier for every case by utilizing specialized classifiers per type of information. Nevertheless, when real data, like the personal video recordings database used in this work, it is difficult to get all the possible benefits due to the complex training phase.

Comparing our best result (meta-classification) against the performance shown in [12], which is based on audiovisual features of personal video recordings, it must be noted that the results in [12] for five genre classification are quite similar to the ones found for the corresponding category selected among the 24 classes in our scheme. However, the average performance of our best classifier (the meta-classifier attains a 57% in 
                           
                              f
                              -
                              measure
                           
                        ) is compared, it is observed that it apparently, performs worse that the one proposed in [12] (which attains an average performance of 75% in 
                           
                              f
                              -
                              measure
                           
                        ). However, this seeming poorer performance of our system is by no means true, such apparent difference in performance is due to the fact that our scheme performs the classification task frame by frame, instead of classifying each video clip as a whole, and, also, recall that our classification scheme considers 24 categories instead of only 5 categories, as in [12]. Additionally, note that our categories are not mutually exclusive, that is, every frame can be assigned several different labels (v.gr. “In vehicle” and “Children”), so the range of possibilities in our classification scheme is much larger.

Recent publications in video classification as [16,49] only consider 5 or 6 mutually exclusive categories respectively and it classify the full video clips as a whole. In [16], the best classification algorithm presented reaches a mean accuracy of 87.48% using commercial videos, 5 categories classifying the full video clips. On the other hand, [49] proposes the classification of mobile videos into 6 categories, in the best cases the accuracy attained 79.95% for whole video clips classification. The scheme described in [49], analyzes multiple videos and multimodal data recordings by simultaneously capturing video data, audio data and data from an additional auxiliary sensor to classify the videos. So, that scheme uses a huge among of information to classify the full videos in just 6 categories. Therefore, the results presented in this paper, corresponding to frame by frame classification of personal video recordings into 24 different categories, are clearly significant when compared against the results of other state of the art schemes.

@&#CONCLUSION@&#

In this work, a classifier of personal video recordings based on simple audiovisual features has been described. This system classifies personal video recordings into 24 different categories. State-of-art machine learning methods, like Support Vector Machines (SVMs) and Gaussian Mixture Models (GMMs), were used, as well as different combination techniques of audiovisual features and classifiers.

Specifically, we have selected the Mel-Frequency Cepstral Coefficients (MFCCs) as audio features, because these descriptors provide very useful information to perform the classification of personal audio recordings. Regarding the image data, we selected color and edge histogram as features with the visual information. It was observed that these types of audio and image features are complementary sources of information so that they can be combined to attain better classification results.

Regarding the classifiers, we evaluated the performance of SVM and GMM based classifiers with both audio and image information. SVM gave the best results using the selected image descriptors and GMM attained better performance using the audio features than SVM. Additionally, we implemented and evaluated a SVM classifier using both audio and image features combined. This combination attained better results than using a single type of information source, supporting the fact that audio and image features provide complementary information for the video classification task.

Finally, we evaluated two different classifier combination techniques. First, the combination of the probability matrices from specialized classifiers was considered. This approach obtained better results than using feature combination since it takes benefit from use of specialized classifiers for the different types of descriptors (GMM for the audio features and SVM for the image features) and it learns the more suitable specialized classifiers for the different cases. Then, we implemented a classifier combination schema using meta-classification. Meta-classification shows a more complete set of characteristics than probability matrix combination, learning the correlations between classes, allowing the utilization of a specialized classifier per type of information. Nevertheless, when using real data, it is difficult to take benefit of all its qualities due to the complex training phase. Although both approaches, meta-classification and probability matrix combination, attained very similar results, the experiments showed that meta-classification gets the best performance over the different approaches evaluated, resulting in a promising 
                        
                           f
                           -
                           measure
                        
                      higher than 57% in average for all categories and higher than 73% over diverse categories, for frame by frame classification into 24 non-mutually-exclusive categories.

@&#ACKNOWLEDGMENTS@&#

This work has been funded by the Ministerio de Economía y Competitividad of the Spanish Government under Project No. TIN2013-47276-C6-2-R, by the Junta de Andalucía under Project No. P11-TIC-7154. We are grateful to Sony Stuttgart for allowing us the use of their personal video database. This work has been partially done at Universidad de Málaga, Campus de Excelencia Internacional (CEI) Andalucía Tech.

@&#REFERENCES@&#

