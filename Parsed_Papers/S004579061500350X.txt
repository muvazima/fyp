@&#MAIN-TITLE@&#A data-hiding technique using scene-change detection for video steganography

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Enhances the security of embedded message by using scene-change detection and discrete cosine transforms of video sequences.


                        
                        
                           
                           Reduces distortions.


                        
                        
                           
                           Enhances video quality using discrete wavelet transforms in video sequences.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Data hiding

Video steganography

Security

Scene change detection

Discrete wavelet transform

@&#ABSTRACT@&#


               Graphical abstract

               
                  
                     
                        
                           Image, graphical abstract
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

Data hiding in digital multimedia, such as text, image, audio, and video, plays an increasingly vital role in the current trend of secure communication [1]. Secure communication is highly desirable in many cases and has thus given rise to the need to develop new approaches in information hiding [2]. Steganography is one of the most significant techniques for secure communication. Steganography is the art of hiding data within data by concealing the presence of messages [3]. The process of data hiding in basic steganography is shown in Fig. 1
                     . In any basic steganographic technique, the cover-medium serves as the carrier of the secret message, the secret message is the message to be embedded into the cover-medium, and the stego-medium is the medium containing the embedded data [4].

Modern Internet technologies demonstrate that videos are important media for sharing data, medical records, banking information, broadcast information, and military intelligence. Videos are particularly suitable for data hiding for a number of reasons: (1) distortions in videos can be handled faultlessly, (2) hidden data are imperceptible to the human visual system (HVS), and (3) videos provide additional hiding space. The present work is undertaken in consideration of the widespread use of video data. As videos are often stored and transmitted in compressed formats, data hiding should withstand lossy compression [5]. Accordingly, many data-hiding schemes are executed in the transform domain by modifying discrete cosine transform (DCT) coefficients.

A considerable number of steganography studies have been conducted on the spatial and transform domains of cover-media for data hiding [6]. In the spatial-domain approach, data are hidden in the pixels of a cover-medium. The least significant bit (LSB)-based data-hiding technique is the most widely used spatial-domain-based steganography. In [7], the data were embedded into a compressed video in the transform domain, which is highly vulnerable to steganalysis. The payload was minimal; nevertheless, the method achieved low-video distortion.

In [8], the distortion levels were reduced by employing a method with a dual-convolution code that minimizes the additive distortion in steganography. In recent years, many data-hiding schemes have been developed in the transform domain, in which the data are embedded into the transform coefficients of the cover-medium using polynomial mathematics [9], Fourier transform, DCT, and discrete wavelet transform (DWT). Transform-domain methods are generally more robust than spatial-domain methods [10].

The present work attempts to improve secret-data security by utilizing DCT and DWT coefficients. The key objectives of this study are as follows:

                        
                           •
                           To enhance the security of hidden data by using scene-change detection and the DCT coefficients of video sequences

To improve video quality after hiding data using DWT in video sequences

The rest of this paper is organized as follows. Section 2 discusses the methods that are currently used in data hiding, including their advantages and limitations. Section 3 explains the proposed scene-change-detection-based data hiding (SCDH) method for video steganography in the DCT and DWT domains. Section 4 presents the analysis and comparison of the experimental results obtained using the proposed SCDH method, and other existing methods. Section 5 concludes the paper.

HVS - human visual systems; MR-FMO - multivariate regression and flexible macro-block ordering; MPEG - moving picture experts group; LSB - least significant bit; DWT - discrete wavelet transform; DCT - discrete cosine transform; UCI - University of California Irvine; SCDH – scene-change-based data hiding technique; DWTSQ - DWT-based steganography for quality; S-DWT - steganography using DWT; PSNR - peak signal-to-noise ratio; MSE - mean squared error; dB - decibels.

@&#RELATED WORK@&#

As the present work utilizes the DCT and DWT coefficients, this section presents studies on transform-domain-based techniques. The DCT transforms the cover-image from the spatial domain into the frequency domain. In [11], an adaptive optimization procedure was adopted for the selection of bi-stable parameters to achieve the maximum correlation coefficients under the minimum computational complexity.

In [12], a coherent steganographic technique was presented to improve the security of the secret messages hidden using the DCT coefficients of a cover-image. In the technique, the cover-image was divided into blocks, and the DCT was applied to each block; the number of the most significant bits of the payload was coherently embedded into the DCT coefficients of the cover-image based on the values of the DCT coefficients [12]. The proposed method improved peak signal-to-noise ratio (PSNR), security, and capacity [12]. In [13], data hiding was performed using the DCT and LSB techniques. The DCT-based steganography technique was used to embed the text messages into the LSB of the DCT coefficient of a digital image; the data-hiding capacity of the system is limited, although the method achieved improved quality [13].

Data hiding in the frequency domain, rather than in the spatial domain, was performed in [14]. In the frequency-domain approach proposed in [14], secret messages were embedded into high-frequency DWT coefficients, the DWT coefficients in the low-frequency sub-bands were unmodified to enhance quality, and some basic mathematical operations were performed on the secret message before embedding. The proposed approach achieved improved security [14].

The major advantage of using wavelet transforms in digital-media processing is the significant localization property, which is suitable for many image-processing applications [15]. In [16], text data of considerable size were embedded into selected images using an entropy function and the embedding process was constrained by how much the bit error rate is minimized.

In [17], data were embedded in the frequency domain without compromising image quality; the hidden data were successfully extracted with minimal distortion. However, the capacity and quality of the cover-medium of the steganographic system adopted in [17] needs to be improved.

In [18], a data-hiding method using the LSB of DWT coefficients was presented. However, the application of a lower sub-band resulted in a high computational burden and further deteriorated the quality of the hidden data. Another DWT-based steganographic method was also presented in [19]; in this method, the data are hidden in images using a biometric feature, that is, the skin-tone region, to which the HVS is not sensitive. However, this method cannot protect images from distortions.

In [20], the data were hidden in the noise region of MPEG (Moving Picture Experts Group) videos with the use of the bit-plane complexity segmentation technique. This technique works better for images with more complex textures than for computer-generated images, which usually have uniform textures.

In [21], two data-hiding approaches, namely, multivariate regression and flexible macro-block ordering, (MR-FMO) were presented. Although these methods achieve high prediction accuracy for MPEG videos, the payload is restricted and interferes with the bit rate of coded videos, thus affecting video quality. In [22], distortions were minimized using syndrome-trellis codes; the Viterbi algorithm was applied to the syndrome trellis codes as a versatile solution for digital images in transform domains.

Various techniques are performed using wavelet coefficients to enhance hiding capacity and perceptual transparency in wavelet domains. Using a wavelet transform, Ali and Fawzi [23] enhanced the capacity of a steganographic system and achieved a high level of privacy by hiding the data in the two-dimensional DWT domain of cover-images. The method achieved acceptable levels of imperceptibility and image distortion; however, the method entails high computational overhead.

Alternatively, a data-hiding scheme using scene-change detection for H.264 video streams was presented in [24]; the H.264 encoder uses differently sized blocks of a video during the inter-prediction stage to hide the scene-change information inside the encoded sequence. An algorithm for scene-change detection for MPEG videos was developed in [25].

Despite their contributions, the aforementioned schemes either affect video quality or reduce security level. Thus, improving data security and preserving video quality in data hiding must be investigated. In this study, data-hiding using scene-change detection called the SCDH method is proposed to improve data security and preserve video quality in the process of data hiding.

@&#PROPOSED METHOD@&#

A schematic of the proposed data-hiding method is shown in Fig. 2
                     . In SCDH, the cover-video and the secret message are the inputs, and the stego-video is the output.

The data-hiding process is performed in the transform domain using the DCT and DWT coefficients of the cover-video. In the SCDH method, the data-hiding process is carried out in three steps: (1) video-series parsing, (2) secret-message embedding, and (3) cover-video sequence and payload normalization using the DWT coefficients.

In video-series parsing, the DCT coefficients are obtained to detect the scene-change point in the cover-video. The scene changes are detected using frame difference, and the secret message is embedded. This embedding process effectively helps in increasing the security level of the SCDH method because hiding the secret message at the scene-change point of the moving video sequence makes the secret message difficult to detect. Then, the cover-video and payload are normalized using the wavelet sub-bands of the DWT coefficients to enhance the quality of the stego-video sequence. The following sub-sections explain the steps involved in the proposed method.

The first step of the proposed data-hiding method is video-series parsing. In this step, the cover-video is compressed to remove the spatial and temporal redundancies with the use of the DCT. This process allows for efficient and fast computation. The DCT coefficients are converted into blocks of pixels in the transform domain. The two-dimensional DCT coefficient, K(p, q), of the n × n video image, f(x, y), is obtained using Eqs. (1) and (2).

                           
                              (1)
                              
                                 
                                    K
                                    
                                       (
                                       
                                          p
                                          ,
                                          q
                                       
                                       )
                                    
                                    =
                                    
                                    ∝
                                    
                                       (
                                       p
                                       )
                                    
                                    ∝
                                    
                                       (
                                       q
                                       )
                                    
                                    
                                       ∑
                                       
                                          x
                                          =
                                          0
                                       
                                       
                                          n
                                          −
                                          1
                                       
                                    
                                    
                                    
                                       ∑
                                       
                                          y
                                          =
                                          0
                                       
                                       
                                          n
                                          −
                                          1
                                       
                                    
                                    f
                                    
                                       (
                                       
                                          x
                                          ,
                                          y
                                       
                                       )
                                    
                                    
                                    c
                                    o
                                    s
                                    
                                       [
                                       
                                          
                                             
                                                
                                                   (
                                                   
                                                      2
                                                      x
                                                      +
                                                      1
                                                   
                                                   )
                                                
                                                p
                                                π
                                             
                                             
                                                2
                                                n
                                             
                                          
                                          
                                       
                                       ]
                                    
                                    c
                                    o
                                    s
                                    
                                       [
                                       
                                          
                                             
                                                (
                                                
                                                   2
                                                   y
                                                   +
                                                   1
                                                
                                                )
                                             
                                             q
                                             π
                                          
                                          
                                             2
                                             n
                                          
                                       
                                       ]
                                    
                                    ,
                                 
                              
                           
                        for 
                           
                              x
                              ,
                              y
                              =
                              0
                              ,
                              
                              1
                              ,
                              …
                              ,
                              n
                              −
                              1
                              ,
                           
                        
                        
                           
                              (2)
                              
                                 
                                    ∝
                                    
                                       (
                                       p
                                       )
                                    
                                    ,
                                    ∝
                                    
                                       (
                                       q
                                       )
                                    
                                    =
                                    
                                       {
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         1
                                                         n
                                                      
                                                   
                                                   
                                                   for
                                                   
                                                   p
                                                   ,
                                                   q
                                                   =
                                                   0
                                                
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                         2
                                                         n
                                                      
                                                   
                                                   
                                                   for
                                                   
                                                   p
                                                   ,
                                                   q
                                                   ≠
                                                   0
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where n is the block size (i.e., for an 8 × 8block, n is 8), (x, y) is the intensity of the original image pixel, and (p, q) is that of the transformed image pixel values.

During the video-parsing process, scene changes are detected using Eq. (3). The parser retains such information as color, spatial correlation, motion vectors, and the DCT coefficients of the video. The SCDH method replaces these coefficients with the pixel values of the secret message. Normally, when a scene change occurs in a video frame, the color and brightness of the picture differ from those of the previous scene sequence. The scene-change detection process detects the changes occurring in the scene by sequentially measuring interframe differences.

Generally, the first step is to segment the video into temporal “shots,” each of which represents an event or a continuous sequence of actions. A shot represents a sequence of frames captured from a unique and continuous camera recording. A measure of dissimilarity between two frames must be defined to segment a video sequence into shots. This measure yields a high value only when two continuous frames fall into different video shots.

For a sequence of two consecutive video frames, fi
                         and 
                           
                              f
                              
                                 i
                                 +
                                 1
                              
                           
                         with 0 ≤ fi
                         ≥ N, N is the number of frames in a video data, fi
                        (x, y) denotes the value of the pixel at position (x, y) for the ith frame, and Ci
                        (x, y) and 
                           
                              
                                 C
                                 
                                    i
                                    +
                                    1
                                 
                              
                              
                                 (
                                 x
                                 ,
                                 y
                                 )
                              
                           
                         denote the DCTs of frames fi
                         and 
                           
                              f
                              
                                 i
                                 +
                                 1
                              
                           
                        , respectively. The interframe difference between the two consecutive frames is denoted by 
                           
                              D
                              (
                              
                                 f
                                 i
                              
                              ,
                              
                                 f
                                 
                                    i
                                    +
                                    1
                                 
                              
                              )
                           
                        . Thus, in SCDH, the scene-change point is identified using Eq. (3).

                           
                              (3)
                              
                                 
                                    D
                                    
                                       (
                                       
                                          
                                             f
                                             i
                                          
                                          ,
                                          
                                          
                                             f
                                             
                                                i
                                                +
                                                1
                                             
                                          
                                       
                                       )
                                    
                                    =
                                    
                                       ∑
                                       x
                                    
                                    
                                       ∑
                                       y
                                    
                                    
                                       [
                                       
                                          
                                             C
                                             i
                                          
                                          
                                             (
                                             
                                                x
                                                ,
                                                y
                                             
                                             )
                                          
                                          −
                                          
                                          
                                             C
                                             
                                                i
                                                +
                                                1
                                             
                                          
                                          
                                             (
                                             
                                                x
                                                ,
                                                y
                                             
                                             )
                                          
                                       
                                       ]
                                    
                                 
                              
                           
                        
                     

One of the key steps of the proposed data-hiding method is embedding the secret message once a scene change is detected. When the scene-change point is greater than zero, the secret message is embedded into different scenes of the cover-video. A cover-video frame is coded to obtain the DCT picture with the proportional average of the pixels. The DCT picture is then used as a reference for the other frames of the video. The scene-change point refers to the changes occurring in the scene when the transition of continuous shots is observed from the video. Once a scene change is detected, the embedding of the secret message proceeds, as shown in Fig. 3.
                        
                     

In Fig. 3, secret-message embedding occurs after the DCT transformation. The secret message bits are embedded into the DCT coefficient of a 8 × 8 block. The embedding process is obtained by replacing the LSB of the DCT coefficient with the message bit for embedding.

After detecting the scene changes in the video frames, the secret message is embedded using the algorithm specified in Fig. 4
                        . The scene-change detection algorithm identifies even the minor changes within a scene. Then, the variable scene-change point is set to 1. If no change is observed in the scenes of the video sequences, the scene-change point detected is set to 0. If the scene-change point is greater than 0, then a secret message is embedded into it. The obtained intermediate video is normalized and DWT is applied to the resulting video sequence that contains the payload. Finally, a stego-video sequence is obtained.

This process involves merging the results of the wavelet decomposition of the normalized versions of the cover-video and the payload into a single fused result. The cover-video and the payload are normalized in the DWT domain so that the video frame pixel values (in integer form), which range from 0 to 255, lie in the floating point range (between 0.0 and 1.0) of the standardized pixel values. The objective behind the use of the normalization process is to ensure that the pixel values do not exceed the maximum values of the corresponding coefficients during fusion. Furthermore, the distortions that occur in the video as a consequence of secret-message embedding are mitigated with better accurate pixel values. As a result, the video quality is enhanced.

In the SCDH method, the wavelet transform, which decomposes a signal into a set of wavelet functions, is utilized. The DWT transforms a discrete time signal into a discrete wavelet. A one-dimensional wavelet transform can be obtained using Eq. (4),

                           
                              (4)
                              
                                 
                                    w
                                    
                                       (
                                       
                                          a
                                          ,
                                          b
                                       
                                       )
                                    
                                    =
                                    
                                       ∫
                                       
                                          −
                                          ∞
                                       
                                       ∞
                                    
                                    x
                                    
                                       (
                                       t
                                       )
                                    
                                    
                                       ψ
                                       
                                          a
                                          ,
                                          b
                                       
                                    
                                    
                                       (
                                       t
                                       )
                                    
                                    d
                                    t
                                 
                              
                           
                        
                     

In practice, such transformation is applied recursively on the low-pass sub-band series until the desired number of iterations is reached. The frequency band with lower-frequency video coefficients in the wavelet domain is called the approximation band. Normalization is applied to both the cover-video and the payload to guarantee that the pixel values do not exceed the maximum value of 1 when the respective coefficients of the cover-video frame and the payload are modified during fusion. Both the cover-video and payload are converted into wavelet domains by applying DWT to further increase the security level and the video quality. The single resulting fused matrix is obtained by adding the respective wavelet coefficients of the cover-video and the payload. Wavelet-based fusion involves merging the wavelet decompositions of the normalized versions of the cover-video and the payload into a single fused result.

As a result, the cover-video image, f, is divided into primary approximation parts, namely, 
                           
                              f
                              h
                              ′
                           
                        , 
                           
                              f
                              v
                              ′
                           
                        , and 
                           
                              f
                              d
                              ′
                           
                        , which respectively represent the horizontal, vertical, and diagonal features, as shown in Eq. (5).

                           
                              (5)
                              
                                 
                                    f
                                    =
                                    
                                       f
                                       m
                                       ′
                                    
                                    +
                                    
                                       (
                                       
                                          
                                             
                                                
                                                   f
                                                   ′
                                                
                                             
                                             h
                                          
                                          +
                                          
                                             
                                                
                                                   f
                                                   ′
                                                
                                             
                                             v
                                          
                                          +
                                          
                                             
                                                
                                                   f
                                                   ′
                                                
                                             
                                             d
                                          
                                       
                                       )
                                    
                                 
                              
                           
                        
                     

First, the DWT is applied to the approximation coefficients of the input cover-video image and the payload (image). If the DWT is applied to the primary image approximation part, 
                           
                              f
                              m
                              ′
                           
                        , then the subsequent approximation level is obtained with the corresponding feature values. The modified DWT for part 
                           
                              f
                              m
                              ′
                           
                         improves the security of the embedded data. The process is repeated until N approximation parts are identified.

In a multilevel decomposition, the length of the separated frames of the video sequence is half of the length of the frame at an earlier stage. Therefore, the use of the DWT allows for independent processing of the resulting components without significant perceptible interaction between these components, thus resulting in better imperceptibility. For this reason, wavelet decomposition is commonly used for the fusion of images.

From the aforementioned description of multilevel decomposition, the dimensions of the approximation component obtained from the decomposition at the primary level onward are 
                           
                              
                                 N
                                 2
                              
                              ×
                              
                                 N
                                 2
                              
                           
                        , 
                           
                              
                                 N
                                 4
                              
                              ×
                              
                                 N
                                 4
                              
                           
                        , and so on. As the point of separation increases, denser but coarser approximations of the video-sequence frames are obtained. Thus, wavelets provide a hierarchical structure to interpret image-quality information without the occurrence of any noise induced by data embedding.

The single resulting fused image obtained by calculating the wavelet coefficients of the relevant sub-bands of the cover-video image and the payload image is expressed as Eq. (6),

                           
                              (6)
                              
                                 
                                    S
                                    
                                       (
                                       
                                          p
                                          ,
                                          q
                                       
                                       )
                                    
                                    =
                                    
                                    α
                                    
                                    C
                                    
                                       (
                                       
                                          p
                                          ,
                                          q
                                       
                                       )
                                    
                                    +
                                    β
                                    
                                    R
                                    
                                       (
                                       
                                          p
                                          ,
                                          q
                                       
                                       )
                                    
                                 
                              
                           
                        where C(p, q) is the original DWT coefficient of the cover-video image; p and q represent the row and column indices of the pixels of the cover-video image, respectively; S is the modified DWT coefficient of the stego-image; R(p, q) is the modified approximation DWT coefficient of the payload; and α and β are the embedding strength factors, where α + β = 1. These two embedding factors are selected such that the payload is indiscernible in the stego-video.

The data-extraction procedure, which aims to retrieve the hidden message, is the reverse process of the data hiding. The data-extraction algorithm for the payload is shown in Fig. 5
                        . The following data-extraction procedure is illustrated in Fig. 6.
                        
                        
                           
                              (1)
                              The stego-video sequence is taken as the input.

Each video sequence is parsed to obtain the DCT coefficients.

The scene changes are detected using the DCT coefficients.

From the detected coefficients, the message bits are extracted using the algorithm given in Fig. 5.
                              

The extracted message and video are obtained as outputs.

The data-extraction process recovers the hidden message, HMk
                        , from the stego-video using Eq. (7).

                           
                              (7)
                              
                                 
                                    
                                    H
                                    
                                       M
                                       k
                                    
                                    ·
                                    
                                       (
                                       
                                          p
                                          ,
                                          q
                                       
                                       )
                                    
                                    =
                                    
                                       (
                                       
                                          S
                                          k
                                       
                                       
                                          (
                                          
                                             p
                                             ,
                                             q
                                          
                                          )
                                       
                                       −
                                       
                                          C
                                          k
                                       
                                       
                                          (
                                          
                                             p
                                             ,
                                             q
                                          
                                          )
                                       
                                       )
                                    
                                    /
                                    
                                       δ
                                       k
                                    
                                    ,
                                 
                              
                           
                        where Sk
                        (p, q) is the stego-video frame, Ck
                        (p, q)is the cover-video frame, kis the video frame number, and δk
                         is the correlation factor used to control the strength of the hidden message in the trade-off among perceptual distortion, message robustness, and security. If the scene-change point is greater than zero and if any scene change occurs in the stego-video sequence, then the embedded secret message is extracted.

The proposed data-hiding method was evaluated using various video data and secret messages from the UCI repositories. Table 1
                         presents the details of the cover-video data used.

The proposed method is compared with existing methods using three metrics, namely, embedding capacity, distortion level, and PSNR. The experiments on the existing methods and the proposed method use the cover-videos provided in Table 1. An algorithm is said to be secure if the difference between the cover-video frame and the stego-video frame is hardly distinguishable. The most common measure for security is the detectability of a stego-video. Detectability refers to the difference between the cover-video frames and the corresponding stego-video frames. The metric for detectability is PSNR. A reduction in detectability leads to reduced embedding capacity. Thus, the proposed method is analyzed in terms of its embedding capacity and imperceptibility to determine the security and the quality of video.

The SCDH method was compared with three existing methods, namely, MR-FMO [21], DWT-based steganography for quality (DWTSQ) [18], and secure steganography using DWT (S-DWT) [19]. The proposed method is evaluated in terms of the capacity of the cover-video, the security of the hidden data, and video quality.

“Security” refers to an eavesdropper's inability to detect the hidden data in a cover-video, and “quality” refers to the clear visibility of the video without observable distortions. The proposed method was also evaluated in terms of embedding capacity and imperceptibility. A low distortion level guarantees good video quality.

“Capacity” is defined as the maximum amount of secret data that can be embedded in a cover-video. Capacity depends on the embedding function and properties of the cover-video. Capacity is the ratio of the amount of embedded data to the size of the original cover-video in percentage. Embedding capacity is calculated using Eq. (8).

                           
                              (8)
                              
                                 
                                    
                                       Capacity
                                       =
                                    
                                    
                                       
                                          Maximum
                                          
                                          size
                                          
                                          of
                                          
                                          embedded
                                          
                                          data
                                          
                                       
                                       
                                          Size
                                          
                                          of
                                          
                                          original
                                          
                                          cover
                                          −
                                          video
                                          
                                       
                                    
                                    
                                       
                                       ×
                                       
                                       100
                                    
                                 
                              
                           
                        
                     

“Imperceptibility” measures the minor changes in the statistical behavior of a cover-video after data hiding. The distortion level is determined using the embedding rate of the secret message. The quality of the stego-video generated using the proposed method was measured using the PSNR. The PSNR calculates the statistical difference between the cover video and the stego-video in decibels and is calculated using Eq. (9). A high PSNR indicates good video quality. In Eq. (9), R represents the peak signal level (i.e., 255) and MSE denotes the mean squared error. The MSE is the average of the squares of errors and quantifies the difference between the estimated values and the true values of the quantity being estimated. The MSE is calculated using Eq. (10).

                           
                              (9)
                              
                                 
                                    P
                                    S
                                    N
                                    R
                                    =
                                    10
                                    
                                    l
                                    o
                                    g
                                    10
                                    
                                       
                                          R
                                          2
                                       
                                       
                                          M
                                          S
                                          E
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (10)
                              
                                 
                                    M
                                    S
                                    E
                                    
                                    =
                                    
                                    
                                       1
                                       
                                          H
                                          
                                          ×
                                          
                                          W
                                       
                                    
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       H
                                    
                                    
                                    
                                       ∑
                                       
                                          j
                                          =
                                          1
                                       
                                       W
                                    
                                    
                                       (
                                       
                                          C
                                          
                                             (
                                             
                                                i
                                                ,
                                                j
                                             
                                             )
                                          
                                          −
                                          
                                          S
                                          
                                             (
                                             
                                                i
                                                ,
                                                j
                                             
                                             )
                                          
                                       
                                       )
                                    
                                    
                                       
                                       2
                                    
                                 
                              
                           
                        where H × W is the cover-video image size and C(i, j)and S(i, j) are the intensity values of the cover-video and the stego-video generated before and after data hiding, respectively.

Several simulations were performed to evaluate the performance of SCDH, and the results were compared with its existing counterparts. Table 2
                         shows the embedding capacities of the proposed and existing methods.

The capacity is computed in percentage using Eq. (8). In Table 2, the best results are bolded. As shown in Table 2, the proposed method, SCDH exhibits improved embedding capacity for all cover-videos. For instance, for video sequence v2, the measured capacities of MR-FMO, DWTSQ, and S-DWT methods are 15%, 13.2%, and 9%, respectively. By contrast, the proposed method is able to achieve the best capacity, which is 22%.

For all the experimented videos, the order of the wavelet methods from the method with the highest capacity is as follows: SCDH, MR-FMO, DWTSQ, and S-DWT. The results prove that the utilization of the DCT coefficients of the cover-video bit streams enhanced the capacity of SCDH. The results in Table 2 also reveal that the proposed method outperforms all the existing wavelet-based methods in terms of security.

In SCDH, the number of video frames is relatively high because of the employment of DCT coefficients and scene-change detection for data embedding. In the transform domain, the DCT coefficients contain difference information, as each coefficient denotes the average of all the pixel values within the pixel blocks. The difference information in the DCT coefficients and the average of all the pixel values within the pixel blocks support secure data embedding. Thus, the embedding capacity of the proposed method is 45–55% higher than that of the MR-FMO method and 25–35% higher than those of the DWTSQ and S-DWT methods.

The variation in the results indicates the good performance of SCDH in terms of improved capacity with respect to the other compared methods. The variation found in the results of the proposed method is approximately 50–60% higher than that found in the MR-FMO method and 35–40% higher than those found in the DWTSQ and S-DWT methods. The results in Table 2 indicate that the proposed algorithm outperforms other existing schemes in terms of hiding capacity.

When SCDH is employed, the number of frames obtained from the cover-video is large. This result suggests that the proposed method can hide a large amount of data in videos. Thus, the capacity of SCDH is higher.

The performance of SCDH was also analyzed in terms of imperceptibility, which is measured based on the distortion level during the embedding of the secret message. The distortion level was computed using the embedding rate of the secret message. The embedding of data in a cover-video is analogous to altering the cover-video. A comparison of the distortion levels caused by the hiding of data in the cover-videos with the use of SCDH and the other methods is shown in Fig. 7
                        . For example, video sequence v3 had distortion levels of 30%, 38%, and 49% when the MR-FMO, DWTSQ, and S-DWT methods were used, respectively, with an embedding rate of 30%. By contrast, the distortion level with SCDH was only 17%, which is considerably lower than those observed with the other methods.

Given its excellent spatial frequency localization property, the DWT is highly suitable for securing secret messages. The SCDH leverages this property to exploit the masking effect on the HVS such that, if a DWT coefficient is modified, then only the region corresponding to that coefficient is modified. Therefore, a change in the pixel value of a video frame does not cause alterations resulting from data hiding. The proposed method, SCDH achieves higher imperceptibility by approximately 40–50%, 30–35%, and 20–25% than the MR-FMO, DWTSQ, and S-DWT methods, respectively. Thus, the results prove that the distortion level resulting from the use of the SCDH is lower than those resulting from the use of the other methods.

The PSNR is a simple way of measuring the differences between the cover-video frames and the corresponding stego-video frames. A high PSNR indicates good video quality. Fig. 8
                         shows the evaluated PSNR values of SCDH and the other state-of-the-art methods.

As Fig. 8 indicates, the PSNR values obtained after hiding the secret data in the test videos with the use of the proposed method are higher than those the other data-hiding methods. The higher PSNR values of SCDH indicate the good quality of the resulting video. Thus, SCDH is proved to provide enhanced security. On the basis of these comparisons, SCDH is capable of generating a resulting video that is closest to the original cover-video, with low distortion level.

The proposed method was tested using the cover-videos presented in Table 1. The qualitative results are presented in this section. The original cover-video, v2, in which the secret message was embedded, is shown in Fig. 9
                        a. The resulting distortions in the stego-video frames after data hiding with the use of the SCDH, MR-FMO, DWTSQ, and S-DWT methods are shown in Figs. 9b, c, d, and e, respectively. Figs. 9 and 10
                         show the resulting stego-video frames from the cover-videos, namely, person.avi and conversation.avi, respectively.

On the basis of the visual comparisons of the results in Figs. 9 and 10, SCDH is capable of generating videos that are closest to the original cover-video sequences, with imperceptible noise. Unlike existing methods, SCDH yields imperceptible distortions in the videos. As shown in the previous section, the results were also evaluated quantitatively to prove the reliability of the proposed method.

The resulting stego-video frames obtained using SCDH and the state-of-the-art methods are in Fig. 9. It can be noted that, the result in Fig. 9b shows minimal distortion in the stego-video frame. Similarly, visual observations can be observed in Fig. 10b.

In the experiments, the secret messages were embedded in the transform domain of the video sequences, resulting in comparatively low distortions. Given the excellent spatial–temporal localization property of the DWT, the SCDH method enhances the security of the embedded message. The visual distortion level of the resultant video sequences after data hiding with the use of the SCDH method is lower compared with those when other methods are used. This result proves that the quality levels of the video sequences are not significantly altered because the DCT and DWT are employed for data hiding, and the changes are imperceptible to the HVS. Data hiding in the transform domain can be robust because the message embedded in the frequency bands of a video has the least impact on video quality. In summary, the proposed method not only achieves high embedding capacity but also improved data security in the DCT and DWT domains and preserves video quality with imperceptible distortions.

@&#CONCLUSION@&#

The proposed data-hiding method is performed in the transform domain and employs scene-change detection in videos. The method used the DCT and DWT coefficients of the cover-videos to preserve the video quality. The DCT coefficients of the videos were used for scene-change detection to improve the security level of the embedded message. The experimental results highlighted the superior performance of the proposed method in terms of security and quality. According to the results, the distortions in the videos were significantly minimized, and the quality levels of the videos were also maintained with the use of SCDH. Thus, the minimal changes caused by the secret-message embedding using scene-change detection contributed to the enhancement of security in SCDH.

The qualitative results of distortion occurrence in the resulting videos after data hiding also show that the overall distortion level resulting from the use of SCDH were lower than those of the other methods. This result indicated that the quality of the video sequences was not significantly altered when SCDH was used. This favorable result of SCDH was ascribed to the involvement of the DWT domain in data hiding, and the imperceptibility of the changes to the HVS. Furthermore, compared with existing methods, the proposed method achieved improved security by adopting a data-hiding process in the DCT and DWT domains and preserved good video quality as a result of imperceptible distortions.

@&#REFERENCES@&#

