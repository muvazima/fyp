@&#MAIN-TITLE@&#A novel power efficient adaptive RED-based flow control mechanism for networks-on-chip

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We suggest a new flow control mechanism for improving power and latency.


                        
                        
                           
                           We present a novel methodology to improve the buffer space utilization.


                        
                        
                           
                           Our method reduces queue blockages to determine a proper size for virtual channels.


                        
                        
                           
                           We apply queue length considerations of a modified version of RED algorithm.


                        
                        
                           
                           We utilize learning automata to adapt the threshold values of RED algorithm.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Network-on-chip (NoC)

Random Early Detection (RED) algorithm

Stochastic learning automata

Performance evaluation

Congestion control management

Power consumption

@&#ABSTRACT@&#


               Graphical abstract
               
                  
                     
                        
                           Image, graphical abstract
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

According to ITRS [1], with the advances in integrated circuit fabrication technology, the number of processing cores in on-chip multiprocessors will increase to 1000 by 2020. In addition, the number of processing and storage elements in on-chip systems has increased to 300 units by 2015. On the other hand, the rapid increase in functionality and complexity of the applications has increased the volume and complexity of the communication patterns within the chips. Therefore, a proper communication infrastructure is required to effectively deal with this inter-chip traffic. Applying packet switching mechanism has become inevitable due to lack of scalability and bandwidth constraints of the traditional crossings, along with the area overhead and the time-consuming design of specific point-to-point connections [2].

On-chip networks have been suggested as a viable solution to the chip-level communication problem. They include a set of functional units, which are connected to each other via a network switch. In many of these systems, the connection network is responsible for a significant portion of the consumed power, which makes power consumption a major constraint of designing such systems. With the increase in processing power and the emergence of large-scale applications, the challenging aspect of communication overheads in single-chip and on-chip systems has attracted much attention [3].

The issues of traditional communicating methods in addition to the benefits and great potential of the on-chip networks have made this communicational mechanism a promising and viable solution to the communication management on chips. Still, these networks suffer from some problems and issues that need to be addressed.

In late 2006, the OCIN meeting was held at Stanford University, with the audience comprising active researchers in the fields of communication networks, network-on-chip, and on-chip system design. The meeting was to review and identify challenges and to provide proper orientations of future researches to solve them. The outcomes of this debate have been published as an article [2]. The meeting identified three major deterrents to the industrial usage of on-chip networks as follows: high power consumption, considerable delay and the issues of adapting the existing CAD tools to the contemporary methods of designing on-chip systems.

In order to reduce the power consumption and increase the performance, various optimizations in the area of on-chip network design have been proposed. They have been proposed for different on-chip network problems such as layout designs, router microarchitecture design, mapping methods, and switching and flow control mechanism issues. Considering the aforementioned optimizations, flow control and buffering issues play a crucial role in power reduction.

Buffer management and the buffer size are key parameters in NoCs. The size of input channel buffers in each NoC router has a significant impact on the total chip area. For instance, increasing the buffer size in each input channel from two to three words in a 4 × 4 Mesh NoC, increases the router size by 30%. Meanwhile, according to the network workload, an increase in buffer size can defer the network transition to the saturation phase. In other words, buffer management is directly related to the flow control policies adopted by networks. The flow control, in turn, affects the network performance and resource usage. Hence, an appropriate flow control policy can make the network performance reach up to 80% of its theoretical value, while by adopting a weak policy, it may only reach 30% of this value [4]. Regarding the mentioned reasons, the method of dynamic buffer allocation has been proposed. In this method, there are buffers which are allocated to each router under special conditions and will be deallocated again. Some of such techniques will be studied in the next section.

One of the major issues of interconnection networks is congestion. In fact, congestion is considered to play a key role in achieving high efficiency and performance. Network congestion occurs when the bandwidth demand exceeds the available bandwidth. In lossless networks, congestion results in more delay and less efficiency. Therefore, not having a suitable congestion management leads to a significant decline in the overall network performance or a part of it. There are two general solutions to this problem: the proactive technique and the reactive technique. The proactive technique prevents the occurrence of congestion by increasing the available resources, whilst the reactive technique attempts to efficiently use the available resources. To this end, some algorithms must be used to analyze the network status in order to eliminate the congestion when the network enters saturation.

Virtual channels (VCs) were initially suggested as a solution to the deadlock problem [5]. The header blocking may occur in all flow control mechanisms, in which there exists a queue for each input. Blocked packets at their head become a bottleneck for other packets behind them, even if there exist sufficient resources for the blocked packets [6].

Basically, VCs divide the queue inside the router. Multiple VCs share physical links between routers. Header blockings are reduced by unifying several separate queues of each input. The arbitration policy of VCs multiplexes the bandwidth of each physical link in a round robin fashion. When a VC packet is blocked, other packets can pass the physical link through another VC. Therefore, VCs increase the functionality of the physical channels and improve the overall network performance. A physical channel and its corresponding VCs are illustrated in Fig. 1
                     . Technically, VCs can be applied to any flow control mechanism in order to reduce the blocking probability of the headers [7].

Artificial intelligence (AI) and adaptive approaches, especially machine learning techniques have an effective role in optimization algorithms. The goal of machine learning is to make the computer (in its most general concept) gradually achieve higher efficiency for an intended task. Machine learning has attracted much attention in recent years making the researchers apply machine learning techniques to a broad range of new problems.


                     Random Early Detection (RED) algorithm was initially applied to the congestion control problem in Transmission Control Protocol (TCP) based networks [8]. The algorithm either accepts or rejects the packets before entering the queue based on calculating the probability of queue occupancy (proportional to the queue length). This technique prevents hasty filling of the queue and strikes a balance between applications with a high input rate and the ones with a low input rate. The algorithm postpones the filing of the queue.

Discarding packets when the buffers are full is the easiest way to overcome congestion. This method, on which the RED is based upon too, is acceptable in lossy networks such as the Internet. In these networks, the latency of re-sending the packets is usually acceptable. On the other hand, in NoCs and High Performance Computing (HPC), the system cannot usually bear this delay. Hence, other techniques must be employed in such systems [9].

The approach proposed in this paper applies queue length considerations of RED algorithm to improve the performance and power consumption of NoCs. Moreover, a stochastic learning-automata-based algorithm has been used to optimize the threshold values required in the RED algorithm. In this method, RED algorithm has been modified for NoCs. Considering the fact that the system cannot bear discarding the packets, we abandon discarding in NoCs and only address the threshold values. Furthermore, a new router microarchitecture is provided, in which dynamic buffer size is used in the flow control of VCs. In addition to improving the buffer space utilization, this alteration can also enhance the network performance.

The remainder of the paper is organized as follows: Section 2 reviews the previous work relevant to this paper. Setting the buffer size and its implementation are also discussed in this section as pervious work. The necessary preliminaries and backgrounds such as the structure of NoCs, RED algorithm, stochastic learning automata and router microarchitecture are provided in Section 3. The proposed approach to reducing queue blockages, power consumption and average latency is explained in detail in Section 4. In Section 5, the proposed approach is evaluated and the experimental results are given under various synthetic traffic patterns for different injection rates and trace-driven SPLASH-2 benchmark suite. Finally, in Section 6, conclusions are provided.

@&#RELATED WORK@&#

Because of the importance of buffers in on-chip networks, setting the buffer size has been discussed in many papers. The existing related literature employs static [10] and dynamic [4,11] methods for adjusting the buffer depth and the number of VCs in on-chip networks. For instance, in [11], each network node considers all possible output ports for sending packets and assigns some weight to each one. The weight is assigned based on different metrics including the distance to the destination port and also the workload of the port. A high-weight packet is sent through the port and the depth of the buffers allocated to each port is set according to the port traffic. In this approach, all the router ports exploit a central buffer that determines two pointers to the appropriate buffer's bounds. The proposed method significantly improves the resource efficiency. This work does not conduct an analysis of the power, which is expected to deteriorate due to the high overhead of calculating the weight of each port in addition to the overhead of the shared buffer structure.

The authors in [4] proposed a register-based implementation of the buffer. Considering the instant network workload, the number of VCs and VC buffer depth are dynamically adjusted in this approach. The proposed technique carries out the flit buffer allocation and VC management in each cycle. Although the approach increases the network performance, a fully connected network between input ports and the buffers increases the power consumption of on-chip networks. In [10], a static mechanism has been provided to regulate the router buffers of the network based upon application-driven traffic. In fact, this method distributes limited available buffers among the input ports of the routers considering the application traffic pattern and the on-chip network parameters such as connectivity and routing algorithm. The experimental results report an 80% reduction in on-chip buffers without any performance overhead.

Applying a central buffer instead of allocating separate buffers to each port, causes a dramatic increase in the buffer efficiency and network output [12]. The efficiency improvement leads to an increase in the size and power consumption of the crossbar and in addition to increasing the hardware complexity of the required buffer controller [13]. Sharing buffers among adjacent ports necessitates a trade-off between the complexity of the crossbar and the efficiency of the buffers.

Thus far, we have just discussed the methods that work on the buffer management and size. Still, there are other ways to deal with congestion which are somewhat similar to what is done in this paper; therefore, they are worth mentioning as the related work. There exist many methods to overcome congestion such as switch-to-switch flow control, end-to-end flow control, prediction-based flow control and adaptive routing, to name a few [14]. In switch-to-switch flow control mechanisms, the router exchange information locally and with neighbors. Meanwhile, in end-to-end flow control mechanisms, the control signals are sent to the source and destination instead of neighbors, depending on the occurrence of congestion. Prediction-based flow control mechanisms focus on the packet injection rate. When congestion is predicted, the packets are prevented from being injected in order to avoid congestion. Adaptive routing mechanisms, also known as load balancing, deal with congestion by diverting packets to routes with low traffic.

Two instances of methods in which the dynamic memory allocation is used are reported in [15] and [16]. In [15], the dynamically-allocated memory is referred to as Set Aside Queue (SAQ). In fact, SAQ is allocated dynamically in order to store packets belonging to a specific congestion tree. Once the congestion has been eliminated, this SAQ is released to be allocated to another congestion tree. In addition, packets which belong to uncongested flows are stored in the main queue. Thus, data is distributed among the main queue and a set of SAQs. After detecting congestion, the information of the congested flow is stored in a control CAM memory in order to facilitate and expedite identifying whether an incoming packet belongs to a congested flow or not. If the packet entering a node corresponds to a congested flow whose information has already been stored in the CAM, the router will allocate a different memory to store this packet. Therefore, packets of each congested flow are stored in a separate memory. When the congestion disappears or the corresponding queue is empty, the router's main memory will retrieve the allocated memory. In [16], a dynamic memory allocation, named Hot-Flow Dynamic Isolation (HFDI), has been mentioned. This article also uses a CAM memory to expedite the detection of a congested flow. In order to prevent and eliminate HOL blocking, each congested flow is dynamically stored in a separate buffer.

In the proposed method, we utilize adaptive machine learning algorithms to evaluate the dynamic behavior of the network and to precisely calculate the related parameters. Consequently, we achieve a substantial reduction in packet blocking and power consumption. In addition, we can accurately estimate the required number of VCs.

This section presents the basic concepts, definitions, and descriptions of the proposed algorithm.

The components of on-chip systems are connected through a router-based network. On-chip networks are comprised of three major parts including the router, network interfaces and interconnections between routers. In each network node, the processing element (PE) is connected to a router and the router is linked to the other routers through point-to-point connections in an interconnection network. The nodes are linked to each other by generating data packets and sending them through this communication infrastructure as shown in Fig. 2.
                        
                     

The routers consist of input–output buffers, a crossbar switch, and a controller. Buffers can be implemented by means of registers or a static memory. Each buffer usually contains a part of a packet. As mentioned earlier and used in this paper, there exist dynamic buffers that have varying sizes according to different conditions, in contrast to static buffers with fixed sizes. You may refer to [17] as an instance of such buffers. In a router with P input and P output ports, a crossbar with a size of P × P provides connections between input and output ports. The controller is composed of an output channel selector unit, VCs, and priority arbitration unit. Its task is to direct input packets to output ports.

Random Early Detection (RED) of packets is one of the active queue management algorithms. The algorithm is considered as a congestion control mechanism in the internet network [18]. However, RED algorithm or any of its refinements have not yet been used in NoCs. In contrast, Droptail algorithm has been applied to NoCs. In this algorithm the packets are discarded once the queue is full and the network in fact reaches saturation. Compared to Droptail, RED algorithm shares the buffer spaces between traffic flows more fairly. The Classic algorithm of RED computes the average queue length and drops packets based on the calculated statistical probabilities. Since discarding packets in interconnection networks is not bearable, another operation will be performed in the proposed method instead of package dropping, which will be explained in the following sections.

As the queue length increases, the input packets become more likely to be removed. When the buffer is almost full, this probability approaches 1, and all the received packets will be dropped.

When the probability of discarding packages reaches 1, it implies that all the packets will be dropped. In a version of RED which is also used in the internet network, many things are done in order to prevent the queue from entering the region above the maximum threshold. In the algorithm we used, no packets are discarded, ensuring that it will not exceed the threshold due to package dropping. In our algorithm, entering this region is only used to dynamically allocate more buffers to the queue. The higher the rate at which a source sends its packets, the more likely a packet is to be removed. The reason behind this is the fact that the removal possibility of a packet for a specific source depends on the size of data in the queue. Parameters and the notations used in RED algorithm description are as listed in Table 1.
                        
                     

By applying a low pass filter, we can calculate the weighted mean of the queue length according to the length of the current queue and the average queue length, denoted by parameters Wq
                         and Avg, respectively. This is obtained by

                           
                              (1)
                              
                                 
                                    Avg
                                    =
                                    
                                       (
                                       1
                                       −
                                       
                                          W
                                          q
                                       
                                       )
                                    
                                    ×
                                    avg
                                    +
                                    
                                       W
                                       q
                                    
                                    ×
                                    queu
                                    
                                       e
                                       
                                          l
                                          e
                                          n
                                       
                                    
                                 
                              
                           
                        where queue
                           len
                         denotes the length of queue and avg parameter reveals the current queue length.

The algorithm performs based upon the average queue length and two threshold values, minimum (min
                           th
                        ) and maximum (max
                           th
                        ) thresholds. In brief, its mechanism is as the following: if the average queue length is less than the minimum threshold value, the packets will not be discarded; if the average queue length is between the minimum and maximum threshold values, packets are discarded with a specific probability, Pa
                        , which is expressed as

                           
                              (2)
                              
                                 
                                    
                                       P
                                       a
                                    
                                    =
                                    
                                       
                                          P
                                          b
                                       
                                       
                                          1
                                          −
                                          
                                             P
                                             b
                                          
                                          ×
                                          count
                                       
                                    
                                 
                              
                           
                        where count is the number of packets received before the last dropping and Pb
                         is the immediately marking probability [18] which is calculated as

                           
                              (3)
                              
                                 
                                    
                                       P
                                       b
                                    
                                    =
                                    
                                       max
                                       p
                                    
                                    
                                       (
                                       
                                          
                                             avg
                                             −
                                             mi
                                             
                                                n
                                                
                                                   t
                                                   h
                                                
                                             
                                          
                                          
                                             ma
                                             
                                                x
                                                
                                                   t
                                                   h
                                                
                                             
                                             −
                                             mi
                                             
                                                n
                                                
                                                   t
                                                   h
                                                
                                             
                                          
                                       
                                       )
                                    
                                 
                              
                           
                        where max
                           p
                         is the maximum probability of dropping a packet. Finally, if this value exceeds the maximum threshold value, all incoming packets will be dropped.

Upon the arrival of a packet, if the average queue length (Avg) is between the two thresholds, the counter variable is incremented by one unit. Therefore, the amount of Pa
                        (from Eq. (2)) will increase which causes the number of wasted packets to increase. Fig. 3
                         depicts the pseudo-code and the graph of the dropping probably according to the average queue length.

In different studies, several approaches have been adopted that intend to keep the queue length in an administrator-specified amount. This approach has a fundamental issue, which is the lack of versatility under different traffic loads, leading to a lack of scalability. In other methods the threshold value is considered to be constant. For example, in [18] the maximum threshold value is assumed at least twice the minimum threshold.

In this paper, the threshold values of RED algorithm are set dynamically by means of learning automata. It is worth mentioning again that RED algorithm has not yet been used in any other work and articles on NoC to date, and we applied this algorithm after some alterations, to NoCs for the very first time as our novel contribution.

Considering the fact that in this paper, the minimum and maximum threshold values of RED algorithm are determined dynamically by a Learning Automata, it is of necessity to explain it more in detail.“An automaton is a machine or control mechanism designed to pursue a predetermined sequence of operations” [19]. The stochastic nature of the automaton is the result of an adaptive learning process. As can be seen in Fig. 4
                        , according to the selected state and given parameters, the automata changes its inner state and chooses the next state.

Formally, automaton can be represented by a quintuple {Φ, α, β, F( ·, ·), H( ·, ·)}[19], where Φ is a set of internal states and is described as

                           
                              (4)
                              
                                 
                                    Φ
                                    
                                       =
                                       {
                                    
                                    
                                       ϕ
                                       i
                                    
                                    :
                                    ∀
                                    i
                                    ,
                                    1
                                    ≤
                                    i
                                    ≤
                                    s
                                    }
                                 
                              
                           
                        
                        α is a set of actions. The formal description of αis as below

                           
                              (5)
                              
                                 
                                    α
                                    
                                       =
                                       {
                                    
                                    
                                       α
                                       i
                                    
                                    :
                                    ∀
                                    i
                                    ,
                                    1
                                    ≤
                                    i
                                    ≤
                                    r
                                    }
                                 
                              
                           
                        
                        β is a set of responses, formally described as below:

                           
                              (6)
                              
                                 
                                    β
                                    =
                                    
                                       β
                                       i
                                    
                                    :
                                    ∀
                                    i
                                    ,
                                    1
                                    ≤
                                    i
                                    ≤
                                    m
                                    
                                    
                                       o
                                       r
                                    
                                    
                                    β
                                    =
                                    
                                       {
                                       
                                          (
                                          a
                                          ,
                                          b
                                          )
                                       
                                       }
                                    
                                 
                              
                           
                        
                        F( ·, ·): Φ × β → Φ is a function that maps the current state and input to the next state.

                           
                              (7)
                              
                                 
                                    ϕ
                                    (
                                    n
                                    +
                                    1
                                    )
                                    =
                                    F
                                    [
                                    ϕ
                                    (
                                    n
                                    )
                                    ,
                                    β
                                    (
                                    n
                                    )
                                    ]
                                 
                              
                           
                        
                        H( ·, ·): Φ × β → αis a function that maps the current state and input to the current output.

                           
                              (8)
                              
                                 
                                    α
                                    (
                                    n
                                    )
                                    =
                                    G
                                    [
                                    ϕ
                                    (
                                    n
                                    )
                                    ]
                                 
                              
                           
                        
                     

In order to describe the reinforcement scheme, p(n) is defined as a vector of action probabilities

                           
                              (9)
                              
                                 
                                    
                                       P
                                       i
                                    
                                    
                                       (
                                       n
                                       )
                                    
                                    =
                                    P
                                    
                                       (
                                       α
                                       
                                          (
                                          n
                                          )
                                       
                                       =
                                       
                                          α
                                          i
                                       
                                       )
                                    
                                    ,
                                    1
                                    <
                                    i
                                    <
                                    r
                                 
                              
                           
                        
                     

Updating action probabilities can be represented as

                           
                              (10)
                              
                                 
                                    P
                                    (
                                    n
                                    +
                                    1
                                    )
                                    =
                                    T
                                    [
                                    p
                                    (
                                    n
                                    )
                                    ,
                                    α
                                    (
                                    n
                                    )
                                    ,
                                    β
                                    (
                                    n
                                    )
                                    ]
                                 
                              
                           
                        where T is a mapping. The formula expresses that the next action probability 
                           
                              P
                              (
                              n
                              +
                              1
                              )
                           
                        is updated based on the current probability P(n), the input from the environment and the resulting action. The general scheme for updating action probabilities of an r-action automaton in an environment with 
                           
                              β
                              =
                              {
                              0
                              ,
                              1
                              }
                           
                         is as

If 
                           
                              α
                              
                                 (
                                 n
                                 )
                              
                              =
                              
                                 α
                                 i
                              
                           
                        , when 
                           
                              β
                              =
                              0
                           
                         then we get

                           
                              (11)
                              
                                 
                                    
                                       P
                                       i
                                    
                                    
                                       (
                                       n
                                       +
                                       1
                                       )
                                    
                                    =
                                    
                                       P
                                       i
                                    
                                    
                                       (
                                       n
                                       )
                                    
                                    +
                                    a
                                    
                                       [
                                       1
                                       −
                                       
                                          P
                                          i
                                       
                                       
                                          (
                                          n
                                          )
                                       
                                       ]
                                    
                                 
                              
                           
                        
                        
                           
                              (12)
                              
                                 
                                    
                                       P
                                       j
                                    
                                    
                                       (
                                       n
                                       +
                                       1
                                       )
                                    
                                    =
                                    
                                       (
                                       1
                                       −
                                       α
                                       )
                                    
                                    
                                       P
                                       j
                                    
                                    
                                       (
                                       n
                                       )
                                    
                                    
                                    ∀
                                    j
                                    ,
                                    j
                                    ≠
                                    i
                                 
                              
                           
                        and when 
                           
                              β
                              =
                              1
                           
                         we get

                           
                              (13)
                              
                                 
                                    
                                       P
                                       i
                                    
                                    
                                       (
                                       n
                                       +
                                       1
                                       )
                                    
                                    =
                                    
                                       (
                                       1
                                       −
                                       b
                                       )
                                    
                                    
                                       P
                                       i
                                    
                                    
                                       (
                                       n
                                       )
                                    
                                 
                              
                           
                        
                        
                           
                              (14)
                              
                                 
                                    
                                       P
                                       j
                                    
                                    
                                       (
                                       n
                                       +
                                       1
                                       )
                                    
                                    =
                                    b
                                    /
                                    
                                       (
                                       r
                                       −
                                       1
                                       )
                                    
                                    +
                                    
                                       (
                                       1
                                       −
                                       b
                                       )
                                    
                                    
                                       P
                                       j
                                    
                                    
                                       (
                                       n
                                       )
                                    
                                    
                                    ∀
                                    j
                                    ,
                                    j
                                    ≠
                                    i
                                    ,
                                 
                              
                           
                        
                        
                           
                              (15)
                              
                                 
                                    0
                                    <
                                    
                                       g
                                       k
                                    
                                    
                                       (
                                       P
                                       
                                          (
                                          n
                                          )
                                       
                                       )
                                    
                                    <
                                    
                                       P
                                       k
                                    
                                    
                                       (
                                       n
                                       )
                                    
                                 
                              
                           
                        where gk
                         and 
                           
                              
                                 h
                                 k
                              
                              ,
                              k
                              =
                              1
                              ,
                              2
                              ,
                              …
                              ,
                              r
                           
                         are continuous, nonnegative functions with the following assumptions:

                           
                              (16)
                              
                                 
                                    0
                                    <
                                    
                                       ∑
                                       
                                          k
                                          =
                                          1
                                          ,
                                          k
                                          ≠
                                          i
                                       
                                       r
                                    
                                    
                                       
                                          P
                                          k
                                       
                                       
                                          (
                                          n
                                          )
                                       
                                       +
                                       
                                          h
                                          k
                                       
                                       
                                          (
                                          P
                                          
                                             (
                                             n
                                             )
                                          
                                          )
                                       
                                    
                                    <
                                    1
                                    ,
                                    
                                    1
                                    <
                                    i
                                    <
                                    r
                                    ,
                                    0
                                    <
                                    
                                       P
                                       k
                                    
                                    <
                                    1
                                 
                              
                           
                        
                     

More details about the stochastic learning automata can be found in [19] for the interested reader.


                        Fig. 5
                         demonstrates the microarchitecture of the router based on VCs. Furthermore, it describes how the router operates. The router is supposed to be a 2-D shape which has five input/output ports, four of which correspond to its adjacent lanes and one port for the local processing elements (PE). The main components of the router include the input buffers, logic counters of path, VC allocator, switch allocator, and crossbar switch.

Switches may have buffers in input, Input Queued (IQ) switches, or a combination of buffers both in input and output, Combined Input and Output Queued (CIOQ) switches. Lower cost of IQ switches makes them more desirable [17]. As it can be seen in Fig. 5, the proposed router microarchitecture also uses IQ switches. Such buffers allow using only one single-port memory. For the time being, we assume that any VC input port has a buffer queue with a depth of four. Buffers are responsible for storing input data. If source routing is not to be used, the computed route remains closed as long as the output ports for these packets are not modified. The allocator (i.e. the channel and switch) makes the selected flits continue their progress until latitudinal lines are cut. Finally, the existing switches in latitudinal lines transfer the flits from their input ports to their output ports. Fig. 5 does not demonstrate the RED and learning automata blocks. These blocks are elaborated on and illustrated in the following section.

In this section, the proposed NoC architecture is presented. To this end, the system model and some basic assumptions are initially described, followed by an explanation of the proposed methodology. Eventually, the suggested architecture is clarified.

The system consists of a set of nodes interconnected by a mesh topology. Nodes of the network (referred to as PEs) are populated with processing and/or storage elements that communicate with each other through the network. Moreover, a conventional five-stage pipelined router is employed [20], which serves as a baseline NoC router. The router has input and output ports corresponding to four neighbors and the local PE port. The major components constituting the router include input buffers, routing computation (RC), VC allocator (VA), switch allocator (SA), and crossbar switch.

Given very limited buffer space in resource-constrained on-chip networks, in comparison with store-and-forward (SAF) and virtual cut-through (VCT) [20] switching mechanisms, routers tend to employ the wormhole flow control [20]. This relaxes the constraints on the buffer size.

In order to remove the serialization delay, we have exploited the look ahead routing (LA) [21]. The route of a packet is determined one hop in advance, enabling the flits to compete for VCs immediately after the write stage of the buffer.

In the proposed approach, we considered some priorities for sent packets. In other words, we prioritized VCs such that each packet enters its related VC according to its priority. In this method, switching of packets is accomplished based on a given priority. However, it should be taken into consideration that if the switching mechanism is used on the basis of the relative priority of each channel, the channel with the maximum priority will always transmit its packets from the switch outside. To deal with this issue, which is known as starvation, the priorities of packets were calculated on the basis of a relative priority and switching was performed accordingly. In this way, services are provided with regard to the waiting time of packets in the queues. The relative priority is calculated as

                           
                              (17)
                              
                                 
                                    
                                       Relative
                                       
                                       Priority
                                       =
                                    
                                    
                                       
                                          Waiting
                                          
                                          time
                                          
                                          in
                                          
                                          the
                                          
                                          queue
                                          +
                                          Priority
                                       
                                       Priority
                                    
                                    +
                                    Priority
                                 
                              
                           
                        
                     

As can be seen, the priory changes dynamically over time, that is, the longer a packet remains in the queue, its priority rises and becomes more likely to leave. This prevents starvation. How this priority is used and what happens in case of equal priorities, are thoroughly explained in Section 4.5 which presents the proposed router microarchitecture.

The format of packets is illustrated in Fig. 6
                        . As we use the current control, packets should be divided into a number of flits to become able to be transmitted to the relevant destinations.

In order to use this type of current control we must divide packets into three types of flits: the header flit, the body flit and the tail flit. In this scheme, MsgID is a unique determinant, consisting of the sender ID, packet serial number, and receiver ID. The source and destination addresses are located in SrcAdd and DesAdd fields, respectively. The Relative Priority field contains the relative priority of each packet. The Relative Priority field was calculated according to Eq. (17). This field is added to the packet structure of NoCs in the proposed method. Adding this field imposes overhead, however, it addresses some major issues such as loss of an important packet and starvation. A packet with high priory (its priory might have been initially set high or has been raised over time because of waiting) may use the memory allocated to packets with lower priorities.


                        Data field is considered the payload of the packet. Body and tail packets are distinguished by their IsTail field value. The value 1 indicates that it is a tail packet, while 0 means the packet is of the body type.

Two buffer structures are generally used in NoCs: unified and non-unified. These two structures are depicted in Fig. 7
                        . Non-unified structure, also named split buffer, is the traditional and common structure used in NoCs. In this structure, each buffer corresponds to a specific input, thus different buffers cannot be used for an input. On the contrary, in a unified structure, as can be seen in Fig. 7(b), a set of buffers are used for all inputs. The more optimized utilization of resources in this structure lowers the total cost of the router. Yet, the main drawback to this structure is its more difficult implementation due to components such as the Buffer Control Unit (BCU).This unit is designed in this paper which is completely described in the next section. Moreover, Fig. 13 gives the details of the BCU.

In the proposed method, an arriving flit is processed by the BCU Unit. Moreover, this unit manages these packets until the departure time from the router. The structure of the BCU unit is depicted in Fig. 8.
                        
                     

The blocks of the VCT, Available Slots Table (AST) and Allocated Slots Unit (ASU) are explained in this section and the two blocks of the learning automata and the proposed algorithm are described in the following section. Also, the relation between blocks is described in Section 4.6 and illustrated in Fig. 13. The proposed method checks if there is enough space for a flit upon its arrival. To this end, a table is required to keep track of available slots, which is called the AST. Whenever a VC needs a slot, it is allocated to the corresponding slot.

If there is not enough space for the incoming flit, it will be dropped. Fig. 9
                         shows the structure of the AST module. This table is in fact a one-dimensional array which stores a list of the empty slots. Initially, this list is full since all the slots are empty at first. Over time the used slots are removed from the list until no more slots are available. As mentioned earlier, in this situation flits are actually dropped. This phenomenon becomes more likely to happen by increasing the injection rate and will continue until the network reaches saturation. Reaching saturation occurs later than usual when using this algorithm, which can be clearly seen in the simulation results illustrated in Section 5.3. In case of sufficient space the incoming flit is placed in the table considering its relative priority filed.

In fact, flits with higher priorities use slots of higher priorities. Moreover, in the case of no available slots for flits with high priorities, slots can release flits with lower priorities in order to be allocated to the ones of high priorities. The other information required is how the priority is set in each VC. To this end, we have another table, named the VC Control Table (VCT), which is shown in Fig. 10
                        . For each VC inside each node, a table akin to the one depicted In Fig. 10 is filled. As the parameters in table indicate, it is used to store the priority of VCs in addition to the queue threshold values of each VC. The threshold values are set dynamically over time by the leaning automata.

Flits leave according to the relative priority filed value, which was calculated by Eq. (17). As stated before, some priority mechanisms are based on the departure time of packets from the router. Through this method, packets with the maximum priority will depart the router sooner. In case of incoming packets with equal priorities, FIFO policy is applied. It is possible for the high-priority packets in VCs to pass through the router, which will likely cause the starvation phenomenon for the low-priority packets already existing in VCs. Consequently, packet priorities must change dynamically. That is, when a packet with lower priority is waiting to leave, its priority increases over time.

ASU is another part of BCU. This unit keeps track of the slots allocated to VCs. This table is required due to the variable space used by each VC and the need to know the slots allocated to each VC for future retrieval, after a VC releases the slots. Fig. 11
                         shows the configuration of ASU.

This section elaborates on the two blocks of learning automata and improved RED algorithm operations.

In the proposed mechanism to control the packet flow a flit is initially checked for available slots upon its arrival at the router. If such a slot exists, subsequently the priority of the incoming flit is identified to determine which channel the flit wants to settle in. Next, the first free slot is identified to be assigned to the incoming flit, using the AST. The flit is then placed in the slot. After successful allocation, the slot number should be removed from the AST and added to the ASU as a used slot.

If there is no adequate space for the incoming flit, other VCs with the lower priorities should be examined one by one. If any of them has an available empty slot, it is deallocated and assigned to the applicant VC, therefore the incoming flit will be able to settle therein.

Eventually, if there is no available space in VCs with lower priorities no corresponding packet will be actually dropped. This is when the network transition to saturation begins.

In order to decrease the probability of losing flits due to the lack of empty slots, the proposed approach uses a modified version of RED algorithm. As shown in Table 1, RED algorithm has various parameters. In the proposed method, two of them, the minimum and maximum threshold values, are set dynamically by the learning automata algorithm. The rest of the parameters are initialized statically and have fixed values. In order to gain an optimized value for the fixed parameters, we used parameter sweeping in our simulation. These values are given in Table 4 in Section 5.3.

After calculating Pa
                         using Eqs. (1)–(3) this probability is used to increase the queue length to prevent the packets from being dropped. As mentioned before, space is dynamically given to and retrieved from the queue. It should be taken into consideration that the buffer of each queue consists of a dynamic and a static part. As a consequence, even if the size of the dynamic part reaches zero, its buffer size will not become zero.

In the proposed method, after a flit arrives at the channel, the learning automata algorithm is initially applied to obtain the threshold parameters. As all learning automata algorithms include some actions, we have defined these actions for our algorithm in Table 2
                        . As can be seen in the table, we have seven actions with the same initial probability of 0.14. Also, the values of parameters value0 and value1 are zero and one flit, respectively, for all actions. The probability of each action changes over time according to the responses received from environment. These actions are used to calculate the threshold values of RED algorithm. The action with the maximum probability is more likely to be selected from the table to update the threshold values (this selection is initially random since the probabilities are equal). As it can be seen in Fig. 13, the learning automata unit gets a feedback from the Compute the avg. busy time unit. The feedback is the respond received from the environment, based on which β is calculated. The feedback is considered a desirable response if its value is less than a threshold value which is specified as the latest blocking time of channel (in this condition β = 0). We alter the probabilities using Eqs. (11) and (12), increasing the probability of the corresponding action while reducing it for the rest of the actions. On the contrary, the value of the feedback exceeding the specified threshold indicates an undesirable response (β = 1). In this condition Eqs. (13) and (14) are used for updating the probabilities. Now, the probability of the corresponding action is reduced while it is increased for the rest of the actions. The learning automata used in the proposed algorithm is of Linear Reward Penalty (LRP) type. Parameters a and b in the mentioned equations denote the reward and fine coefficients in this type of automata, respectively. As shown in Table 4, these parameters are both equal to 0.11.

Thus far, we have described each individual part of the algorithm. It is now time to put together all these parts in order to gain an insight into the entire proposed algorithm. This is also presented in the pseudo code and the flowchart provided in Figs. 12
                         and 13
                        , respectively.

A separate part is considered for the learning automata which determines the minimum and maximum threshold values of RED algorithm, using the input information and its internal static parameters a and b. In the given pseudo-code, this function is named automata (). This part can also be found in the flowchart of Fig. 13 as a separate block, which receives some information from other blocks and sends its output to the RED algorithm block. RED algorithm updates its threshold values regarding the received information. In addition to this part, we need another part to check for available slots upon the arrival of a packet. If an available slot for the received packet exists according to the packet priority, it will be placed in the slot while updating the ASU and AST and calculating the average queue length using Eqs. (1)–(3). Subsequently, similar to RED algorithm, this calculated length is compared with the minimum and maximum threshold values to determine which region it belongs to.

If the average length exceeds the maximum threshold with a probability of 1, a free slot will be added to the corresponding queue, increasing its length dynamically. This, which is also referred to as slot pre-allocation, is done to prevent the flits from being dropped due to lack of empty slots. After doing so, the tables should be updated. If a previously allocated dynamic free slot exists and the average length value is less than the minimum threshold, this slot will be deallocated. Eventually, if the average length is between the minimum and maximum thresholds, the dynamic free slot will be added to the buffer with a probability of Pa. All of these scenarios are depicted in Fig. 13.
                     

On condition of no empty slots, it is necessary to see whether there are available slots in VCs with lower priorities. To do so, the VCT must be looked up. Then, the ASU figures out if the selected VC contains empty slots. If so, the slot will be removed from the VC with lower priority and allocated to the incoming flit. Moreover, entire tables should be updated. Otherwise, the flit will be discarded. An interesting point behind in this phenomenon is that the network begins to reach the saturation region, resulting in more and more flits to be discarded.

@&#EVALUATION@&#

In this section, the proposed mechanism is evaluated. To this purpose, at first the evaluation methodology is presented, followed by a brief description of the traffic patterns used for comparison. Finally, this section ends by providing the results and analysis.

To evaluate the proposed on-chip communication scheme, we implemented the proposed algorithm by means of XMulator, a fully parameterized simulator for interconnection networks [22]. The simulator is augmented with ORION power library [23] to calculate power consumption of networks. Furthermore, in ORION library, the process feature size and the working frequency of NoC were set to 65 nm and 150 MHz, respectively.

Also, for comparison in the real traffic the proposed mechanism was compared with the conventional NoC in addition to the Express Virtual Channel (EVC) scheme, which is described in [24]. The dynamic memory was implemented by means of linked lists and we utilized predefined functions to work with them. All the simulations were performed in a 7 × 7 Mesh topology consisting of 49 nodes. In the simulation process, we set the number of virtual channels to 7 and the size of each flit to 128 bits. The length of each packet was 20 flits, that is, 2560 bits. The details of simulation is reported and listed in Table 3.
                        
                     

Moreover, the routers were implemented in VHDL and synthesized using Xilinx ISE toolkit. Xilinx Virtex-5 FPGA family was used as the platform applied for implementing the methodology. In HDL code, each table was implemented using a register that is read from asynchronously and written into synchronously with clock. The power and area consumption of the router proposed in this paper is naturally more than the base router, which features no corresponding tables or packet priorities. Still, the primary objective of this implementation was to testify that the idea used in this paper is implementable on FPGA. This additional power consumption which is much lower compared to the power consumed by connections and other components was disregarded in our simulations. In fact, the Orion library completely ignores this power consumption.

In order to evaluate the behavior of the network, we applied two different traffic patterns: synthetic and trace-driven patterns [20].

Synthetic patterns are widely used since they allow evaluating network in the most generic way. When used, every node has the same traffic injection rate. We evaluated the complete range of traffic injection rates, from light traffic up to the saturation point. The applied synthetic traffic pattern was Hotspot [20]. In the hotspot, 90% of sources (selected randomly) inject traffic to the same destination (selected randomly); the rest of end nodes inject traffic to random destinations. This traffic pattern allows modeling situations in which one or more end nodes are accessed frequently by the remaining end nodes (a disk server for instance).

On the other hand, in order to assess the performance of the proposed mechanism, we performed simulations with some benchmarking application task-graphs. The benchmark suite includes some existing SoC designs, which have been widely used in the literature: multiwindow display (MWD) with 12 cores, video object plane decoder (VOPD) with 16 cores, H.263 encoder (H.263) and MP3. These benchmarks employ same cores. The task-graph-based traffic generation approach is introduced and used in [25].

Moreover, trace-driven patterns are based on capturing traffic when running real applications. Traces contain the source, destination, injection time and size of the transmitted packets. They allow obtaining results in more realistic scenarios and let us compare them with the results obtained by the synthetic patterns. The traces are collected by executing FFT, LU, BARNES, RADIX, OCEAN, RAYTRACE WATER-NSQUARED, and WATER-SPATIAL applications from SPLASH-2 [25]. These types of applications are widely used when simulating multiprocessor shared memory systems in engineering and scientific computations.

@&#RESULTS AND ANALYSIS@&#

In this section, we first compare the proposed router with those of the conventional NoC [20] as a baseline router and EVC [24], in order to gain a realistic understating of NoC improvements by our approach regarding different evaluation parameters.

In Figs. 14
                         and 15
                        , the average packet latency and the total power consumption in the conventional NoC, EVC and the proposed mechanism under the hotspot traffic pattern are presented.


                        Figs. 16
                         and 17
                         compare the latency and power consumption obtained by applying the proposed NoC with the conventional NoC and EVC under eight SPLASH-2 traces. The outcomes were normalized to the results achieved by the proposed approach. On an average, our approach outperforms the conventional NoC and EVC.

We also compared the proposed design with the conventional NoC and EVC under different application-specific benchmarks to illustrate the significant improvements obtained by the proposed methodology compared with other alternatives. Fig. 18
                         compares the latency of the proposed mechanism with the latency of the conventional NoC and EVC. As can be seen in this figure, the proposed approach consistently outperforms the conventional NoC and EVC.


                        Fig. 19
                         also illustrates the power consumption of considered NoCs. The outcomes were normalized to the results given by the proposed NoC. As this figure indicates, the power consumption values follow the same trend as the latency results, with the proposed method outperforming the conventional NoC and EVC.

As stated earlier, in order to gain accurate results in the presented method, we need to properly determine Wq, Max
                           p, a and b values. Table 4
                         indicates optimal values for these parameters. The values were obtained by performing consecutive simulations and tests. Each of these parameters was swept and the best value for latency and power consumption was chosen.


                        Table 5
                         demonstrates the conclusions extracted from the performed evaluations. As an overall conclusion, it can be seen that in terms of latency and power, our proposed approach has superior performance over the conventional NoC and EVC scheme. These results are the average of all results given before (Figs. 14–19).

For all benchmarks and injection rates, the average speedup of the proposed mechanism was calculated (i.e. the ratio of latency and power consumption of the conventional NoC and EVC scheme to the same quantities of the proposed mechanism) included in Table 5. Compared to the conventional NoC, the algorithm improved the latency and power consumption by 23% and 52%, respectively. It also showed 13% and 36% improvement in latency and power consumption, respectively, compared with EVC.

@&#CONCLUSIONS@&#

In this article, a new flow control mechanism was suggested for improving power consumption, latency and the efficiency of networks-on-chip (NoCs). Once a flit arrives, its priority is initially checked and an appropriate virtual channel (VC) is subsequently prepared to be assigned to it. Preparation of VCs is done by means of a modified RED algorithm. In the procedure, upon the arrival of a flit, assigning some other flits with the same priority to that VC is feasible. Therefore, adjusting the VC after the arrival of each flit is a reasonable and preventive method. For the sake of this adjustment, we used learning automata algorithm alongside the modified RED. By getting feedback from the network, the algorithm accurately adjusts the thresholds of RED algorithm. Due to the dynamic adjustment of the VC size, the buffer space is optimally utilized; which decreases the amount of the packet loss. Consequently, it reduces the number of retransmissions or waiting time of packets due to the lack of sufficient space, which contributes to a considerable decrease in the delay and power consumption. The results showed that an accurate buffer control could decrease the power consumption. On the other hand, due to the lack of sufficient buffer space there are some occasions in which data retransmission is likely to occur, leading to additional power consumption by the network. This extra power consumption was considered in all simulation experiments of the article. The suggested method may be considered a suitable technique for the applications that are prone to the errors occurred due to the lack of sufficient buffer space. The results revealed that the offered mechanism outperforms the conventional NoC with 23% and 52% enhancement in delay and power consumption, respectively. Further, 13% and 36% improvement in the same parameters were achieved in comparison with EVC.

@&#REFERENCES@&#

