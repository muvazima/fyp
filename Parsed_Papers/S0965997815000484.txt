@&#MAIN-TITLE@&#A multiple-GPU based parallel independent coefficient reanalysis method and applications for vehicle design

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           An independent coefficient reanalysis method is reconstructed for multi-GPU platform by using MPI and CUDA.


                        
                        
                           
                           The data partition for multiple GPUs is successfully implemented to achieve good load balance.


                        
                        
                           
                           The suggested non-blocking communication strategy achieves higher speedups compared with blocking one.


                        
                        
                           
                           The bottleneck of GPU memory can be solved.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Parallel

GPU

MPI

Reanalysis

Vehicle design

@&#ABSTRACT@&#


               
               
                  The main limits of reanalysis method using CUDA (Compute Unified Device Architecture) for large-scale engineering optimization problems are low efficiency on single GPU and memory bottleneck of GPU. To breakthrough these bottlenecks, an efficient parallel independent coefficient (IC) reanalysis method is developed based on multiple GPUs platform. The IC reanalysis procedure is reconstructed to accommodate the use of multiple GPUs. The matrices and vectors are successfully partitioned and prepared for each GPU to achieve good load balance as well as little communication between GPUs. This study also proposes an effective technique to overlap the computation and communication by using non-blocking communication strategy. GPUs would continue their succeeding tasks while communication is still carried out simultaneously. Furthermore, the CSR format is used in each GPU for saving the memory. Finally, large-scale vehicle design problems are implemented by the developed solver. According to the test results, the multi-GPU based IC reanalysis method has potential capability for handling the real large scale problem and reducing the design cycle.
               
            

@&#INTRODUCTION@&#

One of the main obstacles in the structural optimization is high computational cost due to the repetitive computations during optimization procedure. Therefore, reanalysis methods have been motivated by the concept that the results of a modified structure can be approximated by an initial complete analysis instead of solving the complete set of modified equations. The most popular reanalysis method might be the combined approximations (CA) method proposed by Kirsch [1]. At the early stage, it was used only for linear reanalysis [2,3]. Sequentially, Leu and Huang [4] attained accurate results and significant reduction in computation effort when they applied CA method for complex nonlinear analysis problems. Furthermore, reanalysis for eigenvalue problems based on the CA method has also been developed in the early 2000s [5,6]. However, most of cases solved in these literatures are space truss structures or other simple problems, and the sizes of these cases are limited. In our previous study, the CA method is difficult to address large-scale problems due to its low efficiency of matrix decomposition operations, so a novel reanalysis method named the independent coefficients (IC) method was proposed by Huang [7] for handling large-scale real engineering problems. Since some operations such as stiffness matrix decomposition are not involved in this method, computational cost of IC is significantly improved. However, it should be pointed out that the computation demanding will rise rapidly if the scale of reduced equilibrium system increases. Therefore, the computational cost is still expensive especially for large-scale problems. The computational cost is mainly caused by the matrix vector multiplication, a dominative operation in the IC method. Other operations (such as vector dot product and scalar operation) are also sensitive to the efficiency of reanalysis. In order to solve large scale engineering optimization problems, the efficiency of IC method would desire to be improved further. Obviously, the most direct way of improving the efficiency is parallelism.

Parallel computing can accelerate science applications tremendously and has become a dynamic field with continuous innovation in computing hardware and programming model. Numerous parallel applications have been implemented on supercomputers [8], but these platforms are extremely expensive. One emerging technology for parallel computing is to port the parallel part of the calculations to a graphics processor unit (GPU) [9]. Compared with supercomputers including numerous CPUs, the use of GPU makes it is possible to get considerable parallel throughput on a much cheaper desktop computer rather than relying on large computer clusters. Currently multi-threaded GPUs can achieve several Teraflops of peak computing power that are orders of magnitude greater than that of modern CPUs by means of a combination of hundreds of parallel processors on graphics card, and 20 times or more higher memory bandwidth than CPU to memory interface in commodity computers. The many-core GPU with these hardware advantages enables programmers to achieve speedups of an order of magnitude over a standard CPU in various domains [10–12] and is growing in popularity [13,14]. Several GPU programming interfaces such as NVIDIA’s compute unified device architecture (CUDA) [15], and open computing language (OpenCL) [16] have been developed to fascinate the programmers while exposing enough of the hardware to allow the resources to be well utilized. As a high-level programming model, CUDA provides a simplified C-based programming language in which most of C language grammars are applicable, and it has been used in various scientific computing domain [17,18]. In order to improve the efficiency of reanalysis problems, Wang firstly presented a parallel CA method by using single GPU to improve the computational efficiency [19].

Although the use of single GPU system makes it possible to improve the performance of several reanalysis problems, many significantly large-scale engineering problems suggest a combination of the power of multiple GPUs to overcome memory limitations of single GPU and to achieve higher efficiency. As a standard library definition for message passing in parallel computer system, message passing interface (MPI) [20] is investigated and employed to establish communications between GPUs. Each CPU core runs a separate process and controls one of the available GPUs. The memory limitations of a single GPU can be overcome by suitable distributing the data among the GPUs, and it is flexible to scale the parallel computing model according to the number of nodes which is equipped with a single GPU or multiple GPUs. Since direct methods used in reanalysis method involves operations like matrix decomposition, and these are not suitable to be implemented on multiple GPUs for memory access pattern. In this study, the iterative method, preconditioned conjugate gradient (PCG) method is chose as the solver for linear system in IC algorithm. It should be noted that many matrix and vector multiplication operations are involved in the PCG solver, and these operations are perfectly suitable to run on multiple GPUs for their fine parallelism granularity.

In this study, a MPI-CUDA implementation of IC method is developed for multiple GPUs. The IC procedure is reconstructed to accommodate the use of multiple GPUs. To improve the scale of solution with IC reanalysis method, this paper efficiently utilizes the CSR format for stiffness matrix storage. The stiffness matrix and corresponding vectors are successfully partitioned on multiple GPUs to achieve good task balance as well as few data transfer, and the multiple GPUs cooperate with each other well to obtain the final optimization results. The proposed MPI-CUDA implementation also incorporates an efficient programming strategy to successively boost the parallel performance by using non-blocking communication MPI functions to overlap the CPU data transfer and GPU computation.

The rest of this paper is organized as follows. In Section 2, the IC method is briefly reviewed. In Section 3, the parallel IC method based on MPI and CUDA will be presented. Section 4 will give some numerical examples to demonstrate the effectiveness of the proposed method. Finally, some conclusions are given.

Given a positive-definite symmetric stiffness matrix 
                        
                           
                              K
                           
                           
                              0
                           
                        
                      and the load vector 
                        
                           
                              R
                           
                           
                              0
                           
                        
                      of an initial design of structure, the resulting displacements 
                        
                           
                              r
                           
                           
                              0
                           
                        
                      can be solved by the equilibrium equations.
                        
                           (1)
                           
                              
                                 
                                    K
                                 
                                 
                                    0
                                 
                              
                              
                                 
                                    r
                                 
                                 
                                    0
                                 
                              
                              =
                              
                                 
                                    R
                                 
                                 
                                    0
                                 
                              
                           
                        
                     A change in the design will lead to the modified stiffness matrix K and the modified load vector R can be given by
                        
                           (2)
                           
                              K
                              =
                              
                                 
                                    K
                                 
                                 
                                    0
                                 
                              
                              +
                              Δ
                              K
                           
                        
                     
                     
                        
                           (3)
                           
                              R
                              =
                              
                                 
                                    R
                                 
                                 
                                    0
                                 
                              
                              +
                              Δ
                              R
                           
                        
                     where 
                        
                           Δ
                        
                     
                     
                        K
                      and 
                        
                           Δ
                        
                     
                     
                        R
                      refer to the changes in the stiffness matrix and the load vector, respectively. The purpose of reanalysis method is to find efficient and high quality expressions of the displacements 
                        r
                      of modified structure without solving the complete set of modified analysis equations.
                        
                           (4)
                           
                              Kr
                              =
                              R
                           
                        
                     The IC method [7] replaces solving Eq. (4) directly with calculating the displacements of the DOFs influenced by the changes. The main strategy of IC method is briefly summarized as follows.

The modified displacements r is assumed by the following expression.
                        
                           (5)
                           
                              r
                              =
                              
                                 
                                    r
                                 
                                 
                                    0
                                 
                              
                              +
                              Δ
                              r
                           
                        
                     Then Eq. (4) can be written as
                        
                           (6)
                           
                              K
                              
                                 
                                    
                                       
                                          
                                             r
                                          
                                          
                                             0
                                          
                                       
                                       +
                                       Δ
                                       r
                                    
                                 
                              
                              =
                              R
                           
                        
                     Rewrite Eq. (6) in the following expression.
                        
                           (7)
                           
                              K
                              Δ
                              r
                              =
                              R
                              -
                              
                                 
                                    Kr
                                 
                                 
                                    0
                                 
                              
                           
                        
                     Define 
                        
                           δ
                        
                      be the residual value of the initial displacements 
                        
                           
                              
                                 r
                              
                              
                                 0
                              
                           
                        
                      as Eq. (8).
                        
                           (8)
                           
                              δ
                              =
                              R
                              -
                              
                                 
                                    Kr
                                 
                                 
                                    0
                                 
                              
                           
                        
                     Eq. (7) becomes
                        
                           (9)
                           
                              K
                              Δ
                              r
                              =
                              δ
                           
                        
                     Since the structural modification usually is local, only few elements of vector 
                        
                           δ
                        
                      are non-zero. In the coming step of IC method, all the DOFs related to the non-zero elements of 
                        
                           δ
                        
                      would be recorded by pre-selecting a small tolerance 
                        
                           ε
                        
                     . In practice, if
                        
                           (10)
                           
                              
                                 
                                    
                                       δ
                                       
                                          
                                             
                                                i
                                             
                                          
                                       
                                    
                                 
                              
                              >
                              ε
                              ,
                           
                        
                     then the i-th row of stiffness matrix K will be inspected to choose the DOFs influenced by structural modifications. If K (
                        
                           i
                           ,
                           j
                           )
                        
                      does not equal to zero, it means that the j-th DOF is influenced by the modification and will be recorded in a vector 
                        
                           
                              
                                 S
                              
                              
                                 d
                              
                           
                        
                     .

Assume that the first 3 DOFs are influenced by the modification, then
                        
                           (11)
                           
                              
                                 
                                    S
                                 
                                 
                                    d
                                 
                              
                              =
                              
                                 
                                    [
                                    1
                                    ,
                                    2
                                    ,
                                    3
                                    ]
                                 
                                 
                                    T
                                 
                              
                              ,
                              
                              
                                 
                                    n
                                 
                                 
                                    d
                                 
                              
                              =
                              3
                           
                        
                     A series of basis vectors 
                        
                           
                              r
                           
                           
                              i
                           
                        
                      are calculated and the linear combination of these basis vectors can approximately express 
                        Δ
                        r
                      as follow.
                        
                           (12)
                           
                              Δ
                              r
                              =
                              
                                 
                                    y
                                 
                                 
                                    1
                                 
                              
                              
                                 
                                    r
                                 
                                 
                                    1
                                 
                              
                              +
                              
                                 
                                    y
                                 
                                 
                                    2
                                 
                              
                              
                                 
                                    r
                                 
                                 
                                    2
                                 
                              
                              +
                              
                                 
                                    y
                                 
                                 
                                    3
                                 
                              
                              
                                 
                                    r
                                 
                                 
                                    3
                                 
                              
                              +
                              ⋯
                              +
                              
                                 
                                    y
                                 
                                 
                                    s
                                 
                              
                              
                                 
                                    r
                                 
                                 
                                    s
                                 
                              
                              =
                              
                                 
                                    r
                                 
                                 
                                    B
                                 
                              
                              y
                           
                        
                     In Eq. (11), 
                        
                           
                              
                                 r
                              
                              
                                 B
                              
                           
                        
                      refers to the series of basis vectors.
                        
                           (13)
                           
                              
                                 
                                    r
                                 
                                 
                                    B
                                 
                              
                              =
                              {
                              
                                 
                                    r
                                 
                                 
                                    1
                                 
                              
                              ,
                              
                                 
                                    r
                                 
                                 
                                    2
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    r
                                 
                                 
                                    s
                                 
                              
                              }
                           
                        
                     
                     s is the number of basis vectors and it equals to 
                        
                           
                              
                                 n
                              
                              
                                 d
                              
                           
                        
                      in IC method, and 
                        
                           
                              
                                 y
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 y
                              
                              
                                 2
                              
                           
                           ,
                           
                              
                                 y
                              
                              
                                 3
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 y
                              
                              
                                 s
                              
                           
                        
                      are the coefficients of corresponding basis vectors. The independent coefficients can be determined in the following steps.

For every basis vector 
                        
                           
                              r
                           
                           
                              i
                           
                        
                     , if the i-th selected DOF is j, then 
                        
                           
                              
                                 r
                              
                              
                                 i
                              
                           
                        
                      can be expressed as
                        
                           (14)
                           
                              
                                 
                                    r
                                 
                                 
                                    i
                                 
                              
                              =
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   0
                                                
                                                
                                                   ⋯
                                                
                                                
                                                   0
                                                
                                                
                                                   1
                                                
                                                
                                                   0
                                                
                                                
                                                   ⋯
                                                
                                                
                                                   0
                                                
                                             
                                          
                                       
                                    
                                 
                                 
                                    T
                                 
                              
                              
                              (
                              The
                              j
                              -
                              th element of
                              
                              
                                 
                                    r
                                 
                                 
                                    i
                                 
                              
                              
                              is
                              
                              1
                              )
                           
                        
                     Pre-multiply the modified stiffness matrix 
                        K
                      and the residual vector 
                        
                           δ
                        
                      in Eq. (9)
                     
                        
                           (15)
                           
                              
                                 
                                    r
                                 
                                 
                                    B
                                 
                                 
                                    T
                                 
                              
                              
                                 
                                    Kr
                                 
                                 
                                    B
                                 
                              
                              y
                              =
                              
                                 
                                    r
                                 
                                 
                                    B
                                 
                                 
                                    T
                                 
                              
                              δ
                           
                        
                     Assume that
                        
                           (16)
                           
                              
                                 
                                    K
                                 
                                 
                                    R
                                 
                              
                              =
                              
                                 
                                    r
                                 
                                 
                                    B
                                 
                                 
                                    T
                                 
                              
                              
                                 
                                    Kr
                                 
                                 
                                    B
                                 
                              
                              ,
                              
                              
                                 
                                    R
                                 
                                 
                                    R
                                 
                              
                              =
                              
                                 
                                    r
                                 
                                 
                                    B
                                 
                                 
                                    T
                                 
                              
                              δ
                           
                        
                     Then a following reduced set of 
                        
                           s
                           ×
                           s
                        
                      equations is obtained
                        
                           (17)
                           
                              
                                 
                                    K
                                 
                                 
                                    R
                                 
                              
                              y
                              =
                              
                                 
                                    R
                                 
                                 
                                    R
                                 
                              
                           
                        
                     After obtaining 
                        y
                      in Eq. (17), the modified displacements 
                        r
                     can be achieved by Eq. (5) when 
                        Δ
                        R
                      has been obtained by solving Eq. (12).

As a simple example, an original structure (Part 1) and the modified structure are shown in Fig. 1
                     . It can be seen that the local modification is an addition of a new crossbeam (Part 2).

Assume that Part 1 (the origin structure except communal nodes) corresponds to the DOFs 1 to n, and Part 2 (new crossbeam include communal nodes) corresponds to DOFs n
                     +1 to 
                        
                           n
                           +
                           m
                        
                     . Eq. (4) can be expressed as
                        
                           (18)
                           
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      K
                                                   
                                                   
                                                      11
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      K
                                                   
                                                   
                                                      12
                                                   
                                                
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      K
                                                   
                                                   
                                                      21
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      K
                                                   
                                                   
                                                      22
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      r
                                                   
                                                   
                                                      1
                                                   
                                                
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      r
                                                   
                                                   
                                                      2
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                              =
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      R
                                                   
                                                   
                                                      1
                                                   
                                                
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      R
                                                   
                                                   
                                                      2
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     where 
                        
                           
                              K
                           
                           
                              11
                           
                        
                      and 
                        
                           
                              K
                           
                           
                              22
                           
                        
                      denote the stiffness matrices of Part 1 and Part 2, respectively. Since the nodes within Part 2 have been added in the modified structure, the corresponding elements in stiffness matrix 
                        
                           
                              K
                           
                           
                              22
                           
                        
                      would also be updated, and the DOFs of Part 2 need to be reanalyzed. Therefore, the basis vectors can be written as
                        
                           (19)
                           
                              
                                 
                                    r
                                 
                                 
                                    B
                                 
                              
                              =
                              
                                 
                                    
                                       
                                          
                                             
                                                0
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      E
                                                   
                                                   
                                                      m
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     where 
                        
                           
                              
                                 E
                              
                              
                                 m
                              
                           
                        
                      is a unit matrix of rank m. The reduced equation then can be obtained as
                        
                           (20)
                           
                              
                                 
                                    K
                                 
                                 
                                    22
                                 
                              
                              y
                              =
                              
                                 
                                    R
                                 
                                 
                                    R
                                 
                              
                           
                        
                     where
                        
                           (21)
                           
                              
                                 
                                    R
                                 
                                 
                                    R
                                 
                              
                              =
                              
                                 
                                    r
                                 
                                 
                                    B
                                 
                                 
                                    T
                                 
                              
                              
                                 
                                    
                                       R
                                       -
                                       
                                          
                                             Kr
                                          
                                          
                                             0
                                          
                                       
                                    
                                 
                              
                           
                        
                     
                  

CUDA [15] is NVIDIA’s parallel programming model and software environment designed to program on GPUs for general computation. It enables programmers to achieve dramatic increases in computing performance when they define similar C functions named kernels. When a kernel program is called by the CPU host program, it will be executed N times in parallel by N different CUDA threads. These threads sharing global memory as a grid are executed by an array of SMs (streaming multiprocessors) on NVIDIA’s GPU devices. A grid is organized by a set of one, two or three-dimensional blocks, and one built-in variable blockIdx can be used to identify blocks. The block is also arranged by one, two or three-dimensional threads sharing local memory. The threads of a given block can be identified by another built-in variable threadIdx and may synchronize via barriers. Threads of a block are executed concurrently on one multiprocessor, which consists of several SP (scalar processor) cores, a multithreaded instruction unit and on-chip memory. SP employs SIMT (single-instruction, multiple-thread) architecture, executing threads in groups of warps, each of which has 32 parallel threads currently. The transfer of data between CPU host memory and GPU device memory can be completed by CUDA runtime API and driver API using PCI-Express. The input data is firstly transferred from the host memory and then processed on the device memory. After attaining output data by the CUDA kernel execution, the output data needs to be sent back to the host memory. Because the PCI-Express employed in this work only has a 4GB/s bi-directional bandwidth, the data transfer is time-consuming. Therefore, it should be avoided as much as possible to achieve high efficiency. For instance, the test shows that it costs 5.8 ms to transfer 20Mb data from host memory to device memory.

The MPI (Message Passing Interface) API [20] is widely used for parallel programming on both shared and distributed memory machines. It offers a highly portable solution for programs to work on a wide variety of machines and hardware topologies. One CPU process mapped to each GPU is the most straightforward way to use a multiple GPU system. Combined with CUDA, each GPU can receive data from the CPU host, perform computation, communicate and synchronize with neighbor GPUs, copy data to the host for the next computation step. In order to reduce the overhead of communication, we can utilize MPI’s non-blocking communication capabilities which allow computations and communications to overlap. Therefore, all CPUs and GPUs constantly calculate until the operations are completed, which leads to improved performance. Fig. 2
                         indicates the basic scheme how multiple GPUs are employed by using MPI and CUDA [21,22].

According to Fig. 2, an efficient parallel reanalysis method on multiple GPUs using MPI and CUDA is developed. Considering the parallelism of the IC method, the flowchart of the parallel IC method based on multiple GPUs is presented in Fig. 3
                        
                        . According to Fig. 3, the parallelization of the IC method is composed of three stages. The procedures of IC method presented in right box will be implemented on multiple GPUs.

In the developed system, the initial equilibrium equations 
                           
                              
                                 
                                    K
                                 
                                 
                                    0
                                 
                              
                              
                                 
                                    r
                                 
                                 
                                    0
                                 
                              
                              =
                              
                                 
                                    R
                                 
                                 
                                    0
                                 
                              
                           
                         and reduced equations 
                           
                              
                                 
                                    K
                                 
                                 
                                    R
                                 
                              
                              y
                              =
                              
                                 
                                    R
                                 
                                 
                                    R
                                 
                              
                           
                         are both solved by the preconditioned conjugate gradient (PCG) method. In order to describe the PCG solver in a common way, these two linear systems can be expressed in the general format like 
                           Ax
                        
                        =
                        
                           b.
                         And the PCG method is described as follows.

A suitable preconditioner 
                           M
                         is an important issue to determine the convergence rate of PCG solver. In recent years, various preconditioning techniques can be found to accelerate the solution of linear systems arising from FEM, for instance, the incomplete Cholesky factorization [23], sparse approximate inverse [24], algebraic multigrid [25] and etc. However, these complex techniques will take high computational cost for preconditioning and are not easy to be implemented on the reanalysis platform based on multiple GPUs. In this study the Jacobi preconditioner is employed for its robustness and simplicity. That means the matrix 
                           M
                         should be degenerated to a vector containing all the diagonal elements in matrix 
                           A
                        .
                           
                              (22)
                              
                                 M
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         A
                                                      
                                                      
                                                         11
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         A
                                                      
                                                      
                                                         22
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         A
                                                      
                                                      
                                                         33
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         A
                                                      
                                                      
                                                         44
                                                      
                                                   
                                                
                                                
                                                   ⋯
                                                
                                                
                                                   
                                                      
                                                         A
                                                      
                                                      
                                                         nn
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        Then the complex solution of 
                           
                              
                                 
                                    Mz
                                 
                                 
                                    k
                                 
                              
                              =
                              
                                 
                                    r
                                 
                                 
                                    k
                                 
                              
                           
                         in step (3) will be degenerated to a vector operation.

As shown in Fig. 3, the initial equilibrium equations are solved by PCG method in the first stage. Since enormous computational effort is needed for solving the linear system, the PCG solver on multiple GPUs is employed. It should be pointed out that the PCG solver is implemented in double precision to meet the simulation demands as well as serial version. In the single GPU platform, the data needed by GPU is just copied from CPU to GPU by PCI-Express easily. Compared with single GPU platform, the storage strategy of the data that needed for multiple GPUs becomes more complex and should be considered carefully, so the data such as matrices and vectors must be partitioned and prepared carefully in advance.

Since many of elements in stiffness matrix are zero and it would cost large CPU RAM and GPU memory, the compressed sparse row (CSR) format [26] is used for matrix storage on both CPUs and GPUs, which provides a good solution in both parallelism granularity and memory access patterns.

For a given stiffness matrix 
                           K
                        , CSR format stores all nonzero elements of the stiffness matrix in row-major order in an array 
                           val
                        . Two other arrays are used for indexing nonzero entries. The elements in the row pointer array 
                           row
                        _
                           ptr
                         point the first nonzero entry in each row. There are rows+1 elements in this array where the last element indicates boundary of the last row. The other array 
                           col
                        _
                           ind
                         contains the column indices of the nonzero entries in each row. Compared with CPU RAM, large-scale reanalysis problems are difficult to be carried out due to limited size of device memory of GPU. For example, only 1GB device memory of NVIDIA GTX 460 can be used. In order to overcome this bottleneck for GPUs, stiffness matrix and corresponding vector entries are required to be distributed among multiple GPUs. Commonly, the stiffness matrix is divided into several sub-matrices and each sub-matrix is assigned to a CPU process, which uses a GPU to perform the computation related to its sub-matrix. A typical example is depicted in Fig. 5
                         for a CSR-stored matrix 
                           K
                         with four rows and the distribution on four GPUs. It can be seen that each sub-matrix is also stored in CSR format on its corresponding GPU. 
                           
                              
                                 
                                    K
                                 
                                 
                                    1
                                 
                              
                           
                         and 
                           
                              
                                 
                                    K
                                 
                                 
                                    2
                                 
                              
                           
                        are the sub-matrices in corresponding GPUs, and the superscripts 1 and 2 denote the IDs of GPUs. In some cases, the number of rows of sub-matrix for each GPU may be slightly different if the number of rows of total matrix cannot be divided by the number of GPUs. This will have very little impact on the task balance between multiple GPUs, and so the performance seems not be influenced. Theoretically, multiple GPUs can handle much larger scale reanalysis problems than single GPU for the combination of GPU memory, and it is about the same times as memory of multiple GPUs compared to single GPU memory.

Similar to matrix partition, some corresponding vectors are also needed to be partitioned between multiple GPUs as shown in Fig. 6
                        . For instance, vector 
                           b
                         is partitioned into four pieces in the same length, and each piece will be stored in one GPU. 
                           
                              
                                 
                                    b
                                 
                                 
                                    1
                                 
                              
                           
                         and 
                           
                              
                                 
                                    b
                                 
                                 
                                    2
                                 
                              
                           
                         are the sub-vectors in corresponding GPUs, and the superscripts 1 and 2 denote the IDs of GPUs. Actually, the lengths of sub-vectors can be slightly different when the length of vector b cannot be divided by the number of GPUs.

The data partition and preparation within PCG method can be described as follows. Since the preparing data for each GPU is independent from other GPUs, these operations are easily parallelized by MPI. The data transfer in this stage needs to be done just for once.
                           
                              
                                 
                                    
                                       
                                          (
                                          1
                                          )
                                          
                                          prepare
                                          
                                          data
                                          
                                          on
                                          
                                          each
                                          
                                          CPU
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            partion matrix
                                                            
                                                            A
                                                            →
                                                            
                                                               
                                                                  A
                                                               
                                                               
                                                                  1
                                                               
                                                            
                                                            ,
                                                            …
                                                            ,
                                                            
                                                            
                                                               
                                                                  A
                                                               
                                                               
                                                                  n
                                                               
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            partion vectors
                                                            :
                                                            x
                                                            →
                                                            
                                                               
                                                                  x
                                                               
                                                               
                                                                  1
                                                               
                                                            
                                                            ,
                                                            …
                                                            ,
                                                            
                                                            
                                                               
                                                                  x
                                                               
                                                               
                                                                  n
                                                               
                                                            
                                                            ;
                                                            r
                                                            →
                                                            
                                                               
                                                                  r
                                                               
                                                               
                                                                  1
                                                               
                                                            
                                                            ,
                                                            …
                                                            ,
                                                            
                                                            
                                                               
                                                                  r
                                                               
                                                               
                                                                  n
                                                               
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            precondtioner
                                                            :
                                                            
                                                            diag
                                                            (
                                                            
                                                               
                                                                  A
                                                               
                                                               
                                                                  1
                                                               
                                                            
                                                            )
                                                            →
                                                            
                                                               
                                                                  M
                                                               
                                                               
                                                                  1
                                                               
                                                            
                                                            ,
                                                            …
                                                            ,
                                                            diag
                                                            (
                                                            
                                                               
                                                                  A
                                                               
                                                               
                                                                  n
                                                               
                                                            
                                                            )
                                                            →
                                                            
                                                               
                                                                  M
                                                               
                                                               
                                                                  n
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                          paralellized by MPI
                                       
                                    
                                    
                                       
                                          (
                                          2
                                          )
                                          
                                          copy
                                          
                                          from
                                          
                                          CPU
                                          
                                          to
                                          
                                          corresponding
                                          
                                          GPU
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  A
                                                               
                                                               
                                                                  1
                                                               
                                                            
                                                            →
                                                            d
                                                            
                                                               
                                                                  A
                                                               
                                                               
                                                                  1
                                                               
                                                            
                                                            ,
                                                            
                                                               
                                                                  A
                                                               
                                                               
                                                                  2
                                                               
                                                            
                                                            →
                                                            d
                                                            
                                                               
                                                                  A
                                                               
                                                               
                                                                  2
                                                               
                                                            
                                                            ,
                                                            …
                                                            ,
                                                            
                                                               
                                                                  A
                                                               
                                                               
                                                                  n
                                                               
                                                            
                                                            →
                                                            d
                                                            
                                                               
                                                                  A
                                                               
                                                               
                                                                  n
                                                               
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               
                                                                  x
                                                               
                                                               
                                                                  1
                                                               
                                                            
                                                            →
                                                            d
                                                            
                                                               
                                                                  x
                                                               
                                                               
                                                                  1
                                                               
                                                            
                                                            ,
                                                            
                                                               
                                                                  x
                                                               
                                                               
                                                                  2
                                                               
                                                            
                                                            →
                                                            d
                                                            
                                                               
                                                                  x
                                                               
                                                               
                                                                  2
                                                               
                                                            
                                                            ,
                                                            …
                                                            ,
                                                            
                                                               
                                                                  x
                                                               
                                                               
                                                                  n
                                                               
                                                            
                                                            →
                                                            d
                                                            
                                                               
                                                                  x
                                                               
                                                               
                                                                  n
                                                               
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               
                                                                  r
                                                               
                                                               
                                                                  1
                                                               
                                                            
                                                            →
                                                            d
                                                            
                                                               
                                                                  r
                                                               
                                                               
                                                                  1
                                                               
                                                            
                                                            ,
                                                            
                                                               
                                                                  r
                                                               
                                                               
                                                                  2
                                                               
                                                            
                                                            →
                                                            d
                                                            
                                                               
                                                                  r
                                                               
                                                               
                                                                  2
                                                               
                                                            
                                                            ,
                                                            …
                                                            ,
                                                            
                                                               
                                                                  r
                                                               
                                                               
                                                                  n
                                                               
                                                            
                                                            →
                                                            d
                                                            
                                                               
                                                                  r
                                                               
                                                               
                                                                  n
                                                               
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               
                                                                  M
                                                               
                                                               
                                                                  1
                                                               
                                                            
                                                            →
                                                            d
                                                            
                                                               
                                                                  M
                                                               
                                                               
                                                                  1
                                                               
                                                            
                                                            ,
                                                            
                                                            
                                                               
                                                                  M
                                                               
                                                               
                                                                  2
                                                               
                                                            
                                                            →
                                                            d
                                                            
                                                               
                                                                  M
                                                               
                                                               
                                                                  2
                                                               
                                                            
                                                            ,
                                                            …
                                                            
                                                               
                                                                  M
                                                               
                                                               
                                                                  n
                                                               
                                                            
                                                            →
                                                            d
                                                            
                                                               
                                                                  M
                                                               
                                                               
                                                                  n
                                                               
                                                            
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                          paralellized by MPI
                                       
                                    
                                 
                              
                           
                        The superscripts 1, 2 and n presents the IDs for multiple GPUs, so 
                           
                              
                                 
                                    A
                                 
                                 
                                    1
                                 
                              
                           
                         denotes that the sub-matrix is stored in CSR format on CPU 1, while d
                           
                              
                                 
                                    A
                                 
                                 
                                    1
                                 
                              
                           
                         is corresponding sub-matrix for GPU 1 where character d means the matrix is stored on the GPU device memory. Similarly, sub-vector 
                           
                              
                                 
                                    x
                                 
                                 
                                    1
                                 
                              
                           
                         stored in CPU 1 will be copied to GPU 1 as vector 
                           
                              d
                              
                                 
                                    x
                                 
                                 
                                    1
                                 
                              
                           
                        . When the partition is completed, the parallel PCG method implemented on multiple GPUs should be performed according to Fig. 7
                        .

It can be seen that the parallel PCG solver consists of various sparse matrix vector multiplication (SPMV) and dot product operations. Compared with dual domain decomposition method such as FETI [27] (Finite Element Tearing and Interconnect) for solving systems of equations, Lagrange multipliers are avoided. Therefore, the proposed partitioning method is more convenient to be implemented on multiple GPUs. Only a few parameters like 
                           
                              ∥
                              
                                 
                                    dr
                                 
                                 
                                    n
                                 
                              
                              ∥
                           
                         and sub-vectors like 
                           
                              d
                              
                                 
                                    p
                                 
                                 
                                    k
                                 
                                 
                                    n
                                 
                              
                           
                         need to be communicated between multiple GPUs by MPI, communication time is also saved. The intermediate results in this part are required to be transferred for each PCG iteration step. Moreover, this parallel PCG method has successfully utilizes the non-blocking communication technology to achieve high performance by overlapping the communication and computation. For instance, when performing multiple SPMV operations during the structure reanalysis process, we have to combine the result from each GPU before moving into the next step. In order to reduce the overhead of communication, we utilize MPI’s non-blocking communication capabilities which allow computations and communication to overlap, so that GPUs can continue their following computation tasks instead of idling, at the same time the CPUs are communicating data with each other.

In the second stage, in order to select the DOFs which should be reanalyzed, the residual value of 
                           
                              
                                 
                                    r
                                 
                                 
                                    0
                                 
                              
                           
                         is calculated by the parallel SPMV operation. The sub-matrices and vectors are transferred to each GPU only once. Sequentially, if the residual value of 
                           
                              
                                 
                                    r
                                 
                                 
                                    0
                                 
                              
                           
                         is greater than a small tolerance 
                           
                              ε
                           
                        , then the i-th row of stiffness matrix 
                           K
                         will be inspected to choose the DOFs influenced by structural modifications.

In the third stage, the parallel SPMV on multiple GPUs are employed to generate the reduced equilibrium equations, and the parallel PCG method is reused for solving the equilibrium system. The data transfer needs to be carried out once for generating reduced equilibrium equations, while has to be done within each PCG iteration step to solve the reduced equilibrium system.

Generally, the parallel IC method consists of various dot product and sparse matrix vector multiplication (SPMV) operations, which are the most important part to implement the MPI-CUDA IC method.

As shown in Fig. 2, a large number of dot products are performed and these operations can consume a non-negligible amount of computational time. Dot product can be divided into two discrete tasks. The first task multiplies the elements of each vector one by one while the second task computes the sum of each of these products to achieve the final result. GPU threads are naturally suitable for the multiplication step due to the inherently parallelism. In this work, the contents of the first vector is overwritten by the product of the elements of each vector by a GPU kernel as 
                           
                              a
                              [
                              i
                              ]
                              =
                              a
                              [
                              i
                              ]
                              ×
                              b
                              [
                              i
                              ]
                           
                        . Then, the final sum for each GPU will be obtained by reduction. And the final sum on each GPU will be accumulated by MPI ultimately. Fig. 8
                         illustrates the basic scheme of dot product using multiple GPUs.

In the context of sequential processor, the reduction computation has to be implemented by a single accumulator variable to sum all elements sequentially in a simple loop. However, a single accumulator variable on GPU will result in very poor performance because it should create a global serialization point. So, a parallel reduction strategy on multiple GPUs is implemented, where each GPU thread sums a fixed number of entries of the input vector, and then the partial sums would be gathered by parallel summing pairs of these partial sums. The final sum on each GPU will be produced after log2 N steps because each step divides the number of partial sums by half. And the final sum for multiple GPUs is obtained by accumulating the final sum on each GPU [22] as shown in Fig. 9
                        .

In the respect of each GPU, each thread is assigned to load one element of the input vector within the dot product operation. Furthermore, at the end of the reduction for each GPU, the first thread of the block (thread 0) accumulates all elements loaded by the threads within this block. This can be parallelized by utilizing the summation tree over the input elements. Fig. 10
                         shows a simple case of a block of eight threads sum values in a tree-like pattern and each step of loop denotes each successive level of the diagram [22]. After the partial sums of all the blocks in the grid has been computed, they have to be added to obtain the corresponding dot product on each GPU. This can be achieved by writing the partial sum of each block into a second array and then launch the parallel reduction kernel repeatedly until the reduction is completed.

As shown in Section 2, the IC method consists of several kernel operations: sparse matrix–vector multiplication (SPMV), vector dot product and scalar operation. Since SPMV dominates the performing time, fast implementation of is required for faster IC method using multiple GPUs.

Considering the implementation of SPMV defined as 
                           
                              d
                              =
                              
                                 
                                    Kr
                                 
                                 
                                    0
                                 
                              
                           
                         on multiple GPUs in Eq. (8), we use MPICH2-1.4.1p1 to implement the communication between multiple GPUs. To balance the workload between multiple GPUs, assuming the number of GPUs is 
                           
                              
                                 
                                    n
                                 
                                 
                                    p
                                 
                              
                           
                        , the matrix rows are partitioned into 
                           
                              
                                 
                                    n
                                 
                                 
                                    p
                                 
                              
                           
                         smaller matrices so that each matrix has a similar number of non-zero entries. Each smaller matrix is distributed to one GPU and is multiplied with the complete input vector 
                           
                              
                                 
                                    r
                                 
                                 
                                    0
                                 
                              
                           
                        . Then part of the result vector comes from each GPU. We distribute the number of rows equally to 
                           
                              
                                 
                                    n
                                 
                                 
                                    p
                                 
                              
                           
                         parts and form a sub-matrix in CSR format within each GPU. In order to perform multiple SPMV operations during the structure optimization process, we have to combine the result from each GPU before moving into the next step. In order to reduce the overhead of communication, we utilize MPI’s non-blocking communication capabilities which allow computations and communications to overlap, which leads to improved performance. Fig. 11
                         illustrates the SPMV parallelization for multiple GPUs.

Host CPU holds a global array for each GPU reads from and writes to for communicating vector entries. The stiffness matrix and corresponding vector entries are partitioned and stored in CSR format among multiple GPUs as shown in Fig. 5, so that loads of SPMV is balanced among GPUs. After each GPU completes its computation the vector 
                           r
                         must be communicated with neighboring GPUs.

Since the dot product between each row of the sparse stiffness matrix and the given vector can be computed independently of all other rows, one parallelization way is to use one thread for each matrix row. However, if the column indices and non-zeros for a given row are stored contiguously, each GPU thread cannot access them simultaneously but sequentially, and its performance will suffers mainly by this data accessing way especially when the number of non-zeros of this row is big (about 40 per row in this work). Moreover, the number of non-zeros per row may vary highly, so that many threads within a warp will remain idle when the thread is still busy with the longest row operation, thus leading to poor GPU utilization.

In order to overcome this weakness, an alternative implementation is applied where one thread warp (containing 32 threads) is assigned to each matrix row. Due to the use of a warp-wide parallel reduction to sum the per-thread results together, the implementation requires coordination among threads within the same warp. Furthermore, the performance is greatly improved for the contiguous data access, therefore overcoming the principal weakness of previous approaches using one thread. Moreover, the use of shared memory is also helpful to improve the performance for its low latency than global memory on GPU. The SPMV body of the overlap strategy used by the parallel reanalysis method based on multiple GPUs is shown in appendix.

In this section, three engineering problems are performed to validate the MPI-CUDA implementation of our parallel reanalysis method based on multiple GPUs described in Section 3. For the purpose of performing reanalysis automatically during the structural optimization when the CAD model has been modified, the CAD software PUM [28–30] is integrated with the developed parallel reanalysis platform. In the first stage of CAD-CAE integration, the PUM provides initial finite element mesh for reanalysis platform to compute the deformation. Then the mesh will be updated by the PUM and the information of added/deleted vertexes and segments will be offered for reanalysis platform until the optimization terminate.

The final results are compared to the solution of CUDA implementation of reanalysis method based on single GPU platform. The simulations are running on a computer with 8GB of memory and Intel Core i7 930 CPU, and two NVIDIA GTX 460 GPUs with NVIDIA driver version 325.23 and CUDA version 4.0. The GTX 460 is comprised of 336 CUDA cores and 1GB of device memory.

A door inner constrained by points A and B is considered as shown in Fig. 12
                        . The point C on door inner is subjected to a single loading condition of concentrated load 800N along vertical direction. To install accessory of a door, a hole is desired to be cut as shown in Fig. 13
                        . The modulus of elasticity is 200000MPa, Poisson rate is 0.3 and mass density is 7980kg/m3 for all these three problems.

In order to compare the performance of the multiple GPU reanalysis and single GPU reanalysis, three scale finite element meshes for the same CAD model are used for presenting the speedup characteristic of the MPI-CUDA parallel IC implementation as listed in Table 1
                        . The accurate criterion is presented as
                           
                              (23)
                              
                                 err
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               r
                                                            
                                                            
                                                               ¯
                                                            
                                                         
                                                         -
                                                         r
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         r
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    r
                                 
                                 
                                    ¯
                                 
                              
                           
                         and r present the displacements calculated by parallel reanalysis implementation and complete analysis, respectively.

Compared with the reanalysis implementation on a single GPU, the computational time of the multiple GPUs version is obviously reduced as shown in Table 1. The reanalysis time based on single GPU or dual-GPU platform is shown in Fig. 14
                        . With the increasing of DOFs, the decrease of computational time is more significant whether employing blocking or non-blocking communication strategy. For instance, the speedup is 1.35 for blocking communication and 1.61 for non-blocking communication in case 3 according to Fig. 15
                        , respectively. Due to the overhead of communication, the speedup of MPI-CUDA reanalysis cannot achieve to be the number of GPUs. It can also be seen that the non-blocking communication strategy performs better in all three cases by overlapping the computation and communication. Furthermore, it can be observed that both single GPU and multiple GPUs reanalysis procedures provide good approximations and converge to the accurate solution as shown in Table 1. For the competence of this work, the serial CPU version is also provided in Table 1, and the computational cost is much larger than parallel version and this can successfully demonstrate the advantage of GPU. The relative errors of displacements for three nodes selected randomly are presented in Table 2
                        . It indicates that MPI-CUDA reanalysis version can achieve the same precision prediction as single GPU implementation. The final design of vehicle door is shown in Fig. 16
                         by using MPI-CUDA reanalysis method, and Fig. 17
                         illustrates the modified vehicle door performed by complete analysis.

A body-in-white (BIW) side panel constrained by points A, B and C is considered as shown in Fig. 18
                        . It should be noted that only the three translational DOFs of points A, B and C are restrained to evaluate the torsional stiffness. The point D on side panel is subjected to a single loading condition of concentrated load 400N along Y direction. To enhance the torsional stiffness, a rib is desired to be added to the structure as shown in Fig. 19
                        . In order to compare the performance of the multiple GPU reanalysis and single GPU reanalysis, three scale FE meshes for the same CAD model are used for presenting the speedup characteristic of the MPI-CUDA parallel IC implementation as listed in Table 3
                        .

Compared with the reanalysis implementation on a single GPU, the computational time of the multiple GPUs version is obviously reduced as shown in Table 3. The reanalysis time based on single GPU and dual-GPU platform is shown in Fig. 20
                        . With the increasing of DOFs, the speedup of parallel computation with non-blocking communication is significantly improved. For instance, according to Fig. 21
                         the speedup is 1.36 for blocking communication and 1.62 for non-blocking communication in case 3, respectively. Additionally, the serial CPU version is also provided in Table 3, and the computational cost is much expensive than parallel version. The relative errors of displacements for three randomly selected nodes are presented in Table 4
                        . It indicates that MPI-CUDA reanalysis version can achieve the same precision prediction as single GPU. The final design of vehicle side panel is shown in Fig. 22
                         by using MPI-CUDA reanalysis method, and Fig. 23
                         illustrates the modified vehicle side panel performed by complete analysis.

During the process of vehicle design, the cross section of some existed parts such as window frame and B-pillar is often required to be modified to satisfy safety criteria. In this section, the cross section of the car door sill is modified to meet performance under distortion condition. The initial model of body in white (BIW) and the shape of cross section are shown by the PUM in Fig. 24
                        . The BIW is constrained by nodes (A, B, C) marked in green dots as illustrated in Fig. 25
                        , while subjected to a concentrated load 400N on the node (D) marked in red along vertical direction.

As shown in Fig. 24(b, c), the cross section should be modified when dragging the control point of section line defined in the PUM. The generation of FE model for BIW is automatically after changing the cross section of door sill by the PUM illustrated in Fig. 24(c). In order to compare the performance of the multiple GPU reanalysis and single GPU reanalysis, three scale FE meshes for the same CAD model are used for presenting the speedup characteristic of the MPI-CUDA parallel IC implementation as listed in Table 5
                        .

Compared with the reanalysis implementation on a single GPU, the computational time of the multiple GPUs version is obviously reduced as listed in Table 5. The comparison of reanalysis costs based on single GPU and dual-GPU platforms is presented in Fig. 26
                        . Similar to the previous problems, the performance of non-blocking communication assisted parallel reanalysis is much better. For instance, the speedup is 1.29 for blocking communication and 1.53 for non-blocking communication in case 2 according to Fig. 27
                        , respectively. The results by the serial CPU reanalysis method are also listed in Tables 5 and 6
                        . The relative errors of displacements of three randomly selected nodes are presented in Table 6. It indicates that the accuracies of single GPU, multi-GPU parallel methods are same to the serial version. Especially, for the case 3 with 2345070 DOFs, it can’t be run by the single GPU due to the limited GPU memory. Finally, the simulation results by the MPI-CUDA reanalysis and complete analysis are illustrated in Figs. 28 and 29
                        
                        , respectively.

In order to improve the efficiency of IC and overcome the memory bottleneck of GPU for solving large-scale engineering problems, a parallel IC method based on multiple GPUs is developed in this study. To verify the performance of developed method, three large-scale engineering problems are tested. The major contributions of this study can be summarized as follows:
                        
                           (1) The IC method is reconstructed for multi-GPU platform by using MPI and CUDA.

(2)The data partition for multiple GPUs is successfully implemented to achieve good load balance as well as few data transformation.

(3)The suggested non-blocking communication strategy can achieve higher speedups compared with blocking communication strategy.

(4)Compared with the single GPU-based IC method, the efficiency of the multiple GPU-based IC method is improved and the bottleneck of GPU memory size can be solved.

@&#ACKNOWLEDGMENTS@&#

This work has been supported by Project of the Key Program of National Natural Science Foundation of China under the Grad Numbers 11172097, 11302266 and 61232014; Program for New Century Excellent Talents in University under the grant number NCET-11-0131.


                     
                        
                     
                  

@&#REFERENCES@&#

