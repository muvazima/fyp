@&#MAIN-TITLE@&#Multiple topic identification in human/human conversations

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           A multiple classification methods for multiple theme hypothesization is proposed.


                        
                        
                           
                           Four methods, one of which is new, are initially used and separately evaluated.


                        
                        
                           
                           A new sequential decision strategy for multiple theme hypothesization is introduced.


                        
                        
                           
                           A new hypothesis refinancing component is presented, based on ASR word lattice.


                        
                        
                           
                           Results show that the strategy makes it possible to obtain reliable service surveys.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Human/human conversation analysis

Multi-topic identification

Spoken language understanding

Interpretation strategies

@&#ABSTRACT@&#


               
               
                  The paper deals with the automatic analysis of real-life telephone conversations between an agent and a customer of a customer care service (ccs). The application domain is the public transportation system in Paris and the purpose is to collect statistics about customer problems in order to monitor the service and decide priorities on the intervention for improving user satisfaction.
                  Of primary importance for the analysis is the detection of themes that are the object of customer problems. Themes are defined in the application requirements and are part of the application ontology that is implicit in the ccs documentation.
                  Due to variety of customer population, the structure of conversations with an agent is unpredictable. A conversation may be about one or more themes. Theme mentions can be interleaved with mentions of facts that are irrelevant for the application purpose. Furthermore, in certain conversations theme mentions are localized in specific conversation segments while in other conversations mentions cannot be localized. As a consequence, approaches to feature extraction with and without mention localization are considered.
                  Application domain relevant themes identified by an automatic procedure are expressed by specific sentences whose words are hypothesized by an automatic speech recognition (asr) system. The asr system is error prone. The word error rates can be very high for many reasons. Among them it is worth mentioning unpredictable background noise, speaker accent, and various types of speech disfluencies.
                  As the application task requires the composition of proportions of theme mentions, a sequential decision strategy is introduced in this paper for performing a survey of the large amount of conversations made available in a given time period. The strategy has to sample the conversations to form a survey containing enough data analyzed with high accuracy so that proportions can be estimated with sufficient accuracy.
                  Due to the unpredictable type of theme mentions, it is appropriate to consider methods for theme hypothesization based on global as well as local feature extraction. Two systems based on each type of feature extraction will be considered by the strategy. One of the four methods is novel. It is based on a new definition of density of theme mentions and on the localization of high density zones whose boundaries do not need to be precisely detected.
                  The sequential decision strategy starts by grouping theme hypotheses into sets of different expected accuracy and coverage levels. For those sets for which accuracy can be improved with a consequent increase of coverage a new system with new features is introduced. Its execution is triggered only when specific preconditions are met on the hypotheses generated by the basic four systems.
                  Experimental results are provided on a corpus collected in the call center of the Paris transportation system known as ratp. The results show that surveys with high accuracy and coverage can be composed with the proposed strategy and systems. This makes it possible to apply a previously published proportion estimation approach that takes into account hypothesization errors.
               
            

@&#INTRODUCTION@&#

In recent years, there has been an increasing research interest in the analysis of human/human spoken conversations. Advances in this research area are described in Tur and De Mori (2011) and Tur and Hakkani-Tür (2011). A scientifically interesting and practically important component of this research is topic identification for which an ample review of the state of the art can be found in Hazen (2011). In spite of the relevant progress achieved so far, it is difficult to reliably identify multiple topics in real-life telephone conversations between casual speakers in unpredictable acoustic environments. Of particular interest are call-center conversations in which customers discuss problems in specific domains with an advisor. This is the case of the application considered in this paper. The purpose of the application is to monitor the effectiveness of a customer care service (ccs) of the ratp Paris transportation system by analyzing real-world human/human telephone conversations in which an agent solicits a customer to formulate a problem and attempts to solve it. The application task is to perform surveys of customer problems in different time periods. Proportions of problem themes computed with the survey data are used for monitoring user satisfaction and establishing priorities of problem solving interventions. Of primary importance for this task is the ability to automatically select, in a given period of time, a sufficiently large sample of conversations automatically analyzed with high accuracy. Application relevant information for evaluating proportions of problem items and solutions is described in the application requirements making evident, not formally defined, but useful, speech analytics. Themes mentioned in the application documentation appear to be the most important and general classes representing concerns that the survey has to address. Themes appear to be an adequate semantic representation of the types of problems in the application ontology that is inferred from the documentation.

Agents follow a pre-defined protocol to propose solutions to user problems about the transportation system and its services. In order to evaluate proportions of problem types, agents compile conversation reports following a domain ontology. Due to time constraints, reports compiled by the agents cover a small proportion of conversations. Furthermore, compiled reports are often incomplete and error-prone. Thus, an automatic classification of conversation themes would be useful for producing a quantity of accurate reports sufficient for performing a reliable survey.

The achievement of acceptable automatic solutions has to overcome problems and limits of known spoken language processing and interpretation systems. A fully automatic system must include an automatic speech recognition (asr) module for obtaining automatic transcriptions of the conversations. The acoustic environment on these conversations is unpredictable with a large variety of noise types and intensity. Customers may not be native French speakers and conversations may exhibit frequent disfluencies. The agent may call another service for gathering information. This may cause the introduction of different types of non-speech sounds that have to be identified and discarded. For all these reasons, the word error rate (wer) of the asr system is highly variable and can be very high. Conversations to be analyzed may be about one or more domain themes. Mentions of relevant themes may be interleaved with mentions of irrelevant comments. Mentions of multiple themes may appear in well separable conversation segments or they may be diffused in zones of a conversation that are difficult to localize. Moreover, mentions can be incomplete or highly imprecise involving repetitions, ambiguities, linguistic and pronunciation errors. Some mentions of an application relevant theme may become irrelevant in certain contexts. For example, a customer may inquiry about an object lost on a transportation mean that was late. In such a case, the loss should be considered as a more relevant theme than the traffic state delay.

In order to properly approach the problems in the above mentioned variety of scenarios and difficulties, methods relying on evaluating the evidence of each feature in the whole conversation should be combined with methods that evaluate feature evidence on specific, automatically detected, conversation segments. In fact, depending on the customer speaking style, mentions of the same pair of themes can be diffused in large portions of a conversation with not clearly defined location for each theme, or they may appear in other conversations in well-localized segments. Thus, it appears useful to consider algorithmic approaches for diffuse theme mentions and other approaches for mentions in specific, localized segments. Furthermore, in order to reduce the effect of asr and classification errors, it is proposed in this paper to investigate the possibility of using at least two different approaches for each of the two possible types of mentions. One of these methods is novel and is introduced, together with a review of the others, in Section 4, while the corpus used for the experiments is described in Section 3. In order to obtain reliable surveys with error-prone approaches, conditions have to be automatically identified for selecting reliably annotated samples and for possibly triggering suitable refinements. An effective sequential theme hypothesization strategy will be introduced in Section 6. A component of the strategy performs refinements consisting in recovering deletion or insertion errors in multiple theme hypotheses having a correct theme already hypothesized. The strategy also includes a new theme evaluation process to compensate some specific classification errors that may be caused by asr errors. The execution of this process is restricted to specific situations. The process action performs semantically coherent, compositions or decompositions of already generated theme hypotheses. New features are used in this process. They are designed to characterize diffuse theme mentions and are extracted from word lattices as described in Wintrode and Kulp (2009) and Hazen (2011a). A moderate amount of human compiled knowledge is conceived to establish preconditions inspired by the application ontology for the application of the refinement process. The components of the theme hypothesization strategy are conceived to have low linear time complexity making their execution faster that the asr process. The experimental results reported in Sections 5 and 6 show that topic identification errors increase with the disagreement among the four systems. Nevertheless, reliable surveys can be obtained with high annotation accuracy in a high proportion of the available conversations with a sequential decision strategy that progressively applies specific decision criteria and features from asr generated lattices of word hypotheses.

@&#RELATED WORK@&#

As mentioned in the topic identification review presented in Hazen (2011) topic identification is performed with supervised and unsupervised classification methods. It is worth mentioning among them, especially for text documents, decision trees (Cohen and Singer, 1999), naive Bayes classifiers (Li and Jain, 1998), other probabilistic classifiers (Lewis and Gale, 1994), support vector machines (Joachims, 1998), k-nearest neighbors and example-based classifiers (Yang and Pedersen, 1997).

Popular features used by the classifiers are terms, for each of which measures such as term frequency (tf), inverse document frequency (idf), and tf-idf combinations are computed. A review on the unsupervised selection of terms such as keywords and word chunks can be found in Chen et al. (2010). These features can be enriched with variations of left and right context and with parts of speech (pos) for each selected word. Other feature selection methods are discussed in Yang and Pedersen (1997).

In order to reduce the dimensionality of the feature space, latent semantic analysis (lsa) (Bellegarda, 2000), its probabilistic version (plsa) and latent Dirichlet allocation (lda) have been used for topic identification. In Chien and Wu (2008) a naive Bayes classifier based on adaptive lsa is proposed for topic identification in spoken documents. With lda (Blei et al., 2003; Wintrode, 2011), the features representing a document are probabilities of the presence of hidden topics in a document. Hidden topics are determined by a specific learning process and differ from the topics to be identified. Approaches based on the assumption that a document is represented by a feature vector that is a combination of a small number of topic vectors are reviewed and compared in Min and Ma (2013).

Recently (Morchid et al., 2014), it has been observed that theme identification accuracy may have large variations for different choices and dimensions of hidden topic spaces. Stable and superior performance has been achieved by introducing a new method called c-vector proposed for integrating features obtained in a large variety of these spaces.

Topic segmentation in spoken conversations is reviewed in Purver (2011). An evaluation of coarse-grain discourse segmentation can be found in Niekrasz and Moore (2010). Interesting solutions have been proposed for linear models of nonhierarchical segmentation. Some approaches propose inference methods for selecting segmentation points at the local maxima of cohesion functions. Some functions use features extracted in each conversation sentence or in a window including a few sentences. Some search methods detect cohesion in a conversation using language models and some others consider hidden topics shared across documents. These aspects are discussed for text processing in Eisenstein and Barzilay (2008) together with a critical review on lexical cohesion. In Balchandran et al. (2010) techniques for topic detection have been proposed for obtaining topic specific language models (lms) used for re-scoring asr results.

Multi-label text classification is discussed in de Carvalho and Freitas (2009) and Tsoumakas and Katakis (2007) for large collections of text documents. In de Carvalho and Freitas (2009) a technique, called creation, is proposed. It consists in creating new composite labels for each association of multiple labels assigned to an instance. An important contribution with useful discussions and a thorough review on learning semantic structures from in-domain text document can be found in Chen (2010). A generative model of content structure is proposed based on an explicit representation of discourse constraints on regularities that are important for topic selection.

This paper adapts concepts found in the recent literature to the design of an automatic process for reliably annotating and selecting conversations for a ccs survey. A new sequential decision strategy is proposed for this purpose. It uses results of four different classification systems conceived for searching mentions of theme relevant features in an entire conversation or in specific segments of it. The motivation for considering these two possibilities is the unpredictable language style of casual real-world application users. One of the four systems uses a new method for extracting features from discourse segments. Rather than building segments by collapsing sentences or paragraphs or by identifying suitable segment bounds, the proposed approach looks for zones of a conversation dense of theme relevant features and uses these data for classification. This is motivated by the observation that often customers tend to introduce their problem with factual descriptions that are irrelevant for the task and agents tend to formulate a solution with a verbose introduction followed by a concise formal explanation dense of theme specific words and phrases.

In Wintrode and Kulp (2009), a method is introduced for performing a supervised classification of a spoken document into a single topic class. Features for the classification are posterior probabilities of word hypotheses computed over an entire conversation and obtained from word lattices generated by an asr system. These lattice features are also applied in Hazen (2011a) to supervised topic classification with minimum error classifiers. Hidden topic features obtained with lda have been combined with other features in support vector machine (svm) classifiers to assign spoken documents to relevance classes (Wintrode, 2011) using asr word lattice hypotheses.

The strategy proposed in this paper is based on an application independent criterion for composing conversations sets of increasing size with limited decrease in automatic annotation accuracy. A specific strategy process attempts to alleviate the effects of possible asr errors by using specific word and distance bigram features extracted from the lattice of word hypotheses generated by an asr system. These bigrams are selected to express distance relations between facts and actions that characterize each theme.

The application task considered in this paper is the automatic annotation of conversations between a customer and the agent of a ccs. Application requirements establish that the conversations have to be annotated in terms of application domain semantic contents. Essential contents are themes belonging to the following set (between parentheses the abbreviations used by the agents):


                     
                        
                           
                              
                                 
                                    T
                                 
                              
                              :
                              =
                              {
                              itinerary
                                 
                              
                                 (itnr)
                              
                              ,
                              lost
                                 
                              and
                                 
                              found
                                 
                              
                                 (objt)
                              
                              ,
                              time
                                 
                              schedules
                                 
                              
                                 (horr)
                              
                              ,
                              transportation
                                 
                              card
                                 
                              navigo
                                 
                              
                                 (nvgo)
                              
                              ,
                              traffic
                                 
                              state
                                 
                              
                                 (etfc)
                              
                              ,
                              fares
                                 
                              
                                 (tarf)
                              
                              ,
                              contraventions
                                 
                              
                                 (pv)
                              
                              ,
                              transportation
                                 
                              card
                                 
                              vgc
                                 
                              
                                 (vgc)
                              
                              ,
                              out
                                 
                              of
                                 
                              domain
                                 
                              
                                 (trsh)
                              
                              }
                           
                        
                     
                  

Identifying these themes is the purpose of the research described in this paper. Conversations may be about more than one theme and the co-presence of related themes appears to be essential to characterize the customer problem and to adequately monitor the time evolution of problem proportions in order to assess the increase or decrease of mentions of user concerns. Such an assessment can be made with surveys containing conversation samples. A method for obtaining acceptable estimations of proportion variability in time using surveys with samples that may contain annotation errors can be found in Camelin et al. (2009). Starting with themes annotated with sufficient accuracy, more specific semantic information can be extracted and used to provide more details in conversation reports. These details can be used for other tasks such as the evaluation of the impact of strikes at specific dates. This aspect will be considered in future work.

Conversations are made available in successive time periods. Samples taken from different periods of a year have been selected to form a corpus of 1654 telephone conversations collected at the call center of the public transportation service in Paris (ratp). The corpus has been manually transcribed after removing any relation to customer identity and other confidential information. In order to perform experiments with a fully automatic system, the corpus was split into three corpora, namely a train, a development and a test set containing respectively 880, 196 and 576 conversations. The sizes of the corpora in terms of conversations annotated with one or more theme labels are shown in Table 1
                     . The table includes, for each set, the number of conversations annotated with a single theme label (#mono-label), the number of conversations annotated with multiple theme labels (#multi-label), the label cardinality (i.e. the average number of labels by conversation, denoted here label card.) and the percentage of conversations annotated with multiple labels.

Conversations have been independently annotated by three human annotators in terms of the main theme and possibly additional themes. When the annotators disagree, a consensus was found after discussion. The corpus with manual transcriptions and annotations will be made publicly available at the end of the decoda project pending the authorization of the data owners. In order to develop an effective strategy, four initial different systems have been considered and developed for automatic annotation. All the four systems use the same type of features extracted from the 1-best sequence of word hypotheses generated by an asr component described in Linarès et al. (2007). The asr system is based on triphone acoustic hidden Markov models (hmm) with mixtures of 230,000 Gaussian distributions. Model parameters were estimated with maximum a posteriori probability (map) adaptation of 150 hours of speech in telephone bandwidth with the data of the train set. A 4-gram language model (lm) was obtained by adapting with the transcriptions of the train set a basic lm whose parameters have been estimated with other corpora. An initial set of experiments was performed with this system. The results show a wer on the test set of 57% (52% for agents and 62% for customers). These high error rates are mainly due to speech disfluencies and to adverse acoustic environments for some conversations when, for example, customers call from train stations or noisy streets with mobile phones. Furthermore, the signal of some sentences is saturated or of low intensity due to the distance between speakers and phones. Such difficult acoustic conditions have a significant impact on the density of the lattices with peaks of 2,600 links per second.

Two systems called density and hmm segment a conversation and attempt to hypothesize theme mentions in segments. The other two systems called cosine and poisson generate annotation hypotheses by cumulating scores for each feature in the entire conversation.

The asr component generates a lattice of word hypotheses using a vocabulary of a fixed number of words obtained from the content of the database of the application domain and words observed in the train corpus. The test data may contain out-of-vocabulary (oov) words not observed in the train set. It was observed that the train corpus vocabulary contains 7920 words while the test corpus contains 3806 words, only 70.8% of them occur in the train corpus and the model perplexity is 82 for the dev set and 86 for the test set. In spite of possibly frequent asr errors, it is worth investigating the possibility of performing reliable theme surveys considering that conversations contain mentions to rich sets of theme relevant facts, entities and actions and many small sets of them are sufficient conditions for theme identification.

Theme identification may use the most likely sequence (1-best sequence) of word hypotheses as features for theme identification. Using features from the lattice of word hypotheses may only reduce the effect of some errors in the 1-best sequence of words in the vocabulary, but may also introduce confusion in the classification decision performed by the theme identification strategy.

In addition to the oov problem, theme identification may also confront the code-switching problem occurring when speakers alternate between language varieties. An analysis of the train set shows that the code-switching problem is not frequent and appears only in some customer segments. This is explained by the fact that the agent has to follow a well-defined problem solving protocol and tends to drive the conversation according to it. Furthermore, most of the varieties and use of oov words relate to descriptions of facts that are irrelevant for the application purpose. In any case, in order to model possible code-switching effects, two systems, namely density and hmm, have been conceived to analyze conversation segments primarily for using localized features to model possible variations in theme mentions.

For the sake of comparison, the four systems perform classifications with different methods using the same features extracted from the 1-best sequence of word hypotheses.

As the four systems are based on different algorithms, it is expected that they make different types of classification errors and tend to produce different annotations mostly when the features are affected by drastic asr errors.

Parameter estimation for each system is based on the minimization of classification errors and comparisons of scores with thresholds for making decisions. For this reason, the consensus among the four systems is expected to be a good confidence index of the classification results. Consensus is thus used for an initial selection of the conversations to be inserted in the survey.

In order to find, in specific conditions of weak consensus, additional conversations to be placed in the survey, the strategy is refined by a conditional execution of a fifth new process based on a new method and new features. These features are specific for each theme and are extracted from the lattice of word hypotheses generated by the asr component. Details of these features will be provided later on with the description of the refinement strategy.

An architecture for multiple theme hypothesization has been conceived for integrating the results of different systems implementing different global and segmental approaches for extracting mentions of application relevant themes.

The architecture has the scheme shown in Fig. 1
                      and is based on three components, namely an asr system that generates word hypotheses, a theme hypothesization component that contains four systems whose results are further processed by a decision making strategy.

The four systems use the most likely (1-best) sequence of word hypotheses generated by the same asr component. Different features extracted from the lattice of word hypotheses are available. They are processed by a fifth system whose output can be used by the strategy for making decisions when there are doubts about the confidence of the outputs of the four other systems. The four basic systems use the same set of features made of words selected based on their purity, word classes and short distance word bigrams.

Theme hypotheses are generated by systems called cosine and poisson using theme mentions detected anywhere in a conversation, while hmm and density localize theme mentions in specific positions of a conversation and make decisions based on features extracted around these positions. The four systems are introduced below.

A short description and preliminary results obtained with cosine and density have been presented in Bost et al. (2013). As density is a novel method, it will be described here in more details.

The features used by the four systems contain a selection of 7217 words in the asr lexicon, a few word classes such as times and prices, and word bigrams. Distance bigrams made of pairs of words distant a maximum of two words are also used. A reduced feature set V
                     
                        f
                      is obtained by selecting features based on their purity and coverage. Purity of a feature f is defined with the Gini criterion as follows:


                     
                        
                           (1)
                           
                              G
                              (
                              f
                              )
                              =
                              
                                 ∑
                                 
                                    t
                                    ∈
                                    
                                       
                                          T
                                       
                                    
                                 
                              
                              
                                 
                                    
                                       ℙ
                                    
                                 
                                 2
                              
                              (
                              t
                              |
                              f
                              )
                              =
                              
                                 ∑
                                 
                                    t
                                    ∈
                                    
                                       
                                          T
                                       
                                    
                                 
                              
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   df
                                                   t
                                                
                                                (
                                                f
                                                )
                                             
                                             
                                                
                                                   df
                                                   
                                                      
                                                         ℂ
                                                      
                                                   
                                                
                                                (
                                                f
                                                )
                                             
                                          
                                       
                                    
                                 
                                 2
                              
                           
                        
                     where 
                        
                           df
                           
                              
                                 ℂ
                              
                           
                        
                        (
                        f
                        )
                      is the number of conversations of the train set 
                        
                           ℂ
                        
                      containing feature f and df
                     
                        t
                     (f) is the number of conversations of the train set containing feature f and annotated with theme t.

Considering a conversation as a document, a score 
                        
                           w
                           t
                        
                        (
                        f
                        )
                      for feature f is defined, using all the samples of t in the train set, as follows:


                     
                        
                           (2)
                           
                              
                                 w
                                 t
                              
                              (
                              f
                              )
                              =
                              
                                 df
                                 t
                              
                              (
                              f
                              )
                              ·
                              
                                 idf
                                 2
                              
                              (
                              f
                              )
                              ·
                              
                                 G
                                 2
                              
                              (
                              f
                              )
                           
                        
                     where idf(f) is the inverse document frequency for feature f.

The cosine system generates a theme hypothesis with a classification process based on a global cosine measure of similarity sc(y, t) between a conversation y and a theme t. Representing y by a vector vy
                         and t by a vector vt
                        , this measure of similarity is computed as follows:


                        
                           
                              (3)
                              
                                 sc
                                 (
                                 y
                                 ,
                                 t
                                 )
                                 =
                                 cos
                                 (
                                 
                                    
                                       
                                          
                                             
                                                
                                                   v
                                                   y
                                                
                                             
                                          
                                       
                                       ,
                                       
                                          
                                             
                                                
                                                   v
                                                   t
                                                
                                             
                                          
                                       
                                    
                                    ˆ
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                          
                                             f
                                             ∈
                                             y
                                             ∩
                                             t
                                          
                                       
                                       
                                          w
                                          y
                                       
                                       (
                                       f
                                       )
                                       .
                                       
                                          w
                                          t
                                       
                                       (
                                       f
                                       )
                                    
                                    
                                       
                                          
                                             
                                                ∑
                                                f
                                             
                                             
                                                w
                                                y
                                             
                                             
                                                
                                                   (
                                                   f
                                                   )
                                                
                                                2
                                             
                                             .
                                             
                                                ∑
                                                f
                                             
                                             
                                                w
                                                t
                                             
                                             
                                                
                                                   (
                                                   f
                                                   )
                                                
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              w
                              y
                           
                           (
                           f
                           )
                         is a score for feature f in conversation y.

Let γ
                        1(y) be the set of themes discussed in y. The following decision rule for automatically annotating conversation y with a theme class label t is initially applied:


                        
                           
                              (4)
                              
                                 t
                                 ∈
                                 
                                    γ
                                    1
                                 
                                 (
                                 y
                                 )
                                 ⇒
                                 sc
                                 (
                                 y
                                 ,
                                 t
                                 )
                                 ⩾
                                 ρ
                                 ·
                                 sc
                                 (
                                 y
                                 ,
                                 
                                    
                                       t
                                       ˆ
                                    
                                 
                                 )
                              
                           
                        where 
                           
                              
                                 t
                                 ˆ
                              
                           
                           :
                           =
                           
                              argmax
                              
                                 
                                    t
                                    ′
                                 
                                 ∈
                                 
                                    
                                       T
                                    
                                 
                              
                           
                           sc
                           (
                           y
                           ,
                           
                              t
                              ′
                           
                           )
                        ; and ρ
                        ∈[0;1] is an empirical parameter whose value is estimated by experiments on the development set. If the score of 
                           
                              t
                              ˆ
                           
                         is too low, then the application of the above rule is not reliable. To overcome this problem, the following additional rule is introduced:


                        
                           
                              (5)
                              
                                 t
                                 ∈
                                 
                                    γ
                                    1
                                 
                                 (
                                 y
                                 )
                                 ⇒
                                 sc
                                 (
                                 y
                                 ,
                                 t
                                 )
                                 ⩾
                                 v
                                 
                                    ∑
                                    
                                       
                                          t
                                          ′
                                       
                                       ∈
                                       
                                          
                                             T
                                          
                                       
                                    
                                 
                                 sc
                                 (
                                 y
                                 ,
                                 
                                    t
                                    ′
                                 
                                 )
                              
                           
                        where 
                           v
                           ∈
                           [
                           0
                           ;
                           1
                           ]
                         is another parameter whose value is estimated by experiments on the development set.

The optimal value for ρ, the proportion of the highest score required for assigning themes to a conversation, has been evaluated to 
                           
                              
                                 ρ
                                 ˆ
                              
                           
                           =
                           0.69
                        , and the optimal value threshold 
                           v
                         to 
                           
                              
                                 v
                                 ˆ
                              
                           
                           =
                           0.16
                        .

Rather than representing a conversation with a bag of words whose order is irrelevant, the density system analyzes sequences of transcribed words for extracting features at specific positions in a sequence. A conversation containing N words is seen, including spaces, as a finite sequence (p
                        1, …, p
                        
                           n
                        ) of n positions (with n
                        =2N
                        −1). Features are described taking into account the position where they are extracted. The kth word unigram in the feature sequence representation is located at position p
                        2k−1, the kth bigram is at position p
                        2k
                        , the kth distant bigram with one intermediate unigram is at position p
                        2k+1.

The contribution to theme t of the features at the ith position in a conversation is:


                        
                           
                              (6)
                              
                                 
                                    w
                                    t
                                 
                                 (
                                 
                                    p
                                    i
                                 
                                 )
                                 =
                                 
                                    1
                                    
                                       ∥
                                       
                                          
                                             
                                                
                                                   v
                                                   t
                                                
                                             
                                          
                                       
                                       ∥
                                    
                                 
                                 
                                    ∑
                                    
                                       f
                                       ∈
                                       
                                          τ
                                          
                                             
                                                p
                                                i
                                             
                                          
                                       
                                    
                                 
                                 
                                    w
                                    t
                                 
                                 (
                                 f
                                 )
                                    
                                    
                                 (
                                 i
                                 =
                                 1
                                 ,
                                 …
                                 ,
                                 n
                                 )
                              
                           
                        where 
                           
                              τ
                              
                                 
                                    p
                                    i
                                 
                              
                           
                         is the set made of the features (unigram and associated bigrams) located at position p
                        
                           i
                         in a conversation.

A thematic density d
                        
                           t
                        (p
                        
                           i
                        ) of theme t is associated with position p
                        
                           i
                         and is defined as follows:


                        
                           
                              (7)
                              
                                 
                                    d
                                    t
                                 
                                 (
                                 
                                    p
                                    i
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                          
                                             j
                                             =
                                             1
                                          
                                          n
                                       
                                       
                                          (
                                          
                                             w
                                             t
                                          
                                          (
                                          
                                             p
                                             j
                                          
                                          )
                                          )
                                       
                                       /
                                       
                                          (
                                          
                                             λ
                                             
                                                
                                                   d
                                                   j
                                                
                                             
                                          
                                          )
                                       
                                    
                                    
                                       
                                          ∑
                                          
                                             j
                                             =
                                             1
                                          
                                          n
                                       
                                       1
                                       /
                                       
                                          (
                                          
                                             λ
                                             
                                                
                                                   d
                                                   j
                                                
                                             
                                          
                                          )
                                       
                                    
                                 
                                 
                                 (
                                 i
                                 =
                                 1
                                 ,
                                 …
                                 ,
                                 n
                                 )
                              
                           
                        where λ
                        ⩾1 is a parameter of sensitivity to proximity whose value is estimated by experiments on the development set and d
                        
                           j
                        
                        :=|i
                        −
                        j| is the distance in a conversation between positions p
                        
                           i
                         and p
                        
                           j
                        .

According to Eq. (7), the density for theme t at the ith position of a conversation is obtained by adding the contributions of the features located at that point and the contributions of each surrounding (i
                           ±
                           d) position weighted according to the distance d. Such a computation requires two nested loops over the n conversation positions, resulting in a quadratic time complexity.

In order to reduce computation costs, recurrence relations have been introduced in the expression of Eq. (7), resulting in a linear computation algorithm.

This algorithm, as well as the recurrence formulas it relies on, are fully specified in Appendix A.

The thematic density at a specific conversation position is used to construct a thematic skeleton by plotting the density of theme hypotheses as function of a position measured in number of words (including spaces) preceding the position.

An example of thematic skeleton for a particular conversation is reported in Appendix B. A similar example can be found in Bost et al. (2013).

A theme t is considered as discussed in a conversation y if it has dominant density at a position of the conversation (rule (8)) and if the sum of its densities in positions where it is dominant exceeds an empirically determined threshold (rule (9)):


                           
                              
                                 (8)
                                 
                                    t
                                    ∈
                                    
                                       γ
                                       2
                                    
                                    (
                                    y
                                    )
                                    ⇒
                                    ∃
                                    i
                                    ∈
                                    〚
                                    1
                                    ;
                                    n
                                    〛
                                    :
                                    ∀
                                    
                                       t
                                       ′
                                    
                                    ∈
                                    
                                       
                                          T
                                       
                                    
                                    ,
                                       
                                    
                                       d
                                       t
                                    
                                    (
                                    
                                       p
                                       i
                                    
                                    )
                                    ⩾
                                    
                                       d
                                       
                                          
                                             t
                                             ′
                                          
                                       
                                    
                                    (
                                    
                                       p
                                       i
                                    
                                    )
                                 
                              
                           
                           
                              
                                 (9)
                                 
                                    t
                                    ∈
                                    
                                       γ
                                       2
                                    
                                    (
                                    y
                                    )
                                    ⇒
                                    
                                       ∑
                                       
                                          i
                                          ∈
                                          I
                                       
                                    
                                    
                                       d
                                       t
                                    
                                    (
                                    
                                       p
                                       i
                                    
                                    )
                                    ⩾
                                    v
                                    
                                       ∑
                                       
                                          j
                                          =
                                          1
                                       
                                       n
                                    
                                    
                                       d
                                       
                                          
                                             t
                                             j
                                          
                                       
                                    
                                    (
                                    
                                       p
                                       j
                                    
                                    )
                                 
                              
                           where γ
                           2(y) is the set of themes hypothesized for conversation y; 
                              v
                              ∈
                              [
                              0
                              ,
                              1
                              ]
                            is a parameter whose value is estimated by experiments on the development set; t
                           
                              j
                            is the theme of dominant density at the jth position in the conversation; and 
                              I
                              :
                              =
                              {
                              i
                              ∈
                              〚
                              1
                              ;
                              n
                              〛
                                 
                              |
                                 
                              ∀
                              
                                 t
                                 ′
                              
                              ∈
                              
                                 
                                    T
                                 
                              
                              ,
                                 
                              
                                 d
                                 t
                              
                              (
                              
                                 p
                                 i
                              
                              )
                              ⩾
                              
                                 d
                                 
                                    
                                       t
                                       ′
                                    
                                 
                              
                              (
                              
                                 p
                                 i
                              
                              )
                              }
                           .


                        hmms have been widely used for topic identification (see, for example, Mittendorf and Schäuble, 1994; Wallace et al., 2013 for a recent application).

For each theme t
                        
                           k
                        , a language model lmk is obtained with the train set. The language model lmk is made of unigram probabilities and of probabilities 
                           
                              
                                 
                                    ℙ
                                 
                              
                              k
                           
                           (
                           wh
                           )
                        , where histories are obtained from chunks automatically selected with the same procedure used in Maza et al. (2011). Conditional probabilities 
                           
                              
                                 ℙ
                              
                           
                           (
                           
                              t
                              i
                           
                           |
                           
                              t
                              j
                           
                           )
                         are estimated from the conversations of the train set annotated with two themes. These probabilities are used together with probabilities 
                           
                              
                                 ℙ
                              
                           
                           (
                           
                              t
                              k
                           
                           )
                         that there is a unique theme in a conversation.

A network is then constructed by connecting in parallel theme hmms modeling single theme conversations and sequences of these hmms for modeling multiple theme conversations. Probabilities of having a unique theme or pairs of themes are associated with each branch of the network. Word generation probabilities of these models are provided by the theme lms. The Viterbi algorithm is then applied using the network as model and the words of the 1-best sequence of conversation y as observations for computing the highest probability among the following candidates 
                           
                              
                                 ℙ
                              
                           
                           (
                           y
                           |
                           
                              t
                              k
                           
                           )
                        , 
                           
                              
                                 ℙ
                              
                           
                           (
                           y
                           |
                           
                              t
                              m
                           
                           
                              t
                              j
                           
                           )
                        , for every unique theme (∀
                        k) and for every pair (∀(m, j)). When the hypothesis of multiple themes is predominant, segments expressing pairs of hypothesized themes are hypothesized by the search algorithm, together with a score for each segment. In this way, segments of theme mentions are hypothesized using models of short term sequential constraints.

The optimal hypothesis 
                           
                              t
                              ˆ
                           
                         consisting of a single theme or a sequence of themes is obtained with the following equation:


                        
                           
                              (10)
                              
                                 
                                    
                                       t
                                       ˆ
                                    
                                 
                                 =
                                 
                                    
                                       arg
                                       
                                       max
                                    
                                    t
                                 
                                 (
                                 
                                    
                                       ℙ
                                    
                                 
                                 (
                                 t
                                 |
                                 y
                                 )
                                 )
                                 =
                                 
                                    
                                       arg
                                       
                                       max
                                    
                                    t
                                 
                                 (
                                 log
                                 (
                                 
                                    
                                       ℙ
                                    
                                 
                                 (
                                 y
                                 |
                                 t
                                 )
                                 )
                                 +
                                 log
                                 (
                                 
                                    
                                       ℙ
                                    
                                 
                                 (
                                 t
                                 )
                                 )
                                 )
                              
                           
                        
                     

Given the relatively small size of the available corpus, the Poisson law is well suited to take into account the sparse distribution of features f in a theme t. The approach followed for finding the results reported in this paper is inspired by Bahl et al. (1988). Relying on the histogram estimate F of feature occurrences, with mean μ
                        
                           f,t
                        , a topic model has been conceived based on the following probability:


                        
                           
                              (11)
                              
                                 
                                    
                                       ℙ
                                    
                                 
                                 (
                                 
                                    F
                                    f
                                 
                                 =
                                 r
                                 |
                                 t
                                 )
                                 =
                                 
                                    
                                       
                                          e
                                          
                                             −
                                             
                                                μ
                                                
                                                   f
                                                   ,
                                                   t
                                                
                                             
                                          
                                       
                                       
                                          μ
                                          
                                             f
                                             ,
                                             t
                                          
                                          r
                                       
                                    
                                    
                                       r
                                       !
                                    
                                 
                              
                           
                        
                     

According to Bahl et al. (1988) it is assumed that the feature frequencies have independent Poisson distribution and 
                           log
                           (
                           
                              
                                 ℙ
                              
                           
                           (
                           y
                           |
                           t
                           )
                           )
                         can be estimated as follows:


                        
                           
                              (12)
                              
                                 log
                                 (
                                 
                                    
                                       ℙ
                                    
                                 
                                 (
                                 y
                                 |
                                 t
                                 )
                                 )
                                 ≃
                                 
                                    ∑
                                    f
                                 
                                 (
                                 
                                    F
                                    f
                                 
                                 log
                                 (
                                 
                                    μ
                                    
                                       f
                                       ,
                                       t
                                    
                                 
                                 )
                                 −
                                 
                                    μ
                                    
                                       f
                                       ,
                                       t
                                    
                                 
                                 −
                                 log
                                 (
                                 
                                    F
                                    f
                                 
                                 !
                                 )
                                 )
                              
                           
                        
                     

The last logarithm in the summation is independent of t and can be ignored leading to:


                        
                           
                              (13)
                              
                                 
                                    
                                       t
                                       ˆ
                                    
                                 
                                 =
                                 
                                    
                                       arg
                                       
                                       max
                                    
                                    t
                                 
                                 (
                                 log
                                 (
                                 
                                    
                                       ℙ
                                    
                                 
                                 (
                                 t
                                 )
                                 )
                                 +
                                 
                                    ∑
                                    f
                                 
                                 (
                                 
                                    F
                                    f
                                 
                                 log
                                 (
                                 
                                    μ
                                    
                                       f
                                       ,
                                       t
                                    
                                 
                                 )
                                 −
                                 
                                    μ
                                    
                                       f
                                       ,
                                       t
                                    
                                 
                                 )
                                 )
                              
                           
                        
                     

The proposed approaches have been evaluated following procedures discussed in Tsoumakas and Katakis (2007) with measures used in Information Retrieval (ir) and accuracy as defined in the following for a corpus 
                           
                              X
                           
                         and a decision strategy γ.


                        Recall:


                        
                           
                              (14)
                              
                                 R
                                 (
                                 γ
                                 ,
                                 
                                    
                                       X
                                    
                                 
                                 )
                                 =
                                 
                                    1
                                    
                                       |
                                       
                                          
                                             X
                                          
                                       
                                       |
                                    
                                 
                                 
                                    ∑
                                    
                                       y
                                       ∈
                                       
                                          
                                             X
                                          
                                       
                                    
                                 
                                 
                                    
                                       |
                                       γ
                                       (
                                       y
                                       )
                                       ∩
                                       M
                                       (
                                       y
                                       )
                                       |
                                    
                                    
                                       |
                                       M
                                       (
                                       y
                                       )
                                       |
                                    
                                 
                              
                           
                        where M(y) indicates the set of themes manually annotated for conversation y and γ(y) the set of themes automatically hypothesized for the same conversation.


                        Precision:


                        
                           
                              (15)
                              
                                 P
                                 (
                                 γ
                                 ,
                                 
                                    
                                       X
                                    
                                 
                                 )
                                 =
                                 
                                    1
                                    
                                       |
                                       
                                          
                                             X
                                          
                                       
                                       |
                                    
                                 
                                 
                                    ∑
                                    
                                       y
                                       ∈
                                       
                                          
                                             X
                                          
                                       
                                    
                                 
                                 
                                    
                                       |
                                       γ
                                       (
                                       y
                                       )
                                       ∩
                                       M
                                       (
                                       y
                                       )
                                       |
                                    
                                    
                                       |
                                       γ
                                       (
                                       y
                                       )
                                       |
                                    
                                 
                              
                           
                        
                     


                        F-score:


                        
                           
                              (16)
                              
                                 F
                                 (
                                 γ
                                 ,
                                 
                                    
                                       X
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       2
                                       P
                                       (
                                       γ
                                       ,
                                       
                                          
                                             X
                                          
                                       
                                       )
                                       R
                                       (
                                       γ
                                       ,
                                       
                                          
                                             X
                                          
                                       
                                       )
                                    
                                    
                                       P
                                       (
                                       γ
                                       ,
                                       
                                          
                                             X
                                          
                                       
                                       )
                                       +
                                       R
                                       (
                                       γ
                                       ,
                                       
                                          
                                             X
                                          
                                       
                                       )
                                    
                                 
                              
                           
                        
                     


                        Accuracy:

The traditional way of computing the accuracy (denoted hereafter acc
                        1) is defined as:


                        
                           
                              (17)
                              
                                 
                                    acc
                                    1
                                 
                                 (
                                 γ
                                 ,
                                 
                                    
                                       X
                                    
                                 
                                 )
                                 =
                                 
                                    1
                                    
                                       |
                                       
                                          
                                             X
                                          
                                       
                                       |
                                    
                                 
                                 
                                    ∑
                                    
                                       y
                                       ∈
                                       
                                          
                                             X
                                          
                                       
                                    
                                 
                                 g
                                 (
                                 γ
                                 (
                                 y
                                 )
                                 ,
                                 M
                                 (
                                 y
                                 )
                                 )
                              
                           
                        where


                        
                           
                              (18)
                              
                                 g
                                 (
                                 γ
                                 (
                                 y
                                 )
                                 ,
                                 M
                                 (
                                 y
                                 )
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   1
                                                
                                                
                                                   if
                                                   
                                                   γ
                                                   (
                                                   y
                                                   )
                                                   =
                                                   M
                                                   (
                                                   y
                                                   )
                                                
                                             
                                             
                                                
                                                   0
                                                
                                                
                                                   otherwise
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

For multiple topic identification, it is also interesting to compute the accuracy in another way (denoted acc
                        2), as proposed by Tsoumakas and Katakis (2007):


                        
                           
                              (19)
                              
                                 
                                    acc
                                    2
                                 
                                 (
                                 γ
                                 ,
                                 
                                    
                                       X
                                    
                                 
                                 )
                                 =
                                 
                                    1
                                    
                                       |
                                       
                                          
                                             X
                                          
                                       
                                       |
                                    
                                 
                                 
                                    ∑
                                    
                                       y
                                       ∈
                                       
                                          
                                             X
                                          
                                       
                                    
                                 
                                 
                                    
                                       |
                                       γ
                                       (
                                       y
                                       )
                                       ∩
                                       M
                                       (
                                       y
                                       )
                                       |
                                    
                                    
                                       |
                                       γ
                                       (
                                       y
                                       )
                                       ∪
                                       M
                                       (
                                       y
                                       )
                                       |
                                    
                                 
                              
                           
                        
                     

In the following if only accuracy is mentioned, it has to be assumed to refer to acc
                        1.

@&#RESULTS@&#

The four systems density, hmm, poisson, and cosine have been separately evaluated on the development and test sets. A linear interpolation of the results obtained with the four systems has also been evaluated and results are reported in the line labeled comb. As the dev set is too small for estimating the weights of the linear combination, equal weights have been used for the scores of the four systems.

As the parameters of the four systems are estimated for separately optimizing the accuracy of each system, a linear combination of decision scores of each system is not as efficient as consensus. The reason is that consensus is based only on each system decision that is reasonable to consider as the most reliable indicator of the evidence ascribed by each system to the theme hypotheses it generates after classifying with a specific approach an entire conversation.

The results are reported for the dev set in Tables 2 and 3
                        
                        , and for the test set in Tables 4 and 5
                        
                        . Label man refers to automatic theme annotation of manual transcriptions, while label asr refers to automatic theme annotation of asr system transcription.

For the sake of comparison, the results obtained with the proposed classification approaches have been compared with those obtained with a support vector machine (svm) using the same features (unigrams and bigrams with possible gap of one word) and a linear kernel. Denoting by 
                           
                              X
                           
                         the set of conversations, a binary classifier 
                           
                              γ
                              k
                           
                           :
                           
                              
                                 X
                              
                           
                           →
                           {
                           
                              t
                              k
                           
                           ,
                           
                              
                                 
                                    t
                                    k
                                 
                              
                              ¯
                           
                           }
                         is defined for every theme 
                           
                              t
                              k
                           
                           ∈
                           
                              
                                 T
                              
                           
                        . For every pair (y, t
                        
                           k
                        ) containing the conversation y and the kth theme, a score is computed by using this kth classifier. The candidate theme hypotheses for a conversation are those whose score is in an interval corresponding to an empirically determined proportion of the highest one. In addition to that, the hypothesis with the highest score must be above a threshold empirically determined for this purpose.

The toolkit used to perform svm-based categorization, svm
                        light, is described in Joachims (2002), Joachims (1999) and Joachims (2002).

The results reported in Tables 2–5 show that the results obtained with the proposed classification methods are not inferior and in many cases are significantly better than those obtained with svm. Furthermore, they also show that just a linear combination of the classifier outputs does not provide significant improvements over the best results obtained with one of the four proposed classifiers.

Some discrepancies between the results obtained with the dev and the test sets can be explained by the fact that parts of the test data were collected during the summer while all the train and dev data were collected in other seasons. Such a consideration is important since during the summer the proportion of traveling visitors with specific problems, speaking styles and accents is significantly higher than in the other seasons. Based on these considerations, lm probabilities are expected to exhibit some variation in the summer period. In order to evaluate this expectation, samples of the same time period have been analyzed in a sequence corresponding to date and time of their collection. A set of adaptation data has been collected using words of the 1-best sequence of word hypotheses generated by the asr component. Using these data for lm adaptation resulted in minor reductions of the wer and minor theme classification improvements that are not worth reporting here, over representing contexts with statistics of words that coexist in a conversation.

The possibility of modeling sequences of theme mentions in a multi-theme conversation has been also considered and the manual transcriptions of the entire train set have been analyzed for this purpose. Unfortunately no reliable cues have been identified to model a theme switch by the customer. Nevertheless, such an analysis suggested that hypothesization of concepts about specific theme contents may benefit from the knowledge of some extent of the concept mention. Such an investigation is part of future work.

A careful analysis has also been made for errors due to discrepancies between a correct theme hypothesization from manual (man) and automatic (asr) annotations. It was observed that most of these errors concern the hypothesization of the trsh class that is not a theme but an indication that the conversation content does not concern the application domain. Errors may concern the hypothesization of trsh. This is the cause of 40% of the errors in dev and 36.59% of the errors in test. The main reason for this is the large variability of the trsh lexicon that makes it difficult to model and classify it as a theme. A model and a classifier that are suitable for theme classification may not work well for rejecting out of domain conversations and different approaches should probably be investigated to further improve theme classification even if the results reported in this paper are more than satisfactory for a complete real-life application.

The design of the decision strategy is inspired by the application requirements. The purpose is to obtain, with little or no human effort, a survey with enough correctly annotated samples for estimating, with sufficient confidence, proportions of user problem themes in a given time period. For this purpose, a large enough and accurately annotated portion of a specific corpus (dev, test) has to be obtained from the entire corpus. Such a selection is composed with a sequential decision strategy conceived to progressively augment the survey with samples that cause an acceptable accuracy decrease. The strategy is represented by a tree. A predicate having a conversation y and a set is associated with each node of the tree. Node predicates describe binary relations having pred(y, rs) where pred represents the relation type and rs is a variable that takes values in sets of conversations. For the predicate associated with the root, values of rs can be the dev set for validating the strategy or the test set for evaluating it. A predicate associated with a tree node returns true after evaluating a specific function on specific properties extracted from specific bindings of its arguments.

An initial version of the strategy is constructed as described in the following subsection. For this strategy, the truth of the node predicates is evaluated by asking a question formulated in terms of agreement among the classifications of the four different systems introduced in Section 4. The motivation is that when the same result is obtained by the four systems using diffuse and localized features the consensus should correspond to a reliable decision. Furthermore, confidence of partial consensus based decisions is expected to decrease with the level of consensus.

Three predicates about full and partial consensus among the four systems are associated to consensus sets obtained with the initial version of the strategy tree. Each predicate has the form MAJm
                        (y, rs) (m
                        =4, 3, 2).

Predicate MAJ4
                        (y, rs) is associated with the root of the strategy tree and is evaluated to true when the four systems hypothesize the same themes for conversation y in set rs. The entire development set (dev) is initially used as value of rs for a preliminary assessment of the conceived strategy. Let {true, false} be the possible values of MAJ4
                        (y, rs). Based on these values, the following subsets of rs are formed:


                        
                           
                              (20)
                              
                                 
                                    YRQ
                                    1
                                 
                                 :
                                 =
                                 {
                                 y
                                 ∈
                                 rs
                                    
                                 |
                                    
                                 
                                    
                                       
                                          
                                             MAJ
                                             4
                                          
                                       
                                    
                                 
                                 (
                                 y
                                 ,
                                 rs
                                 )
                                 =
                                 
                                    
                                       true
                                    
                                 
                                 }
                              
                           
                        
                        
                           
                              (21)
                              
                                 
                                    NRQ
                                    1
                                 
                                 :
                                 =
                                 {
                                 y
                                 ∈
                                 rs
                                    
                                 |
                                    
                                 
                                    
                                       
                                          
                                             MAJ
                                             4
                                          
                                       
                                    
                                 
                                 (
                                 y
                                 ,
                                 rs
                                 )
                                 =
                                 
                                    
                                       false
                                    
                                 
                                 }
                              
                           
                        
                     

Sets YRQ
                        1 and NRQ
                        1 are subsets of the value of variable rs. This value indicates the set associated with the mother node of YRQ
                        1 and NRQ
                        1 in the decision tree. Associations of this type can be inferred from the decision tree structure and will not be mentioned in the following for the sake of simplicity. A decision consisting in making a survey only with conversations in YRQ
                        1 is evaluated by two parameters, namely accuracy (acc
                        1), as defined in Eq. (17), and coverage, computed as follows:


                        
                           
                              (22)
                              
                                 cov
                                 (
                                 
                                    YRQ
                                    1
                                 
                                 )
                                 =
                                 
                                    
                                       |
                                       
                                          YRQ
                                          1
                                       
                                       |
                                    
                                    
                                       |
                                       rs
                                       |
                                    
                                 
                              
                           
                        Let set(MAJ4
                        ):=
                        YRQ
                        1, size(set(MAJ4
                        )):=|YRQ
                        1| and let


                        
                           
                              (23)
                              
                                 corr
                                 (
                                 
                                    YRQ
                                    1
                                 
                                 )
                                 =
                                 
                                    ∑
                                    
                                       y
                                       ∈
                                       
                                          YRQ
                                          1
                                       
                                    
                                 
                                 g
                                 (
                                 γ
                                 (
                                 y
                                 )
                                 ,
                                 M
                                 (
                                 y
                                 )
                                 )
                              
                           
                        where g(γ(y), M(y)) has the same meaning as in Eq. (18).

For rs
                        :=
                        dev, the following values have been observed:
                           
                              •
                              size(set(MAJ4
                                 ))=126, corr(set(MAJ4
                                 ))=116

size(NRQ
                                 1)=70, size(dev)=196


                                 cov(set(MAJ4
                                 ))=0.64


                                 acc
                                 1(set(MAJ4
                                 ))=0.92

Based on the above results, motivations for starting the strategy with predicate MAJ4
                        (y, rs) are listed in the following:
                           
                              1
                              Coverage and accuracy are pretty high and more suitable than what could be achieved by playing with thresholds on system combinations as combination weights could not be accurately estimated with a small development set.

All but one conversations in NRQ
                                 1 contain only one correct theme hypothesis instead of two, missing a second semantically coherent theme hypothesis. For example, a conversation is automatically annotated with only itinerary while it was manually annotated with itinerary and time schedule.

Predicates MAJ3
                         and MAJ2
                         to represent consensus of respectively three or two systems are introduced and the corresponding sets are evaluated as described in Appendix C.

The results obtained with the initial strategy, considering four possible surveys made respectively with sets based on MAJ4
                        , MAJ3
                        , MAJ2
                         are reported in Table 6
                         for rs
                        :=
                        dev with details of coverage, total accuracy (acc
                        1), partial accuracy (acc
                        2), precision, total recall, and F-measure.

An analysis of the errors in the dev set shows that many of them can be clustered and annotated for further processing. This possibility was investigated leading to an augmentation of the decision tree as described in the next subsection.

The errors observed with the application of the initial strategy are due to limits of the classifiers, the types of features used, out of vocabulary words (oov), the errors in the 1-best sequences of words hypotheses generated by the asr system and the simplicity of the strategy itself.

It appears reasonable and interesting to refine the strategy with the purpose of recovering some of the above mentioned errors. The recovery strategy adds nodes and branches to the initial decision strategy tree and executes additional decision processes specific for each of the consensus sets. Each process is conceived after a human analysis of error types in each of the above sets in the train and dev sets. For the sake of clarity, in the following a conversation will be indicated by y
                        
                           i
                         and a theme will be indicated by adding an index to the variable t.

The considered types of possible errors are listed below.
                           
                              •
                              DT: the deletion of a theme t
                                 
                                    b
                                  in a multiple theme conversation manually annotated with t
                                 
                                    a
                                 
                                 t
                                 
                                    b
                                  for which only t
                                 
                                    a
                                  has been automatically annotated. Notice that the order of theme hypotheses is not relevant for the application task even if it could be inferred using the results of density and hmm.

ST: the substitution of a theme t
                                 
                                    c
                                  with a theme t
                                 
                                    d
                                 .

IT: the insertion of a theme t
                                 
                                    i
                                  in a conversation manually annotated with t
                                 
                                    a
                                  and automatically annotated with t
                                 
                                    a
                                 
                                 t
                                 
                                    i
                                 .

The observation of the errors in the train and dev sets shows that most of them are of the type DT, and the errors of the type ST and IT appear in a limited number of specific conditions, the most frequent of which concerns annotations with trash (trsh), a class that is difficult to characterize because of the variability of its semantic content.

The errors in the DT cases automatically annotated with trsh appear to be mostly due to asr errors or to words such as street names and lost objects whose mention is not present in the asr vocabulary. Other frequent errors are due to the detection of a fare without an indication to what item it applies to. This is explained by the failure to hypothesize a type of transportation card, whose mention is present in the lattice of word hypotheses but not in the 1-best sequence.

The error analysis of the train and dev sets reveals that the just mentioned types of errors appear for theme hypotheses specific to each consensus set. It is thus reasonable to perform recovery with a small set of precondition-action rules in which a precondition is a logical expression of hypotheses generated by the four systems and appearing in a specific consensus set. The consequent action consists in performing insertions and/or deletions of theme hypotheses if the modifications are supported by the evidence of new, theme specific features evaluated with the lattice of word hypotheses. Furthermore, the actions must be coherent with the application ontology and the precondition content.

The introduction of a specific set of features for each theme is an important novelty of the proposed approach. These thematic feature sets have a limited size and are obtained automatically from the lattices of all the train samples. Features for each theme are selected based on their purity in the train set. Features are words, word classes and bigrams with a variable distance between the mention of their constituents in a time window that may involve up to three conversation turns. These distance bigrams appear to have high purity expressing distance relations between facts and actions that characterize each theme.

In this way, it is expected that a theme hypothesis receives a high score when the features characterizing it have high evidence in contrast with features characterizing other themes. Features are scored with their contribution to reduction of equivocation (re) when they are used to hypothesize the theme they belong to.

Relevant facts and complementary information pertinent for each theme are concepts listed in Table 7
                        . The expression of these concepts is assumed to be made of words and (distant) bigrams.

The expressions of each concept are listed in a record of features associated with the concept. Concepts such as time and price are decomposed into constituents. Each constituent is represented by a word or a bigram feature.

In some cases the detection of only part of the constituents of a concept may be useful for recovering a deletion. In other cases it may introduce a false alarm. For this reason, the use of lattice features has been constrained with specific preconditions expressing the detection of a significant but incomplete amount of theme features.

Other types of features could be considered but have not been used. For example, prosodic features may contribute to characterize complaints or requests of indulgence. The main reason is that it is difficult to extract reliable prosodic features in the customer turns because the acoustic environments may be affected by a large variety of noise types and intensity.

In order to introduce the form of preconditions let us define:
                           
                              •
                              
                                 H
                                 
                                    c
                                 (i, cs): the automatic annotation by system cosine of conversation y
                                 
                                    i
                                  in the consensus set cs.


                                 H
                                 
                                    d
                                 (i, cs): the automatic annotation by system density of conversation y
                                 
                                    i
                                  in the consensus set cs.


                                 H
                                 
                                    h
                                 (i, cs): the automatic annotation by system hmm of conversation y
                                 
                                    i
                                  in the consensus set cs.


                                 H
                                 
                                    P
                                 (i, cs): the automatic annotation by system poisson of conversation y
                                 
                                    i
                                  in the consensus set cs.

Let (mH
                        →
                        A), (m
                        =2, 3, 4) represent the fact that m and only m annotations of the four systems are equal to A. For example, let assume that, for a conversation in the dev, it has been observed H
                        
                           d
                        (i, cs)=
                        H
                        
                           h
                        (i, cs)=
                        H
                        
                           P
                        (i, cs)=
                        objt (lost_and_found theme) and H
                        
                           c
                        (i, cs)=
                        objt
                        _
                        nvgo (lost a nvgo transportation card).

This situation is generalized leading to the following precondition representation:


                        
                           
                              
                                 P
                                 3
                                 (
                                 L
                                 ,
                                 Q
                                 )
                                 :
                                 (
                                 3
                                 H
                                 →
                                 L
                                 )
                                 ∧
                                 (
                                 1
                                 H
                                 →
                                 LQ
                                 )
                              
                           
                        
                     

Five general forms have been introduced, one for YRQ
                        1 (see Section 6.1), one for YRQ
                        2 (see Appendix C) and three for NRQ
                        2 (see C).

A general form can be specialized by binding the variables L and Q to specific values as in the previous example:


                        
                           
                              
                                 L
                                 =
                                 
                                    objt
                                 
                                 
                                 and
                                 
                                 Q
                                 =
                                 
                                    objt
                                 
                                 _
                                 
                                    nvgo
                                 
                              
                           
                        
                     

Specialized preconditions have been manually derived from the semantic coherence relations introduced later on after the actions.

An action is introduced as consequence of each specialized precondition.

An action AC
                        
                           j(n) asserting an annotation modification H
                        
                           i
                        
                        j(n) of a theme annotation for conversation y
                        
                           i
                         includes two processes, namely:
                           
                              •
                              assert a semantically coherent modification H
                                 
                                    i
                                 
                                 j(n),

evaluate the evidence of H
                                 
                                    i
                                 
                                 j(n).

A precondition-action rule is written as follows:


                        
                           
                              
                                 
                                    PC
                                    
                                       j
                                       (
                                       n
                                       )
                                    
                                 
                                 ⇒
                                 
                                    AC
                                    
                                       j
                                       (
                                       n
                                       )
                                    
                                 
                              
                           
                        here j(n) indicates the jth specification of the nth general precondition. Symbol ⇒ links a precondition to its corresponding action.

Action AC
                        
                           j(n) is formally represented as follows:


                        
                           
                              
                                 
                                    AC
                                    
                                       j
                                       (
                                       n
                                       )
                                    
                                 
                                 :
                                 assert
                                 (
                                 H
                                 
                                    j
                                    (
                                    n
                                    )
                                 
                                 )
                                 ∧
                                 evaluate
                                 (
                                 H
                                 
                                    j
                                    (
                                    n
                                    )
                                 
                                 )
                              
                           
                        
                     

The process assert(Hj(n)) generates hypotheses coherent with the application ontology according to the content of a look-up table corresponding to precondition j(n). The application ontology has been inferred from the application documentation and the look-up table has been compiled based on the analysis of the errors of the train set. Both activities have been performed by a human expert. Common to many application domains is the use semantic knowledge containing structures representing a theme with its related facts and some complementary information. These structures are represented by lambda-expressions of this type:


                        
                           
                              (24)
                              
                                 (
                                 λ
                                    
                                 (
                                 x
                                    
                                 y
                                    
                                 z
                                 )
                                    
                                 (
                                 theme
                                 (
                                 x
                                 )
                                 ∧
                                 facts
                                 (
                                 x
                                 ,
                                 y
                                 )
                                 ∧
                                 
                                    compl
                                    _
                                    i
                                    
                                       nf
                                       1
                                    
                                 
                                 (
                                 x
                                 ,
                                 y
                                 ,
                                 z
                                 )
                                 )
                                 )
                              
                           
                        
                     

The semantic content of a conversation is obtained by binding the variables x, y, z to specific values. Notice that a value can be another semantic structure with all variables bound to specific values. For example, if x
                        =
                        traffic
                        _
                        state, then y may be bound to the elements of the set ts
                        :={strike, delay, accident, anomaly}. Variable z can be bound by other complementary information such as time and location. In many application domains, there are specific facts for each theme and specific relations between different themes.

Given the type of application in which conversations have to follow a defined protocol, it is reasonable to assume that co-existence of mentions of fact predicates and coherent argument values are sufficient conditions for expressing themes characterized by these predicates.

The conversation protocol implies that only themes with semantically coherent facts may coexist in a conversation. This may not be always the case in practice. Nevertheless, this type of semantic coherence can be used for imposing constraints for recovering hypothesized theme errors. For example, a mention of the theme objt may share the fact that the lost object is a type of nvgo card, thus the two themes may coexist. In order to further constrain the application of the recovery strategy, frequent errors observed in the train and dev sets have been clustered into types. The types of errors listed below have been used in the introduced coherent recovery actions:
                           
                              •
                              
                                 fare in conjunction with itinerary and transportation cards.


                                 time in conjunction with itinerary and traffic_state.


                                 lost_found in conjunction with cards.


                                 itinerary in conjunction with traffic_state, cards, lost_found.


                                 fine in conjunction with cards and loss.

Refinement strategy actions are executed only if specific conditions and the asserted hypotheses (Hj(n)) are verified.

The procedure for hypothesis evaluation is now described.

A feature set 
                           
                              Φ
                              k
                           
                           :
                           =
                           {
                           
                              φ
                              1
                              k
                           
                           ,
                           …
                           ,
                           
                              φ
                              n
                              k
                           
                           ,
                           …
                           ,
                           
                              φ
                              
                                 
                                    N
                                    Φ
                                 
                              
                              k
                           
                           }
                           ,
                              
                           k
                           ∈
                           {
                           1
                           ,
                           …
                           ,
                           K
                           }
                         of size N
                        
                           Φ
                         is defined for each theme t
                        
                           k
                        . Only conversations in the train set annotated with a single theme are considered and used for inferring features of Φ
                        
                           k
                         for each k. The ith conversation of the entire corpus is described by the acoustic features A
                        
                           i
                        . The mutual information between the annotated theme t
                        
                           k
                         and A
                        
                           i
                         is defined as:


                        
                           
                              (25)
                              
                                 I
                                 (
                                 
                                    t
                                    k
                                 
                                 ,
                                 
                                    A
                                    i
                                 
                                 )
                                 =
                                 H
                                 (
                                 
                                    t
                                    k
                                 
                                 )
                                 −
                                 H
                                 (
                                 
                                    t
                                    k
                                 
                                 |
                                 
                                    A
                                    i
                                 
                                 )
                              
                           
                        where H(t
                        
                           k
                        ) is the entropy of theme t
                        
                           k
                         and H(t
                        
                           k
                        |A
                        
                           i
                        ) is the conditional entropy of theme t
                        
                           k
                         computed with features Φ
                        
                           k
                         extracted from A
                        
                           i
                        .

Details on the computation of H(t
                        
                           k
                        |A
                        
                           i
                        ) are given in Appendix D.

The automatic feature selection process starts by considering a word vocabulary V
                        1 obtained from the application vocabulary V (5782 words) after removal of the words of a stop list. A list LB of distant bigrams have been added to V
                        1 by considering the association of a word in V
                        1 with any word in V in a window spanning three turns. The elements of V
                        1, LB and a small set of abstract classes such as time and price are considered as possible features belonging to an initial set Φ
                        0. Set Φ
                        
                           k
                         is formed by computing, for every element φ
                        
                           x
                        
                        ∈
                        Φ
                        0 the purity measured by the probability 
                           
                              
                                 ℙ
                              
                           
                           (
                           
                              t
                              k
                           
                           |
                           
                              φ
                              x
                           
                           )
                        , whose computation is described in Appendix D.

The size of the feature set has been obtained by automatically annotating the conversations in the train set manually annotated with only one theme. Automatic annotation was based on the following decision rule:


                        
                           
                              (26)
                              
                                 
                                    T
                                    ˆ
                                 
                                 =
                                 
                                    
                                       arg
                                       
                                       max
                                    
                                    j
                                 
                                    
                                 H
                                 (
                                 
                                    t
                                    j
                                 
                                 |
                                 
                                    A
                                    i
                                 
                                 )
                              
                           
                        
                     

After ordering by feature purity the feature set of each theme the same size of feature set candidates was considered for all themes. The size was progressively increased from 50 to 1000 by steps of 5. For each step, the accuracy of theme hypothesization was computed in the train set. The process of growing the feature set size stopped when no tangible improvements in the classification accuracy was observed. A value N
                        
                           Φ
                        
                        =420 was found in this way.

Overall, 1440 word features and 1657 distant bigram features were obtained. The highest word purity for each theme varies between 0.6 and 0.91 depending on the theme, while the lowest word purity varies between 0.12 and 0.4.

Even with these features, some combinations of theme hypotheses were not well characterized. For example, it was not possible to have an exhaustive list of lost objects. For this purpose, it was considered useful to infer complete topic mentions. Complete topic mentions were manually compiled by selecting, abstracting and combining into patterns some automatically obtained features. For example, pattern structures were derived for abstractions of mentions of departure, arrival, connection to describe an itinerary. Patterns characterizing actions describing loss of an object without specifying the object type were also compiled into an abstract class. Some minor improvements were observed after the introduction of complete topic mentions features.

As an example of how recovery actions are applied, let us consider the case of recovering the deletion of a theme AA when theme B has been hypothesized. The following decision rule is applied:


                        
                           
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                
                                                   
                                                      
                                                         
                                                            semantically
                                                            _
                                                            c
                                                            oherent
                                                         
                                                      
                                                   
                                                   (
                                                   AA
                                                   ,
                                                   B
                                                   )
                                                
                                                
                                                   ∧
                                                
                                             
                                             
                                                
                                                
                                                   
                                                      
                                                         
                                                            greater
                                                            _
                                                            t
                                                            han
                                                         
                                                      
                                                   
                                                   (
                                                   score
                                                   (
                                                   B
                                                   )
                                                   ,
                                                   TH
                                                   (
                                                   B
                                                   )
                                                   )
                                                
                                                
                                                   ∧
                                                
                                             
                                             
                                                
                                                
                                                   [
                                                   
                                                      
                                                         
                                                            rank
                                                            _
                                                            R
                                                            E
                                                            _
                                                            F
                                                            IRST
                                                         
                                                      
                                                   
                                                   (
                                                   B
                                                   )
                                                      
                                                   ∨
                                                
                                             
                                             
                                                
                                                
                                                      
                                                   (
                                                   
                                                      
                                                         
                                                            rank
                                                            _
                                                            R
                                                            E
                                                            _
                                                            F
                                                            IRST
                                                         
                                                      
                                                   
                                                   (
                                                   AA
                                                   )
                                                      
                                                   ∧
                                                
                                             
                                             
                                                
                                                
                                                      
                                                      
                                                   
                                                      
                                                         
                                                            rank
                                                            _
                                                            R
                                                            E
                                                            _
                                                            S
                                                            ECOND
                                                            (
                                                            B
                                                            )
                                                         
                                                      
                                                   
                                                   )
                                                   ]
                                                
                                             
                                             
                                                
                                                
                                             
                                          
                                       
                                    
                                 
                                 ⇒
                                 assert
                                 (
                                 AA
                                 _
                                 B
                                 )
                              
                           
                        
                     

Scores are computed with the reduction of equivocation (re) process. These scores are obtained with features of a conversation whose mention is omitted in the above relation for the sake of simplicity. TH(B) is a threshold evaluated from the dev set to optimize separation between positive and negative examples. With a larger dev set, TH(B) could be estimated as the value that minimizes the relative entropy between manually and automatically annotated distributions of theme proportions. This threshold is applied to all consensus sets. Predicates rank_RE_FIRST(B) and rank_RE_SECOND(B) are evaluated to true when theme B is respectively scored first or second by the re process. Decision of removing false insertion themes is applied in specific conditions when the theme position in the re ranking is not among the first three.

For specific conditions, these types of decisions were also considered:
                           
                              •
                              composition of themes hypothesized by different systems,

recovery of a theme from trsh.

Composition decisions were made based on positions and scores of the themes to be combined in the following situations:
                           
                              •
                              highest re score for single insertion or trsh recovery,

first two positions and co-presence in the hypotheses for the composition of two themes,

composition of three themes in case of co-presence in different system hypotheses of only three semantically coherent themes and high re scores for them.

Based on the frequency of the observed errors in the dev and train, recovery for data in set(MAJ4
                        ) was considered only for the following semantic relations:
                           
                              •
                              
                                 B
                                 =
                                 fare in conjunction with AA
                                 =
                                 nvgo (transportation card),


                                 B
                                 =
                                 time
                                 or
                                 B
                                 =
                                 nvgo in conjunction with AA
                                 =
                                 itinerary,

recovery of B
                                 =
                                 fine
                                 from
                                 AA
                                 =
                                 trash.

The same cases as for set(MAJ4
                        ) were considered for YRQ
                        2 with the addition of
                           
                              •
                              
                                 B
                                 =
                                 traffic
                                 _
                                 state in conjunction with AA
                                 =
                                 itinerary,


                                 B
                                 =
                                 lost in conjunction with AA
                                 =
                                 nvgo,

recovery of B
                                 =
                                 vgc
                                 cardor
                                 B
                                 =
                                 traffic
                                 _
                                 state
                                 from
                                 AA
                                 =
                                 trash.

Recovery in cs
                        3 (subset defined in Appendix C) is more difficult since possible major problems may be caused by errors in the 1-best sequence or by limits of some of the four systems. When the hypotheses generated by the four systems are all different it is likely that the 1-best sequence contains too many errors. In both cases re ranking and scores are used for recovering deletions and also for obtaining a new majority vote with the addition of the re ranking.

The results obtained with the initial strategy on the test set are reported in Table 8
                         and those obtained with the addition of the recovery strategy are reported in Table 9
                        .

Results show a statistically significant contribution of the recovery strategy on acc
                        1 for high coverage with respect to the use of just the initial consensus strategy.

The numbers of concept instances annotated in the manual transcriptions (indicated in the following as man), and detected from the word hypotheses generated by the asr (most likely sequence and lattice) for the dev (8646 turns – 196 dialogues) and the test (27,497 turns – 578 dialogues) sets are reported in Table 10
                        .

The increase of relevant concepts detected in the lattice with respect to the one best sequence is significant. This is done at the expenses of 30% insertions that have little impact on theme recovery since most of them are filtered out by the selective recovery procedure.

By observing the concept mentions in the manually annotated concepts it appears that errors in automatic concept hypothesization are essentially due to asr errors.

Published experimental results on the decoda corpus refer to the identification of the dominant theme. Among them, the highest accuracy of 85% is reported in Morchid et al. (2014).

This value has been obtained with a subset of the corpus used in this paper annotated with a single theme label and a large number of lda feature sets extracted in hidden spaces of different size. This single theme is annotated with one of 8 theme labels. This label set is obtained from the one used in the experiments of this paper by removing trsh and vgc. Such a classification is useful in practice if out domain conversations are discarded by another method and proportions are evaluated for problems of transportation cards ignoring the card type.

Considering that the confidence interval of the results is 0.03, the accuracy acc
                        1 reported in this paper for multiple theme labeling is slightly inferior to the best accuracy reported for just one theme labeling with a smaller set of labels and a smaller corpus. It is worth mentioning that the results reported in this paper for multiple theme are superior or inside the confidence interval when compared with results reported in Morchid et al. (2014) for the identification of one of the above mentioned 8 theme labels obtained with Gaussian and svm classifiers using different types of features including lda hidden topic features computed in a single hidden space.

The errors observed in the test set of YRQ
                        1 using the initial strategy were 45 in 321 conversations. Among them, 6 deletions were corrected to X
                        _
                        Y when X was hypothesized. Using only the complete mentions, 4 more errors of the same type were corrected while 1 false insertion was generated. Among the remaining errors, there were 15 deletion errors of Y with X correct and two insertion errors with X correct. The other errors where 9 substitutions involving trash indicating difficulties in characterizing this type of rejection and the others were confusions between itinerary and traffic_state and between lost_and_found and cards. Listening to these conversations, it appeared that the two themes were mentioned, but the annotators considered one of them not sufficiently relevant.

The errors observed in the test set of YRQ
                        2 were 44 in 131 conversations. Among them, 10 deletions were corrected to X
                        _
                        Y when X was hypothesized. Using only the complete mentions 5 more errors of the same type were corrected.

The errors observed in the test set of YRQ
                        3 were 71 in 117 conversations. Among them, no false recoveries were observed, 5 deletions were corrected to X
                        _
                        Y when X was hypothesized, 6 compositions were correctly executed by merging hypotheses from different systems, 7 recoveries from trash were correctly performed. A minor number of corrections for YRQ
                        3 were made using the additional contribution of re ranking and complete mentions.

For YRQ
                        2 and YRQ
                        3, corrected and remaining errors were of the same nature as those in YRQ
                        1.

A large majority of the remaining errors are deletions of a theme in a conversation in which at least one of the annotated themes was correctly hypothesized. By listening to the conversations annotated with errors it appears that, in many of them, the annotators have made an intelligent choice of what was relevant, while the system ignored part of it or inserted themes that were mentioned, but were not relevant. Based on the analysis it is reasonable to conclude that, with the approach described in Camelin et al. (2009), significant theme proportion variations can be detected in a time period only a little longer than if the manual annotations were used.

Four systems for multiple theme hypothesization in human/human call center conversations have been considered. Two of them, cosine and poisson extract features for theme mentions in an entire conversation. Other two systems density and hmm attempt to localize theme mentions in specific zones of a conversation. A comparison of the results separately obtained with each system show little performance difference, while a study on the consensus among the results of the four systems show that there is a non-negligible proportion of conversations for which the four systems do not completely agree. Nevertheless, in such a case it is still possible to perform reliable decisions on specific conversation sets by considering majority votes. In making these decisions all four systems appear to be useful indicating that their combined use contribute to effectively process diffuse as well as localized theme mentions.

For a significant proportion of those cases for which reliable decisions cannot be made with majority vote strategy it is still possible to trigger a new process with new features for obtaining a high coverage survey with accurately annotated themes. The overall result is that the proposed approach makes it possible to produce practically useful theme proportions in spite of asr errors and the imprecision of classification methods.

The recovery strategy has some precondition-action rules that depend on the application ontology and other that are applicable to a large variety of application domains. This latter set contains rules for recovering false classification as trsh due to concept deletion errors and false insertions of concepts leading to the hypothesization of a domain theme rather than classifying the conversation as trsh. Another general case is the deletion of concepts whose mentions are short words that are likely to be deleted in the 1-best sequence of word hypotheses. These short words are often confused by inserting or deleting one short phoneme such as a plosive consonant. Specific recovery rules can then be conceived by observing a limited number of examples in the train set and using them for setting appropriate contexts for searching instances in the lattice of word hypotheses.

Future work should attempt to provide accurate estimations of the amount of time periods in which corpora have to be collected in order to extract practically useful surveys.

More detailed semantic information has also to be extracted under the control of the application domain ontology. New methods have to be introduced for estimating the amount and degree of user problem solution and for composing all the extracted semantic contents to produce conversation reports. These reports are not necessarily summaries. They should rather be semantic structures of strictly relevant information for the application task.

@&#ACKNOWLEDGEMENTS@&#

This work has been supported by the French National Research Agency (anr) with Project decoda, contract anr-09-coord-005, and the French cluster scs (Secured Communicating Solutions). The corpus used for the experiments has been provided by the ratp (Paris public transportation system).

According to Eq. (7), the density for theme t at the ith position of a conversation is evaluated as follows:


                        
                           
                              (A.1)
                              
                                 
                                    d
                                    t
                                 
                                 (
                                 
                                    p
                                    i
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                          
                                             j
                                             =
                                             1
                                          
                                          n
                                       
                                       
                                          (
                                          
                                             w
                                             t
                                          
                                          (
                                          
                                             p
                                             j
                                          
                                          )
                                          )
                                       
                                       /
                                       
                                          (
                                          
                                             λ
                                             
                                                
                                                   d
                                                   j
                                                
                                             
                                          
                                          )
                                       
                                    
                                    
                                       
                                          ∑
                                          
                                             j
                                             =
                                             1
                                          
                                          n
                                       
                                       1
                                       /
                                       
                                          (
                                          
                                             λ
                                             
                                                
                                                   d
                                                   j
                                                
                                             
                                          
                                          )
                                       
                                    
                                 
                                 
                                 (
                                 i
                                 =
                                 1
                                 ,
                                 …
                                 ,
                                 n
                                 )
                              
                           
                        where 
                           
                              w
                              t
                           
                           (
                           
                              p
                              j
                           
                           )
                         denotes the contribution to theme t of the features located at the jth position; λ
                        ⩾1 is a parameter of sensitivity to proximity and d
                        
                           j
                        
                        :=|i
                        −
                        j| is the distance between positions p
                        
                           i
                         and p
                        
                           j
                        .

The density d
                        
                           t
                        (p
                        
                           i
                        ) for theme t at the ith position can first be expressed as follows:


                        
                           
                              (A.2)
                              
                                 
                                    d
                                    t
                                 
                                 (
                                 
                                    p
                                    i
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                          
                                             j
                                             =
                                             1
                                          
                                          n
                                       
                                       
                                          (
                                          
                                             w
                                             t
                                          
                                          (
                                          
                                             p
                                             j
                                          
                                          )
                                          )
                                       
                                       /
                                       
                                          (
                                          
                                             λ
                                             
                                                
                                                   d
                                                   j
                                                
                                             
                                          
                                          )
                                       
                                    
                                    
                                       
                                          ∑
                                          
                                             j
                                             =
                                             1
                                          
                                          n
                                       
                                       1
                                       /
                                       
                                          
                                             λ
                                             
                                                
                                                   d
                                                   j
                                                
                                             
                                          
                                       
                                    
                                 
                                 =
                                 
                                    
                                       
                                          ∑
                                          
                                             j
                                             =
                                             1
                                          
                                          
                                             i
                                             −
                                             1
                                          
                                       
                                       
                                          (
                                          
                                             w
                                             t
                                          
                                          (
                                          
                                             p
                                             j
                                          
                                          )
                                          )
                                       
                                       /
                                       
                                          (
                                          
                                             λ
                                             
                                                i
                                                −
                                                j
                                             
                                          
                                          )
                                       
                                       +
                                       
                                          w
                                          t
                                       
                                       (
                                       
                                          p
                                          i
                                       
                                       )
                                       +
                                       
                                          ∑
                                          
                                             j
                                             =
                                             i
                                             +
                                             1
                                          
                                          n
                                       
                                       
                                          (
                                          
                                             w
                                             t
                                          
                                          (
                                          
                                             p
                                             j
                                          
                                          )
                                          )
                                       
                                       /
                                       
                                          (
                                          
                                             λ
                                             
                                                j
                                                −
                                                i
                                             
                                          
                                          )
                                       
                                    
                                    
                                       
                                          ∑
                                          
                                             j
                                             =
                                             1
                                          
                                          
                                             i
                                             −
                                             1
                                          
                                       
                                       1
                                       /
                                       
                                          (
                                          
                                             λ
                                             
                                                i
                                                −
                                                j
                                             
                                          
                                          )
                                       
                                       +
                                       1
                                       +
                                       
                                          ∑
                                          
                                             j
                                             =
                                             i
                                             +
                                             1
                                          
                                          n
                                       
                                       1
                                       /
                                       
                                          (
                                          
                                             λ
                                             
                                                j
                                                −
                                                i
                                             
                                          
                                          )
                                       
                                    
                                 
                              
                           
                        
                     

With reference to the ith position, let 
                           L
                           (
                           
                              p
                              i
                           
                           )
                           =
                           
                              ∑
                              
                                 j
                                 =
                                 1
                              
                              
                                 i
                                 −
                                 1
                              
                           
                           
                              (
                              
                                 w
                                 t
                              
                              (
                              
                                 p
                                 j
                              
                              )
                              )
                           
                           /
                           
                              (
                              
                                 λ
                                 
                                    i
                                    −
                                    j
                                 
                              
                              )
                           
                         be the weighted sum of the left-hand side thematic contributions and 
                           R
                           (
                           
                              p
                              i
                           
                           )
                           =
                           
                              ∑
                              
                                 j
                                 =
                                 i
                                 +
                                 1
                              
                              n
                           
                           
                              (
                              
                                 w
                                 t
                              
                              (
                              
                                 p
                                 j
                              
                              )
                              )
                           
                           /
                           
                              (
                              
                                 λ
                                 
                                    j
                                    −
                                    i
                                 
                              
                              )
                           
                         be the weighted sum of the right-hand side thematic contributions. Let 
                           NL
                           (
                           
                              p
                              i
                           
                           )
                           =
                           
                              ∑
                              
                                 j
                                 =
                                 1
                              
                              
                                 i
                                 −
                                 1
                              
                           
                           1
                           /
                           
                              (
                              
                                 λ
                                 
                                    i
                                    −
                                    j
                                 
                              
                              )
                           
                         be the left-hand side summation of the normalization factors and let 
                           NR
                           (
                           
                              p
                              i
                           
                           )
                           =
                           
                              ∑
                              
                                 j
                                 =
                                 i
                                 +
                                 1
                              
                              n
                           
                           1
                           /
                           
                              (
                              
                                 λ
                                 
                                    j
                                    −
                                    i
                                 
                              
                              )
                           
                         be the corresponding right-hand side summation. Introducing these notations in Eq. (A.2), one gets for i
                        =1, …, n:


                        
                           
                              (A.3)
                              
                                 
                                    d
                                    t
                                 
                                 (
                                 
                                    p
                                    i
                                 
                                 )
                                 =
                                 
                                    
                                       L
                                       (
                                       
                                          p
                                          i
                                       
                                       )
                                       +
                                       
                                          w
                                          t
                                       
                                       (
                                       
                                          p
                                          i
                                       
                                       )
                                       +
                                       R
                                       (
                                       
                                          p
                                          i
                                       
                                       )
                                    
                                    
                                       NL
                                       (
                                       
                                          p
                                          i
                                       
                                       )
                                       +
                                       1
                                       +
                                       NR
                                       (
                                       
                                          p
                                          i
                                       
                                       )
                                    
                                 
                                 =
                                 
                                    
                                       L
                                       (
                                       
                                          p
                                          i
                                       
                                       )
                                       +
                                       
                                          w
                                          t
                                       
                                       (
                                       
                                          p
                                          i
                                       
                                       )
                                       +
                                       R
                                       (
                                       
                                          p
                                          i
                                       
                                       )
                                    
                                    
                                       NL
                                       (
                                       
                                          p
                                          i
                                       
                                       )
                                       +
                                       1
                                       +
                                       NL
                                       (
                                       
                                          p
                                          
                                             n
                                             −
                                             i
                                             +
                                             1
                                          
                                       
                                       )
                                    
                                 
                              
                           
                        
                     

The quantities L(p
                        
                           i
                        ), R(p
                        
                           i
                        ), and
                        NL(p
                        
                           i
                        ) can be computed as follows by simple recurrence on the neighbor positions:


                        Initialization:
                     


                        
                           
                              (A.4)
                              
                                 L
                                 (
                                 
                                    p
                                    1
                                 
                                 )
                                 =
                                 R
                                 (
                                 
                                    p
                                    n
                                 
                                 )
                                 =
                                 NL
                                 (
                                 
                                    p
                                    1
                                 
                                 )
                                 =
                                 0
                              
                           
                        
                     


                        Recurrence:
                     


                        
                           
                              (A.5)
                              
                                 L
                                 (
                                 
                                    p
                                    i
                                 
                                 )
                                 =
                                 
                                    ∑
                                    
                                       j
                                       =
                                       1
                                    
                                    
                                       i
                                       −
                                       1
                                    
                                 
                                 
                                    
                                       
                                          w
                                          t
                                       
                                       (
                                       
                                          p
                                          j
                                       
                                       )
                                    
                                    
                                       
                                          λ
                                          
                                             i
                                             −
                                             j
                                          
                                       
                                    
                                 
                                 =
                                 
                                    ∑
                                    
                                       j
                                       =
                                       1
                                    
                                    
                                       i
                                       −
                                       2
                                    
                                 
                                 
                                    
                                       
                                          w
                                          t
                                       
                                       (
                                       
                                          p
                                          j
                                       
                                       )
                                    
                                    
                                       
                                          λ
                                          
                                             i
                                             −
                                             j
                                          
                                       
                                    
                                 
                                 +
                                 
                                    
                                       
                                          w
                                          t
                                       
                                       (
                                       
                                          p
                                          
                                             i
                                             −
                                             1
                                          
                                       
                                       )
                                    
                                    
                                       
                                          λ
                                          
                                             i
                                             −
                                             (
                                             i
                                             −
                                             1
                                             )
                                          
                                       
                                    
                                 
                                 =
                                 
                                    ∑
                                    
                                       j
                                       =
                                       1
                                    
                                    
                                       i
                                       −
                                       2
                                    
                                 
                                 
                                    
                                       
                                          w
                                          t
                                       
                                       (
                                       
                                          p
                                          j
                                       
                                       )
                                    
                                    
                                       
                                          λ
                                          
                                             i
                                             −
                                             1
                                             +
                                             1
                                             −
                                             j
                                          
                                       
                                    
                                 
                                 +
                                 
                                    
                                       
                                          w
                                          t
                                       
                                       (
                                       
                                          p
                                          
                                             i
                                             −
                                             1
                                          
                                       
                                       )
                                    
                                    λ
                                 
                                 =
                                 
                                    ∑
                                    
                                       j
                                       =
                                       1
                                    
                                    
                                       (
                                       i
                                       −
                                       1
                                       )
                                       −
                                       1
                                    
                                 
                                 
                                    
                                       
                                          w
                                          t
                                       
                                       (
                                       
                                          p
                                          j
                                       
                                       )
                                    
                                    
                                       
                                          λ
                                          
                                             (
                                             i
                                             −
                                             1
                                             )
                                             −
                                             j
                                          
                                       
                                       λ
                                    
                                 
                                 +
                                 
                                    
                                       
                                          w
                                          t
                                       
                                       (
                                       
                                          p
                                          
                                             i
                                             −
                                             1
                                          
                                       
                                       )
                                    
                                    λ
                                 
                                 =
                                 (
                                 L
                                 (
                                 
                                    p
                                    
                                       i
                                       −
                                       1
                                    
                                 
                                 )
                                 +
                                 
                                    w
                                    t
                                 
                                 (
                                 
                                    p
                                    
                                       i
                                       −
                                       1
                                    
                                 
                                 )
                                 )
                                 
                                    1
                                    λ
                                 
                                 
                                 (
                                 with
                                    
                                 i
                                 =
                                 2
                                 ,
                                 …
                                 ,
                                 n
                                 )
                              
                           
                        
                        
                           
                              (A.6)
                              
                                 R
                                 (
                                 
                                    p
                                    i
                                 
                                 )
                                 =
                                 
                                    ∑
                                    
                                       j
                                       =
                                       i
                                       +
                                       1
                                    
                                    n
                                 
                                 
                                    
                                       
                                          w
                                          t
                                       
                                       (
                                       
                                          p
                                          j
                                       
                                       )
                                    
                                    
                                       
                                          λ
                                          
                                             j
                                             −
                                             i
                                          
                                       
                                    
                                 
                                 =
                                 
                                    
                                       
                                          w
                                          t
                                       
                                       (
                                       
                                          p
                                          
                                             i
                                             +
                                             1
                                          
                                       
                                       )
                                    
                                    
                                       
                                          λ
                                          
                                             (
                                             i
                                             +
                                             1
                                             )
                                             −
                                             i
                                          
                                       
                                    
                                 
                                 +
                                 
                                    ∑
                                    
                                       j
                                       =
                                       i
                                       +
                                       2
                                    
                                    n
                                 
                                 
                                    
                                       
                                          w
                                          t
                                       
                                       (
                                       
                                          p
                                          j
                                       
                                       )
                                    
                                    
                                       
                                          λ
                                          
                                             j
                                             −
                                             i
                                          
                                       
                                    
                                 
                                 =
                                 
                                    
                                       
                                          w
                                          t
                                       
                                       (
                                       
                                          p
                                          
                                             i
                                             +
                                             1
                                          
                                       
                                       )
                                    
                                    λ
                                 
                                 +
                                 
                                    ∑
                                    
                                       j
                                       =
                                       i
                                       +
                                       2
                                    
                                    n
                                 
                                 
                                    
                                       
                                          w
                                          t
                                       
                                       (
                                       
                                          p
                                          j
                                       
                                       )
                                    
                                    
                                       
                                          λ
                                          
                                             j
                                             −
                                             1
                                             +
                                             1
                                             −
                                             i
                                          
                                       
                                    
                                 
                                 =
                                 
                                    
                                       
                                          w
                                          t
                                       
                                       (
                                       
                                          p
                                          
                                             i
                                             +
                                             1
                                          
                                       
                                       )
                                    
                                    λ
                                 
                                 +
                                 
                                    ∑
                                    
                                       j
                                       =
                                       (
                                       i
                                       +
                                       1
                                       )
                                       +
                                       1
                                    
                                    n
                                 
                                 
                                    
                                       
                                          w
                                          t
                                       
                                       (
                                       
                                          p
                                          j
                                       
                                       )
                                    
                                    
                                       
                                          λ
                                          
                                             j
                                             −
                                             (
                                             i
                                             +
                                             1
                                             )
                                          
                                       
                                       λ
                                    
                                 
                                 =
                                 
                                    1
                                    λ
                                 
                                 (
                                 
                                    w
                                    t
                                 
                                 (
                                 
                                    p
                                    
                                       i
                                       +
                                       1
                                    
                                 
                                 )
                                 +
                                 R
                                 (
                                 
                                    p
                                    
                                       i
                                       +
                                       1
                                    
                                 
                                 )
                                 )
                                 
                                 (
                                 with
                                    
                                 i
                                 =
                                 (
                                 n
                                 −
                                 1
                                 )
                                 ,
                                 …
                                 ,
                                 1
                                 )
                              
                           
                        
                     

In a similar way, one gets:


                        
                           
                              (A.7)
                              
                                 NL
                                 (
                                 
                                    p
                                    i
                                 
                                 )
                                 =
                                 (
                                 NL
                                 (
                                 
                                    p
                                    
                                       i
                                       −
                                       1
                                    
                                 
                                 )
                                 +
                                 1
                                 )
                                 
                                    1
                                    λ
                                 
                                 
                                 (
                                 with
                                 
                                 i
                                 =
                                 2
                                 ,
                                 …
                                 ,
                                 n
                                 )
                              
                           
                        
                     

The use of these recurrences for computing the thematic density is shown in the description of Algorithm 1.


                        
                           Algorithm 1
                           
                              Theme density computation
                           


                              
                                 
                                    
                                       
                                       
                                       
                                          
                                             
                                                require: 
                                                   t
                                                   ∈
                                                   
                                                      
                                                         T
                                                      
                                                   
                                                   ,
                                                      
                                                   n
                                                   ∈
                                                   
                                                      
                                                         ℕ
                                                      
                                                   
                                                   ,
                                                      
                                                   w
                                                   [
                                                   t
                                                   ,
                                                   i
                                                   ]
                                                   ∈
                                                   
                                                      
                                                         
                                                            ℝ
                                                         
                                                         +
                                                      
                                                   
                                                   ,
                                                   λ
                                                   ∈
                                                   [
                                                   1
                                                   ;
                                                   +
                                                   ∞
                                                   [
                                                
                                             
                                          
                                          
                                             1:
                                             
                                                for all 
                                                
                                                   t
                                                   ∈
                                                   
                                                      
                                                         T
                                                      
                                                   
                                                 
                                                do
                                             
                                          
                                          
                                             2:
                                             
                                                
                                                L[1]←0
                                          
                                          
                                             3:
                                             
                                                
                                                R[n]←0
                                          
                                          
                                             4:
                                             
                                                
                                                NL[1]←0
                                          
                                          
                                             5:
                                             
                                                
                                                for 
                                                i
                                                ←2 to n 
                                                do
                                             
                                          
                                          
                                             6:
                                             
                                                
                                                
                                                   L
                                                   [
                                                   i
                                                   ]
                                                   ←
                                                   (
                                                   L
                                                   [
                                                   i
                                                   −
                                                   1
                                                   ]
                                                   +
                                                   w
                                                   [
                                                   t
                                                   ,
                                                   i
                                                   −
                                                   1
                                                   ]
                                                   )
                                                      
                                                   /
                                                      
                                                   λ
                                                
                                             
                                          
                                          
                                             7:
                                             
                                                
                                                
                                                   R
                                                   [
                                                   n
                                                   −
                                                   i
                                                   +
                                                   1
                                                   ]
                                                   ←
                                                   (
                                                   R
                                                   [
                                                   n
                                                   −
                                                   i
                                                   +
                                                   2
                                                   ]
                                                   +
                                                   w
                                                   [
                                                   t
                                                   ,
                                                   n
                                                   −
                                                   i
                                                   +
                                                   2
                                                   ]
                                                   )
                                                      
                                                   /
                                                      
                                                   λ
                                                
                                             
                                          
                                          
                                             8:
                                             
                                                
                                                NL[i]←(NL[i
                                                −1]+1) / λ
                                             
                                          
                                          
                                             9:
                                             
                                                
                                                end for
                                             
                                          
                                          
                                             10:
                                             
                                                
                                                for 
                                                i
                                                ←1 to n 
                                                do
                                             
                                          
                                          
                                             11:
                                             
                                                
                                                
                                                   dens
                                                   [
                                                   t
                                                   ,
                                                   i
                                                   ]
                                                   ←
                                                   L
                                                   [
                                                   i
                                                   ]
                                                   +
                                                   w
                                                   [
                                                   t
                                                   ,
                                                   i
                                                   ]
                                                   +
                                                   R
                                                   [
                                                   i
                                                   ]
                                                
                                             
                                          
                                          
                                             12:
                                             
                                                
                                                dens[t, i]←
                                                dens[t, i] / (NL[i]+1+
                                                NL[n
                                                −
                                                i
                                                +1])
                                          
                                          
                                             13:
                                             
                                                
                                                end for
                                             
                                          
                                          
                                             14:
                                             
                                                end for
                                             
                                          
                                       
                                    
                                 
                              
                           

Positions p
                        
                           i
                         (i
                        =1, …, n) are represented by their respective indices and the density d
                        
                           t
                        (p
                        
                           i
                        ) of the theme t at position p
                        
                           i
                         is indicated as dens[t, i]. Similarly, the contribution 
                           
                              w
                              t
                           
                           (
                           
                              p
                              i
                           
                           )
                         to the theme t of the features located at position p
                        
                           i
                         is indicated as 
                           w
                           [
                           t
                           ,
                           i
                           ]
                        .

The number of the instructions of Algorithm 1 is given by:


                        
                           
                              (A.8)
                              
                                 |
                                 
                                    
                                       T
                                    
                                 
                                 |
                                 (
                                 3
                                 +
                                 3
                                 (
                                 n
                                 −
                                 1
                                 )
                                 +
                                 2
                                 n
                                 )
                                 =
                                 (
                                 5
                                 |
                                 
                                    
                                       T
                                    
                                 
                                 |
                                 )
                                 n
                              
                           
                        
                     

The time complexity is thus a linear function of the number of conversation positions.

The thematic skeleton of a conversation for three themes is shown in Fig. B.2
                     .

The skeletons for λ
                     =1.04 (Fig. B.2a) and λ
                     =3 (Fig. B.2b) were obtained using a manual word transcription of the conversation, which is about a fine (theme indicated as pv) caused by an identity theft. The customer has been fined because he used his mother's transportation card (theme indicated as nvgo). Two functions are plotted for the annotated themes transportation card and fine and a third function is plotted for the theme fare (indicated as tarf) that is not annotated because not completely mentioned in the conversation and considered to be irrelevant for the task.

The figures have been obtained from a conversation whose an excerpt, as manually transcribed, is reported in the following. The conversation positions corresponding to each turn are in brackets.
                        
                           –
                           
                              Customer [22–116]: I am calling because I had a problem yesterday with my nvgo card (…) I used my mother's card (…) The controller confiscated the card.

(…)


                              Agent [320–338]: We are going to receive your card at the collection center.

(…)


                              Customer [544–630]: How much the fine will be? (…) I have been told I have to pay forty euros (…)


                              Agent [642–656]: Right. It should be forty euros.

Distant contextual relations, corresponding to high values of λ, such as 3 in Fig. B.2b, tend to be neglected. In these cases, incorrect decisions may be caused by isolated features that are not relevant for theme hypothesization. For example, the expressions how much (combien in French) in turn [544–630] and forty euros, repeated twice in turns [544–630] and [642–656], tend to show evidence for fare with a peak of density at position 630, whereas it is just, in this case, part of a request about fine.

Local context, more appropriate for theme mention detection, is appropriately represented by a value of λ
                     =1.04 as in Fig. B.2a. This has the effect of reducing the relevance of the fare hypothesis only supported by the expressions how much and forty euros.

In conclusion, values of λ such as 1.04 correspond to thematic coherence leading to more accurate decisions.

A predicate MAJ3
                     (y, NRQ
                     1) takes value true if there is a consensus among only three systems. According to this value, two new subsets YRQ
                     2 and NRQ
                     2 are obtained.

Considering now a new survey set, larger than YRQ
                     1 and defined as set(MAJ3
                     ):=(YRQ
                     1
                     ∪
                     YRQ
                     2), the following values are found:
                        
                           •
                           size(YRQ
                              2)=43, corr(YRQ
                              2)=31


                              cov(set(MAJ3
                              ))=(126+43)/196=0.86


                              acc
                              1(set(MAJ3
                              ))=(116+31)/(126+43)=0.87

A high accuracy is still obtained with a coverage much greater than that of YRQ
                     1.

With similar motivations, the predicate MAJ2
                     (y, NRQ
                     2), was considered and applied. It is worth considering three possible types of samples for which the predicate is asserted true. They are:
                        
                           •
                           
                              XY
                              3: the automatic annotation of two systems is X and the automatic annotation of the other two systems is Y.


                              Za
                              3: the automatic annotation of two systems is Z and the automatic annotations of the other two systems are respectively Z
                              1 and Z
                              2, both different from Z and such that Z
                              1
                              ≠
                              Z
                              2.


                              N
                              3: the four systems provide four different automatic annotations.

Let cs
                     3
                     ⊆
                     NRQ
                     2 be a subset of NRQ
                     2 defined as follows:


                     
                        
                           
                              
                                 cs
                                 3
                              
                              :
                              =
                              
                                 Za
                                 3
                              
                              ∪
                              
                                 LC
                                 3
                              
                           
                        
                     where LC
                     3 is the set XY
                     3 automatically annotated with the results of the linear combination of the four systems. Let: set(MAJ2
                     ):=(YRQ
                     1
                     ∪
                     YRQ
                     2
                     ∪
                     cs
                     3).

In such a case:
                        
                           •
                           size(cs
                              3)=26, corr(cs
                              3)=11


                              cov(set(MAJ2
                              ))=1, acc
                              1(set(MAJ2
                              ))=0.81

Conditional entropy H(t
                     
                        k
                     |A
                     
                        i
                     ) is computed as follows:


                     
                        
                           (D.1)
                           
                              H
                              (
                              
                                 t
                                 k
                              
                              |
                              
                                 A
                                 i
                              
                              )
                              =
                              −
                              
                                 ∑
                                 
                                    n
                                    =
                                    1
                                 
                                 
                                    
                                       N
                                       Φ
                                    
                                 
                              
                              [
                              
                                 
                                    ℙ
                                 
                              
                              (
                              
                                 t
                                 k
                              
                              |
                              
                                 φ
                                 n
                                 k
                              
                              ,
                              
                                 A
                                 i
                              
                              )
                              ×
                              
                                 
                                    ℙ
                                 
                              
                              (
                              
                                 φ
                                 n
                                 k
                              
                              |
                              
                                 A
                                 i
                              
                              )
                              ]
                              
                                 log
                                 2
                              
                              [
                              
                                 
                                    ℙ
                                 
                              
                              (
                              
                                 t
                                 k
                              
                              |
                              
                                 φ
                                 n
                                 k
                              
                              ,
                              
                                 A
                                 i
                              
                              )
                              ×
                              
                                 
                                    ℙ
                                 
                              
                              (
                              
                                 φ
                                 n
                                 k
                              
                              |
                              
                                 A
                                 i
                              
                              )
                              ]
                           
                        
                     
                     
                        
                           
                              ℙ
                           
                        
                        (
                        
                           t
                           k
                        
                        |
                        
                           φ
                           n
                           k
                        
                        ,
                        
                           A
                           i
                        
                        )
                      is approximated with 
                        
                           
                              ℙ
                           
                        
                        (
                        
                           t
                           k
                        
                        |
                        
                           φ
                           n
                           k
                        
                        )
                     .

This conditional entropy can be seen as a reduction of equivocation (re) in the estimation of t
                     
                        k
                      knowing A
                     
                        i
                     . The same number N
                     
                        Φ
                      of features is used for all themes.

Let 
                        
                           
                              ℙ
                           
                        
                        (
                        
                           t
                           k
                        
                        |
                        
                           φ
                           n
                           k
                        
                        )
                      be represented as 
                        
                           
                              ℙ
                           
                        
                        (
                        
                           t
                           k
                        
                        |
                        
                           φ
                           x
                        
                        )
                      for the sake of simplicity. Probability 
                        
                           
                              ℙ
                           
                        
                        (
                        
                           t
                           k
                        
                        |
                        
                           φ
                           x
                        
                        )
                      is computed as follows:


                     
                        
                           (D.2)
                           
                              
                                 
                                    ℙ
                                 
                              
                              (
                              
                                 t
                                 k
                              
                              |
                              
                                 φ
                                 x
                              
                              )
                              =
                              
                                 
                                    c
                                    (
                                    
                                       φ
                                       n
                                    
                                    ,
                                    
                                       t
                                       k
                                    
                                    )
                                    +
                                    K
                                    
                                       
                                          ℙ
                                       
                                    
                                    (
                                    
                                       t
                                       k
                                    
                                    )
                                 
                                 
                                    
                                       
                                          
                                             ∑
                                             
                                                j
                                                =
                                                1
                                             
                                          
                                       
                                       K
                                    
                                    c
                                    (
                                    
                                       φ
                                       x
                                    
                                    ,
                                    
                                       t
                                       j
                                    
                                    )
                                    +
                                    K
                                 
                              
                           
                        
                     where c(φ
                     
                        x
                     , t
                     
                        k
                     ) is the count of the times φ
                     
                        x
                      appears in all the conversations in the train set manually annotated with t
                     
                        j
                     .

Features are ranked based on their purity in each theme t
                     
                        k
                      and an ordered list L
                     
                        k
                      is compiled for each theme.

The presence of a candidate feature 
                        
                           φ
                           x
                           k
                        
                        ∈
                        
                           L
                           k
                        
                      in a conversation is measured by the probability 
                        
                           
                              ℙ
                           
                        
                        (
                        
                           φ
                           x
                           k
                        
                        |
                        
                           A
                           i
                        
                        )
                     . If the feature is a distant bigram involving a word 
                        
                           w
                           m
                        
                      and its context c
                     
                        m,m−τ
                      the feature posterior probability is computed by the following product:


                     
                        
                           (D.3)
                           
                              
                                 
                                    ℙ
                                 
                              
                              (
                              
                                 φ
                                 x
                                 k
                              
                              |
                              
                                 A
                                 i
                              
                              )
                              =
                              
                                 
                                    ℙ
                                 
                              
                              (
                              
                                 c
                                 
                                    m
                                    ,
                                    m
                                    −
                                    τ
                                 
                              
                              |
                              
                                 A
                                 i
                              
                              )
                              ×
                              
                                 
                                    ℙ
                                 
                              
                              (
                              
                                 w
                                 m
                              
                              |
                              
                                 A
                                 i
                              
                              )
                           
                        
                     
                  

Features are considered as terms in a term frequency-inverse document frequency score defined according to Hazen (2011a) as:


                     
                        
                           (D.4)
                           
                              
                                 γ
                                 
                                    d
                                    ,
                                    k
                                 
                              
                              [
                              idf
                              (
                              
                                 φ
                                 x
                                 k
                              
                              )
                              ]
                              =
                              idf
                              (
                              
                                 φ
                                 x
                                 k
                              
                              )
                              ×
                              
                                 ∑
                                 
                                    g
                                    ∈
                                    
                                       G
                                       n
                                    
                                 
                              
                              
                                 
                                    
                                       ℙ
                                    
                                 
                                 g
                              
                              (
                              
                                 φ
                                 x
                                 k
                              
                              |
                              
                                 A
                                 i
                              
                              )
                              =
                              idf
                              (
                              
                                 φ
                                 x
                                 k
                              
                              )
                              ×
                              
                                 
                                    
                                       ∑
                                       
                                          g
                                          ∈
                                          
                                             G
                                             
                                                x
                                                ,
                                                k
                                             
                                          
                                       
                                    
                                    
                                       λ
                                       g
                                    
                                    (
                                    
                                       φ
                                       x
                                       k
                                    
                                    )
                                 
                                 Z
                              
                           
                        
                     where 
                        Z
                        =
                        
                           
                              
                                 ∑
                                 
                                    j
                                    =
                                    1
                                 
                              
                           
                           K
                        
                        
                           ∑
                           
                              g
                              ∈
                              
                                 G
                                 
                                    u
                                    ,
                                    j
                                 
                              
                           
                        
                        
                           λ
                           g
                        
                        (
                        
                           φ
                           j
                           k
                        
                        )
                      is the sum of the scores of all features competing with 
                        
                           φ
                           x
                           k
                        
                     . G
                     
                        u,j
                      is the set of arcs in the graph labeled with the hypothesis.

@&#REFERENCES@&#

