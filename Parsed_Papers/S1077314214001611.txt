@&#MAIN-TITLE@&#An evaluation of crowd counting methods, features and regression models

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           This paper presents an evaluation of existing crowd counting algorithms.


                        
                        
                           
                           We evaluate 5 datasets: UCSD, PETS 2009, Fudan, Mall, Grand Central.


                        
                        
                           
                           We evaluate holistic, local and histogram features (size, shape, edges, keypoints).


                        
                        
                           
                           GPR outperforms linear, KNN and neural network regression.


                        
                        
                           
                           Multiple local features outperform holistic and histogram based features.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Crowd counting

Holistic features

Local features

Histogram features

Regression

@&#ABSTRACT@&#


               
               
                  Existing crowd counting algorithms rely on holistic, local or histogram based features to capture crowd properties. Regression is then employed to estimate the crowd size. Insufficient testing across multiple datasets has made it difficult to compare and contrast different methodologies. This paper presents an evaluation across multiple datasets to compare holistic, local and histogram based methods, and to compare various image features and regression models. A K-fold cross validation protocol is followed to evaluate the performance across five public datasets: UCSD, PETS 2009, Fudan, Mall and Grand Central datasets. Image features are categorised into five types: size, shape, edges, keypoints and textures. The regression models evaluated are: Gaussian process regression (GPR), linear regression, K nearest neighbours (KNN) and neural networks (NN). The results demonstrate that local features outperform equivalent holistic and histogram based features; optimal performance is observed using all image features except for textures; and that GPR outperforms linear, KNN and NN regression.
               
            

@&#INTRODUCTION@&#

Crowd size estimation is an important task for both operational and security purposes. The distribution of people throughout a public space can be used to gather business intelligence, such as consumer shopping patterns, or to ensure that normal operating conditions are maintained. Overcrowding may be an indicator of congestion, delay or security-related abnormalities such as fighting and rioting.

As closed-circuit television (CCTV) becomes ubiquitous, it grows increasingly difficult for human operators to monitor all of the available data due to the sheer number of cameras installed. For example, there are estimated to be between 1.85 million [40] and 4.2 million [64] CCTV cameras installed in the United Kingdom alone. In most cases, security footage is used to investigate events after they occur, rather than to generate real-time alerts during an evolving situation.

In recent years, researchers have turned to computer vision based surveillance technologies to monitor crowds automatically from CCTV. Existing crowd counting algorithms are predominantly holistic in nature, employing machine learning techniques to perform regression between image features and crowd size [71,24,59,65,45,53,48,83,43,8]. In recent years a number of local systems have also been proposed, although many of these algorithms are detection based and rely on assumptions about camera placement or visibility of human features such as head, face or body parts [51,85,15,90]. Other local approaches divide an image into a number of subregions and perform counting locally [47,5,50,13,22,75]. Histogram based approaches have also been proposed in which local information is accumulated into histogram bins and represented on a holistic level [48,49].

Insufficient testing across multiple datasets has made it difficult to compare and contrast different methodologies. A comprehensive analysis across multiple datasets is required to compare local and holistic methods, and to compare various image features and regression models.

This paper uses a cross validation protocol to evaluate the performance of various methods, features and regression models across five public datasets. Image features are categorised into five types: size, shape, edges, keypoints and textures. The regression models evaluated are: Gaussian process regression (GPR), linear regression, K nearest neighbours (KNN) and neural networks (NN). The following methods are evaluated: holistic (in which features are extracted across an image and regression is performed globally); local (in which foreground segmentation is used to localise groups and to perform feature extraction and regression locally); and a histogram based approach [48].

Our experiments demonstrate that local features outperform equivalent holistic features and histogram based features; best performance is observed using all image features except for textures; and that Gaussian process regression outperforms linear, K-nearest neighbours and neural network regression.

The remainder of this paper is structured as follows: Section 2 presents the literature review; Section 3 introduces the benchmark datasets used in this evaluation; Section 4 describes the system design; Section 5 presents the experimental results of the evaluation; and Section 6 discusses the conclusions of this research.

@&#LITERATURE REVIEW@&#

Crowd counting algorithms are generally categorised into two groups: holistic and local. Holistic approaches use global image features to describe each frame in a video sequence, and a classifier or regression model is used to map between the feature space and the crowd size estimate. Local approaches, by contrast, utilise local image features to detect, track or count pedestrians within local regions of an image. In this case the crowd size is the sum of its parts. An intermediate approach has also been proposed [49,48] which utilises blob size histograms based on local segments and expresses this information on a holistic level.

Section 2.1 describes the holistic approaches; Section 2.2 discusses the intermediate approach; and Section 2.3 describes local approaches. Table 1
                      presents a taxonomy of system components used in this evaluation and Table 2
                      summarises the regression based algorithms discussed in the following literature review.

Holistic crowd counting algorithms use global image features to estimate the size of a crowd. They may also be described as “mapping-based” approaches because they map directly between the feature space and the crowd size estimate. Features used by these systems include textures [59], foreground pixels [24] and edge features [48], amongst others, while the classification and regression strategies have included linear regression [24], neural networks [59,48] and Gaussian process regression [8].

Textural approaches are based on the notion that low density crowds exhibit course textures and high density crowds exhibit fine textures. Rather than estimate the number of people directly, these approaches classify the crowd density using a four or five point scale.

Marana [59,57] proposed the use of grey level cooccurrence matrix (GLCM) based statistics [41] for crowd density estimation. Marana also proposed the Minkowski fractal dimension [60]. Xiaohua [83] proposed the use of the 2D discrete wavelet transform (DWT) as a basis for extracting textural features, while Rahmalan [69] proposed Translation Invariant Orthonormal Chebyshev Moments (TIOCM). Rahmalan’s evaluation observed superior performance of textural features on an afternoon dataset, “because the afternoon data has smaller variation of illumination when compared with morning data”. When morning and afternoon datasets were combined to form a larger mixed set, performance decreased compared to the afternoon dataset alone due to these illumination changes over time. This highlights the principle limitation of textural features: they are sensitive to the scene background, and are thus impractical for real world use as they would need to be re-trained after any significant background change.

Other holistic crowd counting algorithms have utilised features such as foreground pixels and edges. While these features are located at points of interest they are aggregated on a holistic level. Regazzoni [71,72] proposed a number of edge features, such as vertical edges, “for detecting the bodies (i.e. legs and arms)”. More recently, a number of algorithms have attempted to segment the foreground using background modelling techniques. The rationale for this approach is described by Cho [16]:
                           It is clear that a human observer has absolutely no problem in distinguishing a very dense crowd from the background. It is believed that human brain is well trained and would be likely to use the ratio of “crowd area” to “background area” as an estimate for the crowd density. This idea could be applied quantitatively to computer-based density estimation if the image-pixels corresponding to the crowd could be separated from those of the background.
                        
                     

Davies [24] found that the relationship between the number of foreground pixels and the number of people in the scene was approximately linear, as was the case for edge pixels. Cho [17,16,18] also used edge and foreground pixel counts and proposed a fast training algorithm for feedforward neural networks. Huang [45] calculated the percentage of foreground pixels in each sub-region of the image, and these values were used to populate a feature vector which served as inputs to a neural network for regression.

For the purposes of indoor crowd estimation over a short period of time, these approaches were shown to be successful. However, these approaches relied on a static background model, making the system sensitive to lighting changes over longer periods of time, whether sudden or gradual. Adaptive background models such as [79,88,89,27,26,25] are robust against such changes, and have been adopted in more recent crowd counting applications.

The analyses of Davies [24], Cho [17,16,18] and Huang [45] were based on scenes with a relatively high camera angle, in which the effects of perspective were not apparent. When perspective distortion is significant, the total number of foreground pixels is less likely to be a reliable indicator of crowding, because objects in the distance appear smaller and therefore contribute fewer pixels to the foreground mask.

Paragios [65] and Ma [53] introduced the use of quasi-calibration, obtained from the “relative size variation of the projection height and widths of a rigid object as the object translates in depth,” [65] which is modelled as a linear function of the row and column coordinates. A ‘density map’ is calculated using the quasi-calibration, whereby a weight is assigned to each pixel to compensate for the effects of perspective. The weighted sum of pixels in the foreground mask was used to detect excessive crowding above a threshold in [53]. Hou [43,44] utilised a similar approach, using a density map to accumulate a weighted foreground pixel count, and performing regression with a neural network to estimate the crowd size.

Chan [8,6,10,7] proposed a holistic algorithm which extracted a very large number of features from each image in order to account for occlusion and other non-linearities such as segmentation errors. The segmentation is based on dynamic textures [9], yielding two boolean foreground masks: one for motion in each direction. Holistic image features included foreground area, perimeter pixel count, edge orientation histogram and textural features. In total, 30 features are extracted and Gaussian process regression (GPR) and Bayesian Poisson regression (BPR) was used to predict the number of pedestrians walking in each direction.

Dimensionality reduction techniques have been used by some authors. Zhang [84] proposes high dimensionality holistic features followed by dimensionality reduction using principal component analysis and kernel dimension reduction. Tan [80] automatically selects a subset of 129 holistic features and uses semi-supervised elastic net regression (sparse linear model) on this reduced feature set.

In summary, holistic approaches are based on the intuition that a global metric (crowd size) is best estimated from global image properties (holistic features). However, crowd size is difficult to monitor due to the high variation in crowd behaviours, distribution and density. Local and intermediate approaches seek to address this.

An intermediate approach was proposed by Kong [49,48] in which blob size histograms to describe image features on a holistic level. The blob size histogram and edge orientation histogram were used to capture the range of object sizes and their appearance in a scene. With each pixel weighted by its value in a density map, the size of each blob is calculated and used to categorise the blob into a histogram bin (see Section 4.7). The blob size histogram serves to separate the blobs present in an image; it would be expected that noise contributes to the smallest histogram bin, while individual pedestrians and small groups contribute to successively larger bins, for example. The exact nature of the relationship is learned by the regression model, but the use of blob size histogram bins as image features should enable the system to distinguish between groups of people and individuals.

Kong also used the Canny edge detector [4] to extract edge pixels and their angle of orientation. These pixels are masked by the foreground so that edges in the background are ignored. An edge angle histogram is constructed with eight bins between 0° and 180°. The edge orientation histogram “can distinguish edges caused by pedestrians, which are usually vertical, with other scene structures such as noise, shadows and cars” [48]. There is support for this statement in other visual surveillance research. For example, Dalal [23] described the histogram of oriented gradients (HOG) for the explicit purpose of human detection (although their approach is block based and employs local normalisation).

The feature vector used by Kong to represent an image is the concatenation of the blob size histogram and the edge angle histogram (Section 4.7). Both linear regression and neural network regression were used to model the crowd size in their approach. We consider this method in our evaluation as an intermediate approach between holistic and local features.

Local approaches to crowd counting utilise detectors or features which are specific to individuals or groups of people within an image. These groups are independently analysed, so that the total crowd estimate is the sum of its parts. Generally these methods can be categorised as follows:
                           
                              1.
                              
                                 Detection based approaches utilise head, face or human detectors, and/or segmentation algorithms to obtain the approximate location of each individual within the scene. Crowd counting is then performed directly as a subsequent step.


                                 Localisation based methods divide an image into a number of subregions and then apply regression-based counting techniques locally.

In sparse crowds it is appropriate to use individual pedestrian detection [23,34,33]. These approaches are best suited for sparse environments in which the detected object is fully visible. As this paper is concerned with crowded and occluded environments, these methods are not discussed in detail here. A survey of existing pedestrian detection methods can be found in [31,28].

An alternative to pedestrian detection is crowd segmentation. This approach attempts to explain the observed image features by estimating the approximate spatial arrangement of pedestrians in the scene. Zhao [86] suggested that this information can be inferred from the foreground mask, and proposed a method for human segmentation within a model-based Bayesian framework. The human 3D model consisted of four ellipsoids with adjustable parameters. The optimal solution is estimated using the RJMCMC algorithm to traverse the solution space non-exhaustively within regions of high probability. A weakness of this technique is the inability to perform real-time segmentation in crowded situations containing more than 10–15 occupants, due to the high dimensionality of the solution space. The utility of various pose models is also questionable in larger crowds where such information is likely to be occluded from view.

RJMCMC has also been used by other authors to perform crowd segmentation. For example, Ge [35,38,39,37] proposed an example-based approach, by constructing a mixture model of Bernoulli shapes to represent foreground humans from a training dataset; and extended this to a multi-camera framework in [38,36].

Instead of using explicit shape models, Dong [29] utilised an example-based approach in which shape descriptors were used to represent blobs in compact form. A blob’s approximate shape is encoded using Fourier descriptors, discarding high frequency coefficients as these contribute little to the overall blob shape. The training dataset contained groups of pedestrians arranged in various configurations, so that new data can be assessed by interpolation using K nearest neighbours (KNN) regression. Dong’s approach was demonstrated on groups of size 1–6. This approach is limited by the amount of training data available and cannot scale to arbitrarily large crowds. As group size increases, it becomes increasingly difficult to obtain sufficient training data for all of the various pedestrian configurations, and the example-based approach becomes insufficient.

A number of other crowd segmentation approaches have been proposed, using for Example 2D models and expectation maximisation [73,52], however they are not designed to operate in arbitrarily large crowds. Other approaches have sought to use symmetry [14], or tracking [62,68]. However as with the above techniques, these are best suited to sparsely populated environments.

Overhead cameras have also been proposed to simplify the counting problem [77,78,81], however as general purpose CCTV cameras are rarely installed in such a configuration these approach are unlikely to be applicable to the majority of existing installations.

Head detection has been proposed by a number of authors [85,51,66,11,12,90]. These approaches are useful in crowds where the face of each individual is always visible to the camera, although they do not provide a general solution to the crowd counting problem.

The aforementioned approaches are detection based algorithms and are generally based on the assumption of low crowd density or specific camera placement. By contrast, localisation based strategies divide the image into a number of subregions and attempt to count groups within the crowd locally.

Conte [19,20,22,21] proposed moving keypoint clustering to perform group localisation. In this approach, SURF [3] was used to detect keypoints within an image. These points are then masked by optical flow so that stationary points are ignored. The remaining moving points are clustered into groups using the K-means algorithm, from which localised group size estimation is performed. These approaches are limited to moving pedestrians because the keypoints are masked by the optical flow field.

Foreground detection has been used by a number of authors. Celik [5] proposed a blob based algorithm which does not require training. It assumes a direct linear relationship between the number of pixels within a blob segment and the number of people represented by that segment, in order to obtain an estimate for each group. Similarly, Kilambi [46,47] and Fehr [32] modelled a group of pedestrians as an elliptical cylinder, assuming a constant spacing between people within the group. Tracking a large blob over several frames increases the robustness of the group size estimate. However, the application to complex crowds in which blobs regularly split and merge may be challenging. Ryan [75,76] applied regression to each blob in an image to obtain a group count for each segment, so that the total crowd size is the sum of the group estimates. This approach extracts training data from each blob in the training dataset and uses these local annotations to train the regression model rather than the holistic count.

A number of authors have used a grid of subregions, whereby an image is divided into a number of smaller cells and analysed locally, or even on a pixelwise basis. These approaches have been used to detect local abnormalities with binary classifiers [82], to classify discrete density levels [30,55,56,54] or to explicitly count crowds within in each cell [13,50].

Chen [13] used local feature mining to count crowds directly. Features were extracted from equally sized cells in a rectangular grid. Multiple output ridge regression was used to capture both global and local trends in the image. Lempitsky [50] estimated the fractional crowd density at each pixel, so that integrating the density over any region would yield the number of people in that region. Each pixel was represented by a feature vector containing local foreground and gradient information. A linear model was used to obtain the fractional density at each pixel. The linear coefficients were selected based on the MESA distance to minimise the maximum error in any subarray of the training images.

In summary, local approaches subdivide the counting problem and perform detection or regression locally.

This section describes the benchmark datasets used in this evaluation. The majority of crowd counting evaluations have focused on single datasets such as UCSD and PETS 2009 [8,19,76] or private datasets [24,59,48]. The use of limited datasets can result in overfitting due to the lack of varying crowding conditions.

Five benchmark datasets were used to evaluate the performance of crowd counting algorithms and parameters in this study. These are summarised in Tables 3 and 4
                     
                     .

The PETS 2009 database was released prior to the Eleventh IEEE International Workshop on Performance Evaluation of Tracking and Surveillance [1] in order to test a multitude of visual surveillance tasks: object tracking, crowd counting and event recognition. Two sequences were designated for counting the number of people in the image, labelled 13–57 and 13–59, and a region of interest is specified for View 1. Additionally, sparse crowd sequences (12–34 and 12–43) and a very densely crowded sequence (14–06) were selected for this analysis. These sequences capture a good variation in crowd properties at different times. Annotations for these datasets were obtained from Milan [63].

The Fudan dataset was introduced by Tan [80] and contains five sequences each of length 300 frames. Holistic ground truth for each frame is provided with the dataset, and additional local annotations were added manually to train the system. These manual annotations were performed on frames 10:20:290 from each sequence, as indicated in Table 3.

The Grand Central dataset was introduced by Zhou [87] to model the collective behaviour of crowds. The footage is provided in greyscale captured from New York’s Grand Central station. Due to the extremely large size of this crowd, annotation of every frame is not feasible, therefore a sparse subset of frames has been selected over a long period of time (33min) and annotated individually.
                        1
                        These annotations will be made publicly available to other researchers. Please contact the authors for a copy of this data.
                     
                     
                        1
                     
                  

The UCSD pedestrian database was introduced by Chan [8] and contains 2000 annotated frames of pedestrian traffic moving in two directions along a walkway.

The Mall pedestrian database was introduced by Chen [13] and contains 2000 annotated frames of pedestrian traffic moving and stopping inside a cluttered indoor shopping centre.

Collectively, these datasets feature a wide variety of environmental conditions and crowd configurations. Details on the resolution and frame rate of the datasets is provided in Table 4, and example images from each are shown in Fig. 1
                     .

A 5-fold cross validation procedure is used to evaluate crowd counting methodologies. The Fudan dataset lends itself to 5-fold cross validation as it is already divided into five sequences of length 300 frames. The UCSD and Mall datasets are each 2000 frames, which are divided into 
                        
                           5
                           ×
                           400
                        
                      frame sequences. The PETS 2009 dataset contains numerous sequences designed for various challenges (tracking, crowd counting and event detection). Five of these were selected, as described in Table 3. Finally, the Grand Central dataset contains extremely large crowds of up to 245 people. In order to capture different crowd properties over time, the frames are annotated at extremely sparse intervals and then divided into five subsets.

At each fold of the cross validation, one sequence is withheld for testing, while the remaining four sequences are used to train the system. From these four training sequences, a subset of frames is selected to train the system. The training subsets used for each sequence are shown in Table 3.

The predictive performance of a crowd counting system is evaluated using three criteria: mean absolute error (MAE), the mean square error (MSE) and mean relative error (MRE). These metrics are commonly used within the field for evaluating system performance [24,48,8].

According to Regazzoni [71]: “End users accept a mean error of 20% with respect to the real number of people present in a controlled area.” This means that a system should achieve 
                        
                           MRE
                           <
                           20
                           %
                        
                      to meet the minimum accuracy requirements of system operators.

In this paper we evaluate crowd counting algorithms under the following categories:
                        
                           1.
                           
                              Holistic. In this approach, features are extracted across an entire region of interest (ROI) and regression is used to estimate the size of the crowd directly.


                              Local. In this approach, features are extracted from local segments in the image and regression is performed locally to estimate the number of people in each segment. The crowd size is a direct summation of these local estimates.


                              Histograms. In this approach, features are extracted from local segments and accumulated into a blob size histogram as proposed by Kong [48], and this is represented at a holistic level. This approach is considered as an intermediate approach between local and holistic methods.

Detection based approaches are omitted from this evaluation because our datasets include heavily occluded crowds in low resolution images.

Holistic crowd counting algorithms employ regression between global image features and crowd size. By contrast, local approaches divide the image into a set of smaller segments to which regression is applied locally. In this evaluation we use foreground segments as the basis for localisation [75,76,5,47]. This approach is adopted because foreground segmentation localises relevant objects in the scene (groups of people) and does not generate an exceedingly large dataset as might be the case for a grid of cells or pixels [13,82,30,50]. The same foreground detection algorithm is used for the holistic, local and histogram features so that all approaches use the same motion segmentation.

In general, features can be categorised under the following headings:
                        
                           1.
                           
                              Size refers to the magnitude of any interesting segments extracted from an image which are deemed to be relevant, such as the foreground pixel count (Section 4.1).


                              Shape pertains to the orientation and shape descriptors of these areas, segments or objects detected in an image (Section 4.2).


                              Edge refers to the relative change in pixel intensities across an image, and this is typically measured by means of a binary edge detector (Section 4.3).


                              Keypoints include any other points of interest, such as corners, that are detected in an image (Section 4.4).


                              Texture refers to general descriptors of an image such as contrast and homogeneity (Section 4.5).

These features are discussed in subsequent Sections 4.1, 4.2, 4.3, 4.4 and 4.5 and summarised in Table 5
                     . Section 4.6 discusses the regression models used in this evaluation. Section 4.7 describes the histogram features as proposed by Kong [48].

Size refers to the magnitude of any detected regions, such as motion segments, in an image. Davies [24] proposed the use of the foreground pixel count as a measure of the holistic crowd size, while Ma [53] introduced the density map Sto weight each foreground pixel to compensate for perspective. We use the method described by Chan [8] which applies a weight 
                           
                              S
                              (
                              i
                              ,
                              j
                              )
                           
                         to each pixel 
                           
                              (
                              i
                              ,
                              j
                              )
                           
                         based on the relative sizes of reference objects in a scene.

The set of foreground pixels within the region of interest is denoted B, and the weighted area of the foreground is denoted A. This is calculated using the density map, S, as follows:
                           
                              (1)
                              
                                 A
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          (
                                          i
                                          ,
                                          j
                                          )
                                          ∈
                                          B
                                       
                                    
                                 
                                 S
                                 (
                                 i
                                 ,
                                 j
                                 )
                              
                           
                        
                     

This area directly captures the size of the foreground normalised for perspective. In practice, the presence of occlusions will lead to non-linearities in the relationship between crowd size and weighted foreground.

The equivalent local feature is extracted by segmenting the foreground into a set of connected components, which are individually labelled, and enumerated by n. The notation 
                           
                              
                                 
                                    B
                                 
                                 
                                    n
                                 
                              
                           
                         is used to represent the set of pixels which belong to the nth blob. In set terminology, the collection of blobs 
                           
                              {
                              
                                 
                                    B
                                 
                                 
                                    n
                                 
                              
                              }
                           
                         is a partition of the set B. The weighted area of each blob, 
                           
                              
                                 
                                    A
                                 
                                 
                                    n
                                 
                              
                           
                        , is:
                           
                              (2)
                              
                                 
                                    
                                       A
                                    
                                    
                                       n
                                    
                                 
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          (
                                          i
                                          ,
                                          j
                                          )
                                          ∈
                                          
                                             
                                                B
                                             
                                             
                                                n
                                             
                                          
                                       
                                    
                                 
                                 S
                                 (
                                 i
                                 ,
                                 j
                                 )
                              
                           
                        
                     

Note that 
                           
                              A
                              =
                              
                                 
                                    ∑
                                 
                                 
                                    n
                                 
                              
                              
                                 
                                    A
                                 
                                 
                                    n
                                 
                              
                           
                         because the holistic foreground area is the sum of its segmented parts.

Another size feature is perimeter length. The set of perimeter pixels 
                           
                              
                                 
                                    P
                                 
                                 
                                    n
                                 
                              
                           
                         is obtained by tracing along the boundary of the nth blob, and the set of all perimeter pixels in an image is denoted 
                           
                              P
                              =
                              
                                 
                                    ∪
                                 
                                 
                                    n
                                 
                              
                              
                                 
                                    P
                                 
                                 
                                    n
                                 
                              
                           
                        . Perimeter pixels are a one-dimensional feature, and are thus weighted using the square root of the density map S as in [48,8]. The weighted perimeter of the nth blob segment is therefore:
                           
                              (3)
                              
                                 
                                    
                                       L
                                    
                                    
                                       n
                                    
                                 
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          (
                                          i
                                          ,
                                          j
                                          )
                                          ∈
                                          
                                             
                                                P
                                             
                                             
                                                n
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       S
                                       (
                                       i
                                       ,
                                       j
                                       )
                                    
                                 
                              
                           
                        
                     

And the equivalent holistic perimeter length is:
                           
                              (4)
                              
                                 L
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          n
                                       
                                    
                                 
                                 
                                    
                                       L
                                    
                                    
                                       n
                                    
                                 
                              
                           
                        
                     

The perimeter length supplements the area feature to provide a more complete description of crowd size.

Perimeter pixels provide valuable shape information about an object. Aside from the perimeter length, which measures the object size, the orientation of the perimeter pixels also contain important shape information. For example, Dong [29] encoded a blob’s approximate shape using Fourier descriptors and Chan [8,7] used a perimeter orientation histogram.

It is therefore intuitive and computationally efficient to use an orientation histogram with 4 bins, each corresponding to the direction of an adjacent pixel 
                           
                              (
                              0
                              °
                              ,
                              45
                              °
                              ,
                              90
                              °
                              ,
                              135
                              °
                              )
                           
                        . When tracing the perimeter from one boundary pixel to the next, the direction of movement determines which histogram bin receives the pixel’s vote. The vote weight is the square root of the density map, 
                           
                              
                                 
                                    S
                                    (
                                    i
                                    ,
                                    j
                                    )
                                 
                              
                           
                        , as perimeter pixels are a one dimensional feature. Vertical edges in the absence of horizontal features are more likely to indicate individuals in a scene, whereas a combination of many perimeter pixels at all orientations may indicate larger crowds.

The value stored in each histogram bin hconstitutes a feature, and the four shape features are denoted 
                           
                              
                                 
                                    V
                                 
                                 
                                    n
                                 
                              
                              (
                              h
                              )
                           
                        , for 
                           
                              h
                              ∈
                              [
                              0
                              ,
                              3
                              ]
                           
                        . The equivalent holistic features are:
                           
                              (5)
                              
                                 V
                                 (
                                 h
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          n
                                       
                                    
                                 
                                 
                                    
                                       V
                                    
                                    
                                       n
                                    
                                 
                                 (
                                 h
                                 )
                              
                           
                        
                     

This is simply the sum of the local perimeter orientation features, taken at a holistic level.

Edges have been commonly used in crowd counting systems. For example, Kong [48] introduced the use of an edge angle histogram on a holistic scale, while Davies [24], Chan [9] and many others have used the total number of edge pixels on a holistic level, regardless of orientation. The boolean edge detection at each pixel is denoted 
                           
                              D
                              (
                              i
                              ,
                              j
                              )
                              ∈
                              
                                 
                                    
                                       0
                                       ,
                                       1
                                    
                                 
                              
                           
                         with 1 denoting an edge.

In this evaluation, an edge orientation histogram is constructed for each foreground segment in an image using the following procedure. For the nth blob segment, a histogram of edge orientations 
                           
                              
                                 
                                    H
                                 
                                 
                                    n
                                 
                              
                           
                         is constructed by allocating each edge pixel to a histogram channel, based on the pixel’s unsigned orientation 
                           
                              ∠
                              G
                              (
                              i
                              ,
                              j
                              )
                           
                        . The orientation bins are evenly divided over the range 
                           
                              [
                              0
                              ,
                              180
                              °
                              ]
                           
                        , and a total of 6 bins are used. Each edge pixel within the blob contributes a weighted vote to a histogram bin, equal to 
                           
                              
                                 
                                    S
                                    (
                                    i
                                    ,
                                    j
                                    )
                                 
                              
                           
                         to normalise for perspective. The value of the hth histogram bin is denoted 
                           
                              
                                 
                                    E
                                 
                                 
                                    n
                                 
                              
                              (
                              h
                              )
                           
                        , and the orientation angle for that bin is lower-bounded by 
                           
                              
                                 
                                    θ
                                 
                                 
                                    h
                                 
                              
                           
                        :
                           
                              (6)
                              
                                 
                                    
                                       E
                                    
                                    
                                       n
                                    
                                 
                                 (
                                 h
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          (
                                          i
                                          ,
                                          j
                                          )
                                          ∈
                                          
                                             
                                                B
                                             
                                             
                                                n
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         S
                                                         (
                                                         i
                                                         ,
                                                         j
                                                         )
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         
                                                            if
                                                            
                                                            
                                                               
                                                                  θ
                                                               
                                                               
                                                                  h
                                                               
                                                            
                                                            ⩽
                                                            ∠
                                                            G
                                                            (
                                                            i
                                                            ,
                                                            j
                                                            )
                                                            <
                                                            
                                                               
                                                                  θ
                                                               
                                                               
                                                                  h
                                                                  +
                                                                  1
                                                               
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            and
                                                            
                                                            D
                                                            (
                                                            i
                                                            ,
                                                            j
                                                            )
                                                            =
                                                            1
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   0
                                                
                                                
                                                   otherwise
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

The edge orientation histogram is used to help distinguish between humans and other structures in the scene [48]. Edges also help to identify occlusions when multiple pedestrians partially block one another from view. Although the blob’s size features are reduced by occlusions, the edge features become stronger due to the overlapping body parts, differing skin tones and conflicting clothing.

At the holistic level, the edge orientation histogram is calculated as follows:
                           
                              (7)
                              
                                 E
                                 (
                                 h
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          n
                                       
                                    
                                 
                                 
                                    
                                       E
                                    
                                    
                                       n
                                    
                                 
                                 (
                                 h
                                 )
                              
                           
                        
                     

Canny edge detection [4] is used due to its use of non-maximum suppression and hysteresis thresholding which results in a cleaner output.

Keypoints refer to specific pixels of interest, such as corners, which are detected in an image. Keypoints are useful for detecting salient points of interest in a scene, and these are often indicative of human crowding. For example, Conte [22] used speeded-up robust features (SURF) [3], to detect keypoints within an image. The number of moving keypoints was used to predict crowding. Similarly, Albiol [2] utilised Harris corners [42] to estimate crowd size on a holistic level.

Two types of feature detectors are considered for this evaluation. Firstly, corners are detected using the ‘FAST’ algorithm recently proposed by Rosten [74], and the set of keypoints detected within the foreground blob segment n is denoted 
                           
                              
                                 
                                    κ
                                 
                                 
                                    n
                                 
                                 
                                    FAST
                                 
                              
                           
                        . Secondly, SURF keypoints [3] are extracted and this set of keypoints is denoted 
                           
                              
                                 
                                    κ
                                 
                                 
                                    n
                                 
                                 
                                    SURF
                                 
                              
                           
                        .

The two keypoint features are then calculated as follows:
                           
                              (8)
                              
                                 
                                    
                                       K
                                    
                                    
                                       n
                                    
                                    
                                       FAST
                                    
                                 
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          (
                                          i
                                          ,
                                          j
                                          )
                                          ∈
                                          
                                             
                                                κ
                                             
                                             
                                                n
                                             
                                             
                                                FAST
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       S
                                       (
                                       i
                                       ,
                                       j
                                       )
                                    
                                 
                              
                           
                        
                        
                           
                              (9)
                              
                                 
                                    
                                       K
                                    
                                    
                                       n
                                    
                                    
                                       SURF
                                    
                                 
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          (
                                          i
                                          ,
                                          j
                                          )
                                          ∈
                                          
                                             
                                                κ
                                             
                                             
                                                n
                                             
                                             
                                                SURF
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       S
                                       (
                                       i
                                       ,
                                       j
                                       )
                                    
                                 
                              
                           
                        
                     

Note that the notation 
                           
                              
                                 
                                    κ
                                 
                                 
                                    n
                                 
                                 
                                    FAST
                                 
                              
                           
                         is used to refer to a set of keypoints, while 
                           
                              
                                 
                                    K
                                 
                                 
                                    n
                                 
                                 
                                    FAST
                                 
                              
                           
                         represents the scalar keypoint feature that is calculated from this set.

The keypoints are masked by the foreground detection result, so that keypoints belonging to background objects and surrounding structures are not included in the feature vector.

The equivalent holistic keypoint features are:
                           
                              (10)
                              
                                 
                                    
                                       K
                                    
                                    
                                       FAST
                                    
                                 
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          n
                                       
                                    
                                 
                                 
                                    
                                       K
                                    
                                    
                                       n
                                    
                                    
                                       FAST
                                    
                                 
                              
                           
                        
                        
                           
                              (11)
                              
                                 
                                    
                                       K
                                    
                                    
                                       SURF
                                    
                                 
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          n
                                       
                                    
                                 
                                 
                                    
                                       K
                                    
                                    
                                       n
                                    
                                    
                                       SURF
                                    
                                 
                              
                           
                        
                     

This is the sum of the local keypoint features.

Textural features have been used in the literature, such as GLCM based features [59] and others (Section 2.1). These features are primarily used for density classification, where crowd densities are measured using a four or five point scale. Less commonly, these features have also been used for direct counting via regression, e.g. Chan [8].

The GLCM is calculated for a given offset 
                           
                              (
                              
                                 
                                    δ
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    δ
                                 
                                 
                                    j
                                 
                              
                              )
                           
                        . Using the notation 
                           
                              (
                              
                                 
                                    i
                                 
                                 
                                    ′
                                 
                              
                              ,
                              
                                 
                                    j
                                 
                                 
                                    ′
                                 
                              
                              )
                              =
                              (
                              i
                              +
                              
                                 
                                    δ
                                 
                                 
                                    i
                                 
                              
                              ,
                              j
                              +
                              
                                 
                                    δ
                                 
                                 
                                    j
                                 
                              
                              )
                           
                        , the GLCM is calculated across a region of interest Ras follows:
                           
                              (12)
                              
                                 G
                                 (
                                 r
                                 ,
                                 c
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          (
                                          i
                                          ,
                                          j
                                          )
                                          ∈
                                          R
                                          ∩
                                          (
                                          
                                             
                                                i
                                             
                                             
                                                ′
                                             
                                          
                                          ,
                                          
                                             
                                                j
                                             
                                             
                                                ′
                                             
                                          
                                          )
                                          ∈
                                          R
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   1
                                                
                                                
                                                   if
                                                   
                                                   
                                                      
                                                         I
                                                      
                                                      
                                                         q
                                                      
                                                   
                                                   (
                                                   i
                                                   ,
                                                   j
                                                   )
                                                   =
                                                   r
                                                   ∩
                                                   
                                                      
                                                         I
                                                      
                                                      
                                                         q
                                                      
                                                   
                                                   (
                                                   
                                                      
                                                         i
                                                      
                                                      
                                                         ′
                                                      
                                                   
                                                   ,
                                                   
                                                      
                                                         j
                                                      
                                                      
                                                         ′
                                                      
                                                   
                                                   )
                                                   =
                                                   c
                                                
                                             
                                             
                                                
                                                   0
                                                
                                                
                                                   otherwise
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    I
                                 
                                 
                                    q
                                 
                              
                           
                         represents the quantisation of image Ito 8 grey levels. The symmetric GLCM is denoted 
                           
                              
                                 
                                    G
                                 
                                 
                                    s
                                 
                              
                              =
                              G
                              +
                              
                                 
                                    G
                                 
                                 
                                    T
                                 
                              
                           
                        . The normalised GLCM represents a probability distribution of pixel cooccurrences:
                           
                              (13)
                              
                                 f
                                 (
                                 r
                                 ,
                                 c
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             G
                                          
                                          
                                             s
                                          
                                       
                                       (
                                       r
                                       ,
                                       c
                                       )
                                    
                                    
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                r
                                                ,
                                                c
                                             
                                          
                                       
                                       
                                          
                                             G
                                          
                                          
                                             s
                                          
                                       
                                       (
                                       r
                                       ,
                                       c
                                       )
                                    
                                 
                              
                           
                        
                     

Contrast, homogeneity, energy and entropy features are then calculated as follows:
                           
                              (14)
                              
                                 
                                    
                                       T
                                    
                                    
                                       c
                                    
                                 
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          r
                                          ,
                                          c
                                       
                                    
                                 
                                 
                                    
                                       (
                                       r
                                       -
                                       c
                                       )
                                    
                                    
                                       2
                                    
                                 
                                 f
                                 (
                                 r
                                 ,
                                 c
                                 )
                              
                           
                        
                        
                           
                              (15)
                              
                                 
                                    
                                       T
                                    
                                    
                                       h
                                    
                                 
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          r
                                          ,
                                          c
                                       
                                    
                                 
                                 
                                    
                                       f
                                       (
                                       r
                                       ,
                                       c
                                       )
                                    
                                    
                                       1
                                       +
                                       
                                          
                                             (
                                             r
                                             -
                                             c
                                             )
                                          
                                          
                                             2
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (16)
                              
                                 
                                    
                                       T
                                    
                                    
                                       e
                                    
                                 
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          r
                                          ,
                                          c
                                       
                                    
                                 
                                 f
                                 
                                    
                                       (
                                       r
                                       ,
                                       c
                                       )
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        
                        
                           
                              (17)
                              
                                 
                                    
                                       T
                                    
                                    
                                       s
                                    
                                 
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          r
                                          ,
                                          c
                                       
                                    
                                 
                                 -
                                 f
                                 (
                                 r
                                 ,
                                 c
                                 )
                                 log
                                 f
                                 (
                                 r
                                 ,
                                 c
                                 )
                              
                           
                        
                     

In this evaluation 
                           
                              (
                              
                                 
                                    δ
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    δ
                                 
                                 
                                    j
                                 
                              
                              )
                              =
                              (
                              1
                              ,
                              0
                              )
                           
                         is used. Additional offsets did not confer an improvement in our experiments. The equivalent local features are calculated by first computing the GLCM for each blob n. This is done by substituting 
                           
                              
                                 
                                    B
                                 
                                 
                                    n
                                 
                              
                           
                         for R in Eq. (12) and then computing features locally (
                           
                              
                                 
                                    T
                                 
                                 
                                    n
                                 
                                 
                                    c
                                 
                              
                              ,
                              
                                 
                                    T
                                 
                                 
                                    n
                                 
                                 
                                    h
                                 
                              
                              ,
                              
                                 
                                    T
                                 
                                 
                                    n
                                 
                                 
                                    e
                                 
                              
                              ,
                              
                                 
                                    T
                                 
                                 
                                    n
                                 
                                 
                                    s
                                 
                              
                           
                        ).

Image texture provides information about crowd density [59,57], and this is predictive of crowd size on a holistic level because the region of interest Ris fixed. However, for local segments, 
                           
                              
                                 
                                    B
                                 
                                 
                                    n
                                 
                              
                           
                        , of variable size, texture is poorly correlated with crowd size (Fig. 2
                        ). Nonetheless local textures are included in this evaluation for completeness.

Four types of regression models are evaluated in this research. For comparison we use Gaussian process regression (GPR) [76,8], linear regression [75,48], K-nearest neighbours (KNN) with 
                           
                              K
                              =
                              1
                              ,
                              2
                              ,
                              4
                              ,
                              8
                              ,
                              16
                              ,
                              32
                           
                         
                        [29], and a neural network (NN) [48,44] with a Sigmoid activation function and one hidden input layer (containing 4, 8, 16 or 32 neurons). In total there are 12 regression models with these various parameters. The system is trained locally and holistically by annotating pedestrians as in [76].

As an intermediate between local and holistic features, the histogram features proposed by Kong [49,48] are also evaluated in this analysis. The blob size histogram and edge orientation histogram were used to capture the range of object sizes and their appearance in a scene. The size of the nth blob is denoted 
                           
                              
                                 
                                    A
                                 
                                 
                                    n
                                 
                              
                           
                         as in Eq. (2). The blob size histogram is then constructed as follows: the value in the kth histogram bin is denoted 
                           
                              H
                              (
                              k
                              )
                           
                        , and the blob size for that bin is lower-bounded by 
                           
                              
                                 
                                    a
                                 
                                 
                                    k
                                 
                              
                           
                        , then:
                           
                              (18)
                              
                                 H
                                 (
                                 k
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          n
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         A
                                                      
                                                      
                                                         n
                                                      
                                                   
                                                
                                                
                                                   
                                                   if
                                                   
                                                   
                                                      
                                                         a
                                                      
                                                      
                                                         k
                                                      
                                                   
                                                   ⩽
                                                   
                                                      
                                                         A
                                                      
                                                      
                                                         n
                                                      
                                                   
                                                   <
                                                   
                                                      
                                                         a
                                                      
                                                      
                                                         k
                                                         +
                                                         1
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   0
                                                
                                                
                                                   
                                                   otherwise
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

That is, each histogram bin accumulates the weighted sum of pixels belonging to those blobs whose size falls within the predefined range established for that bin. Kong uses six histogram bins (
                           
                              k
                              ∈
                              [
                              0
                              ,
                              5
                              ]
                           
                        ) of width 
                           
                              W
                              =
                              500
                           
                        , such that:
                           
                              (19)
                              
                                 
                                    
                                       a
                                    
                                    
                                       k
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   Wk
                                                
                                                
                                                   
                                                   if
                                                   
                                                   k
                                                   <
                                                   6
                                                
                                             
                                             
                                                
                                                   ∞
                                                
                                                
                                                   
                                                   if
                                                   
                                                   k
                                                   =
                                                   6
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

The blob size histogram serves to separate the blobs present in an image and to place them into predefined categories. It would be expected that noise contributes to the smallest histogram bin, while individual pedestrians and small groups contribute to the second or third bins, for example. The relationship is learned by the regression model, but the use of blob size histogram bins as image features will enable it to distinguish between groups of people and individuals.

Kong’s algorithm was implemented as faithfully as possible to [49,48], however some assumptions were necessary. Although Kong used a bin width of 
                           
                              W
                              =
                              500
                           
                         for the blob size histogram, this value is not be suitable for all datasets due to differences in image resolution and camera positioning. Instead, the bin width is set to roughly 
                           
                              
                                 
                                    2
                                 
                                 
                                    3
                                 
                              
                           
                         of the size of a person in the scene, so that smaller blobs (noise) are assigned to the first histogram bin and larger groups occupy the other bins. This provides good separation between different blob sizes, as is the intent of the algorithm.

Kong also used the edge orientation histogram (Section 4.3, Eq. (7)) with eight bins to “distinguish edges caused by pedestrians, which are usually vertical, with other scene structures such as noise, shadows and cars” [48]. These pixels are masked by the foreground so that those edges in the background are ignored.

The feature vector used by Kong to represent an image is the concatenation of the blob size histogram and the edge angle histogram. Both linear regression and neural network regression were used to model the crowd size in [48]. For completeness we also evaluate GPR and KNN regression (Section 4.6) on Kong’s feature set.

@&#EVALUATION@&#

This section presents the results of the evaluation: Section 5.1 compares various feature vectors for crowd counting and Section 5.2 compares a number of regression models. Section 5.3 then compares the performance of local, holistic and histogram based methodologies to one another.

This section compares the performance of various image features, and combinations thereof, for crowd counting. The features were discussed in Section 4 and summarised in Table 5. Gaussian process regression is selected as the regression model in this section because this provides the best predictive performance (Section 5.2).

The features are aggregated on either a holistic or local level. (Histogram features, as described in Section 4.7, are evaluated in Section 5.3). Features are categorised into the following categories: size (S), shape (P), edges (E), keypoints (K) and texture (T). Multiple features are represented by letter combinations, such as ‘EK’, which denotes the concatenation of edge and keypoint features.

Various combinations of these features are assessed, as shown in Tables 6–8
                        
                        
                        . Using the 5 -fold cross validation procedure described in Section 3, error rates are calculated across all frames for each dataset, and reported in terms of MAE and MRE.


                        Table 6 summarises the results for local features. Average error rates are reported under their respective columns, as well as a ranking from 1 to 31 indicating the relative performance of each feature set. For example, when assessing local features on the Fudan dataset, the lowest MRE is observed when size, shape, edges and keypoints are used (SPEK), whereas the highest error rate is observed from texture features alone. These feature sets are ranked 1 and 31 respectively. In each column, the top three results (ranked 1–3) are indicated in bold.

The best performing feature sets for the UCSD dataset contain a combination of either three or four types of features. In general, it can be seen that performance improves on this dataset as more features are included; poor performance is particularly seen when only one feature type is used. Similarly for the PETS 2009 dataset, individual features (particularly shape and texture features taken alone) exhibit relatively poor performance, with the MRE falling above the 20% threshold of acceptability suggested by Regazzoni [71]. However, more features tend to perform better in general, and a mean relative error of 16% is observed for the best local feature sets, as indicated in bold.

The best performance is observed on the Fudan dataset when all features except textures are used, and the worst performance is seen when individual feature types are used. This is consistent with the results for the UCSD and PETS 2009 datasets. Shape and texture features alone also perform poorly on the Mall dataset. Due to the reflective surfaces and complicated structure of this scene, foreground segmentation is relatively poor, resulting in substantial noise and unusually-shaped blobs. It is not surprising, therefore, than shape features perform relatively poorly under these conditions. Nonetheless, performance is still quite good in terms of MRE (less than 10%).

Finally, the Grand Central dataset confirms that single features perform poorly, with more stable performance obtained using combined feature sets. Although the MAE is quite high for this dataset, this is explained by the large crowd size which ranges from 125 to 245 people. The mean relative error is less than 5%, well within the threshold of acceptability.

In summary, the MRE is less than 20% for the most accurate local feature sets on each dataset. The UCSD, Mall and Grand Central Datasets achieve 
                           
                              MRE
                              <
                              10
                              %
                           
                         while the PETS 2009 and Fudan datasets achieve 
                           
                              MRE
                              <
                              20
                              %
                           
                        .


                        Table 7 summarises the results for holistic features. Due to the small number of annotated frames in the Grand Central dataset, it was not possible to obtain a trained model for the holistic system, but the results for the other datasets are shown. Relatively poor performance is observed for features taken individually. Improved performance is seen when a combination of multiple feature types are used. For example, a combination of three features exhibits optimal results on the UCSD, PETS 2009 and Fudan datasets. The Mall dataset is an exception, where keypoints outperform other features such as size and shape due to the relatively noisy motion segmentation.

In order to identify dominant trends the data is pooled across all datasets as follows. Firstly, feature sets are ranked from 1 to 31 as shown in parentheses in Tables 6 and 7, and the average rank across all datasets is reported in Table 8 for each feature set. For example, shape alone (P) obtains an average ranking of 22.5 out of 31 (with the holistic approach), indicating a consistently poor performance for this feature across all datasets. By contrast, when size, shape and keypoints (SPK) are used on the holistic level, an average rank of 8.5 is observed in terms of MRE.

Average ranking across multiple datasets provides a clearer picture than any individual dataset taken alone. It becomes clear from Table 8 that when more local features are used (aside from texture), the average ranking across all datasets is improved. The best local feature vector is: size, shape, edges, keypoints (SPEK) with an average rank of 4.8 out of 31 in terms of MAE. This feature vector suffers a reduction in performance if any feature is omitted, although the omission of size does not make a very large difference (PEK ranks 5.6). This suggests that size may not be as crucial in achieving optimal performance as expected.

The size feature is a direct measure of foreground pixels within each blob segment, and foreground pixel counting has formed the basis of most traditional algorithms in the literature. These results suggest that foreground detection provides a suitable means for segmenting an image (for the purpose of localisation), but the actual size of these segments is not a critical feature for crowd counting; in occluded and complicated crowd scenes, it is the presence of edges, keypoints and shape cues within those segments appear to be the most important features. Nevertheless, segment size does provide an intuitive measurement of the physical space occupied by a group, and it does provide a modest improvement in these experiments.

When holistic features are used, the best performance is observed for the feature set comprised of size, shape, keypoints (SPK). This feature vector does not include edges or textures, which have been used widely in the literature on a holistic level (Section 2).


                        Fig. 2 plots the relationship between selected features and crowd size on the UCSD and Mall datasets. Due to the relatively untextured background of the UCSD dataset (Fig. 1(d)), pedestrians introduce texture to the image, resulting in a relatively linear relationship between entropy and crowd size (
                           
                              ρ
                              =
                              0.94
                           
                        , Fig. 2(a)). By contrast, the Mall dataset features a highly textured background (Fig. 1(e)): in this environment, pedestrians may either contribute or occlude texture, resulting in a poorer correlation (
                           
                              ρ
                              =
                              0.53
                           
                        , Fig. 2(b)).

These results lead to some interesting conclusions:
                           
                              1.
                              When local features are used, optimal performance is observed with size, shape, edge and keypoint features. However, size based features are not as critical for achieving optimal crowd counting results as expected. Optimal performance is observed with other local features such as shape, keypoints and edges. Note, however, that these features are still masked by the foreground detection result, and the segmentation of ‘blobs’ in the foreground is used as a basis for localisation. Therefore background modelling and foreground detection continues to play an important role in these experiments.

When holistic features are used, optimal performance is observed with size, shape and keypoint features. Texture and edge based features do not achieve the best results, despite their widespread usage. This is due to the inclusion of datasets with highly textured backgrounds such as PETS 2009 (Fig. 1) and the Mall dataset (Fig. 1(e)).

Individual features perform relatively poorly compared to multiple feature combinations.

These conclusions can be used to inform the design and implementation of crowd counting systems in practice.

This section compares the performance of various regression models for crowd counting. Holistic, local and histogram based algorithms are evaluated. Local features are evaluated using the feature vector: size, shape, edges, keypoints (SPEK). Holistic features are evaluated using size, shape and keypoints (SPK). These feature vectors were selected due to their optimal performance in Section 5.1. Histogram features are implemented as described by Kong [48] (see Section 4.7).

As in Section 5.1, 5-fold cross validation was used. For comparison we use Gaussian process regression (GPR), linear regression, K-nearest neighbours (KNN) with 
                           
                              K
                              =
                              1
                              ,
                              2
                              ,
                              4
                              ,
                              8
                              ,
                              16
                              ,
                              32
                           
                        , and a neural network (NN) with a Sigmoid activation function and one hidden input layer (containing 4, 8, 16 or 32 neurons). In total there are 12 regression models with various parameters. Note that in some cases training fails with the neural network model and large error values are reported in these instances.


                        Table 9
                         summarises the results of various regression models using local features. Average error rates are reported in terms of MAE and MRE, and regression models are ranked from 1 to 12 on each dataset, with 1 corresponding to the most accurate regression model and 12 the least accurate. The GPR and linear models provide most accurate performance on the UCSD dataset, with a MAE of 1.46 and 1.56 respectively. There is a significant reduction in performance for the third most accurate regression model (KNN with 
                           
                              K
                              =
                              4
                           
                        ), for which a MAE of 2.72 was observed. Similarly on the PETS 2009 dataset, these regression models exhibited a MAE of 1.78, 1.77 and 3.00 respectively. The GPR and linear models rank in the top two positions on the Fudan and Mall datasets, consistent with their performance on the UCSD and PETS 2009 datasets. The GPR model also ranks highest on the Grand Central dataset. These results provide strong support for the use of GPR and linear regression in conjunction with local features.


                        Table 10
                         summarises the results of various regression models using holistic features. As was observed with local features, the GPR and linear models ranked highest on the UCSD, PETS 2009 and Fudan datasets, with other regression models exhibiting a substantial reduction in performance by comparison. GPR also ranked highly on the Mall dataset.


                        Table 11
                         presents the results of various regression models using histogram features. Although suitable performance is observed with most regression models, the optimal model differs between datasets. In order to identify dominant trends, the data is pooled using the average rank across all datasets. Table 12
                         presents these results.

For histogram features, the best performance was seen with the linear model, which had an average rank of 3.0 out of 12 in terms of MAE. The neural networks also ranked very highly when 8 or 16 neurons were used in the hidden layer. These results confirm that linear regression and neural networks are the most appropriate regression models to be used in conjunction with Kong’s histogram based feature set, as proposed by the author [48].

For local features, GPR outperforms the other models with an average ranking of 1.2 out of 12 in terms of MRE. Similarly for holistic features, GPR achieves an average rank of 1.25. These results provide very strong support for the use of Gaussian process regression in both local and holistic crowd counting systems, compared to linear, KNN or NN regression. Linear regression provides optimal performance for the histogram features proposed by Kong.

This section compares the performance of local, holistic and histogram features to one another. Local and holistic features are evaluated using GPR, while histogram features are evaluated using linear regression. These regression models were selected due to their optimal performance in Section 5.2.


                        Table 13
                         presents the performance of the holistic, local and histogram based approaches side-by-side. The following feature vectors were selected due to their optimal performance in Section 5.1:
                           
                              •
                              Size, Shape, Keypoints (SPK).

Size, Shape, Edges, Keypoints (SPEK).

The first feature set is optimal for holistic systems, while the second is optimal for local systems (see Table 8). Histogram features are also presented in Table 13, and because these features are based on blob sizes and edge orientations, we also include the ‘Size, Edges’ feature vector for a similar comparison.

Each row in Table 13 lists the results for a given feature set, and the best result (holistic, local or histogram) is indicated in bold. For each dataset the best result across all system configurations is underlined.

For the Mall dataset, local features outperform holistic and histogram features in all experiments, regardless of the feature vector used. On the Fudan dataset, local features outperform holistic features except for when ‘Size, Edges’ are used; in that case, optimal performance is observed for holistic features (in terms of MAE). Regardless, the best performance on the Fudan dataset (across all system configurations) is observed for a combination of local features (size, shape, edges, keypoints).

The PETS 2009 dataset produces mixed results: when ‘Size, Shape, Keypoints’ are used, holistic features outperform local features. However, local features perform best with the other configurations. Similarly, results are mixed on the UCSD dataset, although optimal performance is observed with local features (size, shape, edges, keypoints).

In each dataset, the best performance is underlined, and the lowest error rates are observed with a local approach in each case. These results provide strong support for the use of local features rather than holistic or histogram features on these datasets.


                        Fig. 3
                         shows two screenshots of the crowd counting algorithm operating using local features: the group estimate for each segment is rounded to the nearest integer and the total crowd estimate is shown at the top of the image. Fig. 4
                         plots the estimate of the local, holistic and histogram based approaches against the ground truth for a number of sequences. These figures provide qualitative evidence for the algorithms evaluated in this paper across a wide range of conditions. The difference between these algorithms is most evident on sequence 13–57 of the PETS 2009 dataset, for which the local approach is most accurate.

In this section we report the processing speed of the algorithms. For comparison, the datasets with the smallest and largest images were selected: the UCSD dataset has a resolution of 
                           
                              236
                              ×
                              158
                           
                         pixels, whereas the PETS 2009 dataset has a resolution of 
                           
                              768
                              ×
                              576
                           
                        .

The baseline algorithm for this section is based on local features with Gaussian process regression (GPR) and a feature vector of ‘Size, Shape, Edges, Keypoints’ (SPEK). This configuration was selected due to its optimal performance in Sections 5.1,. This algorithm operated at 20.2 fps and 2.7 fps on the UCSD and PETS 2009 datasets respectively.


                        Table 14
                         compares the processing speed with various feature vectors. When ‘size’ or ‘shape’ features are omitted from the feature vector (i.e. PEK, SEK), little change is observed in processing speed: the algorithm operates at 20.2 and 2.7–2.8 fps on the UCSD and PETS 2009 datasets. As local features require blob localisation, the calculation of size and shape features require little additional overhead. A small improvement in processing speed is observed by omitting ‘edge’ features: 21.7 and 3.0 fps respectively. The greatest improvement in processing speed occurs when ‘keypoints’ are omitted, as the SURF and FAST algorithms are skipped. In this case the speed increases to 27.8 and 4.3 fps.

Regression models are compared in Table 15
                        . Similar processing speed is observed for the linear, Knearest neighbours and neural network regression models: 22.5 and 2.8 fps for the UCSD and PETS 2009 datasets respectively. Gaussian process regression is slightly slower (20.2 and 2.7 fps) due to the large matrix calculations required to calculate the mean of the predictive distribution [70].

Finally, Table 16
                         compares the local, holistic and histogram based methods. For this comparison, linear regression and a feature vector comprised of ‘Size, Edges’ was used to enable a direct comparison with the method of Kong [48]. No difference was observed between the local, holistic and histogram based methods in our implementation. However, it should be noted that our implementation has not been optimised for each of the methods individually, and additional improvements are likely to be obtained by doing so. For example, the framework calculates holistic features as the sum of local features (Eq. (4)); this method is useful for the current evaluation, in which local and holistic features are compared, however a holistic-only system would likely omit the blob localisation step entirely. For this reason an optimised holistic system would be expected to operate slightly faster than its local counterpart.

These algorithms were implemented in C++ and processed on a single CPU core. The main bottleneck is motion segmentation, which can be improved significantly using GPU acceleration as in [67] for example. Keypoint detection and Gaussian process regression also incur additional overhead, and improvements to these components may be observed with multi-threading or GPU acceleration.

@&#CONCLUSIONS@&#

This paper evaluated feature types and regression models for crowd counting using local, holistic and histogram based approaches. Local features are specific to foreground segments in an image, and are used to estimate the size of each group. The local approach is annotated, trained and tested at a local level, whereas the holistic approach takes place across the entire image. The histogram approach accumulates information about local objects into histogram bins, and this information is represented at a holistic level. The following conclusions were reached as a result of this analysis:
                        
                           •
                           The use of local features consistently outperformed holistic features and histogram features (Section 5.3).

For the local approach, a greater quantity of features generally improved performance compared to fewer features, with the exception of textures (Section 5.1). The best performance was observed with the feature vector: ‘size, shape, edges, keypoints’. The omission of ‘size’ did not significantly reduce the overall performance (Table 8).

For the holistic approach, edge and texture features did not provide optimal performance despite their widespread usage in the literature. Instead, best performance was seen with the feature vector: ‘size, shape, keypoints’ (Section 5.1).

The use of Gaussian process regression consistently outperformed linear regression, K-nearest neighbours and neural networks, for both the local and holistic approach (Section 5.2). For the histogram based approach, the optimal regression models were linear regression and neural networks, consistent with the algorithm proposed by Kong [48].

Future research is warranted across a wider range of datasets to confirm these findings and to establish if additional feature sets or regression models can improve performance further. A comparison of existing motion segmentation algorithms [89,25,79] and localisation strategies [13,75,50] may provide additional insight into current crowd counting technology. The present data suggests that optimal performance is observed when GPR is employed with multiple local features.

@&#ACKNOWLEDGMENT@&#

This research was supported by an Australian Research Council (ARC) Linkage Grant No: LP0990135.

@&#REFERENCES@&#

