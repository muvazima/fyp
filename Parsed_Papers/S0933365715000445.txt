@&#MAIN-TITLE@&#An unsupervised feature learning framework for basal cell carcinoma image analysis

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           A framework for basal cell carcinoma detection based on unsupervised feature learning.


                        
                        
                           
                           Experimental results show an improvement when compared to state-of-the-art methods.


                        
                        
                           
                           The framework integrates a digital staining method which improves interpretability.


                        
                        
                           
                           Digital staining highlights regions in the image which the model relates to cancer.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Digital pathology

Representation learning

Unsupervised feature learning

Basal cell carcinoma

@&#ABSTRACT@&#


               
               
                  Objective
                  The paper addresses the problem of automatic detection of basal cell carcinoma (BCC) in histopathology images. In particular, it proposes a framework to both, learn the image representation in an unsupervised way and visualize discriminative features supported by the learned model.
               
               
                  Materials and methods
                  This paper presents an integrated unsupervised feature learning (UFL) framework for histopathology image analysis that comprises three main stages: (1) local (patch) representation learning using different strategies (sparse autoencoders, reconstruct independent component analysis and topographic independent component analysis (TICA), (2) global (image) representation learning using a bag-of-features representation or a convolutional neural network, and (3) a visual interpretation layer to highlight the most discriminant regions detected by the model. The integrated unsupervised feature learning framework was exhaustively evaluated in a histopathology image dataset for BCC diagnosis.
               
               
                  Results
                  The experimental evaluation produced a classification performance of 98.1%, in terms of the area under receiver-operating-characteristic curve, for the proposed framework outperforming by 7% the state-of-the-art discrete cosine transform patch-based representation.
               
               
                  Conclusions
                  The proposed UFL-representation-based approach outperforms state-of-the-art methods for BCC detection. Thanks to its visual interpretation layer, the method is able to highlight discriminative tissue regions providing a better diagnosis support. Among the different UFL strategies tested, TICA-learned features exhibited the best performance thanks to its ability to capture low-level invariances, which are inherent to the nature of the problem.
               
            

@&#INTRODUCTION@&#

Digital pathology refers to the set of computational methods and technologies that support the different pathology workflow stages, including digital slide acquisition, computer aided diagnosis, prognosis and theragnosis [1]. The importance and popularity of digital pathology have rapidly grown during the last years thanks of the emergence of fast, cost-effective whole slide image acquisition systems. An important component of digital pathology is automatic image analysis, which is fundamental for tasks such as automatic tumor detection and grading [2]. Automatic image analysis encompasses different kinds of computer vision and pattern recognition problems associated with the detection, segmentation and classification of biological structures (pathological and non-pathological).

The success of any histopathology image analysis method depends on how well it captures morphological and architectural characteristics from nuclei, cells, glands, organs and tissues. In turn this depends on how well the method characterizes the visual content of the histopathology image. This characterization is accomplished by a feature extraction process which typically uses canonical (e.g. wavelet transforms) or hand-engineered features (e.g. SIFT). Different visual features have been proposed and extensively evaluated in different histopathology image analysis problems: (1) object level features to characterize biological structures, e.g. size and shape, radiometric and densitometric, texture, chromatin-specific; (2) spatially related features to represent the architectural arrangement of cells or other structures, usually derived from graph-based representations and descriptors, e.g. Voronoi tessellation, Delaunay triangulation, minimum spanning graph, connected graph, relative neighbor graph and others [2,3]; (3) multi-scale feature extraction algorithms, e.g. multi-resolution, pyramid and hierarchical representation of images among others [2,3]. Mostly, feature choice is handmade and based on the particularities of the problem at hand. However, recent investigations in computer vision suggests that strategies which learn the image representation directly, and automatically, from image collections may produce better representations resulting in better performance of automatic analysis algorithms [4]. This approach, known as unsupervised feature learning (UFL), has been applied to different computer vision and pattern recognition problems with great success [5].

This paper explores the application of UFL strategies to the representation and automatic analysis of histopathology images. In particular, the paper presents a framework for basal cell carcinoma (BCC) image analysis which is able to learn an appropriate representation from a representative set of images for automatic carcinoma detection. The experimental evaluation shows that the proposed framework outperforms state-of-the-art BCC histopathology image representation and classification methods. The main contributions of this work are:
                        
                           •
                           a novel hybrid method combining the state-of-the-art visual representations and learning techniques to integrate UFL in a complete data-driven approach;

a strategy to endow the classification method with a visual interpretation layer, which exploits both the hierarchical representation of the model, to highlight those regions of the image that contribute to a higher degree to the model prediction, and the topographic organization of the learned representation, to identify visual patterns associated with tumor and non-tumor images; and

a systematic evaluation of different UFL strategies on BCC histopathology images, which shows that, in this particular problem, learned features outperform state-of-the-art canonical representations.

The rest of the paper is organized as follows: Section 2 makes a review of the relevant literature, Section 3 describes the unified unsupervised learning framework for BCC histopathology image representation and analysis; Section 4 presents the results of experimental evaluation for BCC detection. Finally, conclusions are presented in Section 5.

@&#RELATED WORK@&#

Nowadays, the explosion of big data in pathology research is bringing many opportunities and challenges that have given birth to a novel research area known as digital pathology 
                     [1]. This is possible thanks to the massification of histology slide scanners as part of the pathology routine, the increase on the number of publicly available histology and histopathology image databases, and the development of virtual slide navigation systems [6,7]
                  

Digital or computational pathology is a new emerging area that investigates a complete probabilistic treatment of scientific and clinical workflows in general pathology, i.e. it combines experimental design, statistical pattern recognition and survival analysis within a unified framework to answer scientific and clinical questions in pathology [8]. This area is faced with big challenges such as automatic diagnosis and objective quantification of disease's indicators for personalized diagnosis [1]. Typically, tumor detection and grading are the main goals in pathology digital slide analysis [2]. The main goal of tumor detection is to differentiate between healthy and tumor tissues to support diagnosis. The goal of grading is to quantify architectural and morphological signatures within tumor to classify the grade of aggressiveness, which is very important to determine the appropriate treatment for each patient. These two problems are mainly addressed combining two different, but complementary strategies: (1) hand-crafted features to capture chromatin, morphological and architectural characteristics from nuclei, glands and tissues, and (2) machine learning methods to induce models that use the visual features for detection, segmentation, and classification of biological structures or types of cancer.

There is an extensive literature in automatic histopathology image analysis. Gurcan et al. [3] performed a comprehensive review of state-of-the-art in histopathology image analysis in different pathology problems with 133 references. There, the authors presented the works organized according to type of segmentation, feature extraction, and classification algorithms. For instance, for feature extraction they describe: (i) object-level features to characterize biological structures (e.g. size, shape, radiometric, densitometric, texture, and chromatin-specific [9]); (ii) spatially related features to represent the architectural arrangement of cells or other structures mainly graph-based representations and descriptors, (e.g. Voronoi tessellation, Delaunay triangulation, minimum spanning graph, connected graph, relative neighbor graph, and k-NN [9–12]); (iii) multi-scale feature extraction algorithms (e.g. multi-resolution, pyramid and hierarchical representations [13–15]); and (iv) a common feature selection or dimensionality reduction techniques (e.g. independent component analysis, linear discriminant analysis, principal component analysis, and manifold learning [16–19]). In the case of machine learning algorithms applied to histopathology image analysis, the methods have to deal with large, highly dense datasets, multiple scales and visual features motivating the use of ensemble of classifiers more than a single classifier for tumor detection and grading, e.g. Adaboost learning, graph-based classifiers, and kernel methods like support vector machines [20]. More recently, He et al. [2] presented a new overview of the state of the art in histopathology image analysis by organizing the above different stages and techniques for each of the four most common carcinomas: cervix, prostate, breast and lung. Particularly, the present work focuses in BCC, the most common type of skin cancer. In this sense, automatic image analysis of BCC had been explored previously under traditional approaches by integrating image representation and machine learning methods.

Representation learning have recently drawn a lot of attention thanks of the impressive results in different computer vision and pattern recognition tasks [5]. In general, representation learning encompasses different methods, most of them based on neural networks, that combine linear and non-linear transformations of the data, with the goal of yielding more abstract and ultimately more useful representations. These methods have rapidly increased in popularity and attracted researchers attention due to a remarkable string of empirical successes, both in academy and in industry, beating traditional approaches in each application domain, such as speech recognition, object recognition and image classification, with significant breakthrough results [4,21–24].

These approaches have already been successfully used in histopathology image analysis tasks. Malon et al. [25] were one of the firsts proposing to use convolutional neural networks (CNN), one of the most popular type of supervised representation learning models, to count mitotic figures in breast cancer histology images, to recognize epithelial layers in the stomach, and to detect signet ring cells. CNN had been also used in segmentation and classification of histopathology images for breast cancer cell detection [26,27]. Both works proposed CNN as a mechanism to learn the image representation over squared image regions. Le et al. [28] applied a two layer neural network, to tissue sections for characterizing necrosis, apoptotic, and viable regions of glioblastoma multiforme from the TCGA
                           1
                        
                        
                           1
                           The cancer genome atlas, http://cancergenome.nih.gov/ [accessed 05.02.15].
                         database. They learned the feature representation in an unsupervised way using a reconstruction independent subspace analysis network [29]. Cruz-Roa et al. [30] proposed a CNN-based framework for invasive ductal carcinoma tumor region detection in whole-slide breast cancer images improving the performance in comparison with a set of hand-crafted features used in computer vision and histopathology image analysis. Recently, CNN models had been also applied to the challenging task of automatic mitosis detection in breast cancer histopathology images [31–34]. In [31] the authors propose combining a set of hand-crafted features with a CNN. Whereas, authors in [32] proposed a more sophisticated and large, GPU optimized, CNN architecture trained over a huge amount of training data, around 1 million samples for training. These two strategies got the fourth and first place respectively in the mitosis detection challenge in ICPR’2012 [35]. However, the ICPR’2012 winner [32] used an implementation that is not easy to apply in the clinical practice due to the high computing time required for training and prediction and the special requirements of GPU hardware. Recently, Wang et al. [33,34] proposed an approach which combines hand-crafted features with a CNN using a cascade of classifiers to predict mitosis presence. This approach obtained better results than other hand-crafted features and CNN combination methods [31].

Most of the above previous works in histopathology image analysis are focused on supervised feature learning techniques such as CNN models. These kind of methods are addressed to solve supervised tasks, such as nuclei segmentation or tissue classification, where the learned representation depends on a particular supervised task. Nevertheless, it is possible to look for an appropriate image representation regardless of a particular high-level task. This representation must capture visual patterns which are the building blocks of the visual content. This unsupervised approach has an additional advantage, it does not depend on labeled samples and prior knowledge provided by experts. In many cases this labeled data is not available, particularly in a biomedical domain due to limitations in time or cost. UFL methods have shown successful and promising results to automatically learn the appropriate image representation from raw data [23]. In addition, since 2006 a breakthrough in feature learning was initiated by Hinton et al. [4] proposing novel methods to learn a hierarchy of features one level at a time in a unsupervised way. This allows a faster training of deep architecture models (networks with multiple layers) for representation learning. However, UFL have not been sufficiently explored for histopathology image analysis.

BCC is the most common skin disease [36] and its incidence is growing worldwide [37]. It has different risk factors and its development is mainly due to ultraviolet radiation exposure. Although it does not usually metastasizes or kills, it may cause significant tissue damage, destruction and, in some cases, disfigurement. The prognostic is excellent provided there is an appropriate treatment in early stages. Pathologists confirm whether or not this disease is present after a biopsied tissue is evaluated under microscope. In such evaluation, physicians aim to recognize some characteristic patterns or complex mixes of patterns. Wong et al. [38] describes the structural patterns that characterize the BCC through 11 different complex patterns. The main challenge for automated tumor detection of BCC is the fact that histopathology images reveal a complex mixture of visual patterns with high variability of biological structures associated to different morphology and architectural arrangements of cells in healthy or pathological tissues. Fig. 1
                         shows histopathology image samples stained with hematoxylin–eosin (H&E) from tumoral and non-tumoral tissue samples. These images illustrate the high intra-class visual variability in BCC diagnosis, which is caused by the presence (or absence) of different morphological and architectural structures, both in healthy tissues (eccrine glands, hair follicles, epithelium, collagen, sebaceous glands) and BCC subtypes (morpheaform, nodular and cystic change).

In general, histopathology images have high visual variability coming from several sources: cut orientation, staining and luminance, magnification, and digitalization among others. Despite there is a defined protocol for image acquisition, the human factor introduces variability to the process. As a result, visual content representation of these images is highly challenging and many previous efforts have tried to design handcrafted and task-oriented visual features [2], which are, in many cases, only applicable to the original problem, decreasing its reproducibility and generality. Novel approaches have been adapting ideas from computer vision and machine learning to build automatic methods able to detect pathological structures associated with BCC on histopathology images. Initially, Caicedo et al. [39] proposed to use a set of global hand-crafted features for visual content image representation of BCC images in content-based image retrieval. Then, they proposed in [40,41] a framework to automatically detect different biological structures to diagnose BCC by combining an image representation technique used in computer vision known as bag of features (BOF) and a machine learning kernel methods. Looking for a more detailed diagnosis, Díaz and Romero [42] proposed an annotation model with probabilistic latent semantic analysis and a BOF representation as input features. Other methods for automatic annotation for BCC collections has been proposed, Cruz-Roa et al. [43,44] applied non-negative matrix factorization, combined with a BOF representation, to build a latent topic model, which provided a probabilistic interpretation. Gutiérrez et al. [45] proposed a supervised model, inspired by the organization of the brain visual cortex, to automatically detect regions of interest in BCC images, allowing to concentrate any processing effort on specific image areas. Díaz and Romero [46] proposed a microstructural tissue analysis method in BCC images, which combines a stain correction strategy and an automatic method to identify morphological and architectural features in square image regions based on latent semantic analysis and support vector machines. A preliminary work by Cruz-Roa et al. [47] explored sparse autoencoders (SAE) to do UFL combined with a softmax classifier for BCC detection. An adaptation, including a topographic approach, was proposed in [48]. The present work extends the work in [47,48] in several ways: different UFL strategies along with different network architectures are evaluated, a novel hybrid image representation strategy, combining learned features with a BOF is proposed, and a new visualization strategy to improve the model interpretability is introduced.

Feature extraction is a fundamental process for machine learning. Its goal is to extract useful characteristics from the data which are later fed to a learning algorithm. In computer vision, feature extraction corresponds to calculate values from input images. These values (features) represent particular characteristics of the image and are calculated from the raw pixels. The functions used to compute such features are called feature detectors. Traditional approaches in histopathology image analysis are based on standard or hand-crafted feature detectors which are manually selected to fit the problem at hand. Many efforts have focused on improving classifiers performance or enhancing the representation using the domain knowledge. UFL tackles this problem from a different perspective. Instead of designing custom feature detectors, UFL learns them from data in an unsupervised way.

Here, we propose a strategy based on UFL to represent histopathology images. The proposed framework is depicted in Fig. 2
                     . The first step, detailed in Section 3.1, learns a set of feature detectors from a set of patches randomly sampled from the image collection. Such detectors will capture the most common patterns present at small regions by modeling an autoencoder neural network. In the second step, detailed in Section 3.2, images are represented using either a convolutional or BOF approach. Such representations use feature detectors learned locally in the first step to find visual patterns in the whole image. The third step, detailed in Section 3.3, trains a binary classification model using the representation obtained from either convolutional or BOF approaches. Section 3.4 details the fourth stage, which adds visualization capabilities using the learned features and the classification model.

An UFL model learns a set of feature detectors that can explain better the content of the data. These feature detectors are directly learned from the data through an optimization process. Feature detectors are modeled as linear or non-linear transformations applied to an input image, 
                           
                              f
                              j
                           
                           :
                           
                              
                                 
                                    ℝ
                                 
                              
                              d
                           
                           →
                           
                              
                                 ℝ
                              
                           
                        , where 
                           
                              
                                 
                                    ℝ
                                 
                              
                              d
                           
                         is the image representation space (e.g. 
                           d
                           =
                           h
                           ·
                           w
                         for grayscale images of 
                           h
                           ×
                           w
                         pixels). If there are n different features, all the features may be grouped in a unique transformation 
                           f
                           :
                           
                              
                                 
                                    ℝ
                                 
                              
                              d
                           
                           →
                           
                              
                                 
                                    ℝ
                                 
                              
                              n
                           
                         such that f(I)=(f
                        1(I), …, f
                        
                           n
                        (I)).

There are different approaches to perform UFL. Among the most popular ones, is auto-encoding UFL, which attempts to find a function (the autoencoder) able to reconstruct its input. In this context, feature detectors, f
                        
                           j
                        , may be seen as encoding functions with a corresponding decoding function g, which tries to recover the original input images from the features:


                        
                           
                              (1)
                              
                                 
                                    r
                                    Θ
                                 
                                 (
                                 I
                                 )
                                 =
                                 g
                                 (
                                 f
                                 (
                                 I
                                 )
                                 )
                                 ≈
                                 I
                                 ,
                              
                           
                        where 
                           g
                           :
                           
                              
                                 
                                    ℝ
                                 
                              
                              n
                           
                           →
                           
                              
                                 
                                    ℝ
                                 
                              
                              d
                           
                         is the decoding function, 
                           
                              r
                              Θ
                           
                           :
                           
                              
                                 
                                    ℝ
                                 
                              
                              d
                           
                           →
                           
                              
                                 
                                    ℝ
                                 
                              
                              d
                           
                         is the reconstruction function, and Θ is a set of parameters which determine the functions g and f. This approach may be seen as a 2-layer neural network which looks for an output as similar as possible to the input, i.e. minimizing the reconstruction error.

It should be noted that when the number of features is greater than the dimension of the input data, n
                        >
                        d, the model is tempted to learn feature detectors with zero or identity functions. A regularization term is usually included to prevent such behavior. In summary, the objective function of autoencoders is defined as


                        
                           
                              (2)
                              
                                 J
                                 (
                                 Θ
                                 )
                                 =
                                 L
                                 (
                                 
                                    
                                       X
                                    
                                 
                                 ,
                                 
                                    r
                                    Θ
                                 
                                 (
                                 
                                    
                                       X
                                    
                                 
                                 )
                                 )
                                 +
                                 R
                                 (
                                 
                                    
                                       X
                                    
                                 
                                 ,
                                 Θ
                                 )
                                 ,
                              
                           
                        where 
                           
                              
                                 X
                              
                           
                           ∈
                           
                              
                                 
                                    ℝ
                                 
                              
                              
                                 d
                                 ×
                                 m
                              
                           
                         is the training dataset of size m with linearized images as columns, 
                           L
                           
                              
                                 ·
                              
                           
                         is the loss function of the reconstruction and 
                           R
                           
                              
                                 ·
                              
                           
                         the regularization term. Several variants of autoencoders have been developed and the kind of learned feature detectors depends on functions 
                           L
                         and 
                           R
                        . In this work we evaluated three main UFL approaches [49]: SAE, reconstruct independent component analysis (RICA) and topographic independent component analysis (TICA).

A desired property for the new representation of data is sparseness. Getting a more compact (sparse) representation of data means that the model discovers the most representative structure in the data yielding to more expressive power. SAE objective function is given by:


                           
                              
                                 (3)
                                 
                                    
                                       J
                                       sparse
                                    
                                    
                                       
                                          Θ
                                       
                                    
                                    =
                                    
                                       1
                                       2
                                    
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       m
                                    
                                    
                                       
                                          
                                             
                                                
                                                   r
                                                   Θ
                                                
                                                (
                                                
                                                   x
                                                   
                                                      (
                                                      i
                                                      )
                                                   
                                                
                                                )
                                                −
                                                
                                                   x
                                                   
                                                      
                                                         i
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       2
                                       2
                                    
                                    +
                                    β
                                    
                                       ∑
                                       
                                          j
                                          =
                                          1
                                       
                                       n
                                    
                                    KL
                                    (
                                    ρ
                                    |
                                    
                                    |
                                    
                                       
                                          
                                             ρ
                                             ˆ
                                          
                                       
                                       j
                                    
                                    )
                                    +
                                    
                                       γ
                                       2
                                    
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            W
                                                         
                                                      
                                                   
                                                
                                                F
                                                2
                                             
                                             +
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  W
                                                               
                                                            
                                                            ′
                                                         
                                                      
                                                   
                                                
                                                F
                                                2
                                             
                                          
                                       
                                    
                                    ,
                                 
                              
                           where 
                              
                                 x
                                 
                                    (
                                    i
                                    )
                                 
                              
                              ∈
                              
                                 
                                    
                                       ℝ
                                    
                                 
                                 d
                              
                            is the i-th sample in the training set X, 
                              β
                              ∈
                              
                                 
                                    ℝ
                                 
                              
                            is the hyperparameter that controls the trade-off between regularization and reconstruction, and 
                              KL
                              (
                              ρ
                              |
                              
                              |
                              
                                 
                                    
                                       ρ
                                       ˆ
                                    
                                 
                                 j
                              
                              )
                              =
                              ρ
                              log
                              (
                              ρ
                              /
                              
                                 
                                    
                                       
                                          ρ
                                          ˆ
                                       
                                    
                                    j
                                 
                              
                              )
                              +
                              (
                              1
                              −
                              ρ
                              )
                              log
                              (
                              
                                 (
                                 1
                                 −
                                 ρ
                                 )
                              
                              /
                              
                                 (
                                 1
                                 −
                                 
                                    
                                       
                                          ρ
                                          ˆ
                                       
                                    
                                    j
                                 
                                 )
                              
                              )
                            is the Kullback–Leibler divergence between two Bernoulli distributions with means 
                              ρ
                              ∈
                              
                                 
                                    ℝ
                                 
                              
                           , the desired sparsity parameter percentage, and 
                              
                                 
                                    
                                       ρ
                                       ˆ
                                    
                                 
                                 j
                              
                              ∈
                              
                                 
                                    ℝ
                                 
                              
                           , the average activation of the j-th feature detector. When ρ is close to zero the second term penalizes high activations of feature detectors, in other words, the model tries to find sparse representations for the data. We chose 
                              f
                              
                                 
                                    x
                                 
                              
                              =
                              sigmoid
                              (
                              
                                 
                                    W
                                 
                              
                              x
                              +
                              b
                              )
                            and 
                              g
                              
                                 
                                    s
                                 
                              
                              =
                              
                                 
                                    
                                       W
                                    
                                 
                                 ′
                              
                              s
                              +
                              c
                           . Thus, Θ={W, W′, b, c} is the set of parameters, 
                              
                                 
                                    W
                                 
                              
                              ∈
                              
                                 
                                    
                                       ℝ
                                    
                                 
                                 
                                    n
                                    ×
                                    d
                                 
                              
                            and 
                              
                                 
                                    
                                       W
                                    
                                 
                                 ′
                              
                              ∈
                              
                                 
                                    
                                       ℝ
                                    
                                 
                                 
                                    d
                                    ×
                                    n
                                 
                              
                            are the encoder and decoder weight matrices, and 
                              b
                              ∈
                              
                                 
                                    
                                       ℝ
                                    
                                 
                                 n
                              
                            and 
                              c
                              ∈
                              
                                 
                                    
                                       ℝ
                                    
                                 
                                 d
                              
                            are encoder and decoder bias vectors. Finally, the third term regularizes magnitudes of the weights through the Frobenius norm (
                              
                                 
                                    
                                       ·
                                    
                                 
                                 F
                              
                           ), with the hyperparameter 
                              γ
                              ∈
                              
                                 
                                    ℝ
                                 
                              
                            controlling the importance of the term in the objective function.

Independent component analysis (ICA) discovers robust representations based on the assumption that images can be reconstructed by a linear combination of statistically independent feature detectors. ICA finds such features by solving the following optimization problem:


                           
                              
                                 (4)
                                 
                                    
                                       minimize
                                       
                                          
                                             W
                                          
                                       
                                    
                                    
                                       ∑
                                       
                                          j
                                          =
                                          1
                                       
                                       n
                                    
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       m
                                    
                                    v
                                    (
                                    
                                       f
                                       j
                                    
                                    (
                                    
                                       x
                                       
                                          (
                                          i
                                          )
                                       
                                    
                                    )
                                    )
                                    s
                                    .
                                    t
                                    .
                                    
                                    
                                       
                                          W
                                       
                                    
                                    
                                       
                                          
                                             W
                                          
                                       
                                       T
                                    
                                    =
                                    
                                       
                                          I
                                       
                                    
                                    ,
                                 
                              
                           where 
                              
                                 
                                    W
                                 
                              
                              =
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   W
                                                
                                             
                                             1
                                             T
                                          
                                          …
                                          
                                             
                                                
                                                   W
                                                
                                             
                                             n
                                             T
                                          
                                       
                                    
                                 
                                 T
                              
                           , f
                           
                              j
                           (x)=
                           W
                           
                              j
                           
                           x is the j-th feature detector, and 
                              v
                              :
                              
                                 
                                    ℝ
                                 
                              
                              →
                              
                                 
                                    ℝ
                                 
                              
                            is a smoothed version of the L
                           1-norm. We use 
                              v
                              
                                 
                                    s
                                 
                              
                              =
                              
                                 
                                    
                                       s
                                       2
                                    
                                    +
                                    ϵ
                                 
                              
                           , where 
                              ϵ
                              ∈
                              
                                 
                                    ℝ
                                 
                              
                            is a smoothing parameter. ICA has been applied successfully in object recognition tasks, however it has two main limitations that come from its orthogonality constraint. It can not learn overcomplete representations and its training procedure with classical optimization techniques requires to solve an eigenvalue problem at each iteration, making it computationally expensive. Le et al. [50] proposed a soft-version of ICA called reconstruction ICA or RICA, replacing the orthogonality constraint by a reconstruction penalty and defined by:


                           
                              
                                 (5)
                                 
                                    
                                       J
                                       RICA
                                    
                                    (
                                    
                                       
                                          W
                                       
                                    
                                    )
                                    =
                                    
                                       λ
                                       m
                                    
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       m
                                    
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         W
                                                      
                                                   
                                                   T
                                                
                                                
                                                   
                                                      W
                                                   
                                                
                                                
                                                   x
                                                   
                                                      (
                                                      i
                                                      )
                                                   
                                                
                                                −
                                                
                                                   x
                                                   
                                                      (
                                                      i
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                       2
                                       2
                                    
                                    +
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       m
                                    
                                    
                                       ∑
                                       
                                          j
                                          =
                                          1
                                       
                                       n
                                    
                                    v
                                    (
                                    
                                       
                                          
                                             W
                                          
                                       
                                       j
                                    
                                    
                                       x
                                       
                                          (
                                          i
                                          )
                                       
                                    
                                    )
                                    ,
                                 
                              
                           Notice that in this case g(s)=
                           W
                           
                              T
                           
                           s. This model finds a set of feature weights W that reconstruct the original data using near orthogonal bases, while keeping the data representation (W
                           x) sparse. A key advantage of this formulation is that being an unconstrained problem, efficient implementations of gradient-based optimization methods can be applied to find good solutions. Also note that RICA model only requires to adjust the hyperparameter λ, in contrast to SAE where a trade-off between γ, β and ρ should be found.

Inspired by the biological visual system, topographic models seek to organize learned feature detectors such that similar activations are close together, while different ones are set apart. This arrangement follows the visual cortex model where cells have a specific spatial organization and response of neurons change in a systematic way [49]. Particularly, topographic RICA (TICA) builds a square matrix to organize feature detectors in l groups such that adjacent feature detectors activate in a similar proportion to the same stimulus. TICA cost function is given by:


                           
                              
                                 (6)
                                 
                                    
                                       J
                                       TICA
                                    
                                    (
                                    
                                       
                                          W
                                       
                                    
                                    )
                                    =
                                    
                                       λ
                                       m
                                    
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       m
                                    
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         W
                                                      
                                                   
                                                   T
                                                
                                                
                                                   
                                                      W
                                                   
                                                
                                                
                                                   x
                                                   
                                                      (
                                                      i
                                                      )
                                                   
                                                
                                                −
                                                
                                                   x
                                                   
                                                      (
                                                      i
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                       2
                                       2
                                    
                                    +
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       m
                                    
                                    
                                       ∑
                                       
                                          k
                                          =
                                          1
                                       
                                       l
                                    
                                    
                                       
                                          
                                             
                                                
                                                   H
                                                
                                             
                                             k
                                          
                                          
                                             
                                                (
                                                
                                                   
                                                      W
                                                   
                                                
                                                
                                                   x
                                                   
                                                      (
                                                      i
                                                      )
                                                   
                                                
                                                )
                                             
                                             2
                                          
                                          +
                                          ϵ
                                       
                                    
                                    ,
                                 
                              
                           where l is the number of desired groups in the topography, 
                              
                                 
                                    H
                                 
                              
                              ∈
                              
                                 
                                    
                                       
                                          0
                                          ,
                                          1
                                       
                                    
                                 
                                 
                                    l
                                    ×
                                    n
                                 
                              
                            is the topographic organization with 
                              
                                 
                                    
                                       H
                                    
                                 
                                 k
                                 
                                    
                                       j
                                    
                                 
                              
                              =
                              1
                            if the j-th feature detector belongs to the k-th group, 0 otherwise. This model sets H fixed and learns W. Similarly to RICA, TICA is also unconstrained and it can be treated with efficient optimization solvers like L-BFGS [51].

TICA calculates two types of features: basic features, f
                           
                              j
                           (x)=
                           W
                           
                              j
                           
                           x, and invariant features 
                              
                                 f
                                 j
                                 *
                              
                              (
                              x
                              )
                              =
                              
                                 
                                    
                                       
                                          H
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      W
                                                   
                                                
                                                x
                                             
                                          
                                       
                                       2
                                    
                                 
                              
                           . Invariant features group several basic features based on their adjacency in the topographic map.

UFL methods are computationally expensive when they are applied to medium and large images (e.g. 100×100 and larger). The reason is that the number of the model parameters depends, usually non-linearly, on the size of the input, increasing the computational cost of training and prediction. Several efforts [23,52] have dealt with this issue using GPUs implementations. However, under the assumption that histopathology images have particular properties, such as invariance to translation [53], part-based representation strategies such as BOF and convolutional neural networks allow us to move from patch representation to image representation in a scalable way. In the present work, both strategies were evaluated. The following subsections discuss them in detail.

The BOF strategy [54] represents an image as a frequency histogram of an unordered set of individual patches, such set is known as the dictionary. Each patch is represented by an l-dimensional vector. The dictionary 
                              
                                 
                                    D
                                 
                              
                              ∈
                              
                                 
                                    
                                       ℝ
                                    
                                 
                                 
                                    R
                                    ×
                                    l
                                 
                              
                            is constructed by selecting the R most common patches in the whole image collection. The BOF representation comprises three main stages:
                              
                                 •
                                 
                                    Patch extraction and description: a random set of patches from training images is selected to perform on them a feature extraction process.


                                    Dictionary construction: feature vectors are clustered using a clustering algorithm such as K-means. Each cluster centroid corresponds to a visual word in this BOF learned dictionary.


                                    Histogram image representation: to represent a full-sized image, the image is split into a regular grid and each patch is represented using the selected UFL Method. Next, a similarity measure between one patch and each element of D is calculated, the one with the highest value is counted up. This procedure is repeated for each patch in the grid producing a histogram with R bins.

CNN [55] apply local feature detectors as filters over the whole image to measure the correspondence between the image and each learned pattern. Then, an aggregation or pooling function is applied to reduce the representation dimensionality. The CNN architecture is depicted in Fig. 3
                            and described below.

A single feature detector W
                           
                              j
                            is used as a filter over the image to find where this particular pattern is present in the image. Resultant matrix M
                           
                              j
                            is commonly known as feature map and its size is given by imageSize
                           −
                           filterSize
                           +1 for a squared image of size imageSize and a squared filter of size filterSize. Because histopathology images are represented on the RGB color space, resultant feature map has 3 matrices which are aggregated by an element-wise summation. Therefore, an image will be represented by a set of n feature maps (or l groups for TICA), one per each feature detector.

Learning n
                           >3 feature detectors and concatenating resultant feature maps to represent the image lead to greater dimensionality than raw pixels. To reduce the number of parameters required to train the classifier, an aggregation process is done by grouping contiguous regions from each feature map and applying a pooling function like mean or max. This procedure adds local translation invariance to the model. As an example consider an image of m
                           ×
                           m pixels and a set of n feature detectors of p
                           ×
                           p pixels, resultant feature maps after convolution have (m
                           −
                           p
                           +1)×(m
                           −
                           p
                           +1) size. If we set a pool dimension (size of the region that we are going to aggregate) of q (such that p is a multiple of q), then the resultant features are going to be n matrices of (m
                           −
                           p
                           +1)/q
                           ×(m
                           −
                           p
                           +1)/q size. Consequently, the pool size is given by (m
                           −
                           p
                           +1)/q. Note that (m
                           −
                           p
                           +1)/q
                           <
                           m, thus the greater the pool size, the smaller the dimension of the resultant features.

UFL has been successfully applied in different problems (natural language processing, object recognition, speech recognition, among others) using stacked architectures. stacked architectures construct multiple levels of representations by stacking layers over the raw input data. This approach incorporates several advantages like feature re-using, abstraction and invariance leading to more expressive models, i.e. it can represent inputs with a more compact set of features [5]. Hinton et al. [4] proposed a strategy to train in an efficient way stacked architectures based on a greedy layer-wise approach. Only one layer is trained at a time, then the output of a trained layer is used as input to the next layer.

Softmax regression is a multinomial classifier that generalizes the logistic regression binary classifier [56]. This approach uses a convex cost function and therefore a global solution can be found with gradient-based methods. The objective function is given by:


                        
                           
                              (7)
                              
                                 J
                                 
                                    
                                       Φ
                                    
                                 
                                 =
                                 −
                                 
                                    1
                                    m
                                 
                                 
                                    
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             m
                                          
                                          
                                             ∑
                                             
                                                j
                                                =
                                                1
                                             
                                             C
                                          
                                          1
                                          {
                                          
                                             y
                                             
                                                (
                                                i
                                                )
                                             
                                          
                                          =
                                          c
                                          }
                                          log
                                          
                                             
                                                
                                                   e
                                                   
                                                      
                                                         ϕ
                                                         c
                                                      
                                                      
                                                         s
                                                         
                                                            (
                                                            i
                                                            )
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   ∑
                                                   
                                                      l
                                                      =
                                                      1
                                                   
                                                   C
                                                
                                                
                                                   e
                                                   
                                                      
                                                         ϕ
                                                         l
                                                      
                                                      
                                                         s
                                                         
                                                            (
                                                            i
                                                            )
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                                 +
                                 
                                    λ
                                    2
                                 
                                 
                                    
                                       
                                          Φ
                                       
                                    
                                    F
                                    2
                                 
                                 ,
                              
                           
                        where C is the number of classes, 
                           Φ
                           ∈
                           
                              
                                 
                                    ℝ
                                 
                              
                              
                                 C
                                 ×
                                 n
                              
                           
                         are the parameters of the model with ϕ
                        
                           c
                         as the weight vector associated to class c, s
                        (i)
                        =
                        f(x
                        (i)) is the feature vector for sample 
                           
                              x
                              
                                 (
                                 i
                                 )
                              
                           
                           ,
                           
                              y
                              
                                 
                                    i
                                 
                              
                           
                           ∈
                           
                              
                                 ℕ
                              
                           
                         is the label for 
                           
                              s
                              
                                 
                                    i
                                 
                              
                           
                         and λ is the regularization parameter. 1{statement} function outputs 1 if statement is true, 0 otherwise.

Most learning-based computer vision models are black-box models, i.e. there is no knowledge about what features describing an input image contributed to the final decision, or what regions in the image are related with certain semantic concepts. In many computer vision applications, e.g. robotics, this is not an issue since the predictions are directly used by another system module, for which interpretability of the prediction does not add useful information. However, in medical image analysis applications, predictions are usually used as support of diagnostic decisions made by a human expert. In this context, interpretability of the system prediction is a valuable asset that helps to judge the potential contribution of the system prediction to the diagnostic process.

Our proposed framework adds a visualization layer that allows to crack the box getting insights to understand the behavior of the classification framework. Particularly we perform two visualizations. On the one hand, we show the patterns that the filters are looking for, and which of those patterns are the most relevant to make the classification (local pattern visualization). On the other hand and, more interesting to pathologist, we highlight regions that, according to the learned model, are related with cancerous patterns (digital staining). The strategies applied to perform such visualizations are described below.

The analysis of local feature detectors has two main objectives: to know the patterns that such functions are seeking and to measure how relevant they are to the classification process. We propose two main approaches to fulfill these goals:
                              
                                 •
                                 
                                    Feature detectors visualization: finding what is the input x that maximizes the activation of a given feature detector 
                                       
                                          f
                                          j
                                       
                                       
                                          
                                             x
                                          
                                       
                                     is interesting because this would tell us what pattern this function is looking for. Note that such pattern is proportional to the filter W
                                    
                                       j
                                     learned in the first layer because the output depends on the linear combination W
                                    
                                       j
                                    
                                    x. Then, the pattern can be displayed directly simply by reshaping 
                                       
                                          
                                             
                                                W
                                             
                                          
                                          j
                                       
                                       ∈
                                       
                                          
                                             
                                                ℝ
                                             
                                          
                                          d
                                       
                                     to get an image patch with the same size as input x. Higher layers involve more complex transformations (like pooling or convolution) yielding to a non-convex optimization problem [57], nevertheless the solution can be approximated using gradient-based methods to find the optimal stimuli for each feature detector in higher layers.


                                    Feature detectors organization: we are interested on the patterns organization built by topographic regularization. When images are represented using a BOF strategy and TICA as patch representation, the dictionary 
                                       
                                          
                                             D
                                          
                                       
                                       ∈
                                       
                                          
                                             
                                                ℝ
                                             
                                          
                                          
                                             R
                                             ×
                                             l
                                          
                                       
                                     contains combinations of topographic groups. At the same time, the softmax classifier learns the weight matrix 
                                       Φ
                                       ∈
                                       
                                          
                                             
                                                ℝ
                                             
                                          
                                          
                                             C
                                             ×
                                             R
                                          
                                       
                                     such that 
                                       
                                          ϕ
                                          c
                                          
                                             
                                                r
                                             
                                          
                                       
                                     links the r-th visual word in the dictionary to the c-th class. Then, the relevance of a given feature detector W
                                    
                                       j
                                     is estimated by
                                       
                                          
                                             
                                                relevance
                                                
                                                   
                                                      
                                                         
                                                            W
                                                         
                                                      
                                                      j
                                                   
                                                
                                             
                                             =
                                             Φ
                                             
                                                
                                                   D
                                                
                                             
                                             
                                                
                                                   
                                                      H
                                                   
                                                
                                                
                                                   
                                                      j
                                                   
                                                
                                             
                                             ,
                                          
                                       
                                    where 
                                       
                                          
                                             
                                                H
                                             
                                          
                                          
                                             
                                                j
                                             
                                          
                                       
                                    is the j-th column of the grouping matrix H and represents the membership (1 or 0) of the feature detector W
                                    
                                       j
                                     to the groups. To locate what feature detectors are related with cancer, relevance of all feature detectors are calculated and organized with respect to the position in the topographic map.

In the context of automatic image annotation, interpretability means knowing why the system predicted a particular classification for an input image. Interpretability is important when the output of the automatic image annotation system is an asset to support decisions, e.g. in computer assisted medical diagnosis. A histopathology image can have different structures, and it is possible that some of them do not necessarily correspond to the annotated concept of the image. Digital staining is a new mechanism based on thesis that aims to identify high-level concepts in image regions, by highlighting them using information from the classification model and learned feature detectors.

The digital staining process is performed as follows: when SAE and CNN with pool size of 1 (i.e. pool over all the feature map) represent the content of the image, the softmax classifier learns a set of weights 
                              Φ
                              ∈
                              
                                 
                                    
                                       ℝ
                                    
                                 
                                 
                                    C
                                    ×
                                    n
                                 
                              
                            such that 
                              
                                 ϕ
                                 c
                                 
                                    
                                       j
                                    
                                 
                              
                            links the j-th feature detector to the c-th class. Now, let M
                           
                              j
                            be the feature map generated from an image with the feature detector W
                           
                              j
                           , and let ϕ
                           1 be the vector related with the positive class (cancer), then
                              
                                 (8)
                                 
                                    DS
                                    =
                                    
                                       ∑
                                       
                                          j
                                          =
                                          1
                                       
                                       n
                                    
                                    
                                       ϕ
                                       1
                                       
                                          
                                             j
                                          
                                       
                                    
                                    
                                       
                                          
                                             M
                                          
                                       
                                       j
                                    
                                    ,
                                 
                              
                           is the sum of the feature maps weighted by the classifier parameters. High values in the DS matrix correspond to regions of the image that, according to the learned model, are related with cancer. To “dye” the image, the DS matrix is transformed to a colormap where high values are red and low ones are blue distinguishing tumor and non-tumor regions respectively.

The histopathology BCC dataset comprises 1417 images of 300×300 pixels in RGB color that correspond to field of views with a 10× magnification and stained with H&E [46]. These images were manually annotated by an expert pathologist, indicating the presence of any type of BCC (infiltrative/trabecular, morpheiphorm, nodular) and other healthy architectural and morphological patterns (collagen, epidermis, sebaceous glands, eccrine glands, hair follicles and inflammatory infiltration). In summary, the BCC dataset is composed by 899 non-cancerous images and 518 carcinoma images. The Fig. 4
                         shows different example images for healthy and pathological tissue.

Parameter exploration was performed using 5-fold cross validation in a subset of 746 images. The evaluation performance is calculated over an independent test subset with 671 images, reporting the area under the ROC curve (AUC). To the purpose of these experiments images were half-scaled getting images of 150×150 pixels. The number of feature detectors for all UFL methods was set to 400 and were learned with 100,000 patches randomly sampled for each layer. The first layer was trained sampling 8×8 patches from training dataset, whereas the second layer used 2×2 patches.

Since our main goal was to determine how discriminative the learned features are with respect to the state-of-the-art canonical representations, our baseline, we applied three well known set of hand-crafted descriptors in the automatic histopathology image analysis domain that have been successfully applied to several diagnosis tasks [3,41]: BOF representation with (1) discrete cosine transform (DCT) and (2) Haar-based wavelet transform (Haar) as local features, and (3) Haralick features [3,58]. The latter comprises a set of 28 textural features defined by several expressions, some of those related with statistical properties such as correlation, mean, variance among others calculated from the gray-level co-occurrence matrix.

In order to define a unified, and therefore more comparable and reproducible, pipeline for all image representations, the classification method was fixed to the softmax regression classifier for all experiments. It should be noticed that, since the softmax optimization function is strictly convex [59], the L-BFGS algorithm is able to find the global minimum, therefore it is enough to report one run per configuration.

Regardless of the UFL method applied, preprocessing techniques usually have shown performance improvement in object recognition tasks [60]. A common preprocessing step is to remove correlations of raw pixels, allowing to focus on properties that are not dependent on covariances, such as sparseness [49]. This decorrelation has two main motivations: to accentuate differences between input features and accelerate gradient-based learning [61]. This work performs zero-phase component analysis whitening [62] as a preprocessing step for patches in all experiments. Finally, all optimization functions were minimized using L-BFGS, particularly Mark Schmidt's implementation. 
                           2
                        
                        
                           2
                           MinFunc library, http://www.di.ens.fr/∼mschmidt/Software/minFunc.html [accessed 05.02.15].
                         All implementations were done in Matlab, based on the popular Stanford UFLDL Tutorial. 
                           3
                        
                        
                           3
                           Deep learning tutorial, http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial [accessed 05.02.15].
                        
                     

Considering that the H binary matrix in the TICA formulation represents the membership of features to groups in the topography, and that such matrix is fixed, herein, and following the seminal work of [63], the topography is defined as a 2-D torus lattice for both convenience of visualization and to avoid border effects. Specifically, 
                           
                              
                                 H
                              
                           
                           ∈
                           
                              
                                 
                                    
                                       0
                                       ,
                                       1
                                    
                                 
                              
                              
                                 400
                                 ×
                                 400
                              
                           
                         defines 400 groups, each one composed of a set of 9 features arranged in a 3×3 square (displayed in Section 3.4). Conversely, each feature belongs to 9 groups that capture a particular topographic property (e.g. orientation or scale).

This first experiment makes a comparison between canonical features and features learned with UFL methods for local patch representation.Canonical features correspond to theoretical modeling of visual content representation of raw data in other transformed space. Examples of this kind of features are DCT and 2D wavelet transforms which, in the case of images, represent the original spatial information of visual content in other 2D frequency space depending on the chosen basis (cosines or wavelets).

BOF image representation uses a predefined local patch representation. DCT and Haar have shown to be a good alternative when BOF is applied to represent histopathology images in classification tasks [44,64]. This experiment makes a comparison between these canonical features (DCT and Haar) and features learned by UFL methods (SAE, RICA, TICA) for local patch representation.

Here, a CNN architecture was used to represent the image globally while the pool size was explored. Results depicted in Fig. 5
                         show a clear advantage of learned features over canonical features in terms of AUC performance. Also, the results revealed that the learned features have better performance when the pool size is small, obtaining the best performance for a pool size of 1. In this case, feature activations are aggregated for the whole image independently of their location. This means that the learned features are in fact invariant to translation. This behavior is consistent with the nature of the problem, tumor tissue detection, since cancer cell regions may be present in any region of a histopathology image.

Raina et al. [65] proposed a new framework which involves feature learning from unlabeled data that are not coming from the same generating distribution. This is known as self-taught learning. In order to validate the relevance of this approach in the context of BCC tumor detection, we used three different datasets: (1) original BCC dataset of healthy and cancerous tissues of skin stained with H&E and described in Section 4.1, (2) HistologyDS
                           4
                        
                        
                           4
                           The histology image dataset, http://www.informed.unal.edu.co/histologyDS [accessed 05.02.15].
                         dataset that comprises ∼20,000 images of four fundamental healthy tissues from different organs using several staining techniques and (3) STL-10 dataset
                           5
                        
                        
                           5
                           The STL-10 dataset, http://cs.stanford.edu/∼acoates/stl10/ [accessed 05.02.15].
                         with 100,000 natural scene images. 100,000 patches were randomly sampled from each dataset to train an autoencoder and to compare their performance using the CNN global image representation. Results in Table 1
                         show that features learned from histological images outperform the ones learned from natural scene images. Firstly, it seems that learned features from BCC and HistologyDS dataset captured better visual patterns related to dyes, edges of large nuclei in different orientations and perhaps most interestingly small dots related to common/healthy nuclear patterns that do not appear in the other feature sets (see experiment 4.7). Secondly and more interesting, the second best result is achieved by the HistologyDS. This is consistent with the other findings [65], which had shown that the strategy of learning features from other datasets may produce successful results. In this case, this could be explained because the HistologyDS has high visual variability of cell and tissues from different organs.

In this experiment a comparison between 1-layer CNN and BOF for global image representation is done. Similarly to the UFL setups for learning features described in Section 4, the BOF learned a visual dictionary of 400 visual words. For both global image representation strategies the pool size was set to 1. Results are depicted in Fig. 6
                        . It is noteworthy how topographic organization, i.e. TICA, improves performance for both, CNN and BOF representations. These results show that BOF image representation is comparable to CNN image representation only when TICA UFL is used. This is consistent with the results reported in Fig. 5 where TICA shows the best results for UFL.

CNN is the common approach in UFL methods to move from patch representation to image representation. However, BOF representation can be seen as a new second layer of learning representation with some particularities. Instead of using SAE or RICA models, feature learning in this second layer is done by a K-means algorithm over feature maps from the first layer. This new BOF representation is not a dense representation, such as the feature maps in CNN, but a discrete representation with a regular grid extraction over the whole image. Hence, we can see the BOF image representation works like a 2-layer model whereas the CNN works like a 1-layer model. Then, the remaining question of this experiment is how the addition of more layers affects the global image representation in a stacked architecture. This is addressed in the next subsection.

In order to measure how much an additional layer helps, we combine the representation corresponding to the first layer with a second layer by concatenating features from both layers. The second layer learns feature detectors from 100,000 2×2 patches sampled from the first layer with a pooling size of 20×20. Results are shown in Fig. 7
                        , exhibiting an improved performance when features from the first and second layer are combined, obtaining the best overall performance when TICA is used. Features in the second layer correspond to 16×16 regions in the images. Results suggest that useful discriminative patterns are found in these regions, complementing those found in the original 8×8 regions.


                           Fig. 8
                            shows the encoding filters W learned by SAE, RICA and TICA. Notice that, as we detailed in Section 3.4, such filters represent the optimal stimuli for each unit. In general, all of them learn to detect primitive shapes like edges and textures such as it had been previously shown in other domains [29,66]. Particularly, SAE discovers some patterns related to nuclear shapes (highlighted in Fig. 8). On the other hand, TICA learns a set of invariances organized in the topographic map (highlighted in Fig. 8 (right)). These invariances corresponds to translation (blue square), color (red square), scale (yellow square) and rotation (green square). In BCC histopathology images, these are desirable invariances because they take care of the different visual variations produce by different cell arrangements and types of cut (position and rotation), and differences in staining and digitalization (color and scale).


                           Fig. 9
                            shows the relevance of each feature detector over the topographic map using procedure described in Section 3.4. White regions correspond to local detectors highly correlated with the positive class (cancer), while black regions correspond to features associated to the negative class. This figure confirms that groups, learned in an pure unsupervised way, were able to capture relations between features that belongs to the same class. This is a very interesting result since the model is totally unsupervised, i.e., the visual patterns were found using non-labeled samples.


                           Fig. 10
                            shows optimal stimuli for feature detectors in the second layer. These results are visually similar to findings that have been previously reported in other automatic image analysis and interpretation domains like action recognition [29], where it has been remarked that, while first layer captures low level features like edges, it seems second layer detects more complex shapes like corners and mixture of textures [66]. These results encourage the training of deeper architectures in order to discover more abstract concepts.


                           Table 2
                            shows the predictions, as well as the digital staining, for some image samples taken from the test set. First row shows the real class of the input image shown in the second row. Third and fourth rows show prediction and probability of having BCC cancer respectively. Last row displays digital staining by highlighting regions from blue to red scale, being red and blue cancerous and healthy regions respectively.

Digital staining results were shown to an expert pathologist. The main conclusion is that red regions are related with cell proliferation, a common characteristic presents in BCC tumors [67]. Another observation made by the pathologist is related to the borders in the cancerous regions that have higher intensity values. Such zones usually express a peripheral palisade cells arrangement, another relevant criteria during BCC diagnosis. The above statements would suggest that feature learning strategies are able to capture those kind of patterns that are present in the training dataset for both, cancerous and non-cancerous images. These results suggest that the proposed digital staining could be exploited in other tasks like semantic segmentation. However a caveat is that this behavior also manifests in healthy structures where the germinative epidermal cells or glandular tissues are present. This happens because the cell arrangement in cancerous tissues can be similar to the arrangement in epithelial tissues. Also it should be noticed that the preparation and staining processes impact the color intensity if there is more hematoxylin–eosin, making the model (and even the expert) more prone to highlighting healthy regions. Nonetheless, this enhanced image represents an important addition to support the diagnostic process since it allows pathologists to understand why the automated classifier is suggesting a particular classification.

To evaluate the suitability of our method in a practical scenario, we applied it on a independent dataset with larger images (1024×762 pixels). Fig. 11
                            shows two histopathology images and their respective digital staining images predicted by the model. The top image was diagnosed as BCC morpheaform, while the bottom image contains collagen fibers. Results show that the model is able to perform estimations on larger images while preserving its main features to highlight relevant visual patterns like cell proliferation and peripheral palisade as discussed previously.


                        Table 3
                         shows training times and feature extraction times for several configurations. Despite UFL stage requires a considerable amount of time to adjust the model, this process is done just once. To represent a new set of images, learned feature detectors are used out-of-the-box. Additionally, remarkable efforts has been put to exploit high performance computing architectures such as GPUs [68–70] and parallel architectures [52,71,72] improving the performance of representation learning methods and making them able to deal with large datasets and big data scenarios.

All the experiments were conducted using the 2.40GHz Intel® Xeon® Processor E5645 through a Matlab™ programming interface. Results show that SAE models are computationally more expensive than TICA models. This is because W and W′ encoding and decoding matrices are different in SAE, while in TICA, decoding matrix is just the transpose of the encoding matrix W, requiring less parameters to learn. In average, TICA is also faster than SAE to extract features from an image. This is related to the nonlinearity included by the sigmoid activation in the SAE model. The pool size is not very significant in terms of time for feature extraction process. Excluding SAE second layer, any feature extraction model proposed here takes less than 2s to extract features from a new image, making them a feasible method to use in real world scenarios.

Notice that this approach is not directly comparable to traditional features in terms of computational cost, because the feature learning stage is an additional step in the framework. However, once the feature detectors are learned, these may be use with any classification method.


                        Table 4
                         summarizes the systematic evaluation performed in this work in terms of AUC using a conventional softmax classifier. In addition, we also train a linear SVM model for each representation to validate that our findings are consistent, independently of the selected classifier. First, the results show that learning the representation from data yields better performance than using DCT and Haar canonical feature detectors. However, it is noteworthy that the representation learned with the STL-10 dataset performed worst. This may be explained by the fact that useful patterns for differentiating cancer and non-cancer may not be present in a set of natural images. Thus, the model can not represent in a proper way the content of a histopathology image. Second, TICA model adds invariance properties that significantly improve discriminant capabilities of the classifier. Finally, the visualization of feature detectors in the second layer suggests that the proposed framework is able to learn more abstract concepts when stacked architectures are trained. As a consequence, our best configuration was obtained with the TICA model in a two-layer architecture combining features of both layers.

@&#CONCLUSIONS@&#

This paper presented a novel UFL framework for BCC detection, which was systematically evaluated. This framework integrated UFL, supervised learning and visual interpretability for histopathology image analysis. A comprehensive evaluation of different UFL strategies for the new framework was performed on BCC histopathology images. The evaluation shows that, in this particular problem, learned features outperform state-of-the-art canonical representations.

Feature learning for histopathology image representation is entirely performed in an unsupervised way allowing to discover discriminant patterns in the collection without using prior knowledge from pathologists or hand-engineered features. Consistently discovered patterns are related to visual appearance of tumoral cells or nuclei. However these patterns could be confused with cell proliferation patterns in healthy tissues. Others remarkable findings were that learned features by the model provide translation, rotation, scaling and color invariances, which are desirable properties for this kind of images given their visual particularities produced by different cell arrangement, types of cuts, acquisition, staining and digitalization processes.

The best results were obtained using stacked architectures. This yields hierarchical features which simulate brain's visual cortex by building primitive structures like edges, and then building more complex shapes like corners. These higher-level features improved classification performance in all methods.

The proposed framework was evaluated in a supervised learning task to distinguish between cancer and non-cancer tissues in a BCC dataset. The framework outperformed the canonical state-of-the-art representation by 7% in terms of AUC achieving the best results by TICA combined layers obtaining 98.1% in test set.

The final stage of the framework includes a noteworthy visualization model to highlight tissue regions that the model finds to be related to the appearance of cancerous tissue, working like a digital staining. This novel technique showed that a trained feature learning model can provide a visual interpretability layer which is able to explain the automatic classification decision by detecting and highlighting textures and discriminant visual structures over the image. Indeed, these resultant salient maps of digital staining are potentially useful to be used as image-based biomarkers, a valuable tool for pathologist diagnosis support and cancer research.

Future work includes further evaluation using deeper architectures, supervised feature learning and other types of cancer images using larger whole-slide image collections.

@&#ACKNOWLEDGMENTS@&#

This work was partially funded by projects “Multimodal Image Retrieval to Support Medical Case-Based Scientific Literature Search”, ID R1212LAC006 by Microsoft Research LACCIR, “Diseño implementación de un sistema de cómputo sobre recursos heterogéneos para la identificación de estructuras atmosféricas en predicción climatológica” number 1225-569-34920 through Colciencias contract number 0213-2013 and “Convocatorial del programa nacional de proyectos para el fortalecimiento de la investigación, la creación y la innovación en posgrados de la Universidad Nacional de Colombia 2013–2015” with proposal number 18722. Cruz-Roa also thanks Colciencias for its support through a doctoral grant in call 528/2011. Arevalo also thanks Colciencias for its support through a doctoral grant in call 617/2013. The authors also thank for K40 Tesla GPU donated by NVIDIA and which was used for some feature learning experiments.

@&#REFERENCES@&#

