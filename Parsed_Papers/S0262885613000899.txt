@&#MAIN-TITLE@&#Using texture to complement color in image matting

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           The texture information is leveraged to complement color in image matting.


                        
                        
                           
                           The proposed method combines color and texture information in matting process.


                        
                        
                           
                           The proposed combined method presents the best among current matting methods.


                        
                        
                           
                           The proposed method has first ranks with respect to MSE and Gradient error.


                        
                        
                           
                           Several experiments are carried out to reveal potential power of texture in matting.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Alpha matting

Texture matte

@&#ABSTRACT@&#


               
               
                  Current image matting methods based on color sampling use color to distinguish between foreground and background pixels. However, they fail when the corresponding color distributions overlap. Other methods that define correlation between neighboring pixels based on color aim to propagate the opacity parameter α from known pixels to unknown pixels. However, strong edges of textured regions may block the propagation of α. In this paper, a new matting strategy is proposed that delivers an accurate matte by considering texture as a feature that can complement color even if the foreground and background color distributions overlap and the image is a complex one with highly textured regions. The texture feature is extracted in such a way as to increase distinction between foreground and background regions. An objective function containing color and texture components is optimized to find the best foreground and background pair among a set of candidate pairs. The effectiveness of proposed method is compared quantitatively as well as qualitatively with other matting methods by evaluating their results on a benchmark dataset and a set of complex images. The evaluations show that the proposed method presented the best among state of the art matting methods.
               
            

@&#INTRODUCTION@&#

Digital matting is the accurate extraction of foreground regions from images and videos and is useful for image and video editing operations. The compositing equation is given by [1]
                     
                        
                           (1)
                           
                              
                                 
                                    I
                                    p
                                 
                                 =
                                 
                                    α
                                    p
                                 
                                 
                                    F
                                    p
                                 
                                 +
                                 
                                    
                                       1
                                       −
                                       
                                          α
                                          p
                                       
                                    
                                 
                                 
                                    B
                                    p
                                 
                              
                           
                        
                     where Fp
                      and Bp
                      are the foreground and background colors of pixel p which are linearly combined using αp
                      to represent its observed color Ip
                     . The alpha values lie in [0, 1] with pixels having α
                     =1 and α
                     =0 belonging to foreground and background, respectively. For a three-channel color image, the digital matting task (also known as alpha matting) involves estimation of seven unknowns from three compositing equations for each pixel. Typically, the solution space is constrained through the availability of trimaps, which partition the image into three regions — known foreground, known background and unknown (i.e., consists of a mixture of foreground and background colors). Trimaps are usually user-defined, but can also be guided [2] or be generated automatically [3]. Assumptions on image statistics are also used to constrain the solution of Eq. (1) 
                     [4–6]. The algorithm for matte estimation in this paper is based on trimaps. Once the alpha matte is estimated, an editing operation could combine the extracted foreground image with a new background using the compositing equation.

The accuracy of alpha matte estimation is largely dependent on the accuracy of the trimap. If the trimap is coarse, i.e., it partitions the foreground and background sparsely, then the quality of alpha matting deteriorates. One of the main approaches to alpha matting is to select best known samples of foreground and background regions that contribute to an accurate matte estimation. The α value for an unknown pixel p is calculated from the selected foreground and background samples by
                        
                           (2)
                           
                              
                                 
                                    α
                                    p
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                I
                                                p
                                             
                                             −
                                             B
                                          
                                       
                                       
                                          
                                             F
                                             −
                                             B
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   F
                                                   −
                                                   B
                                                
                                             
                                          
                                       
                                       2
                                    
                                 
                                 .
                              
                           
                        
                     
                  

The other approach is to assume that neighboring pixels are correlated under some image statistics. This involves the estimation of α over a small neighborhood followed by propagation of the α towards unknown pixels guided by gradient of the image. This approach overcomes the disadvantage of a coarse trimap from which reliable samples of foreground and background colors cannot be obtained for unknown pixels that are far away from the trimap. Yet another approach casts the matting problem as an optimization problem where the cost function consists of a data term representing color sampling and a smoothness term representing alpha propagation. Thus, they can be seen as a combination of sampling and α-propagation methods.

Color sampling based matting methods rely on color values of some selected known pixels to estimate the alpha values of unknown pixels. These approaches work well when dealing with images that have well separated foreground and background color distributions. However, they fail to pull alpha matte when the foreground and background color distributions are close together and have some overlap since the color feature cannot discriminate between foreground and background colors anymore. Also, α-propagation based matting methods fail to estimate alpha accurately when dealing with complex images because textured background or foreground of complex images may contain strong edges that block the propagation of alpha. The above two drawbacks are illustrated in Fig. 1
                     . The alpha mattes extracted by color sampling approach of Bayesian matting [7] and shared matting [8], alpha propagation based approach of closed-form matting [4] and the combined approach of robust matting [9] (which takes advantage of color sampling and alpha propagation techniques) show that parts of the book are also considered as foreground as seen in the zoomed version of window 2. Due to the similarity between the color of hair and part of the letters in window 2, which causes the local foreground and background color distributions to overlap, even careful selection of known foreground and background pairs results in some portions of the background being wrongly estimated as belonging to the matte. At the same time, strong edges in the background prevent propagation of alpha, which also contributes to the erroneous matte. In fact, the edges corresponding to some of the letters in the title of the book are visible in these methods.

In this paper, we propose to alleviate the above problems of existing matting methods through two contributions: (i) The first one is the development of a color sampling based alpha matting algorithm where several properties of potential candidates for foreground and background are considered to select a suitable foreground–background pair from which the alpha matte can be calculated. (ii) The second contribution is an evaluation of the role that texture plays in an accurate estimation of alpha. As mentioned, color features fail when there is overlap between foreground and background distributions. In such cases, we show that the texture feature can be used to discriminate between foreground and background. When the alpha matte from the texture features is combined with that obtained from the color features, we achieve a more accurate estimate of the unknown alpha matte. We show the result of our combined matting approach in Fig. 1, where much of the background that appeared in the other methods have been eliminated. Compared to shared sampling [8], our method has performed well as seen in windows 1 and 2, where more of the book region has been correctly identified as belonging to the background.

The paper is organized as follows: in Section 2, we present a brief review of some important matting methods. In Section 3, a new color based matting method is proposed and the role of texture in alpha matting is studied in Section 4. The combination of color and texture features to arrive at the final alpha matte is discussed in Section 5. Experimental results are presented in Section 6 and finally conclusions are presented in Section 7.

@&#RELATED WORK@&#

A comprehensive survey on image and video matting is presented in [10], where the various techniques have been categorized into color sampling methods, alpha propagation methods and their combination. Statistically, neighboring pixels with similar color often have similar α values. Color sampling based methods use this local correlation between unknown pixels and their nearby known foreground and background pixels. A set of known foreground and background candidate samples are selected and an analysis of their spatial, photometric, and textural properties are carried out to identify the best foreground–background pair. Different sample collection approaches are used. Robust matting [9] collects known samples that are spatially close to unknown pixels while shared matting [8] collects samples that lie along rays that are emanated from unknown pixels. Global sampling matting [11] captures all known boundary samples to construct the set of F–B pairs for unknown pixels. The earliest sampling approaches were the blue screen matting idea of Mishima [12] and the interpolation of color distributions from known foreground and background regions [6]. Bayesian matting uses local oriented Gaussian distributions to construct foreground and background color models and formulates alpha estimation in a Bayesian framework to be solved using maximum a posteriori (MAP) technique [7]. Local color based matting methods work well when the trimap is accurate and the unknown region is a narrow band around known regions. For coarse trimaps, an iterative matting approach is proposed in [3] where a user scribbles an initial (coarse) trimap and the algorithm builds a global color model by training Gaussian Mixture Models (GMMs) on known foreground and background colors from which samples are drawn. The known regions are then extended gradually based on a belief propagation framework. In robust matting [9], high confidence foreground–background pairs that linearly explain the color of pixels are used to estimate alpha matte. This idea is extended by [8,13,11] with respect to spatial and gradient properties of the image. Once the best foreground (F) and background (B) samples are selected, α can be computed using Eq. (2).

Matting methods based on alpha propagation use affinities of neighboring pixels to propagate alpha values of known pixels toward unknown ones. In [14], an extension of spectral segmentation is proposed to generate a basis set of fuzzy matting components. Spectral segmentation obtains an unsupervised segmentation of the image into hard components based on the smallest eigenvectors of the graph Laplacian matrix of the image. A random walk on a graph whose edge weights reflect affinity between pixels is used to extract the matte in [15]. For every unknown pixel, the probability of a random walker starting from an unknown pixel to reach foreground before background indicates its alpha value and is obtained by solving a system of linear equations. In [16], the alpha value is based on a weighted geodesic distance covered by a random walker traveling from an unknown pixel to a foreground pixel. Assumption of local color smoothness in foreground and background regions is made by Poisson matting [17] to estimate gradient of alpha matte using image gradient. A closed form solution for alpha matting is proposed in [4] by minimizing a cost function based on local smoothness assumptions on foreground (F) and background (B) colors. The authors show analytically that it is possible to remove F and B from the cost function and define it as a quadratic function based on α. The alpha matting problem is treated as semi-supervised learning task in [18] in which global and local learning processes model the dependencies between alpha and color in the neighborhood of a pixel.

The color sampling and alpha propagation approaches have been combined together to form a third category of matte extraction techniques. Here, the matte estimation is formulated as an optimization problem consisting of a data term and a smoothness term. The iterative matting technique [3] that was referred to earlier can also be seen as belonging to this category since the matte is modeled as a Markov random field and an energy function is minimized using loopy belief propagation. In robust matting [9], the optimization is solved as a graph labeling problem with the data term of the energy function determined by high confidence foreground–background pairs obtained from a robust sampling scheme and the smoothness term is similar to the cost function of closed form matting. This idea is improved by Rhemann et al. [19] and a new confidence metric is proposed to weight the data term of energy function with respect to smoothness term for every pixel and energy function is minimized by solving sparse set of linear equations. In [20], the prior probability of alpha matte is modeled as a convolution of a high-resolution binary segmented image with the point spread function of the imaging process and the result is applied to [13] to obtain an improved matte. In [11], all known foreground and background pixels on the unknown region's boundary are used to construct sets of global F–B samples for unknown pixels. A simple cost function and an efficient random search are used to find the best F–B pair. The estimated alpha matte is used as the data term in a global optimization problem to refine the α estimated from the F–B pairs and the matting Laplacian matrix [4] is the smoothness term.

The color-based sampling techniques discussed above assume smoothness of color in a neighborhood. Moreover, since they rely only on color, if the local distributions of foreground and background color overlap, the methods tend to fail.

The problem is clearly shown in Fig. 2
                     . Foreground and background regions of the image (Fig. 2a) have similar color distribution as shown in Fig. 2b. Four foreground–background pairs (F
                     1, B
                     1), (F
                     2, B
                     2), (F
                     3, B
                     2) and (F
                     3, B
                     1) are collected to estimate α of unknown pixel p with observed gray value 100. The unknown pixel p is close to foreground component of pair (F
                     1, B
                     1) and its α is estimated as 0.75 while it is also close to background component of (B
                     2, F
                     2) and its α is estimated as 0.25 and the estimated α by other F–B pairs are out of the range [0, 1]. In such cases, color feature is unable to select appropriate samples and the question is which pair can accurately estimate α? The proposed method uses texture feature along with color to alleviate the problem by minimizing an objective function that consists of a component that is especially targeted to handle overlapped distributions.

Alpha propagation methods assume that neighboring pixels are correlated under some image statistics. The strong edges of high textured regions may block the propagation of alpha since the correlations are defined based on color similarities. Therefore we proposed to use texture features together with color information to overcome this problem. The use of texture features for matting has not been studied extensively. In [21], good background samples for unknown regions are found by texture synthesis using known background and observed color of unknown pixel. A similar procedure is carried out to gather good foreground samples. With this collection of F and B samples, an alpha estimation method is proposed. While this method uses texture information of known pixels to synthesize texture in unknown regions, our approach is to extract texture features that will aid the color matting method proposed in this paper by delineating foreground and background regions.

Although previous combined matting methods are successful to some degree in complementing poor sample selection methods with better alpha propagation strategies and vice versa, they may not be successful in challenging images like the one in Fig. 1 where robust matting does not perform as well as the proposed matting method that combines color and texture features.

@&#PROPOSED METHOD@&#

The new matting method based on color and texture feature is proposed to illustrate the performance of texture in complementing color to estimate a more accurate matte for textured regions that have foreground and background regions with similar color distributions. The proposed method collects color and texture samples for every unknown pixel and then estimates texture matte by using texture features; the estimated texture matte is then used with color features to estimate a more accurate alpha for unknown pixels. In this context, first the sample selection is presented and then objective function to choose the best F–B pair followed by Laplacian post-processing is described and finally texture feature extraction and its combination with color feature in objective function are explained.

The objective of the proposed color sampling scheme is to ensure that the choice of foreground and background samples are judiciously chosen so that the estimated α when used in conjunction with α extracted from texture feature results in an improved matte. Our sampling approach is inspired by the sample gathering stage of [8], which performs best in a benchmark database [22].

The input image is divided into blocks of size 5×5 and a mixture of Gaussian distributions is assigned to each block to model its color distribution. A set of color models for unknown blocks are constructed by choosing models from some known blocks. This process is illustrated in Fig. 3
                        .

For each unknown block, m line segments are drawn at angular increments of θ
                        
                           inc
                        
                        =2π/m with the first line segment having an orientation of 0°. These line segments choose the model of the first known block that they encounter. Thus, m line segments collect at most m color models of background blocks and at most m color models of foreground blocks. The selected blocks are shown in Fig. 3 for eight line segments. In the experiments, we use 20 line segments to construct a set of foreground and background color models for each unknown block. Each component in a model is then sampled individually and the generated samples are paired together to construct a set of foreground–background (F–B) pairs. Such a sampling ensures that F–B pairs are obtained for every block so that samples are generated even for small clusters, which might be missed if random sampling were employed. This method of sample generation is different from [8] in that in the latter, the F–B pair is formed from each pixel as opposed to sampling from a distribution in a block. The obvious advantage in the proposed method is that it is not affected by noisy pixels since the information in an entire block is captured in the distribution from which samples are generated.

Once the set of F–B pairs for unknown blocks are collected, the task is to select the best F–B pair among the set that can best represent the true foreground and background colors for each pixel in the block and estimate its α using Eq. (2). The selection is done through a brute-force optimization of an objective function based on photometric and spatial image statistics. It consists of three main parts as follows:
                           
                              (3)
                              
                                 
                                    
                                       O
                                       p
                                       1
                                    
                                    
                                       
                                          F
                                          i
                                       
                                       
                                          B
                                          j
                                       
                                    
                                    =
                                    
                                       D
                                       p
                                    
                                    
                                       
                                          F
                                          i
                                       
                                       
                                          B
                                          j
                                       
                                    
                                    ×
                                    
                                       E
                                       p
                                    
                                    
                                       
                                          F
                                          i
                                       
                                       
                                          B
                                          j
                                       
                                    
                                    ×
                                    
                                       S
                                       p
                                    
                                    
                                       
                                          F
                                          i
                                       
                                       
                                          B
                                          j
                                       
                                    
                                 
                              
                           
                        where the D indicates chromatic distortion and E and S use spatial and color statistics of the image to find high quality F–B pair for every pixel. Before discussing each of these terms, we present some notations related to an unknown pixel p. The block of pixel p is Blkp
                         and set of generated F–B pairs for the block is denoted by 
                           
                              S
                              
                                 Bl
                                 
                                    k
                                    p
                                 
                              
                              FB
                           
                        . The ith foreground samples and jth background samples are Fi
                         and Bj
                        , respectively. The set of foreground and background components of F–B pairs are 
                           
                              S
                              
                                 Bl
                                 
                                    k
                                    p
                                 
                              
                              F
                           
                         and 
                           
                              S
                              
                                 Bl
                                 
                                    k
                                    p
                                 
                              
                              B
                           
                         respectively. The number of F–B pairs and the number of foreground and background components are 
                           
                              N
                              
                                 Bl
                                 
                                    k
                                    i
                                 
                              
                              FB
                           
                        , 
                           
                              N
                              
                                 Bl
                                 
                                    k
                                    i
                                 
                              
                              F
                           
                         and 
                           
                              N
                              
                                 Bl
                                 
                                    k
                                    i
                                 
                              
                              B
                           
                        , respectively.


                        D: The linear model of compositing equation successfully explains the color of a pixel as a convex combination when the estimated color is close to its observed color. Hence, D is defined to account for chromatic distortion. For a certain F–B pair, the estimated color of the unknown pixel is obtained by the compositing equation. The distortion between estimated color and observed color is given by
                           
                              (4)
                              
                                 
                                    
                                       D
                                       p
                                    
                                    
                                       
                                          F
                                          i
                                       
                                       
                                          B
                                          j
                                       
                                    
                                    =
                                    
                                       e
                                       
                                          −
                                          
                                             
                                                
                                                   I
                                                   p
                                                
                                                −
                                                
                                                   
                                                      α
                                                      
                                                         F
                                                         i
                                                      
                                                      +
                                                      
                                                         
                                                            1
                                                            −
                                                            α
                                                         
                                                      
                                                      
                                                         B
                                                         j
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where Ip
                         is observed color of pixel p. It has a high value for F–B pairs for which the estimated color is close to the observed color.


                        E: This term involves the distance between an F–B pair and the unknown pixel at block resolution in the spatial domain. If an F–B pair can easily propagate its information to the unknown pixel due to its low spatial distance, then E should be high. Thus,
                           
                              (5)
                              
                                 
                                    
                                       
                                          
                                             E
                                             p
                                          
                                          
                                             
                                                F
                                                i
                                             
                                             
                                                B
                                                j
                                             
                                          
                                          =
                                          
                                             e
                                             
                                                
                                                   −
                                                   
                                                      
                                                         Bl
                                                         
                                                            k
                                                            
                                                               F
                                                               i
                                                            
                                                         
                                                         −
                                                         Bl
                                                         
                                                            k
                                                            p
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         1
                                                         
                                                            N
                                                            
                                                               Bl
                                                               
                                                                  k
                                                                  p
                                                               
                                                            
                                                            F
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         ∑
                                                         
                                                            
                                                               F
                                                               k
                                                            
                                                            ∈
                                                            
                                                               S
                                                               
                                                                  Bl
                                                                  
                                                                     k
                                                                     p
                                                                  
                                                               
                                                               F
                                                            
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         Bl
                                                         
                                                            k
                                                            
                                                               F
                                                               k
                                                            
                                                         
                                                         −
                                                         Bl
                                                         
                                                            k
                                                            p
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          ×
                                          
                                             e
                                             
                                                
                                                   −
                                                   
                                                      
                                                         Bl
                                                         
                                                            k
                                                            
                                                               B
                                                               j
                                                            
                                                         
                                                         −
                                                         Bl
                                                         
                                                            k
                                                            p
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         1
                                                         
                                                            N
                                                            
                                                               Bl
                                                               
                                                                  k
                                                                  p
                                                               
                                                            
                                                            B
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         ∑
                                                         
                                                            
                                                               B
                                                               k
                                                            
                                                            ∈
                                                            
                                                               S
                                                               
                                                                  Bl
                                                                  
                                                                     k
                                                                     p
                                                                  
                                                               
                                                               B
                                                            
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         Bl
                                                         
                                                            k
                                                            
                                                               B
                                                               k
                                                            
                                                         
                                                         −
                                                         Bl
                                                         
                                                            k
                                                            p
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 Bl
                                 
                                    k
                                    
                                       F
                                       i
                                    
                                 
                                 −
                                 Bl
                                 
                                    k
                                    p
                                 
                              
                           
                         is the Euclidean distance between block of foreground sample Fi
                         and the block containing the unknown pixel p in spatial domain and indicates the spatial cost required for Fi
                         to reach the pixel.


                        S: This term is defined with respect to main problem of current color sampling based approaches, viz., the overlapping of foreground and background distributions. It is biased towards those F–B pairs that come from well separated distributions and is formulated as
                           
                              (6)
                              
                                 
                                    S
                                    
                                       
                                          F
                                          i
                                       
                                       
                                          B
                                          j
                                       
                                    
                                    =
                                    
                                       e
                                       
                                          
                                             
                                                −
                                                1
                                             
                                             
                                                d
                                                
                                                   
                                                      F
                                                      i
                                                   
                                                   
                                                      B
                                                      j
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where
                           
                              (7)
                              
                                 
                                    d
                                    
                                       
                                          F
                                          i
                                       
                                       
                                          B
                                          j
                                       
                                    
                                    =
                                    
                                       
                                          
                                             μ
                                             
                                                F
                                                i
                                             
                                          
                                          −
                                          
                                             μ
                                             
                                                B
                                                j
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            N
                                                            
                                                               B
                                                               i
                                                            
                                                         
                                                         −
                                                         1
                                                      
                                                   
                                                   
                                                      σ
                                                      
                                                         B
                                                         i
                                                      
                                                      2
                                                   
                                                   +
                                                   
                                                      
                                                         
                                                            N
                                                            
                                                               F
                                                               i
                                                            
                                                         
                                                         −
                                                         1
                                                      
                                                   
                                                   
                                                      σ
                                                      
                                                         F
                                                         i
                                                      
                                                      2
                                                   
                                                
                                                
                                                   
                                                      N
                                                      
                                                         B
                                                         i
                                                      
                                                   
                                                   +
                                                   
                                                      N
                                                      
                                                         F
                                                         i
                                                      
                                                   
                                                   −
                                                   2
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        is the Cohen's d value of distributions [23] that generated Fi
                         and Bj
                        . 
                           
                              μ
                              
                                 F
                                 i
                              
                           
                        , 
                           
                              σ
                              
                                 F
                                 i
                              
                              2
                           
                         and 
                           
                              N
                              
                                 F
                                 i
                              
                           
                         are mean, variance and population size of distribution that generated sample Fi
                        . Cohen's d value is inversely proportional to the overlap of distributions. When F–B pairs are generated by highly overlapped foreground and background distributions (d
                        →0) then S
                        →0 to reduce their influence. When they are generated from well separated distributions their influence is increased since S
                        →1.

The alpha matte obtained by estimating α for each pixel using the best (F, B) pair in Eq. (2) is further refined to obtain a smooth matte by considering correlation between neighboring pixels. In particular, we adopt the post-processing method of [8] where a cost function consisting of the data term 
                           
                              α
                              ^
                           
                         and a confidence value f together with a smoothness term consisting of the matting Laplacian [4] is minimized with respect to α. The confidence value is the value of the objective function in Eq. (3) for the selected (F, B) pair. The cost function is given by [8]
                        
                           
                              (8)
                              
                                 
                                    α
                                    =
                                    argmin
                                    
                                       α
                                       T
                                    
                                    Lα
                                    +
                                    λ
                                    
                                       
                                          
                                             α
                                             −
                                             
                                                α
                                                ^
                                             
                                          
                                       
                                       T
                                    
                                    D
                                    
                                       
                                          α
                                          −
                                          
                                             α
                                             ^
                                          
                                       
                                    
                                    +
                                    γ
                                    
                                       
                                          
                                             α
                                             −
                                             
                                                α
                                                ^
                                             
                                          
                                       
                                       T
                                    
                                    
                                       Γ
                                       ^
                                    
                                    
                                       
                                          α
                                          −
                                          
                                             α
                                             ^
                                          
                                       
                                    
                                 
                              
                           
                        where λ is a large weighting parameter compared to the estimated alpha 
                           
                              α
                              ^
                           
                         and its associated confidence f while γ is a constant (10−1) that indicates the relative importance of data and smoothness terms. D is a diagonal matrix with values 1 for known foreground and background pixels and 0 for unknown ones, while diagonal matrix 
                           
                              Γ
                              ^
                           
                         has values 0 for known foreground and background pixels and f (the confidence value) for unknown pixels.

As mentioned, when there is significant overlap in the color distribution of foreground and background regions, current color sampling methods may fail to estimate a reliable matte. Moreover, in alpha propagation based methods, since the correlation between neighboring pixels is defined based on color similarities, strong edges of textured regions may block the propagation of alpha precluding the extraction of an accurate matte. In such cases, color information does not suffice and hence, we turn to texture features that may define the correlation between neighbors more robustly. Thus, our solution is to extract a texture feature in such way as to increase the discrimination between foreground and background regions. Any of the texture feature extraction techniques available in the literature can be used for the purpose [24].

The proposed texture feature is generated from chromatic and structural content in an image. Texture properties are considered over an image patch in which the chromatic information is as homogeneous as possible and only the dominant color in the patch is retained. To this end, we smooth the image. However, we need to ensure that strong edges that participate in the texture do not get smoothed inadvertently. Hence, the first step is the application of a set of three edge-preserving bilateral filters to obtain high, medium and low smoothing. The parameters associated with the bilateral filters are the Gaussian window sizes (5, 15, 25), geometric spread (10, 15, 25) and photometric spread (1, 1.5 and 3). The absolute difference between the original image and the bilateral filtered images is denoted by I
                     
                        i
                     
                     
                        AbsDiff
                     
                     =|I
                     0
                     −
                     I
                     
                        i
                     
                     
                        BF
                     | for i
                     =1, 2 and 3 and its intensities are scaled to [0, 1]. The final smoothed image that removes details of color variation is obtained as
                        
                           (9)
                           
                              
                                 
                                    I
                                    Smooth
                                 
                                 =
                                 
                                    w
                                    0
                                 
                                 ⊗
                                 
                                    I
                                    0
                                 
                                 +
                                 
                                    1
                                    3
                                 
                                 
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       3
                                    
                                    
                                       
                                          
                                             
                                                I
                                                i
                                                AbsDiff
                                             
                                             ⊗
                                             
                                                I
                                                i
                                                BF
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     where 
                        
                           
                              w
                              0
                           
                           =
                           1
                           −
                           
                              
                                 
                                    1
                                    3
                                 
                                 
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       3
                                    
                                    
                                       
                                          I
                                          i
                                          AbsDiff
                                       
                                    
                                 
                              
                           
                        
                      and ⊗ is the element wise product operator. w
                     0 takes values in the range [0, 1] and indicates how much information is taken from the input and bilateral filtered images with respect to absolute differences between them.

The original image of Net along with its trimap and three bilateral smoothed images are shown in the first row of Fig. 4
                     . The absolute differences between the original image and the bilateral filtered images are shown in the first three columns of Fig. 4 in the second row. The final smoothed image in which, color values of pixels come from both the original as well as the bilateral smoothed images is shown in fourth column of Fig. 4 in second row. If the difference between original and bilateral filtered images is small, the contribution to the smoothed image comes from the former, otherwise the contribution is from the bilateral filtered images. Therefore, the color details in the smoothed image are replaced by the overall chromatic information in the texture. The chromatic component of the texture feature vector for a pixel is, thus, given by
                        
                           (10)
                           
                              
                                 F
                                 
                                    V
                                    
                                       S
                                       1
                                    
                                    Texture
                                 
                                 =
                                 
                                    
                                       I
                                       Smooth
                                    
                                    
                                       I
                                       1
                                       AbsDiff
                                    
                                    
                                       I
                                       2
                                       AbsDiff
                                    
                                    
                                       I
                                       3
                                       AbsDiff
                                    
                                 
                              
                           
                        
                     which is a 12 dimensional vector.

The structural content of the texture is obtained through a Haar wavelet decomposition of the image into 4 sub-images called Approximation (IA
                     ), Horizontal (IH
                     ), Vertical (IV
                     ) and Diagonal (ID
                     ) details. Each of the sub-images is resized to that of the original image and the mean of the coefficients over a neighborhood of size 3×3 is computed for each pixel. Similarly, the variance of the coefficients over a similar neighborhood is calculated along four directions — horizontal, vertical, major and minor diagonals. Thus the structural component of the texture feature vector for a pixel is given by
                        
                           (11)
                           
                              
                                 F
                                 
                                    V
                                    
                                       S
                                       2
                                    
                                    Texture
                                 
                                 =
                                 
                                    
                                       I
                                       
                                          k
                                          i
                                       
                                       var
                                    
                                    
                                       I
                                       k
                                       mean
                                    
                                 
                                 ,
                                 
                                 k
                                 =
                                 
                                    A
                                    H
                                    V
                                    D
                                 
                                 ,
                                 
                                 i
                                 =
                                 1
                                 ,
                                 …
                                 ,
                                 4
                                 ,
                              
                           
                        
                     and its dimension is 20. The chromatic content and the structural content in a texture are concatenated to form a 32-dimensional texture feature vector given by
                        
                           (12)
                           
                              
                                 F
                                 
                                    V
                                    Texture
                                 
                                 =
                                 
                                    
                                       F
                                       
                                          V
                                          
                                             S
                                             1
                                          
                                          Texture
                                       
                                       ,
                                       F
                                       
                                          V
                                          
                                             S
                                             2
                                          
                                          Texture
                                       
                                    
                                 
                                 .
                              
                           
                        
                     
                  

In order to generate the texture features, we reduce the 32-dimensional feature vector down through a two-stage dimensional reduction process. The first stage involves the application of principal component analysis (PCA) where 95% of the information is retained. In the second stage, we wish to take advantage of the label information available from the trimap in terms of known foreground and background color values of pixels and apply Linear Discriminant Analysis (LDA) to further reduce the dimensions. Thus, the foreground and background pixels are clustered into m and n clusters, respectively, where the number of clusters are optimal and are determined by the Akaike Information Criterion (AIC). The top projections (eigenvectors corresponding to three largest eigenvectors) that represent the best separation between clusters constitutes the reduced dimensions for known data in such way that 90% of information is retained. The selected eigenvectors in PCA and projections in LDA are used to reduce the dimensions of texture feature vector for unknown pixels to ensure that the same dimension reduction process is applied for both known and unknown pixels. The extracted texture feature of the Net image is shown in last column of Fig. 4 in second row that indicates how successfully chromatic and structural information are combined and transformed in such a way to increase distinctions between foreground and background regions.


                     Fig. 5
                      shows how texture feature can discriminate between foreground and background with overlapped color distributions. The original images with foreground and background distributions in red, green and blue channels are shown in Fig. 5a, b, c and d, respectively. The images in Fig. 5a are taken from standard training set of image proposed by [22]. The first dimensions of extracted texture features and their histograms are shown in Fig. 5e and f and as expected, the distributions of foreground and background are more separated in texture space than color space. Foreground and background regions of first and second images have similar color distributions as shown in first and second rows of Fig. 5b, c, d. The first dimension of extracted texture feature is shown in Fig. 5e while texture distributions of foreground and background are more distinct as shown in Fig. 5e. The discussion is same for rest of images in the figure. The discrimination between foreground and background regions in Fig. 5e and f indicates the performance of proposed texture feature in dealing with natural images. The proposed method uses discriminative power of texture along with color information to estimate robust matting when foreground and background have overlapped color distributions.

The combination of texture and color features for alpha matting entails inclusion of an additional parameter that tests for compatibility of the color matte with previously estimated texture matte αT
                     .


                     Tp
                     : Although the texture matte by itself is not accurate over the entire image, those pixels for which it is correct should be included in the combination processes with a higher weighting. The correctness is measured by its compatibility with the color based α given by
                        
                           (13)
                           
                              
                                 
                                    T
                                    p
                                 
                                 
                                    
                                       F
                                       i
                                    
                                    
                                       B
                                       j
                                    
                                    
                                       α
                                       p
                                       T
                                    
                                 
                                 =
                                 
                                    α
                                    p
                                    T
                                 
                                 ×
                                 
                                    α
                                    p
                                 
                                 +
                                 
                                    
                                       1
                                       −
                                       
                                          α
                                          p
                                          T
                                       
                                    
                                 
                                 ×
                                 
                                    
                                       1
                                       −
                                       
                                          α
                                          p
                                       
                                    
                                 
                              
                           
                        
                     where α
                     
                        p
                     
                     
                        T
                      is previously estimated alpha for pixel p based on texture features through optimizing Eq. (3). The αp
                      is estimated alpha for pixel p based on (Fi
                     , Bj
                     ) pair by using Eq. (2). With the inclusion of texture alpha, the objective function to select the best F–B pair regarding estimated texture matte is modified as
                        
                           (14)
                           
                              
                                 
                                    O
                                    p
                                    2
                                 
                                 
                                    
                                       F
                                       i
                                    
                                    
                                       B
                                       j
                                    
                                    
                                       α
                                       p
                                       T
                                    
                                 
                                 =
                                 
                                    O
                                    p
                                    1
                                 
                                 
                                    
                                       F
                                       i
                                    
                                    
                                       B
                                       j
                                    
                                 
                                 ×
                                 
                                    T
                                    p
                                 
                                 
                                    
                                       F
                                       i
                                    
                                    
                                       B
                                       j
                                    
                                    
                                       α
                                       p
                                       T
                                    
                                 
                                 .
                              
                           
                        
                     
                  

The block diagram of the proposed alpha matting system is shown in Fig. 6
                     . The texture matte is obtained by applying the proposed color matting algorithm on the texture features using Eq. (3) followed by Eq. (8) to refine the matte. This texture matte is used in conjunction with the proposed color matting method through Eqs. (14) and (8) to obtain the final matte.

@&#EXPERIMENTAL RESULTS@&#

The effectiveness of the proposed method in alleviating a major drawback of current matting methods is evaluated through comprehensive experiments. We demonstrate the inadequacy of color as a matting feature when there is significant overlap in F and B colors. The effect of texture resolution on matting quality is evaluated in the first experiment. The advantages of combining color and texture in matting are shown in the second experiment. In the third experiment, the effectiveness of the proposed method is quantitatively compared with other matting methods on dataset of images developed by Rhemann et al. [22]. It consists of 8 images for which the ground truth alpha mattes are hidden from the public. For each image, three trimaps are presented; they are small, large (coarse) and user defined. Examples of images and trimaps are shown in Fig. 7
                     . We use these images to compare the performance of the proposed method with other methods listed in the website based on different measures — the mean square error (MSE), Gradient error and the sum of absolute difference (SAD) between the extracted matte and the ground truth. The performance of proposed method and state of the art matting methods are evaluated for highly textured images in the fourth experiment. Applying the proposed method on only color or texture features using Eq. (3) followed by Laplacian post-processing is called color and texture matting, respectively. Combining color and texture features through the proposed method using Eq. (14) is called combined matting in the following experiments.

The proposed method takes advantage of texture information to complement color in the matting process. We show that although texture provides important additional information for the extraction of high quality mattes, the resolution at which texture descriptors are computed does not affect the extracted matte. However, as expected, the matte extracted using only texture information is dependent on the texture resolution. The objective function defined in Eq. (14) combines texture and color information to find the best F–B pairs for unknown pixels. Alpha values are then estimated based only on color components of best F–B pairs which are defined at pixel resolution. Thus, the resolution of estimated mattes is not dependent on the size of texture patches.

We compare, the raw estimated mattes by proposed method based on different texture patch sizes without applying any refinement process. The original images with zoomed regions are shown in the first row of Fig. 8
                        . The estimated mattes using only texture information (texture matte) of zoomed regions of Net, Troll, Giraffe and Barbie images are shown in Fig. 8a, c, e and g, respectively. More details such as hair strands and structure of the net are visible in the texture mattes with 3×3 patch size as shown in second row of Fig. 8c and a. The quality of estimated texture mattes are degraded when size of patches is increased as shown in third and fourth rows of Fig. 8a, c, e and g for patch sizes of 13×13 and 21×21, respectively.

The proposed method uses color and texture information to estimate mattes at pixel resolution regardless of patch size of texture descriptors. Results of proposed method (combined mattes) with different texture patch sizes are shown in Fig. 8b, d, f and h for Net, Troll, Giraffe and Barbie images, respectively. The fine details like hair strands or net structure are clearly visible in the combined mattes as shown in second row of Fig. 8b, d, f and h. The quality of combined mattes is high even when the details information are lost by large texture patches as shown in fourth row of Fig. 8b, d, f and h. Thus, even when the texture patch sizes are changed, there is no marked change in the quality of the extracted matte.

This experiment illustrates, qualitatively and quantitatively, the improvement when texture matte is combined with color to obtain the final matte. We also analyze the contributions of the spatial (E) and color (S) statistics terms of Eq. (3). In the original images shown in Fig. 9a the backgrounds are designed so that there is significant overlap between foreground and background distributions. The first dimension of texture feature of the images is shown in Fig. 9b. The proposed alpha matting is applied on texture and color features individually to estimate texture and color mattes as shown in Fig. 9c and d. As expected the texture matte is not as good as color matte because texture features are defined over patch of pixels while color features are defined for single pixels. This is clear from estimated mattes using color and texture features for the image in row 1 containing hair strands where some parts of background are wrongly estimated as foreground (Fig. 9c, d). The alpha matte for background is more accurately estimated by combining color and texture through proposed method as shown in Fig. 9e. The estimated mattes for plant leaves in row 4 are not accurate and some of the leaves are considered as foreground while they are accurately estimated by combining color and texture features.

A quantitative evaluation to show the effect of combining color and texture matte is shown in Table 1
                         where the sum of absolute difference (SAD) between the generated matte and the ground truth is calculated (the best results are shown in bold). These results have been generated from alpha matting website
                           1
                        
                        
                           1
                           
                              www.alphamatting.com.
                         using images in the database. It is evident that the combined matting method has consistently outperformed either of color matting or texture matting. The only exception is the Troll image for which the color matte performs the best. When the foreground consists of pixel-wide regions, e.g., hair strands, color plays a more important role than texture and hence, color matting has less SAD as seen in the Troll image.

The contributions of E and S terms in the objective function of Eq. (3) is quantitatively evaluated and shown in the last two columns of Table 1. When the E term, which is a function of the distance of a foreground or background block from the block containing the unknown pixel, is removed from the objective function, the bias towards larger separation of color distributions reflected in the S term plays an important part in addition to the chromatic distortion. The results without the E term in the objective function show that the performance is poorer than when the E term is included. Similarly, when the S term is excluded, the objective function contains the chromatic distortion term and the E term. In this case also the performance is poorer for most of the images than when the S term is also included. The exception is for the Doll image and the reason could be that in this particular image, the distance factor is more relevant than the separation of color distributions because the colors are already well separated for most parts of the unknown trimap.

For further evaluation, the proposed combined matting method is compared with other techniques listed in the alpha matting website. Table 2
                         shows the MSE for all the images and for all trimaps. The superscript next to the MSE for each method indicates its rank for the particular image and trimap. Methods with first rank are in bold. The estimated mattes by the proposed combined method for Donkey, Plastic bag, Plant, Net and Pineapple images with small trimaps are ranked 1st among all matting method and also presented very good rank for other images with different trimaps. The average ranks of proposed combined method for small, large and user trimaps are 2.75, 2.5 and 2.5, respectively and overall rank of proposed method for all images with different trimaps is 2.58 which is the best among all matting methods. The results of different matting methods are evaluated by SAD and Gradient error as shown in Table 3
                        . The proposed combined method has overall rank of 3.21 with respect to Gradient error which is the best and has overall rank of 3.92 based on SAD which is second after shared matting whose overall rank is 3.13.

Once the foreground and background colors along with alpha value of unknown pixels are estimated, the foreground object can be seamlessly composed with new background by using the compositing equation and by replacing estimated background colors of pixels with new background colors. Some of images with trimaps are shown in first column of Fig. 10
                         in which red and blue colors on images indicate boundaries of known foreground and background regions. The estimated alpha and foreground color by proposed combined method are shown in second and third column of Fig. 10. The composition of foreground objects with new background and its zoomed region are shown in last column of Fig. 10. The hair strands of Lady Face 1, Lady Face 2 and Doll 2 are smoothly combined with new background as shown in zoomed regions in Fig. 10. The solid foreground of Doll 1 is seamlessly pasted to new background as shown in zoomed regions in third row of Fig. 10. The quality of composition image of peacock with complicated feathers structure as shown in zoomed regions in first row of Fig. 10 indicate how effectively foreground colors are estimated for pixels using the proposed method.

The visual comparison of proposed combined matting method with some color sampling based matting methods like Shared [8], Robust [9] and Bayesian [7] matting and with alpha propagation based matting methods like closed-form matting [4] is shown in Fig. 11
                        . Original images and their zoomed portions are shown in Fig. 11a and b. The extracted mattes for zoomed areas using proposed combined, shared, robust, closed-form and Bayesian matting methods are shown in Fig. 11c–g, respectively. In the Troll image, the foreground and background have similar color distribution therefore some parts of bridge are wrongly estimated as foreground by different matting methods. However this problem is alleviated by combining color and texture through the proposed method.

In the Pineapple image, the leaves of pineapple have highly overlapped color distribution with background causing some part of leaves to be considered as background by shared, robust and Bayesian matting methods. Also the propagation of alpha is blocked by strong edges of leaves due to which the alpha value of background area is wrongly estimated as foreground in closed-form matting. The texture distinction between leaves of pineapple and background area is leveraged along with color information to estimate more accurate alpha matte by the proposed method. Another challenging image is the Plastic bag in which the metallic wire tied around the bag has different color from foreground that makes it hard for color sampling based methods to estimate it as foreground. In addition the closed-form matting performs poorly because of blocked alpha by strong edges of metallic wire. The proposed combined method performs well and estimates the matte for the Plastic bag and the metallic wire as foreground.

This experiment is designed to illustrate the effectiveness of the proposed combined matting method for highly textured images. To this end, a new set of 15 images (some of images are taken from the training set proposed by [22]) is synthesized in such a way that foreground and background have similar color distributions (Fig. 12
                        ). The proposed combined method is compared with shared [8], robust [9] and closed-form [4] matting methods over the new set of images with three type of trimaps called as small, large and very large trimaps. In rows 1 and 2 of Fig. 13
                        , foreground regions have similar color distributions as background ones, e.g., the petals of the flower in the first image have similar colors to the textured background and color sampling methods of shared, robust matting perform poorly (Fig. 13d, e) as compared to the proposed combined method that uses texture feature to complement color (Fig. 13c). The foreground object in the second row image (Fig. 13a) contains holes through which the background having a similar color is visible. The propagation of alpha is blocked by strong edges of foreground and the holes are considered as foreground by closed-form matting (Fig. 13f) while they are visible in the ground truth matte (Fig. 13b). The color similarity of foreground and background makes it hard for color sampling based matting methods to estimate accurate alpha mattes while combining color and texture information through the proposed method leads to the most accurate mattes (Fig. 13c).

For further evaluation, the performance of matting methods is evaluated over more challenging images like a hairy doll in front of textured regions as shown in the third and fourth rows images of Fig. 13a. The alpha is not accurately propagated through the background regions which are partially occluded by doll's hairs as shown in Fig. 13f for closed-form matting. Moreover the estimated mattes by color sampling based matting methods are not accurate. Some parts of textured background are wrongly estimated as foreground as shown in Fig. 13d and e for shared and robust matting. The proposed method still estimates the most accurate mattes as shown in the zoomed regions in row 4 of Fig. 13c.

The most challenging images are fuzzy images in which foreground regions are gradually blended with background ones as shown in rows 5 and 6 of Fig. 13a. The color/texture of foreground and background are blended together leading to new color/texture for fuzzy region that is different from foreground or background as seen in the zoomed window of Fig. 13a. Here, the background has vertical red lines and foreground has diagonal red lines and the fuzzy region has both vertical and diagonal red lines that generate different texture patterns. This phenomenon makes fuzzy images a challenging one for alpha matting especially when foreground and background have similar color distributions. Robust and shared matting methods cannot properly estimate alpha mattes for fuzzy regions as shown in Fig. 13d and e. The propagation of alpha also does not yield a good matte due to blended textures in fuzzy regions as shown in Fig. 13f. The proposed methods smoothly estimate alpha mattes for fuzzy regions as shown in rows 5 and 6 of Fig. 13d and e.

The quality of estimated mattes for 15 synthesized images over three type of trimaps (small, large and very large trimaps) by shared, robust, closed-form and proposed matting methods are quantitatively evaluated according to MSE of [9] as shown in Fig. 14
                        . The proposed combined method is seen to perform the best with least variance in MSE compared to other methods irrespective of the size of the trimaps. The performance of the proposed method in estimating high quality mattes for textured images shows the potential power of texture when it is used as a complementary feature with color one.

@&#LIMITATIONS@&#

The proposed matting method takes advantages of texture information to discriminate between foreground and background regions when they have similar colors. However its quality degrades when both texture and color are unable to discriminate between F and B regions due to their similarity in both color and texture spaces. In order to illustrate the problem, the contrast of two images from [22] is reduced as shown in first column of Fig. 15
                     . The rabbit and donkey has similar colors as background. Moreover texture information is not reliable due to the low contrast of the images. Thus, the proposed method fails to estimate high quality mattes as shown in last column of Fig. 15. In these cases, the result can be improved by refining the trimap which needs more user effort.

@&#CONCLUSION@&#

This paper addresses the problem of matting when there is overlap in color distributions of foreground and background regions. This causes methods that belong to the color sampling approach to select samples that may not be representative of the foreground or background region. Further, in the presence of strong edges of high textured regions, the propagation of α from known pixels to similar unknown pixels could be blocked. Our approach consists of judiciously choosing samples so that the above problems are alleviated. First, we present a color matting method in which an objective function that incorporates color and spatial statistics of image is minimized in a brute-force fashion and it is applied on extracted texture features to obtain a texture matte. Then a texture matte is leveraged to improve the matte by combining color and texture information using new objective function. Experiments show that combining color and texture information leads to improved matte extraction. Comparison with other state of the art methods on a standard benchmarking dataset reveals that the proposed method obtains the first ranks with respect to mean square and Gradient errors and also has the second rank after shared matting with respect to sum of absolute differences. However, we have shown that shared matting does not perform well for complex images that contain highly textured regions.

@&#REFERENCES@&#

