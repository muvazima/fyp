@&#MAIN-TITLE@&#Force based tool wear monitoring system for milling process based on relevance vector machine

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           RVM based monitoring system is realized for tool wear recognition.


                        
                        
                           
                           Multinomial function is used to realize multi categories classification.


                        
                        
                           
                           The classification accuracy of RVM is higher than SVM.


                        
                        
                           
                           The consuming time of RVM classification is less than SVM.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Tool wear monitoring

Relevance vector machine

Multinomial function

Milling process

@&#ABSTRACT@&#


               
               
                  The monitoring of tool wear status is paramount for guaranteeing the workpiece quality and improving the manufacturing efficiency. In some cases, classifier based on small training samples is preferred because of the complex tool wear process and time consuming samples collection process. In this paper, a tool wear monitoring system based on relevance vector machine (RVM) classifier is constructed to realize multi categories classification of tool wear status during milling process. As a Bayesian algorithm alternative to the support vector machine (SVM), RVM has stronger generalization ability under small training samples. Moreover, RVM classifier results in fewer relevance vectors (RVs) compared with SVM classifier. Hence, it can be carried out much faster compared to the SVM. To show the advantages of the RVM classifier, milling experiment of Titanium alloy was carried out and the multi categories classification of tool wear status under different numbers of training samples and test samples are realized by using SVM and RVM classifier respectively. The comparison of SVM with RVM shows that the RVM can get more accurate results under different number of small training samples. Moreover, the speed of classification is faster than SVM. This method casts some new lights on the industrial environment of the tool condition monitoring.
               
            

@&#INTRODUCTION@&#

The monitoring of tool wear for milling process is paramount for guaranteeing the workpiece quality and improving the manufacturing efficiency. To realize more accurate recognition of the tool wear status, multi categories classification is preferred because it can not only judge whether the cutter is broken or not, but also recognize the tool wear scope [1]. In decades, many classifiers have been proposed such as feed forward neural network [2,3], hidden Markov model [4,5], and condition random field [6]. Recently, more attention is paid on the classification under small training samples. Because the tool wear is a gradual process and the tool wear status needs to be measured intermittently, the collection of training samples is time consuming and costive. So it is not realistic to collect large amount of samples to train the classifier. In addition, the complexity of tool wear profile also casts higher demand on the generalization ability of the classifier because it is impossible to collect all data within certain tool wear scope [7,8].

Support vector machine (SVM) is a novel machine learning method based on structure risk minimization principle, which can find global optimum solutions for problems with small training samples, high dimensions and non-linear features [9–11]. Therefore, in recent years many researchers used SVM as a classifier to realize the tool wear monitoring. Salgado and Alonso [12] used the least squares version of support vector machine to estimate the feed cutting force and realize the monitoring of the tool wear in turning. Elangovan et al. [13] analyzed the effect of SVM kernel functions on classification of the tool wear for a single point cutting tool based vibration signal. Shi and Gindy [14] realized the tool wear prediction using the combination of SVM and principal component analysis (PCA) [15]. Binsaeid et al. [16] used SVM to realize the classification of the tool wear status under different cutting conditions. These applications show that the SVM classifier has strong ability to realize the multi categories classification especially in the case of small training samples. However, some shortcomings also exist in the SVM classifier. The first is that the ability of the classifier depends on an important parameter–penalty parameter C 
                     [14]. However, up to now, the selection of C can only be realized by trial and error method, which makes it hard to get the optimum value. The second is that SVM makes unnecessarily liberal use of basis function, which results in the number of support vectors required typically growing linearly with the size of the training set. Therefore, the classification speed of the classifier is lowered obviously [17].

In this paper, relevance vector machine (RVM) combined with multinomial function criteria is proposed to realize the multi categories classification under small training samples. RVM is introduced by Tipping [18] as a Bayesian treatment alternative to the SVM. In comparison with SVM, RVM gains more flexibility and convenience because there is no need to set the penalty parameter which usually requires cross validation and post optimization [7,8]. Moreover, the number of relevance vectors is much smaller than that of support vectors which yields a sparser representation than the SVM. Therefore, RVM classification can be carried out with higher decision speed and better generalization ability [19].

To show the effectiveness of RVM based tool wear monitoring system in the case of small training samples, milling experiment of Titanium alloy was carried out and the force signals during the machining process were collected as the sensory information to depict the variation of the tool wear status. Based on six features extracted in the time domain and frequency domain, the training data under different number of small training samples are utilized to train the RVM and SVM classifier respectively. In addition, both classifiers are adopted to realize the classification under different number of test samples. By comparison, it can be concluded that the generalization ability of RVM is higher than SVM in the case of small training samples. In addition, the classification speed of RVM is higher than SVM, which makes it more suitable for real time tool condition monitoring.

The structure of this paper is organized as follows. In Section 2, the principle of binary RVM is presented. Moreover, multinomial function is proposed to realize the classification of multi categories. In Section 3, online tool wear monitoring system is constructed based RVM classifier and the milling experiment was setup to testify the effectiveness of the monitoring system. Six features are extracted and normalized to denote the relationship between the tool wear status and the force signal. Moreover, different kinds of combination of training and test samples are firstly selected. Based on the collected dataset, RVM are utilized to realize the multi categories tool wear monitoring and compare with SVM classifier. The results show that the RVM can get higher classification accuracy with lower time consuming in the case of small training samples. In Section 4, some useful conclusions are drawn.

Relevance vector machine is a kind of sparse learning classifier which is based on Bayesian principle. For the given dataset D
                        =(X,T), X
                        =(x
                        1,x
                        2,…,x
                        
                           n
                        ,…,x
                        
                           N
                        ) and T
                        =(t
                        1,t
                        2,…,tn
                        ,…,tN
                        ), tn
                        {0,1}, “1” denotes the training sample belongs to this class and “0” refuses it. N is the number of training samples. A probability based decision model is built in which the target value of the classifier is depicted by a posterior probability distribution P(T|X), which is expressed as [18]
                        
                           
                              (1)
                              
                                 P
                                 (
                                 T
                                 |
                                 X
                                 )
                                 =
                                 
                                    
                                       
                                          ∏
                                       
                                       
                                          n
                                          =
                                          1
                                       
                                       
                                          N
                                       
                                    
                                 
                                 σ
                                 
                                    
                                       
                                          
                                             f
                                             (
                                             
                                                
                                                   x
                                                
                                                
                                                   n
                                                
                                             
                                             ,
                                             W
                                             )
                                          
                                       
                                    
                                    
                                       
                                          
                                             t
                                          
                                          
                                             n
                                          
                                       
                                    
                                 
                                 
                                    
                                       [
                                       1
                                       -
                                       σ
                                       {
                                       f
                                       (
                                       
                                          
                                             x
                                          
                                          
                                             n
                                          
                                       
                                       ,
                                       W
                                       )
                                       }
                                       ]
                                    
                                    
                                       1
                                       -
                                       
                                          
                                             t
                                          
                                          
                                             n
                                          
                                       
                                    
                                 
                              
                           
                        where σ(f) is a predefined logistic sigmoid link function which is utilized to generalize the linear model. The mathematical expression of this function is given as
                           
                              (2)
                              
                                 σ
                                 (
                                 f
                                 )
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       1
                                       +
                                       
                                          
                                             e
                                          
                                          
                                             -
                                             f
                                          
                                       
                                    
                                 
                              
                           
                        Here, the function f is the linear weighted sum of M nonlinear basis functions, which is expressed as
                           
                              (3)
                              
                                 f
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       n
                                    
                                 
                                 ,
                                 W
                                 )
                                 =
                                 
                                    
                                       W
                                    
                                    
                                       T
                                    
                                 
                                 
                                    
                                       Φ
                                    
                                    
                                       n
                                    
                                 
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          M
                                       
                                    
                                 
                                 
                                    
                                       w
                                    
                                    
                                       i
                                    
                                 
                                 ϕ
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       n
                                    
                                 
                                 ,
                                 
                                    
                                       
                                          
                                             x
                                          
                                          
                                             ̇
                                          
                                       
                                    
                                    
                                       i
                                    
                                 
                                 )
                              
                           
                        where 
                           
                              (
                              
                                 
                                    
                                       
                                          x
                                       
                                       
                                          ̇
                                       
                                    
                                 
                                 
                                    1
                                 
                              
                              ,
                              
                                 
                                    
                                       
                                          x
                                       
                                       
                                          ̇
                                       
                                    
                                 
                                 
                                    2
                                 
                              
                              ,
                              …
                              
                                 
                                    
                                       
                                          x
                                       
                                       
                                          ̇
                                       
                                    
                                 
                                 
                                    i
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    
                                       
                                          x
                                       
                                       
                                          ̇
                                       
                                    
                                 
                                 
                                    M
                                 
                              
                              )
                           
                         are the relevance vector (RV) sequences which come from the training data X (as shown in Fig. 1
                        ). W
                        =(w
                        1,w
                        2,…,wM
                        )T is the corresponding weight of these vectors. Φ
                        n is a M
                        ×1 vector which is represented as
                           
                              
                                 
                                    
                                       Φ
                                    
                                    
                                       n
                                    
                                 
                                 =
                                 Φ
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       n
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       {
                                       ϕ
                                       (
                                       
                                          
                                             x
                                          
                                          
                                             n
                                          
                                       
                                       ,
                                       
                                          
                                             
                                                
                                                   x
                                                
                                                
                                                   ̇
                                                
                                             
                                          
                                          
                                             1
                                          
                                       
                                       )
                                       ,
                                       ϕ
                                       (
                                       
                                          
                                             x
                                          
                                          
                                             n
                                          
                                       
                                       ,
                                       
                                          
                                             
                                                
                                                   x
                                                
                                                
                                                   ̇
                                                
                                             
                                          
                                          
                                             2
                                          
                                       
                                       )
                                       ,
                                       …
                                       ,
                                       ϕ
                                       (
                                       
                                          
                                             x
                                          
                                          
                                             n
                                          
                                       
                                       ,
                                       
                                          
                                             
                                                
                                                   x
                                                
                                                
                                                   ̇
                                                
                                             
                                          
                                          
                                             M
                                          
                                       
                                       )
                                       }
                                    
                                    
                                       T
                                    
                                 
                              
                           
                        where 
                           
                              ϕ
                              (
                              
                                 
                                    x
                                 
                                 
                                    n
                                 
                              
                              ,
                              
                                 
                                    
                                       
                                          x
                                       
                                       
                                          ̇
                                       
                                    
                                 
                                 
                                    i
                                 
                              
                              )
                           
                         is a Gaussian kernel function whose expression is given as
                           
                              (4)
                              
                                 ϕ
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       n
                                    
                                 
                                 ,
                                 
                                    
                                       
                                          
                                             x
                                          
                                          
                                             ̇
                                          
                                       
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 =
                                 exp
                                 
                                    
                                       
                                          -
                                          
                                             
                                                |
                                                |
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      n
                                                   
                                                
                                                -
                                                
                                                   
                                                      
                                                         
                                                            x
                                                         
                                                         
                                                            ̇
                                                         
                                                      
                                                   
                                                   
                                                      i
                                                   
                                                
                                                |
                                                
                                                   
                                                      |
                                                   
                                                   
                                                      2
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      β
                                                   
                                                   
                                                      2
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where β is the width of kernel function.

It can be seen that the computation of the posterior probability P can be realized when several “most relevant” training data and their weight value are known [20]. Moreover, these relevant vectors do not necessarily reside along the decision boundary (as shown in Fig. 1). Therefore, the RVM typically yields a sparser representation than the SVM [17], which reduce the complexity of the model and improve generalization performance of the classifier.

The training of RVM is to determine the relevance vectors and its corresponding weight value. However, direct estimation from Eq. (1) usually leads to severe over-fitting phenomenon. To avoid that, a hyper-parameter α is proposed to restrict the scope of weight values W 
                        [18]. Therefore, W is viewed as a zero mean value Gaussian probability distribution, which is represented as
                           
                              (5)
                              
                                 P
                                 (
                                 W
                                 |
                                 α
                                 )
                                 =
                                 
                                    
                                       
                                          ∏
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          M
                                       
                                    
                                 
                                 N
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       i
                                    
                                 
                                 |
                                 0
                                 ,
                                 
                                    
                                       α
                                    
                                    
                                       i
                                    
                                    
                                       -
                                       1
                                    
                                 
                                 )
                              
                           
                        
                     

For classification, the weights W cannot be integrated out based on Bayesian principle directly. So here an approximation procedure, which is based on Laplace’s method, is utilized to get the weights value iteratively. The whole process includes three steps [18].
                           
                              
                                 Step 1: Fix α and calculate the “most probable” weights W
                                 MP by calculating the maximum value of the penalized logistic log likelihood function L whose mathematical expression is given as 
                                    
                                       (6)
                                       
                                          L
                                          =
                                          log
                                          {
                                          p
                                          (
                                          w
                                          |
                                          T
                                          ,
                                          α
                                          )
                                          }
                                          =
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   n
                                                   =
                                                   1
                                                
                                                
                                                   N
                                                
                                             
                                          
                                          [
                                          
                                             
                                                t
                                             
                                             
                                                n
                                             
                                          
                                          log
                                          
                                             
                                                y
                                             
                                             
                                                n
                                             
                                          
                                          +
                                          (
                                          1
                                          -
                                          
                                             
                                                t
                                             
                                             
                                                n
                                             
                                          
                                          )
                                          log
                                          (
                                          1
                                          -
                                          
                                             
                                                y
                                             
                                             
                                                n
                                             
                                          
                                          )
                                          ]
                                          -
                                          
                                             
                                                1
                                             
                                             
                                                2
                                             
                                          
                                          
                                             
                                                W
                                             
                                             
                                                
                                                   T
                                                
                                             
                                          
                                          AW
                                       
                                    
                                 where yn
                                 
                                 =
                                 σ{f(x
                                 
                                    n
                                 ,
                                 W)} and A=diag(α
                                 1,α
                                 2,…,αM
                                 ).


                                 Step 2: Eq. (6) is differentiated twice to obtain the Hessian matrix in the form of 
                                    
                                       (7)
                                       
                                          
                                             
                                                ∇
                                             
                                             
                                                W
                                             
                                          
                                          
                                             
                                                ∇
                                             
                                             
                                                W
                                             
                                          
                                          log
                                          (
                                          p
                                          (
                                          W
                                          |
                                          T
                                          ,
                                          α
                                          )
                                          )
                                          
                                             
                                                |
                                             
                                             
                                                
                                                   
                                                      W
                                                   
                                                   
                                                      MP
                                                   
                                                
                                             
                                          
                                          =
                                          -
                                          (
                                          Φ
                                          B
                                          
                                             
                                                Φ
                                             
                                             
                                                T
                                             
                                          
                                          +
                                          A
                                          )
                                       
                                    
                                  
                                 Φ is a M
                                 ×
                                 N matrix Φ
                                 =(Φ
                                 1,
                                 Φ
                                 2,…,
                                 Φ
                                 
                                    n
                                 ,…Φ
                                 
                                    N
                                 ). Where B
                                 =diag(β
                                 1, β
                                 2,…, βN
                                 ) is a diagonal matrix with 
                                    
                                       (8)
                                       
                                          
                                             
                                                β
                                             
                                             
                                                n
                                             
                                          
                                          =
                                          σ
                                          {
                                          f
                                          (
                                          
                                             
                                                x
                                             
                                             
                                                n
                                             
                                          
                                          ,
                                          
                                             
                                                W
                                             
                                             
                                                MP
                                             
                                          
                                          )
                                          }
                                          
                                             
                                                
                                                   1
                                                   -
                                                   σ
                                                   {
                                                   f
                                                   (
                                                   
                                                      
                                                         x
                                                      
                                                      
                                                         n
                                                      
                                                   
                                                   ,
                                                   
                                                      
                                                         W
                                                      
                                                      
                                                         MP
                                                      
                                                   
                                                   )
                                                   }
                                                
                                             
                                          
                                       
                                    
                                  Eq. (8) is negated and inverted to calculate the covariance 
                                    
                                       (9)
                                       
                                          Cov
                                          =
                                          
                                             
                                                (
                                                Φ
                                                B
                                                
                                                   
                                                      Φ
                                                   
                                                   
                                                      T
                                                   
                                                
                                                +
                                                A
                                                )
                                             
                                             
                                                -
                                                1
                                             
                                          
                                          .
                                       
                                    
                                 
                              


                                 Step 3: Based on statistical covariance Cov and W
                                 MP, the hyper-parameter α can be updated using [19] 
                                 
                                    
                                       (10)
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  α
                                                               
                                                               
                                                                  i
                                                               
                                                               
                                                                  new
                                                               
                                                            
                                                            =
                                                            
                                                               
                                                                  
                                                                     
                                                                        γ
                                                                     
                                                                     
                                                                        i
                                                                     
                                                                  
                                                               
                                                               
                                                                  
                                                                     
                                                                        w
                                                                     
                                                                     
                                                                        MP
                                                                        i
                                                                     
                                                                     
                                                                        2
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               
                                                                  γ
                                                               
                                                               
                                                                  i
                                                               
                                                            
                                                            =
                                                            1
                                                            -
                                                            
                                                               
                                                                  α
                                                               
                                                               
                                                                  i
                                                               
                                                            
                                                            Cov
                                                            (
                                                            i
                                                            )
                                                         
                                                      
                                                      
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                  
                                 w
                                 MPi is the ith element of weight W
                                 MP and Cov(i) is the ith diagonal element of the covariance. With the proceeding of the iteration process, many αi
                                  will have large values. Therefore, the corresponding weight value tends to be zero and is pruned out, which results in the sparse representation of the weight value. The iteration process continues until the maximum change in αi
                                  is below a certain threshold value ( or the maximum number of iteration T
                                 0 is reached. The termination criterion is expressed as 
                                    
                                       (11)
                                       
                                          
                                             max
                                          
                                          (
                                          |
                                          
                                             
                                                α
                                             
                                             
                                                i
                                             
                                             
                                                new
                                             
                                          
                                          -
                                          
                                             
                                                α
                                             
                                             
                                                i
                                             
                                          
                                          |
                                          )
                                          <
                                          ε
                                          
                                          or
                                          
                                          T
                                          >
                                          
                                             
                                                T
                                             
                                             
                                                0
                                             
                                          
                                       
                                    
                                 .

For K class case, K
                        −1 binary RVM classifiers are firstly constructed to distinguish between this class and the others. For the new input data x
                        0, the probability of this input belongs to certain class u is given using multinomial function [21]
                        
                           
                              (12)
                              
                                 p
                                 (
                                 k
                                 =
                                 u
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             e
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                         W
                                                      
                                                      
                                                         u
                                                      
                                                      
                                                         T
                                                      
                                                   
                                                   
                                                      
                                                         Φ
                                                      
                                                      
                                                         u
                                                      
                                                   
                                                   (
                                                   
                                                      
                                                         x
                                                      
                                                      
                                                         0
                                                      
                                                   
                                                   )
                                                
                                             
                                          
                                       
                                    
                                    
                                       1
                                       +
                                       
                                          
                                             ∑
                                          
                                          
                                             i
                                             =
                                             1
                                          
                                          
                                             K
                                             -
                                             1
                                          
                                       
                                       
                                          
                                             e
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                         W
                                                      
                                                      
                                                         i
                                                      
                                                      
                                                         T
                                                      
                                                   
                                                   
                                                      
                                                         Φ
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   (
                                                   
                                                      
                                                         x
                                                      
                                                      
                                                         0
                                                      
                                                   
                                                   )
                                                
                                             
                                          
                                       
                                    
                                 
                                 
                                 (
                                 u
                                 =
                                 1
                                 ,
                                 2
                                 ,
                                 …
                                 ,
                                 K
                                 -
                                 1
                                 )
                              
                           
                        where Φu(x
                        0) is kernel function relevant vectors of the uth RVM classifier for the given input x
                        0, W
                        u is the corresponding weight values.

The probability belongs to the class K is described as 
                           
                              (13)
                              
                                 p
                                 (
                                 k
                                 =
                                 K
                                 )
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       1
                                       +
                                       
                                          
                                             ∑
                                          
                                          
                                             i
                                             =
                                             1
                                          
                                          
                                             K
                                             -
                                             1
                                          
                                       
                                       
                                          
                                             e
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                         W
                                                      
                                                      
                                                         i
                                                      
                                                      
                                                         T
                                                      
                                                   
                                                   
                                                      
                                                         Φ
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   (
                                                   
                                                      
                                                         x
                                                      
                                                      
                                                         0
                                                      
                                                   
                                                   )
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Therefore, the sum of all probability value is 1. The final category for the input data x
                        0 corresponds to the class label which has the largest probability.

The flowchart of RVM based classifier construction and multi categories classification is given in Fig. 2
                        .

Based on RVM classifier, the corresponding tool wear monitoring system can be constructed whose framework is illustrated in Fig. 3
                        . The whole system includes two parts. One is RVM training and the other is RVM monitoring. The force signals under different tool wear status were collected by sensor and amplified by charge amplifier. The feature vectors are extracted from original signals and normalized to form the training dataset. RVM classifiers are constructed correspondingly based on algorithm in Section 2 and they are utilized to realize online tool wear monitoring. For the given online sensory information during milling process in real industrial environment, the monitoring system calculates the output probability of each classifier and recognizes the current tool wear status.

To depict the characteristic of the force signal completely, the features in both time and frequency domain are extracted. Within each domain, three features are selected to reflect the amplitude and statistic distribution of the signal respectively. The mathematical expressions [22] of these feature indicators are given in Table 3
                        
                        
                        . To restrict the feature vectors into the similar scope, normalization is adopted which is given as 
                           
                              (14)
                              
                                 
                                    
                                       x
                                    
                                    
                                       new
                                    
                                 
                                 =
                                 
                                    
                                       x
                                       -
                                       
                                          
                                             x
                                          
                                          
                                             min
                                          
                                       
                                    
                                    
                                       
                                          
                                             x
                                          
                                          
                                             max
                                          
                                       
                                       -
                                       
                                          
                                             x
                                          
                                          
                                             min
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Based on the normalized data, both SVM and RVM are utilized to realize the classification of the tool wear status.

To demonstrate the effectiveness of the proposed method, the milling experiment of Titanium alloy was carried out. The framework of the experiment is demonstrated in Fig. 4
                        . The multi teeth cutter is utilized to mill the workpiece and their wear values were measured by microscope with the image processing software. The detailed information of the cutter geometry and cutting parameters are listed in Table 1. During machining process, the force signals along the feed direction were collected using Kistler dynamometer and transformed into digital signals using data acquisition system with 5K sampling rate. The whole process was carried out until the average tool wear value along the flank face is larger than 0.3mm. To realize the classification, all the data are divided into three kinds of tool wear (new, middle and severe) whose scopes are listed in Table 2 
                        [1,8]. The normalized waveform of force signal under three kinds of tool wear status is given in Fig. 5
                        .

To compare the generalization ability of RVM and SVM for the small training samples, the number of training samples is selected as 10, 20, 40, 60, 80 respectively and the number of test samples is selected as 100, 200, 400, 600 respectively. To improve the robustness of the classifier evaluation, each sample is selected randomly [23] from the dataset within each kind of tool wear scope. For every combination of the training and test samples, this process is repeated five times and the average accuracy and consuming time are taken as the final results to evaluate the generalization ability and classification speed of the classifier.

Under different combination of training samples and test samples, both RVM and SVM are utilized to realize the classification of the tool wear status. Here, Gaussian kernel function with bandwidth β
                        =0.5 is assigned for RVM and SVM. The threshold value ɛ in RVM classifier is set to be 0.000001. Moreover, the penalty parameter of SVM is selected by trial and error method and the final value is set to be 1 to obtain the optimal SVM classifier. The bar plot of classification accuracy is given in Fig. 6
                        . The number of training samples and test samples are denoted as N
                        1 and N
                        2 respectively. It can be seen that the accuracy scope of SVM classifier is from 0.77 to 0.9. While for RVM classifier, higher generalization ability can be achieved because Bayesian inference based output and flexible residence of the relevance vectors lower the possibility of over fitting [18]. Therefore, the minimum accuracy of RVM is 0.9727 and the maximum value is 1. To show the accuracy variation of SVM and RVM classifier, the average accuracy under different number of training samples is further calculated and illustrated in Fig. 7
                        . It can be seen that general trend is that the accuracy increase with the increase of N1. However, the variation scope of RVM is less than SVM, which shows that RVM classifier possess stronger stability.

To compare the classification speed of both classifiers, the consuming time during the classification process is further computed and listed in Table 4
                        . It can be seen that for each combination of the dataset, the speed of RVM is faster than SVM obviously. The average classification time of SVM is 0.1118s and the average consumed time of RVM is only 0.0032s, which is 35 times faster than SVM. The reason is that SVM minimizes the training error under the constraint of maximum smoothness, which requires more decision points. While for RVM classifier, the representation of kernel function is sparser than SVM so that less RVs are needed to calculate the output probability of each classifier. By combining the analysis of accuracy and speed, it can be concluded that RVM outperforms SVM in both the classification accuracy and consuming time which makes it more suitable for online classification and small samples learning.

@&#CONCLUSIONS@&#

In this paper, RVM based online tool wear monitoring system is constructed in which RVM binary classifier combined with multinomial function is proposed to realize multi categories tool wear monitoring of the milling process. The main characteristic of RVM lies that the number of relevance vectors is much smaller than that of support vectors which yields a sparser representation than the SVM. Therefore, stronger generalization performance and higher recognition speed can be obtained. Moreover, RVM classifier can be easily utilized in real industrial application because there is no need to set the penalty parameter. The comparisons of RVM and SVM for multi categories tool wear classification testify that RVM classifier can get higher accuracy and less consuming time. The research casts some new lights on the on line tool condition monitoring in industrial environment.

@&#ACKNOWLEDGMENTS@&#

This project is supported by National Natural Science Foundation of China (51175371), National Science and Technology Major Projects (2014ZX04012-014) and Tianjin Science and Technology Support Program (13ZCZDGX04000).

@&#REFERENCES@&#

