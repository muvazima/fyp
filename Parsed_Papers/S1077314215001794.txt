@&#MAIN-TITLE@&#Recent methods and databases in vision-based hand gesture recognition: A review

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           The paper surveys RGB and RGB-D sensors based hand gesture recognition methods.


                        
                        
                           
                           Dynamic as well as static gesture (posture/pose) recognition methods are reviewed.


                        
                        
                           
                           Qualitative as well as quantitative comparison of algorithms is provided.


                        
                        
                           
                           Twenty-six publicly available hand gesture/posture databases are also reviewed.


                        
                        
                           
                           Discussion on unresolved issues and future research directions is provided.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Gesture recognition

Posture recognition

Hand pose estimation

Sign language recognition

Hand gesture dataset

Gesture database

Survey

@&#ABSTRACT@&#


               
               
                  Successful efforts in hand gesture recognition research within the last two decades paved the path for natural human–computer interaction systems. Unresolved challenges such as reliable identification of gesturing phase, sensitivity to size, shape, and speed variations, and issues due to occlusion keep hand gesture recognition research still very active. We provide a review of vision-based hand gesture recognition algorithms reported in the last 16 years. The methods using RGB and RGB-D cameras are reviewed with quantitative and qualitative comparisons of algorithms. Quantitative comparison of algorithms is done using a set of 13 measures chosen from different attributes of the algorithm and the experimental methodology adopted in algorithm evaluation. We point out the need for considering these measures together with the recognition accuracy of the algorithm to predict its success in real-world applications. The paper also reviews 26 publicly available hand gesture databases and provides the web-links for their download.
               
            

@&#INTRODUCTION@&#

Nonverbal communication, which includes communication through hand gestures, body postures, and facial expressions makes up about two-thirds of all communication among human [1]. Hand gestures are one of the most common category of body language used for communication and interaction. Whilst the rest of the body indicates a more general emotional state, hand gestures can have specific linguistic content in it [2]. Due to the speed and expressiveness in interaction, hand gestures are widely used in sign languages and human–computer interaction systems.

One ongoing goal in human–machine interface design is to enable effective and engaging interaction. For example, vision-based hand gesture recognition (HGR) systems can enable contactless interaction in sterile environments such as hospital surgery rooms, or simply provide engaging controls for entertainment and gaming applications. However HGR is not as robust as standard keyboard and mouse based interaction. Issues such as sensitivity to size and speed variations, poor performance against complex backgrounds and varying lighting conditions, and the reliable detection of gesturing phase have limited the use of hand gestures as a reliable modality in interface design.

There are multiple ways to categorize hand gestures, (1) based on observable features and (2) based on the interpretation. In the first category gestures are classified based on temporal relationships, into two types; static and dynamic gestures (Fig. 1). Static hand gestures (aka hand postures/hand poses) are those in which the hand position does not change during the gesturing period. Static gestures mainly rely on the shape and flexure angles of the fingers. In dynamic hand gestures, the hand position changes continuously with respect to time. Dynamic gestures generally have three motion phases: preparation, stroke, and retraction [3]. The message in a dynamic gesture is mainly contained in the temporal sequence in the stroke phase. Dynamic gestures rely on the hand trajectories and orientations, in addition to the shape and fingers’ flex angles.
                        
                     

In the second category, gestures are classified based on the interpreted meaning. For example emblems, illustrators, regulators, affect displays, and adaptors [4,5] are the typical classes to describe gestures. Emblems (also labeled as autonomous gestures) are gestures that can be substituted for spoken words (for example, showing thumbs-up instead of saying all right). Illustrators are gestures used to illustrate spoken words (for example, giving directions by pointing). Regulators support the interaction and communication between speaker and listener (for example, raising hand to manage turn-taking). Affect displays are facial expressions, which when combined with postures reflect the intensity of an emotion (for example, staring at an object and moving the body back reflect the emotion fear). Adaptors are gestures used at some point in time for personal convenience, but have turned into a habit (for example, adjusting glasses in a tensed situation).


                        Fig. 2 shows the block diagram of a typical contactless gesture recognition system. The sensor is a camera in vision-based gesture recognition systems. Berman et al. [6] reviewed different sensors used in gesture recognition systems and provided a comprehensive analysis of integration of sensors into gesture recognition systems and their impact on the system performance. Based on feature extraction, vision-based gesture recognition systems are broadly divided into two categories, appearance-based methods and three dimensional (3D) hand model-based methods. Appearance-based methods utilize features of training image to model the visual appearance, and compare these parameters with the features of test image. Three-dimensional model-based methods rely on a 3D kinematic model, by estimating the angular and linear parameters of the model.

Our study builds on top of earlier attempts to survey the field of HGR. Mitra et al. [7] provided a survey of different gesture recognition methods, covering hand and arm gestures, head and face gestures, and body gestures. The HGR methods investigated in the survey was limited to Hidden Markov Models (HMMs), particle filtering and condensation algorithms, and Artificial Neural Networks (ANNs). Hand modeling and 3D motion based pose estimation methods are reviewed in [8] (ignoring the gesture classification schemes). An analysis of sign languages, grammatical processes in sign gestures, and issues relevant to the automatic recognition of sign languages are discussed in [9]. The latest of the above papers [8] covered developments till the year 2005. The review concluded that the methods studied are experimental and their use is limited to laboratory environments.

This paper reviews recent works in HGR with a focus on the developments in the last 16 years. Algorithms utilizing conventional RGB cameras (Section 2) as well as the new generation RGB-D cameras (Section 3) are surveyed, making the review unique. The HGR methods are classified and analyzed according to the technique used for gesture classification. We perform a quantitative comparison of HGR algorithms based on different attributes of the algorithm and the experimental methodology followed in algorithm testing. A review of available hand gesture databases (Section 4) and a discussion on hand gesture recognition research (Section 5) are also provided. We hope this survey is timely, given the growing research efforts and expanding market for gestural interactive systems.

The techniques used for dynamic HGR can be classified as (a) HMM [10–23] and other statistical methods [24–31], (b) ANN [32–34] and other learning based methods [35,36], (c) Eigenspace based methods [37,38], (d) Curve fitting [39], and (e) Dynamic programming [40]/Dynamic time warping [41,42] (Fig. 3
                        ).

HMM is the most widely used HGR technique. HMM is a statistical model in which the system being modeled is assumed to be a Markov process with unknown parameters. HMM represents the statistical behavior of an observable symbol sequence using a network of hidden states with transition and emission probabilities. The HMM can be used for pattern recognition once the hidden parameters are identified using the observable data.

HMM based dynamic hand gesture recognition methods mainly utilize temporal and spatial features of input images. Chen et al. [14] utilized Fourier descriptor and optical flow based motion analysis to characterize spatial and temporal features respectively. The algorithm extracts hand shape from complex backgrounds by tracking the hand in realtime. HMM based recognizers identify the best likelihood gesture model for a given pattern. The variations in gesture from a reference pattern reduce the likelihood of the gesture with the model. Lee and Kim [10] introduced an HMM based threshold model concept to filter out patterns with less likelihood. Hand movement direction is used to represent the spatio-temporal sequences of gestures. The method reliably detects an end point of a gesture, and finds the start point by backtracking.

HMM is based on homogeneous Markov chains as the dynamics of the system is determined only by time independent transition probabilities. Marcel et al. [15] proposed an extension of HMM, namely Input/Output Hidden Markov Model (IOHMM), for HGR. IOHMM is based on a non-homogeneous Markov chain in which emission and transition probabilities depend on the input. The IOHMM learns to map the input sequences, observations, output sequences, and the gesture classes for all the observations using a supervised discriminant learning. Compared to HMMs, IOHMM is a discriminative approach as it directly models posterior probabilities. The study in [15] was limited to 2 classes. Just et al. [13] extended the study for the recognition of single and double handed gestures and provided a comparison of HMM and IOHMM. Experiments conducted on larger databases, ranging from 7 to 16 gesture classes, concluded that HMM has better performance than IOHMM for large number of classes.

Hand location, angle and velocity features are combined in [11] to implement an HMM for HGR. Hand is localized by skin-color analysis and tracked by connecting the centroid of moving hand regions. The paper compared the utility of the three features, location, angle, and velocity, and concluded that angular features are most effective, having better discriminative power. Location and velocity features are ranked second and third respectively. A similar HMM implementation utilizing angles of motion along the trajectory of hand centroid is provided in [16].

Ramamoorthy et al. developed an HGR system by combining HMM based temporal characterization scheme with a static shape recognition system [12]. They used a Kalman filter based hand contour tracker which provides temporal characteristics of the gesture. Shapes are recognized using contour discriminant based classifier. These symbolic descriptors of the gestures are utilized for training the HMM. The system can reliably recognize dynamic gestures in spite of motion and discrete changes in hand poses. Also the algorithm has the ability to detect the start and end points of gesture sequences.

Dynamic gesture recognition algorithms utilize a backward spotting scheme that first detects the end point of a gesture and then trace back to the start point. Kim et al. [17] proposed an alternate method, a forward spotting scheme, that executes gesture segmentation and recognition simultaneously. The start and end points of gestures are detected by zero crossings of differential probability of the signal. A set of 3D articulation based features are extracted by an association mapping technique that correlates the 2D shape data to the 3D articulation data. Gestures are classified by a majority voting using an accumulative HMM.

Davis and Shah [31] decomposed gestures into four distinct phases which occurs in a fixed order, and developed a Finite State Machine (FSM) model for recognition. Temporal signature of hand motion is extracted and hand gesture are modeled using an FSM in [29]. The concept of motion energy is used to estimate the dominant motion from an image sequence. Hong et al. [30] used 2D positions of the centers of subjects’ head and hands to develop the FSM. A dynamic Bayesian network model is proposed in [24] for the recognition of isolated as well as continuous handed gestures. The features utilized are direction codes for hand motion, positional relation between the two hands, and the positional relation between face and hands.

Chen et al. [25] proposed a two level approach of statistical and syntactic analysis for the recognition of static and dynamic hand gestures respectively. The first level, statistical analysis, is based on Haar-like features and AdaBoost learning algorithm. The second level, syntactic analysis, is based on a stochastic context-free grammar (SCFG). The Haar-like features effectively describe the hand posture pattern and the AdaBoost algorithm constructs a strong classifier by combining a sequence of weak classifiers. The postures detected by the first level are converted to a sequence of terminal strings according to the grammar, in the second stage.

Yang et al. [32,33] utilized a time delay neural network (TDNN) to learn the 2D motion trajectories. TDNN is a multi-layer feed-forward network that utilizes shift windows between all layers to represent temporal relationships between events. The classification in TDNN is dynamic as the network sees only a small window of the input motion pattern, and the window slides over the input data while the network makes a series of local decisions. These local decisions are temporally integrated into a global decision at the output layer.

The region based motion algorithms as in [32] outperform intensity-based methods. For example, motion information in areas with little intensity variation is contained in the contours of the associated regions. The motion segmentation algorithm computes correspondences for such regions and finds the best affine transformation that accounts for the change in contour shape. The affine transformation parameters for region at different scales are used to derive a single motion field, which is then segmented to identify moving regions between two frames.

Chan et al. [34] proposed a combination of HMM and recurrent neural networks (RNN) which provided better performance compared to HMM or RNN used alone. The shape features used are based on Fourier descriptors, which are the inputs to radial basis function (RBF) network for an initial pose classification. The pose likelihood vector from the RBF network along with the motion information is the input to two independent classifiers, HMM and RNN. Outputs from the classifiers are combined linearly for the prediction of the gesture class.

Shen et al. [35] proposed an exemplar-based approach for gesture recognition. Hand gestures are represented using the divergence field of the hand flow motions. The divergence fields of the optical flow between consecutive image frames are derived and salient regions are detected from the divergence field using a Maximally Stable Extremal Regions (MSER) feature detector. Descriptors are extracted from each detected region to characterize local motion patterns. The database gesture sequences with their descriptors are indexed by a pre-trained hierarchical vocabulary. A new gesture sequence is recognized by matching it against the database.

Patwardhan and Roy [37] proposed an eigenspace based framework to model dynamic hand gestures containing both shape and trajectory information. Feature based methods involve a separate time consuming feature detection step which is avoided in this algorithm. The algorithm is invariant to common hand shape deformations: rotation, translation, scale and shear.

Shin et al. [39] proposed a geometric method using Bezier curves for the trajectory analysis and classification of dynamic gestures. Gestures are recognized by fitting the curve to 3D motion trajectory of hand. The gesture speed is incorporated into the algorithm to enable accurate recognition from trajectories having variations in speed.
                           
                           
                        

Kuremoto et al. [40] proposed a one-pass dynamic programming based approach for gesture recognition. A biologically motivated feature extraction system based on retina-V1 model proposed by Tohyama and Fukushima [43] estimates the hand motion. Hand gestures are considered as combinations of templates of simple movements. The movements are used to compose a set of 40 templates of gestures.

Dynamic time warping (DTW), an application of dynamic programming, has been widely used in isolated gesture recognition. Andrea Corradini [41] proposed a template based approach with DTW for the time alignment and normalization by computing a temporal transformation between the two signals to be matched. Lichtenauer et al. [42] proposed Statistical DTW (SDTW) for time warping and two classifiers, namely combined discriminative feature detectors (CDFDs) and quadratic classification on discriminative features fisher mapping (Q-DFFM), for classification. The classifiers are shown to outperform HMM and SDTW.

A summary and comparison of the features of hand gesture recognition algorithms surveyed in this section are provided in Tables 1 and 2.

The hand posture recognition methods reviewed are classified as (a) Supervised learning based methods [35,44–60], (b) unsupervised learning based methods [61], (c) graph matching [62–67], and (d) 3D model based methods [68–72] (Fig. 4).

A distributed locally linear embedding (DLLE) algorithm is proposed in [61] for hand posture recognition and dynamic gesture tracking. Locally linearly embedding (LLE) [73] is an unsupervised learning algorithm that attempts to map high-dimensional data to low-dimensional space while preserving the neighborhood relationship. The paper modified LLE to DLLE to discover the inherent properties of the input data, by noticing that some relevant pieces of information are distributed. DLLE extracts the intrinsic structure of data such as neighborhood relationship. The distances between projected data points in the low-dimensional space depend on the similarity of the input images. A probabilistic neural network (PNN) is used to classify different postures based on the distances in the low dimensional space. PNN has good training speed and classification accuracy with negligible retraining time.

Supervised learning in LLE algorithm is introduced in [52], for recognizing postures in Chinese sign language (CSL). Supervised LLE (SLLE) makes use of the class label information during the classifier training. Hand is detected using skin color and the intrinsic geometry of hand is used for the recognition.

Zhao et al. [51] proposed recursive induction learning based on extended variable-valued logic for hand pose recognition. In inductive learning knowledge is acquired by inducing rules from sets of examples or sets of feature vectors. The paper modified and extended the old concept of Variable-Valued Logic into Extended Variable-valued Logic (EVL) which provided a more powerful representation. A heuristic algorithm namely RIEVL (Rule Induction by Extended Variable-valued Logic) is proposed to learn rules both from examples as well as rule sets. RIEVL produced more compact rules than other induction algorithms. This capability allows to apply a large feature set to hand poses during training, and to derive a reduced rule set with a subset of the training features during recognition. The algorithm automatically selects the most effective features, which makes it suitable for realtime gesture recognition systems.

A common problem of training based methods is their dependence on training data. In order to increase generality and user independence, Licsar and Sziranyi [49,50] proposed a user-adaptive hand posture recognition system with interactive online training. The system is retrained online for faulty detected postures if the recognition accuracy decreases, realizing fast adaptation to new users. A supervised training method corrects for the unrecognized posture classes, and an unsupervised method continuously runs to follow slight changes in posture styles.

A solution to the complex background problem in hand posture detection and recognition is provided in [44]. The algorithm can handle backgrounds including skin-colored complex backgrounds. The system utilizes a Bayesian model of visual attention to generate a saliency map, and to detect, identify, and segment out the hand region from the complex backgrounds. Feature based visual attention is implemented using a combination of high level (shape, texture) and low level (color) image features. The segmented hand postures are classified using the shape and texture features, with a support vector machines (SVM) classifier.

Huang et al. [58] proposed an algorithm for hand posture recognition under varying illumination and pose conditions. The invariance to lighting conditions is achieved using an adaptive skin color model switching method. Insensitivity to hand pose variations is gained using a Gabor filter based pose angle estimation and correction method. The posture are classified using an SVM classifier.

Starting from the late seventies, graph-based techniques are used as a powerful tool for pattern representation and classification. After the initial enthusiasm, graph algorithms have been practically left unused for a long period of time. This is due to the high computational cost of graph algorithms, which still remains an unresolved problem. However, the use of graphs in computer vision and pattern recognition obtained a growing attention from the research community recently, as the computational cost of the graph-based algorithms is now becoming compatible with the computational power of new generation computers [74].

Elastic graph matching (EGM), a type of graph matching, is a neurally inspired pattern recognition architecture [75]. EGM has the inherent ability to handle geometric distortions, does not require a perfectly segmented input image, and can elegantly represent the variances in object appearance [63].

Image regions are represented by vertices in a graph representation. These vertices are related to each other by edges, expressing structural relationships between regions. Triesch et al. [63–66] utilized the elastic graph matching (EGM) technique to develop a system for person independent hand posture recognition against complex backgrounds. Hand postures are represented by labeled graphs with an underlying two dimensional topology. Attached to the nodes are jets, a local image description (image feature) based on Gabor filters. This approach provided scale invariant and user independent recognition, without explicit segmentation of hand region. Different hand postures are represented as attributed graphs and comparisons are made between model graphs (in the database) and data graph (corresponding to the realtime image). The nodes are compared using a similarity function, and the pattern is recognized by calculating the average node similarities.


                           Bunch graphs 
                           [76] are used to model the variability in object appearance. The natural variability in the attributes of corresponding points in several images (of the same object or a class of objects) is captured by labeling each node with a bunch of attribute values, extracted from the corresponding points. This method is used by Triesch et al. [63,64] to model complex background in hand posture images. For the matching process, each of the attribute value in the bunch is compared with the local image information in the data graph, and the maximum of the similarities is taken as the similarity of the bunch graph.

Li and Wachs [67] proposed a hierarchical EGM algorithm for hand gesture recognition. The major improvement to the EGM algorithm is the use of levels of hierarchies assigned to the nodes. The visual features with higher likelihood (to be found on the target image) receive a higher hierarchy level compared to features those are less consistent with the graph model.

Three dimensional model fitting is used in [69] for hand pose estimation. The method estimates all joint angles reconstructing the hand pose as a voxel model. Then model fitting is done between the hand model and the voxel model, in the 3D space. The method uses only geometric information of hand model and the voxel model for model fitting and does not need any heuristic or priori information. However the algorithm requires faster implementation for realtime applications.

Yin and Xie [70] introduced a computer vision model of hand, instead of a kinematic model. The algorithm avoids the complexity in estimation of the angular and linear parameters of the kinematic model. They utilized topological features of the hand for 3D hand posture recognition. The edge point of fingers are extracted as points of interest. The hand is segmented from complex backgrounds using a restricted coulomb energy (RCE) neural network based on color segmentation.

A summary and comparison of the features of hand posture recognition algorithms reviewed in this section are provided in Tables 3
                            and 4
                           .

Depth cameras have been used in computer vision for several years. However the applicability of depth cameras was limited due to its high price and poor quality. The release of low cost color-depth (RGB-D) camera Kinect [77,78] by Microsoft has created a revolution in gesture recognition by providing high quality depth images, addressing issues like complex backgrounds and illumination variation. The device calculates a three dimensional map of the scene using a combination of RGB and IR camera. Recently Han et al. [79] provided a review of how Kinect is useful in addressing the fundamental problems in computer vision. The sensors such as MicrosoftKinect
                     (R) and ASUS Xtion PRO LIVE
                     (R) provide reliable tracking of human body postures in gaming scenarios. Based on the tracking these devices provide features such as the coordinates of a skeletal model, which are utilized for gesture recognition.

The skeletal data from these RGB-D sensors is to be converted to more meaningful and high level features, and algorithms are to be developed for the robust classification of gestures. Recognition of hand gestures is especially challenging due to the complex articulation and relatively smaller area of hand region. In addition, a robust hand gesture recognition algorithm must have invariance with respect to the size and speed of the gesture, and the orientation of gesturer. Rafael et al. [80] evaluated the influence of depth information in the gesture recognition process and concluded that use of depth silhouettes increases the recognition accuracy significantly. Dominio et al. [81] proposed an algorithm to combine multiple depth-based descriptors for hand gesture recognition.

The RGB-D cameras are mostly used for whole body gesture recognition [78,82–85], as these cameras provide skeletal tracking. This section of the paper surveys RGB-D camera based HGR algorithms
                        1
                     
                     
                        1
                        Many of the articles in this area are published conference proceedings reporting developmental work using Kinect sensor. The current survey is limited to selected relevant research articles.
                      by classifying the related literature into two categories, (a) Kinect based approaches, and (b) other RGB-D sensor based approaches.

Zhang et al. [98] proposed a new higher level descriptor called the Histogram of 3D Facets (H3DF), to explicitly encode the 3D shape information from lower level depth information. Kinect based features are utilized for both dynamic hand gesture recognition [89–93,95–97,99–111] and hand posture recognition [88,112–120].

Wu et al. [90] proposed a system to learn gestures from only one learning example per class, namely One-shot-learning. Features are extracted based on Extended-Motion-History-Image (Extended-MHI) and the gestures are classified by calculating the maximum correlation coefficient. Motion history images (MHI) [121] are used to represent motions of an object in a video. All frames in a video sequence are projected onto one image across the temporal axis, to capture the temporal information of the motion sequence. The extended-MHI is proposed to improve the performance of MHI by compensating on the non-moving regions and repetitive actions. Multi-view Spectral Embedding (MSE) algorithm is used to fuse the RGB and depth data in a physically meaningful manner. The MSE algorithm discovers the intrinsic relationship between RGB and depth features, improving on the recognition rate of the algorithm.

Lui [92,99] proposed a gesture recognition algorithm based on a nonlinear regression framework on manifolds. The underlying geometry and a least squares fitting is used to develop the algorithm. The least squares regression is formulated as a composite function, considering geometric properties. Gallo et al. [89] proposed a Kinect based gesture recognition system with its application to exploration of medical image data. Various gestures for functions like zooming, animation, region of interest extraction, rotation and translation of medical images are recognized by topological analysis of the hand region. Euclidean distance metric and covariances of a log-Euclidean metric are used as features in [93]. The gestures are classified using nearest neighbor classifier.

A novel one-shot-learning approach for gesture recognition from motion depth images based on template matching is presented in [100]. The method is based on the computation of space-time descriptors from the query video which measures the likeness of a gesture in a lexicon. The classifier is based on correlation coefficient from standard deviation of Fourier transform of the image and the MHI.

An algorithm for detection and recognition of hand gestures by combining DTW with probability estimates is proposed in [102]. The algorithm has robustness against position and orientation of the gesturer and speed of the gesture. Cheng et al. [103,104] proposed DTW based algorithms for 3D hand gesture recognition. A parameterized searching window is introduced in the cost matrix of traditional DTW approach to detect the beginning and end of specific gestures from an infinite trajectory gesture sequences.

Another algorithm for one-shot learning gesture recognition from RGB-D data is proposed by Wan et al. [101]. A new spatio-temporal feature representation called 3D enhanced motion scale-invariant feature transform (3D EMoSIFT) is used. The new feature set is invariant to scale and rotation as it fuses RGB-D data. A sparse coding method namely simulation orthogonal matching pursuit (SOMP) is applied to represent each feature by a linear combination of a small number of codewords.

A novel hand motion capture procedure based on 14-patch hand partition scheme is proposed in [117] for collecting real posture dataset in unconstrained conditions. Liang et al. [122] proposed a robust hand parsing scheme to extract a high-level description of the hand from the depth images. The method is robust to complex hand configurations.

Ren et al. [88,112] proposed a hand posture recognition system having robustness against variations in hand orientation, scale, and articulation. A distance metric called Finger-Earth Mover’s Distance (FEMD) is proposed for hand dissimilarity measure. The algorithm can recognize hand postures in spite of the variations, as it only matches the fingers (not the whole hand shape). A comparison of FEMD with shape context algorithm [123] is provided in [113]. The FEMD based algorithm has better accuracy and computational speed in comparison. In addition Zhou et al. [113] presented an application of FEMD based hand posture recognition algorithm for playing Sudoku game.

An algorithm for static and dynamic hand shape classification using randomized decision forests is proposed in [91]. The hand shape is classified using the data from depth sensors. The system performs independent of lighting conditions and it does not need a hand registration step. Class labels are assigned to each pixel on a depth image, and the final class label is determined by voting.

Kirac et al. [116] proposed a scheme for extracting the hand skeleton using random regression forests in realtime. The algorithm is robust to self-occlusion and low resolution of the depth camera, and can estimate the joint positions even if all of the pixels related to a joint are out of the camera frame. A feature set namely Oriented Radial Distribution is proposed in [118], which can simultaneously localize fingertips and encode hand postures globally.

Holte et al. [86] utilized an intensity-depth camera (CSEM Swissranger SR-2) to develop a view invariant gesture recognition algorithm. On contrary to the usual trajectory based approach gestures are recognized based on motion primitives in the 3D data. The primitives are represented in a view invariant manner using harmonic shape context. A probabilistic edit distance classifier is used for classification. The algorithm has orientation invariance, it is trained on data from one viewpoint and tested on data from a different viewpoint.

Probabilistic 2D templates created using hand motion trajectory are used in [94] for the recognition of dynamic gestures. The probabilistic template takes into account different trajectory distortions with different probabilities. A longest common subsequence (LCS) classifier is modified to most probable longest common subsequence (MPLCS) classifier, to measure the similarity between the probabilistic template and the hand gesture sample. Erden et al. [124] designed a hand gesture based remote control system which combines infrared sensors with an RGB camera.

Time-of-flight (ToF) and RGB cameras are combined in [87] to develop a hand detection algorithm based on depth and color. The position of hand is tracked in 3D in spite of its overlap with body parts and other hands in the background. The gestures are recognized using a nearest neighbor search after a dimensionality reduction using Average Neighborhood Margin Maximization (ANMM) [125].

A summary and comparison of the features of hand gesture and posture recognition algorithms surveyed in this section are provided in Tables 5 and 6
                           
                           .

Researchers from University of Cambridge and Microsoft Research have conducted a study [126] on how to instruct subjects to develop best representative gesture datasets for training machine learning algorithms. They used two measures, correctness and coverage, to evaluate how good the dataset is in representing real world data from a deployed system. The measure correctness refers to the similarity of subject movements to what the system developer needs them to perform. It depends on the understanding by the subject. The measure coverage refers to completeness of the dataset in representing natural and possible variations of associated movement patterns. Coverage is decided by the freedom given to the subject. They investigated the most appropriate semiotic modality of instructions and their order to achieve the best correctness and coverage, both for the dataset and the learnt gesture recognition system. The modalities investigated include descriptive text, static image sequence, and video. Video followed by text is selected as the best order of modality to facilitate both understanding and freedom of subjects.

Standard hand gesture databases are necessary for the reliable testing and comparison of hand gesture recognition algorithms. The availability of hand gesture databases was limited till the year 2007 and has been increased recently (Fig. 6). This section provides a review of publicly available hand gesture datasets. Table 7 lists hand posture and gesture databases with the web-links for their download. Table 8
                     
                      describes these datasets with details such as number of classes, subjects, and samples available. The works utilized the datasets are also included to facilitate possible comparative study. A total of 26 datasets
                        2
                     
                     
                        2
                        
                           Table 7 and 8 list and describe the first 22 hand gesture databases. The other 4 related hand databases are provided in Section 4.23.
                      are available at the publication time of this review.

The dataset contains three hand posture datasets, the Jochen Triesch Static Hand Posture Database [64], the Jochen Triesch Static Hand Posture Database II [63], and the Sebastien Marcel Static Hand Posture Database [133], and one dynamic hand gesture database, the Sebastien Marcel Dynamic Hand Posture Database [15]. The hand posture datasets have simple as well as complex backgrounds. The dynamic gestures include various commanding signals for Click, Stop-grasp-ok, Rotate, and No.

This dataset contains hand posture images. It has sequences of static images corresponding to hand motions, making it suitable for testing dynamic hand gesture recognition algorithms [137]. The data set consists of gestures defined by 3 primitive hand shapes (flat, spread, and V-shape) and 3 primitive motions (leftward, rightward, and contract). The target task for this data set is to classify hand shapes and motions at the same time. The dataset has fairly large intra-class variations in spatial and temporal alignment of hand gestures.

The database is useful in testing both hand gesture and posture recognition algorithms, as it contains both movement patterns and specific hand shapes [35]. It has 10 classes of dynamic hand gestures (eg. move right, move left, rotate up) performed with 7 different hand poses (eg. thumb, fist, all fingers extended), summing to 70 gesture samples per subject.

The database includes 24 body and hand gestures, selected from NATOPS (Naval Air Training and Operating Procedures Standardization) aircraft handling signals [132]. A stereo camera was used to collect the database. The database consists of videos with RGB and depth data. It also contains the extracted body and hand feature sets in Matlab and CSV formats.

This dataset contains 48 class alphabetical gestures (alphanumeric characters & graphic elements) recorded from 20 persons, 10 times each gesture [11]. The dataset contains sequences of x–y coordinates representing unspotted gestures.

The dataset contains 3D trajectories of segmented hand gestures, including the coordinates of head and torso [13,131]. Each trajectory is stored as a text file in the dataset. The dataset has both single handed (like stop, point left, point right) and two handed (like swim, fly, clap) gestures. Gesture trajectories contain 3D coordinates of center of the head, two hands and the torso.

The gesture dataset consists of 14 dynamic gestures, which are subsets of military signals (like turn left, go back, and speed up) [136]. The dataset is divided into two, training and testing sets. Training set is captured using a fixed camera with the person viewed against a simple and static background. Testing set is captured from a moving camera, in the presence of background clutter and other moving objects.

The 6D motion gesture database (6DMG) provides a comprehensive data of motion gestures, including the position, orientation, acceleration, and angular speed [130]. The data is stored in raw binary form and the dataset comes with sample C++ programs to access and visualize the data.

This dataset is created as part of a gesture recognition challenge; the ChaLearn gesture challenge 
                        [82,90–92,100,127]. The ChaLearn gesture data 2011 consists of a total of 62,000 samples. The dataset from 20 subjects is grouped into different batches each with 100 samples. The data is recorded with Kinect camera and consists of both RGB and depth videos of dynamic gestures. The dataset also 8000 samples of translated, scaled and occluded data.

In comparison to other datasets, the gestures in ChaLearn gesture data are useful in wide application domains. It contains nine categories of gestures corresponding to various application domains. The categories are (a) emblems (e.g. Indian Mudras), (b) illustrators (e.g. Italian gestures), (c) regulators (gesticulations performed to accompany speech), (d) pantomimes (gestures made to mimic actions), (e) signs (from sign languages for the deaf), (f) signals (e.g. marshaling signals to guide machinery or vehicle), (g) body language gestures (e.g. scratching head, crossing arms), (h) actions (e.g. drinking or writing), and (i) dance postures. Each set of data contains a number of actions presented separately once for training purpose. Combinations of one or more actions in a video sequence are available for testing.

In comparison to the ChaLearn gesture data, the testing using ChaLearn multi-modal gesture data [128] is more challenging. The ChaLearn multi-modal gesture data includes recording of continuous sequences, presence of distracter gestures, relatively large number of categories, lengthy gesture sequences, and gestures by a variety of users. Several modalities are provided in the data set, including audio, RGB, depth maps, user masks, and user skeletal model.
                     

This data is acquired using a Kinect camera and 4 inertial motion units attached to the right arm and the neck of subjects. Gestures are started from 3 different resting postures and recorded in 2 different lighting conditions [134].

SKIG dataset [135] has 10 categories of hand gestures, recorded from 6 subjects using RGB and Kinect cameras. The dataset is recorded with 3 different backgrounds (wooden board, white paper, and paper with characters) and 2 illumination conditions (light and dark).

Microsoft Research Cambridge-12 (MSRC-12) is a 12 class dynamic gesture dataset recorded using the skeletal data from Kinect [126]. The dataset consists of sequences of human movements, represented using body-part locations (20 skeletal joints). The data set includes 594 sequences and 719,359 frames.

The postures in this dataset are captured with various position and size of the hand within the image frame. Both color and gray-scale versions of the dataset are available. The hand postures in the dataset have less inter-class variation in appearance, which makes the recognition task challenging [45].

This complex background hand posture dataset [44] has three subsets; A, B, and C. Subset A has images with complex natural backgrounds and subset B has images with noises like body/face of the posturer or a group of other human in the background. Subset C consists of only background images (to be used as negative images for hand posture detection). The postures have various hand shapes and sizes, and are collected from subjects with various ethnicities. Subset A has 2000 images, B has 750 images, and C has 2000 images.

This is a kinect posture dataset [88] with 10 classes. It contains both color images and depth maps. The dataset is collected under cluttered backgrounds.

The hand gesture annotation in this dataset [118] is done among 9 gesture classes and the dataset has strong intra-class variation. Fingertip annotation is done in the dataset using colored gloves, easing the detection and localization of fingertips.

The NYU hand pose dataset [138] has 72,757 and 8252 numbers of frames in the training and test sets respectively. The data is captured using 3 Kinect sensors providing a frontal view and 2 side views. Training set is captured form 1 user and test set is captured from 2 users.

This dataset contains a variety of 22 sequences, demonstrating different view-points, scales, poses, occlusions, and camera technologies. The dataset is useful to evaluate hand detection and pose estimation algorithms.

This hand gesture dataset [139] contains 12 class data from 11 subjects. Also it contains synthetically generated data and is useful in evaluating hand posture recognition algorithms.

This dataset [140] contains gestures from Polish Sign Language and ASL, and is organized into three series acquired under different conditions. It has up to 32 gesture classes acquired from 18 different subjects.

ASL Finger Spelling Dataset [141] consists of 24 hand postures, English letters from a to y except j. It contains an easy set, Dataset A, captured from 5 subjects without lighting variation, and a hard set, Dataset B, captured from 9 subjects with lighting variations.

A few other publicly available databases relevant to hand gesture recognition research are (a) MSRA Hand Tracking database [142] (http://research.microsoft.com/en-us/um/people/yichenw/handtracking/index.html), (b) American Sign Language Lexicon Video Dataset [143] (http://www.bu.edu/asllrp/cslgr/), (c) Bosphorus hand databases (http://bosphorus.ee.boun.edu.tr/hand/Home.aspx), and (d) UNIGE-HANDS Hand detection dataset [159] (http://alejobetancourt.com/resume/dataset?id=1).

@&#DISCUSSION@&#

The chart in Fig. 5
                      shows the fast growth in hand gesture recognition research
                        3
                     
                     
                        3
                        Based on relevant articles covered in this review.
                      and that in Fig. 6 shows the growth in release of hand gesture databases. In spite of these developments there are still unresolved challenges in gesture recognition. This section briefly reviews some of the unresolved issues in the field, provides a comparison of different approaches, and discusses a few future research directions.

The recognition of illustrators is challenging as the meaning of these gestures is depended on the context. The context reference is to be recognized in addition to the recognition of an illustrator gesture. Among various illustrators, the pointing gesture is very useful in applications like mobile robot commanding. Understanding a pointing gesture in 3D involves detection of the gesture, finding the hand position, and identification of the pointed direction. The difficultly in accurate estimation of the pointing direction makes pointing gesture recognition challenging. Heuristic such as the direction at which the subject looks is useful in recognizing a pointing gesture. For example a line joining the center of the eyes with the tip of the index finger can provide an estimate of the pointed direction, which in turn can be utilized to identify the targeted point [144,145].

The pointing direction estimation using head-hand line is effective when pointing hand extends outwards and lies on the surface of an imaginary hemisphere centered on the shoulder [146,147]. However this method is not effective in the case of compact pointing gestures in which a person moves only the forearm. Such small pointing gestures can be recognized by modeling the kinematic characteristics of forearm and pointing finger [146]. Head orientation can be utilized as a feature to improve the performance of pointing gesture recognizer [148]. In comparison, the direction estimation using head-hand line outperforms that based on orientation of the forearm, in the case of a normal pointing gesture [148]. Raheja et al. [149] proposed an algorithm for hand gesture pointing location detection which is based on locations of head, shoulders and elbows. The method proposed by Pateraki et al. [150,151] combined face pose and head orientation with the hand direction.

Appearance-based approaches provide better realtime performance compared to 3D hand model based approaches, as the image feature extraction process is faster. Appearance-based models lead to computationally efficient algorithms that work well under constrained situations, but lack the generality desirable for human computer interaction. Appearance based methods mainly utilizes the 2D shape data of the hand which is dependent on the viewing angle. The use of such methods is limited by the viewing perspectives. A wide class of hand gestures could be covered in 3D hand model based approaches, as the models offer a way for elaborate hand gesture modeling. However 3D models need large image database to cover all the characteristic shapes and its variations under different views. Matching the test image with all the models in the database is time consuming and computationally expensive which limits the usage of 3D models for realtime applications.

Selectivity and invariance are two desired qualities for any image based pattern recognition process. Template based approaches provide good selectivity for shape patterns, lacking invariance. Histogram based approaches have invariance property. However histogram approaches consider the integrated image information, which makes it unsuitable for shape recognition tasks like hand posture recognition. Shape-texture patterns extracted using biologically inspired approaches [152] provide features having both selectivity and invariance, and are useful in hand posture recognition [44].

Orientation and angular features of gestures provide better invariance compared to positional features. On the other hand positional features are simple and can be extracted with better accuracy. Texture based features have the capability to capture spatial properties better in comparison to that captured by features such as color.

The RGB-D sensors enable extraction of invariant features in spite of complex backgrounds and variations in scale, lighting, and view-points. The accurate depth data and position information form these sensors speed-up the extraction of hand models, increasing the utility of model based approaches.

HMM based methods are effective and are widely used for HGR. However HMM based approaches require a large number of training samples and have the disadvantage of elaborate training procedure. The computational costs of HMM based algorithms increase with the gesture vocabulary. In addition, the performance of HMM based algorithms reduces when there are variations between training and testing conditions. Finding the optimal parameter sets and trajectory spotting for temporal segmentation are other bottlenecks in using HMM.

The design of a TDNN is attractive as its compact structure economizes on the weights, and makes it possible to develop more generic feature detectors. The hierarchy of delays in TDNN optimizes these feature detectors by increasing their scope at each layer. Temporal integration of features at the output layer makes the network shift invariant (insensitivity to exact hand position). The total number of weights in the network is relatively small since only a small window of the input pattern is fed to TDNN at any instance. This helps to reduce the training time.

Graph based algorithms have the disadvantage of high computational complexity, which leads to its unsuitability for realtime applications. However each node in the graph can be modeled with a bunch of node features, which is useful in addressing issues due to complex backgrounds [63] and size or shape variations.

Identification of the gesturing phase is a major challenge in HGR. The presence of unpredictable and ambiguous non-gesture hand motions makes the task challenging. Capability to reject unknown classes is one of the important requirements for an automatic gesture recognizer. The threshold model concept introduced by Lee and Kim [10] is useful for this purpose. The simultaneous gesture segmentation and recognition algorithm proposed by Kim et al. [17] utilized a continuous probability estimation of gestures and non-gestures to find the start/ end points. Kang et al. [153] proposed a recognition based gesture spotting scheme to filter out unintentional movements. Recently Yin et al. [154] used a concatenated HMM to perform gesture spotting in continuous data stream, attaining encouraging experimental results.

The transition movements between adjacent gestures is another related issue in automatic recognition of continuous gestures, especially in applications like sign language recognition. Yang et al. [155] addressed the issue of handling movement epenthesis using a dynamic programming based approach. Li et al. [156] proposed and compared three methods based on a gesture model for end point localization. The methods investigated are a multi-scale search, dynamic time warping, and dynamic programming. In comparison, the dynamic programming based method outperformed the other two. A nested, level-building based dynamic programming approach is proposed by Sarkar et al. [157] to address the uncertainties of sign boundaries in sentences.

Matching an image sequence to a model is a central issue in HGR. Yang et al. [158] proposed a minimization algorithm to match groups of image primitives with statistical (HMM) as well as non-statistical (sample-based) models. The algorithm neither needed a perfect segmentation of the scene nor the tracking of features across frames.

The recent trend of One-shot-learning 
                        [90,97,100,101] in gesture recognition is promising. The one-shot-learning consists of learning a gesture by observing only one instance of that gesture, similar to the learning in human. It has created the opportunity to take-up the challenge of extraction of discriminative features as well as design of competitive classifiers using only one training example per class. Also one-shot-learning facilitates critical comparison between gesture recognition algorithms.

The hand gestures utilized in existing gesture recognition systems are limited to a carefully chosen vocabulary of symbolic gestures (emblems and illustrators), mainly used for issuing commands. Recognition of gestures from regulators, affect displays, and adaptors (Section 1) are necessary for the natural interaction between humans and machines. Algorithms with better invariance capabilities, having the potential to recognize a wide number of classes without extensive training, are to be developed for making machines with capability to understand human intentions and motion patterns better.

The impact of embodied interactions through gestures on enhancing visual processing and attention is least explored. For example exploring how a waving hand captures human attention will be useful for developing the attentional mechanism of an interactive robot. Another future research direction is the exploration of primate brain areas to develop computational models to imitate the gestural pattern recognition process.

@&#REFERENCES@&#

