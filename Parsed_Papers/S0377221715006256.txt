@&#MAIN-TITLE@&#A novel multi-objective particle swarm optimization with multiple search strategies

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Two search strategies are designed for updating the velocity of each particle.


                        
                        
                           
                           An evolutionary search strategy is performed on the external archive of PSO.


                        
                        
                           
                           A new definition of personal-best and global-best particles is given.


                        
                        
                           
                           A novel multi-objective PSO is designed based on decomposition approach.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Multiple objective programming

Particle swarm optimization

Evolutionary computation

Multiple search strategies

@&#ABSTRACT@&#


               
               
                  Recently, multi-objective particle swarm optimization (MOPSO) has shown the effectiveness in solving multi-objective optimization problems (MOPs). However, most MOPSO algorithms only adopt a single search strategy to update the velocity of each particle, which may cause some difficulties when tackling complex MOPs. This paper proposes a novel MOPSO algorithm using multiple search strategies (MMOPSO), where decomposition approach is exploited for transforming MOPs into a set of aggregation problems and then each particle is assigned accordingly to optimize each aggregation problem. Two search strategies are designed to update the velocity of each particle, which is respectively beneficial for the acceleration of convergence speed and the keeping of population diversity. After that, all the non-dominated solutions visited by the particles are preserved in an external archive, where evolutionary search strategy is further performed to exchange useful information among them. These multiple search strategies enable MMOPSO to handle various kinds of MOPs very well. When compared with some MOPSO algorithms and two state-of-the-art evolutionary algorithms, simulation results show that MMOPSO performs better on most of test problems.
               
            

@&#INTRODUCTION@&#

In many real-world engineering applications, the problem that needs to optimize multiple objectives simultaneously is often encountered, which is called multi-objective optimization problems (MOPs) (Deb, Pratap, Agarwal, & Meyarivan, 2002; Ishibuchi & Murata, 1998; Samanlioglu, 2013). For example, the goals in job shop scheduling are commonly required to minimize the makespan, total workload, and critical workload, while the targets in product design are certainly needed to minimize the cost of product and optimize its quality. Since the conflicts exist among the objectives, the improvement of one objective may deteriorate other objectives and resultantly it generates a set of equally-optimal solutions, which is termed Pareto-optimal set (PS). The corresponding mapping of PS in objective space is termed Pareto-optimal front (PF). As the size of PF may be infinite, it is impractical to find out all the Pareto-optimal solutions. Thus, an important job of MOPs is to obtain a finite size of PS that is distributed uniformly along the PF, which supports the decision maker to select the appropriate solutions for different practical cases (Lin & Chen, 2013; Zhang & Li, 2007).

Currently, nature-inspired metaheuristic algorithms have been recognized to be well suitable for solving MOPs since they can handle some complex problems that are characterized with multimodality, nonlinearity, and discontinuity (Jones, Mirrazavi, & Tamiz, 2002). Among them, particle swarm optimization (PSO) is an interesting nature-inspired algorithm that mimics the social cooperative and competitive behavior of bird flocking and fish schooling (Kennedy & Eberhart, 1995). Due to the fast convergence speed and easy implementation, it has attracted a great interest of researchers and been designed for solving many single-objective optimization problems (SOPs) and various engineering applications (Dang et al., 2013; Nayeri, Yang, & Elsherbeni, 2013; Unler & Murat, 2010). The promising results provided by PSO for solving SOPs validate its effectiveness and efficiency to locate the optimal results in a large and complex problem landscape. This motivates the researchers to extend PSO for MOPs and plenty of multi-objective PSO (MOPSO) algorithms are presented accordingly (Moubayed, Pertovski, & McCall, 2014; Coello Coello, Pulido, & Lechuga, 2004; Goh, Tan, Liu, & Chiam, 2010; Zhan et al., 2013). Generally, most of the existing MOPSO algorithms can be classified into two categories. The first class embeds the Pareto dominance relationship into PSO, which is used to determine the personal best and global best particles (Nebro et al., 2009; Sierra & Coello Coello, 2005; Wang & Yang, 2010). The second kind adopts decomposition approach to transform MOPs into a set of SOPs, where traditional PSO can be directly applied to solve MOPs (Moubayed, Pertovski, & McCall, 2010; Martinez & Coello Coello, 2011; Peng & Zhang, 2008). These MOPSO algorithms perform very well in solving some MOPs. However, when tackling the complex MOPs characterized with multimodality and the existence of many local PFs, e.g., WFG1 (Huband, Barone, While, & Hingston, 2005) and DTLZ3 (Deb, Thiele, Laumanns, & Zitzler, 2005), most MOPSO algorithms fail to effectively approach the true PF. This is mainly because they only adopt a single search strategy to update the velocity of each particle, which may lack the capabilities to tackle some kinds of complex MOPs.

To repair this weakness, multiple search strategies may be an alternative technology as it has been studied experimentally in PSO for solving SOPs and proven to be an effective and efficient approach to enhance the capabilities of PSO when handling various types of SOPs (Hu, Wu, & Weir, 2013; Li, Yang, & Nguyen, 2012; Zuo, Zhang, & Tan, 2014). Inspired by the reported multiple search strategies for SOPs, it is reasonable to believe that multiple search strategies can be applied in MOPSO to further improve its convergence speed and the robustness when dealing with different kinds of MOPs. Therefore, a novel MOPSO algorithm with multiple search strategies is presented in this paper, called MMOPSO. Decomposition approach is adopted in MMOPSO to decompose MOPs into a set of SOPs and then each particle is assigned to optimize each SOP. Two search strategies for updating the particle's velocity are designed to accelerate the convergence speed and maintain the population diversity respectively. Their cooperation is controlled by a pre-defined threshold. All the non-dominated solutions visited by the particles are stored in a finite-size external archive. Once the external archive is full, only the non-dominated solutions with bigger crowding-distance values will be remained, which are considered to be the elitist solutions and good representatives of the entire PF. To let the elitist information be shared among the external archive, an evolutionary search strategy, composed by simulated binary crossover (SBX) and polynomial mutation (PM), is performed, which enhances the exploratory capabilities of MMOPOS. When compared with the existing MOPSO algorithms, the novelty of MMOPSO can be described as follows.

                        
                           (1)
                           Different from the single search pattern adopted in most MOPSO algorithms, two search strategies are designed in MMOPSO for updating the velocity of each particle, which are aimed at accelerating the convergence speed and maintaining the population diversity respectively. Their executions are determined by a pre-defined threshold to retain the balance of exploitation and exploration.

An evolutionary search strategy is run on the external archive of PSO, which is beneficial for the information exchange among the elitist individuals. The evolutionary operators can provide another search power for PSO and remedy the weaknesses of PSO-based search when handling some difficult MOPs.

New definitions of personal-best and global-best particles are given in MMOPSO. Traditionally, personal-best and global-best particles are the best ones visited by each particle and the swarm respectively. Whereas, in MMOPSO, as decomposition approach is adopted to transform MOPs into a set of SOPs, personal-best and global-best particles are respectively considered to be the best values of each aggregation problem and all SOPs. Therefore, MMOPSO can focus on optimizing each aggregation problem by using PSO search.

The advantages of multiple search strategies will be investigated and validated by the experimental studies. Total 24 standard benchmark problems, including Fonseca (Fonseca & Flemming, 1998), Kursawe (1990), Schaffer (1985), ZDT (Zitzler, Deb, & Thiele, 2000), WFG (Huband et al., 2005) and DTLZ (Deb et al., 2005) series test problems, are utilized to evaluate the comprehensive performance of MMOPSO. When compared with some MOPSO algorithms and two state-of-the-art multi-objective evolutionary algorithms (MOEAs), e.g., DDMOPSO (Moubayed et al., 2014), CMPSO (Zhan et al., 2013), SMPSO (Nebro et al., 2009), dMOPSO (Martinez & Coello Coello, 2011), OMOPSO (Sierra & Coello Coello, 2005), NSGA-II (Deb et al., 2002) and MOEA/D (Li & Zhang, 2009), MMOPSO performs better on most of test problems when considering both of the convergence speed and population diversity.

The rest of this paper is organized as follows. Section 2 introduces the related background, including some important terms of MOPs, decomposition approach, traditional PSO and the existing MOPSO algorithms. In Section 3, the details of MMOPSO are described, where the framework of MMOPSO and multiple search approaches are illustrated. The experimental studies are given in Section 4, which compare the performance of MMOPSO with various multi-objective optimization algorithms and analyze the advantages of multiple search strategies in MMOPSO. At last, conclusions are summarized in Section 5.

@&#RELATED WORK@&#

A continuous and unconstrained multi-objective optimization problem can be formulated as follows.

                           
                              (1)
                              
                                 
                                    
                                       Min
                                       
                                          x
                                          ∈
                                          Ω
                                       
                                    
                                    
                                    F
                                    
                                       (
                                       x
                                       )
                                    
                                    =
                                    
                                       
                                          (
                                          
                                             f
                                             1
                                          
                                          
                                             (
                                             x
                                             )
                                          
                                          ,
                                          
                                             f
                                             2
                                          
                                          
                                             (
                                             x
                                             )
                                          
                                          ,
                                          …
                                          ,
                                          
                                             f
                                             m
                                          
                                          
                                             (
                                             x
                                             )
                                          
                                          )
                                       
                                       T
                                    
                                 
                              
                           
                        where 
                           
                              x
                              =
                              (
                              
                                 x
                                 1
                              
                              ,
                              
                                 x
                                 2
                              
                              ,
                              …
                              ,
                              
                                 x
                                 n
                              
                              )
                           
                         is a n-dimensional decision vector bounded in the decision space Ω, m is the number of objective functions and the mapping function F: Ω → Rm
                         defines m objective functions bounded in the objective space Rm
                        . Since the objectives often contradict each other, the improvement of one objective may deteriorate other objectives. Therefore, the output of MOPs is generally a set of equally-optimal solutions, which can be determined by Pareto optimality (Bosman & Thierens, 2003).


                        
                           Definition 1
                           
                              (Pareto-dominance): A decision vector x is said to dominate another decision vector y (noted as x≻y) if and only if

                                 
                                    (2)
                                    
                                       
                                          
                                             (
                                             ∀
                                             i
                                             ∈
                                             
                                                {
                                                1
                                                ,
                                                2
                                                ,
                                                …
                                                ,
                                                m
                                                }
                                             
                                             :
                                             
                                                f
                                                i
                                             
                                             
                                                (
                                                x
                                                )
                                             
                                             ≤
                                             
                                                f
                                                i
                                             
                                             
                                                (
                                                y
                                                )
                                             
                                             )
                                          
                                          ∧
                                          
                                             (
                                             ∃
                                             
                                             j
                                             ∈
                                             
                                                {
                                                1
                                                ,
                                                2
                                                ,
                                                …
                                                ,
                                                m
                                                }
                                             
                                             :
                                             
                                                f
                                                j
                                             
                                             
                                                (
                                                x
                                                )
                                             
                                             <
                                             
                                                f
                                                j
                                             
                                             
                                                (
                                                y
                                                )
                                             
                                             )
                                          
                                       
                                    
                                 
                              
                           


                        
                           Definition 2
                           
                              (Pareto-optimal): A solution x is said to be Pareto-optimal if and only if

                                 
                                    (3)
                                    
                                       
                                          ¬
                                          ∃
                                          y
                                          ∈
                                          Ω
                                          :
                                          y
                                          ≻
                                          x
                                          .
                                       
                                    
                                 
                              
                           


                        
                           Definition 3
                           
                              (Pareto-optimal set): The set 
                                 PS
                               includes all Pareto-optimal solutions, defined as

                                 
                                    (4)
                                    
                                       
                                          PS
                                          =
                                          {
                                          x
                                          |
                                          ¬
                                          ∃
                                          y
                                          ∈
                                          Ω
                                          :
                                          y
                                          ≻
                                          x
                                          }
                                          .
                                       
                                    
                                 
                              
                           


                        
                           Definition 4
                           
                              (Pareto-optimal front): The set 
                                 PF
                               includes the values of all the objective functions corresponding to the Pareto-optimal solutions in 
                                 PS
                              .

                                 
                                    (5)
                                    
                                       
                                          PF
                                          =
                                          {
                                          F
                                          
                                             (
                                             x
                                             )
                                          
                                          =
                                          
                                             
                                                (
                                                
                                                   f
                                                   1
                                                
                                                
                                                   (
                                                   x
                                                   )
                                                
                                                ,
                                                
                                                   f
                                                   2
                                                
                                                
                                                   (
                                                   x
                                                   )
                                                
                                                ,
                                                …
                                                ,
                                                
                                                   f
                                                   m
                                                
                                                
                                                   (
                                                   x
                                                   )
                                                
                                                )
                                             
                                             T
                                          
                                          |
                                          x
                                          ∈
                                          PS
                                          }
                                          .
                                       
                                    
                                 
                              
                           

Recently, decomposition approach is widely embedded into nature-inspired metaheuristic for solving MOPs (Gong et al., 2014; Liu, Gu, & Zhang, 2014). It is based on the facts that a Pareto-optimal solution for MOPs, under some mild conditions, could be an optimal solution of a scalar optimization problem, whose optimization target is an aggregation of all the objectives. Therefore, the finding of PF can be decomposed into a set of SOPs (Li & Zhang, 2009; Zhang & Li, 2007). Currently, the popular decomposition approaches include the weighted sum, Tchebycheff and boundary intersection approaches. Among them, boundary intersection method has shown certain advantages over the other two approaches as discussed in (Martinez & Coello Coello, 2011; Zhang & Li, 2007). Thus, boundary intersection method is adopted in MMOPSO, which uses the pre-defined weighted vectors λ and a penalty value θ to minimize the distance d
                        1 to the utopia vector and the direction error to the weighted vector d
                        2 from the solution F(x) in the objective space, as defined by

                           
                              (6)
                              
                                 
                                    
                                       Min
                                       
                                          x
                                          ∈
                                          Ω
                                       
                                    
                                    
                                    g
                                    
                                       (
                                       x
                                       |
                                       λ
                                       ,
                                       
                                          z
                                          *
                                       
                                       )
                                    
                                    =
                                    
                                       d
                                       1
                                    
                                    +
                                    θ
                                    
                                       d
                                       2
                                    
                                 
                              
                           
                        where

                           
                              (7)
                              
                                 
                                    
                                       d
                                       1
                                    
                                    =
                                    
                                       
                                          
                                             ∥
                                          
                                          
                                             
                                                
                                                   (
                                                   F
                                                   
                                                      (
                                                      x
                                                      )
                                                   
                                                   −
                                                   
                                                      z
                                                      *
                                                   
                                                   )
                                                
                                                T
                                             
                                             λ
                                          
                                          
                                             ∥
                                          
                                       
                                       
                                          ∥
                                          λ
                                          ∥
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (8)
                              
                                 
                                    
                                       d
                                       2
                                    
                                    =
                                    
                                       ∥
                                       
                                          
                                             (
                                             F
                                             
                                                (
                                                x
                                                )
                                             
                                             −
                                             
                                                z
                                                *
                                             
                                             )
                                          
                                          −
                                          
                                             d
                                             1
                                          
                                          
                                             λ
                                             
                                                ∥
                                                λ
                                                ∥
                                             
                                          
                                       
                                       ∥
                                    
                                 
                              
                           
                        and 
                           
                              
                                 z
                                 *
                              
                              =
                              
                                 (
                                 
                                    z
                                    1
                                    *
                                 
                                 ,
                                 
                                    z
                                    2
                                    *
                                 
                                 ,
                                 …
                                 ,
                                 
                                    z
                                    m
                                    *
                                 
                                 )
                              
                           
                         is the reference point, i.e., 
                           
                              
                                 z
                                 i
                                 *
                              
                              =
                              min
                              
                                 {
                                 
                                    f
                                    i
                                 
                                 
                                    (
                                    x
                                    )
                                 
                                 |
                                 x
                                 ∈
                                 Ω
                                 }
                              
                           
                         for each 
                           
                              i
                              =
                              1
                              ,
                              2
                              ,
                              …
                              ,
                              m
                           
                        . In practical implementation, as z* is unavailable in advance, it is usually replaced by the minimal value of each objective found by the algorithms so far. The generation approaches for uniform weighted vectors λ have been introduced in (Li & Zhang, 2009; Zhang & Li, 2007).

Particle swarm optimization is an interesting nature-inspired metaheuristic originally proposed by Kennedy and Eberhart (1995) for dealing with global optimization problems. By simulating the movement rules of bird flocking and fish schooling, it is very capable for locating the optimal value in a large searching space. In PSO, a swarm is composed by a certain number of particles. Each particle represents a potential solution for the optimization problem, which is characterized by its position and moving velocity. Here, it is assumed that there are N particles in a swarm. When searching an n-dimensional hyperspace, the position of particle i (i = 1, 2
                           
                              ,
                              …
                              ,
                           
                         
                        N) indicates the solution location in search space, as represented by 
                           
                              
                                 x
                                 i
                              
                              =
                              
                                 (
                                 
                                    x
                                    
                                       i
                                       1
                                    
                                 
                                 ,
                                 
                                    x
                                    
                                       i
                                       2
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    x
                                    
                                       i
                                       n
                                    
                                 
                                 )
                              
                           
                        . The positional movement of particle i is recorded using its velocity, as described by 
                           
                              
                                 v
                                 i
                              
                              =
                              
                                 (
                                 
                                    v
                                    
                                       i
                                       1
                                    
                                 
                                 ,
                                 
                                    v
                                    
                                       i
                                       2
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    v
                                    
                                       i
                                       n
                                    
                                 
                                 )
                              
                           
                        . Each particle i will memorize its historically best position as noted by 
                           
                              p
                              b
                              e
                              s
                              
                                 t
                                 
                                    
                                    i
                                 
                              
                              =
                              
                                 (
                                 
                                    p
                                    
                                       i
                                       1
                                    
                                 
                                 ,
                                 
                                    p
                                    
                                       i
                                       2
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    p
                                    
                                       i
                                       n
                                    
                                 
                                 )
                              
                           
                         and the best one among all pbesti
                         in a swarm is acknowledged as the globally best position gbest. Each particle i is evolved by exploiting positional information from the selected global leader and its own personal best to update its velocity and position values, as expressed in Eqs. (9) and (10).

                           
                              (9)
                              
                                 
                                    
                                       v
                                       i
                                    
                                    
                                       (
                                       t
                                       +
                                       1
                                       )
                                    
                                    =
                                    w
                                    
                                       v
                                       i
                                    
                                    
                                       (
                                       t
                                       )
                                    
                                    +
                                    
                                       c
                                       1
                                    
                                    
                                       r
                                       1
                                    
                                    
                                       (
                                       
                                          x
                                          
                                             p
                                             b
                                             e
                                             s
                                             
                                                t
                                                i
                                             
                                          
                                       
                                       −
                                       
                                          x
                                          i
                                       
                                       
                                          (
                                          t
                                          )
                                       
                                       )
                                    
                                    +
                                    
                                       c
                                       2
                                    
                                    
                                       r
                                       2
                                    
                                    
                                       (
                                       
                                          x
                                          
                                             g
                                             b
                                             e
                                             s
                                             t
                                          
                                       
                                       −
                                       
                                          x
                                          i
                                       
                                       
                                          (
                                          t
                                          )
                                       
                                       )
                                    
                                 
                              
                           
                        
                        
                           
                              (10)
                              
                                 
                                    
                                       x
                                       i
                                    
                                    
                                       (
                                       t
                                       +
                                       1
                                       )
                                    
                                    =
                                    
                                       x
                                       i
                                    
                                    
                                       (
                                       t
                                       )
                                    
                                    +
                                    
                                       v
                                       i
                                    
                                    
                                       (
                                       t
                                       +
                                       1
                                       )
                                    
                                 
                              
                           
                        where t is the iteration number, w is the inertial weight, c
                        1 and c
                        2 are two learning factors from the personal and global best particles respectively, r
                        1 and r
                        2 are two random numbers generated uniformly in the range [0, 1].

Particle swarm optimization is originally designed for solving SOPs. To extend PSO for tackling MOPs, Pareto ranking method or decomposition approach is embedded into PSO. Thus, the existing MOPSO algorithms can be generally classified into two categories. The first class uses Pareto ranking to determine the personal best and global best particles. The global best particles are generally the non-dominated solutions found during the particle movement and they can be exploited to guide the particle swarm to approach the entire PF. The reported MOPSOs, such as OMOPSO (Sierra & Coello Coello, 2005) and SMPSO (Nebro et al., 2009), belong to this category. The second type adopts decomposition approach for transforming MOPs into a set of SOPs and then PSO can be directly applied to solve each SOP. This kind of MOPSOs can exploit the reported technologies of PSO to better solve MOPs. The representatives of these MOPSO algorithms include SDMOPSO (Moubayed et al., 2010), dMOPSO (Martinez & Coello Coello, 2011), CMPSO (Zhan et al., 2013) and DDMOPSO (Moubayed et al., 2014). All of these representative MOPSOs are introduced briefly as follows.

OMOPSO is proposed by Sierra and Coello Coello (2005), which uses Pareto dominance and crowding-distance information to identify the list of leader solutions. To enhance the search capability, two mutation operators, i.e., uniform and non-uniform mutations, are respectively executed to balance the abilities of exploration and exploitation. Moreover, an external archive is exploited to collect all the non-dominated solutions visited by the swarm and the concept of ε-dominance is utilized to limit the size of this archive.

SMPSO is an improved version of OMOPSO as designed by Nebro et al. (2009), which embeds a velocity construction procedure in the movement of particles to prevent the so-called “swarm explosion” effect (Clerc & Kennedy, 2002) in OMOPSO. Thus, SMPSO is able to produce new effective particles in the cases that the velocity of particle becomes too high. Besides that, polynomial mutation is performed after PSO search as a turbulence factor and an external archive is used to preserve a number of the historically found non-dominated solutions.

MOPSO/D is reported by Peng and Zhang (2008), which may be the first attempt to embed decomposition approach into MOPSO. It follows the framework of MOEA/D (Zhang & Li, 2007) and replaces the genetic search method with the traditional PSO search approach. The updates of personal and global particles are fully decided by the aggregation values of all objectives. After that, a turbulence operator is performed and an external archive based on ε-dominance is used to collect a number of non-dominated solutions that are historically found during the PSO search.

SDMOPSO is an enhanced algorithm from MOPSO/D as designed by Moubayed et al. (2010), which tackles the drawback of MOPSO/D by fully exploiting the salient properties of neighborhood relations in PSO. The particle's global best is only picked from the neighboring solutions and each particle is only associated with a unique weight vector that gives the best scalar aggregated fitness value. Moreover, a crowding archive is also adopted in SDMOPSO to maintain the diversity of the swarm leaders.

dMOPSO is presented by Martinez and Coello Coello (2011), which is fully dependent on decomposition approach to solve MOPs. The position of each particle is updated using a set of global particles, which are determined based on the scalar aggregated values. The distinct feature of dMOPSO is that a memory re-initialization procedure is used when the particle exceeds a certain age, which is aimed at maintaining the diversity of the swarm and avoiding the trap in local PFs. However, as pointed out by Moubayed et al. (2014), the absence of dominance relation in dMOPSO may lead to the fail to cover the entire PF in some complex MOPs.

CMPSO is designed by Zhan et al. (2013), which is a novel coevolutionary technique for PSO to solve MOPs. It provides a simple and straightforward way to solve MOPs by letting each swarm correspond with each objective. An external shared archive is used to store all the visited non-dominated solutions and allow the information exchange among the elitist individuals. Two novel approaches are presented to enhance its performance. The first method embeds the elitist information from the shared archive to update the particle's velocity, while the second approach presents an elitist learning strategy for archive update to improve the swarm diversity and avoid the trap in local PFs.

The original DDMOPSO is proposed by Moubayed, Pertovski, and McCall (2012), which integrates both of dominance and decomposition approaches for solving MOPs. Afterward, an improved version is also presented by the same authors (Moubayed et al., 2014), which can fast converge to the true PF without using the genetic operators. It proposes a new mechanism for the selection of the particle leaders and a novel archiving technique that collects the non-dominated particles based on the crowding-distance values in both objective and solution spaces.

Inspired by the above MOPSO algorithms, the proposed MMOPSO algorithm also decomposes MOPs into a set of aggregation problems and adopts a crowding archive to preserve a number of the non-dominated solutions. However, when compared with the above MOPSO algorithms, the distinct features of MMOPSO include two search strategies that are designed to update the velocity of each particle, an evolutionary search strategy that is performed on the archive to exchange beneficial information among the elitist individuals, a new definition of the personal best and global best particles. All of the new features improve MMOPSO on both of the convergence speed and swarm diversity, which will be analyzed and discussed in the experimental studies.

The proposed MMOPSO algorithm is based on decomposition approach to transform MOPs into a set of scalar aggregation problems, which adopts boundary intersection method as introduced in Section 2.2. Each particle in MMOPSO is aimed at optimizing each aggregation problem by updating its flight velocity and then all the non-dominated solutions visited by the particles are maintained in an external finite-size archive. Once the archive is full, the non-dominated solutions with bigger crowding-distance values will be remained. In the following subsections, the main procedures of MMOPSO, such as two search strategies for velocity update, evolutionary search on the external archive and archive update, are respectively described. At last the complete MMOPSO algorithm is illustrated.

In traditional PSO algorithm, the velocity and position values of the particles are usually updated using the positional information of the personal-best and global-best particles, as defined in Eqs. (9) and (10). However, as pointed out above, this single search pattern may cause some difficulties when solving some complex MOPs. Therefore, inspired by the multiple search patterns reported for solving SOPs (Hu et al., 2013; Li et al., 2012; Zuo et al., 2014), two velocity update equations are incorporated into MMOPSO, which are respectively used for exploitation and exploration in search space. They are cooperated with the decomposition approach, attempting to optimize each aggregation problem. Assuming that there are N particles in a swarm, for each particle i (i = 1, 2
                           
                              ,
                              …
                              ,
                           
                         
                        N), it associates with a unique weight vector λi
                         used in Eqs. (6)–(8). In MMOPSO, the velocity of particle i (i = 1, 2
                           
                              ,
                              …
                              ,
                           
                         
                        N) is updated as defined in Eq. (11) or (12).

                           
                              (11)
                              
                                 
                                    
                                       v
                                       i
                                    
                                    
                                       (
                                       t
                                       +
                                       1
                                       )
                                    
                                    =
                                    w
                                    
                                       v
                                       i
                                    
                                    
                                       (
                                       t
                                       )
                                    
                                    +
                                    
                                       c
                                       1
                                    
                                    
                                       r
                                       1
                                    
                                    
                                       (
                                       
                                          x
                                          
                                             p
                                             b
                                             e
                                             s
                                             
                                                t
                                                i
                                             
                                          
                                       
                                       −
                                       
                                          x
                                          i
                                       
                                       
                                          (
                                          t
                                          )
                                       
                                       )
                                    
                                 
                              
                           
                        
                        
                           
                              (12)
                              
                                 
                                    
                                       v
                                       i
                                    
                                    
                                       (
                                       t
                                       +
                                       1
                                       )
                                    
                                    =
                                    w
                                    
                                       v
                                       i
                                    
                                    
                                       (
                                       t
                                       )
                                    
                                    +
                                    
                                       c
                                       2
                                    
                                    
                                       r
                                       2
                                    
                                    
                                       (
                                       
                                          x
                                          
                                             g
                                             b
                                             e
                                             s
                                             t
                                          
                                       
                                       −
                                       
                                          x
                                          i
                                       
                                       
                                          (
                                          t
                                          )
                                       
                                       )
                                    
                                 
                              
                           
                        where t is the iteration number, w is the inertial weight, c
                        1 and c
                        2 are two learning factors, and r
                        1 and r
                        2 are two uniformly distributed random numbers in [0, 1]. It is noted that 
                           
                              x
                              
                                 p
                                 b
                                 e
                                 s
                                 
                                    t
                                    i
                                 
                              
                           
                         in Eq. (11) is picked from the solutions among the external archive 
                           
                              A
                              =
                              {
                              
                                 A
                                 1
                              
                              ,
                              
                                 A
                                 2
                              
                              ,
                              …
                              ,
                              
                                 A
                                 
                                    |
                                    A
                                    |
                                 
                              
                              }
                           
                        , which gives the best value of each aggregation problem corresponding with the weight vector λi
                         (Moubayed et al., 2014). The pseudo-code for the selection of each 
                           
                              x
                              
                                 p
                                 b
                                 e
                                 s
                                 
                                    t
                                    i
                                 
                              
                           
                         is described in Fig. 1.
                        
                     

On the other hand, as the solutions in external archive A are all non-dominated, they can be considered to be the global-best values for MOPs. Thus, xgbest
                         is randomly selected from the external archive A. Therefore, when using Eq. (11) to update the velocity, it will quickly guide the corresponding particle to approach the neighboring region around the optimal aggregated value, which enhances the ability of exploitation and resultantly accelerates the convergence speed. Otherwise, the velocity is renewed by Eq. (12); it will lead the targeted particle to search the intermediate region between xgbest
                         and itself. This is beneficial for the enhancement of exploration and simultaneously improves the swarm diversity. In MMOPSO, the advantages of the two search patterns are combined by using a pre-defined threshold δ, as follows:

                           
                              (13)
                              
                                 
                                    {
                                    
                                       
                                          
                                             
                                                
                                                   v
                                                   i
                                                
                                                
                                                   (
                                                   t
                                                   +
                                                   1
                                                   )
                                                
                                                =
                                                w
                                                
                                                   v
                                                   i
                                                
                                                
                                                   (
                                                   t
                                                   )
                                                
                                                +
                                                
                                                   c
                                                   1
                                                
                                                
                                                   r
                                                   1
                                                
                                                
                                                   (
                                                   
                                                      x
                                                      
                                                         p
                                                         b
                                                         e
                                                         s
                                                         
                                                            t
                                                            i
                                                         
                                                      
                                                   
                                                   −
                                                   
                                                      x
                                                      i
                                                   
                                                   
                                                      (
                                                      t
                                                      )
                                                   
                                                   )
                                                
                                                
                                                if
                                                
                                                
                                                
                                                   r
                                                   3
                                                
                                                <
                                                δ
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   v
                                                   i
                                                
                                                
                                                   (
                                                   t
                                                   +
                                                   1
                                                   )
                                                
                                                =
                                                w
                                                
                                                   v
                                                   i
                                                
                                                
                                                   (
                                                   t
                                                   )
                                                
                                                +
                                                
                                                   c
                                                   2
                                                
                                                
                                                   r
                                                   2
                                                
                                                
                                                   (
                                                   
                                                      x
                                                      
                                                         g
                                                         b
                                                         e
                                                         s
                                                         t
                                                      
                                                   
                                                   −
                                                   
                                                      x
                                                      i
                                                   
                                                   
                                                      (
                                                      t
                                                      )
                                                   
                                                   )
                                                
                                                
                                                else
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where r
                        3 is a uniformly distributed random number in [0, 1]. The appropriate setting of δ can keep the balance between exploitation and exploration. Indicated by the experimental studies, δ is generally set in [0.5, 0.9] to put more attention on the exploitation of the current search region.

After the PSO-based optimization, the visited non-dominated solutions with bigger crowding-distance values are preserved in external archive A, which are considered to be good representatives of the entire PF. To allow the beneficial information exchange among the archive, MMOPSO performs evolutionary search on each non-dominated solution in the archive. The embedded evolutionary search power can repair the potential vulnerability of PSO search. This is supported by the recent research studies in evolutionary algorithms that the hybridized search power can enhance the search capability and the robustness to tackle various kinds of MOPs (Chen, Lin, & Ji, 2010; Sindhya et al., 2013; Tang & Wang, 2013). In MMOPSO, the evolutionary operators, such as simulated binary crossover (SBX) and polynomial mutation (PM), are performed, as they are widely adopted in multi-objective optimization algorithms (Chen et al., 2010; Deb et al., 2002; Gong, Jiao, Du, & Bo, 2008; Lin & Chen, 2013). SBX operator allows the elitist solutions to exchange useful gene segments while PM operation injects a small turbulence to search the local region. To perform SBX and PM operators on external archive A, an elitist subset E is firstly selected from A, which contains a number of non-dominated solutions with bigger crowding-distance values in A. The size of E is generally smaller than A and set to be half of |A| in this paper. For each solution Ai
                         (
                           
                              i
                              =
                              1
                              ,
                              2
                              ,
                              …
                              ,
                              |
                              A
                              |
                           
                        ), a random integer j is generated in [1,|E|]. Then, Ai
                         and Ej
                         are used as parent solutions to execute SBX operator. One of the child solutions from SBX operator is randomly selected and then further to perform PM operator. The implementations of SBX and PM operators can be found in (Chen et al., 2010; Gong et al., 2008; Lin & Chen, 2013). The pseudo-code of this evolutionary search strategy is described in Fig. 2
                        , where SBX(Ai, Ej
                        ) means to perform SBX operator on parent solutions Ai
                         and Ej, C
                        1 and C
                        2 are the resultant child solutions generated from SBX operator, PM(Ck
                        ) indicates the execution of PM operator on Ck
                        . After that, a new solution set S is generated, which will be added into the external archive by archive update operation as introduced in the following subsection.

After the execution of PSO search or evolutionary search, the new generated non-dominated solutions are collected into the external archive. As the size of archive is finite, whereas the number of non-dominated solutions may be infinite, it is necessary to use a proper selection mechanism for archive update, which can help to guide the search direction toward the true PF. Here, the popular archive update mechanism used in (Nebro et al., 2009; Zhan et al., 2013) is also adopted, which is based on both of Pareto dominance and crowding distance. Assuming that the new generated solution set is S and the solution set in external archive is A, the pseudo-code of the archive update procedures can be briefly described in Fig. 3
                        , where N is the maximum size of A. In Fig. 3, the function CheckDominance(
                        x, y
                        ) returns the Pareto dominance relationship between solutions x and y. If the function returns 1, it means that x dominates y. Otherwise, the function returns -1 when y dominates or is equal with x. Another function CrowdingDistanceAssignment(A) will calculate the crowding distance value (Deb et al., 2002) for each solution in A.

The above subsections have described the procedures of velocity update, evolutionary search and archive update, which compose the main components of MMOPSO. Besides that, the other parts are presented in the pseudo-code of MMOPSO, as illustrated in Fig. 4
                        , where N is the size of population and external archive, ev represents the number of function evaluations, max_ev indicates the maximum number of function evaluations, r is a uniformly distributed random number in [0, 1] and δ is a predefined threshold to control the velocity update.

In the initialization phase, N weight vectors are firstly initialized and then a swarm with N particles is randomly generated, where each particle associates with a unique weight vector. The external archive A is initialized to be empty. After evaluating the objectives of each particle, the archive update procedures are performed to preserve the non-dominated solutions in archive A. Then, MMOPSO turns into the loop of evolutionary process until the function evaluation times ev reaches the predefined maximum times max_ev.

During the evolutionary phase, the PSO search is first executed. The velocity of each particle is updated by using Eqs. (11) or (12), which is determined by the threshold δ. Once the random number r is smaller than δ, Eq. (11) is used to update the velocity, where the selection for 
                           x
                        
                        
                           
                              pbest
                           
                         as introduced in Algorithm 1 is run to find the personal-best particle that can give the best aggregation value. Otherwise, the velocity is updated using Eq. (12), where xgbest
                         is randomly picked from external archive A. After the positional information for each particle is renewed, the objectives of new particles are evaluated. Then, the archive update procedure as described in Algorithm 3 is executed to gather the new non-dominated solutions with bigger crowding-distance values. After that, the evolutionary search process is run to allow the information exchange among the archive A, the detailed implementation of which is illustrated in Algorithm 2. Evolutionary operators, such as SBX and PM, are operated accordingly. Then, the objectives of the mutant solutions are computed and the archive update process in Algorithm 3 is activated again. The above evolutionary phase will repeat until the predefined maximum function evaluation times are achieved. At the end of algorithm, the non-dominated solutions in archive A are reported as the final approximated PF.

In this section, several experimental studies are performed to examine the performance of MMOPSO. Firstly, the related background about the simulations is introduced, including the standard benchmark problems, performance metric and the corresponding parameter settings. Secondly, the performance of MMOPSO is compared with some MOPSO algorithms and two state-of-the-art MOEAs, e.g., DDMOPSO (Moubayed et al., 2014), CMPSO (Zhan et al., 2013), SMPSO (Nebro et al., 2009), dMOPSO (Martinez & Coello Coello, 2011), OMOPSO (Sierra & Coello Coello, 2005), NSGA-II (Deb et al., 2002) and MOEA/D (Li & Zhang, 2009). Thirdly, in order to validate the advantages of multiple search strategies in MMOPSO, the performance of MMOPSO is further compared with the two variants of MMOPSO, i.e., MMOPSO-I and MMOPSO-II. MMOPSO-I replaces the velocity update equation in Eq. (13) with the traditional one in Eq. (9), while MMOPSO-II removes the evolutionary search strategy, making it a pure PSO algorithm. At last, the time complexity analysis of MMOPSO is provided.

In this study, twenty four standard benchmark problems without any inequality or equality constraints are used to evaluate the performance of MMOPSO. They can be classified into three categories. The first kind is low-dimensional bi-objective problems, such as Schaffer (1985), Fonseca and Flemming (1998), and Kursawe (1990). They are shortly written as SFK test problems. The second class is high-dimensional bi-objective problems, including ZDT1∼ZDT4 and ZDT6 (Zitzler et al., 2000). The third type is scalable objective problems, covering WFG1∼WFG9 (Huband et al., 2005) and DTLZ1∼DTLZ7 (Deb et al., 2005). In our experimental studies, the WFG and DTLZ family problems are respectively scaled to two and three objectives. It is noted that for ZDT1-ZDT3, the number of decision variables is 30, while the sizes of decision variables in ZDT4, ZDT6 and all DTLZ problems are 10. The number of decision variables in all WFG problems is 12, which is consisted by 4 position parameters and 8 distance parameters. These test problems are characterized with convexity, concavity, discontinuity, non-uniformity and the trap of many local PFs. Therefore, they are widely applied in the experimental studies to test the comprehensive performance of multi-objective optimization algorithms (Moubayed et al., 2014; Gong et al., 2008; Lin & Chen, 2013; Zhan et al., 2013).

One important job of MOPs is to find a uniformly distributed subset that approximates the true PF as close as possible, which can be provided to the decision maker as the alternative solutions for various practical cases. Since the inverted generational distance (IGD) metric (Li & Zhang, 2009) can examine both of the convergence and diversity, it is adopted in our experimental studies to assess the optimization performance.

Let S be a uniformly distributed subset selected from the true PF and S' is the approximated set that is obtained by a multi-objective optimization algorithm. The IGD value of S to S', i.e., IGD(S, S') is defined as

                           
                              (14)
                              
                                 
                                    I
                                    G
                                    D
                                    
                                       (
                                       S
                                       ,
                                       
                                          S
                                          ′
                                       
                                       )
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             
                                                |
                                                S
                                                |
                                             
                                          
                                          
                                             d
                                             (
                                             
                                                S
                                                i
                                             
                                             ,
                                             
                                                S
                                                ′
                                             
                                             )
                                          
                                       
                                       
                                          |
                                          S
                                          |
                                       
                                    
                                 
                              
                           
                        where |S| returns the number of solutions in set S and d(Si, S') computes the minimum Euclidean distance from Si
                         to the solutions of S' in objective space. When acquiring this IGD value, the true PF has to be available in advance. Generally, a lower value of IGD(S, S') is preferred as it indicates that S' is distributed more uniformly and closer to the true PF.

In this study, in order to validate the optimization performance of MMOPSO in a convincible way, MMOPSO is compared with some MOPSO algorithms, including DDMOPSO, CMPSO, SMPSO, dMOPSO and OMOPSO. Moreover, MMOPSO is also compared with two state-of-the-art MOEAs, i.e., MOEA/D and NSGA-II. It is noted that the source codes of SMPSO, dMOPSO, OMOPSO, MOEA/D and NSGA-II can be found in jMetal (Durillo & Nebro, 2011) and the source code of DDMOPSO is provided by the authors that is also implemented in jMetal. Besides that, CMPSO and MMOPSO are realized by us in the framework of jMetal. All the above algorithms have shown the promising performance when tackling various kinds of MOPs. Therefore, the comparisons of MMOPSO with them can make the results more convincible.

The parameter settings of all the algorithms are summarized in Table 1. For the compared algorithms, these parameter settings are all recommended by their authors. As most of parameters in MMOPSO also exist in the compared algorithms, they are set the same with the compared algorithms for fair comparison. For MMOPSO, DDMOPSO, dMOPSO and OMOPSO, the control parameters c
                        1, c
                        2 are randomly generated from [1.5, 2.0] and the inertial weight ω is selected from [0.1, 0.5] randomly. In SMPSO, the control parameters c
                        1, c
                        2 are randomly chosen from [1.5, 2.5] and the inertial weight ω is also randomly selected from [0.1, 0.5]. For CMPSO, the control parameters c
                        1, c
                        2, and c
                        3 are all set to 2.0 and the inertial weight ω is linearly decreasing from 0.9 to 0.4. N is the sizes of swarm and external archive for all the algorithms except CMPSO. As multiple populations are respectively evolved to optimize multiple objectives in CMPSO, a small population size is recommended by the authors. Thus, the swarm size Np
                         in CMPSO is set to 20 while the external archive Na
                         is also set to 200. pc
                         and pm
                         are respectively the crossover and mutation probabilities used in evolutionary operators. ηc
                         and ηm
                         are the distribution indexes of SBX and PM respectively. For MOEA/D, T defines the size of the neighborhood in the weight coefficients, δ controls the probability that parent solutions are chosen from T neighbors and nr
                         is the maximal number of parent solutions that are replaced by each child solution.

It is noted that the setting of N listed in Table 1 is only for the bi-objective problems. For the triple-objective test problems, the sizes of population and external archive are all set to 595 except for CMPSO. The swarm size Np
                         and the external archive Na
                         in CMPSO are respectively set to 60 and 595 for triple-objective test problems. The expected maximal generation is 300. Therefore, the maximal numbers of function evaluations (FEs) are 60,000 and 178,500 for bi-objective and triple-objective problems, respectively. All the algorithms are run by 30 times in jMetal using a personal computer with a 3.20 Giga Hertz CPU, 2 Giga Byte memory and windows 7 operating system. Their mean values and standard deviations (std) for each test problem are collected for comparison, where the best results are identified with bold font in comparison tables. Moreover, the t-test with significant level 
                           
                              α
                              =
                              0.05
                           
                         is also performed to examine that whether the IGD mean values obtained by MMOPSO are statistically different from that obtained by the other algorithms. The p-value returned by the t-test is also collected in the comparison table, where the p-value bigger than 0.05 means that the compared IGD mean values are statistically similar. It is noted that the underlined IGD results for the compared algorithms indicate that they are statistically similar with that obtained by MMOPSO under the t-test.


                           Table 2 summarizes the results of all the algorithms on the FKS and ZDT test problems. Our MMOPSO algorithm obtains the best results on Kursawe, ZDT3 and ZDT4, while SMPSO performs best on ZDT1 and ZDT2. Moreover, MOEA/D gets the best results on Fonseca and ZDT6, and OMOPSO performs best on Schaffer. Since the corresponding test problems are not so difficult, it is observed that the compared algorithms perform well on most of test problems. However, it is important to point out that some of the compared algorithms are lack of capabilities in handling test problems with specific characteristics. For example, NSGA-II, MOEA/D and dMOPSO can't effectively approach the true PF of Schaffer; DDMOPSO gives the worst result on ZDT2; OMOPSO and DDMOPSO are unable to find the true PF of ZDT4 due to the existence of many local PFs.

The t-test results indicate that MMOPSO performs similarly with OMOPSO on Schaffer, ZDT1 and ZDT6, with SMPSO on ZDT3, and with DDMOPSO on Fonseca and Schaffer. Moreover, the final comparison results of MMOPSO with the compared algorithms are clearly concluded in the last row of Table 2, where Better/Similar/Worse indicates that the number of test problems that the results obtained by MMOPSO are better than, similar with or worse than that of the compared algorithms. It is quite obvious that MMOPSO performs better than or similarly with NSGA-II, CMPSO and DDMOPSO on all the test problems. For MOEA/D and dMOPSO, MMOPSO obtains the better results on 6 out of 8 test problems. Besides that, MMOPSO performs better than OMOPSO, and worse than SMPSO. Actually, both of MMOPSO and SMPSO are able to solve the corresponding test problems well. To visually show the optimization performance, the best results of MMOPSO on these test problems are plotted in Fig. 5
                           , where the true PFs are identified with the red lines and the approximated PFs are marked with black diamonds. It is evident that the found approximated PF is distributed uniformly on the true PF.


                           Table 3
                            presents the simulation results obtained by all the algorithms on the WFG test problems. Our proposed MMOPSO algorithm achieves the best performance on WFG1, WFG3, WFG4, WFG7 and WFG9, while OMOPSO performs best on WFG2 and WFG6. MOEA/D and CMPSO get the best results on WFG8 and WFG5, respectively. It is noted that most of the compared algorithms fail to approach the true PF of WFG1, whereas MMOPSO performs better. Actually, observed from Table 3, MMOPSO is able to deal with most of WFG problems quite well.

The t-test results show that MMOPSO gets the statistically similar results with MOEA/D on WFG2, with OMOPSO on WFG5, with SMPSO on WFG5 and WFG6, with CMPSO on WFG8, and with DDMOPSO on WFG2. The comparison summary in the last row of Table 3 illustrates that MMOPSO performs better than the compared targets on most of WFG test problems. Fig. 6
                            plots the best results obtained by MMOPSO on the WFG test problems, which can visually show the promising performance of MMOPSO. Except for WFG8, MMOPSO is able to effectively approach the true PFs.


                           Table 4
                            gives all the experimental results on the DTLZ test problems. Our proposed MMOPSO algorithm performs best on DTLZ1, DTLZ5 and DTLZ6, while dMOPSO obtains the best results on DTLZ2 and DTLZ4. MOEA/D and CMPSO get the best performance on DTLZ3 and DTLZ7, respectively. The simulations show that some MOPSO algorithms, such as OMOPSO, CMPSO and DDMOPSO, can't effectively approach the true PF of DTLZ3 as it is easy to get trapped in local PFs, while MMOPSO and SMPSO can handle it quite well.

The t-test results show that MMOPSO, NSGA-II, OMOPSO, SMPSO and CMPSO obtain the similar results on DTLZ4. Moreover, MMOPSO performs similarly with SMPSO and OMOPSO on DTLZ6. The comparison conclusion on the last row of Table 4 indicates that MMOPSO performs better than NSGA-II, OMOPSO, SMPSO and DDMOPSO. When compared with MOEA/D, dMOPSO and CMPSO, our proposed algorithm MMOPSO still have some advantages on these DTLZ problems. Fig. 7
                            gives the plots of best results obtained by MMOPSO on the DTLZ problems, where the true PFs are marked with red surface and the approximated PFs are identified with black diamonds. These plots further confirm that MMOPSO can find the approximated PF that is distributed uniformly and very close to the true PF.

At last, all the comparison summaries on the last rows of Tables 2–4 are collected in Table 5
                           , which gives the comprehensive performance of MMOPSO when compared with the other algorithms on all the test problems. It is obvious that MMOPSO performs better than or similarly with NSGA-II, OMOPSO, dMOPSO, CMPSO and DDMOPSO on at least 20 out of 24 test problems. When compared with MOEA/D and SMPSO, MMOPSO also obtains the better or similar results on 17 and 18 out of 24 test problems, respectively. These experimental results justify the advantages of MMOPSO when handling various kinds of test problems.

In order to investigate the advantages of multiple search strategies in MMOPSO, two variants of MMOPSO are included for comparison, i.e., MMOPSO-I and MMOPSO-II. They have the similar components with MMOPSO, except that MMOPSO-I replaces the velocity update equation in Eq. (13) with the traditional one in Eq. (9) and MMOPSO-II removes the evolutionary search component on the archive. In Table 6
                        , the comparison results of MMOPSO, MMOPSO-I and MMOPSO-II on all the test problems are listed.

Observed from Table 6, MMOPSO performs best on 18 out of 24 test problems, which validates the advantages of MMOPSO when compared with MMOPSO-I and MMOPSO-II. The t-test results also reveal that MMOPSO obtains better results than MMOPSO-I and MMOPSO-II on 12 and 10 test problems, respectively. Moreover, it has the similar performance with MMOPSO-I on 11 test problems and with MMOPSO-II on 13 test problems. In other words, MMOPSO performs better than or similarly with MMOPSO-I and MMOPSO-II on 23 out of 24 test problems. This justifies the effectiveness of multiple search strategies in MMOPSO.

To investigate the effect of evolutionary search on the external archive, it can be observed from the performance of MMOPSO-II that it is unable to approach the true PFs of some test problems, such as ZDT4, WFG1, DTZL1 and DTZL3. These test problems are mostly characterized with the trap of many local PFs. Thus, it indicates that the PSO search pattern in MMOPSO is easy to get trapped in local PFs, while the embedding of evolutionary search power on archive can remedy this shortcoming, which provides the capability to jump out of the local PFs. On the other hand, although MMOPSO-I performs much better than MMOPSO-II on ZDT4, WFG1, DTZL1 and DTLZ3, it still cannot gain the satisfactory results as obtained by MMOPSO. In most cases, MMOPSO-I performs worse than MMOPSO. This justifies that the two search strategies for velocity update are beneficial for enhancing the PSO search capability to handle different MOPs.

In this subsection, the time complexity analysis of MMOPSO is provided and compared with the other algorithms. Based on the pseudo-code of MMOPSO in Fig. 4, the time complexity of MMOPSO is determined by the evolutionary loop in lines 12–30. It is noted that when computing the following time complexity, the impact of decision variables and objectives are generally ignored as they are much smaller than the sizes of population and external archive N. In line 12, the time complexity of Algorithm 1 in Fig. 1 is O(N
                        2); in lines 13–23, two search strategies are executed to update the velocity of each particle and the corresponding time complexity is O(N). Algorithm 3 and Algorithm 2 are serially operated in lines 25–26, and their time complexity are respectively O(N
                        2) and O(N) as observed in Figs. 2 and 3. In lines 27–28, the time complexity is obviously O(N) as it only evaluates the objectives of N new solutions and then update the reference point z*. At last, Algorithm 3 is activated again with the time complexity O(N
                        2). Thus, based on the above analysis, the time complexity of MMOPSO is O(3N
                        2+3N)∼ O(N
                        2). As discussed in Moubayed et al. (2014), the computational complexities of MOEA/D, dMOPSO, OMOPSO and DDMOPSO are all O(N
                        2) when the size of external archive is set the same with the population size N. Besides that, SMPSO shares the similar structure with OMOPSO and thus its computational complexity is also O(N
                        2), while the time complexity of NSGA-II is O(N
                        2) (Deb et al., 2002). For CMPSO, its time complexity is dependent on the size of sub-swarm. By simply assuming the sub-swarm size is also N, the time complexity of CMPSO is also O(N
                        2). Therefore, based on the theoretical analysis, MMOPSO has the similar time complexity with the compared algorithms.

To further study the extra computational burden induced by the designed multiple search strategies, Table 7
                         gives the average computational time of MMOPSO-I, MMOPSO-II and MMOPSO in solving all the WFG test problems. Besides that, two decomposition-based MOPSOs, i.e., dMOPSO and DDMOPSO, are also included for comparison. It is noted that the average time for each WFG problem is obtained by 30 independent runs and the lowest one is highlighted with boldface. As observed from Table 7, MMOPSO needs 2.11 seconds in average to solve one WFG test problem, which performs slower than MMOPSO-I and faster than MMOPSO-II. This indicates that the proposed multiple search strategies won't bring much computational burden. Even compared with MMOPSO-I, it is still worthy of spending 17 percent extra average time in order to obtain the superior performance as provided by MMOPSO in Table 6. Moreover, MMOPSO performs slightly faster than dMOPSO and much faster than DDMOPSO, which justify the computational efficiency of MMOPSO when compared with other decomposition-based MOPSOs. However, the execution time of dMOPSO is more stable than that of MMOPSO and DDMOPSO. This is because dMOPSO only adopts the aggregated values to update the globally best particles, while MMOPSO and DDMOPSO utilize the crowding-distance metric to store the non-dominated solutions. Thus, the execution times of MMOPSO and DDMOPSO are greatly affected by the number of non-dominated solutions found during the search phase. Especially for DDMOPSO, it is worth noting that its execution time is much longer than that of dMOPSO and MMOPSO. This is mainly due to the fact that the new archiving approach in DDMOPSO needs to compute the crowding-distance values in both objective and decision spaces, and its source code has not been fully optimized by the authors.

@&#CONCLUSIONS@&#

In this paper, a novel MOPSO algorithm with multiple search strategies is presented, which is based on decomposition approach to transform MOPs into a set of aggregation problems. Each particle in the swarm is accordingly assigned to optimize each aggregation problem. A novel velocity update approach is designed to renew the particle velocity by using two search strategies, which can concurrently promote the convergence speed and remain the population diversity. Additionally, evolutionary search strategy is further performed on the non-dominated solutions in the external archive, which is able to exchange their beneficial information. This embedded evolutionary search power can repair the weakness of PSO search pattern and resultantly enhance the comprehensive performance of MMOPSO in solving various kinds of MOPs. The effectiveness and efficiency of multiple search strategies are also justified by the experimental studies. When compared with some MOPSO algorithms and two state-of-the-art MOEAs, such as DDMOPSO, CMPSO, SMPSO, dMOPSO, OMOPSO, MOEA/D and NSGA-II, the experimental results illustrate that MMOPSO performs best on most of test problems.

Our future study will further enhance the performance of MMOPSO, and extend it for tackling MOPs with more than three objectives. Furthermore, the applications of MMOPSO for some practical engineering problems will also be investigated in our future work.

@&#ACKNOWLEDGMENTS@&#

This work was supported by the National Nature Science Foundation of China under Grant nos.
61402291 and 61170283, National High-Technology Research and Development Program (“863” Program) of China under Grant 2013AA01A212, Ministry of Education in the New Century Excellent Talents Support Program under Grant NCET-12-0649, Foundation for Distinguished Young Talents in Higher Education of Guangdong under Grant 2014KQNCX129, Shenzhen Technology Plan under Grant JCYJ20140418095735608, and Natural Science Foundation of SZU under Grant 201531. The authors are grateful to both the editor and anonymous reviewers for their constructive comments, which greatly improve the quality of this paper.

@&#REFERENCES@&#

