@&#MAIN-TITLE@&#A unified framework for translation and understanding allowing discriminative joint decoding for multilingual speech semantic interpretation

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           A comparison between the methods used for speech translation and understanding.


                        
                        
                           
                           A unified framework for translation and understanding.


                        
                        
                           
                           A discriminative joint decoding for multilingual speech semantic interpretation.


                        
                        
                           
                           The proposition is competitive with state-of-the-art techniques.


                        
                        
                           
                           The framework can be generalized to other components of a dialogue system.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Multilingual speech understanding

Conditional random fields

Hypothesis graphs

Statistical machine translation

Dialogue systems

@&#ABSTRACT@&#


               
               
                  Probabilistic approaches are now widespread in most natural language processing applications and selection of a particular approach usually depends on the task at hand. Targeting speech semantic interpretation in a multilingual context, this paper presents a comparison between the state-of-the-art methods used for machine translation and speech understanding. This comparison justifies our proposition of a unified framework for both tasks based on a discriminative approach. We demonstrate that this framework can be used to perform a joint translation-understanding decoding which allows to combine, in the same process, translation and semantic tagging scores of a sentence. A cascade of finite-state transducers is used to compose the translation and understanding hypothesis graphs (1-bests, word graphs or confusion networks). Not only this proposition is competitive with the state-of-the-art techniques but also its framework is even more attractive as it can be generalized to other components of human–machine vocal interfaces (e.g. speech recognizer) so as to allow a richer transmission of information between them.
               
            

@&#INTRODUCTION@&#

Nowadays, probabilistic approaches are widely used in natural language processing (NLP) applications (speech recognition, machine translation, syntactic parsing, part-of-speech or semantic tagging). Many different approaches are available in the literature and their relative performances differ according to the targeted tasks. So, considering a specific task, it not always possible to know the best performing approach before evaluating most of the models. Though some important features of the various approaches may prevent us from testing all the combinations with respect to some of the task characteristics.

For instance, for spoken language understanding (SLU), the task of extracting the meaning from a user's utterance, conditional random fields (CRF) (Lafferty et al., 2001) have been shown to be the most efficient approach so far (Hahn et al., 2010); whereas for statistical machine translation (SMT), the log-linear phrase-based statistical machine translation (LLPB-SMT) model (Koehn et al., 2003) is the most commonly used and has shown its potential for many language pairs and domains of application.

However despite the differences in their formal descriptions, distinctions between probabilistic approaches tend to fade away when confronted with practical considerations and the numerous assumptions required for their implementations. Some works already proposed the use of discriminative approaches, such as CRF, for SMT (Och and Ney, 2002; Liang et al., 2006; Lavergne et al., 2011) while at the same time the phrase-based translation pipeline was also investigated in the context of other natural language processing tasks such as grapheme–phoneme conversion (Rama et al., 2009) or part-of-speech tagging (Gascó i Mora and Sánchez Peiró, 2007).

So far, many works have considered the issue of multilingual systems for different NLP tasks such as cross-lingual information retrieval (e.g. Capstick et al., 2000; Jagarlamudi and Kumaran, 2008), cross-lingual information distillation (Singla and Hakkani-Tur, 2008), multilingual speech recognition adaptation (Schultz, 2004) and cross-lingual spoken language understanding (e.g. Minker, 1998; Lefèvre et al., 2010).

In this paper, our overall objective is to develop efficient approaches for speech understanding in a multilingual context (where SMT is also involved, as explained later). In this outlook, the state-of-the-art approaches are investigated for each of the underlying issues: CRF for speech understanding and LLPB-SMT for translation. In a first step, we propose to use and optimize the LLPB-SMT approach for speech understanding, and also to integrate a CRF-based model in a machine translation module to evaluate the practical interest of each method. This preliminary study serves to highlight the specificities of each task and to evaluate the performance of the respective approaches on these tasks.

Besides, we have shown in a previous work that the use of machine translation is an effective solution for the portability of an understanding system from a language to another (Jabaian et al., 2010). For one of the best performing configurations, the portability is simply obtained by cascading a translation module with an understanding system, the overall idea being to translate the user's inputs into a language for which we already have a performing SLU system. This idea is to be contrasted with this consisting in trying to build a performing system in the new language, for which we generally lack enough usable data.

But, in this context, it has been stated that the best understanding output is not always generated from the best translation hypothesis. From our experience, it is often due to bad word reordering. Therefore, the selection of the best translation is not sufficient to optimize the overall system in a multilingual understanding scenario. Consequently, based on the initial comparison made between both tasks, we propose a model that can jointly decode the user's inputs in terms of translation and understanding hypotheses. This joint decoding selects a translation taking into account the semantic tagging generated for this translation. It is no longer searched for the best possible translation, but for the translation that can be semantically labelled in the best possible way.

The reported experiments are carried out on the French Media man–machine dialogue corpus (Bonneau-Maynard et al., 2006). Manually transcribed and conceptually annotated data allow to train an initial CRF-based understanding system for French. In order to use this system for Italian semantic tagging, an Italian to French translation system, based on either LLPB-SMT or CRF, is trained with few manual data. This system is used during decoding to infer translations (from Italian into French) in order to provide inputs to the French understanding system. We show how these models are merged in a single decoding loop by means of hypothesis graphs and combined scores. To define the performance upper-bounds of the proposed methods, all experiments are performed using a manual transcription of the speech data.

Preliminary results have already been presented in Jabaian et al. (2013b). This current version is not only a refinement of these experiments but it also provides a deeper comparison of the techniques at hand with more result analysis. For instance, we investigate more systematically the effect of the tuning mechanism used to optimize the performance of the joint models and show that all the results are noticeably improved. This optimization, based on the minimum error rate training (MERT) algorithm (Och, 2003), is first applied to each model (translation and understanding) separately, then it is generalized to perform an optimized joint decoding using both systems.

The paper is organized as follows: Section 2 presents the use of the log-linear phrase-based stochastic machine translation approach for speech understanding. Then Section 3 describes the use of Conditional Random Fields for machine translation and the parameter tuning of this model. Our proposal of a joint decoding process between translation and understanding is introduced in Section 4. Section 5 gives an overview of the experimental study along with the results.

The log-linear phrase-based statistical machine translation (LLPB-SMT) (Koehn et al., 2003) is amongst the most popular approaches for SMT. This approach relies upon a log-linear combination of several models, including reordering models, translation models and n-gram language models.

The translation model P(s
                     ∣
                     t) represents the probability that a source sentence s is the translation of a sentence t from all possible sentences in the target language. The probabilistic language model p(t) assigns a probability to a word sequence t by means of a probability distribution. The decoding with the LLPB-SMT model is then expressed as 
                        
                           
                              argmax
                              t
                           
                        
                        P
                        (
                        s
                        ∣
                        t
                        )
                        .
                        P
                        (
                        t
                        )
                     . The reordering model introduces the possibility to modify the time sequence of the source sentence to match monotonically the target sentence. For sake of simplicity we will not detail here the implication it has during the decoding step.

The model coefficients in the LLPB-SMT system are trained using the maximum class posterior criterion or using an error criterion by application of the MERT algorithm (Och, 2003) with respect to the measured translation quality.

Some works already investigated to which extent statistical machine translation approaches can be ported to a task of speech understanding. For example (Macherey et al., 2001) proposed to apply an SMT system to SLU and showed that using a max-entropy SMT approach for understanding gives good performance compared to other classical understanding methods (Macherey et al., 2009). In the same line we propose to consider the understanding of a user utterance as a translation from a sequence of words (source language) to a sequence of concepts (target language).

This approach assumes that the sequences of concepts are the translations of the original sequences of words and then the best sequence of concept c derived from a sequence of word s can be defined as:


                        
                           
                              (1)
                              
                                 
                                    
                                       c
                                       ˆ
                                    
                                 
                                 =
                                 
                                    
                                       argmax
                                       c
                                    
                                 
                                 P
                                 (
                                 s
                                 ∣
                                 c
                                 )
                                 .
                                 P
                                 (
                                 c
                                 )
                              
                           
                        where P(s
                        ∣
                        c) is a word-to-concept translation model and P(c) is a concept n-gram language model.

It is important to mention that this model is somewhat a derivation of the approach devised in Pieraccini et al. (1991) where Hidden Markov Models are used for word-to-concept translation modelling.

Despite the apparent similarities between the tasks, SLU has its own characteristics that must be taken into consideration in order to improve the performance which can be obtained with LLPB-SMT.

The differences between a classical translation task (from a natural source language to a target one) and the use of translation for understanding (translation from a natural language into semantic tags) can be summarized as follows:
                           
                              •
                              unbalanced token association. Contrary to regular translation where word-to-word alignment is on average linking a source word to a target word, concepts, seen as target words, are most of the time linked to several source words. Even considering the phrase-based approach, linking sub-sentential segments, the phrase table (translation model in this case) is still highly unbalanced in the understanding case and this questions several aspects of the system training phase;

no null fertility. In a translation task, a source word can be skipped (null fertility), whereas for understanding every word must be aligned to a concept. Even though words that do not contribute to the meaning of the sentence, considering the task domain, are labelled with a specific NULL concept;

no reorderings. Semantics of a sentence generally follows the order in which words occur unlike in a translation task where translated words may have a very different order between target and source sentences depending on the considered language pair and their syntactic proximity;

no long-range target language model. The number of concept tokens is always very low compared to the usual target vocabulary size and the concept sequences are shorter. For these two reasons understanding can use language models with short histories;

no common error measurement. Evaluation measures are different in both tasks (BLEU, Papineni et al., 2002) for translation vs. CER – concept error rate – for understanding). Therefore the tools used for optimizing translation systems should be adapted to optimize CER score instead of BLEU.

A major difficulty of the translation task is that it requires automatic alignment of a word from the source language to its corresponding word in the target language. Since corpora used for training translation systems are usually aligned at the sentence level, an automatic alignment step is necessary to obtain word alignment. However, most of understanding corpora are labelled and aligned at the segment level and therefore the use of alignment information can be beneficial to help the alignment process. In this respect, the use of the BIO (begin inside outside) tagging (Ramshaw and Marcus, 1995) ensures that each word in the source sentence is aligned to its corresponding concept and therefore no additional automatic alignment is required. In that way, the extraction of the phrase table is obtained from a corpus with a perfect alignment (no alignment errors). This somehow addresses the two first points.

Following the assumption of the third point that the semantics of a sentence follows the order in which words occur, we propose to introduce a constraint of monotony, which conducts the decoder to follow strictly the order of words to generate concepts.

Finally, according to the last point, since we want to evaluate the hypotheses generated by this approach from an understanding perspective (evaluating the CER and not the BLEU score) we propose to modify the MERT algorithm (Och, 2003) used to optimize the system's parameters with respect to a development set so as to maximize the CER directly.

Last but not least it is important to note that there is a major difference between the two tasks in the way they are dealing with out of vocabulary (OOV) words. Generally in SMT, the OOV words encountered during the decoding are copied (projected untranslated) to the output. For a translation task this projection is a good strategy when OOV words are named entities (such as people names, places) which are likely to remain unchanged in several languages.

In understanding, the output vocabulary is limited to predefined concepts and thus the projection of OOV named entities in the output is not a suitable strategy. On the other hand, it is possible to anticipate the problem by enriching the training corpus with named entity lists from the target domain. For example, in the context of a hotel reservation task, a large number of OOV words comes from city names missing in the training data. Adding a list of cities to the training data may decrease the impact of the OOV words on the result.

Several efficient methods have been proposed for concept tagging (He and Young, 2006; Lefèvre, 2006; Raymond and Riccardi, 2007; Wang et al., 2009; Hahn et al., 2010). Due to their good performance, Raymond and Riccardi (2007) proposed to use conditional random fields (CRFs) (Lafferty et al., 2001) for language understanding and they are now largely used as a state-of-the-art model for this task. Linear-chain CRFs represent a log-linear model, normalized at the sentence level. In this formulation concept sequences are represented in the BIO formalism, see below, so that each word has a concept associated to it with a segmental information.

For a sequence of concepts 
                        
                           
                              C
                              ˆ
                           
                        
                        =
                        
                           c
                           1
                        
                        ,
                        …
                        ,
                        
                           c
                           N
                        
                      that can be hypothesized by the SLU module from a word sequence of length N 
                     
                        W
                        =
                        
                           w
                           1
                        
                        …
                        ,
                        
                           w
                           N
                        
                     , linear-chain CRFs model the probability between concepts and words as follows:


                     
                        
                           (2)
                           
                              P
                              (
                              
                                 c
                                 1
                                 N
                              
                              |
                              
                                 w
                                 1
                                 N
                              
                              )
                              =
                              
                                 1
                                 
                                    Z
                                    (
                                    
                                       w
                                       1
                                       N
                                    
                                    )
                                 
                              
                              
                                 ∏
                                 
                                    m
                                    =
                                    1
                                 
                                 N
                              
                              H
                              (
                              
                                 c
                                 
                                    m
                                    −
                                    1
                                 
                              
                              ,
                              
                                 c
                                 m
                              
                              ,
                              ϕ
                              (
                              
                                 w
                                 1
                                 N
                              
                              ,
                              n
                              )
                              )
                           
                        
                     with


                     
                        
                           (3)
                           
                              H
                              (
                              
                                 c
                                 
                                    n
                                    −
                                    1
                                 
                              
                              ,
                              
                                 c
                                 n
                              
                              ,
                              ϕ
                              (
                              
                                 w
                                 1
                                 N
                              
                              ,
                              n
                              )
                              )
                              =
                              
                                 ∑
                                 i
                              
                              
                                 λ
                                 i
                              
                              
                                 h
                                 i
                              
                              (
                              
                                 c
                                 
                                    n
                                    −
                                    1
                                 
                              
                              ,
                              
                                 c
                                 n
                              
                              ,
                              ϕ
                              (
                              
                                 w
                                 1
                                 N
                              
                              ,
                              n
                              )
                              )
                           
                        
                     
                  

Log-linear models are based on feature functions h
                     
                        i
                      representing the information extracted from the training corpus, λ are estimated during the training process and Z is a normalization term. 
                        ϕ
                        (
                        
                           w
                           1
                           N
                        
                        ,
                        n
                        )
                      represents a pattern function that will be used to define the feature functions during training.

There has been a lot of research to improve the machine translation quality using alternative methods for translation, such as hierarchical translation (Chiang, 2007), hybrid approaches (phase-based and hierarchical (Dymetman and Cancedda, 2010) or n-gram-based translation models (Mari`no et al., 2006).

Recently, efforts have been made in order to benefit from a good performance of the discriminative methods in machine translation. Lavergne et al. (2011) proposes a translation system based on CRF scoring while Gao and He (2013) proposes to score the translation phrase-table using a Markov Random Field model.

We now envisage the use of an understanding system to address the translation task. Of course the differences listed in the previous section remain true although their influence is now in the reverse direction. The translation is viewed as a tagging of the source sequence, the possible tags being the words of the target language themselves. Training a tagger based on a CRF approach requires a corpus annotated (translated corpus) at word level. The application of the IBM models (Brown et al., 1993) provides automatic word alignments from a bilingual corpus originally aligned at the sentence level.

As for the understanding task, where many words may be associated with a single concept, several source words can be aligned with only one target word. To deal with that issue, tags are handled as it is done for the understanding task using the BIO formalism. For example, the French sequence “Je voudrais” aligned to the Italian word “vorrei” can be represented as: “je, B_vorrei” “voudrais, I_vorrei”.

The main difficulty for training CRF models for translation is related to the high number of tags (corresponding to the target language vocabulary size). Though some solutions exist. For instance, Riedmiller and Braun (1993) proposes to use the RPROP algorithm for feature optimization to deal with a large number of features. This algorithm reduces the memory requirements compared to other optimization algorithms (Turian et al., 2006). Another limitation to the use of CRF for translation is that it does not take into account word reordering and that the target language model is limited by the computational complexity of the decoding.

So it appears that the MT task is more demanding in terms of modelling constraints than understanding (reduce to mere sequence labelling in this case). It could be profitable to benefit from the discriminative power of the CRF approach but specificities of the MT task must be handled properly.


                        Raymond et al. (2006) proposes a composition of Weighted Finite State Transducers (WFSTs) for semantic interpretation. Furthermore, in order to obtain an efficient CRF-based translation system, Lavergne et al. (2011) proposes a model based on finite state transducers in which the different stages of the translation process (segmentation, reordering, phrase translation etc.) are treated separately and can be composed all together. This model, called CRFPB-SMT hereafter, embeds a mechanism for modelling a translation table by sub-sentential segments (called tuples, but analogous to phrases) and uses CRF as the probabilistic models providing the hypothesis scores.

The CRFPB-SMT decoder is based on a composition of WFSTs representing the following steps: segmentation and reordering of the source sentence according to the word tuples, application of the translation, hypothesis scoring based on CRF, and composition with a target language model. Kumar and Byrne (2003) proposed a comparable architecture that used the less efficient Alignment Template Translation Models instead of CRF as the translation model.

This architecture allows to consider the translation of a sentence as a composition of transducers in the following order:


                        
                           
                              (4)
                              
                                 
                                    λ
                                    translation
                                 
                                 =
                                 
                                    λ
                                    S
                                 
                                 ∘
                                 
                                    λ
                                    R
                                 
                                 ∘
                                 
                                    λ
                                    T
                                 
                                 ∘
                                 
                                    λ
                                    F
                                 
                                 ∘
                                 
                                    λ
                                    L
                                 
                              
                           
                        where λ
                        
                           S
                         is the acceptor of the source sentence s; λ
                        
                           R
                         implements the segmentation and reordering steps; λ
                        
                           T
                         is a dictionary of tuples, combining sequences of the source language and their possible translations based on the tuples inventory; λ
                        
                           F
                         is a feature matcher, which assigns probability scores to tuples using a CRF model; λ
                        
                           L
                         is a language model of the target language.

The reordering model λ
                        
                           R
                         is trained using the approach proposed by Crego and Mari no (2006). This approach is based on a part-of-speech tagging of the sentences to reorder the source side of the training corpus.

Tuples are extracted from parallel sentences in two steps. First, an iterative procedure allows to combine words. Each group contains words from the source sentence aligned with one or many words from the target sentence. In a second step, tuples are extracted from these groups respecting the word order in the target sentence and following the constraint that no word in a tuple is aligned to a word outside the tuple. The tuple extraction process ends when no smallest tuples can be found without violating the previous constraints.

The feature matcher λ
                        
                           F
                         relies on a CRF model to assign probability scores to tuples. This transducer is obtained as a composition of a set of weighted transducers, each one representing a class of functions (in practice the unigram and the bigram feature functions of the CRF model).

Several algorithms have been proposed to tune the weights of the log-linear translation model aiming to obtain an optimal translation. Minimum error rate training (MERT) algorithm proposed by Och (2003) is the most used in state-of-the-art LLPB-SMT systems.

In this line, we propose to use MERT in order to optimize the weights of the different scored components in the CRFPB-SMT model. In such optimization various parameters used during the decoding are tuned on a development corpus, then the translation is performed using the parameter values found during tuning.

The study of the relations between the different approaches in the previous sections was mainly aimed at being able to combine them efficiently for multilingual understanding.

In a previous work (Jabaian et al., 2010), we have shown that among several possibilities the best way to port an understanding system to a new language is also the simplest: to translate user utterances in the new language back to the language of the existing understanding system and then to use this system to label the translation hypothesis.

This strategy supposed that a SLU system is available for the source language, and SMT is used to translate the target test sentences back to the source language. The translations become the inputs of the source SLU system. So, this proposition is based on a pipeline of a translation system (LLPB-SMT) and an understanding system (CRF). This strategy might be highly dependent on the performance of the SMT used to translate the data. The overall performance depends also on the robustness of the source SLU tagger to translation errors.

The SMT system can be obtained following standard routines (based on large collections of parallel texts), or by developing a domain-adapted SMT system right from the start. The best hypothesis generated by the translation system is conveyed as input to the understanding system. However, other translation hypotheses could have tagging results with higher scores. So, the selection of the best translation does not necessarily optimize the behaviour of the overall system.

A joint decoding between translation and understanding can be an efficient solution to this problem. The joint decoding has the advantage of optimizing the selection of the translation taking into account the tags that can be assigned to the best possible translations.

Moreover the CRFPB-SMT approach can be generalized to the understanding task (comparable to what was done in Section 2 with LLPB-SMT). Therefore an understanding system λ
                        
                           understanding
                         can be obtained in the same way as proposed in Section 3. This representation allows to obtain a graph of understanding hypotheses similar to that obtained for translation. Since the translation output vocabulary is the same as for the understanding input, these two graphs can be easily composed using the composition function (∘) to derive a joint graph:
                           
                              (5)
                              
                                 
                                    λ
                                    joint
                                 
                                 =
                                 
                                    λ
                                    translation
                                 
                                 ∘
                                 
                                    λ
                                    understanding
                                 
                              
                           
                        
                     

This composition takes a sentence in the target language as input and assigns a sequence of concepts to that sentence using a semantic tagger available for the source language. Although graphs are composed sequentially this consists globally in a joint decoding between the translation and the understanding systems since the probability scores of their models are taken into account simultaneously to determine the best overall hypothesis.

Interestingly enough the transducer λ
                        
                           joint
                         can be generalized quite easily to allow a composition with a speech recognition graph in the context of a human–machine dialogue system (Jabaian and Lefèvre, 2013).

@&#RELATED WORKS@&#

Joint decoding between components in human–machine interaction systems has already been the subject of many attempts in the past. However we are not aware of any comparable work dealing with joint decoding in a multilingual dialogue context. In a standard pipeline architecture, the system transmits the best transcription hypothesis from the automatic speech recognition system to the speech understanding module. While being the best according to the speech recognizer model this hypothesis is still noisy and may prevent to retrieve the true meaning of the sentence. Therefore several studies have proposed a joint decoding between speech recognition and understanding which afford to take into account the n-best recognition hypotheses during the semantic tagging instead of only the best one.

An early work (Tur et al., 2002) proposed to produce word-level confusion networks from ASR word graphs (lattices) instead of the ASR 1-best hypothesis. It has been shown that using lattices as SLU input decreased the classification error rate in the case of the AT&T's “How May I Help You” dialogue system. In parallel Servan et al. (2006) has shown that an understanding process can be integrated into an ASR system, without using the usual sequential approach that looks first for the best string of words before looking for the best string of concepts. The positive results obtained by these proposals have encouraged further work along the same line. The understanding system in this proposal was represented as a WFST, where the weights were obtained by maximum likelihood estimates on the training data. Then joint decoding is obtained by the composition of the recognition graph with the understanding graph. A similar work (Hakkani-Tur et al., 2006) proposed to exploit a confusion network out of several recognition graph outputs for the tasks of named entity detection and extraction and also call classification in a spoken dialogue system.

Given that the most successful models in the state-of-the-art are CRF, Deoras et al. (2012), Tur et al. (2013) used them instead of WFST for understanding and proposed a node spiting algorithm to obtain a segmentation of the acoustics from the speech lattice. Similarly, our proposal seeks to perform a joint decoding for translation and understanding (for our language portability scenario). But since the two systems are different in nature, their joint optimization is made difficult. This is why, in this paper, we try to first standardize the systems over the considered tasks.

@&#EXPERIMENTS AND RESULTS@&#

Experiments reported in the paper have been carried out using the French Media dialogue corpus (Bonneau-Maynard et al., 2006). The evaluation of the translation performance is based on the BLEU score while the evaluation of the understanding performance is based on the Concept Error Rate (CER) score. Details about the Media corpus and evaluation criteria are presented in Section 5.1.

First, we start by evaluating and comparing the LLPB-SMT and CRF approaches for understanding (Section 5.2), then we evaluate and compare these approaches for translation (Section 5.3). The proposed CRFPB-SMT model has been evaluated for translation and understanding, as reported in Section 5.4. Finally we evaluate our proposition for a joint decoding between translation and understanding in a multilingual dialogue context in Section 5.5.

All experiments presented in the paper use the Media French dialogue corpus (Bonneau-Maynard et al., 2006). Media covers the domain of hotel reservation and tourism information. This corpus is annotated with 99 different semantic labels that represent the domain semantics. An example (translated from French) of the semantic annotation used in the Media corpus is given in Table 1
                        . Some attempts to bring further the quality and informativeness of the semantic representation have also been reported in the context of the Media corpus, see for instance (Meurs et al., 2009) for an example of introducing graphs of semantic frames in replacement of the flat semantic tag representation.

The corpus is composed of 1257 dialogues involving 250 speakers, collected with a Wizard-of-Oz setting (a human operator simulating the dialogue system) and grouped into 3 parts: a training set (13k sentences), a development set (1.3k sentences) and a test set (3.5k sentences). A subset of the training data (about 5.6k sentences), as well as the test set and the development set, are manually translated into Italian.

The French annotated corpus is used to train models for understanding, and the manually translated subset of this corpus is used as a parallel corpus to train the translation models.

CER is the evaluation criterion used to evaluate the understanding task. It can be defined as the ratio of the sum of concepts deleted, substituted and inserted to the total number of concepts in the reference. On the other hand the BLEU score (Papineni et al., 2002), used to evaluate the translation task, is based on some statistics over the counts of n-grams shared between the hypothesis and the reference.

The MOSES toolkit (Koehn et al., 2007) is used to train an LLPB-SMT for spoken language understanding in French. The first attempts showed clearly inferior performance to those obtained by a CRF baseline model (CER 23.2% after MERT tuning for LLPB-SMT compared to 12.9% for CRF
                           1
                        
                        
                           1
                           Please refer to Jabaian et al. (2011) for more details on the CRF system and Jabaian et al. (2013a) for a comparable experiment but where the objective was to use LLPB-SMT to train an understanding system for Italian.
                        ). Incremental improvements of the model as proposed in Section 2 are evaluated in Table 2
                        .

Using a monotony constraint during decoding allows a reduction of 0.5% absolute in CER. Rewriting the training data into the BIO formalism reduces significantly the CER (2.4% absolute). Optimizing the CER score instead of BLEU reduces the CER of an extra 0.4%. Finally, the addition of a list of named entities (cities) in the training set efficiently handles the OOV word issue and provides a final CER reduction of 0.8%.

Results show that despite all the improvements on the LLPB-SMT approach, the CRF based approach still performs better for the understanding task (CER 12.9% for CRF vs. 18.3% for LLPB-SMT).

The error analysis for each model shows that CRF results present a high level of deletions compared to the other types of errors, while LLPB-SMT results have a better trade-off between deletion and insertion errors, even though it ends up with a higher CER because of direct substitutions. A major amount of errors made by the LLPB-SMT approach is coming from an incorrect segmentation, including over-segmentation of the sentence. For example, a sentence like “Je voudrais reserver en fait pour la ville de Nice du premier au trois novembre” (which means “I would like to book in fact at Nice city from the first to the third of November”) has one insertion coming from an over-segmentation of the sentence while this sentence is perfectly annotated by the CRF models (see Fig. 1
                        ).

This characteristic of the LLPB-SMT leads to a balanced distribution of errors between deletions, insertions and substitutions, while for CRF the majority of errors comes from deletions. For instance, the sentence “un autre quartier du côt 'e de Saint-michel” (“another neighbourhood near Saint-Michel”) will generate three deletions using a CRF model, while it generates one insertion, one substitution and one deletion using a LLPB-SMT model (see Fig. 2
                        ).

This analysis led us to try to combine three outputs (1-best CRF, 2-best CRF and 1-best LLPB-SMT) in order to benefit from their respective qualities to enhance the global performance. The combination is obtained through a confusion network built upon the three hypotheses and the concept sequence corresponding to the highest scoring path is output. The performance is significantly improved (−1.1% CER) showing that the methods are complementary, despite different levels of performance.

To sum up this evaluation, it shows that the LLPB-SMT method, despite showing the worst performance, has a positive influence on the combination. So it is still interesting to note that the hypotheses from the two approaches have slightly different repartition and type of errors and then can be advantageously combined.

In order to evaluate a CRF model for translation we use the manually translated part of Media as a parallel corpus to train the translation model (Italian to French). The GIZA++ toolkit (part of the MOSES distribution) was used to automatically obtain a word-to-word alignment between the two languages and WAPITI (Lavergne et al., 2010) was used to obtain the parameters of the CRF model based on this alignment.

The CRF model for translation is trained using the RPROP algorithm as proposed in Section 3. Characteristic functions of 3-grams on the observations (source sequence) and bi-grams on labels (target sequence) are used to train the model. The performance obtained is presented in Table 3
                        . The results show that the performance of the CRF model (BLEU 42.5) is significantly worse than the performance obtained by the LLPB-SMT method using MOSES with basic settings (BLEU 47.2).
                           2
                        
                        
                           2
                           Please refer to Jabaian et al. (2011) for more details on the LLPB-SMT system.
                        
                     

In order to get more insights in the reasons of the gap in performance between the two methods, the LLPB-SMT approach is re-evaluated in the same conditions as the CRF approach. The LLPB-SMT method implements a reordering model while CRF, dedicated to sequential labelling, does not include such model. To measure its influence a monotony constraint is added during decoding for the LLPB-SMT model. Also the performance of the LLPB-SMT baseline model is obtained using a 3-gram language model. However, the computational complexity of the CRF approach does not allow to use such a language model size on the label side. So in order to evaluate the CRF and LLPB-SMT approaches under the same conditions, and since increasing the size of the feature functions of the CRF model was not possible at this point, we propose to evaluate the impact of reducing the language model from 3-grams to 2-grams in the LLPB-SMT system.

Furthermore, while using the CRF model, OOVs are translated by other words in the corpus according to the context of the sentence, unlike with the LLPB-SMT approach which tends to project the OOVs unmodified in the translated sentence. These OOVs, being in most cases city or place names, their translations do not change much from one language to another, and therefore their projection in the translated output is advantageous for LLPB-SMT models, as already stated. In that purpose, a pre-processing step for OOVs is implemented in the CRF approach to retrieve them in the source sentences and push them to the outputs.

The results presented in Table 3 show that the monotone decoding decreases the performance of the LLPB-SMT model by nearly 1% absolute. The use of a bigram language model increases the loss of an extra 0.3%. OOV processing allows the CRF model to recover 1.0% of BLEU score compared to the CRF baseline model. On the whole, despite downgrading the LLPB-SMT model and improving the CRF model, their performance are closer but a 2.5% absolute gap remains (BLEU 43.5% for CRF vs. 46.0% for LLPB-SMT). So after evaluating the importance of some features of the LLPB-SMT model, some endeavours are required to port them to a CRF-based model, this is the purpose of the CRFPB-SMT model.

A CRFPB-SMT model has been constructed for translation following the description in Section 3. This model was built using the N-CODE tool (Crego et al., 2011), implemented to train translation models based on n-grams (Mari`no et al., 2006). This tool uses the OpenFst library (Allauzen et al., 2007) to build a translation graph by composition of several transducers. The difference between the model implemented by this tool and the model we aim to develop lies mainly in the parameters of the so-called translation model. So we adapted the tool to use a CRF model to estimate the translation probabilities and a normalization of the probability scores obtained with the model is done over the different paths in the graph (as proposed by Lavergne et al., 2011).

In N-CODE, the reordering model (proposed in Crego and Mari no, 2006) is based on a set of rules extracted automatically from training data. This approach requires a grammatical labelling of the source training sentences and a word alignment between the source and target sentences to train the λ
                        
                           R
                         model. The TreeTagger tool (Schmid, 1994) was used for grammatical labelling and GIZA++ for word alignment. The language model used in our experiments is a 3-gram model trained on the target side of our training corpus using the SRILM toolkit (Stolcke, 2002).


                        Table 4
                         presents a comparison between three models on an Italian to French translation task: CRFPB-SMT, LLPB-SMT (baseline) and the CRF (presented in the previous section). The results show that the CRFPB-SMT approach based on transducers gives intermediate performance between those obtained by the LLPB-SMT and CRF approaches. Introducing two important features for translation (word reordering and longer n-grams) does not permit to fill the gap with LLPB-SMT as it only increases the CRF-based system performance by 0.6% absolute. However, despite a remaining gap of 3.1% absolute, it could be noted that the performance are already fairly high for a translation task (despite a small-sized training set), which in our context can be explained by the limited vocabulary of the domain (in the order of several thousands).

The same mechanism used to obtain a translation graph is applied to the understanding task. In a first step, the graph of concept hypotheses is obtained by composing all steps in CRFPB-SMT (λ
                        
                           S
                        
                        ∘
                        λ
                        
                           R
                        
                        ∘
                        λ
                        
                           T
                        
                        ∘
                        λ
                        
                           F
                        
                        ∘
                        λ
                        
                           L
                        ). This approach gives a CER of 15.3%. To take into account the specificities of understanding (no requirements for a reordering model or a long-range language model in the target side), we propose to simplify the process by combining only λ
                        
                           S
                        
                        ∘
                        λ
                        
                           T
                        
                        ∘
                        λ
                        
                           F
                        . This allows to increase the performance of this approach by 2.2% absolute (15.3% vs. 13.1%) to obtain almost the same performance as pure CRF (12.9%). So it seems that in this case some features well suited to the translation task are not really appropriate for the understanding task. The comparison between the performance of the different versions is recapitulated in Table 5
                        .

In order to avoid the need to predefine the appropriate composition of models depending on the target task (full composition for translation vs. simplified composition for understanding) we propose to use the MERT algorithm to optimize the weights of the different scored components of the model (reordering, translation and language models).

The results presented in Table 6
                         show that the use of MERT in the CRFPB-SMT model is advantageous compared to the basic configuration. For SLU, the results show that the weight optimization improves significantly the performance of the CRFPB-SMT system (CER decreases from 15.3% to 13.0%). The CRFPB-SMT approach based on transducers, after MERT optimization, gives comparable performance to the one obtained by the state-of-the-art CRF approach (13.0% vs. 12.9%).

So not only the performance of the systems is increased but in a fully automated way, saving a lot of expertise design. That can be seen in the comparable results obtained by the simplified proposed composition (adaptation to the understanding task) and the generic approach tuned with MERT (13.1% vs. 13.0%). To be thorough, we have also observed, through not reported complementary experiments, that the use of MERT in the simplified proposed composition does not increase its performance. This observation highlights the importance of the use of MERT in order to reduce the need of a predefined configuration of the system.

In the same line, the MERT optimization increases significantly the performance of the CRFPB-SMT system for translation (BLEU increases from 44.1 to 45.7 for Italian-to-French translation). Subsequently, CRFPB-SMT (+MERT) is used for all reported experiments hereafter.

A joint decoding for translation and understanding is now evaluated, as proposed in Section 4. This decoding semantically labels Italian sentences, by combining an Italian to French translation system and a French understanding system. For that, the acceptor of the French understanding model (given in the last row of Table 5 and described in Section 5.4) is adapted to accept graphs as inputs (instead of 1-best hypotheses). The model is then able in its turn to generate a weighted hypothesis graph of semantic tags that takes into account translation variants and their scores.

Two categories of scores are considered during the decoding (translation and understanding). As a starter the final score for each path of the graph is simply the sum of the translation and understanding scores along a particular path in the graph. The best path is then selected among all possible paths of the graph. This path represents a joint decoding between translation and understanding.

The joint decoding is operated in two different configurations: in the first one, the translation system used is a LLPB-SMT model (using the MOSES toolkit) while we used a CRFPB-SMT (as described in Section 5.4) in the second one. In both cases the performances are compared with or without taking into account the hypotheses graph in the joint decoding. When the hypothesis graph is not considered it is either the 1-best or the oracle from the translation graph which is transmitted to the understanding system. Oracle scores represent an evaluation based on the translation hypothesis which is the closest to the reference in terms of editing operations. They allow to measure more precisely the impact of the translation quality on the overall understanding performance.


                        Table 7
                         gathers all the comparative results. The oracle scores (for translation and understanding) were also evaluated for all the different experimental settings, and the BLEU score of the translation selected by the joint decoding was also computed (last column of Table 7).

The first line of Table 7 corresponds to the baseline combination wherein the output of a LLPB-SMT model is directly given as input to the SLU (CRF) model. The results show that the translation graph improves the performance of the system compared to the system using the 1-best only (CER 19.7% vs. 19.9% for LLPB-SMT and 20.4% vs. 20.9% for CRF). Using a translation graph also gives better performance compared to the combination with the translation oracle (CER 19.7% vs. 19.8% for LLPB-SMT and 20.4 vs. 20.7 for CRF).

The difference between the performance obtained by the joint decoding using LLPB-SMT model for translation and this obtained using a CRF model (CER 19.7% vs. 20.4%) can be explained by the difference between the performance of these two models (BLEU 46.9% vs. 45.7%).

It is important to mention that only combinations taking an input graph for SLU allow to select the translation according to the final semantic labelling that will be obtained. In other cases, the selection of the translation is done independently. We note that the BLEU score of the translation selected by joint decoding is lower than for the best possible translation (46.3 vs. 47.2 for LLPB-SMT and 45.3 vs. 45.7 for CRF) despite that the former is better in terms of CER. This confirms the interest of the joint graph-based method which allows to select the translation that can be conceptually labelled in the best possible way.

Oracle scores show that the hypothesis selected when decoding is not necessarily the closest to the reference. However, this result shows that the system performance can be further improved by adjusting the weights of the models since better hypotheses are in the graph. In the joint decoding so far, translation and understanding scores have equivalent weights (overall score is a sum of these two scores). This simple configuration is certainly not the best possible and by giving different weights to the models the system performance can be improved.

As a first attempt, we propose to evaluate the performance (CER and BLEU) obtained by giving different weights to the models. Thus the joint decoding can be presented as a linear combination:


                        
                           
                              (6)
                              
                                 
                                    λ
                                    joint
                                 
                                 =
                                 
                                    γ
                                    smt
                                 
                                 
                                    λ
                                    smt
                                 
                                 ∘
                                 
                                    γ
                                    slu
                                 
                                 
                                    λ
                                    slu
                                 
                              
                           
                        
                     

In our experiments γ
                        
                           smt
                        
                        +
                        γ
                        
                           slu
                         are normalized and sum to 1.

The results of this evaluation are plotted in Fig. 3
                         with respect to γ
                        
                           smt
                        . It can be observed that the BLEU score increases slightly but constantly while rising the weight of the translation model. On the other hand the best CER is obtained with a balanced repartition of scores (middle of the curve) and increasing the influence of the understanding model did not improve necessarily the CER.

These preliminary results show that the weight adjustment for a joint decoding can lead to a better solution, at least in terms of BLEU score. For that, we propose to apply the MERT algorithm in the same way that we did for tuning the weights of each system independently.

A development set of the Media corpus (different from the test set) is used for tuning. This tuning takes into account the feature functions of the two composed models (λ
                        
                           R
                        , λ
                        
                           F
                        , λ
                        
                           L
                         of the translation model and λ
                        
                           R
                        , λ
                        
                           F
                        , λ
                        
                           L
                         of the understanding model). The weights tuning of these components represents a joint optimization of the translation and understanding models.

This joint optimization increases the overall performance of the system by 0.3% absolute (CER 20.1% compared to 20.4% obtained by the composition without tuning). It is important to mention that a comparable gain in performances (0.2% absolute) can also be obtained by a joint optimization of a LLPB-SMT translation model and a CRFPB-SMT understanding model.

@&#CONCLUSION@&#

In this paper, stochastic approaches for both speech understanding and automatic translation tasks were evaluated and compared. It has been shown that the discriminative CRF approach is the best approach for speech understanding, despite all the adaptations of LLPB-SMT approach for the task. Using a CRF approach for translation has several limitations and the performance of this approach can be improved by using a model based on transducers allowing the integration of appropriate steps in the process (reordering, segmentation and target language model scoring).

To step further and address multilingual understanding we proposed and evaluated an approach for joint decoding of the translation and the understanding systems. Such a decoding has been shown to achieve good performance while at the same time providing a homogeneous system for the two underlying tasks.

In the context of a complete human–machine dialogue system, a joint decoding between speech recognition and translation can be straightforwardly added. This allows the recognition system to transmit a richer information to the understanding system and the understanding system in its turn will transmit a better information to the dialogue manager which might positively influence the overall system performance.

@&#ACKNOWLEDGEMENTS@&#

This work is partially supported by the ANR-funded MaRDi project (ANR-12-CORD-0021). More information on the project website, http://mardi.metz.supelec.fr.

@&#REFERENCES@&#

