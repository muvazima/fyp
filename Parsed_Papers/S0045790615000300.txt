@&#MAIN-TITLE@&#A face detection method based on kernel probability map

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A multi-stage face detection method is proposed using color and texture information.


                        
                        
                           
                           Using skin detection, candidate windows are extracted.


                        
                        
                           
                           Candidate windows are verified based on combination of KPM and LHP.


                        
                        
                           
                           Both qualitative and quantitative results confirm the merit of the algorithm.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Face detection

Skin detection

Face segmentation

Face extraction

Image processing

Kernel probability map

@&#ABSTRACT@&#


               
               
                  Face detection is one of the most important parts of biometrics and face analysis science. In this paper, a novel multi-stage face detection method is proposed which can remarkably detect faces in different images with different illumination conditions, variety of poses and disparate sizes. The idea is to utilize a preprocessing step to filter many non-face windows by means of a skin segmentation procedure in order to boost the speed of the detection and also utilize the color information as much as possible. Subsequently, candidate windows are fed to a Local Hierarchical Pattern (LHP) generator unit where a new texture pattern is produced. Based on this pattern, a kernel probability map is calculated for each window, and by summing probabilities of all kernels and comparing it with a predefined threshold, decision is made about content of the window. Not only does this algorithm effectively eliminate many non-face regions, but it is also capable of detecting faces with relatively acceptable rate in different conditions.
               
            

@&#INTRODUCTION@&#

Face detection is one of the most prominent algorithms in biometrics, and a considerable number of practical systems depend on face segmentation algorithms. Face and skin detection applications are myriad including human computer interaction (HCI) [1], face recognition, face tracking, facial expression recognition, fatigue detection, pedestrian and upper body detection [2,3], visual surveillance, gesture recognition [4], robotics, video and image indexing, multimedia and forensic applications. Just to mention in more detail, one specific example of these applications is the driving monitoring system. A glance at actuarial of sleepy people driving cars reported by National Sleep Foundation’s 2005 Sleep in America poll shows that, 60% of adult drivers (about 168million people) have admitted that they have driven a vehicle while feeling drowsy in the past year, and more than one-third (37% or 103million people) have actually fallen asleep at the wheel [5]. As a matter of fact, more than 1 tenth of those who have nodded off, say they have done so at least once a month and four percentages of them avouched that they have had an accident or near accident since they were too tired to drive. The National Highway Traffic Safety Administration have estimated that 100,000 police-reported crashes are the direct result of driver fatigue each year resulted in an estimated 1550 deaths, 71,000 injuries, and $12.5billion in monetary losses [5]. Fatigue directly affects the driver’s reaction response time and drivers are often too tired to realize the level of inattention. Thus, a driving monitoring system shall be a promising solution to this serious problem. In such systems, several features including different signs and body gestures in the eyes or head such as repetitious yawning, heavy eyes and slow reaction will be exploited as a drowsiness cue. An essential step before facial feature extraction in such systems is detection of human face. In such applications, both the accuracy and speed of the face detection algorithm is determinative.

Albeit face detection is utilized in many applications and systems, it is still an unsolved problem due to many difficulties and challenges involved in it. Shape of jaw, length of forehead, lips, eyes and other features are varied person to person that make it very difficult to propose a method only based on either the template or shape of the face. In addition, though the color of the face is almost identical for different people (i.e. in specific range of a color space), intensity (or brightness) varies significantly, and nonlinear and uneven illumination makes it worse. Besides various illumination and imaging conditions, disparate responses of camera devices makes it also very hard to use methods just based on skin and face color. Different face poses, occlusion of part of a face, different facial expressions and rotation of faces are other notable challenges [6]. The challenge is to design a system that work reliable in all conditions. Accordingly, recent trends in face detection methods show that the proposed algorithms are quite different from the ones proposed before. Early methods were often based on a single feature or task, while new methods combine different features, e.g., motion, color, shape, edge, and in higher levels, neural networks, Adaboost, SVM, etc., to overcome the challenges.

The proposed method in this work utilizes the color, texture and shape of the face to increase the robustness of the system. The novelty of this method is in its skin segmentation procedure and also in its texture based candidate verifier. The skin classifier is constructed based on a novel algorithm that can efficiently eliminate many non-face regions. It utilizes the idea of ternary image and performs several spatial processing steps to specify high probable skin regions. This algorithm is capable of eliminating 75 percentages of each image in average, while missing a negligible number of faces. After recognizing the potential face regions, a new texture based method is employed to recognize the faces inside these regions. The employed texture method is combined with the concept of kernel probability map which remarkably increases the robustness.

The paper is organized as follows. In Section 2, a brief discussion on face detection algorithms is presented. In Section 3, the proposed algorithm is presented and discussed in detail. Experimental results are provided in Section 4, and finally, conclusion is presented in Section 5.

Numerous number of papers have been published in last two decades each proposing a particular method of detecting faces in images. These works are grouped into 4 categories; namely, knowledge-based methods, feature invariant methods, template matching methods, and appearance-based methods [1]. Knowledge-based methods are based on some predefined rules in geometry of the face. Though the simplicity might be an advantage for such algorithms, the robustness is highly questionable. Considering different shapes of the faces (in appearance and size), and difficulties associated with rotation in the head pose or wearing a glass or mustache, make these methods inefficient for use in a general system. Due to lack of robustness, new methods based on exclusive usage of rules in facial appearance have been rarely introduced in recent years. In feature invariant methods, one or several apparent features of the face such as edge, texture, color and even motion is used to localize the position of the face. Skin color based models due to their simplicity are found widely, however yet false detection is too high as skin models are either computationally inefficient or their false positive rates are too high. In [7], Gaussian color model in YCbCr color space is utilized to find faces. These models are not applicable in many practical systems as they are limited to their training faces. In [8], an embedded implementation of a skin detection method on FPGAs is proposed by means of YCbCr and I1I2I3 color spaces. These colors are combined using an explicitly defined method of skin segmentation in order to separate skin pixels from non-skin ones. However, since no illumination cancellation algorithm is considered, the system is not robust against illumination and imaging conditions. In [9], a method based on the edge and skin tone information of the input color image is presented. After conducting skin segmentation in YCbCr and RGB space, the result is modified using a skin tone percentage index method, and then, the edge map of the image is combined with the skin tone image to separate all non-face regions from candidate faces. In [6,10,11], skin color models and their applications to face detection are probed and investigated in detail.

In template matching algorithms, faces are found by means of measuring correlation between several predefined or deformable templates and under-test windows. One of the best representatives of this method is the one presented in [12], in which Karen et al. proposed a method of detecting any arbitrary object by using sequential detectors. Detectors or classifiers are developed through a constrained optimization framework where the inner product of template with the under test image is minimized. Appearance-based methods include a large number of pre-test face images utilized to train a classifier; and then, in the test phase, input images are fed into the classifiers. VJ (Viola–Johns) face detector [13] and neural network based classifiers are two well-known methods in the last category. In [14], a segmentation-based illumination normalization method is proposed to compensate non-uniform illumination, and a Haar-classifier is followed for face detection. Neural network based systems are countless. In [15], a convolution based neural network is proposed with a quite good detection rate of 87% and low false positive of 4%. In [16], a face detector is presented which uses principle component analysis in order to extract a feature vector and develop a weak classifier. Subsequently, Adaboost algorithm is applied in order to boost the performance and construct the final strong classifiers. In [17,18], variety of appearance based methods, including different variations of VJ face detectors are elucidated in detail.

A group of relatively newer face detectors are based on combination of former methods. In [19], a face detection method based on both LUT skin segmentation and boosted cascade of classifiers is proposed. This system is evaluated in different data bases and it is reported that both speed and accuracy are improved compared to those of some former systems. A face detection system for surveillance applications is proposed in [20] based on a skin segmentation and Adaboost classifier. Simulation results in different lighting conditions showed that the system is not very effective in low light and high light conditions and it misses many faces. Wang et al. [21] introduced another system based on modified LBP texture, lip feature extraction and skin color segmentation. These features are combined to detect faces in an image. Even though this system seems to have a high quality with great speed, it is not still efficient as it does not consider the luminance component of YCbCr space that will degrade the performance of the system [6]. In [22], an improved version of Gaussian distribution based face detection is introduced. This system is a multi-layer system, consisting of a skin-color detection, labial feature extraction, hole feature extraction, binary image projection, Gaussian distribution and boundary adjusting. The important point here is that how much the estimated Gaussian function is close to the ideal one, and no test has been performed to verify the premise that distribution of skin color pixels in RGB color space is completely Gaussian. In addition, not any formal data base is used to evaluate the performance.

The proposed method in this work utilizes the color information of human skin and also many training faces in order to build a KPM (Kernel Probability Map). It indirectly uses the template of the face when it compares the under test window’s KPM with the reference one. The LHP is a particular texture derived from LBP (Local binary Pattern) family of textures. This new feature is not a binary one, but is locally calculated similar to LBP.

In this section, a general overview of the proposed face detection system is presented and then each part of the system is elucidated in detail. The system utilizes a window-based scanning method which is used in many other image processing tasks too. In this method, the image is segmented into a set of windows in which each may or may not contain a face. For faces with sizes more than a typical window size, the algorithm is performed repeatedly on different scales of an image. Therefore, each face will be included at least in one window. This process is known as generating mosaic images in literature. The proposed method uses a skin segmentation algorithm which is capable of removing more than 70 percentages of non-face portions of an image (in average) while it can preserve more than 97 percentages of faces. Each window is processed, and the final result is a mask which specifies the ROI (Regions of Interest) to search for faces. This is done for all the scales. The resulting mask covers all candidate windows that probably contain faces. In the next stage, the LHP conversion of the image is calculated and then it is utilized in kernel probability map. Each window is processed in some parallel pose units. The structure for each pose unit is the same and the only difference is their kernel reference maps that will be explained later. By using kernel probability map in each kernel, the probability that a window contains a face is calculated and the result will be compared with a threshold value. The reference maps and threshold values are provided in training stage. The output of all pose units will be used in the decision unit to make the final decision. Different steps of the algorithm are summarized in Fig. 1
                         and block diagram of the design is depicted in Fig. 2
                        .

In the following, the blocks shown in Fig. 2 are explained in detail.

The optimum performance of a skin detection algorithm is not dependant on the chosen color space [23]. However, in this paper, the YCbCr color space is used for some reasons. YCbCr is similar to the human visual perception process and it is a discrete color space which makes it easy to realize clustering algorithms. One advantage of YCbCr color space compared to some other color spaces like HSV and TSL is that it can be obtained easily from RGB data by using a linear transformation. Comparing to RGB, the redundancy of YCbCr channels is much lower and its components are highly independent. YCbCr is widely used in European television studios, JPEG, MPEG and CCD cameras, so it can be used directly in many applications without transforming the image into another color space. Also, in [24], it has been shown statistically that the performance of YCbCr-based methods compared with other color spaces is often superior.

The act of making mosaic images [1] (different scales of one image) is carried out in order to include faces with sizes larger than default window size. The scaling factor is chosen 1.1 since it has been experimentally yielded to acceptable results. The size of an input image is optional, but window’s size is set to 48∗48pixels. This size is enough for images in the range of 640∗480, but the algorithm may miss very small faces for lower dimension images. For any given image with an arbitrary size, down sampling continues until the size of the image will be less than window’s size. The overall process of face extraction is undertaken for all scales (similar to every other face detection method) and the final result is based on fusion of all outputs.

The skin detection process contains a training stage that is performed offline. Necessary statistical data obtained in this stage are preserved for further usages. It is used to partition the color space into 3 sets of pixels. These sets will be used in the pixel based stage to build the ternary image. For each input image, each pixel is processed in a pixel-based, neighbor-based and a window-based process and the final output will be elected windows.

In order to train the system, about 63million skin pixels have been collected from World Wide Web. The intensity heat maps of these pixels are depicted in Fig. 3
                        . Each map is divided into 3 non-overlapping partitions. The boundary of each partition is estimated by a polygon in order to keep up both the simplicity and accuracy of the algorithm. For each map, 2 polygons are estimated. Pixels which are observed with high frequency are surrounded by the inner polygon. The outer polygon separates pixels which are not necessarily highly repeated, but, they can still be part of the skin in some conditions, and non-observed pixels. Thus, these two boundaries separate pixels into the pixels observed with low frequency, observed with high frequency, and pixels which are not observed at all. This has to be carried out for each map.

Accordingly, three sets of pixels; T1, T2, and T3 are defined. T1 contains pixels which are simultaneously located inside inner polygons of all three maps. T2 contains pixels which are inside the outer polygons and are outside the inner polygons of three maps. And the last one T3 is filled with pixels which are not located inside the outer polygons of all three maps. As it will be shown later, by this partitioning, dealing with nonlinear illuminations will be much easier. The data needed are coefficients of polygon lines which are extracted in training stage and they are kept in the memory of the processor.

Segmentation of an image into a ternary one is the first step of the algorithm. Here, contrary to most existing methods, the image is not roughly classified into two groups of skin and non-skin pixels; and the reason is that there are lots of uncertain pixels that cannot easily be put in either of two groups. In the proposed ternary segmentation, if for some reason, like non-uniform illumination, decision on some pixels is difficult, instead of randomly discard or accept them, they are kept aside to be processed in next steps based on their neighbors and region information (spatial based segmentation).

In pixel-based segmentation, each pixel of the input image is classified in accordance with the 3 predefined sets. T1 pixels are set white (255), T3 pixels are set black (0) and T2 pixels are set gray (128). Fig. 4
                         shows an input image selected from LFW data base and its ternary conversion. The figure shows that the polygon estimation has correctly segmented the image and the idea of ternary image is reasonable. Using simple methods such as explicitly defined regions or even Bayesian classifier (LUT) will not lead to accurate results. Using some of these methods has been yielded to false alarm rate in simulation. In Fig. 4, those skin regions that are not highly affected by nonlinear illumination are detected correctly and these pixels more or less will be specified by most of the methods. These pixels are in T1 set and shown by white points in the figure. The gray pixels (in set T2) are those for which a strict decision cannot be made. Some of them are truly skin pixels, while some are not; it is necessary to consider a method for correct classification.

It has been observed that there are many skin pixels surrounded by a considerable number of white pixels; but, they have not been set white yet. This may have been due to illumination, a shadow, moustache, etc. The neighbor based procedure compensates these effects to some extent. In addition, isolated pixels mainly caused by noise will be eliminated too.

Each pixel of the ternary image is processed in the neighbor based algorithm. A 3×3 and a 5×5 neighborhood are considered for all of the pixels except skin pixels which have been verified before (the T1 pixels) and bordering pixels. For each under test pixel (UTP), based on the number of T1, T2 and T3 type pixels in its neighborhood, and by using a set of rules, a score is calculated. To that end, the numbers of surrounding white, gray and black points are obtained, and subsequently, considering both the simplicity of the algorithm (it should be as fast as possible) and the fact that false rejection rate shall be as low as possible (with the cost of higher false detection), a semi look-up-table procedure is used to calculate the score. It is done for both the 3×3 and 5×5 neighborhoods. The final score (for each UTP) is computed as:
                           
                              (1)
                              
                                 ξ
                                 =
                                 K
                                 ×
                                 T
                                 +
                                 Φ
                              
                           
                        where ζ is the total score, K is a simple constant to give more weight to nearer pixels, and T and Φ are, respectively, the computed scores (as explained above) for the 3×3 and 5×5 neighborhoods of the UTP. Using two experimentally chosen threshold values Thr1 and Thr2, a hysteresis thresholding is applied to increase the robustness of the algorithm. Based on the value of ζ and Thr1 and Thr2 (Thr1
                        <Thr2), a ternary decision is made on UTP. If ζ is lower than Thr1, then UTP will be considered black, if it is higher than Thr2, the pixel will be considered white, and if it is between Thr1 and Thr2 then the pixel will be considered gray again. The values of Thr1 and Thr2 are statistically determined by applying different values of thresholds and observing the effectiveness of the algorithm in such a way that high false detection is acceptable while false rejection is minimized. Even though it is possible that the algorithm specifies some non-face regions as a face candidate, the follow up texture based segmentation procedure will eliminate most of them. In fact, the output of this stage is another ternary image that for some of its pixels still no decision has been made.

Without deteriorating the generality of the algorithm, the size of output windows is considered to be 48×48. In window based segmentation, each window is determined to be a candidate or not. Decision is made based on the number of white and gray points in the window. If there are no white points, then it is reasonable to bypass it and if it contains many white points, then there is a high probability that the window contains whole or at least a part of human face. Thus, a set of rules are utilized to filter windows. Thresholds could be adaptively determined by the system; however, to boost the speed of the algorithm, they are determined empirically. An important factor considered in setting thresholds and rules in skin segmentation part is minimizing false rejection rate as much as possible. The false positives will be filtered out by subsequent stages. To improve the results, after window processing, a morphological operation is performed to add missing parts of a face that are probably occluded by other objects like glasses or even eyes since they have different colors from skin color. Therefore, windows surrounded by candidate windows will be annexed to previously chosen ones (hole filling). Fig. 5
                         represents two images selected from LFW data base, and the result of applying the algorithm on them.

The LHP conversion unit is designed to convert the luminance of the image into LHP (Local Hierarchical Pattern). As it is observed in Fig. 6
                        , using luminance of an image to build kernel probability map will not produce satisfactory results, and faces and non-faces are not separated at all. However, texture is a great tool that can effectively discriminate between face and non-face windows when combined with the idea of kernel probability map. Fig. 7
                         represents the probability of a window based on LHP discriminator. In each of the charts, the probability sum in specific window of an image, for identical face and non-face number of images, is represented. Both Figs. 6 and 7 show the advantage of LHP over the luminance, as when using LHP, the distance between probability of faces and non-faces are noticeable and it logically helps better separation of face and non-face windows.

Another important feature of LHP is its capability to cancel illumination effect to some extent. Illumination in nature has low frequency components and it is almost stable all over an image. The fact that LHP is a local operator and it is based on differences makes the illumination cancellation better. Fig. 8
                         graphically compares luminance of 5 faces with their LHP representations. It can be observed that LHP contains huge section of face information and it is almost unaffected in different illumination conditions. Thus, it can be expected that if in color segmentation part, the algorithm could not remove parts of the image due to uneven and nonlinear illuminations, by this local transformation, it would be possible to counteract this effect and detect faces even in different lightings conditions.

Similar to LBP textures, LHP of each pixel depends on the values of its surrounding pixels. For the center pixel, shown in Fig. 9
                        , LHP is calculated as:
                           
                              (2)
                              
                                 LHP
                                 
                                 (
                                 x
                                 ,
                                 y
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          k
                                          =
                                          1
                                       
                                       
                                          9
                                       
                                    
                                 
                                 
                                    
                                       π
                                    
                                    
                                       k
                                    
                                 
                              
                           
                        where πk
                         (1⩽
                        k
                        ⩽8) for each of the neighbors is calculated by:
                           
                              (3)
                              
                                 
                                    
                                       π
                                    
                                    
                                       k
                                    
                                 
                                 =
                                 Ψ
                                 
                                 (
                                 
                                    
                                       Y
                                    
                                    
                                       k
                                    
                                 
                                 -
                                 A
                                 )
                              
                           
                        in which Yk
                         is the luminance of neighbor k, A is median of pixels’ luminance in the 3×3 neighborhood, and Ψ is a roof function with nonlinear coefficients. For each pixel, Eq. (2) is calculated. These coefficients are empirically considered in such a way that illumination will be eliminated as much as possible and maximum separation is obtained when the texture is combined with kernel probability map. The gray level of the LHP image is directly related to simple and small nano-patterns (edges, points, etc.) of the input image. The LHP image will be utilized in order to build the kernel probability map.

KPM (Kernel Probability Map) is a concept used here to measure the probability that a window holds a face. KPM is a matrix with certain number of rows and columns. Rows are related to the number of kernels in a window and columns are related to the number of bins (for each kernel). Each pixel is related to one kernel. In general, a kernel can be considered as the average of an n
                        ×
                        n neighborhood, but in this paper, a one to one relation is utilized i.e., for a 48×48 window there would be 2304 possible kernels. In order to build the reference KPM, more than 400 48×48 face windows were utilized. For a fixed pose, it is reasonable to suppose that in each kernel of a window, the intensity of the LHP image has a certain value and it can be probabilistically calculated. In other words, for each kernel, by using 400 face window images, a probability function is estimated. Fig. 10
                         illustrates examples of probability functions (PF) for kernels number 100 and 200. In the figure, Y-axis is the normalized probability and X-axis represents the number of bins.

Bins are used because it was not necessary to consider all 256 possible intensity levels. 256 different gray levels are mapped into 27 bins or classes (non-linearly). PFs are made based on abundance of values of different classes in all 400 images. Then, each PF is normalized to its maximum value. This provides the reference map matrix (Eq. (4)) which is implementable in software or hardware platforms such as FPGAs easily.
                           
                              (4)
                              
                                 P
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         p
                                                      
                                                      
                                                         1
                                                         ,
                                                         1
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         p
                                                      
                                                      
                                                         1
                                                         ,
                                                         2
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         p
                                                      
                                                      
                                                         1
                                                         ,
                                                         3
                                                      
                                                   
                                                
                                                
                                                   .
                                                   .
                                                   .
                                                
                                                
                                                   
                                                      
                                                         p
                                                      
                                                      
                                                         1
                                                         ,
                                                         b
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         p
                                                      
                                                      
                                                         2
                                                         ,
                                                         1
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         p
                                                      
                                                      
                                                         2
                                                         ,
                                                         2
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         p
                                                      
                                                      
                                                         2
                                                         ,
                                                         3
                                                      
                                                   
                                                
                                                
                                                   .
                                                   .
                                                   .
                                                
                                                
                                                   
                                                      
                                                         p
                                                      
                                                      
                                                         2
                                                         ,
                                                         b
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   .
                                                   .
                                                   .
                                                
                                                
                                                
                                                
                                                
                                                   .
                                                   .
                                                   .
                                                
                                             
                                             
                                                
                                                   .
                                                   .
                                                   .
                                                
                                                
                                                
                                                
                                                
                                                   .
                                                   .
                                                   .
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         p
                                                      
                                                      
                                                         n
                                                         ,
                                                         1
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         p
                                                      
                                                      
                                                         n
                                                         ,
                                                         2
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         p
                                                      
                                                      
                                                         2
                                                         ,
                                                         3
                                                      
                                                   
                                                
                                                
                                                   .
                                                   .
                                                   .
                                                
                                                
                                                   
                                                      
                                                         p
                                                      
                                                      
                                                         n
                                                         ,
                                                         b
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        In Eq. (4), matrix P is the output of the training stage for one specific pose. Parameters b and n are the number of bins and number of kernels respectively. After fetching one window from the LHP unit, the intensity of each kernel will be calculated and in accordance with that, the probability of that kernel will be specified by using matrix P. In other words, the value of kernel nth will determine one specific element of the nth kernel (row) of P which gives one number between 0 and 1. It is considered as the probability of kernel nth to be part of a face. Simply, all probability values of a window will be summed up and the final number will specify the possibility that the window contains a face.

Not only is the method computationally cost effective and simply implementable, but also as simulation results showed, it is accurate enough to give acceptable detection rate and false alarm rate compared with other methods. It is noteworthy to point out that the above idea was also tested for differential kernels (building KPM based on difference of neighbor kernels instead of their intensity themselves), but the output was not interesting and the windows containing faces were not accurately separated from the non-faces windows. Combination of LHP concept and KPM definition leads to a noticeable performance as it was depicted in Figs. 6 and 7; where the luminance itself was not strong enough to discriminate between face and non-face windows. In other words, variation of Y values is too high to build an efficient KPM based on that. LHP extracts most prominent patterns in a window. In the proposed multi-layer face detection method, both color (skin segmentation) and shape of the face (LHP+KPM matrix) are utilized to detect faces in an image.

@&#RESULTS AND DISCUSSION@&#

The proposed algorithm has been developed via MATLAB environment. Qualitative results are shown in Figs. 11–14
                        
                        
                        
                        . In the figures, randomly selected images from LFW database are utilized to demonstrate the effectiveness of the method. In Fig. 11, the result of performing the proposed method on images captured in different lighting and illumination conditions is provided. As it is observed, the proposed method is capable of classifying face regions in color images in most of imaging conditions. Fig. 12 shows images associated with people having various races, ethnicities and nationalities. As it is also observed, the proposed method can handle different images from different groups of people around the world. Fig. 13 provides images useful for assessing the effectiveness of the method in dealing with different poses and orientations. Multifarious images are presented each distinguished with a particular pose or orientation. Faces rotated up to 30° are detected with no critical difficulties. In Fig. 14, the result of applying the method on different facial statuses which could be challenging in many conditions is presented. The faces have been detected with remarkable rate. The results of Figs. 11–14 show the algorithm is capable of processing arbitrary multifarious images with acceptable rate.

As mentioned before the proposed algorithm has been developed via MATLAB environment. Images were tested and the number of truly detected windows (associated with faces) and the numbers of incorrectly detected or missed faces have been calculated. In order to measure the performance of the proposed method, 4 standard data bases have been used to test the system. This helps to fairly compare the performance of proposed method with other available methods. Most face detection papers report their results in terms of FP, FN, TP, recall and precision, which are already defined in the literature for assessing any classification problem.

Generally, present algorithm is capable of processing approximately 10 frames (640×480) per second in a system with dual 2.17GHz processor and 3GB RAM where a 32-bit window 7 is the operating system. Table 1
                         shows the performance of the proposed system under LFW, Caltech, Bao, and GTDB data bases. As it is shown in Table 1, the proposed algorithm finds faces with a considerable detection rate. Caltech contains approximately 450 images with complex background and varied illumination. The robustness of the system against illumination and imaging conditions was verified through processing of these images. It is observed that only 11 faces are missed which were mostly occluded or very dark. For Bao data base, 210 images were tested. 149 of them were single face and the rest contained multiple faces. In this database, it was observed that most misses were related to faces which have highly rotated. GTDB with 750 faces is a database with images similar to that of Caltech. Best performance is obtained in GTDB. Table 2
                         provides comparison information between the proposed method and some other recently developed algorithms. As it is observed, in overall, the algorithm is capable of finding faces, with a remarkable rate, and false alarm rate is also acceptable.

@&#CONCLUSION@&#

In this paper, a new face detection method which can be utilized in variety of applications is proposed. The method is performed in separate phases. In one phase, a skin segmentation algorithm based on color information is performed in which regions of interest with high probability of existing faces are selected. In this part, a new idea of ternary image is proposed. It is different from the, rough image binary conversion in LUT, explicitly defined, Gaussian based, or even adaptive skin segmentation algorithms. After segmentation, in the second phase, candidate windows are fed to the LHP generator to produce a new pattern of image. This helps building kernel probability map of each window. Finally, the probability that each window contains a face is calculated. If it is more than a threshold value, the window is reported to have a face. This system is robust against different illumination conditions and various poses, which is shown both qualitatively and quantitatively through the results. At software level, the method is limited from the speed point of view. FPGA implementation of the algorithm is underway to make the system work in real-time, where all pixel based operations can be done in parallel in each step of the algorithm. Also the idea of using spatial analysis in skin classification is considered, which can hugely increase the accuracy of the system.

@&#REFERENCES@&#

