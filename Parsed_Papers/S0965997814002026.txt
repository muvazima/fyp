@&#MAIN-TITLE@&#The comparison of two domain repartitioning methods used for parallel discrete element computations of the hopper discharge

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Two methods are employed for dynamic domain decomposition of hopper discharge.


                        
                        
                           
                           Implementation of the k-way graph partitioning method in DEM codes is more complex.


                        
                        
                           
                           A higher speed-up is measured, applying the recursive coordinate bisection method.


                        
                        
                           
                           Parallel efficiency of 0.87 is attained simulating 5.1×106 particles on 2048 cores.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Dynamic domain decomposition

Domain repartitioning

Discrete element method

Multilevel k-way graph partitioning

Recursive coordinate bisection

Hopper discharge

@&#ABSTRACT@&#


               
               
                  The paper presents an application of two domain repartitioning methods to solving hopper discharge problem simulated by the discrete element method. Quantitative comparison of parallel speed-up obtained by using the multilevel k-way graph partitioning method and the recursive coordinate bisection method is presented. The detailed investigation of load balance, interprocessor communication and repartitioning is performed. Speed-up of the parallel computations based on the dynamic domain decomposition is investigated by a series of benchmark tests simulating the granular visco-elastic frictional media in hoppers containing 0.3×106 and 5.1×106 spherical particles. A soft-particle approach is adopted, when the simulation is performed by small time increments and the contact forces between the particles are calculated using the contact law. The parallel efficiency of 0.87 was achieved on 2048 cores, modelling the hopper filled with 5.1×106 particles.
               
            

@&#INTRODUCTION@&#

The discrete element method (DEM) is a numerical technique originally developed by Cundall and Strack [1] for predicting the behaviour of soil grains. The Lagrangian approach is applied to a system of moving particles with a given shape and material properties. The state of the particles is obtained by time integration of the dynamics equations derived from the classical Newtonian mechanics. The Newton’s second law is used for translation and rotation of each individual particle. All the forces and moments acting on each particle are evaluated at every time step. Thus, DEM enables us to investigate granular flow characteristics at the particle level, and to evaluate particle motion precisely. The increasing availability of computing power over the past three decades, along with refinements and adaptations of the original method, has made DEM applicable to a wide range of industries. Numerous examples include granular flows, milling, powder mixing, fluidized beds and fracture modelling [2].

The granular flow from hoppers and silos has a wide range of industrial applications. The conducted research is mainly focused on several aspects as follows: wall pressure, discharge rate and internal properties. The study of the bulk material pressure on the walls of a hopper plays a very important role in hopper design [3]. The prediction of the discharge rate is important for effective operation and control of the transport system [4]. The combined approach of DEM and the averaging method offers a possibility to link the fundamental understanding generated from DEM-based simulations to engineering application often achieved by continuum modelling [5].

The main advantage of using DEM for simulation of granular systems is that, by tracking the motion of each individual particle, the detailed information about the system behaviour across a range of time- and length-scales can be obtained. However, the simulation of systems at this level of detail has the disadvantage of making DEM computationally very expensive. DEM simulations on a single workstation or ordinary PC tend to be limited to systems of several tens of thousands of particles and short time intervals. The recent simulation of large-scale systems is performed by employing the parallel computation techniques, though the number of the used particles was still much smaller than that required in industry where typically over a billion particles are dealt with.

In spite of a progress in developing parallel DEM software and a high degree of natural algorithmic concurrency inherent in explicit time integration procedures, a detailed study of parallel performance is rather limited. It is difficult to find the quantitative comparison of speed-ups obtained employing different decomposition strategies and software libraries. Moreover, the presented speed-up analyses are rarely performed in solving the complex applications with a rapidly changing workload configuration. In the present research, two alternative repartitioning strategies for the dynamic domain decomposition are considered to investigate the benefits of their application to hopper discharge flows. This paper is based upon Markauskas and Kaceniauskas [6], however, the current paper includes the following additional research: the detailed investigation of repartitioning strategies, load balance, interprocessor communication and the solution of a larger benchmark problem.

In Section 2, the related works are overviewed and discussed. Section 3 describes the governing relations of the discrete element method. In Section 4, the employed domain decomposition methods and their software implementation are presented. The measured parallel performance is evaluated and discussed in Section 5, while the concluding remarks are presented in Section 6.

Parallelisation of DEM codes has become an obvious option for rapidly increasing computational capability, along with recent remarkable advances in hardware performance. Early attempts to parallelize computations of particle systems were based on several main ideas, including force decomposition, particle decomposition and domain decomposition [7]. In the first class of methods, each processor holds the information of all the particles, while a pre-determined set of force computations is assigned to each processor. Washington and Meegoda [8] divided the interparticle force computation among the processors but stored all particle information on each processor. They achieved a speed-up of 8.7 for 1672 particles with the help of 512 processors. Following the second class of methods, Darmana et al. [9] employed the particle decomposition approach and obtained speed-up of 20 for the simulation of the buoyancy driven flow in a bubble column with 105 bubbles, using 32 processors. The disadvantages of this method are associated with large memory requirements for holding all particle information. Kafui et al. [10] employed the particle subset method, where the particles are divided among the processors based on a graph partitioning algorithm. In this algorithm, a graph of particles connected by contacts is developed and the particles in close proximity are assigned to a single processor. They estimated the performance for the bubbling bed with 5.0×104 particles and reported a speed-up of 35 for 64 processors. Plimpton [7] concluded that particle decomposition and force decomposition are not optimal for large simulations due to high communication costs associated with the synchronization of data.

Recent advances in multithreading-type architectures encourage the development of DEM codes based on the above discussed methods for shared-memory parallel computers, particularly, to avoid the memory access conflicts. Attempts to perform straightforward parallelization of DEM codes by using OpenMP have not resulted in very high parallel efficiency and scalability. Frenning [11] obtained a speed-up of 3.75, simulating 700 particles only on 4 cores. Renouf et al. [12] parallelized the NSCD algorithm and attained a speed-up of 11 with 16 cores of SGI Origin 3800, simulating 1016 polydisperse disks. Shigeto and Sakai [13] performed exhaustive investigation and reported a speed-up of 5.4 measured in performing single-precision floating-point computations of 3.2×105 particles on Intel Xeon W5590 CPU with 16 logical cores. It is evident that the shared-memory configuration is limited to the number of processors that can efficiently access the common memory.

On the other hand, it is increasingly being found that graphics processing units (GPUs) have a much better performance to price ratio than architectures based on supercomputers and PC clusters. Radeke et al. [14] investigated the influence of the particle size on mixing statistics, modelling more than two million particles per Giga Byte of GPU memory. Shigeto and Sakai [13] also conducted a parallel DEM simulation on GPUs. Nishiura and Sakaguchi [15] developed several novel algorithms for shared-memory concurrent computation of particle simulations and measured their efficiency and scalability on various shared-memory architectures, including GPUs. He could observe the best performance, simulating up to 1.0×106 particles on the vector processor SX-9/E, but the quite close performance was attained on GPU Tesla C1060. Xu et al. [16] achieved quasi-real-time simulation of an industrial rotating drum, when about 9.6×106 particles were treated with 270 GPUs. In general, the performance of DEM simulation on a GPU was shown to be several dozen times faster than that on a single-thread CPU.

In the third class of methods, the domain decomposition [7] is employed. The basic idea of this technique is the partitioning of the computational domain into subdomains, each being assigned to a processor. Two basic types, static and dynamic domain decomposition strategies, are extensively used in solving time-dependent granular flows. Static domain decomposition works by assuming the fixed interdomain boundaries. Jabbarzadeh et al. [17] applied the parallel link-cells method based on the static domain decomposition to simulation of the short-range interaction of branched molecules. A maximum speed-up of 8.5 was reached by using 16 processors, simulating more than 51,864 molecules. Maknickas et al. [18] employed static domain decomposition with MPI in the parallelization of their DEM code. Static load balancing among the processors was enforced by employing regular partitions with nearly equal numbers of particles. Extensive use of local data structures kept interprocessor communications to a minimum. For 16 processors, a speed-up of 11 and the efficiency of 0.7 were obtained, simulating a 1.0×105 particle system. The effect of the material polydispersity on the performance of the static domain decomposition is presented in [19]. The analysis of tri-axial compaction with nearly 1.0×105 heterogeneous particles showed a speed-up of 8.81 for 10 processors. Parallel DEM computations on gLite grid infrastructure revealed similar speed-up [20]. A bubbling fluidized bed with 2.5×105 particles was simulated by the open source code MFIX based on the static domain decomposition [21]. The DEM part of the solver showed reasonable scalability up to 256 processors with an efficiency of 0.8. However, the dynamically changing workload configuration may lead to load imbalance and low parallel efficiency.

In the case of the hopper discharge problem, processor workload, interprocessor communication, and data storage requirements undergo continuous evolution during the simulation. More flexible, but more complicated dynamic domain decomposition is one of the solutions to this load balance problem that will allow higher scalability in parallel computing performance [22].

In the case of DEM, Owen and Feng [23] used a topological dynamic domain decomposition method based on a dynamic graph repartitioning. However, the parallel speed-up, equal to 4.41 or 5, was measured only on 6 processors of the shared-memory machine SGI Origin 2000. Zhang et al. [24] proposed a fast adaptive balancing method for particle-based simulations, in which a binary tree structure was used to partition the simulation region into subdomains. A higher efficiency of load balancing is obtained, adjusting the balance among the hierarchically grouped domains by compressing and stretching the cells in a group. Fleissner and Eberhard [25] applied an orthogonal recursive bisection of the simulation domain for recursive particle grouping and assignment to parallel processors. The parallel speed-up of 10.2 was achieved by using 16 nodes and simulating 1.0×106 of particles.

Walther and Sbalzarini [26] presented large-scale parallel simulations of granular flow, using novel, portable DEM software, employing adaptive domain decomposition based on a multilevel k-way graph partitioning [27] and load balancing techniques. Parallel simulations of 122 million frictional, viscoelastic particles revealed speed-up of 76.8 on up to 192 processors of a Cray XT3 machine. Wang et al. [28] adopted a dual-level scheme to implement the dynamic domain repartitioning based on the recursive coordinate bisection (RCB) method [29]. However, rather low speed-up of 4.84 was measured, simulating the hopper filling process on 8 processors of the PC cluster.

A simple iterative algorithm based on moving planes was applied to balance the workload of parallel hopper discharge simulations [30]. The performed investigation showed that the workload was dependent on the granular flow character. Moreover, the load balance was significantly influenced by the number of the contacting particles. A speed-up of 34 was measured, simulating 0.3×106 particles on 48 cores. However, the scalability of the code was limited by the inflexible decomposition topology, leading to communication overhead.

Since the degree of natural algorithmic concurrency inherent in explicit time integration procedures is high, the dynamic domain decomposition can yield high speed-up on different hardware configurations. However, the choice of a proper domain decomposition strategy, a particular implementation and computational resources for the hopper discharge problem still present a challenge to engineers and researchers, employing DEM simulations. Nevertheless, since particles move freely across subdomains, the problems remain in internode communication, even when the dynamic domain decomposition is applied.

The motion of a visco-elastic particle system is governed by the Newton’s second law, i.e.
                        
                           (1)
                           
                              
                                 
                                    m
                                 
                                 
                                    i
                                 
                              
                              
                                 
                                    
                                       
                                          d
                                       
                                       
                                          2
                                       
                                    
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                 
                                 
                                    
                                       
                                          dt
                                       
                                       
                                          2
                                       
                                    
                                 
                              
                              =
                              
                                 
                                    F
                                 
                                 
                                    i
                                 
                              
                              ,
                           
                        
                     
                     
                        
                           (2)
                           
                              
                                 
                                    I
                                 
                                 
                                    i
                                 
                              
                              
                                 
                                    
                                       
                                          d
                                       
                                       
                                          2
                                       
                                    
                                    
                                       
                                          θ
                                       
                                       
                                          i
                                       
                                    
                                 
                                 
                                    
                                       
                                          dt
                                       
                                       
                                          2
                                       
                                    
                                 
                              
                              =
                              
                                 
                                    T
                                 
                                 
                                    i
                                 
                              
                              ,
                           
                        
                     where mi
                      and Ii
                      are the mass and moment of inertia of the particle, respectively, while vectors 
                        x
                        i
                      and 
                        θ
                        i
                      initiate the position of the particle centre and the orientation of the particle i. The vectors 
                        F
                        i
                      and 
                        T
                        i
                      present the sum of the contact and gravity forces, as well as the respective torques:
                        
                           (3)
                           
                              
                                 
                                    F
                                 
                                 
                                    i
                                 
                              
                              =
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       j
                                       =
                                       1
                                       ,
                                       j
                                       ≠
                                       i
                                    
                                    
                                       N
                                    
                                 
                              
                              
                                 
                                    F
                                 
                                 
                                    ij
                                    ,
                                    cont
                                 
                              
                              +
                              
                                 
                                    m
                                 
                                 
                                    i
                                 
                              
                              g
                              ,
                           
                        
                     
                     
                        
                           (4)
                           
                              
                                 
                                    T
                                 
                                 
                                    i
                                 
                              
                              =
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       j
                                       =
                                       1
                                       ,
                                       j
                                       ≠
                                       i
                                    
                                    
                                       N
                                    
                                 
                              
                              
                                 
                                    T
                                 
                                 
                                    ij
                                 
                              
                              =
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       j
                                       =
                                       1
                                       ,
                                       j
                                       ≠
                                       i
                                    
                                    
                                       N
                                    
                                 
                              
                              
                                 
                                    d
                                 
                                 
                                    cij
                                 
                              
                              ×
                              
                                 
                                    F
                                 
                                 
                                    ij
                                    ,
                                    cont
                                 
                              
                              ,
                           
                        
                     where 
                        g
                      is the acceleration due to the gravity, 
                        d
                        ij
                      is the vector, pointing from the particle centre to the contact centre. The interparticle force vector 
                        F
                        ij,cont
                     , describing the contact between the particles i and j, may be expressed in terms of normal and tangential components 
                        F
                        n,ij
                      and 
                        F
                        t,ij
                     , respectively. The normal component 
                        F
                        n,ij
                     , presenting a repulsion force, comprises elastic and viscous ingredients. The tangential component 
                        F
                        t,ij
                      reflects static or dynamic frictional behaviour. The static force describes friction prior to gross sliding and comprises elastic and viscous ingredients, while the dynamic force describes friction after gross sliding and is expressed by the Coulomb’s law.

For evaluating the contact forces (3–4), all contacts between the particles and their neighbours should be detected. A cell-based method is used for contact detection [18]. A three-dimensional domain of the granular medium is divided into the cubic cells of the size slightly larger than the diameter of the largest particle. Then, the search for contact is performed only between the particles in the neighbouring cells. The numerical integration of the equations of motion (1–2) is performed to obtain the dynamical state of all particles in time t, resulting from the action of the particle forces (3–4). These equations are solved by using the Verlet scheme.

The dynamic domain decomposition methods have been implemented in the DEMMMAT_PAR code [19] developed in Vilnius Gediminas Technical University. The use of an efficient parallel algorithm is of major importance, because many particles and time steps are required to solve real-scale problems. Moreover, domain decomposition plays a crucial role in gaining satisfactory parallel performance in the case of the rapidly changing workload configuration.

The parallel algorithm (Fig. 1
                        ) is designed as follows. At the initial stage, particles are generated by the master processor. Then, the initial domain decomposition is performed and particles are distributed among processors by using MPI communication routines. Further, the calculations are performed in a time loop. The particles are advanced to new positions. The processors exchange particles with all their information if they move from one subdomain to another. This optional interprocessor communication is not required for an alternative algorithm, where the particle decomposition strategy [10] is used instead of the space domain approach employed in the presented research.

Another DEM procedure consuming a lot of CPU time is contact detection required for computation of contact forces. In sequential computations of the investigated hopper discharge, the contact detection and the calculation of contact forces takes from 80% to 90% of the computing time, while the time integration consumes from 10% to 20% of the total time. It is worth noting that the percentage of consumed time depends on the number of contacting particles in the different stages of hopper discharge [30]. The parallel execution of contact detection also requires interprocessor communication. The exchange of particle data from ghost regions near the subdomain boundaries is performed between the neighbouring subdomains. There is no need to send all particle information, but data transfer required for computation of contact forces is obligatory. In our implementation, each processor computes the forces for particles located in the processor subdomain, while extra particles, residing in ghost regions, are stored only for data required for these computations. An alternative solution based on the symmetry of contact forces would be to calculate the contact forces for the ghost particles and to send the obtained values to the neighbouring subdomains [25]. The current time step is finished by the condition, which redirects the program flow to the end of computations if the end of the investigated time interval is reached.

In the problems commonly solved by DEM, particles move through the whole computational domain, dramatically changing the initial workload configuration and causing the significant load imbalance. It is obvious that the dynamic load balancing becomes necessary for solving such complex problems as hopper discharge. The dynamic load balancing based on the domain repartitioning is performed in the following modules. The time tcalc
                         taken by the process in the computational part of the algorithm is considered to be a workload for each process. The master node receives the workloads from all processes and evaluates the load balance. The repartitioning is triggered if a processor satisfies the following condition:
                           
                              (5)
                              
                                 
                                    
                                       
                                          
                                             
                                                t
                                             
                                             
                                                calcIni
                                             
                                          
                                          -
                                          
                                             
                                                t
                                             
                                             
                                                calc
                                             
                                          
                                       
                                    
                                 
                                 >
                                 
                                    
                                       k
                                    
                                    
                                       repart
                                    
                                 
                                 
                                 
                                    
                                       t
                                    
                                    
                                       calcIni
                                    
                                 
                                 ,
                              
                           
                        where tcalcIni
                         is the initial computing time, measured during the first time steps after the last domain decomposition, while krepart
                         is the user-defined coefficient. In this research, krepart
                        
                        =0.05 is used. If the workload deviates from the initial value by more than 5% for any process, the domain repartitioning is performed. The small values of krepart
                         (close to 1%) cause frequent repartitioning, which consumes inappropriately large amount of computational resources. Moreover, the graph partitioning methods tolerate a certain load imbalance trying to reduce interprocessor communication, therefore, small values of the parameter have no sense. The large values of the parameter (exceeding 10%) lead to load imbalance, which can reduce the parallel speed-up of computations. In the current work, two domain repartitioning methods described below are applied to perform quantitative comparison of the attained parallel performance.

The first method employed for domain repartitioning is the recursive coordinate bisection (RCB), which was first proposed as a static load-balancing algorithm by Berger and Bokhari [29]. Fig. 2
                         illustrates the application of the RCB method to decomposition of a two-dimensional domain. This approach is attractive as a dynamic load-balancing algorithm because it implicitly produces incremental partitions. The computational domain is divided into two regions by a cutting plane orthogonal to one of the coordinate axes so that half the workload is in each of the regions. The method considers the geometric locations of the particles, determines in which coordinate direction the region is most elongated, and then divides the region by splitting it in that direction. The subregions are then further divided by recursive application of the same splitting algorithm until the number of subregions equals the number of processors. Any number of equally-sized sets can be created, appropriately adjusting the partition sizes. Sets of non-uniform sizes can be easily generated for non-homogeneous parallel machines, implementing the RCB available in Zoltan library [31]. In the current work, the weight of each particle Wparticle
                         is calculated by the formula:
                           
                              (6)
                              
                                 
                                    
                                       W
                                    
                                    
                                       particle
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             t
                                          
                                          
                                             calc
                                          
                                       
                                    
                                    
                                       
                                          
                                             n
                                          
                                          
                                             onProc
                                          
                                       
                                    
                                 
                                 ,
                              
                           
                        where nonProc
                         is the number of particles on the considered processor.

The repartitioning is performed by the RCB method in the following steps:
                           
                              •
                              Computing the weights of particles by using formula (6).

Partitioning by RCB subroutines of Zoltan library.

Obtaining new domain limits and domain neighbours.

Exchanging the particles.

In the case of static domain decomposition, or even in the simple dynamic domain decomposition based on the moving planes [30], the domain neighbours are known before the time loop of the simulation is entered (Fig. 1). In the case of the RCB application, the domain neighbours and the number of neighbours can change during repartitioning, therefore, an adaptation of data structures is required each time repartitioning is performed. Finally, it makes the parallel code more complex. It is worth noting that the RCB method does not explicitly optimize interprocessor communication. The resulting quality of domain decomposition may vary on the problem in consideration, or even on the time moment, when repartitioning is performed. Consequently, simulation of the whole time interval of a physical process is preferable because the initial steps of the hopper discharge do not reveal all aspects of the rapidly changing workload configuration.

The second strategy used for domain decomposition is based on the multilevel k-way graph partitioning available in ParMETIS library [27]. ParMETIS routines take a graph and compute the k-way partitioning, where k is equal to the number of subdomains desired. Moreover, ParMETIS attempts to minimize the number of edges that are cut by partitioning, which can be very important for minimizing communication among processors. The efficient routine ParMETIS_V3_PartKway makes no assumptions on how the graph is initially distributed among the processors. However, rapidly changing workload configuration of hopper discharge simulation requires periodic repartitioning, which minimizes both the interprocessor communication occurring in computation and the data redistribution costs required to balance the load. ParMETIS provides the routine ParMETIS_V3_AdaptiveRepart for repartitioning of poorly balanced decompositions. ParMETIS_V3_AdaptiveRepart is parallel implementation of the unified repartitioning algorithm that combines the best characteristics of remapping and diffusion-based repartitioning schemes. However, since parallel performance can vary significantly among different types of applications, it can be difficult to select the best repartitioning algorithm and to assign the appropriate values of parameters to the job.

Domain repartitioning (Fig. 1) starts from the preparation of data structures. It is worth mentioning that this step is more complex in the case of the multilevel k-way graph partitioning method. RCB needs only the coordinates of particles, while the ParMETIS library needs a graph. In the presented research, the space is partitioned into cells employed for the neighbours search. The particles are assigned to cells, while the required graph is assembled from vertices according to the grid of cells. The weight of the graph vertex Wvertex
                         is computed according to the number of particles in the considered cell npartCell
                         and the weight of particles:
                           
                              (7)
                              
                                 
                                    
                                       W
                                    
                                    
                                       vertex
                                    
                                 
                                 =
                                 
                                    
                                       W
                                    
                                    
                                       particle
                                    
                                 
                                 ·
                                 
                                    
                                       n
                                    
                                    
                                       partCell
                                    
                                 
                                 .
                              
                           
                        
                     


                        Fig. 3
                         illustrates the construction of the graph. One of the difficulties is associated with empty cells. For such cell the weight of vertex is equal to zero. There may be a lot of such empty cells, because, in many DEM problems, particles are located only in a small part of the computational domain. ParMETIS assigns such vertices to only one subdomain during partitioning. As a result, the subsequent repartitioning is performed longer on the processor with such subdomain. Another problem is that even for empty cells there may be required some additional memory allocations and computations in the computational part of DEM program if cell-based algorithms are used for contact detection. The third problem is associated with a condition, when the domain boundary is on the edge of the occupied cells. In this case, the exchange of the particles from ghost cells should be performed anyway. This may increase the amount of communications considerably. In the presented research, empty cells have been removed from the constructed graph. Initially, the particles are assigned to cells (Fig. 3a). The empty cells are removed to save computational resources (Fig. 3b). Finally, the graph is assembled assigning the vertices to the non-empty cells (Fig. 3c). The drawback of this solution is the complexity of the algorithm and some additional comp utational time required for each repartitioning.

The repartitioning is performed by ParMETIS in the following steps:
                           
                              •
                              Constructing the graph:
                                    
                                       –
                                       compute the weights of graph vertices by using formula (7),

exchange the IDs of the neighbouring vertices between the subdomains,

fill the arrays describing the graph.

Partitioning by using ParMETIS subroutines.

Obtaining the domain neighbours and marking the ghost cells.

Exchanging the particles.

A maximal matching heuristic is used to map the numbers of partitions for reducing the particles’ redistribution cost. The results obtained by using both domain repartitioning methods are compared and presented in the following section.

The main hopper discharge computations and parallel speed-up measurements were performed on HECTOR computer of EPCC in Edinburgh. The main HECTOR facility, known as phase 3, is a Cray XE6 system. Each node contains two AMD 2.3GHz 16-core processors. There is presently 32GB of the main memory available per node, which is shared among its thirty-two cores. The processors are connected with a high-bandwidth interconnect, using Cray Gemini communication chips. The code development and initial computations were performed on the computer cluster VILKAS and the OpenStack cloud infrastructure of Vilnius Gediminas Technical University.

The hopper discharge actually means the flow of the particles and their falling from the hopper due to the opening of the orifice. The computational domain of the hopper containing 300,000 particles is shown in Fig. 4
                        .

The rigid container walls are considered to be fixed frictional boundaries. The dimensions of the orifice are 36×36mm. Granular material is modelled as the assemblies of non-cohesive spherical particles N
                        =300,000 and N
                        =5,098,402 generated with uniform distribution. The particle radii Ri
                         vary over the range from 1.6 to 1.8mm and 0.63 to 0.69 in the case of N
                        =300,000 and N
                        =5,098,402, respectively. The total volume V of the material is equal, in both cases, to V
                        =7.75×10−4
                        m3. The elasticity modulus of the particle material is equal to E
                        =1×106
                        Pa, while the restitution coefficient is 0.5. Interparticle friction is characterized by the friction coefficient μ
                        =0.4. The material parameters for particle–wall interactions are assumed to be the same as those used for describing the interparticle relations. The initial state of the particulate material was generated numerically by simulating the process of filling. The state of the hopper 0.7s after the opening of the orifice is shown in Fig. 4. Initially, the particles were coloured, depending on the altitude. The figure allows us to follow the entire particles’ flow structure and their interlayer migration as well as detecting a zone of intense mixing.

The parallel performance of the developed code was evaluated by measuring the speed-up Sp
                         and the efficiency Ep
                        :
                           
                              (8)
                              
                                 
                                    
                                       S
                                    
                                    
                                       p
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             t
                                          
                                          
                                             1
                                          
                                       
                                    
                                    
                                       
                                          
                                             t
                                          
                                          
                                             p
                                          
                                       
                                    
                                 
                                 ,
                                 
                                 
                                    
                                       E
                                    
                                    
                                       p
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             S
                                          
                                          
                                             p
                                          
                                       
                                    
                                    
                                       p
                                    
                                 
                                 ,
                              
                           
                        where t
                        1 is the program execution time for a single processor; tp
                         is the wall clock time for a given job to execute on p processors.


                        Fig. 5
                         shows the quantitative comparison of measured speed-ups, assuming the linear scaling between 1 and 16 cores, when the dynamic domain decomposition of the hopper containing 300,000 particles was performed. The speed-up obtained by using the multilevel k-way graph partitioning method (where the curve KGR represents the usage of ParMETIS_V3_PartKway) is compared to that attained by using the recursive coordinate bisection method (the curve RCB) implemented in the Zoltan library. The special curve “Ideal” illustrates the ideal speed-up. The speed-up obtained by using RCB was almost linear up to 64 processes employed. A small reduction of the speed-up owing to communication overhead was obtained for a larger number of processes. Assuming the linear scaling between 1 and 16 cores, the speed-up equal to 110 was attained by using the multilevel k-way graph partitioning method of ParMETIS on 128 cores. More precisely, the speed-up was equal to 6.9 relative to the computing time measured on 16 cores. However, a higher speed-up equal to 117 (7.3 comparing to 16 cores) was attained by using the RCB method on the same number of cores. Thus, the results presented in Fig. 5 shows that RCB outperforms ParMETIS_V3_PartKway in the case of hopper discharge flow of 300,000 particles.

The quantitative comparison of the results obtained by using different repartitioning methods is performed to find the most efficient domain decomposition method in the case of a highly changing workload configuration of hopper discharge. Particularly, the detailed investigation of load balance, interprocessor communication and repartitioning frequency is presented in the case of 32 processes employed. Fig. 6
                         shows time variation of the relative load. The maximal (minimal) curve of the relative load is obtained by selecting the maximal (minimal) simulation time from the results of 32 processors and dividing it by the averaged load. The time consumed for repartitioning and data transfer is not considered in this figure. The curves RCB, AGR and KGR represent the relative loads obtained by using the RCB, ParMETIS_V3_AdaptiveRepart and ParMETIS_V3_PartKway, respectively.

It can be observed that repartitioning performed by the RCB method is able to maintain the maximal load and the minimal load close to the prescribed limit of the allowable load imbalance, which is equal to 5%. A larger load imbalance can be observed in the cases of rapidly changing workload configuration (for example, t
                        =0.1s or t
                        =0.75s). However, it is more difficult to maintain the prescribed limit by using both methods implemented in the ParMETIS library (the curves KGR and AGR). ParMETIS_V3_AdaptiveRepart produced partitions that caused the most poorly balanced workload, while it saved some interprocessor communication time, performing the domain repartitioning. However, in the case of hopper discharge flow, a poorly balanced workload caused a significantly longer simulation time. It is worth noting that the workload depends on the number of the contacting particles [30]. Thus, the total workload decreases due to the increasing number of particles discharged from the hopper.


                        Fig. 7
                         shows a time history of invocation of repartitioning procedures according to the same criterion defined by formula (5). It can be easily observed that the application of the RCB method (the curve RCB) rarely requires the invocation of repartitioning, because it resulted in a well-balanced workload. The subroutine ParMETIS_V3_AdaptiveRepart (the curve AGR) was invoked less frequently than the subroutine ParMETIS_V3_PartKway (the curve KGR). However, after the short period of time (about 0.2s) the quality of some resulting partitions started to decrease, causing the increase of load imbalance and the computing time (Fig. 6). In general, nearly linear dependency of the invocation number of repartitioning on time was observed after 0.3s.


                        Fig. 8
                         presents a time history of the average data transfer measured by solving the hopper discharge problem on 32 processors. After a short period of time (approximately equal to 0.2s) the domain partitions performed by ParMETIS_V3_AdaptiveRepart (the curve AGR) degenerated and produced more intense interprocessor communication than the domain partitions performed by ParMETIS_V3_PartKway (the curve KGR). Moreover, the observed difference increased in time. In the most cases, data transfer among the processes measured by employing domain repartitioning of the RCB method (the curve RCB) is significantly lower. The observed phenomenon is quite unexpected because ParMETIS is designed to reduce interprocessor communication.

The only large increase of data transfer observed in the results of RCB at 0.6s was caused by inappropriate domain partitioning (Fig. 9
                        a). It is obvious that RCB unsuccessfully partitioned the subdomain, occupied by the particles just discharged from the hopper, in two perpendicular directions. Moreover, the narrow layer of particles, lying on the bottom of the computational domain, was produced by unsuccessful partitioning. It is evident that areas of common boundaries of neighbouring subdomains were inappropriately large, which produced the high communication overhead observed in Fig. 8. It seems that RCB experienced difficulties coupled with this domain configuration. In this particular case, the subroutine ParMETIS_V3_PartKway generated better suited partitions (Fig. 9b), not requiring very intense interprocessor communication.

In general, the results obtained in parallel performance and, particularly, interprocessor communication differ considerably from the expected results. It can be explained by the fact that the geometry of the hopper is enough simple to handle the load balance with the RCB method and to get reasonable cost of data transfer without direct attempts to minimize interprocessor communication. The graph partitioning methods tolerate a certain load imbalance trying to minimize edge-cuts and to reduce interprocessor communication. However, in the case of hopper discharge, topology of connections between neighbouring particles and that of the relevant graph edges can considerably change during the short time interval. Thus, the investigated multilevel k-way graph partitioning methods were not able to keep data transfer at the expected level (Fig. 8). Recently, the repartitioning hypergraph model [31] has been developed to adapt multilevel graph partitioning techniques to the needs of dynamic load balancing and to account for both interprocessor communication and data migration cost. Quantitative comparison of the speed-ups attained employing three different methods might be an interesting direction for the future research.

The detailed investigation of load balancing and interprocessor communication presented in two previous subsections shows that the RCB method outperformed the multilevel k-way graph partitioning in the case of the considered hopper discharge simulation. Thus, RCB-based repartitioning was applied to domain decomposition of the hopper filled with 5,098,402 particles. Parallel computations of a larger benchmark were performed by using 128–2048 cores. The dependency of the computing time on the number of the employed cores is presented in Fig. 10
                        . The curve shows the reduction of the computing time increasing the number of employed cores, when the RCB method for repartitioning was used. The significant reduction of the computing time could be observed in using a large number of cores.


                        Fig. 11
                         shows the contribution of computation (Comput), interprocessor communication (Comm) and repartitioning (Repart) to the total simulation time of a larger benchmark. The columns RCB, AGR and KGR represent the relative loads obtained by using the RCB, ParMETIS_V3_AdaptiveRepart and ParMETIS_V3_PartKway, respectively. Repartitioning performed by the RCB method consumed a reasonable amount of time, not exceeding 5.4% of the total benchmark time measured in the case of 2048 cores. Repartitioning performed by KGR took 9.4% of the total execution time on 512 cores. The AGR method performed repartitioning slightly faster than the KGR method due to reduced particle data exchange among processors in the last repartitioning stage. However, degradation of partitions produced by AGR caused more intense interprocessor communication and load imbalance, which resulted in even longer computing time. The performance attained employing the RCB method was significantly higher that achieved by using the KGR method and the AGR method, therefore, computations on 1024 and 2048 cores were performed by using only the RCB method. It also demonstrated a satisfactory ratio of communications to computations, which was equal to 27% for 2048 cores employed. Interprocessor communications consumed from 4.3% (128 cores) to 20.1% (2048 cores) of the total benchmark time.

Assuming the linear scaling between 1 and 128 cores, the parallel speed-up attained by simulating the hopper containing 5 millions of particles is presented in Fig. 12
                        . In spite of the large number of the processors employed, the software demonstrated good parallel performance. When the number of processors did not exceed 512, the linear speed-up was gained relative to a run of 128 processes. A slight reduction of the efficiency, owing to communication overhead, is obtained for a larger number of processors. Assuming the linear scaling between 1 and 128 cores, the speed-up equal to 1785 was attained on 2048 cores. More precisely, the speed-up of 13.9 was gained relatively to a run of 128 processes, which is equivalent to the high parallel efficiency of 0.87. The presented results show that the implemented dynamic domain decomposition and load balancing based on the RCB method is well designed for simulating the hopper discharge problem.

@&#CONCLUSIONS@&#

In this paper, the development of parallel DEM software and its application to the simulation of hopper discharge flow is described. Two alternative methods are employed for domain repartitioning and load balancing. Quantitative comparison of parallel performance achieved by using the multilevel k-way graph partitioning method and the recursive coordinate bisection method demonstrates the benefits and drawbacks of their application to the rapidly changing workload configuration of the hopper discharge simulation. It can be observed that more complex adaptation of the k-way graph partitioning method to DEM computations does not result in higher parallel performance. A higher speed-up and lower load imbalance are measured, applying the recursive coordinate bisection method to the simulation of the hopper, filled with 0.3×106 spherical particles. High parallel efficiency of 0.87 was attained by simulating the hopper filled with 5.1×106 particles on 2048 cores. Domain repartitioning based on the recursive coordinate bisection method took only 5.4% of the total simulation time in the case of 2048 cores employed. The parallel performance of the developed DEM software can compete with the results obtained and reported by other researchers.

@&#ACKNOWLEDGEMENTS@&#

This work was performed under the HPC-EUROPA2 project (project number: 228398) with the support of the European Commission Capacities Area – Research Infrastructures Initiative. The researcher made use of the facilities of HECToR, the UK’s national high-performance computing service, which is provided by UoE HPCx Ltd at the University of Edinburgh, Cray Inc and NAG Ltd, and funded by the Office of Science and Technology through EPSRC’s High End Computing Programme. The employed cloud infrastructure of Vilnius Gediminas Technical University was supported by the Ministry of Education and Science of the Republic of Lithuania through the VP1-3.1-ŠMM-08-K programme, project “Research and development of technologies for virtualization, visualization and security e-services”.

@&#REFERENCES@&#

