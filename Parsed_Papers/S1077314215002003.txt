@&#MAIN-TITLE@&#An efficient feature descriptor based on synthetic basis functions and uniqueness matching strategy

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           SYBA is built on the basis of the compressed sensing theory.


                        
                        
                           
                           The descriptor is robust, simple and computationally efficient.


                        
                        
                           
                           Evaluated the descriptor performance statistically on BYU feature matching dataset.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Feature detection

Feature descriptor

Synthetic basis functions

Feature matching

@&#ABSTRACT@&#


               Graphical abstract
               
                  
                     
                        
                           Image, graphical abstract
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

Computer vision applications often involve computationally-intensive tasks such as target tracking [1–3], object identification [4,5], image rectification, localization, pose estimation [1,6–9], optical flow [10,11] and many more. The initial steps of all these applications are the detection, description, and matching of high quality feature points, with feature description and matching being the most challenging and time-consuming processes. They focus on computing abstractions of image information that are associated with the points of interest detected by the feature detector.

There exist a large number of feature descriptors [12–28], but not every one of them is suitable for hardware implementation for real-time applications. An efficient feature descriptor for real-time applications should not be too computationally complex. To be hardware-friendly, it should not use many square root, division, or exponential operations that require floating-point computations. A good feature descriptor is able to describe a feature point and measure its similarity to other feature points. It allows a feature point to be correctly identified and matched to a feature point in another image that has similar characteristics. A well-known orientation and magnitudes-of-intensity gradient-based feature descriptor is Scale Invariant Feature Transform (SIFT) [12]. It works well on intensity images and provides descriptors that are invariant to rotation and scaling. However, increased complexity and robustness come with the increase in computation and storage requirements and make it not suitable for many resource-limited platforms and real-time applications. Another well-known descriptor, Speeded-UP Robust Features (SURF) computes descriptors using integral images and 2-D Haar Wavelet transform [13,14].  A minor drawback is that it requires 256 bytes to encode 64 floating-point values.

Ke and Sukthankar [15] developed a descriptor and applied dimensional reduction technique, Principal Component Analysis (PCA), to the normalized image gradient patch. PCA-SIFT performs better than SIFT descriptor on artificially generated data. At the same time, it has the benefit of reducing high frequency noise in the descriptors. The drawback is that it is not tuned to obtain a sub-space that will be discriminative for matching [16]. Another, dimensional reduction technique Linear Discriminant Embedding (LDE) was developed by Hua et al. [16]. In order to perform well, LDE requires labeled training data, which are difficult to obtain. They are both suitable for feature description and require smaller descriptor size than many well-known approaches.
                  

Simonyan et al. [17] and Trzcinski et al. [18] developed descriptors based on training data. Both descriptors require complex operations and require computational resources that are not suitable for hardware implementations.  The accuracy of these descriptors may be affected when they are applied to applications that are completely different from the training dataset. Simonyan et al. proposed to use floating point and then convert it to binary which clearly causes the loss of matching accuracy [17]. Similarly, other descriptors [19–22] require time-consuming training process and complex operations. Even though some descriptors [17–21] use smaller descriptor size than SYBA, they are not suitable for low resource platforms. In recent years, new feature descriptors such as Binary Robust Independent Elementary Features (BRIEF) [23,24], Binary Robust Invariant Scalable Keypoints (BRISK) [25], and Aggregated LOcal HAar (ALOHA) [26] have been reported. These feature descriptors are all based on intensity comparisons. ALOHA is based on a set of Haar-like pixel patterns defined within an image patch. It performs intensity difference tests to encode the image patch into a binary string. ALOHA requires larger patch size and slightly fewer operations than BRIEF descriptor. Even with larger patch size the results are not robust to viewpoint changes and illumination changes. BRIEF descriptor trades reliability and robustness for processing speed, consisting of a binary string that contains the results of simple image intensity comparisons at random pre-determined pixel locations. BRISK relies on configurable circular sampling patterns from which it computes brightness comparisons to represent a binary descriptor. Overall, BRISK descriptor requires significantly more computation and slightly more storage space than BRIEF descriptor. Both of these algorithms use faster feature detectors and smaller descriptor size than SIFT and SURF. As another alternative to SIFT and SURF descriptor, Rublee et al. introduced a new version of BRIEF called rBRIEF descriptor [27]. It is based on a specific set of 256 learned pixel pairs selected for reducing correlation among the binary tests.

Feature description has been an active area of research in computer vision and machine learning. The main objective of this work is to develop a simple and hardware-friendly feature descriptor to reduce the computational power requirement and increase the speed and accuracy of feature matching. Our new descriptor is inspired by recent work in compressed sensing [29]. Compressed sensing theory is used to encode and decode a signal efficiently and reduces bandwidth and storage requirements. It is able to uniquely describe a signal with synthetic basis functions, which makes it a perfect theory for feature description.

To understand the theory, consider the popular game of Battleship, in which the best result can be obtained by using an adaptive strategy of counting the number of hits in recursively subdivided half-planes. The major drawbacks of this adaptive strategy are that it requires memory space to record and processing power to analyze all previous guesses and guess results in order to determine the next guess. Anderson developed a new compressed sensing algorithm based on this adaptive strategy, using synthetic basis functions instead of subdivided half-planes to minimize memory space requirement [29].

Using the battleship game as an analogy, the basic idea of using synthetic basis functions for compressed sensing is to use a random pattern (as shown in Fig. 1(b–d)) as a guess. The biggest advantage of using synthetic basis functions is that it does not require memory for storing previous guesses. Reducing memory space requirement makes using synthetic basis functions an excellent choice for feature description for resource-limited systems.

As an example, in Fig. 1(a) the orange squares represent battleship locations in a 15×15 area and the black squares in Fig. 1(b–d) represent the guessed locations of the battleships. The maximum number of different random patterns (or turns) that is required to locate all battleships using this non-adaptive approach is surprisingly the same as the original adaptive strategy approach (but with the benefit of significantly reduced memory space), which is given by Anderson [29]
                     
                        
                           (1)
                           
                              
                                 M
                                 =
                                 C
                                 
                                    (
                                    
                                       K
                                       
                                       l
                                       n
                                       
                                          N
                                          K
                                       
                                    
                                    )
                                 
                                 ,
                              
                           
                        
                     where N is the number of squares on the game board (n × n) and K is the number of queried battleship locations. C is round up number to the nearest integer. M is the number of random patterns (turns) required to locate all ships and is the smallest when K = N/2. This very small number of random patterns is sufficient to locate all battleships.


                     Fig. 1(a) shows that there are seven battleships in a 15 × 15 area. As shown in Fig. 1(e), out of seven battleships, six of them (squares in blue) are hit or guessed correctly because their locations coincide with six of the black squares in the random pattern (or turn) shown in Fig. 1(b). One ship is missed (square in orange) using the same pattern (turn) shown in Fig. 1(b) because its location coincides with a white square. Similarly, there are six and five battleships (squares in blue) are hit or guessed correctly using random patterns (or turns) shown in Fig. 1(c) and (d). Their results are shown in Fig. 1(f) and (g), respectively. According to Eq. 1, the number of random patterns or turns required to locate all battleships in a guessing game of this size (15×15), using the unique (non-repetitive) basis patterns (K = N/2), is 113 ln (225/113) or 78.

Inspired by this compressed sensing theory, we have developed a new descriptor algorithm using synthetic basis functions, called SYthetic BAsis (SYBA). It uses a number of randomly generated synthetic basis images (SBIs) as the guesses in a “battleship game” to measure the similarity between a small image region surrounding a detected feature point, called a feature region image (FRI), and the SBIs. The similarities between an FRI in the image and all SBIs are then used as the feature descriptor.

This work involves a unique way of measuring descriptor similarity in order to match similar features between two images. This unique way of measuring descriptor similarity is less complex than Mahalanobis and Euclidean methods. This work also includes a feature matching strategy that contains a two-pass search to enforce the uniqueness constraint and global minimum requirement to determine the best matching feature pairs. The new descriptor, called SYBA, is introduced in Section 2. Experimental results based on feature matching comparison with two widely used binary descriptors, BRIEF-32 and rBRIEF, are presented in Section 3. In Section 3, also includes the statistical T-test experiments on newly created dataset. Section 4 summaries the paper with discussion of the performance and ideas for future work.

Well-known binary descriptors are often used for benchmarking feature description performance. BRIEF descriptor compares the intensity of two randomly selected pixels and uses the intensity difference as a descriptor [23]. Rather than intensity difference, SYBA compares a feature region image with a number of synthetic basis images and uses the similarity measures as the feature descriptor. The creation of the synthetic basis images and the computation of the SYBA descriptor are the two major parts of this algorithm.

Synthetic basis images are sparse images. They differ from the basis dictionary images created from natural or man-made objects in our previous work [30,31]. All basis dictionary images created from natural or man-made objects are not always sparse. There are two major differences between the basis functions created from random numbers (i.e. synthetic) and basis images created from natural/man-made objects. The computation time for basis images created from natural and man-made objects require several hours, while basis images created using random numbers (i.e. synthetic) require at most a few seconds. Memory space required to create synthetic basis images is far less than basis images created from natural/man-made objects. These are the reasons the synthetic basis are much better.

The number of synthetic basis images (M) represents the number of “turns” as in the battleship game and is calculated according to Eq. (1). Of course, a larger number of synthetic basis images are required for a larger pixel region surrounding the detected feature points or feature region image (FRI). The maximum number of synthetic basis images required is 9 (when K = N/2) for a 5 × 5 FRI, whereas a 30 × 30 FRI requires 312 synthetic basis images.

Two examples of synthetic basis images are shown in Fig. 2
                        . One is 30 × 30 and the other is 5 × 5. Synthetic basis images similar to these two are used for SYBA descriptor calculation. The first step to creating a synthetic basis image is to determine its dimension (N = n×n). Once the dimension of the synthetic basis image is determined, K (=N/2) normally distributed pseudo random numbers are generated from [1, … , N] to represent the black squares in the synthetic basis image. Note that even for small SBIs (e.g., 5 × 5) that are generated in this manner, all SBIs in one set (M) will be uniquely represented (with a probability equals to 0.99999893), and thus no specially designed patterns are needed.

The main function of the SYBA descriptor is to “describe” the FRI of an image feature point in a unique way so that feature points between two images can be matched. SYBA descriptor does not require complex descriptor calculations and yet is able to provide good feature matching accuracy. SYBA descriptor algorithm is illustrated in Fig. 3.
                        
                     

The first step of the SYBA algorithm is to detect feature points and generate a feature list. Any feature detector can be used for this purpose. This work uses SURF as the feature detector because it is several times faster than SIFT [13]. For each feature on the feature list, its feature region is cropped and saved as a 30 × 30 FRI. The second step of the algorithm is to calculate the average intensity (g) of the FRI as

                           
                              (2)
                              
                                 
                                    g
                                    =
                                    
                                    
                                       
                                          
                                             ∑
                                             
                                                x
                                                ,
                                                y
                                             
                                          
                                          I
                                          
                                             (
                                             
                                                x
                                                ,
                                                y
                                             
                                             )
                                          
                                       
                                       p
                                    
                                    ,
                                 
                              
                           
                        where p is the number of pixels in the image (900 in this case), and I(x, y) is the intensity value at pixel location (x, y). A binary FRI is then generated based on the average intensity g. If I(x, y) is brighter than g, the binary FRI at the pixel location (x, y) is set to one, otherwise the value is set to zero. The last step of the algorithm is to calculate the similarity between the binary FRI and each of the SBIs in order to generate a descriptor for each binary FRI on the feature list.

A unique SYBA similarity measure (SSM) is developed to measure the similarity between the FRI and a selected number of SBIs. The result of SSM represents an accurate feature description because it takes into account the spatial and structural information of the feature region. The output of the SSM is then used to describe the feature point for feature matching as shown Fig. 3.
                     

For the experiments, SYBA with two different sizes was implemented. One was computed with SBIs of size 5 × 5 and named SYBA5 × 5. The maximum number of SBIs required for SYBA5 × 5 is 9 when half of the pixels (N = 25 and K = 13) are black. Fig. 4(
                        a) shows an example of 9 5 × 5 SBIs labeled from 1 to 9. The other size used for experiments was 30 × 30 and named SYBA30 × 30. The maximum number of SBIs required for SYBA30 × 30 is 312 when half of the pixels (N = 900 and K = 450) are black. Once the required SBIs are generated, they should not be changed in order to use the same patterns to test the next image.


                        Fig. 4 shows an example of how the SSM is calculated between a 30 × 30 FRI and SYBA5 × 5. The SSM between a 30 × 30 binary FRI and SYBA30 × 30 is be calculated in a similar manner. The first step of the SSM calculation is to divide the 30 × 30 binary FRI into 36 equal-sized 5 × 5 pixel subregions (as shown in Fig. 4(b)). The next step is to count how many pixels in the 5 × 5 subregion of the binary FRI are hit by each of the 9 SBIs in Fig. 4(a). Each of these 36 5 × 5 subregions is compared with each of the 9 5 × 5 SBIs and the number of times both contain a black pixel at the same location is counted as a hit.

The maximum possible number of hits for comparing a 5 × 5 subregion with a 5 × 5 SBI is 13 because there are only 13 (K = 25/2) black pixels in each SBI. For example, the highlighted 5 × 5 subregion shown in Fig. 4 (b) compared with SBI #1 has 5 hits (shown in Fig. 4(c)). The same subregion in Fig. 4(b) compared with the SBI #2 has 4 hits (shown in Fig. 4(d)). After comparing with all 9 SBIs, each subregion will yield 9 numbers ranging from 0 to 13. The number of hits in each subregion is stacked in the feature descriptor. Each subregion will use these 9 numbers as its feature descriptor. Therefore, a 30 × 30 FRI with 36 5 × 5 subregions will require a feature descriptor size of 36 (5 × 5 subregions) × 9 (5 × 5 SBIs) × 4 bits (0–13) = 1,296 bits or 162 bytes.

For the SYBA30 × 30 implementation, the entire 30 × 30 FRI is used to compare with 312 30 × 30-pixel SBIs. The maximum number of hits between the FRI and each SBI is 450 because there are only 450 (K = 900/2) black pixels in the entire 30 × 30 SBI. The resulting feature descriptor size for SYBA30 × 30 is 1(FRI 30 × 30 region) ×312 (30 × 30 SBIs) × 9 bits (0–450) = 2,808 bits or 351 bytes.

SYBA descriptor size can be easily adjusted by changing the sizes of SBI and FRI. A generalized approach that describes SYBA descriptor size is as follow. Choose the FRI dimension F first and then choose the SBI dimension S to be an integer factor Q of F so that S×Q = F. Note that M is a function of K and N (Eq. (1)), K is a function of N 
                        (
                        
                           K
                         = 
                           N
                        
                        /2)
                        , N divisible by S, and Q = F/S. These relationships allow complete parameterization of SYBA in terms of just F (the dimension of an FRI) and S (the dimension of an SBI). The SYBA descriptor size is Q×Q (# of subregions) × M (# of SBIs) × log2K bits.

Although the compressed sensing theory is able to uniquely represent a signal, the feature representation may not be unique due to the way the descriptor is calculated as shown in Fig. 4. We do lose some spatial uniqueness by only counting the number of “hits” and not tracking where the “hits” are like in the Battleship game. The tradeoff we make to simplify our descriptor calculation sacrifices (statistically) the uniqueness a little.

The SYBA descriptor is used to find the best matching feature points between two image frames. In this process, 324 (36 (5 × 5 subregions) × 9 (5 × 5 SBIs)) descriptor elements ranging from 0 to 13 are used as feature descriptors for SYBA5 × 5, and 312 (1 (30 × 30 region) × 312 (30 × 30 SBIs)) descriptor elements ranging from 0 to 450 are used as feature descriptors for SYBA30 × 30. To minimize computational complexity, for determining similarity we use the L1 norm rather than other common comparison metrics such as Euclidean or Mahalanobis distance, which require complex operations such as square and square root.

The L1 norm is computed as the sum of absolute differences:

                           
                              (3)
                              
                                 
                                    d
                                    =
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       n
                                    
                                    
                                       |
                                       
                                          
                                             x
                                             i
                                          
                                          −
                                          
                                             y
                                             i
                                          
                                       
                                       |
                                    
                                 
                              
                           
                        where, xi
                         is the score for region of the feature point in the first image, and yi
                         is the score for region of the feature point in the second image, n is total number of regions used in the basis comparison (324 for SYBA5 × 5 and 312 for SYBA30 × 30). The similarity between two features is represented by d and the smallest L1 norm (d) represents the best match of features between two images. Eq. (4) shows an example of SYBA descriptor calculation. Each row represents a feature descriptor. The d value for Eq. (4) is 5 between the two example feature descriptors.

                           
                              (4)
                              
                                 
                                    
                                 
                              
                           
                        
                     

To match the features, we first determine point-to-point correspondences using the similarity measure. We select each descriptor in the first image and compare it to all descriptors in the second image by calculating the d value as shown above. The remaining process is divided into two steps: (1) two-pass search, and (2) global minimum requirement. First we use a two-pass search to find feature pairs that uniquely match to each other. We then use a global minimum requirement to screen for possible good matching feature pairs from the remaining feature points.


                        1) Two-Pass Search:
                     

In this step, the first pass is to find the minimum distance d between one feature in the first image and all features in the second image. The feature that has the smallest distance in the second image is considered a match to the feature in the first image. The second pass it to confirm that the matched feature in the second image also has the shortest distance to its match in the first image. If the second pass fails to confirm the reciprocal of the shortest distance between the two, then they are not matched. They will remain on the feature list and to be tested in the second step. This two-pass search ensures a unique one-to-one match and eliminates any possible ambiguity. Because our aim is to find unique matching feature point pairs between two images, a feature that matches to two or more features that have the same shortest distance is not considered and will remain on the feature list. After the completion of the two-pass search, the matched feature pairs are excluded from any further matching processes. The remaining feature points in both images are then further tested in the second step.


                        Fig. 5
                         shows an example of the two-pass search. As shown in Fig. 5(a), there are 8 feature points in image-1 (vertical) and 7 feature points in image-2 (horizontal). The similarity between feature points in image-1 and feature points in image-2 is calculated using Eq. 3. The last (right) column shows the minimum d value of each row by comparing each feature point in image-1 with all of the feature points in image-2. For feature point-3 of image-1, there are two smallest distances of 3 in image-2: feature point-2 and feature point-3. This distance is shown as (3, 3) in the last column. Also for the feature point-7 in image-1, there are two equal d values (2, 2) for feature point-1 and feature point-3 in image-2. The row minimum d values are highlighted by horizontal black lines in Fig. 5(b). The last (bottom) row shows the minimum d value of each column. The minimum d value is found by comparing each feature point of image-2 with all feature points of image-1. The column minimum d values are highlighted by vertical black lines in Fig. 5(b). Feature points are considered uniquely matched if they have the mutually shortest distance. Four pairs of these mutual matches are highlighted in blue crossed lines in Fig. 5(b).

Because the aim is to find unique matching feature point pairs between two images in this step, any matches that have more than one smallest d value are not considered a match. Point 3 in image-1 and Point 2 in image-2 are not considered a match because Point 3 in image-1 and Point 3 in image-2 also have a minimum distance 3. Only a unique smallest d value in the same row or column can be called a matching pair. Three unique matching pairs between feature points in image-1 and feature points in image-2 are highlighted in blue in Fig. 5(c) using this two-pass search. Feature point numbers 1, 4, and 5 in image-1 match to feature point numbers 1, 5, and 3 in image-2, respectively. Since these feature points have been paired with their unique matches, they will not match to any other points. The rows and columns of these matched points in both images are highlighted in 45-degree oblique black lines and removed from further searches. The remaining unmatched feature points (not highlighted in Fig. 5(c)) will be sent to the second matching step.


                        2) Global minimum requirement:
                     

After the two-pass search is performed, global minimum requirement is applied to the remaining feature points. In this step, we find the minimum d values for all remaining feature points. For one-to-one matches between two images, the smallest unique d value is considered a match. This process repeats until all possible pairs are found. Any remaining feature points are without a matching point. Fig. 6
                         illustrate the process of applying global minimum requirement.

An example of this global minimum requirement applying to the remaining feature points from the two-pass search is shown in Fig. 6. Three global minima are found as shown in Fig. 6(a). Feature point-3 in image-1 is uniquely matched to feature point-2 in image-2. The row and column of this feature point are highlighted with blue rectangles and will not be considered for further search. The remaining possible matches are shown in Fig. 6(b). The next lowest distance in the remaining points is 5. There are three possible matching pairs with a distance 5 but only one is a unique match (Point 7 in image-1 to Point 7 in image-2). Again, the row and column of this matched point are highlighted with blue rectangles and are removed from further search. The remaining possible matches are shown in Fig. 6(c). The next lowest distance in the remaining points is 6, which matches Point 2 in image-1 to Point 6 in image-2. After row 2 and column 6 are removed from this search, the only possible matches are row 8 and column 4 as shown in Fig. 6(d). The same process can be performed until no minimum can be found. As shown in Figs. 5 and 6, seven matches are found. Of these 7 matches, 3 matches were found using the two-pass search and 4 matches were found using global minimum requirement. Note that a global minimum can be adjusted to terminate the search at any stage. A smaller global minimum will return fewer but better matches whereas a larger global minimum will return more but lower quality matches. The matching feature pairs of the example shown in Figs. 5 and 6 are listed in Table 1.
                        
                     

@&#EXPERIMENTS@&#

Four experiments were performed to validate SYBA's performance. First, matching accuracy of two versions of SYBA descriptor is compared with several common feature descriptors using our Idaho dataset [31]. The second experiment compares SYBA to two versions of BRIEF, SURF, and ALOHA descriptors (all are binary descriptors) to demonstrate the performance of the SYBA descriptor using the popular Oxford dataset. The third experiment was performed on multi-view stereo correspondence dataset to show the descriptor's performance on patched dataset [22]. The last experiment was performed on a newly created BYU feature matching dataset to statistically analyze the descriptor's performance.

The dataset used for testing was the Idaho dataset [31]. The Idaho dataset contains a total of 597 images. Fig. 7
                         shows two example images from the Idaho dataset. Idaho was created from real-world images taken from a downward-facing camera on an actual air flight. The images in the Idaho dataset were taken from a camera running at 30 frames per second, with 640 × 480 pixel resolutions. The Idaho test set features large blank areas of fields with few features, populated urban scenes, and natural features such as mountains and rivers. The images used for the dataset were obtained from video frames that were one second apart to allow noticeable camera movement.

To measure the performance of SYBA, we performed the same evaluation as that used on our previous Tree-BASIS algorithm [31]. That is, a homography was computed from feature descriptors matched between two images. Feature descriptor performance is measured by the percentage of correct homography computations. Table 2
                         shows the memory usage and homography accuracy of SIFT, SURF, two versions of BRIEF, and two versions of our BASIS, and two versions of the new SYBA. Only BRIEF-32 has comparable result to the proposed SYBA.

BRIEF is considered a well-known binary descriptor and proven to perform better than BRISK and many others in the literature. Most publications in binary descriptors use BRIEF's performance as a benchmark. Its implementation is readily available for comparison. It does not require off-line computation and training so its performance does not depend on the training dataset, which allows us to perform a more subjective comparison. In addition BRIEF and SYBA are both binary descriptors and both use randomly generated patterns. BRIEF-32 descriptor [23] requires fewer comparisons compared to BRISK and has been proven to outperform several other existing fast descriptors such as SURF (except on Graffiti sequence [23]) [13], U-SURF [13] and Compact Signature [32]. We compared SYBA with BRIEF-32 and rBRIEF, a new binary descriptor called ALOHA [26], and the popular SURF in this work.

Six commonly used image sequences [33] were tested for accuracy comparisons. These six image sequences were designed to test the robustness of feature descriptor with image perturbations such as blurring, lighting variation, viewpoint change, or image compression. These sequences (in parentheses) include the following (example images are shown in Fig. 8
                        ):

                           
                              •
                              Image compression artifacts - UBC JPEG test sequence (Fig. 8(a)),

Illumination change - Leuven Light test sequence (Fig. 8(b)),

Image blurring - Bikes test sequence (Fig. 8(c)) and Trees test sequence (Fig. 8(d)),

Viewpoint change - Wall test sequence (Fig. 8(e)) and Graffiti test sequence (Fig. 8(f)).

Each sequence consists of a 6 images. For our experiments, the first image in the sequence was used as the reference image. The subsequent 5 images were used as the tested images for matching. The image perturbations become more severe from one image to the next in the sequence. For example, matching feature points between the first and the third images is more challenging than matching feature points between the first and the second images. In this work, similar to the recognition rate in [23,26], the detection rate is defined as the ratio of the number of correct matches (Nc
                        ) to the total number of matches found (N).

Open source computer vision library (OpenCV) implementations of BRIEF and rBRIEF (ORB descriptor [27]) descriptors were used to compare feature descriptor performance. In these implementations, the region size was fixed to 48×48 for BRIEF and 31 × 31 for rBRIEF. To calculate the mean intensity, a 9 × 9 size region was used for BRIEF and a 5 × 5 size region was used for rBRIEF. As explained previously, two versions of SYBA were compared against BRIEF, rBRIEF, ALOHA, and SURF. In both SYBA versions, the feature region image size was kept at 30 × 30, whereas two different synthetic basis image sizes 5 × 5 and 30 × 30 were used for SYBA5 × 5 and SYBA30 × 30, respectively.
                     

Both BRIEF and rBRIEF descriptors use SURF to detect features, without any pyramidal analysis. The SURF feature detector was also used for SYBA in order to use the same feature points to compare their performance. The number of detected features ranged from 500 to 1500 depending on the image sequence. Fig. 9
                         illustrates the performance of two versions of SYBA and the other four methods. For this assessment the detection thresholds were set such that all outputs have a nearly equal number of correspondences. Both SYBA versions were more robust than BRIEF and rBRIEF for images that are corrupted by compression artifacts in the "UBC JPEG compression" dataset (Fig. 9(a)). SYBA30 × 30 outperformed BRIEF by more than 15% and ALOHA and SURF by more than 30% for image pair 1|6. For the "Leuven light" image dataset, which is corrupted by illumination change, the detection rate of SYBA30×30 is more than 3% higher than BRIEF and ALOHA and 25% higher than SURF for image pair 1|6 (Fig. 9(b)).
                     

For the "Bikes" and "Trees" sequences that are corrupted by image blurring, SYBA outperformed all other four algorithms. The accuracy difference was even more obvious for the strongest blurring conditions. For image pair 1|6, the differences were 7% for the "Bikes" sequence and 10% for the "Trees" sequence (Fig. 9(c)–(d)). For the "Wall" and "Graffiti" sequences which are corrupted by viewpoint change, BRIEF performed slightly better than SYBA only for image pair 1|5 in the "Wall" sequence and for image pair 1|6 in the "Graffiti" sequence (Fig. 9(e)–(f)). SURF performed slightly better than others in the "Graffiti" sequence except image pair 1|5. As mentioned previously, SURF feature detector was used for feature detection in this study for fair comparison because both BRIEF versions use it as well. This might have given SURF slight advantage in matching features. SYBA performed better than other algorithms for all other image pairs. It is noted that rBRIEF exhibits lower performances in all cases because rBRIEF has been optimized for being used with orientation information delivered by the detector (which was not available in these experiments).
                     

In order to better highlight the advantages of the SYBA descriptor over BRIEF and rBRIEF, a recall vs. precision curve was used to further evaluate the performance. We did not include ALOHA and SURF in this study because their poor performance with the majority of the image sequences. Fig. 10 shows the recall vs. precision curve using threshold-based similarity matching (sliding the Hamming distance from minimum to maximum) on this dataset. Again, for this assessment the detection thresholds were set such that all outputs have a nearly equal number of correspondences in the spirit of fairness. SYBA outperformed both BRIEF algorithms for high recall values. For 90% recall, SYBA precision exceeds 92%, while BRIEF fell to 75% and rBRIEF fell to 72%. SYBA demonstrated the best discrimination capability in this experimental setup. In order to better point out the merit of the SYBA descriptor statistically, T-test is performed on the newly created BYU feature matching dataset. The experimental result is discussed in Section 3.3.

Different computing platforms have varying computational power, which makes it difficult to compare the processing speed subjectively. We used the number of operations to compare the processing speed instead. SYBA5 × 5 requires 324 (9 × 36) comparisons between SBIs and the feature region image and 324 summation operations to calculate the descriptor. SYBA30 × 30 requires 312 comparisons and 312 summations but on a 30 × 30 sub-image. Both versions of BRIEF require a total of 1536 operations [26].

In the second experiment, we evaluated the performance of the SYBA descriptor using another publically available dataset [22]. This dataset consists of three sets of patches. These patches are sampled from the Statue of Liberty (New York), Notre Dame (Paris) and Half Dome (Yosemite). Each of them contains over 400,000 scale- and rotation-normalized 64 × 64 patches. These patches are sampled around interest points detected using multi-scale Harris corners. Sample patches from the Liberty, Notre Dame, and Half Dome set are shown in Fig. 11. This dataset also contains training data for descriptors like BinBoost and D-BRIEF that require training data. Training sets typically contained from 10,000 to 500,000 patch pairs depending on the applications. SYBA does not require any kind of training data.
                        
                        
                     

For descriptor evaluation we compared SYBA with two versions of BRIEF descriptor, as it does not required any training data as well. In these implementations, patches are resized to 48 × 48 for BRIEF and 31 × 31 for rBRIEF, as the region size was fixed to 48 × 48 for BRIEF and 31 × 31 for rBRIEF (ORB descriptor [27]). In both SYBA versions, the feature region image size was kept at 30 × 30. Two different synthetic basis image sizes (5 × 5 and 30 × 30) were used for SYBA5 × 5 and SYBA30 × 30. In our experiments, we resized patches to 30 × 30 and followed the same procedure to calculate the SYBA descriptor as explained in Section 2.

We performed the experiments following the online instruction [34]. Instead of matching one patch to the rest of ∼400,000 patches, we randomly selected 1000 patches for each matching processing to reduce the computation time for comparison. One of these 1000 patches is from the match information provided online [34] and the remaining 999 patches were selected randomly. For each patch, the tested feature descriptor reported the best match and the non-match. As a result of this process, we created 50,000 pairs of matching patches and 50,000 pairs of non-matching patches for each set and submitted them to the website to evaluate the performance.

For comparison of descriptors we reported the results in term of 95% error rate the same as [22]. The term 95% error rate represents the incorrect matches obtained when 95% of the true matches are found. For reference, we also provided results obtained with SIFT. For SIFT, we used the publicity available Matlab implementation of Vedaldi [35]. Table 3 clearly shows that SYBA5×5 provided up to 5% improvement over BRIEF and up to 5.5% improvement over rBRIEF at 95% error rate. It also shows that SYBA provided comparable accuracy as to the much larger and more computationally expensive SIFT descriptor.

The Oxford dataset does not contain more than two sequences of images for blurring and viewpoint change and has only one sequence of images for compression artifact and illumination variation. It is not sufficient for better evaluation of descriptor performance. The multi-view stereo correspondence dataset in Section 3.3 is not prepared for evaluating the descriptor performance statistically. A new dataset has been created called the BYU feature matching dataset [36]. It consists of 20 sets of images. Each set includes four image sequences. Each image sequence has six images that have gone through image transformations that include blurring, compression, illumination variation, and viewpoint change. The first of the six images in each sequence is the original image and the subsequent images have increasing level of image transformation. An example of the original image from the BYU feature matching dataset is shown in Fig. 12.
                     

The aim is to measure the descriptor performance statistically with this new dataset. A t-test is a statistical hypothesis test, in which the statistically significant difference between two means of two samples is compared. The same test procedure discussed previously was followed for the BYU feature matching dataset. The average detection rate for each image pair (i.e. image pair 1|2, pair 1|3, and so on) was calculated and then the difference was compared. The results of this test help to understand descriptor performance on different sets of image pairs for each image perturbation.

Similar to Fig. 9, Fig. 13 illustrates the performance of two versions of SYBA and two versions of BRIEF. In this figure, pairs which have statistical significance computed with standardize p-value (< 0.05) are denoted with an asterisk. For this assessment the detection thresholds were set such that all outputs have a nearly equal number of correspondences. Both versions of SYBA were more robust than BRIEF-32 and rBRIEF for images that are corrupted by compression artifacts in the new dataset (Fig. 13(a)). SYBA30 × 30 outperformed BRIEF-32 by more than 18% for image pair 1|6. For image corrupted by illumination change, the detection rate of SYBA30 × 30 is more than 9% higher for image pair 1|6 (Fig. 13(b)).

For the image dataset that is corrupted by image blurring, SYBA outperformed both versions of BRIEF algorithm. The accuracy difference is even more obvious for the strongest blurring conditions. For image pair 1|6, the difference was 9% in this blurring sequence (Fig. 13(c)). For the image dataset that is corrupted by viewpoint change, BRIEF performed comparably with SYBA30×30 only for image pair 1|6 (Fig. 13(d)). SYBA outperformed both versions of BRIEF algorithms for all other image pairs. It is noted that rBRIEF exhibits lower performances in all cases because rBRIEF has been optimized for being used with orientation information delivered by the detector (which is not available in these experiments).

SYBA performed better for sequences with compression artifacts, illumination change, image blurring, and small viewpoint change. Slightly lower accuracy (but still better than others) for very large viewpoint change does not affect SYBA's performance because the viewpoint change is usually small for many embedded vision applications such as unmanned air vehicle pose estimation or unmanned ground vehicle autonomous navigation. SYBA30 × 30 performed better than SYBA5 × 5 but required a larger descriptor size. The size of SBI can be easily adjusted for different application requirements. SYBA is proven to be a good candidate for embedded vision applications due to its computational simplicity and superior performance.

@&#CONCLUSION@&#

In this paper we have presented a new feature descriptor called SYBA. This unique approach was inspired by a new compressed sensing theory. SYBA has been compared favorably to BRIEF, which is currently arguably the best binary descriptor in the literature, and other more common descriptors such as SIFT, SURF, ALOHA, BASIS< and Tree BASIS. SYBA requires a slightly larger descriptor than BRIEF, but it provides better description and matching results. We have successfully applied SYBA to four different vision applications and seen very accurate results. These include soccer game event annotation in broadcast video [37], tracking of multiple moving targets from an unmanned aerial vehicle [38], drift reduction for visual odometry [39], and motion analysis for advanced driving assistance systems. SYBA is an excellent candidate for hardware implementation due to its ability to create a feature descriptor without using complex computations that require floating-point operations. Future work will focus on applying SYBA to various computer vision applications that require accurate feature matching. Hardware implementation for the embedded vision sensor will also be explored.

@&#REFERENCES@&#

