@&#MAIN-TITLE@&#Identifying connected components in Gaussian finite mixture models for clustering

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Clusters are identified as connected components from high density regions.


                        
                        
                           
                           Gaussian finite mixture models are used for density estimation.


                        
                        
                           
                           Identified clusters are not constrained to have a Gaussian shape.


                        
                        
                           
                           Clusters need not be obtained by combining mixture components.


                        
                        
                           
                           A dimension reduction step is used in cases of higher data dimensionality.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Finite mixture of Gaussian distributions

Cluster analysis

Connected components

High density regions

Cluster cores

@&#ABSTRACT@&#


               
               
                  Model-based clustering associates each component of a finite mixture distribution to a group or cluster. Therefore, an underlying implicit assumption is that a one-to-one correspondence exists between mixture components and clusters. In applications with multivariate continuous data, finite mixtures of Gaussian distributions are typically used. Information criteria, such as BIC, are often employed to select the number of mixture components. However, a single Gaussian density may not be sufficient, and two or more mixture components could be needed to reasonably approximate the distribution within a homogeneous group of observations. A clustering method, based on the identification of high density regions of the underlying density function, is introduced. Starting with an estimated Gaussian finite mixture model, the corresponding density estimate is used to identify the cluster cores, i.e. those data points which form the core of the clusters. Then, the remaining observations are allocated to those cluster cores for which the probability of cluster membership is the highest. The method is illustrated using both simulated and real data examples, which show how the proposed approach improves the identification of non-Gaussian clusters compared to a fully parametric approach. Furthermore, it enables the identification of clusters which cannot be obtained by merging mixture components, and it can be straightforwardly extended to cases of higher dimensionality.
               
            

@&#INTRODUCTION@&#

Clustering methods aim to identify groups of similar observations that are relatively separate from each other. Since we ignore the “true” groupings, even though one does exist, these are also called unsupervised learning methods. In the model-based approach to clustering, each component of a finite mixture of density functions belonging to a given parametric class is associated with a group or cluster. Multivariate Gaussian distribution is often adopted with continuous data. However, a non-Gaussian cluster may require more than one single mixture component. As a result, there is no longer a one-to-one relationship between mixture components and clusters. In the same vein, it was noted that “it can be misleading to identify the number of Gaussian components with the number of clusters” (Hennig, 2010, p. 5).

This problem was recently addressed by Baudry et al. (2010), who proposed a method based on an entropy criterion for hierarchically combining mixture components to form clusters. Hennig (2010) also discussed hierarchical merging methods based on unimodality and misclassification probabilities. The main limitation of these approaches is that clusters can only be obtained by merging two or more components. Therefore, data points assigned to a single Gaussian component cannot be subsequently allocated to different clusters.

In the development of the method described in this paper, we adopted the definition introduced by Hartigan (1975, p. 205) from among several definitions of the concept of ‘cluster’: “Clusters may be thought of as regions of high density separated from other such regions by regions of low density”. The idea of using a density estimate as the basis for clustering methods has been considered in various recent papers, both in statistical and in machine learning literature. An earlier application in computer vision and image processing is the mean shift algorithm (Fukunaga and Hostetler, 1975), which is a mode-seeking algorithm for detecting the modes of a nonparametric density estimate. More recently, Stuetzle (2003) presented a method which exploits the connection between the minimum spanning tree of a sample and the nearest neighbour density estimation. An alternative to mode clustering is level set clustering, where the aim is to find the hierarchical structure of connected components of a density level set (Stuetzle and Nugent, 2010). This working definition of ‘cluster’ was also adopted recently by Azzalini and Torelli (2007) and Menardi and Azzalini (2014). In particular, Azzalini and Torelli (2007) employed nonparametric density estimation to identify the mode function, where each mode is associated with a subset of points with high density, formed by means of suitable manipulation of the associated Delaunay triangulation.

Unlike traditional methods of cluster analysis based on heuristic or distance-based procedures, finite mixture modelling provides a formal statistical framework on which to base the clustering procedure. Unfortunately, the relationship between the number of modes and the number of components in the mixture is very complex. Carreira-Perpiñán and Williams (2003) showed that if the components of a mixture model have the same covariance matrix (up to a scaling factor), then the number of modes cannot exceed the number of components. However, the number of modes can be larger than the number of components, when the components are allowed to have different covariance matrices.

In this paper, we propose a method which, assuming the working definition of clusters given by Hartigan (1975), adapt the methodology of Azzalini and Torelli (2007) to model-based clustering. Starting with an estimated Gaussian finite mixture model, the corresponding density estimate is used to identify the cluster cores, i.e. those data points which form the core of the clusters. Then, the remaining observations are allocated to those cluster cores for which the probability of cluster membership is the highest. This approach improves the identification of non-Gaussian clusters compared to a fully parametric approach. Furthermore, it enables the identification of clusters which cannot be obtained by combining mixture components, and, finally, it can be straightforwardly expanded to cases of higher dimensionality.


                     A motivating example: Old Faithful data. Consider the data for the waiting time between eruptions (waiting) and the duration of the eruptions (eruptions) for the Old Faithful geyser in Yellowstone National Park, Wyoming, USA. Fig. 1
                     (a) shows the clustering partition obtained by fitting the best Gaussian finite mixture model according to BIC. The selected model is a mixture of three components with common full covariance matrix. However, there appear to be two separate groups of points. The cluster with high values for both waiting and eruptions clearly cannot be fitted by a single Gaussian component. Nonetheless, the corresponding bivariate density estimate shown in Fig. 1(b) indicates the presence of two separate regions of high density. The method proposed in this paper aims to deal with similar situations.

The outline of this article is as follows. Section  2 gives a brief review of background material on model-based clustering. Section  3 contains the proposal for identifying the cluster cores from connected components, and discusses the classification of the remaining unallocated points. Section  4 presents the extension of the proposed method to the case of high dimensional features. Sections  5 and 6 illustrate empirical results using synthetic and real data examples, respectively. The final section provides some concluding remarks.

@&#BACKGROUND@&#

Clustering algorithms based on probability models have become increasingly popular in recent years and have been used to cluster data in a variety of fields. The model-based approach to clustering assumes that the data are generated by a finite mixture of probability distributions with each group or cluster following a different multivariate probability density distribution. The same parametric family, but with different parameters, is often assumed for all the components.

Let 
                        X
                        =
                        
                           {
                           
                              
                                 
                                    x
                                 
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 
                                    x
                                 
                              
                              
                                 2
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 
                                    x
                                 
                              
                              
                                 n
                              
                           
                           }
                        
                        ,
                        
                           
                              
                                 x
                              
                           
                           
                              i
                           
                        
                        ∈
                        
                           
                              R
                           
                           
                              p
                           
                        
                     , be the data sampled from a 
                        p
                     -dimensional continuous random distribution with unknown density 
                        f
                        
                           (
                           
                              x
                           
                           )
                        
                     . A typical assumption for continuous variables is that the data can be described by a mixture of multivariate normal component densities. Thus, an estimate of 
                        f
                        
                           (
                           
                              x
                           
                           )
                        
                      can be obtained for a given sample, using a Gaussian finite mixture model (GMM) with 
                        G
                      components of the form 
                        
                           (1)
                           
                              f
                              
                                 (
                                 
                                    x
                                 
                                 )
                              
                              =
                              
                                 
                                    ∑
                                 
                                 
                                    g
                                    =
                                    1
                                 
                                 
                                    G
                                 
                              
                              
                                 
                                    π
                                 
                                 
                                    g
                                 
                              
                              
                                 
                                    ϕ
                                 
                                 
                                    g
                                 
                              
                              
                                 (
                                 
                                    x
                                 
                                 |
                                 
                                    
                                       
                                          μ
                                       
                                    
                                    
                                       g
                                    
                                 
                                 ,
                                 
                                    
                                       
                                          Σ
                                       
                                    
                                    
                                       g
                                    
                                 
                                 )
                              
                              ,
                           
                        
                      where 
                        
                           
                              π
                           
                           
                              g
                           
                        
                      represents the mixing probabilities 
                        
                           (
                           
                              
                                 π
                              
                              
                                 g
                              
                           
                           >
                           0
                           ,
                           ∑
                           
                              
                                 π
                              
                              
                                 g
                              
                           
                           =
                           1
                           )
                        
                     , and 
                        
                           
                              ϕ
                           
                           
                              g
                           
                        
                        
                           (
                           ⋅
                           |
                           
                              
                                 
                                    μ
                                 
                              
                              
                                 g
                              
                           
                           ,
                           
                              
                                 
                                    Σ
                                 
                              
                              
                                 g
                              
                           
                           )
                        
                      the multivariate Gaussian density for the 
                        g
                     -th component 
                        
                           (
                           g
                           =
                           1
                           ,
                           …
                           ,
                           G
                           )
                        
                      with parameters 
                        
                           (
                           
                              
                                 
                                    μ
                                 
                              
                              
                                 g
                              
                           
                           ,
                           
                              
                                 
                                    Σ
                                 
                              
                              
                                 g
                              
                           
                           )
                        
                     . Thus, clusters are ellipsoidal, centred at the mean vector 
                        
                           
                              
                                 μ
                              
                           
                           
                              g
                           
                        
                     , and with other geometric features, such as volume, shape and orientation, determined by 
                        
                           
                              
                                 Σ
                              
                           
                           
                              g
                           
                        
                     . Parsimonious parameterisation of covariance matrices can be adopted by means of eigenvalue decomposition in the form 
                        
                           
                              
                                 Σ
                              
                           
                           
                              g
                           
                        
                        =
                        
                           
                              λ
                           
                           
                              g
                           
                        
                        
                           
                              
                                 D
                              
                           
                           
                              g
                           
                        
                        
                           
                              
                                 A
                              
                           
                           
                              g
                           
                        
                        
                           
                              
                                 D
                              
                           
                           
                              g
                           
                           
                              ⊤
                           
                        
                     , where 
                        
                           
                              λ
                           
                           
                              g
                           
                        
                      is a scalar controlling the volume of the ellipsoid, 
                        
                           
                              
                                 A
                              
                           
                           
                              g
                           
                        
                      is a diagonal matrix specifying the shape of the density contours, and 
                        
                           
                              
                                 D
                              
                           
                           
                              g
                           
                        
                      is an orthogonal matrix which determines the orientation of the corresponding ellipsoid (Banfield and Raftery, 1993; Celeux and Govaert, 1995). Fraley et al. (2012, Table 1) reported some parameterisation of within-group covariance matrices available in MCLUST software, and the corresponding geometric characteristics. Maximum likelihood estimation can be performed via the expectation-maximisation (EM) algorithm (Dempster et al., 1977; McLachlan and Krishnan, 2008).

In the model-based approach to clustering, observations are assigned to clusters according to Bayes’ rule, i.e. every observation is allocated to the cluster with the highest posterior probability that the observation originated from this group.

Model selection, which requires the identification of the number of mixture components and the covariance parameterisation for each component, is usually based on criteria which penalise the likelihood based on model complexity. Bayesian information criterion (BIC,  Schwartz, 1978; Neath and Cavanaugh, 2012) has been widely used for mixture models, both for density estimation (Roeder and Wasserman, 1997) and for clustering (Fraley and Raftery, 1998). Keribin (2000) showed that BIC is consistent for choosing the number of components in a mixture model, under the assumption that the likelihood is bounded. This may not be true in general of Gaussian mixture models, but it does hold if, for instance, the variance is bounded below, a constraint which is imposed in practice in MCLUST software. Some authors (Celeux and Soromenho, 1996; Ray and Lindsay, 2008) noted the tendency of BIC to underestimate the number of clusters. This seems to be the case of overlapping Gaussian clusters with small sample sizes. Furthermore, BIC tends to select the number of mixture components needed to provide a good approximation to the density, rather than the number of clusters as such. Among other criteria that have been proposed, the integrated complete-data likelihood (ICL) criterion (Biernacki et al., 2000) showed good performance in selecting the number of clusters. See also the model selection tools based on quadratic risk discussed by Ray and Lindsay (2008).

A detailed review of finite mixture modelling is provided by McLachlan and Peel (2000), whereas for a recent review on selecting the number of components in GMMs see McLachlan and Rathnayake (2014).

@&#METHODOLOGY@&#

A density-based approach following Hartigan’s definition of clusters (Hartigan, 1975, p. 205) requires the density function 
                           f
                           
                              (
                              
                                 x
                              
                              )
                           
                        . For any threshold value 
                           c
                           ≥
                           0
                        , the upper level set is defined as 
                           
                              
                                 L
                                 
                                    (
                                    c
                                    )
                                 
                                 =
                                 
                                    {
                                    
                                       x
                                    
                                    :
                                    
                                       x
                                    
                                    ∈
                                    
                                       
                                          R
                                       
                                       
                                          p
                                       
                                    
                                    ,
                                    f
                                    
                                       (
                                       
                                          x
                                       
                                       )
                                    
                                    >
                                    c
                                    }
                                 
                                 ,
                              
                           
                         and with associated probability 
                           
                              
                                 p
                              
                              
                                 c
                              
                           
                           =
                           
                              
                                 ∫
                              
                              
                                 L
                                 
                                    (
                                    c
                                    )
                                 
                              
                           
                           f
                           
                              (
                              
                                 x
                              
                              )
                           
                           d
                           
                              x
                           
                        . Thus, the upper level set 
                           L
                           
                              (
                              c
                              )
                           
                         identifies the subset of 
                           
                              
                                 R
                              
                              
                                 p
                              
                           
                        , the density of which is above 
                           c
                        . This set may or may not be connected. In the latter case, two or more regions of high density are detected. Hartigan (1975) defined the high density clusters at level 
                           c
                         as the connected components of 
                           L
                           
                              (
                              c
                              )
                           
                        .

The set of all high-density clusters, i.e. the collection of the level set clusters at different values of 
                           c
                        , is named level set cluster tree. This collection is a tree because it fulfils the following hierarchical property: for any two high-density clusters A and B, either A is a subset of B, B is a subset of A, or they are disjoint. Note that, although it will not be pursued further, this idea enables the level set tree to be represented with a dendrogram.


                        Azzalini and Torelli (2007) defined the mode function 
                        
                           m
                           
                              (
                              p
                              )
                           
                         as a step function, which gives the number of connected components of 
                           L
                           
                              (
                              c
                              )
                           
                         as 
                           p
                         varies in (0,1), with the following properties: (i) 
                           m
                           
                              (
                              p
                              )
                           
                           ≥
                           1
                         for 
                           0
                           <
                           p
                           <
                           1
                        ; (ii) the number of modes 
                           M
                         is given by the total number of increments of 
                           m
                           
                              (
                              p
                              )
                           
                        , counted with their multiplicity; (iii) if 
                           f
                         is unimodal, i.e.  
                           M
                           =
                           1
                        , then 
                           m
                           
                              (
                              p
                              )
                           
                           =
                           1
                         for 
                           0
                           <
                           p
                           <
                           1
                        ; (iv) by convention, assume 
                           m
                           
                              (
                              p
                              )
                           
                           =
                           0
                         for 
                           p
                           =
                           0
                         and 
                           p
                           =
                           1
                        .

Since 
                           f
                           
                              (
                              
                                 x
                              
                              )
                           
                         is unknown, the level set must be estimated from the data. The empirical level set is, therefore, given by 
                           
                              
                                 L
                              
                              
                                 ̂
                              
                           
                           
                              (
                              c
                              )
                           
                           =
                           
                              {
                              
                                 x
                              
                              :
                              
                                 x
                              
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    p
                                 
                              
                              ,
                              
                                 
                                    f
                                 
                                 
                                    ̂
                                 
                              
                              
                                 (
                                 
                                    x
                                 
                                 )
                              
                              >
                              c
                              }
                           
                        , for 
                           0
                           ≤
                           c
                           ≤
                           max
                           
                              
                                 f
                              
                              
                                 ̂
                              
                           
                        . Azzalini and Torelli (2007) considered nonparametric density estimation for 
                           f
                           
                              (
                              
                                 x
                              
                              )
                           
                        . Here, we consider model-based density estimation using GMMs, as in (1).

Since the main interest relies on the clustering of sample data points 
                           X
                           =
                           
                              {
                              
                                 
                                    
                                       x
                                    
                                 
                                 
                                    1
                                 
                              
                              ,
                              
                                 
                                    
                                       x
                                    
                                 
                                 
                                    2
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    
                                       x
                                    
                                 
                                 
                                    n
                                 
                              
                              }
                           
                        , we may restrict attention to the sample level set 
                           
                              (2)
                              
                                 S
                                 
                                    (
                                    c
                                    )
                                 
                                 =
                                 
                                    {
                                    
                                       
                                          
                                             x
                                          
                                       
                                       
                                          i
                                       
                                    
                                    :
                                    
                                       
                                          
                                             x
                                          
                                       
                                       
                                          i
                                       
                                    
                                    ∈
                                    X
                                    ,
                                    
                                       
                                          f
                                       
                                       
                                          ̂
                                       
                                    
                                    
                                       (
                                       
                                          
                                             
                                                x
                                             
                                          
                                          
                                             i
                                          
                                       
                                       )
                                    
                                    >
                                    c
                                    }
                                 
                                 ,
                              
                           
                         with associated relative frequency 
                           
                              
                                 
                                    
                                       p
                                    
                                    
                                       ̂
                                    
                                 
                              
                              
                                 c
                              
                           
                           =
                           
                              |
                              S
                              
                                 (
                                 c
                                 )
                              
                              |
                           
                           /
                           n
                        , where 
                           
                              |
                              A
                              |
                           
                         denotes the cardinality of set 
                           A
                        .

Once the sample level set is obtained, the main goal becomes that of identifying the connected sets, i.e. the connected components of 
                           S
                           
                              (
                              c
                              )
                           
                         as 
                           c
                         varies. This may be viewed as the problem of detecting the connected components of a graph, the vertices of which are the elements of 
                           S
                           
                              (
                              c
                              )
                           
                        . Following the approach of Azzalini and Torelli (2007), let us consider the Delaunay triangulation of sample points 
                           
                              
                                 
                                    x
                                 
                              
                              
                                 i
                              
                           
                           
                           
                              (
                              i
                              =
                              1
                              ,
                              …
                              ,
                              n
                              )
                           
                         obtained from a Voronoi tessellation. The Voronoi diagram is a partition of 
                           
                              
                                 R
                              
                              
                                 p
                              
                           
                         in 
                           n
                         regions 
                           V
                           
                              (
                              
                                 
                                    
                                       x
                                    
                                 
                                 
                                    1
                                 
                              
                              )
                           
                           ,
                           …
                           ,
                           V
                           
                              (
                              
                                 
                                    
                                       x
                                    
                                 
                                 
                                    n
                                 
                              
                              )
                           
                        , to the extent that each Voronoi cell 
                           V
                           
                              (
                              
                                 
                                    
                                       x
                                    
                                 
                                 
                                    i
                                 
                              
                              )
                           
                         is the set of all points in 
                           
                              
                                 R
                              
                              
                                 p
                              
                           
                         whose distance to 
                           
                              
                                 
                                    x
                                 
                              
                              
                                 i
                              
                           
                         is not greater than their distance to any other point, i.e.  
                           
                              
                                 V
                                 
                                    (
                                    
                                       
                                          
                                             x
                                          
                                       
                                       
                                          i
                                       
                                    
                                    )
                                 
                                 =
                                 
                                    {
                                    
                                       x
                                    
                                    ∈
                                    
                                       
                                          R
                                       
                                       
                                          p
                                       
                                    
                                    |
                                    
                                       ‖
                                       
                                          x
                                       
                                       −
                                       
                                          
                                             
                                                x
                                             
                                          
                                          
                                             i
                                          
                                       
                                       ‖
                                    
                                    ≤
                                    
                                       ‖
                                       
                                          x
                                       
                                       −
                                       
                                          
                                             
                                                x
                                             
                                          
                                          
                                             j
                                          
                                       
                                       ‖
                                    
                                    ,
                                    
                                    j
                                    =
                                    1
                                    ,
                                    …
                                    ,
                                    n
                                    }
                                 
                                 .
                              
                           
                         The Delaunay graph can be obtained from the Voronoi diagram, by connecting the pair 
                           
                              (
                              
                                 
                                    
                                       x
                                    
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    
                                       x
                                    
                                 
                                 
                                    j
                                 
                              
                              )
                           
                         with an edge when the corresponding Voronoi cells share a portion of their boundary facets. Note that only Delaunay triangulation is needed to obtain the connected sets, and this may be obtained directly, without building the Voronoi diagram and using efficient algorithms (Barber et al., 1996, 2013).

Thus, after removing the sample points 
                           
                              
                                 
                                    x
                                 
                              
                              
                                 i
                              
                           
                           ∉
                           S
                           
                              (
                              c
                              )
                           
                         and all the edges with at least one vertex among these points, we obtain a set of points which form one or more connected components. Each connected component is a mode at density level 
                           c
                        .


                        Old Faithful data example (continued). Fig. 2
                        (a) displays the estimated density for the Old Faithful data with a plane at a given density level 
                           c
                        . Panel (b) of Fig. 2 shows the Voronoi diagrams for all the points, and the Delaunay triangulation for those points included in the sample level set 
                           S
                           
                              (
                              c
                              )
                           
                         with 
                           
                              
                                 
                                    
                                       p
                                    
                                    
                                       ̂
                                    
                                 
                              
                              
                                 c
                              
                           
                           =
                           0.26
                        . From this graph, two connected components are clearly identified corresponding to local modes of the estimated density.

In order to obtain the connected components at a given density level as discussed above, a range of 
                           c
                         values must be specified. These values can be obtained from the density estimates computed for the sample data points corresponding to a set of equally spaced values of 
                           p
                           ∈
                           
                              (
                              0
                              ,
                              1
                              )
                           
                        . By default we set the number of grid points to 
                           min
                           
                              (
                              
                                 ⌊
                                 log
                                 
                                    (
                                    n
                                    )
                                 
                                 ×
                                 10
                                 ⌉
                              
                              ,
                              n
                              )
                           
                        , where 
                           
                              ⌊
                              a
                              ⌉
                           
                         indicates the nearest integer value of 
                           a
                        .

According to Eq. (2), the set 
                           S
                           
                              (
                              
                                 
                                    c
                                 
                                 
                                    p
                                 
                              
                              )
                           
                         is given for each value of 
                           p
                         by those observations, the density of which is larger than the threshold value 
                           
                              
                                 c
                              
                              
                                 p
                              
                           
                        . Simultaneously, the empirical mode function 
                           
                              
                                 m
                              
                              
                                 ̂
                              
                           
                           
                              (
                              p
                              )
                           
                         is obtained by counting the corresponding number of connected components. The total number of increments of 
                           
                              
                                 m
                              
                              
                                 ̂
                              
                           
                           
                              (
                              p
                              )
                           
                        , counted with their multiplicity, is equal to the number of modes 
                           M
                        . Cluster cores are formed by the data lying in the regions around the detected modes, hence the number of clusters is given by the number of cluster cores identified.


                        Old Faithful data example (continued).
                        Fig. 3
                        (a) shows the empirical mode function for the Old Faithful data. The graph shows the number of modes found as a function of the proportion of data points above a threshold density level. When the fraction of data above a density level is small there is only one connected component. As the density level decreases, the proportion of data points above this level increases, up to a certain point where two groups are identified. When most of the data points are above a small density level, the two groups merge into a single connected component. Therefore, a clear indication of a bimodal distribution appears from this plot. Fig. 3(b) shows the corresponding cluster cores identified, whereas the remaining unfilled points refer to those observations which are not initially assigned to any cluster.

Once cluster cores have been identified, some observations usually remain unlabelled and need to be classified. Semi-supervised learning is a class of techniques that make use of both unlabelled and labelled data for building a classifier. For a general introduction to semi-supervised learning see Zhu and Goldberg (2009), in particular Ch. 3 which is devoted to mixture models, and McLachlan and Peel (2000, Sec. 2.19), where it is discussed under the name of partial classification.

However, in our case unallocated points are not positioned randomly in the feature space, but are placed on the outskirts of cluster cores. From the perspective of a model-based classification, unlabelled points can be classified on the basis of a model estimated using points already allocated to any of the cluster cores. In this Section, we describe the algorithm adopted for this particular semi-supervised classification task.

We start by fitting a GMM for classification using observations from the cluster cores and their labels. Unlabelled points can be assigned to clusters following an iterative block assignment scheme: 
                           
                              1.
                              from the GMM estimated on the 
                                    
                                       
                                          n
                                       
                                       
                                          inc
                                       
                                    
                                  allocated points, calculate the conditional probability 
                                    
                                       
                                          
                                             
                                                z
                                             
                                             
                                                ̂
                                             
                                          
                                       
                                       
                                          i
                                          k
                                       
                                    
                                  that the unallocated observation 
                                    i
                                  belongs to cluster core 
                                    k
                                 ;

compute the log-ratios 
                                    
                                       
                                          r
                                       
                                       
                                          i
                                          k
                                       
                                    
                                    =
                                    log
                                    
                                       (
                                       
                                          
                                             
                                                
                                                   z
                                                
                                                
                                                   ̂
                                                
                                             
                                          
                                          
                                             i
                                             k
                                          
                                       
                                       /
                                       
                                          (
                                          1
                                          −
                                          
                                             
                                                
                                                   
                                                      z
                                                   
                                                   
                                                      ̂
                                                   
                                                
                                             
                                             
                                                i
                                                k
                                             
                                          
                                          )
                                       
                                       )
                                    
                                  for all the unallocated observations;

update the classification by assigning those observations whose 
                                    
                                       
                                          r
                                       
                                       
                                          i
                                          k
                                       
                                    
                                    ≥
                                    
                                       
                                          q
                                       
                                       
                                          k
                                       
                                    
                                 , where 
                                    
                                       
                                          q
                                       
                                       
                                          k
                                       
                                    
                                  is the 
                                    
                                       
                                          
                                             
                                                n
                                             
                                             
                                                inc
                                             
                                          
                                          /
                                          n
                                       
                                    
                                  quantile of the empirical distribution of log-ratios 
                                    
                                       
                                          r
                                       
                                       
                                          i
                                          k
                                       
                                    
                                  within group 
                                    k
                                 , to the cluster core for which 
                                    
                                       
                                          
                                             
                                                z
                                             
                                             
                                                ̂
                                             
                                          
                                       
                                       
                                          i
                                          k
                                       
                                    
                                  is the maximum;

if 
                                    
                                       
                                          n
                                       
                                       
                                          inc
                                       
                                    
                                    <
                                    n
                                  repeat steps 1–3, where 
                                    
                                       
                                          n
                                       
                                       
                                          inc
                                       
                                    
                                  is the updated number of allocated points.


                        Old Faithful data example (continued). Fig. 4
                         shows the final clustering obtained after all the unallocated points have been assigned to the cluster cores.


                        Table 1
                         summarises the main steps of the proposed algorithm for Gaussian Mixture Modelling with Highest Density (GMMHD) clustering.

The use of Delaunay triangulation discussed in Section  3.1 presents a computational complexity which grows exponentially with the dimensionality of data, thus making triangulation not feasible in large dimensions. In this section, we propose a method for finding connected sets in higher dimensional spaces. The basic idea is to project the data on to a suitable subspace of reduced dimensionality, where connected components can be easily found.


                     Scrucca (2010) recently proposed a dimension reduction method for model-based clustering in the Gaussian framework, called GMMDR (Gaussian Mixture Modelling on a Dimension Reduced subspace). Given a 
                        G
                     -component Gaussian mixture model of the form in (1), the procedure aims at finding the smallest subspace which captures the clustering information contained in the data. The core of the method is to identify those directions where the cluster means, 
                        
                           
                              
                                 μ
                              
                           
                           
                              g
                           
                        
                     , and the cluster covariances, 
                        
                           
                              
                                 Σ
                              
                           
                           
                              g
                           
                        
                     , vary as much as possible, provided that each direction is 
                        
                           Σ
                        
                     -orthogonal to the others, where 
                        
                           Σ
                        
                      is the marginal covariance matrix. The basis of the subspace in the original proposal is estimated with the purpose of displaying the underlying characteristics of clusters. However, in the present case, we are more interested in finding those directions which show the maximal separation among clusters, and a simple modification of the method allows us to achieve this goal, as described below.

To recover the directions with most separation among clusters, consider the kernel matrix 
                        
                           (3)
                           
                              
                                 M
                              
                              =
                              
                                 
                                    ∑
                                 
                                 
                                    g
                                    =
                                    1
                                 
                                 
                                    G
                                 
                              
                              
                                 
                                    π
                                 
                                 
                                    g
                                 
                              
                              
                                 (
                                 
                                    
                                       
                                          μ
                                       
                                    
                                    
                                       g
                                    
                                 
                                 −
                                 
                                    μ
                                 
                                 )
                              
                              
                                 
                                    
                                       (
                                       
                                          
                                             
                                                μ
                                             
                                          
                                          
                                             g
                                          
                                       
                                       −
                                       
                                          μ
                                       
                                       )
                                    
                                 
                                 
                                    ⊤
                                 
                              
                              ,
                           
                        
                      where 
                        
                           μ
                        
                        =
                        
                           
                              ∑
                           
                           
                              g
                              =
                              1
                           
                           
                              G
                           
                        
                        
                           
                              π
                           
                           
                              g
                           
                        
                        
                           
                              
                                 μ
                              
                           
                           
                              g
                           
                        
                      is the global mean, and 
                        
                           Σ
                        
                        =
                        
                           
                              n
                           
                           
                              −
                              1
                           
                        
                        
                           
                              ∑
                           
                           
                              i
                              =
                              1
                           
                           
                              n
                           
                        
                        
                           (
                           
                              
                                 
                                    x
                                 
                              
                              
                                 i
                              
                           
                           −
                           
                              μ
                           
                           )
                        
                        
                           
                              
                                 (
                                 
                                    
                                       
                                          x
                                       
                                    
                                    
                                       i
                                    
                                 
                                 −
                                 
                                    μ
                                 
                                 )
                              
                           
                           
                              ⊤
                           
                        
                      is the covariance matrix. Directions are obtained via the generalised eigen-decomposition of the kernel matrix 
                        
                           M
                        
                     , defined as 
                        
                           
                              
                                 M
                              
                              
                                 
                                    
                                       v
                                    
                                 
                                 
                                    i
                                 
                              
                              =
                              
                                 
                                    l
                                 
                                 
                                    i
                                 
                              
                              
                                 Σ
                              
                              
                                 
                                    
                                       v
                                    
                                 
                                 
                                    i
                                 
                              
                              ,
                           
                        
                      where 
                        
                           
                              l
                           
                           
                              1
                           
                        
                        ≥
                        
                           
                              l
                           
                           
                              2
                           
                        
                        ≥
                        ⋯
                        ≥
                        
                           
                              l
                           
                           
                              d
                           
                        
                        >
                        0
                      and 
                        
                           
                              
                                 v
                              
                           
                           
                              i
                           
                           
                              ⊤
                           
                        
                        
                           Σ
                        
                        
                           
                              
                                 v
                              
                           
                           
                              j
                           
                        
                        =
                        1
                      if 
                        i
                        =
                        j
                     , and 0 otherwise. Thus, the basis of the dimension reduction subspace 
                        S
                        
                           (
                           
                              β
                           
                           )
                        
                      is given by the eigenvectors 
                        
                           [
                           
                              
                                 
                                    v
                                 
                              
                              
                                 1
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 
                                    v
                                 
                              
                              
                                 d
                              
                           
                           ]
                        
                        ≡
                        
                           β
                        
                     .

As mentioned, GMMDR directions in the original proposal (Scrucca, 2010) were estimated using information provided by the variation on both cluster locations and cluster dispersions. However, if clusters separation is the goal, an appropriate modification of the kernel matrix allows to consider only the information arising from variation on cluster means as proposed by Scrucca (2014) (cf. Eq. (3) above with Eq. (2) in  Scrucca, 2010, p. 474).

The dimensionality of the subspace 
                        S
                        
                           (
                           
                              β
                           
                           )
                        
                      is 
                        d
                        =
                        min
                        
                           (
                           G
                           ,
                           p
                           −
                           1
                           )
                        
                     , and directions are ordered on the basis of the corresponding eigenvalues. Directions associated with approximately zero eigenvalues can be discarded in practice, because clusters will overlap substantially along these directions.

For an 
                        
                           (
                           n
                           ×
                           p
                           )
                        
                      sample data matrix 
                        
                           X
                        
                     , the sample version 
                        
                           
                              
                                 M
                              
                           
                           
                              ̂
                           
                        
                      of the kernel 
                        
                           M
                        
                      is obtained using the corresponding estimates from fitting a GMM, as in (1). The sample directions are calculated from the generalised eigen-decomposition of 
                        
                           
                              
                                 M
                              
                           
                           
                              ̂
                           
                        
                      with respect to 
                        
                           
                              
                                 Σ
                              
                           
                           
                              ̂
                           
                        
                     , the sample covariance matrix. Then, the GMMDR variables, 
                        
                           Z
                        
                        =
                        
                           X
                        
                        
                           
                              
                                 β
                              
                           
                           
                              ̂
                           
                        
                     , are the projections of the 
                        
                           (
                           n
                           ×
                           p
                           )
                        
                      data matrix 
                        
                           X
                        
                      on to the subspace 
                        S
                        
                           (
                           
                              
                                 
                                    β
                                 
                              
                              
                                 ̂
                              
                           
                           )
                        
                     .

Because some directions are associated with small eigenvalues, we would like to discard them because they provide little or no clustering information. The method discussed in Scrucca (2010) for subset selection, based on the proposal of Raftery and Dean (2006), can be used to prune the subset of irrelevant GMMDR variables. Two subsets of features, 
                        s
                      and 
                        
                           
                              s
                           
                           
                              ′
                           
                        
                        =
                        
                           {
                           s
                           ∖
                           i
                           }
                        
                        ⊂
                        s
                     , can be compared using the BIC difference 
                        
                           (4)
                           
                              
                                 
                                    BIC
                                 
                                 
                                    diff
                                 
                              
                              
                                 (
                                 
                                    
                                       
                                          Z
                                       
                                    
                                    
                                       i
                                       ∖
                                       s
                                    
                                 
                                 )
                              
                              =
                              
                                 
                                    BIC
                                 
                                 
                                    clust
                                 
                              
                              
                                 (
                                 
                                    
                                       
                                          Z
                                       
                                    
                                    
                                       s
                                    
                                 
                                 )
                              
                              −
                              
                                 
                                    BIC
                                 
                                 
                                    notclust
                                 
                              
                              
                                 (
                                 
                                    
                                       
                                          Z
                                       
                                    
                                    
                                       s
                                    
                                 
                                 )
                              
                              =
                              
                                 
                                    BIC
                                 
                                 
                                    clust
                                 
                              
                              
                                 (
                                 
                                    
                                       
                                          Z
                                       
                                    
                                    
                                       s
                                    
                                 
                                 )
                              
                              −
                              
                                 [
                                 
                                    
                                       BIC
                                    
                                    
                                       clust
                                    
                                 
                                 
                                    (
                                    
                                       
                                          
                                             Z
                                          
                                       
                                       
                                          
                                             
                                                s
                                             
                                             
                                                ′
                                             
                                          
                                       
                                    
                                    )
                                 
                                 +
                                 
                                    
                                       BIC
                                    
                                    
                                       reg
                                    
                                 
                                 
                                    (
                                    
                                       
                                          
                                             Z
                                          
                                       
                                       
                                          i
                                       
                                    
                                    |
                                    
                                       
                                          
                                             Z
                                          
                                       
                                       
                                          
                                             
                                                s
                                             
                                             
                                                ′
                                             
                                          
                                       
                                    
                                    )
                                 
                                 ]
                              
                              ,
                           
                        
                      where 
                        
                           
                              BIC
                           
                           
                              clust
                           
                        
                        
                           (
                           
                              
                                 
                                    Z
                                 
                              
                              
                                 s
                              
                           
                           )
                        
                      is the BIC value for the best clustering model fitted using features in 
                        s
                        ,
                        
                           
                              BIC
                           
                           
                              clust
                           
                        
                        
                           (
                           
                              
                                 
                                    Z
                                 
                              
                              
                                 
                                    
                                       s
                                    
                                    
                                       ′
                                    
                                 
                              
                           
                           )
                        
                      is the BIC value for the best clustering model fitted using features in 
                        
                           
                              s
                           
                           
                              ′
                           
                        
                     , and 
                        
                           
                              BIC
                           
                           
                              reg
                           
                        
                        
                           (
                           
                              
                                 
                                    Z
                                 
                              
                              
                                 i
                              
                           
                           |
                           
                              
                                 Z
                              
                              
                                 
                                    
                                       s
                                    
                                    
                                       ′
                                    
                                 
                              
                           
                           )
                        
                      is the BIC value for the regression of the 
                        i
                     -th feature on the remaining features in 
                        
                           
                              s
                           
                           
                              ′
                           
                        
                     .

Since the space of all possible subsets contains 
                        
                           
                              2
                           
                           
                              d
                           
                        
                        −
                        1
                      elements, an exhaustive search is not feasible. Thus, Scrucca (2010) proposed the use of a greedy search algorithm to find a local optimum in the model space. The algorithm is a forward-only procedure, as the backward step is unnecessary due to 
                        
                           Σ
                        
                     -orthogonality of GMMDR directions. The algorithm is based on the following steps: 
                        
                           1.
                           select the first feature to be the one which maximises the BIC difference in (4) between the best clustering model and the model which assumes no clustering, i.e. a single component;

select the next feature, amongst those not previously included, to be the one which maximises the BIC difference in (4);

iterate the previous step until all the BIC differences for the inclusion of a variable become negative.

Once the relevant GMMDR directions have been obtained, the GMMHD algorithm in Table 1 can be applied on the selected features.

We consider a simulated two-dimensional dataset with overlapping components as discussed in Baudry et al. (2010, Sec. 4.1). A sample of 600 observations was generated from a mixture of six Gaussian components, and it is available in the R  package MCLUST. Fig. 5
                        (a) clearly shows that four clusters exist: two clusters are given by axis-aligned components with diagonal covariance matrices, which form two “crosses”, and two elliptical clusters which are not axis-aligned.

The GMM model with the largest BIC is the one with six components (see Fig. 5(b)). Thus, GMM correctly selected the true generating model, but a visual inspection quickly reveals that this is not a good solution for clustering. Baudry et al. (2010) recovered the four cluster solution by merging two of the four crossing components. Fig. 5(c) and (d), respectively, shows the cluster cores and the final clustering obtained with GMMHD. Our procedure is clearly able to identify the clusters related to the four modes of highest density. The final partition obtained is equivalent to that resulting from the merging of some of the mixture components, with the exception of one observation, i.e. the one with coordinates (1.91,2.63). This fact underlines one limit of the method based on the merging of mixture components: observations assigned to a component will necessarily be grouped together, even in the final clustering.


                        Wong and Lane (1983) presented an example using data from bivariate elongated clusters (see Fig. 6
                        (a)). This is considered a challenging problem, since the groups are not linearly separable. The clustering obtained from applying GMM is shown in Fig. 6(b). Despite the fact that the two clusters are clearly not Gaussian, the procedure correctly selects two components. However, the two points in the extreme arms of the elongated clusters are assigned to the wrong groups. In this case, merging mixture components may not provide any improvement.

The application of the proposed procedure is reported in the bottom panels of Fig. 6. Panel (c) shows the cluster cores identified, which clearly distinguish two local modes. The corresponding clusters, obtained by applying the semi-supervised procedure discussed in Section  3.4, are shown in panel (d). Despite the strong non-normality of the clusters, all points are correctly classified.

Reasonable clustering methods should not only be able to recognise the presence of homogeneous groups in the data, but also to detect situations where there is no evidence of clusters. For this reason, we present a simulation study designed to investigate the behaviour of GMMHD for unimodal, skewed data. For comparison, we also considered the results from applying GMM using the MCLUST (Fraley et al., 2014) and the pdfCluster (Azzalini and Menardi, 2013) packages for R. We used the default settings for both packages.

Samples of size 
                           n
                           =
                           200
                         with dimensionality 
                           p
                           =
                           
                              {
                              2
                              ,
                              5
                              ,
                              10
                              }
                           
                         were generated from the following distributions: 
                           
                              •
                              independent 
                                    
                                       
                                          χ
                                       
                                       
                                          2
                                       
                                    
                                  distributions with 10 degrees of freedom. A bivariate example is shown in Fig. 7
                                 (a). Since this is a skewed distribution, GMM requires more than one component to approximate the underlying distribution (see Fig. 7(b)), whereas GMMHD is able to recover the unimodal nature of the data (see Fig. 7(c)).

multivariate skew 
                                    t
                                 -distribution with location 
                                    
                                       ξ
                                    
                                    =
                                    
                                       
                                          
                                             [
                                             0
                                             ,
                                             …
                                             ,
                                             0
                                             ]
                                          
                                       
                                       
                                          ⊤
                                       
                                    
                                 , scale 
                                    
                                       Ω
                                    
                                    =
                                    
                                       
                                          
                                             I
                                          
                                       
                                       
                                          p
                                       
                                    
                                 , asymmetry parameter 
                                    
                                       α
                                    
                                    =
                                    
                                       
                                          
                                             [
                                             3
                                             ,
                                             …
                                             ,
                                             3
                                             ]
                                          
                                       
                                       
                                          ⊤
                                       
                                    
                                 , and degrees of freedom 
                                    ν
                                    =
                                    3
                                 . The multivariate skew 
                                    t
                                 -distribution is obtained as 
                                    
                                       X
                                    
                                    =
                                    
                                       ξ
                                    
                                    +
                                    
                                       
                                          V
                                       
                                       
                                          −
                                          1
                                          /
                                          2
                                       
                                    
                                    
                                       Z
                                    
                                 , where 
                                    
                                       Z
                                    
                                    ∼
                                    skew-Normal
                                    
                                       (
                                       
                                          0
                                       
                                       ,
                                       
                                          Ω
                                       
                                       ,
                                       α
                                       )
                                    
                                 , and 
                                    V
                                    ∼
                                    
                                       
                                          χ
                                       
                                       
                                          2
                                       
                                    
                                    
                                       (
                                       ν
                                       )
                                    
                                    /
                                    ν
                                 . A bivariate example is shown in Fig. 8
                                 (a), with the clustering obtained by GMM and GMMHD in Fig. 8(b) and (c), respectively.


                        Tables 2 and 3
                        
                         list the distribution of the number of clusters detected in 100 samples by each method used in the comparison. As expected, GMM is biased towards the identification of elliptical clusters; hence it fails in the examples considered with a strong skewness. GMMHD accurately recovers the presence of a single cluster in both cases, and it does not suffer from increasing dimensionality. On the contrary, pdfCluster often selects too many clusters, especially in the first case when 
                           p
                           =
                           10
                        .

This dataset (Lubischew, 1962) contains 74 observations on 6 physical measurements of body characteristics from three species of flea beetles: Ch. Concinna, Ch. Heptapotamica, and Ch. Heikertingeri.

GMM provides a 5-cluster solution, where one species is correctly identified (Ch. Concinna), and the remaining two species (Ch. Heikertingeri and Heptapotamica) are divided into two subgroups. The adjusted Rand index for this partition (ARI;  Hubert and Arabie, 1985) is equal to 
                           0.7676
                        . This is a measure of agreement between two data partitions. Its expected value, when two random partitions are compared, is zero, and it achieves the maximum value of one, when two partitions perfectly coincide

As this is a six-dimensional dataset, we applied the methodology presented in Section  4 for reducing the dimensionality of the data. Fig. 9
                        (a) shows the empirical mode function, obtained with the GMMHD method applied to the data projected on to the first two GMMDR directions. The final clustering is shown in Fig. 9(b). Now the species are undoubtedly identified and separated, with ARI=1.0. Furthermore, note that the species of Ch. Heikertingeri (marked as + in Fig. 9(b)) shows a non-elliptical cluster. This explains why the GMM fit required a larger number of components than the clusters present in the data.

As a comparison, we applied the pdfCluster method of Menardi and Azzalini (2014). Despite what was reported in the paper, pdfCluster gives a perfect clustering only for values of the tuning parameter 
                           λ
                           ∈
                           
                              [
                              0.26
                              ,
                              0.32
                              ]
                           
                        , whereas four clusters are estimated with ARI=0.9681 for the default 
                           λ
                           =
                           0.1
                        .


                        Franczak et al. (2014) analysed a subset of yeast data, which contains cellular localisation sites of 1,484 proteins. They considered two localisation sites, CYT (cytosolic or cytoskeletal) and ME3 (membrane protein, no N-terminal signal), and three variables for clustering: McGeoch’s method for signal sequence recognition (mcg), the score of the ALOM membrane spanning region prediction program (alm), and the score for the discriminant analysis of the amino acid content of vacuolar and extracellular proteins (vac).


                        Franczak et al. (2014) fitted a mixture of shifted asymmetric Laplace (SAL) distributions for clustering purposes, which gave favourable results (ARI=0.8134) in contrast to GMM. In fact, the GMM with the largest BIC is model EEI with 8 components (ARI=0.4972), where a large number of components are required to account for the asymmetry in the data. After projecting the data on to the first two GMMDR directions obtained from the 8-component GMM, we apply the GMMHD methodology. The identified cluster cores are shown in Fig. 10
                        (a), whereas the final clustering is in Fig. 10(b). From the latter plot we can see that, although the two classes are clearly not Gaussian, GMMHD is able to recover the true clusters accurately. Table 4
                         reports the clustering obtained, which gave us a slight improvement in accuracy (ARI=0.8427).


                        Forina et al. (1986) reported data on 178 wines grown in the same region in Italy but derived from three different cultivars (Barbera, Barolo, Grignolino). For each wine 13 measurements of chemical and physical properties were made, such as the level of alcohol, the level of magnesium, and the colour intensity,. The dataset is available at UCI Machine learning data repository (http://archive.ics.uci.edu/ml/datasets/Wine).

Modelling the data on a standardised scale selects the GMM with eight components and VEI covariance parameterisation (diagonal, varying volume and equal shape), which results in a low clustering accuracy (ARI=0.4808). The GMMHD method applied to the first two GMMDR variates, selected as discussed in Section  4, yields the cluster cores shown in panel (a) of Fig. 11
                        . The final clustering is displayed in panel (b) of Fig. 11 and cross-tabulated against the true wine cultivars in Table 5
                        . As it can be seen, the partition obtained corresponds very closely to the true classification of wines (ARI=0.9651). This result compares favourably against those reported by Menardi and Azzalini (2014) using both pdfCluster (ARI=0.8319) and pdfCluster* (ARI=0.8962) methods.

@&#DISCUSSION@&#

This paper addresses an important problem of model-based clustering based on Gaussian mixture models, namely that the number of mixture components selected is not necessarily equal to the number of clusters. This arises when more than one single Gaussian component is needed to approximate non-Gaussian clusters. The clustering approach we propose relies on density estimation obtained from fitting a finite mixture of Gaussian distributions. Based on this, regions of high density are identified to form connected components. The corresponding cluster cores are obtained, and the remaining observations are allocated to them.

The proposed approach appears to improve the identification of non-Gaussian clusters, and enables clusters to be identified which cannot be obtained by combining mixture components. Furthermore, the method compares favourably with the approach based on nonparametric density estimation, especially as dimensionality increases.

Whereas the method is already satisfactory in many ways, there are some aspects which can be further improved. Future work will focus on improving the computational requirements, especially as the number of observations and/or the number of variables increases. The case of 
                        n
                        ≪
                        p
                     , such as in clustering data from microarray experiments, is of particular interest in applications. It could also be interesting to extend the approach based on connected components under mixture of skew-normal distributions (Lin et al., 2007b; Lin, 2009) and mixture of skew-t distributions (Lin et al., 2007a; Lin, 2010; Lee and McLachlan, 2013). How to deal with missing values is another area for possible extensions. Multiple imputation (Schafer, 1997) is a flexible and convenient paradigm for analysing data with missing values. Other approaches use EM-type algorithms for parameter estimation under various missing mechanisms (Lin, 2014; Wang, 2015). A comparison between the two paradigms would also be interesting.

All the computational work in this paper was carried out within R  (R Core Team, 2014). The GMMHD method introduced here will soon be available in the R  package MCLUST.

@&#ACKNOWLEDGEMENTS@&#

The author is grateful to the Associate Editor and two anonymous referees for their insightful comments and valuable suggestions that led to a substantial improvement of the manuscript. This work was partly supported by the Eunice Kennedy Shriver National Institute of Child Health and Development through grant R01 HD070936.

@&#REFERENCES@&#

